{
 "title": "Interspeech 2012",
 "location": "Portland, OR, USA",
 "startDate": "9/9/2012",
 "endDate": "13/9/2012",
 "chair": "General Chair: Richard Sproat",
 "conf": "Interspeech",
 "year": "2012",
 "name": "interspeech_2012",
 "series": "Interspeech",
 "SIG": "",
 "title1": "Interspeech 2012",
 "date": "9-13 September 2012",
 "booklet": "interspeech_2012.pdf",
 "papers": {
  "lee12_interspeech": {
   "authors": [
    [
     "Chin-Hui",
     "Lee"
    ]
   ],
   "title": "An information-extraction approach to speech analysis and processing",
   "original": "i12_0001",
   "page_count": 5,
   "order": 1,
   "p1": "1",
   "pn": "5",
   "abstract": [
    "The field of automatic speech recognition (ASR) has enjoyed more than 30 years of technology advances due to the extensive utilization of the hidden Markov model (HMM) framework and a concentrated effort by the community to make available a vast amount of language resources. However the ASR problem is still far from being solved because not all information available in the speech knowledge hierarchy can be directly and effectively integrated into the current top-down knowledge integration framework in the state-of-the-art systems to improve ASR performance and enhance system robustness. It is believed that some of the current knowledge insufficiency issues can be partially addressed by processing techniques that can take advantage of the full set of acoustic and language information in speech. On the other hand in human speech recognition (HSR) and spectrogram reading we often determine the linguistic identity of a sound based on detected cues and evidences that exist at various levels of the speech knowledge hierarchy, ranging from acoustic phonetics to syntax and semantics. This calls for a bottom-up knowledge integration framework that links speech processing with information extraction, by spotting speech cues with a bank of attribute detectors, weighing and combining acoustic evidences to form cognitive hypotheses, and verifying these theories until a consistent recognition decision can be reached. The recently proposed ASAT (automatic speech attribute transcription) framework is an attempt to mimic some HSR capabilities with asynchronous speech event detection followed by bottom-up speech knowledge integration and verification. In the last few years it has demonstrated potentials and offered insights in detection-based speech processing and information extraction. This presentation is intended to illustrate new possibilities of speech research via linking analysis and processing of raw speech signals with extracting multiple layers of useful speech information. By organizing these probabilistic evidences from the speech knowledge hierarchy, and integrating them into the already-powerful, top-down HMM framework we can facilitate a knowledge-rich, bottom-up and data-driven framework that will lower the entry barriers to ASR research and further enhance the capabilities and reduce some of the limitations in the state-of-the-art ASR systems. Everyone in and out of the current ASR community will be able to contribute to this worthwhile effort to build a collaborative ASR community of the 21st Century.\n",
    "Index Terms: automatic speech attribute transcription, automatic speech recognition, speech attribute detection, bottom-up knowledge integration\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-1"
  },
  "dannenberg12_interspeech": {
   "authors": [
    [
     "Roger B.",
     "Dannenberg"
    ]
   ],
   "title": "Music understanding and the future of music performance",
   "original": "i12_0550",
   "page_count": 0,
   "order": 2,
   "p1": "550",
   "pn": "",
   "abstract": [
    "Music understanding is the automatic recognition of pattern and structure in music. Music understanding problems include (1) matching and searching symbolic and audio music sequences, (2) parsing music to discover musical objects such as sections, notes, and beats, and (3) the interpretation and generation of expressive music performance. In spite of some impressive results, music understanding is only beginning to find a range of applications. Dannenberg will describe how current research might impact music performance in the future through intelligent audio editors, computer support for popular music performance, and intelligent music displays.\n",
    ""
   ]
  },
  "riley12_interspeech": {
   "authors": [
    [
     "Michael",
     "Riley"
    ]
   ],
   "title": "Weighted transducers in speech and language processing",
   "original": "i12_1347",
   "page_count": 0,
   "order": 3,
   "p1": "1347",
   "pn": "",
   "abstract": [
    "This talk will describe the use of weighted transducers in speech and language processing. Weighted transducers are automata where each transition has an input label, an output label, and a weight. They have key applications in speech recognition and synthesis, machine translation, optical character recognition, pattern matching, string processing, machine learning, information extraction and retrieval among others. We give a brief history of the field, an overview of the theory and algorithms of weighted finite-state and pushdown transducers, a discussion of available software libraries, and a description of applications to speech recognition, speech synthesis, machine translation and NLP.\n",
    ""
   ]
  },
  "lahvis12_interspeech": {
   "authors": [
    [
     "Garet",
     "Lahvis"
    ]
   ],
   "title": "Finding meaning in rodent ultrasonic vocalizations",
   "original": "i12_2129",
   "page_count": 0,
   "order": 4,
   "p1": "2129",
   "pn": "",
   "abstract": [
    "Many rodent species (Rodentia) emit vocalizations that communicate referential and emotional information. These vocalizations are emitted at frequencies that exceed human hearing abilities. With the advent of digital recordings of these ultrasonic vocalizations (USVs), there have been tremendous advances in our understanding of USV function. For instance, we now know that variations in predator threat elicit subtle differences in prairie dog alarm calls, which, in turn, engender predator-specific evasive behaviors. This model of referential communication (stimulus → vocalization → behavioral response) has also been employed to elucidate how emotional information is conveyed. For instance, rats and mice can learn that a tone predicts a stressful experience simply by hearing a companion rodent undergo this stressful contingency. These rodent studies have catalyzed new understanding of the neurobiological substrates of empathy. Rats also utilize different USV frequencies to indicate aggression versus affiliation. In turn, these USVs elicit different patterns of brain activity and behavioral responses of the rodents that hear these calls. These studies have opened new pathways to our understanding of rat psychological experience . what they prefer, what they find aversive. Mice also use different call patterns for different levels of social interaction. By integrating the use of spectrographic analyses with behavioral measures of classical and operant learning, we can begin to unravel the richness of rodent social experience and begin to explain the mechanisms of human social relationships, as never before possible.\n",
    ""
   ]
  },
  "yu12_interspeech": {
   "authors": [
    [
     "Dong",
     "Yu"
    ],
    [
     "Li",
     "Deng"
    ],
    [
     "Frank",
     "Seide"
    ]
   ],
   "title": "Large vocabulary speech recognition using deep tensor neural networks",
   "original": "i12_0006",
   "page_count": 4,
   "order": 5,
   "p1": "6",
   "pn": "9",
   "abstract": [
    "Recently, we proposed and developed the context-dependent deep neural network hidden Markov models (CD-DNN-HMMs) for large vocabulary speech recognition and achieved highly promising recognition results including over one third fewer word errors than the discriminatively trained, conventional HMM-based systems on the 300hr Switchboard benchmark task. In this paper, we extend DNNs to deep tensor neural networks (DTNNs) in which one or more layers are double-projection and tensor layers. The basic idea of the DTNN comes from our realization that many factors interact with each other to predict the output. To represent these interactions, we project the input to two nonlinear subspaces through the double-projection layer and model the interactions between these two subspaces and the output neurons through a tensor with three-way connections. Evaluation on 30hr Switchboard task indicates that DTNNs can outperform DNNs with similar number of parameters with 5% relative word error reduction.\n",
    "Index Terms: automatic speech recognition, tensor deep neural networks, CD-DNN-HMM, large vocabulary\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-2"
  },
  "kingsbury12_interspeech": {
   "authors": [
    [
     "Brian",
     "Kingsbury"
    ],
    [
     "Tara N.",
     "Sainath"
    ],
    [
     "Hagen",
     "Soltau"
    ]
   ],
   "title": "Scalable minimum Bayes risk training of deep neural network acoustic models using distributed hessian-free optimization",
   "original": "i12_0010",
   "page_count": 4,
   "order": 6,
   "p1": "10",
   "pn": "13",
   "abstract": [
    "Training neural network acoustic models with sequence-discriminative criteria, such as state-level minimum Bayes risk (sMBR), been shown to produce large improvements in performance over cross-entropy. However, because they entail the processing of lattices, sequence criteria are much more computationally intensive than cross-entropy. We describe a distributed neural network training algorithm, based on Hessian-free optimization, that scales to deep networks and large data sets. For the sMBR criterion, this training algorithm is faster than stochastic gradient descent by a factor of 5.5 and yields a 4.4% relative improvement in word error rate on a 50-hour broadcast news task. Distributed Hessianfree sMBR training yields relative reductions in word error rate of 7-13% over cross-entropy training with stochastic gradient descent on two larger tasks: Switchboard and DARPA RATS noisy Levantine Arabic. Our best Switchboard DBN achieves a word error rate of 16.4% on rt03-FSH.\n",
    "Index Terms: deep learning, discriminative training, secondorder optimization, distributed computing\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-3"
  },
  "saon12_interspeech": {
   "authors": [
    [
     "George",
     "Saon"
    ],
    [
     "Brian",
     "Kingsbury"
    ]
   ],
   "title": "Discriminative feature-space transforms using deep neural networks",
   "original": "i12_0014",
   "page_count": 4,
   "order": 7,
   "p1": "14",
   "pn": "17",
   "abstract": [
    "We present a deep neural network (DNN) architecture which learns time-dependent offsets to acoustic feature vectors according to a discriminative objective function such as maximum mutual information (MMI) between the reference words and the transformed acoustic observation sequence. A key ingredient in this technique is a greedy layer-wise pretraining of the network based on minimum squared error between the DNN outputs and the offsets provided by a linear feature-space MMI (FMMI) transform. Next, the weights of the pretrained network are updated with stochastic gradient ascent by backpropagating the MMI gradient through the DNN layers. Experiments on a 50 hour English broadcast news transcription task show a 4% relative improvement using a 6-layer DNN transform over a state-of-the-art speakeradapted system with FMMI and model-space discriminative training.\n",
    "Index Terms: speech recognition, deep neural networks\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-4"
  },
  "tuske12_interspeech": {
   "authors": [
    [
     "Zoltán",
     "Tüske"
    ],
    [
     "Martin",
     "Sundermeyer"
    ],
    [
     "Ralf",
     "Schlüter"
    ],
    [
     "Hermann",
     "Ney"
    ]
   ],
   "title": "Context-dependent MLPs for LVCSR: TANDEM, hybrid or both?",
   "original": "i12_0018",
   "page_count": 4,
   "order": 8,
   "p1": "18",
   "pn": "21",
   "abstract": [
    "Gaussian Mixture Model (GMM) and Multi Layer Perceptron (MLP) based acoustic models are compared on a French large vocabulary continuous speech recognition (LVCSR) task. In addition to optimizing the output layer size of the MLP, the ef- fect of the deep neural network structure is also investigated. Moreover, using different linear transformations (time deriva- tives, LDA, CMLLR) on conventional MFCC, the study is also extended to MLP based probabilistic and bottle-neck TANDEM features. Results show that using either the hybrid or bottle- neck TANDEM approach leads to similar recognition perfor- mance. However, the best performance is achieved when deep MLP acoustic models are trained on concatenated cepstral and context-dependent bottle-neck features. Further experiments re- veal the importance of the neighbouring frames in case of MLP based modeling, and that its gain over GMM acoustic models is strongly reduced by more complex features.\n",
    "Index Terms: HMM, GMM, MLP, bottle-neck, hybrid, ASR, TANDEM\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-5"
  },
  "maas12_interspeech": {
   "authors": [
    [
     "Andrew L.",
     "Maas"
    ],
    [
     "Quoc V.",
     "Le"
    ],
    [
     "Tyler M.",
     "O'Neil"
    ],
    [
     "Oriol",
     "Vinyals"
    ],
    [
     "Patrick",
     "Nguyen"
    ],
    [
     "Andrew Y.",
     "Ng"
    ]
   ],
   "title": "Recurrent neural networks for noise reduction in robust ASR",
   "original": "i12_0022",
   "page_count": 4,
   "order": 9,
   "p1": "22",
   "pn": "25",
   "abstract": [
    "Recent work on deep neural networks as acoustic models for automatic speech recognition (ASR) have demonstrated substantial performance improvements. We introduce a model which uses a deep recurrent auto encoder neural network to denoise input features for robust ASR. The model is trained on stereo (noisy and clean) audio features to predict clean features given noisy input. The model makes no assumptions about how noise affects the signal, nor the existence of distinct noise environments. Instead, the model can learn to model any type of distortion or additive noise given sufficient training data. We demonstrate the model is competitive with existing feature denoising approaches on the Aurora2 task, and outperforms a tandem approach where deep networks are used to predict phoneme posteriors directly.\n",
    "Index Terms: neural networks, robust ASR, deep learning\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-6"
  },
  "chen12_interspeech": {
   "authors": [
    [
     "Xie",
     "Chen"
    ],
    [
     "Adam",
     "Eversole"
    ],
    [
     "Gang",
     "Li"
    ],
    [
     "Dong",
     "Yu"
    ],
    [
     "Frank",
     "Seide"
    ]
   ],
   "title": "Pipelined back-propagation for context-dependent deep neural networks",
   "original": "i12_0026",
   "page_count": 4,
   "order": 10,
   "p1": "26",
   "pn": "29",
   "abstract": [
    "The Context-Dependent Deep-Neural-Network HMM, or CDDNN-HMM, is a recently proposed acoustic-modeling technique for HMM-based speech recognition that can greatly outperform conventional Gaussian-mixture based HMMs. For example, a CD-DNN-HMM trained on the 2000h Fisher corpus achieves 14.4% word error rate on the Hub5'00-FSH speakerindependent phone-call transcription task, compared to 19.6% obtained by a state-of-the-art, conventional discriminatively trained GMM-based HMM.   That CD-DNN-HMM, however, took 59 days to train on a modern GPGPU — the immense computational cost of the minibatch based back-propagation (BP) training is a major roadblock. Unlike the familiar Baum-Welch training for conventional HMMs, BP cannot be efficiently parallelized across data.   In this paper we show that the pipelined approximation to BP, which parallelizes computation with respect to layers, is an efficient way of utilizing multiple GPGPU cards in a single server. Using 2 and 4 GPGPUs, we achieve a 1.9 and 3.3 times end-to-end speed-up, at parallelization efficiency of 0.95 and 0.82, respectively, at no loss of recognition accuracy.\n",
    "Index Terms: speech recognition, deep neural networks, parallelization, GPGPU\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-7"
  },
  "vinyals12_interspeech": {
   "authors": [
    [
     "Oriol",
     "Vinyals"
    ],
    [
     "Li",
     "Deng"
    ]
   ],
   "title": "Are sparse representations rich enough for acoustic modeling?",
   "original": "i12_2570",
   "page_count": 4,
   "order": 11,
   "p1": "2570",
   "pn": "2573",
   "abstract": [
    "We propose a novel approach to acoustic modeling based on recent advances in sparse representations. The key idea in sparse coding is to compute a compressed local representation of a signal via an over-complete basis or dictionary that is learned in an unsupervised way. In this study, we compute the local representation on speech spectrogram as the raw “signal” and use it as the local sparse code to perform a standard phone classification task. A linear classifier is used that directly receives the coding space for making the classification decision. The simplicity of the linear classifier allows us to assess whether the sparse representations are sufficiently rich to serve as effective acoustic features for discriminating speech classes. Our experiments demonstrate competitive error rates when compared to other shallow approaches. An examination of the dictionary learned in sparse feature extraction demonstrates meaningful acoustic-phonetic properties that are captured by a collection of the dictionary entries.\n",
    "Index Terms: sparse coding, acoustic modeling, phone recognition, acoustic-phonetic properties\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-8"
  },
  "xiao12_interspeech": {
   "authors": [
    [
     "Yeming",
     "Xiao"
    ],
    [
     "Zhen",
     "Zhang"
    ],
    [
     "Shang",
     "Cai"
    ],
    [
     "Jielin",
     "Pan"
    ],
    [
     "Yonghong",
     "Yan"
    ]
   ],
   "title": "A initial attempt on task-specific adaptation for deep neural network-based large vocabulary continuous speech recognition",
   "original": "i12_2574",
   "page_count": 4,
   "order": 12,
   "p1": "2574",
   "pn": "2577",
   "abstract": [
    "In the state-of-the-art automatic speech recognition (ASR) systems, adaption techniques are used to the mitigate performance degradation caused by the mismatch in the training and testing procedure. Although there are bunch of adaption techniques for the hidden Markov models (HMM)-GMM-based system, there is rare work about the adaption in the hybrid artificial neural network~(ANN)/HMM-based system. Recently, there is a resurgence on ANN/HMM scheme for ASR with the success of context dependent deep neural network HMM~(CD-DNN/ HMM). Therefore in this paper, we present our initial efforts on the adaption techniques in the CD-DNN/HMM system. Specially, a linear input network(LIN)-based method and a neural network retraining(NNR)-based method is experimentally explored for the the task-adaptation purpose. Experiments on conversation telephone speech data set shows that these techniques can improve the system significantly and LINbased method seems to work better with medium mount of adaptation data.\n",
    "Index Terms: deep neural network, pre-training, speaker adaptation, LVCSR\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-9"
  },
  "jaitly12_interspeech": {
   "authors": [
    [
     "Navdeep",
     "Jaitly"
    ],
    [
     "Patrick",
     "Nguyen"
    ],
    [
     "Andrew",
     "Senior"
    ],
    [
     "Vincent",
     "Vanhoucke"
    ]
   ],
   "title": "Application of pretrained deep neural networks to large vocabulary speech recognition",
   "original": "i12_2578",
   "page_count": 4,
   "order": 13,
   "p1": "2578",
   "pn": "2581",
   "abstract": [
    "The use of Deep Belief Networks (DBN) to pretrain Neural Networks has recently led to a resurgence in the use of Artificial Neural Network - Hidden Markov Model (ANN/HMM) hybrid systems for Automatic Speech Recognition (ASR). In this paper we report results of a DBN-pretrained contextdependent ANN/HMM system trained on two datasets that are much larger than any reported previously with DBN-pretrained ANN/HMM systems - 5870 hours of Voice Search and 1400 hours of YouTube data. On the first dataset, the pretrained ANN/HMM system outperforms the best Gaussian Mixture Model - Hidden Markov Model (GMM/HMM) baseline, built with a much larger dataset by 3.7% absolute WER, while on the second dataset, it outperforms the GMM/HMM baseline by 2.9% absolute. Maximum Mutual Information (MMI) fine tuning and model combination using Segmental Conditional Random Fields (SCARF) give additional gains of 0.1% and 0.4% on the first dataset and 0.6% and 1.1% absolute on the second dataset.\n",
    "Index Terms: Deep Belief Networks, Acoustic Modeling, Artificial Neural Network, ANN/HMM\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-10"
  },
  "qian12_interspeech": {
   "authors": [
    [
     "Yanmin",
     "Qian"
    ],
    [
     "Jia",
     "Liu"
    ]
   ],
   "title": "Cross-lingual and ensemble MLPs strategies for low-resource speech recognition",
   "original": "i12_2582",
   "page_count": 4,
   "order": 14,
   "p1": "2582",
   "pn": "2585",
   "abstract": [
    "Recently there has been some interest in the question of how to build LVCSR systems for the low-resource languages. The scenario we focus on here is having only one hour of acoustic training data in the \"target\" language, but more plentiful data in other languages. This paper presents approaches using MLP based features: we construct a low-resource system with additional sources of information from the non-target languages to train the cross-lingual MLPs. A hierarchical architecture and multi-stream strategy are applied on the cross-lingual phone level, to improve the neural network more discriminatively. Additionally, an elaborate ensemble system with various acoustic feature streams and context expansion lengths is proposed. After system combination with these two strategies we get significant improvements of more than 8% absolute versus a conventional baseline in this low-resource scenario with only one hour of target training data.\n",
    "Index Terms: low-resource language; cross-lingual posterior features; hierarchical architectures; ensemble system\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-11"
  },
  "vu12_interspeech": {
   "authors": [
    [
     "Ngoc Thang",
     "Vu"
    ],
    [
     "Wojtek",
     "Breiter"
    ],
    [
     "Florian",
     "Metze"
    ],
    [
     "Tanja",
     "Schultz"
    ]
   ],
   "title": "Initialization schemes for multilayer perceptron training and their impact on ASR performance using multilingual data",
   "original": "i12_2586",
   "page_count": 4,
   "order": 15,
   "p1": "2586",
   "pn": "2589",
   "abstract": [
    "In this paper we present our latest investigation on initialization schemes for Multilayer Perceptron (MLP) training using multilingual data. We show that the overall performance of an MLP network improves significantly by initializing it with a multilingual MLP. We propose a new strategy called \"open target language\" MLP to train more flexible models for language adaptation, which is particularly suited for small amounts of training data. Furthermore, by applying Bottle-Neck feature (BN) initialized with multilingual MLP the ASR performance increases on both, on those languages which were used for multilingual MLP training, and on a new language. Our experiments show word error rate improvements of up to 16.9% relative on a range of tasks for different target languages (Creole and Vietnamese) with manually and automatically transcribed training data.\n",
    "Index Terms: multilingual multilayer perceptron, Bottle-Neck feature, language adaptation\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-12"
  },
  "siniscalchi12_interspeech": {
   "authors": [
    [
     "Sabato Marco",
     "Siniscalchi"
    ],
    [
     "Jinyu",
     "Li"
    ],
    [
     "Chin-Hui",
     "Lee"
    ]
   ],
   "title": "Hermitian based hidden activation functions for adaptation of hybrid HMM/ANN models",
   "original": "i12_2590",
   "page_count": 4,
   "order": 16,
   "p1": "2590",
   "pn": "2593",
   "abstract": [
    "This work is concerned with speaker adaptation techniques for artificial neural network (ANN) implemented as feed forward multi-layer perceptrons (MLPs) in the context of large vocabulary continuous speech recognition (LVCSR). Most successful speaker adaptation techniques for MLPs consist of augmenting the neural architecture with a linear transformation network connected to either the input or the output layer. The weights of this additional linear layer are learned during the adaptation phase while all of the other weights are kept frozen in order to avoid over-fitting. In doing so, the structure of the speaker-dependent (SD) and speaker-independent (SI) architecture differs and the number of adaptation parameters depends upon the dimension of either the input or output layers. We propose a more flexible neural architecture for speaker-adaptation to overcome the limits of current approaches. This flexibility is achieved by adopting hidden activation functions that can be learned directly from the adaptation data. This adaptive capability of the hidden activation function is achieved through the use of orthonormal Hermite polynomials. Experimental evidence gathered on the Nov92 task demonstrates the viability of the proposed technique.\n",
    "Index Terms: Connectionist speech recognition systems, Neural networks, Adaptation algorithms, Speech recognition\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-13"
  },
  "kubo12_interspeech": {
   "authors": [
    [
     "Yotaro",
     "Kubo"
    ],
    [
     "Takaaki",
     "Hori"
    ],
    [
     "Atsushi",
     "Nakamura"
    ]
   ],
   "title": "Integrating deep neural networks into structural classification approach based on weighted finite-state transducers",
   "original": "i12_2594",
   "page_count": 4,
   "order": 17,
   "p1": "2594",
   "pn": "2597",
   "abstract": [
    "Recently, deep neural networks have been collecting attention of speech researchers due to its capability of handling nonlinearity in speech feature vectors. On the other hand, speech recognition based on structured classification is also considered important since it successfully exploits interdependency of several information sources. In this paper, we focus on the structured classification method based on weighted finite-state transducers (WFSTs) that introduces linear classification term for each arc transition cost in decoding network to capture contextural information of labels. Since these two approaches attempt to improve representation of features and labels, respectively, the combination of these models would be efficient because of complementarity. Thus, this paper proposes a method that combines deep neural network techniques with WFST-based structured classification approaches. In the proposed method, DNNs are used to extract classification friendly features; and then, the features are classified by using WFST-based structured classifiers. The proposed method is evaluated by using TIMIT continuous phoneme recognition tasks. We confirmed that combining structured classification leads to stable performance improvements even from the well-optimized deep neural network acoustic models.\n",
    "Index Terms: Speech recognition, deep neural networks, structured classification, weighted finite-state transducers\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-14"
  },
  "deng12_interspeech": {
   "authors": [
    [
     "Li",
     "Deng"
    ],
    [
     "Brian",
     "Hutchinson"
    ],
    [
     "Dong",
     "Yu"
    ]
   ],
   "title": "Parallel training for deep stacking networks",
   "original": "i12_2598",
   "page_count": 4,
   "order": 18,
   "p1": "2598",
   "pn": "2601",
   "abstract": [
    "The Deep stacking network (DSN) is a special type of deep architecture developed to enable parallel learning of its weight parameters distributed over large CPU clusters. This capability of DSN in learning parallelism is unique among all deep models explored so far. As a prospective key component of next-generation speech recognizers, the architectural design of the DSN and its parallel learning enable DSNfs scalability over a potentially unlimited amount of training data and over CPU clusters. In this paper, we present our first parallel implementation of the DSN learning algorithm. Particularly, we show the tradeoff between the time/memory saving via a high degree of parallelism and the associated cost arising from inter-CPU communication. In addition, in phone classification experiments, we demonstrate a significantly lowered error rate achieved by DSN with full-batch training, which is enabled by parallel implementation in a CPU cluster, than with the corresponding mini-batch training exploited prior to the work reported in this paper.\n",
    "Index Terms: parallel and distributed computing, deep stacking networks, full-batch training, phone classification\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-15"
  },
  "qian12b_interspeech": {
   "authors": [
    [
     "Yanmin",
     "Qian"
    ],
    [
     "Jia",
     "Liu"
    ]
   ],
   "title": "Articulatory feature based multilingual MLPs for low-resource speech recognition",
   "original": "i12_2602",
   "page_count": 4,
   "order": 19,
   "p1": "2602",
   "pn": "2605",
   "abstract": [
    "Large vocabulary continuous speech recognition is particularly difficult for low-resource languages. In the scenario we focus on here is that there is a very limited amount of acoustic training data in the target language, but more plentiful data in other languages. In our approach, we investigate approaches based on Automatic Speech Attribute Transcription (ASAT) framework, and train universal classifiers using multilanguages to learn articulatory features. A hierarchical architecture is applied on both the articulatory feature and phone level, to make the neural network more discriminative. Finally we train the multilayer perceptrons using multi-streams from different languages and obtain MLPs for this low-resource application. In our experiments, we get significant improvements of about 12% relative versus a conventional baseline in this low-resource scenario.\n",
    "Index Terms: low-resource language; multilayer perceptrons; articulatory features; hierarchical architectures\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-16"
  },
  "fernandezastudillo12_interspeech": {
   "authors": [
    [
     "Ramón",
     "Fernandez Astudillo"
    ],
    [
     "Alberto",
     "Abad"
    ],
    [
     "João Paulo da Silva",
     "Neto"
    ]
   ],
   "title": "Uncertainty-driven compensation of multi-stream MLP acoustic models for robust ASR ramon",
   "original": "i12_2606",
   "page_count": 4,
   "order": 20,
   "p1": "2606",
   "pn": "2609",
   "abstract": [
    "In this paper we show how the robustness of multi-stream multi-layer perceptron (MLP) acoustic models can be increased through uncertainty propagation and decoding. We demonstrate that MLP uncertainty decoding yields consistent improvements over using minimum mean square error (MMSE) feature enhancement in MFCC and RASTA-LPCC domains. We introduce as well formulas for the computation of the uncertainty associated to the acoustic likelihood computation and explore different stream integration schemes using this uncertainty on the AURORA4 corpus.\n",
    "Index Terms: uncertainty propagation, observation uncertainty, MLP, multi-stream\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-17"
  },
  "boril12_interspeech": {
   "authors": [
    [
     "Hynek",
     "Bořil"
    ],
    [
     "Abhijeet",
     "Sangwan"
    ],
    [
     "John H. L.",
     "Hansen"
    ]
   ],
   "title": "Arabic dialect identification - \"is the secret in the silence?\" and other observations",
   "original": "i12_0030",
   "page_count": 4,
   "order": 21,
   "p1": "30",
   "pn": "33",
   "abstract": [
    "Conversational telephone speech (CTS) collections of Arabic dialects distributed trough the Linguistic Data Consortium (LDC) provide an invaluable resource for the development of robust speech systems including speaker and speech recognition, translation, spoken dialogue modeling, and information summarization. They are frequently relied on also in language (LID) and dialect identification (DID) evaluations. The first part of this study attempts to identify the source of the relatively high DID performance on LDCfs Arabic CTS corpora seen in recent literature. It is found that recordings of each dialect exhibit unique channel and noise characteristics and that silence regions are sufficient for performing reasonably accurate DID. The second part focuses on phonotactic dialect modeling that utilizes phone recognizers and support vector machines (PRSVM). New N-gram normalization of PRSVM input supervectors is introduced and shown to outperform the standard approach used in current LID and DID systems.\n",
    "Index Terms: Arabic dialect identification, channel characteristics, LDC corpora, PRSVM\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-18"
  },
  "greenberg12_interspeech": {
   "authors": [
    [
     "Craig S.",
     "Greenberg"
    ],
    [
     "Alvin F.",
     "Martin"
    ],
    [
     "Mark A.",
     "Przybocki"
    ]
   ],
   "title": "The 2011 NIST language recognition evaluation",
   "original": "i12_0034",
   "page_count": 4,
   "order": 22,
   "p1": "34",
   "pn": "37",
   "abstract": [
    "In 2011, NIST held the most recent in an ongoing series of Language Recognition Evaluations originating in 1996. The 2011 NIST Language Recognition Evaluation (LRE11) featured 24 languages, including nine languages new to the LRE series, from two different source types, and had participation from 23 research organizations.   LRE11 utilized a new evaluation metric, which focused on difficult to distinguish language pairs. The most difficult pairs were generally contained within clusters of linguistically similar languages. For example, the Hindi/Urdu pair and the Lao/Thai pair both proved to be very challenging to distinguish. Pashto and Bengali were found to be confusable with a wide range of languages, and some progress was observed in distinguishing American English from Indian English.\n",
    "Index Terms: language recognition, language detection, NIST LRE, NIST evaluation\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-19"
  },
  "rodriguezfuentes12_interspeech": {
   "authors": [
    [
     "Luis Javier",
     "Rodríguez-Fuentes"
    ],
    [
     "Mikel",
     "Penagarikano"
    ],
    [
     "Amparo",
     "Varona"
    ],
    [
     "Mireia",
     "Diez"
    ],
    [
     "Germán",
     "Bordel"
    ],
    [
     "Alberto",
     "Abad"
    ],
    [
     "David",
     "Martínez"
    ],
    [
     "Jesus",
     "Villalba"
    ],
    [
     "Alfonso",
     "Ortega"
    ],
    [
     "Eduardo",
     "Lleida"
    ]
   ],
   "title": "The BLZ submission to the NIST 2011 LRE: data collection, system development and performance",
   "original": "i12_0038",
   "page_count": 4,
   "order": 23,
   "p1": "38",
   "pn": "41",
   "abstract": [
    "This paper describes the most relevant features of a collaborative multi-site submission to the NIST 2011 Language Recognition Evaluation (LRE), consisting of one primary and three contrastive systems, each fusing different combinations of 13 state-of-the-art (acoustic and phonotactic) language recognition subsystems. The collaboration focused on collecting and sharing training data for those target languages for which few development data were provided by NIST, and on defining a common development dataset to train backend and fusion parameters and select the best fusions. Official and post-key results are presented and compared, revealing that the greedy approach applied to select the best fusions provided suboptimal but very competitive performance. Several factors contributed to the high performance attained by BLZ systems, including the availability of training data for low resource target languages, the reliability of the development dataset (consisting only of data audited by NIST), the diversity of modeling approaches, features and datasets in the systems considered for fusion, and the effectiveness of the search for optimal fusions.\n",
    "Index Terms: Spoken Language Recognition, NIST 2011 LRE, Multiclass Discriminative Fusion, Greedy Search\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-20"
  },
  "dharo12_interspeech": {
   "authors": [
    [
     "Luis Fernando",
     "D'Haro"
    ],
    [
     "Ondřej",
     "Glembek"
    ],
    [
     "Oldřich",
     "Plchot"
    ],
    [
     "Pavel",
     "Matějka"
    ],
    [
     "Mehdi",
     "Soufifar"
    ],
    [
     "Ricardo",
     "Cordoba"
    ],
    [
     "Jan",
     "Černocký"
    ]
   ],
   "title": "Phonotactic language recognition using ivvectors and phoneme posteriogram counts",
   "original": "i12_0042",
   "page_count": 4,
   "order": 24,
   "p1": "42",
   "pn": "45",
   "abstract": [
    "This paper describes a novel approach to phonotactic LID, where instead of using soft-counts based on phoneme lattices, we use posteriogram to obtain n-gram counts. The high-dimensional vectors of counts are reduced to low-dimensional units for which we adapted the commonly used term i-vectors. The reduction is based on multinomial subspace modeling and is designed to work in the total-variability space. The proposed technique was tested on the NIST 2009 LRE set with better results to a system based on using soft-counts (Cavg on 30s: 3.15% vs 3.43%), and with very good results when fused with an acoustic i-vector LID system (Cavg on 30s acoustic 2.4% vs 1.25%). The proposed technique is also compared with another low dimensional projection system based on PCA. In comparison with the original soft-counts, the proposed technique provides better results, reduces the problems due to sparse counts, and avoids the process of using pruning techniques when creating the lattices.\n",
    "Index Terms: subspace modeling, multinomial distributions, LID\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-21"
  },
  "mccree12_interspeech": {
   "authors": [
    [
     "Alan",
     "McCree"
    ],
    [
     "Bengt",
     "Borgström"
    ]
   ],
   "title": "Supervector LDA: a new approach to reduced-complexity i-vector language recognition",
   "original": "i12_0046",
   "page_count": 4,
   "order": 25,
   "p1": "46",
   "pn": "49",
   "abstract": [
    "In this paper, we extend our previous analysis of Gaussian Mixture Model (GMM) subspace compensation techniques using Gaussian modeling in the supervector space combined with additive channel and observation noise. We show that under the modeling assumptions of a totalvariability i-vector system, full Gaussian supervector scoring can also be performed cheaply in the total subspace, and that i-vector scoring can be viewed as an approximation to this. Next, we show that covariance matrix estimation in the i-vector space can be used to generate PCA estimates of supervector covariance matrices needed for Joint Factor Analysis (JFA). Finally, we derive a new technique for reduced-dimension i-vector extraction which we call Supervector LDA (SV-LDA), and demonstrate a 100-dimensional i-vector language recognition system with equivalent performance to a 600-dimensional version at much lower complexity.\n",
    "Index Terms: language recognition, Gaussian mixture model, Wiener filter, factor analysis, i-vector, LDA\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-22"
  },
  "matejka12_interspeech": {
   "authors": [
    [
     "Pavel",
     "Matějka"
    ],
    [
     "Oldřich",
     "Plchot"
    ],
    [
     "Mehdi",
     "Soufifar"
    ],
    [
     "Ondřej",
     "Glembek"
    ],
    [
     "Luis Fernando",
     "D'Haro"
    ],
    [
     "Karel",
     "Veselý"
    ],
    [
     "František",
     "Grézl"
    ],
    [
     "Jeff",
     "Ma"
    ],
    [
     "Spyros",
     "Matsoukas"
    ],
    [
     "Najim",
     "Dehak"
    ]
   ],
   "title": "Patrol team language identification system for DARPA RATS P1 evaluation",
   "original": "i12_0050",
   "page_count": 4,
   "order": 26,
   "p1": "50",
   "pn": "53",
   "abstract": [
    "This paper describes the language identification (LID) system developed by the Patrol team for the first phase of the DARPA RATS (Robust Automatic Transcription of Speech) program, which seeks to advance state of the art detection capabilities on audio from highly degraded communication channels. We show that techniques originally developed for LID on telephone speech (e.g., for the NIST language recognition evaluations) remain effective on the noisy RATS data,provided that careful consideration is applied when designing the training and development sets. In addition, we show significant improvements from the use of Wiener filtering, neural network based i-vector, language dependent i-vector modeling, and fusion.\n",
    "Index Terms: language identification, noisy speech\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-23"
  },
  "hu12_interspeech": {
   "authors": [
    [
     "Fang",
     "Hu"
    ],
    [
     "Yungang",
     "Wu"
    ],
    [
     "Wen",
     "Xu"
    ],
    [
     "Demin",
     "Han"
    ]
   ],
   "title": "Articulatory strategies in obstruent production in Mandarin esophageal speech",
   "original": "i12_0054",
   "page_count": 4,
   "order": 27,
   "p1": "54",
   "pn": "57",
   "abstract": [
    "Based on the comparison between 4 esophageal speakers and 4 normal laryngeal speakers, this study investigated the voice onset time (VOT) characteristics and the linguopalatal articulation in the production of Mandarin obstruent consonants. Results show that esophageal speakers distinguish unaspirated vs. aspirated plosives or affricates in a similar way as laryngeal speakers do. However, the aspirated plosives and affricates have a shorter VOT whereas the unaspirated plosives and affricates have a longer VOT in esophageal speech than in laryngeal speech. Interestingly, esophageal speech exhibits a significantly more extensive linguopalatal contact than normal speech does. Results suggest that articulatory strategies have been adjusted to facilitate the linguopalatal articulation as well as the sub-tosupra- laryngeal coordination by using a narrower air way in the production of esophageal speech.\n",
    "Index Terms: articulatory strategies, obstruent production, Mandarin esophageal speech\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-24"
  },
  "bechet12_interspeech": {
   "authors": [
    [
     "Marion",
     "Béchet"
    ],
    [
     "Fabrice",
     "Hirsch"
    ],
    [
     "Camille",
     "Fauth"
    ],
    [
     "Rudolph",
     "Sock"
    ]
   ],
   "title": "Consonantal space area in children with a cleft palate: an acoustic study",
   "original": "i12_0058",
   "page_count": 4,
   "order": 28,
   "p1": "58",
   "pn": "61",
   "abstract": [
    "The aim of this acoustic study is to examine place of articulation during the production of voiced plosives by children with a cleft palate. The data is compared with those obtained from control children without speech disorders.   Formant values of F2 and F3 are measured at burst release of the plosives. Analyses are carried out for 52 children, from 9 to 18 years old. Two groups were established: one of 26 children with a cleft palate, and one of 26 children with no speech disorder.   Results reveal differences in F2/F3 values between disordered children and unimpaired ones, and also between the different age groups, for disordered and control children. Differences in articulatory strategies are inferred from the analyses of F2/F3 relations\n",
    "Index Terms: Formant, cleft palate, children, clinical phonetics\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-25"
  },
  "paja12_interspeech": {
   "authors": [
    [
     "Milton Sarria",
     "Paja"
    ],
    [
     "Tiago H.",
     "Falk"
    ]
   ],
   "title": "Automated dysarthria severity classification for improved objective intelligibility assessment of spastic dysarthric speech",
   "original": "i12_0062",
   "page_count": 4,
   "order": 29,
   "p1": "62",
   "pn": "65",
   "abstract": [
    "In this paper, automatic dysarthria severity classification is explored as a tool to advance objective intelligibility prediction of spastic dysarthric speech. A Mahalanobis distance-based discriminant analysis classifier is developed based on a set of acoustic features formerly proposed for intelligibility prediction and voice pathology assessment. Feature selection is used to sift salient features for both the disorder severity classification and intelligibility prediction tasks. Experimental results show that a two-level severity classifier combined with a 9-dimensional intelligibility prediction mapping can achieve 0.92 correlation and 12.52 root-mean-square error with subjective intelligibility ratings. The effects of classification errors on intelligibility accuracy are also explored and shown to be insignificant.\n",
    "Index Terms: Intelligibility, dysarthria, diagnosis\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-26"
  },
  "kacha12_interspeech": {
   "authors": [
    [
     "Abdellah",
     "Kacha"
    ],
    [
     "Francis",
     "Grenez"
    ],
    [
     "Jean",
     "Schoentgen"
    ]
   ],
   "title": "Assessment of disordered voices using empirical mode decomposition in the log-spectral domain",
   "original": "i12_0066",
   "page_count": 4,
   "order": 30,
   "p1": "66",
   "pn": "69",
   "abstract": [
    "Empirical mode decomposition (EMD) algorithm is proposed as an alternative to decompose the log of the magnitude spectrum of the speech signal into its harmonic, envelope and noise components and the harmonic-to-noise ratio is used to summarize the degree of disturbance in the speech signal. The empirical mode decomposition algorithm is a tool for the analysis of multi-component signals. The analysis method does not require a priori fixed basis function like conventional analysis methods (e.g. Fourier transform and wavelet transform).The proposed method is tested on synthetic vowels and natural speech. The corpus of synthetic vowels comprises 48 stimuli of synthetic sounds [a] that combine three values of vocal frequency, four levels of jitter frequency and four levels of additive noise. The corpora of natural speech comprise a concatenation of the vowel [a] with two Dutch sentences produced by 28 normophonic and 223 speakers with different degrees of dysphonia.\n",
    "Index Terms: Disordered voices, empirical mode decomposition, harmonic-to-noise ratio.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-27"
  },
  "fuchs12_interspeech": {
   "authors": [
    [
     "Anna Katharina",
     "Fuchs"
    ],
    [
     "Martin",
     "Hagmüller"
    ]
   ],
   "title": "Learning an artificial <i>F</i><sub>0</sub>-contour for ALT speech",
   "original": "i12_0070",
   "page_count": 4,
   "order": 31,
   "p1": "70",
   "pn": "73",
   "abstract": [
    "The Artificial Larynx Transducer (ALT), as a possibility to re-obtain audible speech for people who had to undergo a total laryngectomy, is known since decades. Not only the design and underlying technique but also the poor speech quality and intelligibility have not improved until now. In a world where technology rules the daily live, it is necessary to use the known technology to improve the quality of live for handicapped people.   One reason for the lack of naturalness is the constant vibration of the ALT. A method to substantially improve ALT speech is to introduce a varying fundamental frequency (F0) - contour. In this paper we present a new method to automatically learn an artificial F0-contour. The used model is a Gaussian mixture model (GMM) which is trained with a database containing speech of ALT users as well as healthy people. Informal listening tests suggest that this approach is a first step for a subsequent overall enhancement technique for speech produced by an ALT.\n",
    "Index Terms: alaryngeal speech, Artificial Larynx Transducer (ALT), fundamental frequency, speech enhancement, laryngectomy, GMM\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-28"
  },
  "richmond12_interspeech": {
   "authors": [
    [
     "Korin",
     "Richmond"
    ],
    [
     "Steve",
     "Renals"
    ]
   ],
   "title": "Ultrax: an animated midsagittal vocal tract display for speech therapy",
   "original": "i12_0074",
   "page_count": 4,
   "order": 32,
   "p1": "74",
   "pn": "77",
   "abstract": [
    "Speech sound disorders (SSD) are the most common communication impairment in childhood, and can unfortunately hamper social development and learning. Current speech therapy interventions must rely predominantly on the auditory skills of the child, as little technology is available to assist in diagnosis and therapy of SSDs. Realtime visualisation of tongue movements would bring enormous benefit. An ultrasound scanner offers this possibility, though its display has certain limitations which may make it hard to interpret. Our ultimate goal is to address these deficiencies: to exploit ultrasound to track tongue movement, but to display a simplified, diagrammatic vocal tract that is easier to interpret. In this paper, we first outline our general approach to this problem, which combines a latent space model with a dimensionality reducing model of vocal tract shapes. Then, we present pilot work to assess the feasibility of this approach. Specifically, we use MRI scans to train a model of vocal tract shapes, then attempt to animate that model using electromagnetic articulography (EMA) data from the same speaker. Piloting with EMA data is an intermediate step. It is simpler than using ultrasound, but still provides valuable insight. Based on these initial experiments, we argue the approach is promising.\n",
    "Index Terms: Ultrasound, speech therapy, vocal tract visualisation\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-29"
  },
  "hwang12_interspeech": {
   "authors": [
    [
     "Hsin-Te",
     "Hwang"
    ],
    [
     "Yu",
     "Tsao"
    ],
    [
     "Hsin-Min",
     "Wang"
    ],
    [
     "Yih-Ru",
     "Wang"
    ],
    [
     "Sin-Horng",
     "Chen"
    ]
   ],
   "title": "A study of mutual information for GMM-based spectral conversion",
   "original": "i12_0078",
   "page_count": 4,
   "order": 33,
   "p1": "78",
   "pn": "81",
   "abstract": [
    "The Gaussian mixture model (GMM)-based method has dominated the field of voice conversion (VC) for last decade. However, the converted spectra are excessively smoothed and thus produce muffled converted sound. In this study, we improve the speech quality by enhancing the dependency between the source (natural sound) and converted feature vectors (converted sound). It is believed that enhancing this dependency can make the converted sound closer to the natural sound. To this end, we propose an integrated maximum a posteriori and mutual information (MAPMI) criterion for parameter generation on spectral conversion. Experimental results demonstrate that the quality of converted speech by the proposed MAPMI method outperforms that by the conventional method in terms of formal listening test.\n",
    "Index Terms: Voice conversion, mutual information, GMM.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-30"
  },
  "li12_interspeech": {
   "authors": [
    [
     "Na",
     "Li"
    ],
    [
     "Yu",
     "Qiao"
    ]
   ],
   "title": "Bayesian mixture of probabilistic linear regressions for voice conversion",
   "original": "i12_0082",
   "page_count": 4,
   "order": 34,
   "p1": "82",
   "pn": "85",
   "abstract": [
    "The objective of voice conversion is to transform the voice of one speaker to make it sound like another. The GMM-based statistical mapping technique has been proved to be an efficient method for converting voices. We generalized this technique to Mixture of Probabilistic Liner Regressions (MPLR) by using general mixture model of source vectors. In this paper, we improve MPLR by considering a prior for the transformation parameters of liner regressions, which leads to Bayesian Mixture of Probabilistic Liner Regressions (BMPLR). BMPLR has the effectiveness and robustness of Bayesian inference. Especially when the number of training data is limited and the mixture number is larger, BMPLR can largely relieve the overfitting problem. This paper presents two formulations for BMPLR, depending on how to model noise in probabilistic regression function. In addition, we derive equations for MAP estimation of transformation parameters. We examine the proposed method on voice conversion of Japanese utterances. The experimental results exhibit that BMPLR achieves better performance than MPLR.\n",
    "Index Terms: Bayesian linear regression, mixture of proba- bilistic regressions, voice conversion,\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-31"
  },
  "erro12_interspeech": {
   "authors": [
    [
     "Daniel",
     "Erro"
    ],
    [
     "Eva",
     "Navas"
    ],
    [
     "Inma",
     "Hernáez"
    ]
   ],
   "title": "Iterative MMSE estimation of vocal tract length normalization factors for voice transformation",
   "original": "i12_0086",
   "page_count": 4,
   "order": 35,
   "p1": "86",
   "pn": "89",
   "abstract": [
    "We present a method that determines the optimal configuration of a bilinear vocal tract length normalization function to transform the frequency axis of one voice according to a specific target voice. Given a number of parallel utterances of the involved speakers, the single parameter of this function can be calculated through an iterative procedure by minimizing an objective error measure defined in the cepstral domain. This method is also applicable when multiple warping classes are considered, and it can be complemented with amplitude correction filters. The resulting physically motivated cepstral transformation results in highly satisfactory conversion accuracy and improved quality with respect to standard satistical systems.\n",
    "Index Terms: vocal tract length normalization, voice conversion, frequency warping plus amplitude scaling, speech synthesis.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-32"
  },
  "percybrooks12_interspeech": {
   "authors": [
    [
     "Winston",
     "Percybrooks"
    ],
    [
     "Elliot",
     "Moore"
    ]
   ],
   "title": "An HMM approach to residual estimation for high resolution voice conversion",
   "original": "i12_0090",
   "page_count": 4,
   "order": 36,
   "p1": "90",
   "pn": "93",
   "abstract": [
    "Voice conversion systems aim to process speech from a source speaker so it would be perceived as spoken by a target speaker. This paper presents a procedure to improve high resolution voice conversion by modifying the algorithm used for residual estimation. The proposed residual estimation algorithm exploits the temporal dependencies between residuals in consecutive speech frames using a hidden Markov model. A previous residual estimation technique based on Gaussian mixtures is used as comparison. Both algorithms are subjected to tests to measure perceived identity conversion and converted speech quality. It was found that the proposed algorithm generates converted speech with significantly better quality without degraded identity conversion performance with respect to the baseline, working particularly well for female target speakers and cross-gender conversions.\n",
    "Index Terms: Voice conversion, residual estimation, HMM, MOS test, ABX test\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-33"
  },
  "toda12_interspeech": {
   "authors": [
    [
     "Tomoki",
     "Toda"
    ],
    [
     "Takashi",
     "Muramatsu"
    ],
    [
     "Hideki",
     "Banno"
    ]
   ],
   "title": "Implementation of computationally efficient real-time voice conversion",
   "original": "i12_0094",
   "page_count": 4,
   "order": 37,
   "p1": "94",
   "pn": "97",
   "abstract": [
    "This paper presents an implementation of real-time processing of statistical voice conversion (VC) based on Gaussian mixture models (GMMs). To develop VC applications for enhancing our human-to-human speech communication, it is essential to implement real-time conversion processing. Moreover, it is useful to further reduce computational complexity of the conversion processing for making VC applications available in limited resources. In this paper, we propose an implementation method of real-time VC based on low-delay conversion processing considering dynamic features and a global variance. Moreover, we also propose computationally efficient VC processing based on fast source feature extraction and diagonalization of full covariance matrices. Some experimental results are presented to show that the proposed methods works reasonably well.\n",
    "Index Terms: voice conversion, real-time processing, lowdelay conversion, computational efficiency\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-34"
  },
  "saito12_interspeech": {
   "authors": [
    [
     "Daisuke",
     "Saito"
    ],
    [
     "Nobuaki",
     "Minematsu"
    ],
    [
     "Keikichi",
     "Hirose"
    ]
   ],
   "title": "Effects of speaker adaptive training on tensor-based arbitrary speaker conversion",
   "original": "i12_0098",
   "page_count": 4,
   "order": 38,
   "p1": "98",
   "pn": "101",
   "abstract": [
    "This paper introduces speaker adaptive training techniques to tensor-based arbitrary speaker conversion. In voice conversion studies, realization of conversion from/to an arbitrary speakerfs voice is one of the important objectives. For this purpose, eigenvoice conversion (EVC) based on an eigenvoice Gaussian mixture model (EV-GMM) was proposed. Although the EVC can effectively construct the conversion model for arbitrary target speakers using only a few utterances, it does not effectively improve the performance even when using a lot of adaptation data, because of an inherent problem in GMM supervectors. We previously proposed tensor-based speaker space as the solution for this problem, and realized more flexible control of speaker characteristics. In this paper, for larger improvement of the performance of VC, speaker adaptive training and tensorbased speaker representation are integrated. The proposed method can construct the flexible and precise conversion model, and experimental results of one-to-many voice conversion demonstrate the effectiveness of the proposed approach.\n",
    "Index Terms: voice conversion, Gaussian mixture model, eigenvoice, Tucker decomposition, speaker adaptive training\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-35"
  },
  "weninger12_interspeech": {
   "authors": [
    [
     "Felix",
     "Weninger"
    ],
    [
     "Björn",
     "Schuller"
    ]
   ],
   "title": "Discrimination of linguistic and non-linguistic vocalizations in spontaneous speech: intra- and inter-corpus perspectives",
   "original": "i12_0102",
   "page_count": 4,
   "order": 39,
   "p1": "102",
   "pn": "105",
   "abstract": [
    "We present a large-scale study on classification of linguistic and non-linguistic vocalizations including laughter, vocal noise, hesitation and consent on four corpora amounting to 46 hours of spontaneous conversational speech. We consider training and testing on speaker-independent subsets of single corpora (intra-corpus) as well as inter-corpus experiments where models built on one or more corpora are evaluated on a disjoint corpus. Our results reveal that while inter-corpus performance is considerably lower than comparable intra-corpus results, this effect can be countered by data agglomeration; furthermore, we observe that inter-corpus classification accuracies indicate suitability of corpora for building generalizing models.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-36"
  },
  "avanzi12_interspeech": {
   "authors": [
    [
     "Mathieu",
     "Avanzi"
    ],
    [
     "Pauline",
     "Dubosson"
    ],
    [
     "Sandra",
     "Schwab"
    ],
    [
     "Nicolas",
     "Obin"
    ]
   ],
   "title": "Accentual transfer from Swiss-German to French. a study of \"francais federal\"",
   "original": "i12_0106",
   "page_count": 4,
   "order": 40,
   "p1": "106",
   "pn": "109",
   "abstract": [
    "This study aims at examining the accentual and phrasing properties of a variety of L2 French commonly called “Français Fédéral”, a variety of French spoken in Switzerland by speakers who have a Swiss-German dialect as a mother tongue. For this, we compared the data of 4 groups of 4 speakers: 2 groups of 4 native French speakers from Neuchâtel and from Paris, and 2 groups of 4 Swiss-German French speakers from Bern and Zürich. The data are semi-automatically processed, and three main prosodic features relating to accentuation and phrasing are examined: prominence distribution and metrical weight of the Phonological Phrase, respect of Phonological Phrase formation constraints (Align-XP and No-clash), and realizations of sandhis phenomena within and across the Phonological Phrases boundaries. Our findings suggest that “Français Fédéral” share several features with a lexical accentuation system rather than with a supra-lexical accentuation system.\n",
    "Index Terms: Accentual transfer, Phonological Phrase, “français fédéral”, Align-XP, accentual clash, sandhis\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-37"
  },
  "jannedy12_interspeech": {
   "authors": [
    [
     "Stefanie",
     "Jannedy"
    ],
    [
     "Melanie",
     "Weirich"
    ]
   ],
   "title": "Phonology & the interpretation of fine phonetic detail in Berlin German",
   "original": "i12_0110",
   "page_count": 4,
   "order": 41,
   "p1": "110",
   "pn": "113",
   "abstract": [
    "Young multi-ethnolectal speakers of Hamburg-German introduced an alternation of /ç/ to [ʧ] following a lax front vowel /ɪ/. We conducted perception studies exploiting this contrast in Berlin (Germany), a city with large multi-ethnic neighborhoods. This alternation is pervasive and noticeable, it is mocked and stigmatized and there is an awareness that many young speakers (including ethnically Germans) from neighborhoods with larger migrant populations like Kreuzberg (KB) substitute /ç/ with /ʃ/ while speakers from less stigmatized vicinities like Zehlendorf (ZD) do not.   The categorization of items on two 14-step synthesized continua from \"Fichte\" 'spruce' to \"fischte\" '3rd p. sg. to fish' by 99 listeners shows that the interpretation of fine phonetic detail is strongly influenced by the co-presentation of the label KB or ZD in contrast to no label (control). Analyses of the reaction times (RTs) show that significantly more time is needed to process stimuli in KB and less in ZD. Moreover, younger listeners (below 30 years) perceive more /ʃ/ variants than older listeners. Phonological generalization over phonetic input is dependent on associative information: perceptual divergence is found within the confines of a single large urban area.\n",
    "Index Terms: sociophonetics; urban German; palatal fricative\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-38"
  },
  "ishi12_interspeech": {
   "authors": [
    [
     "Carlos T.",
     "Ishi"
    ],
    [
     "Chaoran",
     "Liu"
    ],
    [
     "Hiroshi",
     "Ishiguro"
    ],
    [
     "Norihiro",
     "Hagita"
    ]
   ],
   "title": "Evaluation of a formant-based speech-driven lip motion generation",
   "original": "i12_0114",
   "page_count": 4,
   "order": 42,
   "p1": "114",
   "pn": "117",
   "abstract": [
    "The background of the present work is the development of a tele-operation system where the lip motion of a remote humanoid robot is automatically controlled from the operatorfs voice. In the present paper, we introduce an improved version of our proposed speech-driven lip motion generation method, where lip height and width degrees are estimated based on vowel formant information. The method requires the calibration of only one parameter for speaker normalization, so that no training of dedicated models is necessary. Lip height control is evaluated in a female android robot Geminoid-F and in an animated face. Subjective evaluation indicated that naturalness of lip motion generated in the robot is improved by the inclusion of a partial lip width control (with stretching of the lip corners). Highest naturalness scores were achieved for the animated face, showing the effectiveness of the proposed method.\n",
    "Index Terms: lip motion, formant, tele-operation, humanoid robot.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-39"
  },
  "kallay12_interspeech": {
   "authors": [
    [
     "Jeffrey",
     "Kallay"
    ],
    [
     "Jeffrey",
     "Holliday"
    ]
   ],
   "title": "Using spectral measures to differentiate Mandarin and Korean sibilant fricatives",
   "original": "i12_0118",
   "page_count": 4,
   "order": 43,
   "p1": "118",
   "pn": "121",
   "abstract": [
    "We explore the use of two spectral measures calculated in ERB space for differentiating between the frication noise of sibilants in Mandarin Chinese and Korean. The peak frequency (peakERB) of the spectral representation was used to capture differences in front cavity size and a compactness index (CI) was used to capture the bandwidth of the peak. In both /a/ and /i/ vowel contexts the peakERB measure differentiated between Mandarin [s], [ɕ], and [ʃ], and also between Korean [sh] and [s*] which, although considered to be articulated at the same place, differ in front cavity size due to the tighter lingual constriction of [s*]. The CI measure helped further differentiate the fricatives, with Mandarin [ɕ] having a broader peak (higher CI) than [s] or [ʃ], and Korean [sha] having a broader peak than [s*a]. When applied to the L2 Korean productions of L1 Mandarin speakers, we found evidence for both Korean fricatives assimilating to Mandarin [s] before /a/, and to Mandarin [ɕ] before /i/.\n",
    "Index Terms: sibilant fricatives, Mandarin Chinese, Korean\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-40"
  },
  "jian12_interspeech": {
   "authors": [
    [
     "Hua-Li",
     "Jian"
    ],
    [
     "Richard",
     "Konopka"
    ]
   ],
   "title": "EFL conversational triads: foreigner-directed speech and hyperarticulation",
   "original": "i12_0122",
   "page_count": 4,
   "order": 44,
   "p1": "122",
   "pn": "125",
   "abstract": [
    "The study examines interactions associated with foreigner-directed speech (FDS) within EFL triadic conversation exam where learners differ in outspokenness. The results showed that learners' outspokenness affected foreign examiners' vowel articulation. More vowels underwent distinct change across learners' positive-coded utterances in F2, whereas in F1 the vowel change in the high-communicative group during the introduction of a new question was the greatest source of variation. The findings suggest that native speakers tend to hyperarticulate during interactions with the low-communicate group, and probably with didactic intent to instruct or smooth communicative situations, in particular during more encouraging speech acts.\n",
    "Index Terms: EFL, foreigner directed speech, hyperarticulation, vowel space\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-41"
  },
  "ouyang12_interspeech": {
   "authors": [
    [
     "Iris Chuoying",
     "Ouyang"
    ],
    [
     "Khalil",
     "Iskarous"
    ]
   ],
   "title": "Syllable perception depends on tone perception",
   "original": "i12_0126",
   "page_count": 4,
   "order": 45,
   "p1": "126",
   "pn": "129",
   "abstract": [
    "We conducted a perception study on Mandarin, a tone language where pitch carries contrastive information, to investigate whether pitch changes can override spectral information in determining the number of syllables in an utterance. We generated F0 contours and simulated tonal coarticulation using the qTA model. The perception of syllable numbers depended on the perception of tones, and this effect held across speech rate. Combining with prior work, the results indicate that laryngeal and supralaryngeal events interact in syllable perception in tone languages. We discuss how our findings support the notion of language-specific perception.\n",
    "Index Terms: syllable perception, tonal coarticulation, speech rate, Mandarin\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-42"
  },
  "dicanio12_interspeech": {
   "authors": [
    [
     "Christian T.",
     "DiCanio"
    ],
    [
     "Hosung",
     "Nam"
    ],
    [
     "Douglas H.",
     "Whalen"
    ],
    [
     "H. Timothy",
     "Bunnell"
    ],
    [
     "Jonathan D.",
     "Amith"
    ],
    [
     "Rey",
     "Castillo Garcia"
    ]
   ],
   "title": "Assessing agreement level between forced alignment models with data from endangered language documentation corpora",
   "original": "i12_0130",
   "page_count": 4,
   "order": 46,
   "p1": "130",
   "pn": "133",
   "abstract": [
    "Automatic forced alignment between transcriptions has achieved high levels of agreement for languages with large corpora, but the technique holds great promise for work on all languages. Here, we apply two forced alignment programs to data from an endangered Mixtecan language of Mexico. Both yielded a majority of boundaries within 20 ms of hand-labeled ones. Phonemes with fairly steady-state elements (e.g. nasals, fricatives) were more accurately labeled than others. Forced alignment thus may increase efficiency of labeling texts from smaller languages, at least in cases where the phoneme inventories are similar to those of the languages of the training.\n",
    "Index Terms: speech recognition, phonetics, linguistics\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-43"
  },
  "fujimoto12_interspeech": {
   "authors": [
    [
     "Masako",
     "Fujimoto"
    ],
    [
     "Seiya",
     "Funatsu"
    ],
    [
     "Ichiro",
     "Fujimoto"
    ]
   ],
   "title": "How consonants, dialect and speech rate affect vowel devoicing?",
   "original": "i12_0134",
   "page_count": 4,
   "order": 47,
   "p1": "134",
   "pn": "137",
   "abstract": [
    "We examined the glottal opening pattern during devoicing environment,with respect to the factors that facilitate or suppress devoicing. The results indicated that glottal opening patterns are twofold: a single phase- and a double phase opening for /CVC/. Only single phase openings appeared at typical consonantal environments for a Tokyo speaker. Gestural reorganization is assumed for these cases. Double phase opening appeared for an atypical consonantal environment for the Tokyo speaker. For a speaker of Osaka dialect, in which devoicing is less frequent, double phase opening appeared regardless of a typical or atypical consonantal environment. The effect of atypical consonantal environments and dialect on devoicing are due to glottal gesture overlap. In faster speech, a double phase tends to merge which facilitates devoicing. In consecutive devoicing environments, the vowel in a typical consonantal environment is the first candidate with/without devoicing of the following vowel. The following vowel can be devoiced if the glottal opening for the preceding /CVC/ is reorganized. During the phrase final /u/, both single- and double phase openings appeared for Tokyo speakers. Greater interpersonal variations appeared for devoicing at phrase final position.\n",
    "Index Terms: vowel devoicing, consonantal environment, speech rate, dialects, phrase final position, consecutive devoicing environments\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-44"
  },
  "nadeu12_interspeech": {
   "authors": [
    [
     "Marianna",
     "Nadeu"
    ]
   ],
   "title": "Effects of stress and speech rate on vowel quality in Catalan and Spanish",
   "original": "i12_1396",
   "page_count": 4,
   "order": 48,
   "p1": "1396",
   "pn": "1399",
   "abstract": [
    "This study examines the effects of lexical stress on vowel quality in Iberian Spanish and Central Catalan and the relationship between phonetic variation caused by stress and speech rate. For Catalan, both absence of stress and fast speech rate result in a shrunken vowel space. For Spanish, the effects of speech rate are in line with those found for Catalan. Yet, unstressed vowels are characterized by lower F1 than their stressed counterparts. Results are discussed in the light of the Hyperarticulation Hypothesis and the Sonority Expansion Hypothesis.\n",
    "Index Terms: Catalan, Spanish, stress, speech rate, reduction\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-45"
  },
  "mcauliffe12_interspeech": {
   "authors": [
    [
     "Michael",
     "McAuliffe"
    ],
    [
     "Molly",
     "Babel"
    ]
   ],
   "title": "Predictability affects vowel dispersion and dynamics in the Buckeye corpus",
   "original": "i12_1400",
   "page_count": 4,
   "order": 49,
   "p1": "1400",
   "pn": "1403",
   "abstract": [
    "Research on vowel space dispersion has found that vowel spaces are less dispersed when lexical and contextual factors favor identification of a word (e.g., a highly predictable word), and vowel spaces are more dispersed when lexical and contextual factors create an environment for less predictability. Examining sound changes in progress, recent work has demonstrated that, for some vowel changes, a vowel will be produced in a more innovative way when the lexical item in which it occurs is highly semantically predictable. The data for such claims have been collected in laboratory settings. In this paper we investigate whether these findings from the laboratory extend into more naturalistic settings, and we examine the precise character of the reduction by examining dynamic movement across a vowel's duration.\n",
    "Index Terms: reduction in speech production, spontaneous speech, acoustics, corpus linguistics, SSANOVA\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-46"
  },
  "fox12_interspeech": {
   "authors": [
    [
     "Robert Allen",
     "Fox"
    ],
    [
     "Ewa",
     "Jacewicz"
    ]
   ],
   "title": "Dialectal and generational variations in vowels in spontaneous speech",
   "original": "i12_1404",
   "page_count": 4,
   "order": 50,
   "p1": "1404",
   "pn": "1407",
   "abstract": [
    "The pronunciation patterns across dialect regions in the United States are changing. This paper examines acoustic differences between the vowels of old and middle-aged adults produced in spontaneous speech. The new developments in regional vowels systems which differentiate American English dialects found recently in citation-form vowels and in read speech were confirmed in the present analysis. While providing additional evidence for the existence of sound change on the basis of spontaneous speech data, this work brings to light important challenges facing researchers using spontaneous speech in laboratory analyses such as the loss of fine experimental control in the examination of spectral dynamics or the usefulness of statistical assessment.\n",
    "Index Terms: dialect variation, sound change, spontaneous speech, vowel acoustics, spectral dynamics.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-47"
  },
  "scarborough12_interspeech": {
   "authors": [
    [
     "Rebecca",
     "Scarborough"
    ],
    [
     "Georgia",
     "Zellou"
    ]
   ],
   "title": "Perceiving listener-directed speech: effects of authenticity and lexical neighborhood density",
   "original": "i12_1408",
   "page_count": 4,
   "order": 51,
   "p1": "1408",
   "pn": "1411",
   "abstract": [
    "Numerous studies have examined the properties of “clear” speech—speech produced in the context of real or imagined communicative difficulties. In general, clear speech is characterized by hyperarticulation. However, the effect of clear speech on coarticulation varies: simulated clear speech contexts have less nasal coarticulation while speech directed toward a real listener has more. Additionally, both hyperarticulation and nasal coarticulation vary based on neighborhood density (ND): words from dense phonological neighborhoods have a greater degree of both, relative to words from sparse neighborhoods. This study examines what consequences these effects in production have for perception by means of a lexical decision task with speech from two different “clear” conditions. The findings indicate that real listener directed speech is perceived faster than simulated clear speech. Further, Hi ND words (with hyperarticulation and increased coarticulation, as in real listener directed speech) were faster than Lo ND words.\n",
    "Index Terms: clear speech, listener-directed speech, perception, nasal coarticulation, hyperarticulation, neighborhood density\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-48"
  },
  "chen12b_interspeech": {
   "authors": [
    [
     "Ying",
     "Chen"
    ],
    [
     "Vsevolod",
     "Kapatsinski"
    ],
    [
     "Susan",
     "Guion-Anderson"
    ]
   ],
   "title": "Acoustic cues of vowel quality to coda nasal perception in southern Min",
   "original": "i12_1412",
   "page_count": 4,
   "order": 52,
   "p1": "1412",
   "pn": "1415",
   "abstract": [
    "This paper investigates the effect of vowel quality on the perception of coda nasals in Southern Min. The perceptual confusion experiment revealed that /m/ is the most confusable coda nasal, followed by /ŋ/ and then /n/. The high front vowel /i/ resulted in more misidentification of following coda nasals than mid vowel /ə/ and low vowel /a/. Within the same vowel context, higher formant frequency at the juncture of vowels and nasals and greater formant change from the midpoint to the endpoint of vowels provided more salient acoustic cues to place of articulation of post-vocalic nasals and thus resulted in higher accuracy of coda nasal identification.\n",
    "Index Terms: coda nasal, perception, vowel, acoustic cues\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-49"
  },
  "simonet12_interspeech": {
   "authors": [
    [
     "Miquel",
     "Simonet"
    ],
    [
     "José I.",
     "Hualde"
    ],
    [
     "Marianna",
     "Nadeu"
    ]
   ],
   "title": "Lenition of /d/ in spontaneous Spanish and Catalan",
   "original": "i12_1416",
   "page_count": 4,
   "order": 53,
   "p1": "1416",
   "pn": "1419",
   "abstract": [
    "The alternation between voiced plosives and spirants in Iberian languages is described as the complementary distribution between two allophones. The present study explores the acoustics of /d/ in two corpora of spontaneous speech and examines the hypothesis that constriction degree in /d/ is governed by finer-grained speech production factors than claimed before. Three acoustic metrics were developed as indexes of articulatory weakening. The findings suggest that variations in the implementation of /d/ result from gradient modulations in constriction degree on a unimodal, rather than bimodal, statistical-acoustic distribution. The preceding segment is a strong predictor of the weakening (i.e., spirantization) of Catalan and Spanish /d/.\n",
    "Index Terms: Spanish, Catalan, lenition, phonetics, phonology.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-50"
  },
  "feher12_interspeech": {
   "authors": [
    [
     "Thomas",
     "Fehér"
    ],
    [
     "Dietmar",
     "Richter"
    ],
    [
     "Oliver",
     "Jokisch"
    ],
    [
     "Rüdiger",
     "Hoffmann"
    ]
   ],
   "title": "Distance-dependent noise reduction for two-channel microphones",
   "original": "i12_0138",
   "page_count": 4,
   "order": 54,
   "p1": "138",
   "pn": "141",
   "abstract": [
    "The article introduces a novel approach for noise reduction based on distance approximation of signal sources. This is achieved by estimation of the specific acoustic impedance of the incoming signal. The method uses small-sized two-microphone arrangements or so called twochannel microphones which barely exceed the spatial extend of a single microphone.   The novel adaptive block-online algorithm significantly reduces the level of distant signals having the same angle of arrival as the closer source signal. The overall method has been tested in an anechoic chamber and a more realistic room with 250 ms reverberation time. The sources are located at distances of 5 cm and 1 m from the test microphone. The experiment shows a level reduction of 5.1 dB with regard to a distant noise source. The signal level of the close-tomicrophone speaker is not effected.   The approach is targeting on new applications but also provides algorithmic improvements for existing applications - e.g. for the noise reduction in hand-held microphones or for distance-dependent noise gates.\n",
    "Index Terms: Active noise reduction, adaptive filters, spatial filters\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-51"
  },
  "xue12_interspeech": {
   "authors": [
    [
     "Wei",
     "Xue"
    ],
    [
     "Wenju",
     "Liu"
    ]
   ],
   "title": "Direction of arrival estimation based on subband weighting for noisy conditions",
   "original": "i12_0142",
   "page_count": 4,
   "order": 55,
   "p1": "142",
   "pn": "145",
   "abstract": [
    "In this paper, we present a novel DOA estimation method for human speech using subband weighting. Existing DOA estimation methods still can not perform quite reliably in low SNR condition. To improve the robustness of DOA estimator in noisy environment, we propose a novel DOA estimation approach. Firstly, the speech signal of each channel is passed through a Gammatone filterbank to obtain a set of time-domain subband signals. Secondly, we achieve TDOA estimation based on a new cost function in each subband. subband weight is calculated to emphasis the estimation results of subbands with high possibility containing speech. Finally, DOA is determined by the estimated TDOA and geometry of microphone array. Experimental results show that the proposed subband weighting based method outperforms SRP-PHAT and broadband MUSIC algorithm in highly noisy environment.\n",
    "Index Terms: Direction of arrival estimation, array signal processing, Gammatone filters, subband weighting.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-52"
  },
  "marinhurtado12_interspeech": {
   "authors": [
    [
     "Jorge I.",
     "Marin-Hurtado"
    ],
    [
     "David V.",
     "Anderson"
    ]
   ],
   "title": "Binaural noise reduction using frequency-warped FIR filters",
   "original": "i12_0146",
   "page_count": 4,
   "order": 56,
   "p1": "146",
   "pn": "149",
   "abstract": [
    "Binaural noise-reduction techniques based on Multichannel Wiener filter (MWF) have been reported as promissing candidates to be used in binaural hearing aids because of their efficient noise reduction at any arbitrary direction of arrival of the target signal and the preservation of localization cues. The implementation of these techniques have been reported for FFT-based processing and wavelet-packet-based (WP) processing. In these implementations, the processing delay is large and limited to the block processing inherent in the FFT and WP computation. This paper proposes a different implementation for MWF by means of frequency-warped FIR filters, providing a performance near a WP-based MWF and smaller processing delay.\n",
    "Index Terms: Noise Reduction, Frequency-Warped FIR Filters, Multichannel Wiener Filter (MWF), Binaural Hearing Aids, Latency Reduction\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-53"
  },
  "yu12b_interspeech": {
   "authors": [
    [
     "Meng",
     "Yu"
    ],
    [
     "Jack",
     "Xin"
    ]
   ],
   "title": "Exploring off time nature for speech enhancement",
   "original": "i12_0150",
   "page_count": 4,
   "order": 57,
   "p1": "150",
   "pn": "153",
   "abstract": [
    "It is well known that human conversational speech is “sparse” in time domain, comprising of many “off” time segments. This suggests the utility of the “off” time nature for the task of speech enhancement. We propose an efficient dualmicrophone method based on regularized cross-channel cancellation to distinguish the overlapping and single speech segments in the multi-speaker conversational environment. Fortunately, the regularized cancellation results can be reused for speech enhancement along an interference-suppression chain. We present evaluations of the proposed overlapping speech detection and integrated speech enhancement approaches using an IEEE speech database and real room recordings under various acoustic environments, showing promising improvements for speech enhancement by exploring the off time nature.\n",
    "Index Terms: Overlapping speech detection, cross-channel cancellation, speech enhancement, l1 optimization.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-54"
  },
  "bao12_interspeech": {
   "authors": [
    [
     "Xulei",
     "Bao"
    ],
    [
     "Jie",
     "Zhu"
    ]
   ],
   "title": "Model-based single-channel dereverberation in noisy acoustical environments",
   "original": "i12_0154",
   "page_count": 4,
   "order": 58,
   "p1": "154",
   "pn": "157",
   "abstract": [
    "This paper illustrates a new system for recovering clean speech signals from noisy acoustical environments using one microphone. At the beginning of this paper, we propose an assumption that the background noise is comprised of reverberant noise and direct-path noise. And a novel late reverberant spectral variance (LRSV) estimator is generated referring to this assumption, which can be used in the noisy acoustical environments. Whatfs more, the shape parameters of this LRSV estimator are updated by taking some frames of previous late reverberation into account. At last, a new spectral process system is developed to help making the LRSV estimator more efficient. The experimental results show the benefits of our new system when it is used to reduce the interference in both noise-free and noisy acoustical environments.\n",
    "Index Terms: statistical reverberation model; late reverberant spectral variance estimator; spectral processing\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-55"
  },
  "mirbagheri12_interspeech": {
   "authors": [
    [
     "Majid",
     "Mirbagheri"
    ],
    [
     "Sahar",
     "Akram"
    ],
    [
     "Shihab",
     "Shamma"
    ]
   ],
   "title": "An auditory inspired multimodal framework for speech enhancement",
   "original": "i12_0158",
   "page_count": 4,
   "order": 59,
   "p1": "158",
   "pn": "161",
   "abstract": [
    "A new multimodal framework for speech enhancement in noisy environments based on human auditory system model is proposed in this paper. Unlike existing engineering architectures each of which specifically designed for certain speech sensors (extracted pitch, visual cues, etc.), our proposed model provides the capacity to integrate cues of different type into the enhancement system by introducing the notion of temporal coherence. The short-time coherence coefficients (STCC) between sound components and cues computed through an estimate of mutual information are used as a measure of target speech dominance and consequently the gain coefficients. The objective evaluation results for two exemplars in this framework show that the new methodology is effective in practice.\n",
    "Index Terms: speech enhancement, multimodal, mutual information, auditory\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-56"
  },
  "hazrati12_interspeech": {
   "authors": [
    [
     "Oldooz",
     "Hazrati"
    ],
    [
     "Jaewook",
     "Lee"
    ],
    [
     "Philipos C.",
     "Loizou"
    ]
   ],
   "title": "Binary mask estimation for improved speech intelligibility in reverberant environments",
   "original": "i12_0162",
   "page_count": 4,
   "order": 60,
   "p1": "162",
   "pn": "165",
   "abstract": [
    "A blind (non-ideal) time-frequency (T-F) masking technique is proposed for suppressing reverberation. A binary mask is estimated at each T-F unit by extracting a single variance-based feature from the reverberant signal and comparing its value against an adaptive threshold. The performance of the estimated binary mask is evaluated using intelligibility listening tests with hearing impaired listeners in four moderate to highly reverberant conditions. Results indicated that the proposed T-F masking technique yielded significant improvements in intelligibility even in highly reverberant conditions (T60=1.0 s). This improvement was attributed to the recovery of the vowel/consonant boundaries which are severely smeared in reverberation.\n",
    "Index Terms: Binary mask, cochlear implant (CI), dereverberation\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-57"
  },
  "petkov12_interspeech": {
   "authors": [
    [
     "Petko N.",
     "Petkov"
    ],
    [
     "W. Bastiaan",
     "Kleijn"
    ],
    [
     "Gustav Eje",
     "Henter"
    ]
   ],
   "title": "Enhancing subjective speech intelligibility using a statistical model of speech",
   "original": "i12_0166",
   "page_count": 4,
   "order": 61,
   "p1": "166",
   "pn": "169",
   "abstract": [
    "The intelligibility of speech in adverse noise conditions can be improved by modifying the characteristics of the clean speech prior to its presentation. An effective and flexible paradigm is to select the modification by optimizing a measure of objective intelligibility. Here we apply this paradigm at the text level and optimize a measure related to the classification error probability in an automatic speech recognition system. The proposed method was applied to a simple but powerful band-energy modification mechanism under an energy preservation constraint. Subjective evaluation results provide a clear indication of a significant gain in subjective intelligibility. In contrast to existing methods, the proposed approach is not restricted to a particular modification strategy and treats the notion of optimality at a level closer to that of subjective intelligibility. The computational complexity of the method is sufficiently low to enable its use in on-line applications.\n",
    "Index Terms: speech modification, subjective intelligibility, statistical model of speech\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-58"
  },
  "mousa12_interspeech": {
   "authors": [
    [
     "Amr El-Desoky",
     "Mousa"
    ],
    [
     "M. Ali",
     "Basha Shaik"
    ],
    [
     "Ralf",
     "Schlüter"
    ],
    [
     "Hermann",
     "Ney"
    ]
   ],
   "title": "Morpheme level feature-based language models for German LVCSR",
   "original": "i12_0170",
   "page_count": 4,
   "order": 62,
   "p1": "170",
   "pn": "173",
   "abstract": [
    "One of the challenges for Large Vocabulary Continuous Speech Recognition (LVCSR) of German is its complex morphology and high level of compounding. It leads to high Out-of-vocabulary (OOV) rates, and poor Language Model (LM) probabilities. In such cases, building LMs on morpheme level can be considered a better choice. Thereby, higher lexical coverage and lower LM perplexities are achieved. On the other side, a successful approach to improve the LM probability estimation is to incorporate features of words using feature-based LMs. In this paper, we use features derived for morphemes as well as words. Thus, we combine the benefits of both morpheme level and feature rich modeling. We compare the performance of stream-based, class-based and factored LMs (FLMs). Relative reductions of around 1.5% in Word Error Rate (WER) are achieved compared to the best previous results obtained using FLMs.\n",
    "Index Terms: language model, morpheme, streambased, class-based, factored\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-59"
  },
  "yamamoto12_interspeech": {
   "authors": [
    [
     "Hitoshi",
     "Yamamoto"
    ],
    [
     "Paul R.",
     "Dixon"
    ],
    [
     "Shigeki",
     "Matsuda"
    ],
    [
     "Chiori",
     "Hori"
    ],
    [
     "Hideki",
     "Kashioka"
    ]
   ],
   "title": "Tied-state mixture language model for WFST-based speech recognition",
   "original": "i12_0174",
   "page_count": 4,
   "order": 63,
   "p1": "174",
   "pn": "177",
   "abstract": [
    "This paper describes a language model combination method for automatic speech recognition (ASR) systems based on Weighted Finite-State Transducers (WFSTs). The performance of ASR in real applications often degrades when an input utterance is out of the domain of the prepared language models. To cover a wide range of domains, it is possible to utilize a combination of multiple language models. To do this, we propose a language model combination method with a two-step approach; it first uses a union operation to incorporate all components into a single transducer and then merges states of the transducer to mix n-grams included in multiple models and to retain unique n-grams in each model simultaneously. The method has been evaluated in speech recognition experiments on travel conversation tasks and has demonstrated improvements in recognition performance.\n",
    "Index Terms: Language model combination, WFST\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-60"
  },
  "alumae12_interspeech": {
   "authors": [
    [
     "Tanel",
     "Alumäe"
    ],
    [
     "Kaarel",
     "Kaljurand"
    ]
   ],
   "title": "Maximum entropy language model adaptation for mobile speech input",
   "original": "i12_0178",
   "page_count": 4,
   "order": 64,
   "p1": "178",
   "pn": "181",
   "abstract": [
    "This paper describes unsupervised adaptation of language model for many related target domains. In mobile speech input, subject and vocabulary of the language depend highly on the usage context. We use automatically transcribed speech data to select a subset from the language model training data for building a maximum entropy model adapted to speech input. This model is further adapted for most popular mobile applications. When used in interpolation with the background N-gram model, the adapted models give over 10% relative word error rate reduction in Estonian mobile speech input experiments.\n",
    "Index Terms: language model adaptation, maximum entropy, mobile speech input\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-61"
  },
  "lecorve12_interspeech": {
   "authors": [
    [
     "Gwénolé",
     "Lecorvé"
    ],
    [
     "John",
     "Dines"
    ],
    [
     "Thomas",
     "Hain"
    ],
    [
     "Petr",
     "Motlicek"
    ]
   ],
   "title": "Supervised and unsupervised web-based language model domain adaptation",
   "original": "i12_0182",
   "page_count": 4,
   "order": 65,
   "p1": "182",
   "pn": "185",
   "abstract": [
    "Domain language model adaptation consists in re-estimating probabilities of a baseline LM in order to better match the specifics of a given broad topic of interest. To do so, a common strategy is to retrieve adaptation texts from the Web based on a given domain-representative seed text. In this paper, we study how the selection of this seed text influences the adaptation process and the performances of resulting adapted language models in automatic speech recognition. More precisely, the goal of this original study is to analyze the differences of our Web-based adaptation approach between the supervised case, in which the seed text is manually generated, and the unsupervised case, where the seed text is given by an automatic transcript. Experiments were carried out on data sourced from a real-world use case, more specifically, videos produced for a university YouTube channel. Results show that our approach is quite robust since the unsupervised adaptation provides similar performance to the supervised case in terms of the overall perplexity and word error rate.\n",
    "Index Terms: Language model, domain adaptation, supervision, Web data\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-62"
  },
  "tam12_interspeech": {
   "authors": [
    [
     "Yik-Cheung",
     "Tam"
    ],
    [
     "Paul",
     "Vozila"
    ]
   ],
   "title": "A hierarchical Bayesian approach for semi-supervised discriminative language modeling",
   "original": "i12_0186",
   "page_count": 4,
   "order": 66,
   "p1": "186",
   "pn": "189",
   "abstract": [
    "Discriminative language modeling provides a mechanism for differentiating between competing word hypotheses, which are usually ignored in traditional maximum likelihood estimation of N-gram language models. Discriminative language modeling usually requires manual transcription which can be costly and slow to obtain. On the other hand, there are vast amount of untranscribed speech data on which offline adaptation technique can be applied to generate pseudo-truth transcription as an approximation to manual transcription. Viewing manual and pseudo-truth transcriptions as two domains, we perform hierarchical Bayesian domain adaptation on discriminative language models sharing a common prior model. Domain-specific and prior models are estimated jointly using training data. In the N-best list rescoring experiment, hierarchical Bayesian domain adaptation has yielded better recognition performance than the model trained only on manual transcription, and seems robust against inferior prior.\n",
    "Index Terms: Hierarchical Bayesian domain adaptation, Discriminative language modeling, semi-supervised learning\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-63"
  },
  "wu12_interspeech": {
   "authors": [
    [
     "Youzheng",
     "Wu"
    ],
    [
     "Kazuhiko",
     "Abe"
    ],
    [
     "Paul R.",
     "Dixon"
    ],
    [
     "Chiori",
     "Hori"
    ],
    [
     "Hideki",
     "Kashioka"
    ]
   ],
   "title": "Leveraging social annotation for topic language model adaptation",
   "original": "i12_0190",
   "page_count": 4,
   "order": 67,
   "p1": "190",
   "pn": "193",
   "abstract": [
    "Social annotations such as Yahoo! Answers already define broad coverage of hierarchical topic categories and include millions of documents annotated by web users. This paper argues that topic language model (LM) adaptation via effective leveraging of such social annotations, while possibly noisy, may be more effective than unsupervised methods such as clustering-based and LDA-based algorithms. Experimental results on the IWSLT-2011 TED ASR data sets demonstrate that we can achieve modest improvements when compared with the unsupervised methods.\n",
    "Index Terms: Topic language model adaptation, social annotations, speech recognition.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-64"
  },
  "sundermeyer12_interspeech": {
   "authors": [
    [
     "Martin",
     "Sundermeyer"
    ],
    [
     "Ralf",
     "Schlüter"
    ],
    [
     "Hermann",
     "Ney"
    ]
   ],
   "title": "LSTM neural networks for language modeling",
   "original": "i12_0194",
   "page_count": 4,
   "order": 68,
   "p1": "194",
   "pn": "197",
   "abstract": [
    "Neural networks have become increasingly popular for the task of language modeling. Whereas feed-forward networks only exploit a fixed context length to predict the next word of a sequence, conceptually, standard recurrent neural networks can take into account all of the predecessor words. On the other hand, it is well known that recurrent networks are difficult to train and therefore are unlikely to show the full potential of recurrent models.   These problems are addressed by a the Long Short-Term Memory neural network architecture. In this work, we apply this type of network to an English and a large French language modeling task. Experiments show improvements of about 8% relative in perplexity over standard recurrent neural network LMs. In addition, we gain considerable improvements in WER on top of a state-of-the-art speech recognition system.\n",
    "Index Terms: language modeling, recurrent neural networks, LSTM neural networks\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-65"
  },
  "xu12_interspeech": {
   "authors": [
    [
     "Puyang",
     "Xu"
    ],
    [
     "Brian",
     "Roark"
    ],
    [
     "Sanjeev",
     "Khudanpur"
    ]
   ],
   "title": "Phrasal cohort based unsupervised discriminative language modeling",
   "original": "i12_0198",
   "page_count": 4,
   "order": 69,
   "p1": "198",
   "pn": "201",
   "abstract": [
    "Simulated confusions enable the use of large text-only corpora for discriminative language modeling by hallucinating the likely recognition outputs that each (correct) sentence would be confused with. In [1], a novel approach was introduced to simulate confusions using phrasal cohorts derived directly from recognition output. However, the described approach relied on transcribed speech to derive cohorts. In this paper, we extend the phrasal cohort technique to the fully unsupervised scenario, where transcribed data are completely absent. Experimental results show that even if the cohorts are extracted from untranscribed speech, the unsupervised training can still achieve over 40% of the gains of the supervised approach. The results are presented on NIST data sets for a state-of-the-art LVCSR system.\n",
    "Index Terms: unsupervised training, discriminative language modeling\n",
    "",
    "",
    "Sagae, K., Lehr, M., Prud'hommeaux, E., Xu, P., Glenn, N., Karakos, D., Khudanpur, S., Roark, B., Saraclar, M., Shafran, I., Bikel, D., Callison-Burch, C., Cao, Y., Hall, K., Hasler, E., Koehn, P., Lopez, A., Post, M. and Riley, D., “Hallucinated n-best lists for discriminative language modeling”, in Proc. ICASSP, 2012.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-66"
  },
  "karakos12_interspeech": {
   "authors": [
    [
     "Damianos",
     "Karakos"
    ],
    [
     "Brian",
     "Roark"
    ],
    [
     "Izhak",
     "Shafran"
    ],
    [
     "Kenji",
     "Sagae"
    ],
    [
     "Maider",
     "Lehr"
    ],
    [
     "Emily",
     "Prud'hommeaux"
    ],
    [
     "Puyang",
     "Xu"
    ],
    [
     "Nathan",
     "Glenn"
    ],
    [
     "Sanjeev",
     "Khudanpur"
    ],
    [
     "Murat",
     "Saraclar"
    ],
    [
     "Dan",
     "Bikel"
    ],
    [
     "Mark",
     "Dredze"
    ],
    [
     "Chris",
     "Callison-Burch"
    ],
    [
     "Yuan",
     "Cao"
    ],
    [
     "Keith",
     "Hall"
    ],
    [
     "Eva",
     "Hasler"
    ],
    [
     "Philip",
     "Koehn"
    ],
    [
     "Adam",
     "Lopez"
    ],
    [
     "Matt",
     "Post"
    ],
    [
     "Darcey",
     "Riley"
    ]
   ],
   "title": "Deriving conversation-based features from unlabeled speech for discriminative language modeling",
   "original": "i12_0202",
   "page_count": 4,
   "order": 70,
   "p1": "202",
   "pn": "205",
   "abstract": [
    "The perceptron algorithm was used in [1] to estimate discriminative language models which correct errors in the output of ASR systems. In its simplest version, the algorithm simply increases the weight of n-gram features which appear in the correct (oracle) hypothesis and decreases the weight of n-gram features which appear in the 1-best hypothesis. In this paper, we show that the perceptron algorithm can be successfully used in a semi-supervised learning (SSL) framework, where limited amounts of labeled data are available. Our framework has some similarities to graph-based label propagation in the sense that a graph is built based on proximity of unlabeled conversations, and then it is used to propagate confidences (in the form of features) to the labeled data, based on which perceptron trains a discriminative model. The novelty of our approach lies in the fact that the confidence \"flows\" from the unlabeled data to the labeled data, and not vice-versa, as is done traditionally in SSL. Experiments conducted at the 2011 CLSP Summer Workshop on the conversational telephone speech corpora Dev04f and Eval04f demonstrate the effectiveness of the proposed approach.\n",
    "",
    "",
    "B. Roark, M. Saraclar, and M. Collins, “Discriminative n-gram language modeling,” Computer Speech and Language, vol. 21, no. 2, pp. 373–392, 2007. [Online]. Available: http://dx.doi.org/10.1016/j.csl.2006.06.006\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-67"
  },
  "dikici12_interspeech": {
   "authors": [
    [
     "Erinç",
     "Dikici"
    ],
    [
     "Arda",
     "Çelebi"
    ],
    [
     "Murat",
     "Saraçlar"
    ]
   ],
   "title": "Performance comparison of training algorithms for semi-supervised discriminative language modeling",
   "original": "i12_0206",
   "page_count": 4,
   "order": 71,
   "p1": "206",
   "pn": "209",
   "abstract": [
    "Discriminative language modeling (DLM) has been shown to improve the accuracy of automatic speech recognition (ASR) systems, but it requires large amounts of both acoustic and text data for training. One way to overcome this is to use simulated hypotheses instead of real hypotheses for training, which is called semi-supervised training. In this study, we compare six different perceptron algorithms with the semisupervised training approach. We formulate the DLM both as a structured prediction and a reranking problem, optimizing different criteria in each. We find that ranking variants achieve similar or better word error rate (WER) reduction with respect to structured perceptrons when used with real, simulated, or a combination of such data.\n",
    "Index Terms: discriminative training, semi-supervised learning, language modeling, hypothesis simulation, ranking perceptron\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-68"
  },
  "thadani12_interspeech": {
   "authors": [
    [
     "Kapil",
     "Thadani"
    ],
    [
     "Fadi",
     "Biadsy"
    ],
    [
     "Dan",
     "Bikel"
    ]
   ],
   "title": "On-the-fly topic adaptation for YouTube video transcription",
   "original": "i12_0210",
   "page_count": 4,
   "order": 72,
   "p1": "210",
   "pn": "213",
   "abstract": [
    "Automatic closed-captioning of video is a useful application of speech recognition technology but poses numerous challenges when applied to open-domain user-uploaded videos such as those on YouTube. In this work, we explore a strategy to improve decoding accuracy for video transcription by decoding each video with a language model (LM) adapted specifically to the topics that the video covers. Taxonomic topic classifiers are used to determine the topic content of videos and to build a large set of topic-specific LMs from web documents. We consider strategies for selecting and interpolating LMs in both supervised and unsupervised scenarios in a two-pass lattice rescoring framework. Experiments on a YouTube video corpus show a 10% relative reduction in WER over generic single-pass transcriptions as well as a statistically significant 2.5% reduction over rescoring with a very large non-adapted LM built from all the documents.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-69"
  },
  "jabaian12_interspeech": {
   "authors": [
    [
     "Bassam",
     "Jabaian"
    ],
    [
     "Fabrice",
     "Lefèvre"
    ],
    [
     "Laurent",
     "Besacier"
    ]
   ],
   "title": "Portability of semantic annotations for fast development of dialogue corpora",
   "original": "i12_0214",
   "page_count": 4,
   "order": 73,
   "p1": "214",
   "pn": "217",
   "abstract": [
    "Generalization of spoken dialogue systems increases the need for fast development of spoken language understanding modules for semantic tagging of speakerfs turns. Statistical methods are performing well for this task but require large corpora to be trained. Collecting such corpora is expensive in time and human expertise. In this paper we propose a semi automatic annotation process for fast production of dialogue corpora. The approach consists in automatically pre-annotating the corpus and then manually correct the annotation. To perform the preannotation we propose to port an existing corpus and to adapt it to the new data. The French MEDIA dialogue corpus is used as a starting point to produce two new corpora: one for a new language (Italian) and another for a new domain (theatre ticket reservation). We show that the automatic pre-annotation leads to a significant gain in productivity compared to a fully manual annotation and thus allow to derive new adaptation data which can be used to further improve the systems.\n",
    "Index Terms: Spoken Dialogue Systems, Spoken Language Understanding, Language Portability, Statistical Machine Translation\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-70"
  },
  "griol12_interspeech": {
   "authors": [
    [
     "David",
     "Griol"
    ],
    [
     "Zoraida",
     "Callejas"
    ],
    [
     "Ramón",
     "López-Cózar"
    ]
   ],
   "title": "Optimization of dialog strategies using automatic dialog simulation and statistical dialog management techniques",
   "original": "i12_0218",
   "page_count": 4,
   "order": 74,
   "p1": "218",
   "pn": "221",
   "abstract": [
    "In this paper, we present a technique for learning optimal dialog management strategies. An automatic dialog generation technique, including a simulation of the communication channel, has been developed to acquire the required data, train dialog models, and explore new dialog strategies in order to learn the optimal one. A set of quantitative and qualitative measures has been defined to evaluate the quality of the strategies learned. We provide empirical evidence of the benefits of our proposal through its application to explore the space of possible dialog strategies for the UAH spoken dialog system.\n",
    "Index Terms: Dialog Strategy, User Simulation, Dialog Management, Statistical Methodologies, Spoken Dialog Systems\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-71"
  },
  "sugiyama12_interspeech": {
   "authors": [
    [
     "Hiroaki",
     "Sugiyama"
    ],
    [
     "Toyomi",
     "Meguro"
    ],
    [
     "Yasuhiro",
     "Minami"
    ]
   ],
   "title": "Preference-learning based inverse reinforcement learning for dialog control",
   "original": "i12_0222",
   "page_count": 4,
   "order": 75,
   "p1": "222",
   "pn": "225",
   "abstract": [
    "Dialog systems that realize dialog control with reinforcement learning have recently been proposed. However, reinforcement learning has an open problem that it requires a reward function that is difficult to set appropriately. To set the appropriate reward function automatically, we propose preference-learning based inverse reinforcement learning (PIRL) that estimates a reward function from dialog sequences and their pairwise-preferences, which is calculated with annotated ratings to the sequences. Inverse reinforcement learning finds a reward function, with which a system generates similar sequences to the training ones. This indicates that current IRL supposes that the sequences are equally appropriate for a given task; thus, it cannot utilize the ratings. In contrast, our PIRL can utilize pairwise preferences of the ratings to estimate the reward function. We examine the advantages of PIRL through comparisons between competitive algorithms that have been widely used to realize the dialog control. Our experiments show that our PIRL outperforms the other algorithms and has a potential to be an evaluation simulator of dialog control.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-72"
  },
  "meena12_interspeech": {
   "authors": [
    [
     "Raveesh",
     "Meena"
    ],
    [
     "Gabriel",
     "Skantze"
    ],
    [
     "Joakim",
     "Gustafson"
    ]
   ],
   "title": "A data-driven approach to understanding spoken route directions in human-robot dialogue",
   "original": "i12_0226",
   "page_count": 4,
   "order": 76,
   "p1": "226",
   "pn": "229",
   "abstract": [
    "In this paper, we present a data-driven chunking parser for automatic interpretation of spoken route directions into a route graph that is useful for robot navigation. Different sets of features and machine learning algorithms are explored. The results indicate that our approach is robust to speech recognition errors.\n",
    "Index Terms: spoken language understanding, route directions, human-robot interaction\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-73"
  },
  "komatani12_interspeech": {
   "authors": [
    [
     "Kazunori",
     "Komatani"
    ],
    [
     "Akira",
     "Hirano"
    ],
    [
     "Mikio",
     "Nakano"
    ]
   ],
   "title": "Detecting system-directed utterances using dialogue-level features",
   "original": "i12_0230",
   "page_count": 4,
   "order": 77,
   "p1": "230",
   "pn": "233",
   "abstract": [
    "We have developed a method to determine whether a user utterance is directed at the system or not. A spoken dialogue system should not respond to audio inputs that are not directed at it (i.e., a userfs mutter), and it therefore needs to detect such inputs to avoid unsuitable responses. We classify the two cases by logistic regression based on a feature set including utterance timing, utterance length, and dialogue status. We conducted experiments using 5395 user utterances for both transcription and automatic speech recognition results. Results showed that the classification accuracy improved by 11.0 and 4.1 points, respectively. We also discuss which features are effective in the classification.\n",
    "Index Terms: spoken dialogue system, system-directed utterance, utterance timing\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-74"
  },
  "planells12_interspeech": {
   "authors": [
    [
     "Joaquin",
     "Planells"
    ],
    [
     "Lluís-F.",
     "Hurtado"
    ],
    [
     "Emilio",
     "Sanchis"
    ],
    [
     "Encarna",
     "Segarra"
    ]
   ],
   "title": "An online generated transducer to increase dialog manager coverage",
   "original": "i12_0234",
   "page_count": 4,
   "order": 78,
   "p1": "234",
   "pn": "237",
   "abstract": [
    "This paper presents a new approach for dynamically increasing the coverage of a Statistical Dialog Manager. A Stochastic Finite-State Transducer for dialog management is estimated using a dialog simulator. This corpus-based model can cover most typical user behavior; however, sometimes unexpected situations may arise. Whenever these situations occur, the Dialog Manager model has no information to determine the next action. To deal with this problem, an Online Dialog Simulator is used in order to obtain synthetic dialogs for re-estimating the model and allowing it the dialog to continue. This approach has been evaluated with real users in a sport facilities booking task.\n",
    "Index Terms: spoken dialog systems, user simulation, dialog management, coverage problems\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-75"
  },
  "kazemzadeh12_interspeech": {
   "authors": [
    [
     "Abe",
     "Kazemzadeh"
    ],
    [
     "James",
     "Gibson"
    ],
    [
     "Juanchen",
     "Li"
    ],
    [
     "Sungbok",
     "Lee"
    ],
    [
     "Panayiotis G.",
     "Georgiou"
    ],
    [
     "Shrikanth",
     "Narayanan"
    ]
   ],
   "title": "A sequential Bayesian dialog agent for computational ethnography",
   "original": "i12_0238",
   "page_count": 4,
   "order": 79,
   "p1": "238",
   "pn": "241",
   "abstract": [
    "We present an sequential Bayesian belief update algorithm for an emotional dialog agentfs inference and behavior. This agentfs purpose is to collect usage patterns of natural language description of emotions among a community of speakers, a task which can be seen as a type of computational ethnography. We describe our target application, an emotionally-intelligent agent that can ask questions and learn about emotions through playing the emotion twenty questions (EMO20Q) game. We formalize the agentfs algorithms mathematically and algorithmically and test our model experimentally in an experiment of 45 human-computer dialogs with a range of emotional words as the independent variable. We found that (44%) of these dialog games are completed successfully, in comparison with earlier work in which human-human dialogs resulted in 85% successful completion on average. Despite lower than human performance, especially on difficult emotion words, the subjects rated that the agentfs humanity was 6.1 on a 0 to 10 scale. This indicates that the algorithm we present produces realistic behavior, but that issues of data sparsity may remain.\n",
    "Index Terms: dialog agents, emotion recognition, chatbot, EMO20Q,\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-76"
  },
  "seide12_interspeech": {
   "authors": [
    [
     "Frank",
     "Seide"
    ],
    [
     "Sean",
     "McDirmid"
    ]
   ],
   "title": "Clippyscript: a programming language for multi-domain dialogue systems",
   "original": "i12_0242",
   "page_count": 4,
   "order": 80,
   "p1": "242",
   "pn": "245",
   "abstract": [
    "The past year has witnessed the revival of spoken dialogue systems, which are becoming multi-domain and ubiquitous. In this context, the problem of efficient scripting of dialogues is becoming increasingly important. As of today, statistical approaches to dialogue control are not yet feasible; the problem remains quite firmly one of manual coding.   This paper describes a programming language we christened ClippyScript, which is aimed at rapid manual scripting of multi-domain dialogue systems. Only four main keywords - MODULE, SLOTS, GRAMMAR, and ACTIONS - plus a concept of focus provide the necessary abstractions for language understanding, dynamic grammars, hierarchical slot filling, multiple domains, access to data services, and performing of the dialogue goal. The languagefs expressive power is boosted by the ability of embedding code snippets in a high-level programming language (C#).\n",
    "Index Terms: spoken dialogue systems, scripting languages\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-77"
  },
  "engelbrecht12_interspeech": {
   "authors": [
    [
     "Klaus-Peter",
     "Engelbrecht"
    ],
    [
     "Sebastian",
     "Möller"
    ]
   ],
   "title": "Correlation between model-based approximations of grounding-related cognition and user judgments",
   "original": "i12_0246",
   "page_count": 4,
   "order": 81,
   "p1": "246",
   "pn": "249",
   "abstract": [
    "As spoken dialog systems become more complex, efficient ways to evaluate them in early development stages are required. User simulation has been successfully used for this purpose. While current user models describe behavior on the level of overt be-havior, modeling aspects of cognition can reveal direct insights into usability problems. Thus, in this paper we propose two models related to grounding in dialog: a model of the belief the user has about the system state, and a model of vocabulary alignment. We show that parameters derived from these models are significantly correlated with the users' quality perception.\n",
    "Index Terms: user model, evaluation, grounding\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-78"
  },
  "callejas12_interspeech": {
   "authors": [
    [
     "Zoraida",
     "Callejas"
    ],
    [
     "David",
     "Griol"
    ],
    [
     "Klaus-Peter",
     "Engelbrecht"
    ]
   ],
   "title": "Assessment of user simulators for spoken dialogue systems by means of subspace multidimensional clustering",
   "original": "i12_0250",
   "page_count": 4,
   "order": 82,
   "p1": "250",
   "pn": "253",
   "abstract": [
    "The assessment of user simulators in terms of their similarity with real users implies processing and interpreting large dialogue corpora, for which many interaction parameters can be considered. In this setting, the high dimensionality of the data makes it difficult to compare the dialogues as it is not always appropriate to consider all features equally in order to carry out meaningful interpretations. We propose to use subspace clustering for the assessment of users simulators, as this technique has been successfully applied to tackle and classify high-dimensional information in other areas of study. We created and assessed a user simulator for the Let's Go spoken dialogue system. The experimental results show that the proposed approach is easy to set up and helps to better interpret whether the user simulator has similar behaviours to real human users by creating clusters with different dimensions which cannot be identified with plain clustering techniques.\n",
    "Index Terms: user simulation, spoken dialogue systems, evaluation, clustering.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-79"
  },
  "kretzschmar12_interspeech": {
   "authors": [
    [
     "Florian",
     "Kretzschmar"
    ],
    [
     "Sebastian",
     "Möller"
    ]
   ],
   "title": "“help me, i need more user tests!” user simulations as supportive tool in the development process of spoken dialogue systems",
   "original": "i12_0322",
   "page_count": 4,
   "order": 83,
   "p1": "322",
   "pn": "325",
   "abstract": [
    "In this paper we present our experiences in developing a spoken dialogue system supported by tests with a user simulation. Since the code of dialogue systems with modest complexity can easily get unclear, it is almost impossible to deliver error-free systems without user tests in the development process. We show how we included our user simulation environment SpeechEval in the development process of three VoiceXML dialogue systems and discuss advantages and drawbacks compared to tests with real users.\n",
    "Index Terms: spoken dialogue systems, user simulation, user tests\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-80"
  },
  "witt12_interspeech": {
   "authors": [
    [
     "Silke M.",
     "Witt"
    ]
   ],
   "title": "Caller response timing patterns in spoken dialog systems",
   "original": "i12_0326",
   "page_count": 4,
   "order": 84,
   "p1": "326",
   "pn": "329",
   "abstract": [
    "This paper contains an analysis of caller response timing patterns in spoken dialog systems. The findings presented here are based on data from live commercial dialog systems. It is shown that caller responses after a system finished playing the prompt resemble a uni-modal distribution and can be modeled with a rational distribution function. This finding allows understanding when callers tend to respond to a system depending on the dialog state type. Based on these findings a no-speech timeout optimization method is being proposed.\n",
    "Index Terms: spoken dialog systems, turn-taking behavior, caller response timing, timeout optimization\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-81"
  },
  "hakkanitur12_interspeech": {
   "authors": [
    [
     "Dilek",
     "Hakkani-Tür"
    ],
    [
     "Gokhan",
     "Tur"
    ],
    [
     "Larry",
     "Heck"
    ],
    [
     "Ashley",
     "Fidler"
    ],
    [
     "Asli",
     "Celikyilmaz"
    ]
   ],
   "title": "A discriminative classification-based approach to information state updates for a multi-domain dialog system",
   "original": "i12_0330",
   "page_count": 4,
   "order": 85,
   "p1": "330",
   "pn": "333",
   "abstract": [
    "We propose a discriminative classification approach for updating the current information state of a multi-domain dialog system based on user responses. Our method uses a set of lexical and domain independent features to compare the spoken language understanding (SLU) output for the current user turn with the previous information state. We then update the information state accordingly, employing a discriminative machine learning approach. Using a data set collected from our conversational interaction system, we investigate the impact of features based on context dependent and context independent SLU tagging schemas. We show that the proposed approach outperforms two non-trivial baselines, one based on manually crafted rules and the other on classification with lexical features alone. Furthermore, such an approach allows the addition of new domains to the dialog manager in a seamless way.\n",
    "Index Terms: multi-domain spoken dialog systems, multi-turn spoken language understanding, learning information state updates\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-82"
  },
  "shriberg12_interspeech": {
   "authors": [
    [
     "Elizabeth",
     "Shriberg"
    ],
    [
     "Andreas",
     "Stolcke"
    ],
    [
     "Dilek",
     "Hakkani-Tür"
    ],
    [
     "Larry",
     "Heck"
    ]
   ],
   "title": "Learning when to listen: detecting system-addressed speech in human-human-computer dialog",
   "original": "i12_0334",
   "page_count": 4,
   "order": 86,
   "p1": "334",
   "pn": "337",
   "abstract": [
    "New challenges arise for addressee detection when multiple people interact jointly with a spoken dialog system using unconstrained natural language. We study the problem of discriminating computer-directed from human-directed speech in a new corpus of human-human-computer (H-H-C) dialog, using lexical and prosodic features. The prosodic features use no word, context, or speaker information. Results with 19% WER speech recognition show improvements from lexical features (EER=23.1%) to prosodic features (EER=12.6%) to a combined model (EER=11.1%). Prosodic features also provide a 35% error reduction over a lexical model using true words (EER from 10.2% to 6.7%). Modeling energy contours with GMMs provides a particularly good prosodic model. While lexical models perform well for commands, they confuse free-form system-directed speech with human-human speech. Prosodic models dramatically reduce these confusions, implying that users change speaking style as they shift addressees (computer versus human) within a session. Overall results provide strong support for combining simple acoustic-prosodic models with lexical models to detect speaking style differences for this task.\n",
    "Index Terms: addressee detection, spoken dialog system, prosody, language model, GMM, boosting, logistic regression.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-83"
  },
  "tur12_interspeech": {
   "authors": [
    [
     "Gokhan",
     "Tur"
    ],
    [
     "Minwoo",
     "Jeong"
    ],
    [
     "Ye-Yi",
     "Wang"
    ],
    [
     "Dilek",
     "Hakkani-Tür"
    ],
    [
     "Larry",
     "Heck"
    ]
   ],
   "title": "Exploiting the semantic web for unsupervised natural language semantic parsing",
   "original": "i12_0338",
   "page_count": 4,
   "order": 87,
   "p1": "338",
   "pn": "341",
   "abstract": [
    "In this paper, we propose to bring together the semantic web experience and statistical natural language semantic parsing modeling. The idea is that, the process for populating knowledge-bases by semantically parsing structured web pages may provide very valuable implicit annotation for language understanding tasks. We mine search queries hitting to these web pages in order to semantically annotate them for building statistical unsupervised slot filling models, without even a need for a semantic annotation guideline. We present promising results demonstrating this idea for building an unsupervised slot filling model for the movies domain with some representative slots. Furthermore, we also employ unsupervised model adaptation for cases when there are some in-domain unannotated sentences available. Another key contribution of this work is using implicitly annotated natural-language-like queries for testing the performance of the models, in a totally unsupervised fashion. We believe, such an approach also ensures consistent semantic representation between the semantic parser and the backend knowledge-base.\n",
    "Index Terms: semantic parsing, semantic web, semantic search, dialog, natural language understanding\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-84"
  },
  "fandrianto12_interspeech": {
   "authors": [
    [
     "Andrew",
     "Fandrianto"
    ],
    [
     "Maxine",
     "Eskenazi"
    ]
   ],
   "title": "Prosodic entrainment in an information-driven dialog system",
   "original": "i12_0342",
   "page_count": 4,
   "order": 88,
   "p1": "342",
   "pn": "345",
   "abstract": [
    "This paper explores entrainment of two speaking styles, shouting and hyperarticulation, in an information-driven spoken dialog system. Both styles present difficulties for automatic speech recognition. We describe and evaluate the system's detection and reaction mechanisms for these speaking styles, which involve deploying appropriate dialog-level strategies. The three strategies tested do induce style change more effectively than the baseline of no strategy. This can translate into both better recognition and a higher chance of task success. Shouting is found to be more amenable to modification than hyperarticulation and the effect of the former on system performance is more profound.\n",
    "Index Terms: spoken dialog, entrainment\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-85"
  },
  "schuller12_interspeech": {
   "authors": [
    [
     "Björn",
     "Schuller"
    ],
    [
     "Stefan",
     "Steidl"
    ],
    [
     "Anton",
     "Batliner"
    ],
    [
     "Elmar",
     "Nöth"
    ],
    [
     "Alessandro",
     "Vinciarelli"
    ],
    [
     "Felix",
     "Burkhardt"
    ],
    [
     "Rob van",
     "Son"
    ],
    [
     "Felix",
     "Weninger"
    ],
    [
     "Florian",
     "Eyben"
    ],
    [
     "Tobias",
     "Bocklet"
    ],
    [
     "Gelareh",
     "Mohammadi"
    ],
    [
     "Benjamin",
     "Weiss"
    ]
   ],
   "title": "The INTERSPEECH 2012 speaker trait challenge",
   "original": "i12_0254",
   "page_count": 4,
   "order": 89,
   "p1": "254",
   "pn": "257",
   "abstract": [
    "The INTERSPEECH 2012 Speaker Trait Challenge provides for the first time a unified test-bed for \"perceived\" speaker traits: Personality in the five OCEAN personality dimensions, likability of speakers, and intelligibility of pathologic speakers. In this paper, we describe these three Sub- Challenges, Challenge conditions, baselines, and a new feature set by the openSMILE toolkit, provided to the participants.\n",
    "Index Terms: Computational Paralinguistics, Speaker Traits, Personality, Likability, Pathology\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-86"
  },
  "polzehl12_interspeech": {
   "authors": [
    [
     "Tim",
     "Polzehl"
    ],
    [
     "Katrin",
     "Schoenenberg"
    ],
    [
     "Sebastian",
     "Möller"
    ],
    [
     "Florian",
     "Metze"
    ],
    [
     "Gelareh",
     "Mohammadi"
    ],
    [
     "Alessandro",
     "Vinciarelli"
    ]
   ],
   "title": "On speaker-independent personality perception and prediction from speech",
   "original": "i12_0258",
   "page_count": 4,
   "order": 90,
   "p1": "258",
   "pn": "261",
   "abstract": [
    "In this paper, we present ongoing experiments and insights regarding automatic assessment of perceived personality. While within the INTERSPEECH Speaker Trait Challenge participants will train systems in order to recognize binary targets along the Big 5 personality trait, we will analyze and discuss properties of the data, the labeling scheme and the predictive quality. Conducting factor analyses, estimating reliability, and building regression models capturing dimensions of personality we compare all results to our former and current work and introduce a new extension of our personality database. Eventually, this paper contributes in methodology and understanding on how to asses the perceived personality from an unknown speaker by humans and machines.\n",
    "Index Terms: extra-linguistic speech properties, personality modeling from speech, speaker characteristics\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-87"
  },
  "audhkhasi12_interspeech": {
   "authors": [
    [
     "Kartik",
     "Audhkhasi"
    ],
    [
     "Angeliki",
     "Metallinou"
    ],
    [
     "Ming",
     "Li"
    ],
    [
     "Shrikanth S.",
     "Narayanan"
    ]
   ],
   "title": "Speaker personality classification using systems based on acoustic-lexical cues and an optimal tree-structured Bayesian network",
   "original": "i12_0262",
   "page_count": 4,
   "order": 91,
   "p1": "262",
   "pn": "265",
   "abstract": [
    "Automatic classification of human personality along the Big Five dimensions is an interesting problem with several practical applications. This paper makes some contributions in this regard. First, we propose a few automatically-derived personality-discriminating lexical features which provide information complementary to the conventional acoustic-prosodic cues. We also design a frame-level Gaussian mixture model based system which adds complimentary information to the systems trained on global statistical functionals. Next, we note that the Big Five dimensions are correlated and thus model the dependency between these dimensions in the form of an optimal tree-structured Bayesian network. Our final sub-system consists of within class covariance normalization followed by L1-regularized logistic regression. Fusion of all these sub-systems achieves better classification performance than independently trained classifiers using just acoustic features.\n",
    "Index Terms: Speaker Personality Classification, Bayesian Network Structure Learning, Gaussian Mixture Models, Within Class Covariance Normalization\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-88"
  },
  "chastagnol12_interspeech": {
   "authors": [
    [
     "Clément",
     "Chastagnol"
    ],
    [
     "Laurence",
     "Devillers"
    ]
   ],
   "title": "Personality traits detection using a parallelized modified SFFS algorithm",
   "original": "i12_0266",
   "page_count": 4,
   "order": 92,
   "p1": "266",
   "pn": "269",
   "abstract": [
    "We present in this paper a contribution to the INTERSPEECH 2012 Speaker Trait Challenge. We participated in the Personality Sub-Challenge, where the main characteristics of speakers according to the five OCEAN dimensions had to be determined based on short audio recordings solely. We considered the task as a general optimization problem and applied a parallelized version of a modified SFFS algorithm, wrapped around a SVM classifier, along with parameter tuning. Our system has yielded higher than baseline scores on all five dimensions, adding more than 20 percentage points to the recognition score of the Openness dimension.\n",
    "Index Terms: personality detection, feature selection\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-89"
  },
  "pohjalainen12_interspeech": {
   "authors": [
    [
     "Jouni",
     "Pohjalainen"
    ],
    [
     "Serdar",
     "Kadioglu"
    ],
    [
     "Okko",
     "Räsänen"
    ]
   ],
   "title": "Feature selection for speaker traits",
   "original": "i12_0270",
   "page_count": 4,
   "order": 93,
   "p1": "270",
   "pn": "273",
   "abstract": [
    "This study focuses on handling high-dimensional classification problems by means of feature selection. The data sets used are provided by the organizers of the Interspeech 2012 Speaker Trait Challenge. A combination of two feature selection approaches gives results that approach or exceed the challenge baselines using a k-nearest-neighbor classifier. One of the feature selection methods is based on covering the data set with correct unsupervised or supervised classifications according to individual features. The other selection method applies a measure of statistical dependence between discretized features and class labels.\n",
    "Index Terms: pattern recognition, feature selection, high-dimensional data, speaker characteristics\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-90"
  },
  "wagner12_interspeech": {
   "authors": [
    [
     "Johannes",
     "Wagner"
    ],
    [
     "Florian",
     "Lingenfelser"
    ],
    [
     "Elisabeth",
     "André"
    ]
   ],
   "title": "A frame pruning approach for paralinguistic recognition tasks",
   "original": "i12_0274",
   "page_count": 4,
   "order": 94,
   "p1": "274",
   "pn": "277",
   "abstract": [
    "In conventional paralinguistic classification approaches, information gained by low level features is described over broad segments (like whole turns) via statistical functionals. This procedure presumes meaningful information to be embodied within the whole segment. This assumption may be misleading if distinctive cues within a sample are surrounded by non-meaningful information or noise. In this case it would surely be beneficial to keep only parts of the sample that are most relevant for the recognition task. In this paper we propose a novel cluster-based approach, which aims at identifying frames likely to carry distinctive information. Evaluation is done within the INTERSPEECH 2012 Speaker Trait Challenge. Results show that under certain configurations frame pruning in fact leads to an improvement in recognition accuracy. On the observed corpus most stable improvements were achieved at a frame drop of 4-8%.\n",
    "Index Terms: paralinguistic recognition, frame pruning, personality traits\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-91"
  },
  "ivanov12_interspeech": {
   "authors": [
    [
     "Alexei",
     "Ivanov"
    ],
    [
     "Xin",
     "Chen"
    ]
   ],
   "title": "Modulation spectrum analysis for speaker personality trait recognition",
   "original": "i12_0278",
   "page_count": 4,
   "order": 95,
   "p1": "278",
   "pn": "281",
   "abstract": [
    "We explore the utility of individually selected modulation spectral features for speech and speaker characterization in general, and specifically to prediction of the perceived speaker personality profile. We suggest a method of construction of a sparse feature space and a method of finding the approximately best feature subset for attributing a specific characteristic of speech or speaker. The current selection method is based on the Kolmogorov-Smirnov statistical test applied to individual features. We assume that the characterization task is defined empirically and no a-priory theory exist to explain characteristic attribution processes. Experimental results indicate that employment of selected modulation spectral features works better than the current state-of-the-art in prediction of personality traits.\n",
    "Index Terms: speech characterization, modulation spectrum analysis, feature selection\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-92"
  },
  "cummins12_interspeech": {
   "authors": [
    [
     "Nicholas",
     "Cummins"
    ],
    [
     "Julien",
     "Epps"
    ],
    [
     "Jia Min Karen",
     "Kua"
    ]
   ],
   "title": "A comparison of classification paradigms for speaker likeability determination",
   "original": "i12_0282",
   "page_count": 4,
   "order": 96,
   "p1": "282",
   "pn": "285",
   "abstract": [
    "In this paper we investigate the performance of different classification paradigms, testing each with a range of acoustic features, to find a system that is well suited to speaker likeability classification. We introduce a Sparse Representation Classifier for paralinguistic classification and explore the role of training data selection for a GMM classifier. Results demonstrate that (1) Single dimensional features of pitch direction, shimmer and spectral roll-off were the most suitable features found when testing on the development set but we were unable to reproduce their performance in the final classification task, (2) Using UBM training data selection increased accuracy of MFCC's and (3) Sparse Representation showed promise as a paralinguistic classifier with results comparable to that of SVM.\n",
    "Index Terms: Likeability, single and multi-dimensional feature selection, Sparse Representation Classifier, UBM data selection.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-93"
  },
  "lu12_interspeech": {
   "authors": [
    [
     "Dingchao",
     "Lu"
    ],
    [
     "Fei",
     "Sha"
    ]
   ],
   "title": "Predicting likability of speakers with Gaussian processes",
   "original": "i12_0286",
   "page_count": 4,
   "order": 97,
   "p1": "286",
   "pn": "289",
   "abstract": [
    "Predicting the likability of speakers based on their voices is a challenging problem. In this paper, we study this problem in the context of the Likability Sub-challenge of InterSpeech 2012 Speaker Trait Challenge. We apply and extend the technique of Gaussian Processes to predict likability using acoustic features provided by the sub-challenge organizers. Our best performing systems improve published baselines modestly on the test set. We also show that likability of male speakers can be more accurately predicted than that of female speakers. Additionally, using a sparse subset of features typically leads to noticeably improved results. The proposed methods are promising and to fully explore their potentials, we plan to make further submissions to the competition.\n",
    "Index Terms: likability of voice, Gaussian Processes, sparse models, intelligibility of voice\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-94"
  },
  "brueckner12_interspeech": {
   "authors": [
    [
     "Raymond",
     "Brueckner"
    ],
    [
     "Björn",
     "Schuller"
    ]
   ],
   "title": "Likability classification - a not so deep neural network approach",
   "original": "i12_0290",
   "page_count": 4,
   "order": 98,
   "p1": "290",
   "pn": "293",
   "abstract": [
    "This papers presents results on the application of restricted Boltzmann machines (RBM) and deep belief networks (DBN) on the Likability Sub- Challenge of the Interspeech 2012 Speaker Trait Challenge. RBMs are a particular form of log-linear Markov Random Fields and generative models which try to model the probability distribution of the underlying input data which can be trained in an unsupervised fashion. DBNs can be constructed by stacking RBMs and are known to yield an increasingly complex representation of the input data as the number of layers increases. Our results show that the Likability Sub-Challenge classification task does not benefit from the modeling power of DBN, but that the use of an RBM as the first stage of a two-layer neural network with subsequent fine-tuning improves the baseline result of 59.0% to 64.0%, i.e. a relative 8.5% improvement of the unweighted average evaluation measure.\n",
    "Index Terms: Likability, speaker trait challenge, restricted Boltzmann machines, deep belief networks\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-95"
  },
  "wu12b_interspeech": {
   "authors": [
    [
     "Dongrui",
     "Wu"
    ]
   ],
   "title": "Genetic algorithm based feature selection for speaker trait classification",
   "original": "i12_0294",
   "page_count": 4,
   "order": 99,
   "p1": "294",
   "pn": "297",
   "abstract": [
    "Personality, likability, and pathology are important speaker traits that convey rich information beyond the actual language. They have promising applications in human-machine interaction, health informatics, and surveillance. However, they are less researched than other paralinguistics phenomena such as emotion, age and gender. In this paper we propose a novel feature selection approach for speaker trait classification from a large number of acoustic features. It combines Fisher Information Metric feature filtering and Genetic Algorithm based feature selection, and fuses several elementary Support Vector Machines with different feature subsets to achieve robust classification performance. Experiments on an INTERSPEECH 2012 Speaker Trait Challenge dataset show that our approach outperforms both baseline approaches.\n",
    "Index Terms: Paralinguistics, Speaker Trait Classification, Personality, Likability, Pathology, Genetic algorithm, Fisher Information Metric, SVM\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-96"
  },
  "weiss12_interspeech": {
   "authors": [
    [
     "Benjamin",
     "Weiss"
    ],
    [
     "Felix",
     "Burkhardt"
    ]
   ],
   "title": "Is 'not bad' good enough? aspects of unknown voices' likability",
   "original": "i12_0510",
   "page_count": 4,
   "order": 100,
   "p1": "510",
   "pn": "513",
   "abstract": [
    "From the DTAG likability database the 30 most and 30 least likable ones have been selected for further phonetic analysis and expert questionnaire. In contrast to dislikable speakers, likable ones exhibit almost no perceivable accent, command style or disfluencies and were rated very high on all six questionnaire scales. Dislikable speakers are only moderately rated. However, dislikable speakers display also lower pronunciation precision, different amount of jitter, lower articulation rate and higher pitch, indicating that just the absence of negatively perceived characterizations is not enough to become a likable speaker.\n",
    "Index Terms: likability, speaker traits, speaking style\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-97"
  },
  "sanchez12_interspeech": {
   "authors": [
    [
     "Michelle Hewlett",
     "Sanchez"
    ],
    [
     "Aaron",
     "Lawson"
    ],
    [
     "Dimitra",
     "Vergyri"
    ],
    [
     "Harry",
     "Bratt"
    ]
   ],
   "title": "Multi-system fusion of extended context prosodic and cepstral features for paralinguistic speaker trait classification",
   "original": "i12_0514",
   "page_count": 4,
   "order": 101,
   "p1": "514",
   "pn": "517",
   "abstract": [
    "As automatic speech processing has matured, research attention has expanded to paralinguistic speech problems that aim to detect \"beyondthe- words\" information. This paper focuses on the identification of seven speaker trait categories from the Interspeech Speaker Trait Challenge: likeability, intelligibility, openness, conscientiousness, extraversion, agreeableness, and neuroticism. Our approach combines multiple features including prosodic, cepstral, shifted-delta cepstral, and a reduced set of the OpenSMILE features. Our classification approaches included GMMUBM, eigenchannel, support vector machines, and distance based classifiers. Optimized feature reduction and logistic regression-based score calibration and fusion led to results that perform competitively against the challenge baseline in all categories.\n",
    "Index Terms: speaker traits, prosody, MFCCs, Gaussian mixture modeling\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-98"
  },
  "buisman12_interspeech": {
   "authors": [
    [
     "Harm",
     "Buisman"
    ],
    [
     "Eric",
     "Postma"
    ]
   ],
   "title": "The log-Gabor method: speech classification using spectrogram image analysis",
   "original": "i12_0518",
   "page_count": 4,
   "order": 102,
   "p1": "518",
   "pn": "521",
   "abstract": [
    "We explored the suitability of the log-Gabor method, a speech analysis method inspired by Ezzat, Bouvrie & Poggio (2007), for automatic classification of personality and likability traits in speech. The core idea underlying the log-Gabor method is to treat the spectrogram as an image of spectro-temporal information. The image is transformed into Gabor energy values using the two-dimensional logarithmic Gabor transform, which is a standard feature extraction method in visual texture analysis. The aggregated energy values are mapped onto classes by means of a support vector machine (SVM). The log-Gabor method performed above baseline on both the INTERSPEECH Personality and Likability Sub-Challenges: 74.2% on the Likability task (baseline 58.0%) and 78.1% on the Personality task (baseline 70.3%). These results lead us to conclude that the log-Gabor method is a feasible method for extracting perceptual cues from speech.\n",
    "Index Terms: spectro-temporal analysis, spectrogram analysis, log Gabor filters, likability classification, personality classification, support vector machines\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-99"
  },
  "attabi12_interspeech": {
   "authors": [
    [
     "Yazid",
     "Attabi"
    ],
    [
     "Pierre",
     "Dumouchel"
    ]
   ],
   "title": "Anchor models and WCCN normalization for speaker trait classification",
   "original": "i12_0522",
   "page_count": 4,
   "order": 103,
   "p1": "522",
   "pn": "525",
   "abstract": [
    "This paper presents an improved version of anchor model applied to solve the two-class classification tasks of the INTERSPEECH 2012 speaker trait Challenge. The introduction of within-class covariance normalization applied to the log-likelihood scores of the anchor space can not only improve the results compared to the unnormalized version but also exceed the performance of GMM or GMM-UBM systems. Furthermore, our results on development set show a relative improvement of 6.1%, 8.6% and 3.2% for the Personality, likability and pathology sub-challenges respectively compared to the best baseline systems provided by the organizers.\n",
    "Index Terms: anchor model, WCCN, speaker trait classification, GMM model, Interspeech 2012 challenge\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-100"
  },
  "montacie12_interspeech": {
   "authors": [
    [
     "Claude",
     "Montacié"
    ],
    [
     "Marie-José",
     "Caraty"
    ]
   ],
   "title": "Pitch and intonation contribution to speakers' traits classification",
   "original": "i12_0526",
   "page_count": 4,
   "order": 104,
   "p1": "526",
   "pn": "529",
   "abstract": [
    "The article describes the system we submitted for the three sub-challenges of INTERSPEECH 2012 Speaker Trait Challenge for the classification of the five personality traits of OCEAN, likability and intelligibility. The system was based on a two-class SVM-classifier using one-speakerleave- out cross-validation (OSLO) to optimize SVM complexity parameter and to select the feature set for trait classification. Pitch, intonation and spectrum contributions to speakers' traits classification were studied. One feature set was specially designed for intonation modeling. Variations from the official feature set based on pitch and spectrum were assessed in traits classification. Preliminary results on the Test set have shown a significant improvement of 4.7% on the likability trait, an improvement of 0.4% on the intelligibility trait and no improvement on the OCEAN traits compared to the official baseline unweighted accuracy results.\n",
    "Index Terms: speaker's trait detection, pitch, intonation\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-101"
  },
  "anumanchipalli12_interspeech": {
   "authors": [
    [
     "Gopala Krishna",
     "Anumanchipalli"
    ],
    [
     "Hugo",
     "Meinedo"
    ],
    [
     "Miguel",
     "Bugalho"
    ],
    [
     "Isabel",
     "Trancoso"
    ],
    [
     "Luís C.",
     "Oliveira"
    ],
    [
     "Alan W.",
     "Black"
    ]
   ],
   "title": "Text-dependent pathological voice detection",
   "original": "i12_0530",
   "page_count": 4,
   "order": 105,
   "p1": "530",
   "pn": "533",
   "abstract": [
    "This work presents some features exploiting the underlying text in detection of pathological voices. While global characteristics of the speaker's source and spectral features have been successfully employed in pathological voice detection, the underlying text has largely been ignored. In this work, we focus on experiments that exploit the text stimulus that is read by the subject. Features derived from text include the mean cepstral distortion of the subject from an average intelligible speaker, and prosodic features include the speaking rate, statistics of phoneme durations etc. The phonetic labelling information is also exploited to ignore all the unvoiced regions of the speech samples to improve the discriminability between intelligible and pathological voices. We also design features that capture the speaker's overall closeness to intelligible instances of the same text stimulus from other speakers. Our experiments show that the proposed text-derived features improve the detection of pathological voices by 20%.\n",
    "Index Terms: Pathological voices, example based detection, text-driven features, fusion of classification methods\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-102"
  },
  "kim12_interspeech": {
   "authors": [
    [
     "Jangwon",
     "Kim"
    ],
    [
     "Naveen",
     "Kumar"
    ],
    [
     "Andreas",
     "Tsiartas"
    ],
    [
     "Ming",
     "Li"
    ],
    [
     "Shrikanth",
     "Narayanan"
    ]
   ],
   "title": "Intelligibility classification of pathological speech using fusion of multiple high level descriptors",
   "original": "i12_0534",
   "page_count": 4,
   "order": 106,
   "p1": "534",
   "pn": "537",
   "abstract": [
    "Pathological speech usually refers to the condition of speech distortion resulting from atypicalities in voice and/or in the articulatory mechanisms owing to disease, illness or other physical or biological insult to the production system. While automatic evaluation of speech intelligibility and quality could come in handy in these scenarios to assist in diagnosis and treatment design, the many sources and types of variability often make it a very challenging computational processing problem. In this work we design multiple subsystems to address different aspects of pathological speech characteristics. These subsystems are then fused at the binary hard score level (intelligible or not intelligible) using Bayesian networks. Results show that subsystems, such as multiple language phoneme probability system, prosodic and intonational subsystem, and voice quality and pronunciation subsystem, have discriminating power for intelligibility (9.8%, 17.1%, 14.6% higher than by-chance respectively). Noisy-Majority based fusion shows 66.4% accuracy, but the performance improvement by fusion is not made. Also, voice clustering based joint classification is applied to minimize misclassification of the best subsystem, and it shows the best classification accuracy (79.9% on dev set, 76.8% on test set).\n",
    "Index Terms: pathological speech, intelligibility of speech, fusion of multiple subsystems\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-103"
  },
  "stark12_interspeech": {
   "authors": [
    [
     "Anthony",
     "Stark"
    ],
    [
     "Alireza",
     "Bayestehtashk"
    ],
    [
     "Meysam",
     "Asgari"
    ],
    [
     "Izhak",
     "Shafran"
    ]
   ],
   "title": "Interspeech pathology challenge: investigations into speaker and sentence specific effects",
   "original": "i12_0538",
   "page_count": 4,
   "order": 107,
   "p1": "538",
   "pn": "541",
   "abstract": [
    "In this paper, we report our experiments on Interspeech 2012 Speaker Trait Pathology challenge task. Specifically, we investigate two factors that impact the acoustic properties of the utterances collected in this task. Although the task treats utterances as independent data points, multiple utterances are recorded from individual speakers. Furthermore, the utterances correspond to readings of 17 given written sentences. In one experiment, we attempt to reduce variation due to speaker through dimensionality reduction. While these experiments showed promising results on development set, the performance did not translate to the evaluation test. In another, we learn classifiers conditioned on the sentences to capture sentence-specific signatures. This approach showed improved performance over the baseline on development set and the improvement translated to marginal gains on evaluation set. These experiments demonstrates the need to pay attention to the independence assumptions while collecting and defining clinical tasks.\n",
    "Index Terms: speech pathology, intelligibility\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-104"
  },
  "zhou12_interspeech": {
   "authors": [
    [
     "Xinhui",
     "Zhou"
    ],
    [
     "Daniel",
     "Garcia-Romero"
    ],
    [
     "Nima",
     "Mesgarani"
    ],
    [
     "Maureen",
     "Stone"
    ],
    [
     "Carol",
     "Espy-Wilson"
    ],
    [
     "Shihab",
     "Shamma"
    ]
   ],
   "title": "Automatic intelligibility assessment of pathologic speech in head and neck cancer based on auditory-inspired spectro-temporal modulations",
   "original": "i12_0542",
   "page_count": 4,
   "order": 108,
   "p1": "542",
   "pn": "545",
   "abstract": [
    "Oral, head and neck cancer represents 3% of all cancers in the United States and is the 6th most common cancer worldwide. Depending on the tumor size, location and staging, patients are treated by radical surgery, radiology, chemotherapy or a combination of those treatments. As a result, their anatomical structures for speech are impaired and this leads to some negative impact on their speech intelligibility. As a part of the INTERSPEECH 2012 speaker trait Pathology sub-challenge, this study explored the use of auditory-inspired spectro-temporal modulation features for automatic speech intelligibility assessment of those pathologic speech. The averaged spectro-temporal modulations of speech considered as either intelligible or non-intelligible in the challenge database were analyzed and it was found that the non-intelligible speech tends to have its modulation amplitude peaks shift towards a smaller rate and scale. Based on SVM and GMM, variants of spectro-temporal modulation features were tested on the speaker trait challenge problem and the resulting performances on both the development and the test datasets are comparable to the baseline performance.\n",
    "Index Terms: Oral, head and neck cancer, speech pathology, speech intelligibility, spectro-temporal modulation, support vector machine (SVM), Gaussian mixture model (GMM)\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-105"
  },
  "huang12_interspeech": {
   "authors": [
    [
     "Dong-Yan",
     "Huang"
    ],
    [
     "Yongwei",
     "Zhu"
    ],
    [
     "Dajun",
     "Wu"
    ],
    [
     "Rongshan",
     "Yu"
    ]
   ],
   "title": "Detecting intelligibility by linear dimensionality reduction and normalized voice quality hierarchical features",
   "original": "i12_0546",
   "page_count": 4,
   "order": 109,
   "p1": "546",
   "pn": "549",
   "abstract": [
    "Voice disorders could increase unhealthy social behavior and voice abuse, and dramatically affect the patients' quality of life. Therefore, automatic intelligibility detection of pathological voices has an important role in the opportune treatment of pathological voices. This paper aims at designing an intelligibility detection system which is characterized by two aspects. First, the system is based on features inspired from voice pathology such as voice quality features, spectral and harmonicity features, and hierarchical features. Second, the intelligibility detection is based on fusion of linear dimensionality reduction such as asymmetric sparse PLS trained by different sets of normalized features. An optimal unweighted recall performance is 71.88% on the test set, an improvement of 2.28% absolute (3.28% relative) over the baseline model accuracy of 69.60%.\n",
    "Index Terms: Intelligibility detection, voice quality, hierarchical features, dimensionality reduction\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-106"
  },
  "kumatani12_interspeech": {
   "authors": [
    [
     "Kenichi",
     "Kumatani"
    ],
    [
     "Bhiksha",
     "Raj"
    ],
    [
     "Rita",
     "Singh"
    ],
    [
     "John",
     "McDonough"
    ]
   ],
   "title": "Microphone array post-filter based on spatially-correlated noise measurements for distant speech recognition",
   "original": "i12_0298",
   "page_count": 4,
   "order": 110,
   "p1": "298",
   "pn": "301",
   "abstract": [
    "This paper presents a new microphone-array post-filtering algorithm for distant speech recognition (DSR). Conventionally, post-filtering methods assume static noise field models, and using this assumption, employ a Wiener filter mechanism for estimating the noise parameters. In contrast to this, we show how we can build the Wiener post-filter based on actual noise observations without any noise-field assumption. The algorithm is framed within a state-of-the-art beamforming technique, namely maximum negentropy (MN) beamforming with super directivity. We investigate the effectiveness of the proposed post-filter on DSR through experiments on noisy data collected in a car under different acoustic conditions. Experiments show that the new post-filtering mechanism is able to achieve up to 20% relative reduction of word error rates (WER) under the represented noise conditions, as compared to a single distant microphone. In contrast, super-directive (SD) beamforming followed by Zelinski post-filtering achieves a relative WER reduction of only up to 11%. Other post-filters evaluated perform similarly in comparison to the proposed post-filter.\n",
    "Index Terms: Microphone array, Post-filter, Distant speech recognition, Automotive speech application\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-107"
  },
  "weninger12b_interspeech": {
   "authors": [
    [
     "Felix",
     "Weninger"
    ],
    [
     "Martin",
     "Wöllmer"
    ],
    [
     "Björn",
     "Schuller"
    ]
   ],
   "title": "Combining bottleneck-BLSTM and semi-supervised sparse NMF for recognition of conversational speech in highly instationary noise",
   "original": "i12_0302",
   "page_count": 4,
   "order": 111,
   "p1": "302",
   "pn": "305",
   "abstract": [
    "We address the speaker independent automatic recognition of spontaneous speech in highly instationary noise by applying semi-supervised sparse non-negative matrix factorization (NMF) for speech enhancement coupled with our recently proposed front end utilizing bottleneck (BN) features generated by a bidirectional Long Short-Term Memory (BLSTM) recurrent neural network. In our evaluation, we unite the noise corpus and evaluation protocol of the 2011 PASCAL CHiME challenge with the Buckeye database, and we demonstrate that the combination of NMF enhancement and BNBLSTM front end introduces significant and consistent gains in word accuracy in this highly challenging task at signal-to-noise ratios from -6 to 9 dB.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-108"
  },
  "lu12b_interspeech": {
   "authors": [
    [
     "Liang",
     "Lu"
    ],
    [
     "K. K.",
     "Chin"
    ],
    [
     "Arnab",
     "Ghoshal"
    ],
    [
     "Steve",
     "Renals"
    ]
   ],
   "title": "Noise compensation for subspace Gaussian mixture models",
   "original": "i12_0306",
   "page_count": 4,
   "order": 112,
   "p1": "306",
   "pn": "309",
   "abstract": [
    "Joint uncertainty decoding (JUD) is an effective model-based noise compensation technique for conventional Gaussian mixture model (GMM) based speech recognition systems. In this paper, we apply JUD to subspace Gaussian mixture model (SGMM) based acoustic models. The total number of Gaussians in the SGMM acoustic model is usually much larger than for conventional GMMs, which limits the application of approaches which explicitly compensate each Gaussian, such as vector Taylor series (VTS). However, by clustering the Gaussian components into a number of regression classes, JUD-based noise compensation can be successfully applied to SGMM systems. We evaluate the JUD/SGMM technique using the Aurora 4 corpus, and the experimental results indicated that it is more accurate than conventional GMM-based systems using either VTS or JUD noise compensation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-109"
  },
  "sun12_interspeech": {
   "authors": [
    [
     "Yang",
     "Sun"
    ],
    [
     "Mathew M.",
     "Doss"
    ],
    [
     "Jort F.",
     "Gemmeke"
    ],
    [
     "Bert",
     "Cranen"
    ],
    [
     "Louis ten",
     "Bosch"
    ],
    [
     "Lou",
     "Boves"
    ]
   ],
   "title": "Combination of sparse classification and multilayer perceptron for noise-robust ASR",
   "original": "i12_0310",
   "page_count": 4,
   "order": 113,
   "p1": "310",
   "pn": "313",
   "abstract": [
    "On the AURORA-2 task good results at low SNR levels have been obtained with a system that uses state posterior estimates provided by an exemplar-based sparse classification (SC) system. At the same time, posterior estimates obtained with multilayer perceptron (MLP) yield good results at high SNRs. In this paper, we investigate the effect of combining the estimates from the SC and MLP systems at the probability level. More precisely, the probabilities are combined by a sum rule or a product rule using static and inverse-entropy based dynamic weights. In addition, we investigate a modified dynamic weighting approach which enhances the contribution of SC stream based on the information about static weights and average dynamic weights obtained on cross validation data. Our studies on AURORA-2 task shows that in all conditions the modified dynamic weighting approach yields a dual-input system that performs better than or equal to the best stand-alone system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-110"
  },
  "li12b_interspeech": {
   "authors": [
    [
     "Weifeng",
     "Li"
    ],
    [
     "Hervé",
     "Bourlard"
    ]
   ],
   "title": "Sub-band based log-energy and its dynamic range stretching for robust in-car speech recognition",
   "original": "i12_0314",
   "page_count": 4,
   "order": 114,
   "p1": "314",
   "pn": "317",
   "abstract": [
    "Log energy and its delta parameters, typically derived from full-band spectrum, are commonly used in automatic speech recognition (ASR) systems. In this paper, we address the problem of estimating log energy in the presence of background noise (usually resulting in a reduction in dynamic ranges of spectral energies). We theoretically show that the background noise affects the trajectories of the \"conventional\" log energy and its delta parameters, resulting in very poor estimation of the actual log energy and its delta parameters, which no longer describe the speech signal. We thus propose to estimate log energy from the sub-band spectrum, followed by a dynamic range stretching. Based on speech recognition experiments conducted on CENSREC-2 in-car database, the proposed log energy (and its corresponding delta parameters) is shown to perform very well, resulting in an average relative improvement of 27.2% compared with the baseline front-ends. Moreover, it is also shown that further improvement can be achieved by incorporating those new MFCCs obtained through non-linear spectral contrast stretching.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-111"
  },
  "bouallegue12_interspeech": {
   "authors": [
    [
     "Mohamed",
     "Bouallegue"
    ],
    [
     "Mickael",
     "Rouvier"
    ],
    [
     "Driss",
     "Matrouf"
    ],
    [
     "Georges",
     "Linarès"
    ]
   ],
   "title": "Noise compensation for speech recognition using subspace Gaussian mixture models",
   "original": "i12_0318",
   "page_count": 4,
   "order": 115,
   "p1": "318",
   "pn": "321",
   "abstract": [
    "In this paper, we adress the problem of additive noise which degrades substantially the performances of speech recognition system. We propose a cepstral denoising based on the Subspace Gaussian Mixture Models paradigm (SGMM). The acoustic space is modeled by using a UBM-GMM. Each phoneme is modeled by a GMM derived from the UBM. The concatenation of the means of a given GMM leads to a very high dimention space, called the supervector space. The SGMM paradigm allows to model the additive noise as an additive component located in a subspace of low dimension (with respect to the supervector space). For each speech segment, this additive noise component is estimated in a model space. From this estimation, a specific frame transformation is obtained and applied to such a data frame. In this work, training data are assumed to be clean, so the cleaning process is applied only on test data. The proposed approach is tested on data recorded in a noisy environment and also on artificially noised data. With this approach we obtain, on data recorded in a noisy environment, a relative WER reduction of 15%.\n",
    "Index Terms: Acoustic modeling, Noise compensation, Subspace Gaussian Mixture Models, robust speech recognition\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-112"
  },
  "ringeval12_interspeech": {
   "authors": [
    [
     "Fabien",
     "Ringeval"
    ],
    [
     "Mohamed",
     "Chetouani"
    ],
    [
     "Björn",
     "Schuller"
    ]
   ],
   "title": "Novel metrics of speech rhythm for the assessment of emotion",
   "original": "i12_0346",
   "page_count": 4,
   "order": 116,
   "p1": "346",
   "pn": "349",
   "abstract": [
    "Whereas rhythmic speech analysis is known to bear great potential for the recognition of emotion, it is often omitted or reduced to the speaking rate or segmental durations. An obvious explanation is that the characterisation of speech rhythm is not an easy task itself and there exist many types of rhythmic information. In this paper, we study advanced methods to define novel metrics of speech rhythm. Their ability to characterise spontaneous emotions is demonstrated on the recent Audio/Visual Emotion Challenge Task on 3.6 hours of natural human affective conversational speech. Emotion is assessed for the four dimensions Arousal, Expectancy, Valence, and Power as binary classification tasks on the word level. We compare our new rhythmic feature types to the official 2 k brute-force acoustic baseline feature set on the Audio Sub-Challenge. In the results, the rhythmic features achieve a promising relative improvement of 16% for Valence, whereas the performance is more contrasted for the three others dimensions.\n",
    "Index Terms: speech rhythm, prosodic features, emotion recognition\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-113"
  },
  "wollmer12_interspeech": {
   "authors": [
    [
     "Martin",
     "Wöllmer"
    ],
    [
     "Florian",
     "Eyben"
    ],
    [
     "Björn",
     "Schuller"
    ],
    [
     "Gerhard",
     "Rigoll"
    ]
   ],
   "title": "Temporal and situational context modeling for improved dominance recognition in meetings",
   "original": "i12_0350",
   "page_count": 4,
   "order": 117,
   "p1": "350",
   "pn": "353",
   "abstract": [
    "We present and evaluate a novel approach towards automatically detecting a speaker's level of dominance in a meeting scenario. Since previous studies reveal that audio appears to be the most important modality for dominance recognition, we focus on the analysis of the speech signals recorded in multi-party meetings. Unlike recently published techniques which concentrate on frame-level hidden Markov modeling, we propose a recognition framework operating on segmental data and investigate context modeling on three different levels to explore possible performance gains. First, we apply a set of statistical functionals to capture large-scale feature-level context within a speech segment. Second, we consider bidirectional Long Short-Term Memory recurrent neural networks for long-range temporal context modeling between segments. Finally, we evaluate the benefit of situational context incorporation by simultaneously modeling speech of all meeting participants. Overall, our approach leads to a remarkable increase of recognition accuracy when compared to hidden Markov modeling.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-114"
  },
  "swerts12_interspeech": {
   "authors": [
    [
     "Marc",
     "Swerts"
    ],
    [
     "Kitty",
     "Leuverink"
    ],
    [
     "Madelene",
     "Munnik"
    ],
    [
     "Vera",
     "Nijveld"
    ]
   ],
   "title": "Audiovisual correlates of basic emotions in blind and sighted people",
   "original": "i12_0354",
   "page_count": 4,
   "order": 118,
   "p1": "354",
   "pn": "357",
   "abstract": [
    "This study is concerned with the expression and recognition of basic emotions in blind and sighted people. We collected audiovisual recordings from blind and sighted people who were asked to produce specific utterances in such a way that they would fit different emotional contexts (i.e., angry, happy, sad, scared). In a perception experiment, 75 sighted participants had to guess which emotion from a blind or sighted person was enacted in one of three conditions (audio-only, video-only, audiovisual). While emotions expressed by sighted people were comparatively more easy to judge in audiovisual and video-only presentations, it turned out to be the case that emotions of blind people were more often correctly classified in the audio-only condition. Interestingly, the general patterns in classification accuracy were remarkably similar across conditions, and across speaker type.\n",
    "Index Terms: blind and sighted people, audiovisual prosody, emotional expressions\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-115"
  },
  "cao12_interspeech": {
   "authors": [
    [
     "Houwei",
     "Cao"
    ],
    [
     "Ragini",
     "Verma"
    ],
    [
     "Ani",
     "Nenkova"
    ]
   ],
   "title": "Combining ranking and classification to improve emotion recognition in spontaneous speech",
   "original": "i12_0358",
   "page_count": 4,
   "order": 119,
   "p1": "358",
   "pn": "361",
   "abstract": [
    "We introduce a novel emotion recognition approach which integrates ranking models. The approach is speaker independent, yet it is designed to exploit information from utterances from the same speaker in the test set before making predictions. In this manner, it achieves much higher precision in identifying emotional utterances than a conventional SVM classifier. Furthermore we test several possibilities for combining conventional classification and predictions based on ranking. All combinations improve overall prediction accuracy. All experiments are performed on the FAU AIBO database which contains realistic spontaneous emotional speech. Our best combination system achieves 6.6% absolute improvement over the Interspeech 2009 emotion challenge baseline system on the 5-class classification tasks.\n",
    "Index Terms: emotion classification, ranking models, spontaneous speech\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-116"
  },
  "zhang12_interspeech": {
   "authors": [
    [
     "Zixing",
     "Zhang"
    ],
    [
     "Björn",
     "Schuller"
    ]
   ],
   "title": "Active learning by sparse instance tracking and classifier confidence in acoustic emotion recognition",
   "original": "i12_0362",
   "page_count": 4,
   "order": 120,
   "p1": "362",
   "pn": "365",
   "abstract": [
    "Data scarcity is an ever crucial problem in the field of acoustic emotion recognition. How to get the most informative data from a huge amount of data by least human work and at the same time to obtain the highest performance is quite important. In this paper, we propose and investigate two active learning strategies in acoustic emotion recognition: Based on sparse instances or based on classifier confidence scores. The first strategy focuses on the unbalanced problem of binary or multiple classes. And the later strategy pays more attention on clearing up the boundary confusion between different classes. Our experimental results show that by using active learning based on sparse instances or based on classifier confidence, the amount of transcribed data needed is significantly reduced and the unweigted accuracy boosts greatly as well.\n",
    "Index Terms: Active Learning, Acoustic Emotion Recognition, Sparse Instances, Confidence Scores\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-117"
  },
  "rozgic12_interspeech": {
   "authors": [
    [
     "Viktor",
     "Rozgić"
    ],
    [
     "Sankaranarayanan",
     "Ananthakrishnan"
    ],
    [
     "Shirin",
     "Saleem"
    ],
    [
     "Rohit",
     "Kumar"
    ],
    [
     "Aravind Namandi",
     "Vembu"
    ],
    [
     "Rohit",
     "Prasad"
    ]
   ],
   "title": "Emotion recognition using acoustic and lexical features",
   "original": "i12_0366",
   "page_count": 4,
   "order": 121,
   "p1": "366",
   "pn": "369",
   "abstract": [
    "In this paper we present an innovative approach for utterance-level emotion recognition by fusing acoustic features with lexical features extracted from automatic speech recognition (ASR) output. The acoustic features are generated by combining: (1) a novel set of features that are derived from segmental Mel Frequency Cepstral Coefficients (MFCC) scored against emotion-dependent Gaussian mixture models, and (2) statistical functionals of low-level feature descriptors such as intensity, fundamental frequency, jitter, shimmer, etc. These acoustic features are fused with two types of lexical features extracted from the ASR output: (1) presence/absence of word stems, and (2) bag-of-words sentiment categories. The combined feature set is used to train support vector machines (SVM) for emotion classification. We demonstrate the efficacy of our approach by performing four-way emotion recognition on the University of Southern California's Interactive Emotional Motion Capture (USC-IEMOCAP) corpus. Our experiments show that the fusion of acoustic and lexical features delivers an emotion recognition accuracy of 65.7%, outperforming the previously reported best results on this challenging dataset.\n",
    "Index Terms: emotion recognition, model-based acoustic features, lexical features\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-118"
  },
  "weninger12c_interspeech": {
   "authors": [
    [
     "Felix",
     "Weninger"
    ],
    [
     "Erik",
     "Marchi"
    ],
    [
     "Björn",
     "Schuller"
    ]
   ],
   "title": "Improving recognition of speaker states and traits by cumulative evidence: intoxication, sleepiness, age and gender",
   "original": "i12_1159",
   "page_count": 4,
   "order": 122,
   "p1": "1159",
   "pn": "1162",
   "abstract": [
    "We address the iterative refinement of classifier decisions for recognition of intoxication, sleepiness, age and gender from speech. The nature of these tasks as being emedium-termf or elong-termf, as opposed to short-term states such as emotion, makes it possible to collect cumulative evidence in the form of utterance level decisions; we show that by fusing these decisions along the time axis, more and more reliable decisions can be gained. In extensive test runs on three official INTERSPEECH Challenge corpora, we show that the average recall can be improved by up to 5%, 6%, 10% and 11% absolute by longer-term observation of speaker sleepiness, gender, intoxication, and age, respectively, compared to the accuracy of a decision from a single utterance.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-119"
  },
  "ding12_interspeech": {
   "authors": [
    [
     "Ni",
     "Ding"
    ],
    [
     "Julien",
     "Epps"
    ]
   ],
   "title": "Speaker clustering in emotion recognition",
   "original": "i12_1163",
   "page_count": 4,
   "order": 123,
   "p1": "1163",
   "pn": "1166",
   "abstract": [
    "Speaker variability is a known challenge for emotion recognition, however little work has been done on speaker similarity in terms of its contribution to the performance in the emotion classification task. In this paper, we investigate this topic, and find a clear link between speaker proximity and the recognition accuracy. Motivated by this result, emotion based speaker clustering is proposed as a new strategy for speaker adaptation. It involves using speaker proximity to cluster individual speakers' emotion models in the training set on a per-emotion basis, and adapting the test speaker's emotion from the closest cluster. A series of tests were conducted to explore how system performance varies with clustering method, the number of clusters and the amount of adapting data. Results on the LDC Emotion Prosody and FAU Aibo Corpora show that this method outperforms speaker bootstrap, both in terms of relieving computation load and producing higher accuracy.\n",
    "Index Terms: speaker clustering, emotion recognition, acoustic adaptation\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-120"
  },
  "kim12b_interspeech": {
   "authors": [
    [
     "Samuel",
     "Kim"
    ],
    [
     "Sree Harsha",
     "Yella"
    ],
    [
     "Fabio",
     "Valente"
    ]
   ],
   "title": "Automatic detection of conflict escalation in spoken conversations",
   "original": "i12_1167",
   "page_count": 4,
   "order": 124,
   "p1": "1167",
   "pn": "1170",
   "abstract": [
    "This paper investigates the automatic recognition of conflict escalations during spontaneous conversations. In our previous work, we studied if the level of conflict in a segment of conversation can be automatically inferred by means of prosodic and conversational features. This work investigates the possibility of automatically recognizing if the conflict is increasing, i.e., escalating, or not. The dataset used for the study consists of political debates where short clips are classified into escalation, de-escalation and constant labels. Results show a Weighted Accuracy (WA) equals to 69.6% and an Unweighted Accuracy (UA) equals to 50.7% thus revealing lower accuracies compared to the simple conflict detection task (WA 86.1%, UA 78.2%). While the task appears more difficult compared to conflict detection, results are significantly better than chance level showing the feasibility of this approach. Furthermore, the paper investigates the use of a speaker diarization algorithm to extract features in a completely automatic fashion highlighting some limitations of diarization system.\n",
    "Index Terms: Spoken Language Understanding, Conflicts, Paralinguistic, Spontaneous Conversation, Prosodic features, Turn-taking features\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-121"
  },
  "reichel12_interspeech": {
   "authors": [
    [
     "Uwe D.",
     "Reichel"
    ],
    [
     "Thomas",
     "Kisler"
    ]
   ],
   "title": "The entropy of intoxicated speech.lexical creativity and heavy tongues",
   "original": "i12_1171",
   "page_count": 4,
   "order": 125,
   "p1": "1171",
   "pn": "1174",
   "abstract": [
    "Spontaneous speech produced in sober and intoxicated conditions has been compared in information theoretic terms on the phoneme and word level to examine phonological and lexical aspects of intoxication. Word level entropy has been calculated to capture roughly the effect of alcohol on cognitive lexical creativity. Phoneme level entropy is intended to reflect heavy tongue influences on phoneme combinations. Moreover, mispronunciations have been investigated by relating canonical to realised pronunciation by means of mutual information and the Levenshtein distance. To account for the gradual nature of intoxication, examinations have been carried out regarding the offsets and slopes of linear functions mapping the blood alcohol concentration to the information theoretic variables. It turned out that male speakers compensate less for the alcohol-induced degradations with regard to lexical creativity and articulatory precision than female speakers. Furthermore, the pronunciation of male speakers generally deviates more from canonical forms.\n",
    "Index Terms: intoxication, entropy, transcription similarity\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-122"
  },
  "bone12_interspeech": {
   "authors": [
    [
     "Daniel",
     "Bone"
    ],
    [
     "Chi-Chun",
     "Lee"
    ],
    [
     "Shrikanth S.",
     "Narayanan"
    ]
   ],
   "title": "A robust unsupervised arousal rating framework using prosody with cross-corpora evaluation",
   "original": "i12_1175",
   "page_count": 4,
   "order": 126,
   "p1": "1175",
   "pn": "1178",
   "abstract": [
    "This paper presents an unsupervised method for producing a bounded rating of affective arousal from speech. One of the major challenges in such behavioral signal classification is the design of methods that generalize well across domains and datasets. We propose a framework that provides robustness across databases by: selecting coherent features based on empirical and theoretical evidence, fusing activation confidences from multiple features, and effectively weighting the soft-labels without knowing the true labels. Spearman's rank-correlation (and binary classification accuracy) on four arousal databases are: 0.62 (73%), 0.77 (86%), 0.70 (82%), and 0.65 (73%).\n",
    "Index Terms: arousal rating, activation, unsupervised, knowledge-based, inter-rater reliability, cross-corpora\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-123"
  },
  "busso12_interspeech": {
   "authors": [
    [
     "Carlos",
     "Busso"
    ],
    [
     "Tauhidur",
     "Rahman"
    ]
   ],
   "title": "Unveiling the acoustic properties that describe the valence dimension",
   "original": "i12_1179",
   "page_count": 4,
   "order": 127,
   "p1": "1179",
   "pn": "1182",
   "abstract": [
    "One of the main challenges in emotion recognition from speech is to discriminate emotions in the valence domain (positive versus negative). While acoustic features provide good characterization in the activation/arousal dimension (excited versus clam), they usually fail to discriminate between sentences with different valence (e.g., happy versus anger). This paper focuses exclusive on this dimension, which is key in many behavioral problems (e.g., depression). First, a regression analysis is conducted to identify the most informative features. Separate support vector regression (SVR) models are trained with various feature groups. The results reveal that spectral and F0 features produce the most accurate predictions of valence. Then, sentences with similar activation, but with different valence are carefully studied. The discriminative power in valence domain of individual features is studied with logistic regression analysis. This controlled experiment reveals differences between positive and negative emotions in the F0 distribution (e.g., positive skewness). The study also uncovers characteristic trends in the spectral domain.\n",
    "Index Terms: valence, emotion recognition, speech analysis, emotion representation\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-124"
  },
  "valente12_interspeech": {
   "authors": [
    [
     "Fabio",
     "Valente"
    ],
    [
     "Samuel",
     "Kim"
    ],
    [
     "Petr",
     "Motlicek"
    ]
   ],
   "title": "Annotation and recognition of personality traits in spoken conversations from the AMI meetings corpus",
   "original": "i12_1183",
   "page_count": 4,
   "order": 128,
   "p1": "1183",
   "pn": "1186",
   "abstract": [
    "Recognition of personality traits is a well studied problem in psychology while only recently it has been addressed by speech and language technology research. This paper describes annotation and experiments towards automatically inferring speakers personality traits in spontaneous conversations. In the first part, the work describes the annotation framework based on the Big-Five personality traits model (Extraversion, Agreeableness, Conscientiousness, Neuroticism and Openness) applied to 128 speakers from the AMI corpus. As the corpus contains rich annotations, those data can generalize previous studies based on enacted speech or dialogues. In the second part, the paper describes experiments based on various features including prosody, words n-gram, dialog acts and speech activity. Results reveal that high/low extraversion, consciousness and neuroticism traits can be automatically recognized with accuracy rate of 74.5%, 67.6% and 68.7%, respectively, while agreeableness and openness classification error rates are not statistically better than chance. Non-linguistic features (prosody, speech activity, overlaps and interruptions) outperform linguistic features (words n-gram and dialog acts) in this setup.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-125"
  },
  "lyu12_interspeech": {
   "authors": [
    [
     "Shao-ren",
     "Lyu"
    ]
   ],
   "title": "The effects of lexical tones and nasal coda /-n/ to sadness in Taiwan Hakka",
   "original": "i12_1187",
   "page_count": 4,
   "order": 129,
   "p1": "1187",
   "pn": "1190",
   "abstract": [
    "This paper concerns the relation between the emotion of sadness & lexical tone types, and the relation between the emotion of sadness & nasal coda /-n/ for non-Hakka speakers with Hakka stimuli. We try to probe what factors cause non-Hakka speakers to receive the expression of sadness in Hakka language successfully. The results showed that in both level tones and checked tone, the average F0 values is crucial to the perception of sadness; falling tone is predominant in the perception of sadness than rising tones. The contribution of nasal coda /-n/ to the sadness is also significant.\n",
    "Index Terms: sadness, lexical tones, nasal coda, Hailu Hakka\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-126"
  },
  "deng12b_interspeech": {
   "authors": [
    [
     "Jun",
     "Deng"
    ],
    [
     "Björn",
     "Schuller"
    ]
   ],
   "title": "Confidence measures in speech emotion recognition based on semi-supervised learning",
   "original": "i12_2226",
   "page_count": 4,
   "order": 130,
   "p1": "2226",
   "pn": "2229",
   "abstract": [
    "Even though the accuracy of predictions made by speech emotion recognition (SER) systems is increasing in precision, little is known about the confidence of the predictions. To shed some light on this, we propose a confidence measure for SER systems based on semi-supervised learning. During the semi-supervised learning procedure, five frequently used databases with manually created confidence labels are implemented to train classifiers. When the SER system predicts the label for an unknown test utterance, these classifiers serve as a reliability estimator for the utterance and output a series of confidence ratios that are combined into a single confidence measure. Our experimental results impressively show that the proposed confidence measure is effective in indicating how much we can trust the predicted emotion.\n",
    "Index Terms: speech emotion recognition, confidence measure, semi-supervised learning, cross-corpus\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-127"
  },
  "xia12_interspeech": {
   "authors": [
    [
     "Rui",
     "Xia"
    ],
    [
     "Yang",
     "Liu"
    ]
   ],
   "title": "Using i-vector space model for emotion recognition",
   "original": "i12_2230",
   "page_count": 4,
   "order": 131,
   "p1": "2230",
   "pn": "2233",
   "abstract": [
    "Using i-vector space features has been shown to be very successful in speaker and language identification. In this paper, we evaluate using the i-vector framework for emotion recognition from speech. Instead of using standard i-vector features, we propose to use concatenated emotion specific i-vector features. For each emotion category, a GMM supervector is generated via adaptation of the neural one from a big corpus. An i-vector feature vector is then obtained using each emotion specificGMMsupervector. The concatenation of these emotion dependent i-vector features is used as the feature vector in the SVM model for emotion classification. Our experimental results on acted and spontaneous data sets demonstrate that our proposed method outperforms baselines using either static features or dynamic features.\n",
    "Index Terms: emotion recognition, i-vector\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-128"
  },
  "obin12_interspeech": {
   "authors": [
    [
     "Nicolas",
     "Obin"
    ]
   ],
   "title": "Cries and whispers.classification of vocal effort in expressive speech",
   "original": "i12_2234",
   "page_count": 4,
   "order": 132,
   "p1": "2234",
   "pn": "2237",
   "abstract": [
    "The expansion of the video games industry raises innovative and challenging issues for speech technologies, e.g. the development of automatic content-based speech processing and speech recognition systems in the context of video games post-production. This paper presents a large-scale study on the classification of vocal effort in expressive speech for video games. Changes in vocal effort conduct to substantial modifications in the configuration of voice production mechanisms. In particular, registers of vocal effort affect especially voice quality which reflects qualitative modifications of the source excitation characteristics. This study introduces robust source characteristics to measure various types of voice quality (e.g., breathy, creaky, tense) for the classification of vocal effort into whispered, normal, and shouted speech. The system is evaluated in the real scenario of video games production with the complete speech recordings of a massive role-playing video game. The proposed features significantly improve the classification from 81.1% to 87% over conventional MFCCs. These advancements confirm the role of the source and voice quality for the description of changes in vocal effort.\n",
    "Index Terms: speech recognition, vocal effort, voice quality, glottal source, GMM-UBM/SVM\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-129"
  },
  "fewzee12_interspeech": {
   "authors": [
    [
     "Pouria",
     "Fewzee"
    ],
    [
     "Fakhri",
     "Karray"
    ]
   ],
   "title": "Emotional speech: a spectral analysis",
   "original": "i12_2238",
   "page_count": 4,
   "order": 133,
   "p1": "2238",
   "pn": "2241",
   "abstract": [
    "Feature extraction and dimensionality reduction may be found as the most imperative parts of the emotional speech recognition problem. In this work, we propose a new set of speech features, based on the distribution of energy in frequency domain. To investigate the applicability of the proposed model, we have set the first international audio/visual emotion challenge (AVEC 2011) as the benchmark. As for the modeling and dimensionality reduction, we have employed the lasso. It is shown how 15 explicit spectral energy features, as suggested in this work, can lead to a more accurate model than those of all the participants in the audio sub-challenge. This is while this number of features is less than ten percent of the smallest set of features participated in the challenge. Centre for Patter Analysis and Machine Intelligence,\n",
    "Index Terms: emotional speech recognition, feature extraction, dimensionality reduction\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-130"
  },
  "rosenberg12_interspeech": {
   "authors": [
    [
     "Andrew",
     "Rosenberg"
    ]
   ],
   "title": "Classifying skewed data: importance weighting to optimize average recall",
   "original": "i12_2242",
   "page_count": 4,
   "order": 134,
   "p1": "2242",
   "pn": "2245",
   "abstract": [
    "Promoted in part by its use in the Interspeech Challenges in 2009-2012, Average Recall has emerged as an attractive evaluation measure of classifier performance where the data has a skewed class distribution. In this paper, we show that importance weighting can be used to directly optimize Average Recall. We compare this approach to sampling techniques that have been previously used to classify skewed data. We demonstrate the use of this approach on the Interspeech 2009 Emotion Challenge tasks, and prosodic analysis tasks.\n",
    "Index Terms: skewed class distributions, prosody, prosodic analysis, emotion classification.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-131"
  },
  "oertel12_interspeech": {
   "authors": [
    [
     "Catharine",
     "Oertel"
    ],
    [
     "Marcin",
     "Włodarczak"
    ],
    [
     "Jens",
     "Edlund"
    ],
    [
     "Petra",
     "Wagner"
    ],
    [
     "Joakim",
     "Gustafson"
    ]
   ],
   "title": "Gaze patterns in turn-taking",
   "original": "i12_2246",
   "page_count": 4,
   "order": 135,
   "p1": "2246",
   "pn": "2249",
   "abstract": [
    "This paper investigates gaze patterns in turn-taking. We focus on the difference in gaze patterns in speaker changes resulting in gaps and overlaps. We also investigate gaze patters around backchannels and around silences not involving speaker changes.\n",
    "Index Terms: gaze, turn-taking, dialogue, inter-speaker coordination\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-132"
  },
  "fecher12_interspeech": {
   "authors": [
    [
     "Natalie",
     "Fecher"
    ]
   ],
   "title": "The \"audio-visual face cover corpus\": investigations into audio-visual speech and speaker recognition when the speaker's face is occluded by facewear",
   "original": "i12_2250",
   "page_count": 4,
   "order": 136,
   "p1": "2250",
   "pn": "2253",
   "abstract": [
    "The Audio-Visual Face Cover Corpus consists of high-quality audio and video recordings of 10 native British English speakers wearing different types of 'facewear'. Speakers read aloud a set of 64 /C1VC2/ syllables embedded in a carrier phrase. 18 English consonants occurred twice each in onset and coda positions. Speakers recited the list 1+8 times, i.e. once in control condition (no facewear) and eight times while wearing a forensically-relevant face covering. Audio recordings were made by simultaneously capturing the speech via a headband microphone and two shotgun microphones placed facing and behind the speaker. Footage of the subject's head and shoulders was filmed from two camera angles, frontal and half-profile. In total, 6,120 utterances were recorded per device. This paper aims to specify the database design, to introduce forensic-phonetic research utilising the data, and to demonstrate the corpus's potential applications in related fields of study and in casework conducted by forensic speech scientists.\n",
    "Index Terms: speech database, audio-visual, forensic speech science, facewear, disguise, acoustic phonetics, perception\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-133"
  },
  "can12_interspeech": {
   "authors": [
    [
     "Doğan",
     "Can"
    ],
    [
     "Panayiotis G.",
     "Georgiou"
    ],
    [
     "David C.",
     "Atkins"
    ],
    [
     "Shrikanth S.",
     "Narayanan"
    ]
   ],
   "title": "A case study: detecting counselor reflections in psychotherapy for addictions using linguistic features",
   "original": "i12_2254",
   "page_count": 4,
   "order": 137,
   "p1": "2254",
   "pn": "2257",
   "abstract": [
    "Motivational Interviewing (MI) is a goal-oriented psychotherapy focused on addictions, which helps clients (i.e., patients) explore and resolve their ambivalence about the problem at hand. Measuring the counselor's proficiency with MI has typically been assessed via behavioral coding - a time consuming, low technology approach. This paper examines a computational approach to assessing the quality of MI. Specifically, we focus on a particular aspect of the counselor behavior - reflections - believed to be a critical indicator of MI therapy quality. We automatically tag reflection instances in a maximum entropy Markov modeling framework using several lexical features with rich linguistic and contextual information obtained from the session transcripts.\n",
    "Index Terms: dialog act tagging, behavioral signal processing, motivational interviewing skills code\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-134"
  },
  "leon12_interspeech": {
   "authors": [
    [
     "Phillip L. De",
     "Leon"
    ],
    [
     "Bryan",
     "Stewart"
    ],
    [
     "Junichi",
     "Yamagishi"
    ]
   ],
   "title": "Synthetic speech discrimination using pitch pattern statistics derived from image analysis",
   "original": "i12_0370",
   "page_count": 4,
   "order": 138,
   "p1": "370",
   "pn": "373",
   "abstract": [
    "In this paper, we extend the work by Ogihara, et al. to discriminate between human and synthetic speech using features based on pitch patterns. As previously demonstrated, significant differences in pitch patterns between human and synthetic speech can be leveraged to classify speech as being human or synthetic in origin. We propose using mean pitch stability, mean pitch stability range, and jitter as features extracted after image analysis of pitch patterns. We have observed that for synthetic speech, these features lie in a small and distinct space as compared to human speech and have modeled them with a multivariate Gaussian distribution. Our classifier is trained using synthetic speech collected from the 2008 and 2011 Blizzard Challenge along with Festival pre-built voices and human speech from the NIST2002 corpus. We evaluate the classifier on a much larger corpus than previously studied using human speech from the Switchboard corpus, synthetic speech from the Resource Management corpus, and synthetic speech generated from Festival trained on the Wall Street Journal corpus. Results show 98% accuracy in correctly classifying human speech and 96% accuracy in correctly classifying synthetic speech.\n",
    "Index Terms: Speaker recognition, Speech synthesis, Security\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-135"
  },
  "wen12_interspeech": {
   "authors": [
    [
     "Zhengqi",
     "Wen"
    ],
    [
     "Hideki",
     "Kawahara"
    ],
    [
     "Jianhua",
     "Tao"
    ]
   ],
   "title": "Pitch-scaled analysis based residual reconstruction for speech analysis and synthesis",
   "original": "i12_0374",
   "page_count": 4,
   "order": 139,
   "p1": "374",
   "pn": "377",
   "abstract": [
    "The typical problem in LPC-like vocoder is buzzing sound which is mainly due to the simple pulse train or noise excitation model. One way to improve it is to reconstruct the residual obtained from inverse filtering. So a new parametric representation of speech based on pitch-scaled analysis is proposed in this paper. Pitch-scaled analysis is used to extract the periodic spectrum of residual with half pitch period length. Then these periodic spectrums are decorrelated by principal component analysis (PCA) to reduce their dimension. Aperiodic measure is defined as the harmonic-to-noise ratio in the frequency domain where voicing cut-off frequency (VCO) is used to control the smoothness of aperiodicity. Periodic spectrum and aperiodic measure together with F0 are indicated as excitation parameters in the proposed LPC vocoder. Experimental results show that this proposed vocoder can get a mean opinion score (MOS) of 4.1 for a female voice before dimensionality reduction and keep the high-quality property after parameter compression.\n",
    "Index Terms: speech parametric representation, pitch-scaled analysis, voicing cut-off frequency, principal component analysis\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-136"
  },
  "huang12b_interspeech": {
   "authors": [
    [
     "Feng",
     "Huang"
    ],
    [
     "Tan",
     "Lee"
    ]
   ],
   "title": "Robust pitch estimation using <i>l</i><sub>1</sub>-regularized maximum likelihood estimation",
   "original": "i12_0378",
   "page_count": 4,
   "order": 140,
   "p1": "378",
   "pn": "381",
   "abstract": [
    "This paper presents a new method of robust pitch estimation using sparsity-based estimation techniques. The method is developed based on sparse representation of a temporal-spectral pitch feature. The robust pitch feature is obtained by accumulating spectral peaks over consecutive frames. It is expressed as a sparse linear combination of an over-complete set of peak spectrum exemplars. The probability distribution of the noise is assumed to be Gaussian with non-zero mean. The weights of the linear combination are estimated by maximizing the likelihood of the feature under sparsity constraint. The sparsity constraint is incorporated as an l1 regularization term. From the estimated weights, the major constituent exemplars are identified and the fundamental frequency is determined. Experimental results show that, with this method, pitch estimation accuracy is significantly improved, particularly at low signal-to-noise ratios.\n",
    "Index Terms: Robust pitch estimation, speech sparsity, l1 regularization, peak spectrum\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-137"
  },
  "degottex12_interspeech": {
   "authors": [
    [
     "Gilles",
     "Degottex"
    ],
    [
     "Yannis",
     "Stylianou"
    ]
   ],
   "title": "A full-band adaptive harmonic representation of speech",
   "original": "i12_0382",
   "page_count": 4,
   "order": 141,
   "p1": "382",
   "pn": "385",
   "abstract": [
    "In this paper we present a full-band Adaptive Harmonic Model (aHM) that is able to accurately reconstruct stationary and non stationary parts of speech. The model does not require any voiced/unvoiced decision, neither an accurate estimation of the pitch contour. Its robustness is based on a previously suggested adaptive Quasi-Harmonic model (aQHM) which provides a mechanism for frequency correction and adaptivity of its basis functions to the characteristics of the input signal. The suggested method overcomes limitations of the initial method based on aQHM in detecting frequency tracks over time, especially at mid and high frequencies, by employing a bandlimited iterative procedure for the re-estimation of the fundamental frequency. Listening tests show that reconstructed speech using aHM is mainly indistinguishable from the original signal, outperforming standard sinusoidal models (SM) and the aQHM-based method, while it uses less parameters for the reconstruction than SM.\n",
    "Index Terms: Sinusoidal model, quasi-harmonic model, nonstationary basis, speech analysis.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-138"
  },
  "kawahara12_interspeech": {
   "authors": [
    [
     "Hideki",
     "Kawahara"
    ],
    [
     "Masanori",
     "Morise"
    ],
    [
     "Ryuichi",
     "Nisimura"
    ],
    [
     "Toshio",
     "Irino"
    ]
   ],
   "title": "Deviation measure of waveform symmetry and its application to high-speed and temporally-fine <i>F</i><sub>0</sub> extraction for vocal sound texture manipulation",
   "original": "i12_0386",
   "page_count": 4,
   "order": 142,
   "p1": "386",
   "pn": "389",
   "abstract": [
    "A simple and high-speed F0 extractor with high temporal resolution is proposed based on a waveform symmetry measure. Strictly speaking, it is not an F0 extractor. Instead, it is a detector of the lowest prominent sinusoidal component with a salience measure. It can make use of an F0 refinement procedure, when the signal under investigation is a sum of harmonic sinusoidal components. The refinement procedure is based on a stable representation of instantaneous frequency of periodic signals. Application of the proposed algorithm revealed that rapid temporal modulations in both F0 trajectory and spectral envelope exist typically in expressive voices such as lively singing performance. Manipulation of these temporal fine structures (texture) effectively modified perceptual expressiveness, while somewhat preserving perceptual vocal effort and register.\n",
    "Index Terms: speech analysis, speech synthesis, expressive speech, singing voices\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-139"
  },
  "yoshizato12_interspeech": {
   "authors": [
    [
     "Kota",
     "Yoshizato"
    ],
    [
     "Hirokazu",
     "Kameoka"
    ],
    [
     "Daisuke",
     "Saito"
    ],
    [
     "Shigeki",
     "Sagayama"
    ]
   ],
   "title": "Hidden Markov convolutive mixture model for pitch contour analysis of speech",
   "original": "i12_0390",
   "page_count": 4,
   "order": 143,
   "p1": "390",
   "pn": "393",
   "abstract": [
    "This paper proposes a statistical model of speech F0 contours, which is based on the discrete-time version of the Fujisaki model. Our motivation for formulating this model is incorporating F0 contours into various statistical speech processing problems. In this paper, we describe the formulation of the model and quantitatively evaluates the performance of the model through Fujisaki-model parameter estimations from real speech F0 contours. Compared with another speech F0 model we have proposed, the present model prefer fitting observed F0 contours because the previous model is based on a squared error criterion in the Fujisaki-model commands domain and the present model is in the F0 contours domain.\n",
    "Index Terms: speech F0 contours, statistical model, Fujisaki model, hidden Markov model, EM algorithm\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-140"
  },
  "sjerps12_interspeech": {
   "authors": [
    [
     "Matthias",
     "Sjerps"
    ],
    [
     "James M.",
     "McQueen"
    ],
    [
     "Holger",
     "Mitterer"
    ]
   ],
   "title": "Extrinsic normalization for vocal tracts depends on the signal, not on attention",
   "original": "i12_0394",
   "page_count": 4,
   "order": 144,
   "p1": "394",
   "pn": "397",
   "abstract": [
    "When perceiving vowels, listeners adjust to speaker-specific vocal-tract characteristics (such as F1) through \"extrinsic vowel normalization\". This effect is observed as a shift in the location of categorization boundaries of vowel continua. Similar effects have been found with nonspeech. Non-speech materials, however, have consistently led to smaller effect-sizes, perhaps because a of lack of attention to non-speech. The present study investigated this possibility. Non-speech materials that had previously been shown to elicit reduced normalization effects were tested again, with the addition of an attention manipulation. The results show that increased attention does not lead to increased normalization effects, suggesting that vowel normalization is mainly determined by bottom-up signal characteristics.\n",
    "Index Terms: vowel normalization, speech perception, attention\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-141"
  },
  "scharenborg12_interspeech": {
   "authors": [
    [
     "Odette",
     "Scharenborg"
    ],
    [
     "Esther",
     "Janse"
    ],
    [
     "Andrea",
     "Weber"
    ]
   ],
   "title": "Perceptual learning of /f/-/s/ by older listeners",
   "original": "i12_0398",
   "page_count": 4,
   "order": 145,
   "p1": "398",
   "pn": "401",
   "abstract": [
    "Young listeners can quickly modify their interpretation of a speech sound when a talker produces the sound ambiguously. Young Dutch listeners rely mainly on the higher frequencies to distinguish between /f/ and /s/, but these higher frequencies are particularly vulnerable to age-related hearing loss. We therefore tested whether older Dutch listeners can show perceptual retuning given an ambiguous pronunciation in between /f/ and /s/. Results of a lexically-guided perceptual learning experiment showed that older Dutch listeners are still able to learn non-standard pronunciations of /f/ and /s/. Possibly, the older listeners have learned to rely on other acoustic cues, such as formant transitions, to distinguish between /f/ and /s/. However, the size and duration of the perceptual effect is influenced by hearing loss, with listeners with poorer hearing showing a smaller and a shorter-lived learning effect.\n",
    "Index Terms: perceptual learning, older listeners, aging, hearing loss, human word recognition.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-142"
  },
  "hatano12_interspeech": {
   "authors": [
    [
     "Hiroaki",
     "Hatano"
    ],
    [
     "Tatsuya",
     "Kitamura"
    ],
    [
     "Hironori",
     "Takemoto"
    ],
    [
     "Parham",
     "Mokhtari"
    ],
    [
     "Kiyoshi",
     "Honda"
    ],
    [
     "Shinobu",
     "Masaki"
    ]
   ],
   "title": "Correlation between vocal tract length, body height, formant frequencies, and pitch frequency for the five Japanese vowels uttered by fifteen male speakers",
   "original": "i12_0402",
   "page_count": 4,
   "order": 146,
   "p1": "402",
   "pn": "405",
   "abstract": [
    "We conducted quantitative analyses of a magnetic resonance imaging (MRI) database to examine the correlation between physical measures (vocal tract length and body height) and acoustic parameters (pitch and formant frequencies) of vowels. The vocal tract length was measured from MRI data for the five Japanese vowels produced by fifteen male Japanese speakers between the ages of 24 and 55. The voice features were computed from vowel sounds recorded during scan. The vocal tract length showed a weak positive correlation with the speakers' age (correlation coefficient r = 0.51) but not with the speaker body height (r = 0.08). There was only weaker correlations between the vocal tract length and the first four formant frequencies except that F1 and F2 of the vowel /e/ show negative correlations with the vocal tract length (F1: r = -0.65, F2: r = -0.56). The result suggests that the vocal tract length is one of the dominant factors causing individual differences in the formant frequencies for the vowel /e/, produced by not forming a strong constriction. Furthermore, the pitch frequency was negatively correlated with the body height (r = -0.61).\n",
    "Index Terms: magnetic resonance imaging (MRI), vocal tract length, body height, formant frequencies, pitch frequency\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-143"
  },
  "jagbandhu12_interspeech": {
   "authors": [
    [
     "J.",
     "Jagbandhu"
    ],
    [
     "K. S.",
     "Nataraj"
    ],
    [
     "Prem C.",
     "Pandey"
    ]
   ],
   "title": "Detection of transition segments in VCV utterances for estimation of the place of closure of oral stops for speech training",
   "original": "i12_0406",
   "page_count": 4,
   "order": 147,
   "p1": "406",
   "pn": "409",
   "abstract": [
    "Speech-training aids providing a visual feedback of articulatory efforts can be used for improving articulation by the hearing-impaired children. LPC-based estimation of vocal tract shape works satisfactorily for vowels but fails during stop closure. The vocal tract shape during the stop closures of vowel-consonant-vowel (VCV) utterances can be estimated by bivariate surface modeling of the vocal tract area function during the transition segments preceding and following the stop closure. The accuracy of the estimated shape during the closure depends on the detection of the transitions. A technique for detecting the VC and CV transitions in VCV utterances based on a measure of the rate of change of vocal tract area function is presented. The automatically marked start and end points of transitions showed a good match with the manually marked ones and resulted in a consistent estimation of the place of closure of velar, alveolar, and bilabial stops.\n",
    "Index Terms: speech training aid, vocal tract shape, transition segment detection\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-144"
  },
  "dubois12_interspeech": {
   "authors": [
    [
     "Cyril",
     "Dubois"
    ],
    [
     "Rudolph",
     "Sock"
    ]
   ],
   "title": "Audiovisual discrimination of CV syllables: a simultaneous fMRI-EEG study",
   "original": "i12_0410",
   "page_count": 4,
   "order": 148,
   "p1": "410",
   "pn": "413",
   "abstract": [
    "We carried out a simultaneous fMRI-EEG experiment based on discriminating syllabic minimal pairs involving three phonological contrasts characterized by different degrees of visual distinctiveness (vocalic labialization, consonantal place of articulation or voicing). Audiovisual CV syllable pairs were presented either with a static facial configuration or with a dynamic display of articulatory movements. In the sounddisturbed MRI environment, the significant improvement of syllabic discrimination achieved in the dynamic audiovisual modality, compared to the static audiovisual modality was associated with activation of the occipito-temporal cortex (MT + V5) bilaterally, and of the left premotor cortex. MT + V5 was activated in response to facial movements independently of their relation to speech, the latter was specifically activated by phonological discrimination. Significant ERP's to syllabic discrimination were recorded around 150 and 250 ms. Our results provide arguments for the involvement of the speech motor cortex in phonological discrimination, and suggest a multimodal representation of speech units.\n",
    "Index Terms: Visual distinctiveness, motor cortex, MT+ V5, multimodal representation\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-145"
  },
  "kertkeidkachorn12_interspeech": {
   "authors": [
    [
     "Natthawut",
     "Kertkeidkachorn"
    ],
    [
     "Surapol",
     "Vorapatratorn"
    ],
    [
     "Sirinart",
     "Tangruamsub"
    ],
    [
     "Proadpran",
     "Punyabukkana"
    ],
    [
     "Atiwong",
     "Suchato"
    ]
   ],
   "title": "Contribution of spectral shapes to tone perception",
   "original": "i12_0414",
   "page_count": 4,
   "order": 149,
   "p1": "414",
   "pn": "417",
   "abstract": [
    "Tones in tonal languages are defined based on the characteristics of their fundamental frequencies. However, it has been shown that fundamental frequencies alone did not lead to good tone identification, both by human and machines. This paper reports a study based on tone perception experiments in which participants identified Thai tones from recorded mono-syllabic, bi-syllabic, and tri-syllabic stimuli as well as their modified counterparts. Stimuli were modified in various aspects so that effects of fundamental frequencies, syllable energy envelopes, and spectral shapes to tone perception can be observed. Results indicate that the spectral shape, especially in the vicinity of the first formant, contributes significantly to the participants' abilities to correctly identify tones and the contribution is more prominent with more syllables in the stimuli.\n",
    "Index Terms: tone perception, Thai tones, formant frequencies, fundamental frequencies, pitches\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-146"
  },
  "tantibundhit12_interspeech": {
   "authors": [
    [
     "Charturong",
     "Tantibundhit"
    ],
    [
     "Chutamanee",
     "Onsuwan"
    ],
    [
     "P.",
     "Phienphanich"
    ],
    [
     "Chai",
     "Wutiwiwatchai"
    ]
   ],
   "title": "Methodological issues in assessing perceptual representation of consonant sounds in Thai",
   "original": "i12_0418",
   "page_count": 4,
   "order": 150,
   "p1": "418",
   "pn": "421",
   "abstract": [
    "This work is an attempt to evaluate different experimental methods, ABX vs. AXB, and the use of reaction time (RT) measurement in assessing perceptual sensitivity to phonemic similarity based on perceptual representation of Thai initial consonants. Thirty phoneme pairs are selected to represent varying degrees of similarity: highly similar, moderately similar, and clearly distinct. All the phoneme pairs are presented in noise in ABX and AXB tasks to twenty-two normal hearing Thai listeners. Order of the two tasks is counter-balanced across listener groups. Percent correct responses (p(C)), RTs, and preference rating are collected. The findings show that, p(C) is significantly higher in AXB than ABX despite no significant difference in RT values. In both ABX and AXB, listeners' p(C) across 3 levels of similarity varies significantly with the highest score in the clearly distinct group, and lowest score in the highly similar group. RT values across the 3 levels follow similar patterns but are not always statistically significant. ABX and AXB tasks could systematically be used to assess perceptual representation of speech sounds, with AXB eliciting higher p(C) and preference rating. It is suggested that some irregular patterns found in one part of the RT data may reflect some perceptual sensitivity pertaining to perceptual phoneme-cluster boundary.\n",
    "Index Terms: Thai, initial consonant, perceptual similarity/distance, AXB, ABX, reaction time\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-147"
  },
  "meyer12_interspeech": {
   "authors": [
    [
     "Julien",
     "Meyer"
    ]
   ],
   "title": "Pitch and phonological perception of tone in the Suruí language of Rondônia (Brazil): identification task of LHL and LHH tonal patterns",
   "original": "i12_0422",
   "page_count": 4,
   "order": 151,
   "p1": "422",
   "pn": "425",
   "abstract": [
    "The present study explored the relation between pitch and phonological perception of tone in the Surui language of Rondonia (Brazil), given that pitch realization of tone in Surui is a complex phenomenon. F0 values and pitch contour of the vowel nucleus were found to influence the perception of tone in this language of the Monde family, but these effects were found to be dominated by the influence of the pitch contour of the preceding syllable. These perceptual results, completed by an analysis on production, contribute to indicate the existence of a Mid tone in Surui, allophone of the High tone, which would be the result of a regressive downstep.\n",
    "Index Terms: Tone perception, Suruí language, Mondé languages, AXB test, downstep\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-148"
  },
  "cao12b_interspeech": {
   "authors": [
    [
     "Rui",
     "Cao"
    ],
    [
     "Ratree",
     "Wayland"
    ],
    [
     "Edith",
     "Kaan"
    ]
   ],
   "title": "The role of creaky voice in Mandarin tone 2 and tone 3 perception",
   "original": "i12_0426",
   "page_count": 4,
   "order": 152,
   "p1": "426",
   "pn": "429",
   "abstract": [
    "This study investigated the role of creaky voice in the categorial perception of Mandarin Tone 2 and Tone 3 among native English and native Mandarin listeners. Results showed that the presence of creaky voice affected the perception of Mandarin Tone 3. Both native and nonnative listeners reported hearing Tone 3 at a later F0 turning point with the presence of creaky voice.\n",
    "Index Terms: speech perception, Mandarin tones, creaky voice\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-149"
  },
  "tyler12_interspeech": {
   "authors": [
    [
     "Michael D.",
     "Tyler"
    ],
    [
     "Mona M.",
     "Faris"
    ]
   ],
   "title": "Can litheners retune native categories acroth a thoneme boundary?",
   "original": "i12_0430",
   "page_count": 4,
   "order": 153,
   "p1": "430",
   "pn": "433",
   "abstract": [
    "Lexically guided perceptual retuning studies have demonstrated that listeners use their knowledge of phonemes in words to accommodate to artificially generated sounds that are halfway between two phonemes. However, it is unknown whether listeners accommodate in the same way when words are pronounced with an incorrect native phoneme (e.g., flower pronounced as “thlower”). Monolingual Australian-English listeners completed one of two exposure phases where the /f/ or /s/ in words was pronounced as /Θ/ (“th”), followed by a visual lexical decision task with cross-modal priming. If training is effective, identity-priming should be observed when a /Θ/-bearing auditory prime (e.g., thoil) precedes a trainingcongruent matched visual target (e.g., foil or soil). Priming was observed for participants in the /f/-training, but not the /s/-training condition. A second experiment with intact /f/- or /s/-primes confirmed stimulus validity by showing an identity priming pattern. We conclude that lexically-guided perceptual retuning may be possible across a category boundary, but the native phonological system and/or acoustic similarity may impose limits on which native phonemes can be substituted effectively.\n",
    "Index Terms: speech perception, lexically guided retuning, cross-modal priming, spoken word recognition.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-150"
  },
  "morley12_interspeech": {
   "authors": [
    [
     "Eric",
     "Morley"
    ],
    [
     "Esther",
     "Klabbers"
    ],
    [
     "Jan P. H. van",
     "Santen"
    ],
    [
     "Alexander",
     "Kain"
    ],
    [
     "Seyed Hamidreza",
     "Mohammadi"
    ]
   ],
   "title": "Synthetic <i>F</i><sub>0</sub> can effectively convey speaker ID in delexicalized speech",
   "original": "i12_0434",
   "page_count": 4,
   "order": 154,
   "p1": "434",
   "pn": "437",
   "abstract": [
    "We investigate the extent to which F0 can convey speaker ID in the absence of spectral, segmental, and durational information. We propose two methods of F0 synthesis based on the Linear Alignment Model (LAM, van Santen 2000): one parametric, the other corpus-based. Through a perceptual experiment, we show that F0 alone is able to convey information about speaker ID. We find that F0 synthesized with either LAMbased method conveys speaker ID almost as effectively as natural F0.\n",
    "Index Terms: F0, prosody, speech synthesis, speaker identity, recombinant synthesis\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-151"
  },
  "baumann12_interspeech": {
   "authors": [
    [
     "Timo",
     "Baumann"
    ],
    [
     "David",
     "Schlangen"
    ]
   ],
   "title": "Evaluating prosodic processing for incremental speech synthesis",
   "original": "i12_0438",
   "page_count": 4,
   "order": 155,
   "p1": "438",
   "pn": "441",
   "abstract": [
    "Incremental speech synthesis (iSS) accepts input and produces output in consecutive chunks that only together result in a full utterance. Systems that use iSS thus have the ability to adapt their utterances while they are ongoing. Having available less than the full utterance to plan the acoustic realisation has downsides, however, as global optimisation is not possible anymore. In this paper we present a strategy for incrementalizing the symbolic pre-processing component of speech synthesis and assess the influence of a reduction in \"lookahead\", i. e. in knowledge about the rest of the utterance, on prosodic quality. We found that high quality incremental output can be achieved even with a lookahead of slightly less than one phrase, allowing for timely system reaction.\n",
    "Index Terms: speech synthesis, spoken dialogue systems, incrementality, prosody\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-152"
  },
  "iwata12_interspeech": {
   "authors": [
    [
     "Kazuhiko",
     "Iwata"
    ],
    [
     "Tetsunori",
     "Kobayashi"
    ]
   ],
   "title": "Expressing speaker's intentions through sentence-final intonations for Japanese conversational speech synthesis",
   "original": "i12_0442",
   "page_count": 4,
   "order": 156,
   "p1": "442",
   "pn": "445",
   "abstract": [
    "In this study, we investigated speaker's intentions that the listeners perceive through subtly different sentence-final intonations. Approximately 2,000 sentence utterances were recorded and the fundamental frequency (F0) contours at the last vowel of those sentences were classified through one of the standard clustering algorithms. There found various F0 contours, namely, not only simple rising and falling intonations but also rise-fall and fall-rise intonations. In order to reveal the relationship between the intonation and the intentions, 10 representative contours were selected on the basis of the results of the clustering. Using the selected contours, a subjective evaluation was conducted. Six Japanese sentences that could have different meanings according to the sentence-final intonations were synthesized and the F0 contour at the last vowel of each sentence was replaced with the contours. The results of the evaluation by nine listeners showed that, for example, a certain falling intonation could express the intention of the econvictionf and another one that slightly differ in the shape could convey edoubt.f It was found that the subtle difference in the sentence-final F0 shape conveyed various nuances and connotations.\n",
    "Index Terms: speech synthesis, sentence-final intona- tion, speaker's intention\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-153"
  },
  "parlikar12_interspeech": {
   "authors": [
    [
     "Alok",
     "Parlikar"
    ],
    [
     "Alan W.",
     "Black"
    ]
   ],
   "title": "Modeling pause-duration for style-specific speech synthesis",
   "original": "i12_0446",
   "page_count": 4,
   "order": 157,
   "p1": "446",
   "pn": "449",
   "abstract": [
    "A major contribution to speaking style comes from both the location of phrase breaks in an utterance, as well as the duration of these breaks. This paper is about modeling the duration of style specific breaks. We look at six styles of speech here. We present analysis that shows that these styles differ in the duration of pauses in natural speech. We have built CART models to predict the pause duration in these corpora and have integrated them into the Festival speech synthesis system. Our objective results show that if we have sufficient training data, we can build style specific models. Our subjective tests show that people can perceive the difference between different models and that they prefer style specific models over simple pause duration models.\n",
    "Index Terms: Speech Synthesis, Style-specific Pause Duration, Phrasing\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-154"
  },
  "gruber12_interspeech": {
   "authors": [
    [
     "Martin",
     "Gruber"
    ]
   ],
   "title": "Enumerating differences between various communicative functions for purposes of Czech expressive speech synthesis in limited domain",
   "original": "i12_0450",
   "page_count": 4,
   "order": 158,
   "p1": "450",
   "pn": "453",
   "abstract": [
    "This paper deals with determination of a penalty matrix that should represent differences between various communicative functions. These are supposed to describe expressivity that can occur in expressive speech and were designed to fit a limited domain of conversations between seniors and a computer on a given topic. The penalty matrix is assumed to increase a rate of the expressivity perception in synthetic speech produced by unit selection method. It should reflect both acoustic differences and differences based on human perception of expressivity.\n",
    "Index Terms: expressive speech synthesis, unit selection, target costs, communicative functions\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-155"
  },
  "norrenbrock12_interspeech": {
   "authors": [
    [
     "Christoph R.",
     "Norrenbrock"
    ],
    [
     "Florian",
     "Hinterleitner"
    ],
    [
     "Ulrich",
     "Heute"
    ],
    [
     "Sebastian",
     "Möller"
    ]
   ],
   "title": "Quality analysis of macroprosodic <i>F</i><sub>0</sub> dynamics in text-to-speech signals",
   "original": "i12_0454",
   "page_count": 4,
   "order": 159,
   "p1": "454",
   "pn": "457",
   "abstract": [
    "We present a study on the relation between fundamental frequency (F0) and its perceptual effect in the context of text-to-speech (TTS) synthesis. Features that essentially capture the intonational (macro-prosodic) properties of spoken speech are introduced and analysed with regard to the following questions: (i) How does the prosodic variation of TTS signals differ from natural speech? (ii) Is there a functional relationship between the prosodic variation of TTS signals and its perceived quality? In answering these questions we present novel approaches for the construction of non-intrusive quality estimators. The results reveal a substantial degree of systematic influence of prosodic variation on TTS quality.\n",
    "Index Terms: Speech quality, instrumental quality assessment, text-to-speech (TTS), prosody.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-156"
  },
  "hashimoto12_interspeech": {
   "authors": [
    [
     "Hiroya",
     "Hashimoto"
    ],
    [
     "Keikichi",
     "Hirose"
    ],
    [
     "Nobuaki",
     "Minematsu"
    ]
   ],
   "title": "Improved automatic extraction of generation process model commands and its use for generating fundamental frequency contours for training HMM-based speech synthesis",
   "original": "i12_0458",
   "page_count": 4,
   "order": 160,
   "p1": "458",
   "pn": "461",
   "abstract": [
    "Generation process model of fundamental frequency (F0) contours can well represent F0 movements of speech keeping a clear relation with back-grounding linguistic information of utterances. Therefore, by using the model, improvement of HMM-based speech synthesis is expected. One of major problems preventing the use of the model is that the performance of automatic extraction of the model parameters from observed F0 contours is still rather limited. A new method of automatic extraction was developed. Its algorithm is inspired from how humans do, and extracts phrase components first, while conventional methods extract accent component first. Also the method uses linguistic information of texts, which is the same as that used in HMM-based speech synthesis. A significant improvement of extraction is realized. Using the method, the model parameters are extracted for the speech corpus of HMM training, and F0 contours generated by the model are used for the HMM training instead of the original F0 contours. Listening experiment of synthetic speech indicates improvements in speech quality.\n",
    "Index Terms: F0 contour, generation process model, speech synthesis, parameter extraction\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-157"
  },
  "koriyama12_interspeech": {
   "authors": [
    [
     "Tomoki",
     "Koriyama"
    ],
    [
     "Takashi",
     "Nose"
    ],
    [
     "Takao",
     "Kobayashi"
    ]
   ],
   "title": "Discontinuous observation HMM for prosodic-event-based <i>F</i><sub>0</sub> generation",
   "original": "i12_0462",
   "page_count": 4,
   "order": 161,
   "p1": "462",
   "pn": "465",
   "abstract": [
    "This paper examines F0 modeling and generation techniques for spontaneous speech synthesis. In the previous study, we proposed a prosodic-unit HMM where the synthesis unit is defined as a segment between two prosodic events represented by a ToBI label framework. To take the advantage of the prosodic-unit HMM, continuous F0 sequences must be modeled from discontinuous F0 data including unvoiced regions. The conventional F0 models such as the MSD-HMM and the continuous F0 HMM are not always appropriate for such demand. To overcome this problem, we propose an alternative F0 model named discontinuous observation HMM (DO-HMM) where the unvoiced frames are regarded as missing data. We objectively evaluate the performance of the DO-HMM by comparing it with the conventional F0 modeling techniques and discuss the results.\n",
    "Index Terms: HMM-based speech synthesis, F0 modeling, prosody generation, discontinuous observation HMM, spontaneous speech.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-158"
  },
  "meng12_interspeech": {
   "authors": [
    [
     "Fanbo",
     "Meng"
    ],
    [
     "Zhiyong",
     "Wu"
    ],
    [
     "Helen",
     "Meng"
    ],
    [
     "Jia",
     "Jia"
    ],
    [
     "Lianhong",
     "Cai"
    ]
   ],
   "title": "Hierarchical English emphatic speech synthesis based on HMM with limited training data",
   "original": "i12_0466",
   "page_count": 4,
   "order": 162,
   "p1": "466",
   "pn": "469",
   "abstract": [
    "Emphasis is an important form of expressiveness in speech. Hidden Markov model (HMM) based synthesis has shown great flexibility in generating expressive speech. This paper proposes a hierarchical model based on HMM aiming at synthesizing emphatic speech of both high emphasis quality and high naturalness with limited data. The decision tree (DT) is constructed with non-emphasis-questions using both neutral and emphasis corpora. We classify the data in each leaf of the DT into 6 emphasis categories according to the emphasis-related questions. The data of the same emphasis category are grouped into one sub-node and are used to train one HMM. As there might be no data of some specific emphasis categories in the leaves of the DT, a method based on the cost calculation is proposed to select a suitable HMM trained from the data of other sub-node in the same leaf for predicting parameters. Further a compensation model is proposed to adjust the predicted parameters. Experiments show that the proposed hierarchical model can synthesize emphatic speech with high quality for both naturalness and emphasis, using limited amount of training data.\n",
    "Index Terms: emphatic speech synthesis, hidden Markov model (HMM), hierarchy, compensation model\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-159"
  },
  "hoffmann12_interspeech": {
   "authors": [
    [
     "Sarah",
     "Hoffmann"
    ],
    [
     "Beat",
     "Pfister"
    ]
   ],
   "title": "Employing sentence structure: syntax trees as prosody generators",
   "original": "i12_0470",
   "page_count": 4,
   "order": 163,
   "p1": "470",
   "pn": "473",
   "abstract": [
    "In this paper, we describe a prosody generation system for speech synthesis that makes direct use of syntax trees to obtain duration and pitch. Instead of transforming the tree through special rules or extracting isolated features from the tree, we make use of the tree structure itself to construct a superpositional model that is able to learn the relation between syntax and prosody. We implemented the system in our SVOX text-to-speech system and evaluated it against the existing rule-based system. Informal listening tests showed that structural information from the tree is carried over to the prosody.\n",
    "Index Terms: speech synthesis, prosody, syntax analysis\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-160"
  },
  "ohishi12_interspeech": {
   "authors": [
    [
     "Yasunori",
     "Ohishi"
    ],
    [
     "Hirokazu",
     "Kameoka"
    ],
    [
     "Daichi",
     "Mochihashi"
    ],
    [
     "Kunio",
     "Kashino"
    ]
   ],
   "title": "A stochastic model of singing voice <i>F</i><sub>0</sub> contours for characterizing expressive dynamic components",
   "original": "i12_0474",
   "page_count": 4,
   "order": 164,
   "p1": "474",
   "pn": "477",
   "abstract": [
    "We present a novel stochastic model of singing voice fundamental frequency (F0) contours for characterizing expressive dynamic components, such as vibrato and portamento. Although dynamic components can be important features for any singing voice applications, modeling and extracting these components from a raw F0 contour have yet to be accomplished. Therefore, we describe a process for generating dynamic components explicitly and represent the process as a stochastic model. Then we develop an algorithm for estimating the model parameters based on statistical techniques. Experimental results show that our method successfully extracts the expressive components from raw F0 contours.\n",
    "Index Terms: Singing voice, Fundamental frequency, Second-order linear system, Stochastic model\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-161"
  },
  "silovsky12_interspeech": {
   "authors": [
    [
     "Jan",
     "Silovsky"
    ],
    [
     "Petr",
     "Cerva"
    ],
    [
     "Jindrich",
     "Zdansky"
    ],
    [
     "Jan",
     "Nouza"
    ]
   ],
   "title": "Study on integration of speaker diarization with speaker adaptive speech recognition for broadcast transcription",
   "original": "i12_0478",
   "page_count": 4,
   "order": 165,
   "p1": "478",
   "pn": "481",
   "abstract": [
    "In this paper we study a close incorporation of speaker diarization with speaker adaptive speech recognition in our broadcast transcription system. We provide our motivation for utilization of speech transcripts in the diarization process and analyze the effect it yields in terms of diarization performance or computational cost. Further, speaker adaptation performed according to various scenarios of speaker segmentation and diarization of an audio stream is evaluated. For better insight, the limit performance is evaluated substituting most of the components of the system by the oracle ones.\n",
    "Index Terms: Speaker diarization, i-vectors, speaker adaptation, CMLLR, broadcast transcription\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-162"
  },
  "shum12_interspeech": {
   "authors": [
    [
     "Stephen",
     "Shum"
    ],
    [
     "Najim",
     "Dehak"
    ],
    [
     "James",
     "Glass"
    ]
   ],
   "title": "On the use of spectral and iterative methods for speaker diarization",
   "original": "i12_0482",
   "page_count": 4,
   "order": 166,
   "p1": "482",
   "pn": "485",
   "abstract": [
    "This paper extends upon our previous work using i-vectors for speaker diarization. We examine the effectiveness of spectral clustering as an alternative to our previous approach using K-means clustering and adapt a previously-used heuristic to estimate the number of speakers. Additionally, we consider an iterative optimization scheme and experiment with its ability to improve both cluster assignments and segmentation boundaries in an unsupervised manner. Our proposed methods attain results similar to those of a state-of-the-art benchmark set on the multi-speaker CallHome telephone corpus.\n",
    "Index Terms: speaker diarization, factor analysis, Total Variability, spectral clustering\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-163"
  },
  "knox12_interspeech": {
   "authors": [
    [
     "Mary Tai",
     "Knox"
    ],
    [
     "Nikki",
     "Mirghafori"
    ],
    [
     "Gerald",
     "Friedland"
    ]
   ],
   "title": "Where did i go wrong?: identifying troublesome segments for speaker diarization systems",
   "original": "i12_0486",
   "page_count": 4,
   "order": 167,
   "p1": "486",
   "pn": "489",
   "abstract": [
    "The focus of this work is to identify types of segments that are difficult for state-of-the-art speaker diarization systems. The diarization outputs of five state-of-the-art systems are analyzed on short/long segments as well as segments surrounding speaker changepoints. We found that for all five systems as the duration of the segment decreased the diarization error rate (DER) increased. Also, segments immediately preceding and following speaker changepoints performed much worse than their respective counterparts. In fact, at least 40% of the DER for all five systems is attributed to time within 0.5 seconds of a speaker changepoint. We hope the results of this work motivate future improvements of speaker diarization systems.\n",
    "Index Terms: speaker diarization, error analysis, rich transcription\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-164"
  },
  "yella12_interspeech": {
   "authors": [
    [
     "Sree Harsha",
     "Yella"
    ],
    [
     "Fabio",
     "Valente"
    ]
   ],
   "title": "Speaker diarization of overlapping speech based on silence distribution in meeting recordings",
   "original": "i12_0490",
   "page_count": 4,
   "order": 168,
   "p1": "490",
   "pn": "493",
   "abstract": [
    "Speaker diarization of meetings can be significantly improved by overlap handling. Several previous works have explored the use of different features such as spectral, spatial and energy for overlap detection. This paper proposes an effective method to estimate probabilities of speech and overlap classes at a segment level which are later incorporated into an HMM/GMM baseline system. The estimation is motivated by the observation that significant portion of overlaps in spontaneous conversations take place where the amount of silence is less, e.g., during speaker changes. Experiments on the AMI corpus reveal that the probability of occurrence of overlap in a segment is inversely proportional to the amount of silence in it. Whenever this information is combined with acoustic information from MFCC features in an HMM/GMM overlap detector, improvements are verified in terms of F-measure. Furthermore the paper investigates the use of exclusion and labelling strategies based on such detector for handling overlap in diarization reporting F-measure improvements from 0.29 to 0.43 in case of exclusion and from 0.15 to 0.22 in case of labelling. Consequently speaker diarization error is reduced by 8% relative compared to the baseline based solely on acoustic information.\n",
    "Index Terms: speaker diarization, meeting recordings, diarization error, spontaneous overlap speech\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-165"
  },
  "bozonnet12_interspeech": {
   "authors": [
    [
     "Simon",
     "Bozonnet"
    ],
    [
     "Ravichander",
     "Vipperla"
    ],
    [
     "Nicholas",
     "Evans"
    ]
   ],
   "title": "Phone adaptive training for speaker diarization",
   "original": "i12_0494",
   "page_count": 4,
   "order": 169,
   "p1": "494",
   "pn": "497",
   "abstract": [
    "The linguistic content of a speech signal is a source of unwanted variation which can degrade speaker diarization performance. This paper presents our latest work to reduce its impact. The new approach, referred to as Phone Adaptive Training (PAT), is analogous to speaker adaptive training used in automatic speech recognition. We report an oracle experiment which shows that PAT has the potential to deliver a 33% relative improvement in the diarization error rate of our baseline system. Practical experiments show significant improvements across two standard, independent evaluation datasets.\n",
    "Index Terms: Speaker Diarization, Phone Adaptive Training, Speaker Discrimination\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-166"
  },
  "kelly12_interspeech": {
   "authors": [
    [
     "Finnian",
     "Kelly"
    ],
    [
     "Andrzej",
     "Drygajlo"
    ],
    [
     "Naomi",
     "Harte"
    ]
   ],
   "title": "Compensating for ageing and quality variation in speaker verification",
   "original": "i12_0498",
   "page_count": 4,
   "order": 170,
   "p1": "498",
   "pn": "501",
   "abstract": [
    "Performing speaker verification in the simultaneous presence of ageing progression and changing speech sample quality is an important, open problem. The issues of ageing and quality variation go hand in hand; the effect of ageing increases with time, while variations in quality are also more likely to be encountered as time passes. In this work we demonstrate the effect of ageing on speaker verification performance, and show the relationship between quality variation and verification score via a range of established quality measures. We employ a stacked classifier framework to combine the output of the baseline verification system with ageing information and quality measures. This new approach to long-term speaker verification allows for a multi-dimensional decision boundary that significantly improves upon the baseline performance. The proposed framework is evaluated on the Trinity College Dublin Speaker Ageing Database.\n",
    "Index Terms: speaker verification, ageing, stacked classifier\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-167"
  },
  "leeuwen12_interspeech": {
   "authors": [
    [
     "David van",
     "Leeuwen"
    ],
    [
     "Mohamad Hasan",
     "Bahari"
    ]
   ],
   "title": "Calibration of probabilistic age recognition",
   "original": "i12_0502",
   "page_count": 4,
   "order": 171,
   "p1": "502",
   "pn": "505",
   "abstract": [
    "The task in automatic age recognition in speech technology typically is one of regression, i.e., predicting the age of a speaker from his/ her speech. In this paper we are interested in the probabilistic interpretation of the posterior distribution of the predicted age. We review a number of measures for assessing the probabilistic properties of the posterior distribution, and link these to detection theory, which is very well understood from the automatic speaker recognition literature. We show that the Gaussian posterior distributions predicted by least square support vector regression behave well, and that there is only a small room for improvement of their posterior distributions under the Gaussian assumption.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-168"
  },
  "bahari12_interspeech": {
   "authors": [
    [
     "Mohamad Hasan",
     "Bahari"
    ],
    [
     "Mitchell",
     "McLaren"
    ],
    [
     "Hugo",
     "Van hamme"
    ],
    [
     "David van",
     "Leeuwen"
    ]
   ],
   "title": "Age estimation from telephone speech using i-vectors",
   "original": "i12_0506",
   "page_count": 4,
   "order": 172,
   "p1": "506",
   "pn": "509",
   "abstract": [
    "Motivated by the success of i-vectors in the field of speaker recognition, this paper proposes a new approach for age estimation from telephone speech patterns based on i-vectors. In this method, each utterance is modeled by its corresponding ivector. Then, Support Vector Regression (SVR) is applied to estimate the age of speakers. The proposed method is trained and tested on telephone conversations of the National Institute for Standard in Technology (NIST) 2010 and 2008 Speaker Recognition Evaluations databases. Evaluation results show that the proposed method outperforms different conventional methods in speaker age estimation.\n",
    "Index Terms: speaker age estimation, i-vector, support vector regression\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-169"
  },
  "rath12_interspeech": {
   "authors": [
    [
     "Shakti P.",
     "Rath"
    ],
    [
     "Martin",
     "Karafiát"
    ],
    [
     "Ondřej",
     "Glembek"
    ],
    [
     "Jan",
     "Černocký"
    ]
   ],
   "title": "A factorized representation of FMLLR transform based on QR-decomposition",
   "original": "i12_0551",
   "page_count": 4,
   "order": 173,
   "p1": "551",
   "pn": "554",
   "abstract": [
    "In this paper, we propose a novel representation of the FMLLR transform. This is different from the standard FMLLR in that the linear transform (LT) is expressed in a factorized form such that each of the factors involves only one parameter. The representation is mainly motivated by QR-decomposition of a square matrix and hence is referred to as QR-FMLLR. The mathematical expressions and steps for maximum likelihood (ML) estimation of the parameters are presented. The ML estimation of QR-FMLLR does not require the use of numerical technique, such as gradient ascent, and it does not involve matrix inversion and computation of matrix determinant. On an LVCSR task, we show the performance of QR-FMLLR to be comparable to the standard FMLLR. We conjecture that QR-FMLLR is amenable to speaker adaptation using data that varies from very short to large and present a brief discussion on how this can be achieved.\n",
    "Index Terms: FMLLR, QR Decomposition, Orthogonal Matrix, Givens Rotation, Upper Triangular Matrix\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-170"
  },
  "tomar12_interspeech": {
   "authors": [
    [
     "Vikrant Singh",
     "Tomar"
    ],
    [
     "Richard C.",
     "Rose"
    ]
   ],
   "title": "A correlational discriminant approach to feature extraction for robust speech recognition",
   "original": "i12_0555",
   "page_count": 4,
   "order": 174,
   "p1": "555",
   "pn": "558",
   "abstract": [
    "A non-linear discriminant analysis based approach to feature space dimensionality reduction in noise robust automatic speech recognition (ASR) is proposed. It utilizes a correlation based distance measure instead of the conventional Euclidean distance. The use of this \"correlation preserving discriminant analysis\" (CPDA) procedure is motivated by evidence suggesting that correlation based cepstrum distance measures can be more robust than Euclidean based distances when speech is corrupted by noise. The performance of CPDA is evaluated in terms of the word error rate obtained by using CPDA derived features on the Aurora 2 speech in noise corpus, and is compared to the commonly used linear discriminant analysis (LDA) approach to feature space transformations.\n",
    "Index Terms: Correlation preserving discriminant analysis, graph embedding, dimensionality reduction, speech recognition\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-171"
  },
  "weng12_interspeech": {
   "authors": [
    [
     "Chao",
     "Weng"
    ],
    [
     "Biing-Hwang (Fred)",
     "Juang"
    ],
    [
     "Daniel",
     "Povey"
    ]
   ],
   "title": "Discriminative training using non-uniform criteria for keyword spotting on spontaneous speech",
   "original": "i12_0559",
   "page_count": 4,
   "order": 175,
   "p1": "559",
   "pn": "562",
   "abstract": [
    "In this work, we investigate the feasibility of applying our prior works on discriminative training (DT) using non-uniform criteria to a keyword spotting task on spontaneous conversational speech. One of DT methods, minimum classification error (MCE), is recast and efficiently implemented in the weighted finite state transducer (WFST) framework to fit a keyword spotting task. To validate our approach, we evaluate it on a conversational speech task, the credit card use subset of Switchboard, in both kinds of keyword spotting scenarios: one is when a large vocabulary continuous speech recognition (LVCSR) decoder is available, the other is when a simple word-loop grammar of limited vocabulary is used. The results show our approach performs well in both cases, achieving 2.77% and 3.15% figure of merits (FOMs) absolute improvements\n",
    "Index Terms: LVCSR, keyword spotting, DT, non-uniform criteria, WFST\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-172"
  },
  "suzuki12_interspeech": {
   "authors": [
    [
     "Masayuki",
     "Suzuki"
    ],
    [
     "Gakuto",
     "Kurata"
    ],
    [
     "Masafumi",
     "Nishimura"
    ],
    [
     "Nobuaki",
     "Minematsu"
    ]
   ],
   "title": "Discriminative reranking for LVCSR leveraging invariant structure",
   "original": "i12_0563",
   "page_count": 4,
   "order": 176,
   "p1": "563",
   "pn": "566",
   "abstract": [
    "An invariant structure is one of the long-span acoustic representations, where acoustic variations caused by non-linguistic factors are effectively removed from speech. We present in this paper a new method to leverage the invariant structures as features of discriminative reranking for Large Vocabulary Continuous Speech Recognition (LVCSR). First we use a traditional HMM-based LVCSR system to get a list of N-best candidates with phone alignments and construct an invariant structure for each candidate using its phone alignment. Here, the invariant structure is composed of lengths between every two phonemes in the candidate. Then we estimate a score of each phoneme-pair in the invariant structure, and rerank the N-best candidates using a weighted sum of the phoneme-pair scores, where the weights are trained discriminatively by averaged perceptron. Experimental results show a relative CER improvement of 6.69% over the baseline HMM-based LVCSR system.\n",
    "Index Terms: Invariant Structure, LVCSR, Discriminative reranking\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-173"
  },
  "hu12b_interspeech": {
   "authors": [
    [
     "Ting-yao",
     "Hu"
    ],
    [
     "Yu",
     "Tsao"
    ],
    [
     "Lin-shan",
     "Lee"
    ]
   ],
   "title": "Discriminative fuzzy clustering maximum a posterior linear regression for speaker adaptation",
   "original": "i12_0567",
   "page_count": 4,
   "order": 177,
   "p1": "567",
   "pn": "570",
   "abstract": [
    "We propose a discriminative fuzzy clustering maximum a posterior linear regression (DFCMAPLR) model adaptation approach to compensate the acoustic mismatch due to speaker variability. The DFCMAPLR approach adopts the MAP criterion and a discriminative objective function to estimate shared affine transform and fuzzy weight sets, respectively. Then, through a linear combination of the calculated fuzzy weights and shared affine transforms, more specific affine transforms are formed for model adaptation. By incorporating the MAP criterion and the discriminative information, DFCMAPLR can calculate shared affine transforms reliably and enhance the discriminative power of the adapted acoustic model. Based on the experimental results on the ASTTEL200 Mandarin corpus, we verified that DFCMAPLR outperforms not only the conventional maximum likelihood linear regression (MLLR) but also the fuzzy clustering MLLR(FCMLLR), which estimates the shared affine transform and fuzzy weight sets both based on the maximum likelihood criterion. Moreover, when compared to the baseline result, DFCMAPLR provides a clear improvement of 9.86% (24.04% to 21.67%) relative average phone error rate (PER) reduction.\n",
    "Index Terms: speech recognition, speaker adaptation, FCMLLR\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-174"
  },
  "tahir12_interspeech": {
   "authors": [
    [
     "Muhammad Ali",
     "Tahir"
    ],
    [
     "Markus",
     "Nussbaum-Thom"
    ],
    [
     "Ralf",
     "Schlüter"
    ],
    [
     "Hermann",
     "Ney"
    ]
   ],
   "title": "Simultaneous discriminative training and mixture splitting of HMMs for speech recognition",
   "original": "i12_0571",
   "page_count": 4,
   "order": 178,
   "p1": "571",
   "pn": "574",
   "abstract": [
    "A method is proposed to incorporate mixture density splitting into the acoustic model discriminative training for speech recognition. The standard method is to obtain a high resolution acoustic model by maximum likelihood training and density splitting, and then improving this model by discriminative training. We choose a log-linear form of acoustic model because for a single Gaussian density per triphone state the log-linear MMI optimization is a convex optimization problem, and by further splitting and discriminative training of this model we can get a higher complexity model. Previously it was shown that we achieve large gains in the objective function and corresponding moderate gains in the word error rate on a large vocabulary corpus. This paper incorporates the state of the art minimum phone error training criterion into the framework, and shows that after discriminative splitting, a subsequent log-linear MPE training achieves better results than Gaussian mixture model MPE optimization alone.\n",
    "Index Terms: speech recognition, log linear modelling, discriminative training\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-175"
  },
  "boucheron12_interspeech": {
   "authors": [
    [
     "Laura",
     "Boucheron"
    ],
    [
     "Phillip L. De",
     "Leon"
    ]
   ],
   "title": "Low-SNR, speaker-dependent speech enhancement using GMMs and MFCCs",
   "original": "i12_0575",
   "page_count": 4,
   "order": 179,
   "p1": "575",
   "pn": "578",
   "abstract": [
    "In this paper, we propose a two-stage speech enhancement technique. In the training stage, a Gaussian Mixture Model (GMM) of the melfrequency cepstral coefficients (MFCCs) of a user's clean speech is computed wherein the component densities of the GMM serve to model the user's \"acoustic classes.\" In the enhancement stage, MFCCs from a noisy speech signal are computed and the underlying clean acoustic class is identified via a maximum a posteriori (MAP) decision and a novel mapping matrix. The associated GMM parameters are then used to estimate the MFCCs of the clean speech from the MFCCs of the noisy speech. Finally, the estimated MFCCs are transformed back to a time-domain waveform. Our results show that we can improve PESQ in environments as low as -10 dB SNR.\n",
    "Index Terms: Speech enhancement, MFCC, GMM\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-176"
  },
  "koutsogiannaki12_interspeech": {
   "authors": [
    [
     "Maria",
     "Koutsogiannaki"
    ],
    [
     "Michelle",
     "Pettinato"
    ],
    [
     "Cassie",
     "Mayo"
    ],
    [
     "Varvara",
     "Kandia"
    ],
    [
     "Yannis",
     "Stylianou"
    ]
   ],
   "title": "Can modified casual speech reach the intelligibility of clear speech?",
   "original": "i12_0579",
   "page_count": 4,
   "order": 180,
   "p1": "579",
   "pn": "582",
   "abstract": [
    "Clear speech is a speaking style adopted by speakers in an attempt to maximize the clarity of their speech and is proven to be more intelligible than casual speech. This work focuses on modifying casual speech to sound as intelligible as clear speech. To that purpose, a database of read speech sentences, recorded both in clear and in casual speaking style is analyzed. Based on the analysis of the database, speaking rate is the prevalent characteristic that differs between the two speaking styles. To examine if speaking rate plays role in the intelligibility advantage of clear speech, clear speech signals are time scaled in higher speaking rate to match the duration of the casual signals. Subjective and objective measures on time-scaled clear speech and casual speech, revealed that the low speaking rate is the main factor that contributes to clear speech intelligibility. However, when attempting to expand casual signals in time, the intelligibility of the casual signals deteriorates. Since time scale modifications on casual signals seem inappropriate of increasing intelligibility, spectral transformations on the casual speech signals are performed. Subjective and objective tests show a significant enhancement of the intelligibility of casual signals, reaching the intelligibility scores of clear signals.\n",
    "Index Terms: Clear speech, Casual speech, Speech intelligibility, Spectral modifications\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-177"
  },
  "carlin12_interspeech": {
   "authors": [
    [
     "Michael A.",
     "Carlin"
    ],
    [
     "Nicolas",
     "Malyska"
    ],
    [
     "Thomas F.",
     "Quatieri"
    ]
   ],
   "title": "Speech enhancement using sparse convolutive non-negative matrix factorization with basis adaptation",
   "original": "i12_0583",
   "page_count": 4,
   "order": 181,
   "p1": "583",
   "pn": "586",
   "abstract": [
    "We introduce a framework for speech enhancement based on convolutive non-negative matrix factorization that leverages available speech data to enhance arbitrary noisy utterances with no a priori knowledge of the speakers or noise types present. Previous approaches have shown the utility of a sparse reconstruction of the speech-only components of an observed noisy utterance. We demonstrate that an underlying speech representation which, in addition to applying sparsity, also adapts to the noisy acoustics improves overall enhancement quality. The proposed system performs comparably to a traditional Wiener filtering approach, and the results suggest that the proposed framework is most useful in moderate- to low-SNR scenarios.\n",
    "Index Terms: speech enhancement, convolutive non-negative matrix factorization, basis adaptation, sparsity\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-178"
  },
  "kolossa12_interspeech": {
   "authors": [
    [
     "Dorothea",
     "Kolossa"
    ],
    [
     "Robert",
     "Nickel"
    ],
    [
     "Steffen",
     "Zeiler"
    ],
    [
     "Rainer",
     "Martin"
    ]
   ],
   "title": "Inventory-based audio-visual speech enhancement",
   "original": "i12_0587",
   "page_count": 4,
   "order": 182,
   "p1": "587",
   "pn": "590",
   "abstract": [
    "In this paper we propose to combine audio-visual speech recognition with inventory-based speech synthesis for speech enhancement. Unlike traditional filtering-based speech enhancement, inventory-based speech synthesis avoids the usual trade-off between noise reduction and consequential speech distortion. For this purpose, the processed speech signal is composed from a given speech inventory which contains snippets of speech from a targeted speaker. However, the combination of speech recognition and synthesis is susceptible to noise as recognition errors can lead to a suboptimal selection of speech segments. The search for fitting clean speech segments can be significantly improved when audio-visual information is utilized by means of a coupled HMM recognizer and an uncertainty decoding framework. First results using this novel system are reported in terms of several instrumental measures for three types of noise.\n",
    "Index Terms: audio-visual speech enhancement, speech synthesis, unit selection, missing data techniques\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-179"
  },
  "jokinen12_interspeech": {
   "authors": [
    [
     "Emma",
     "Jokinen"
    ],
    [
     "Paavo",
     "Alku"
    ],
    [
     "Martti",
     "Vainio"
    ]
   ],
   "title": "Utilization of the lombard effect in post-.ltering for intelligibility enhancement of telephone speech",
   "original": "i12_0591",
   "page_count": 4,
   "order": 183,
   "p1": "591",
   "pn": "594",
   "abstract": [
    "Post-filtering methods are used in mobile communications to improve the quality and intelligibility of speech. This paper introduces a noiseadaptive post-filtering algorithm that models the spectral effects observed in natural Lombard speech. The proposed method and another postfiltering technique were compared to unprocessed speech and natural Lombard speech in subjective listening tests in terms of intelligibility and quality. The results indicate that the proposed method outperforms the reference method in difficult noise conditions.\n",
    "Index Terms: Speech enhancement, post-filtering, intelligibility, Lombard effect\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-180"
  },
  "duan12_interspeech": {
   "authors": [
    [
     "Zhiyao",
     "Duan"
    ],
    [
     "Gautham J.",
     "Mysore"
    ],
    [
     "Paris",
     "Smaragdis"
    ]
   ],
   "title": "Speech enhancement by online non-negative spectrogram decomposition in nonstationary noise environments",
   "original": "i12_0595",
   "page_count": 4,
   "order": 184,
   "p1": "595",
   "pn": "598",
   "abstract": [
    "Classical single-channel speech enhancement algorithms have two convenient properties: they require pre-learning the noise model but not the speech model, and they work online. How- ever, they often have difficulties in dealing with non-stationary noise sources. Source separation algorithms based on non- negative spectrogram decompositions are capable of dealing with non-stationary noise, but do not possess the aforemen- tioned properties. In this paper we present a novel algorithm that combines the advantages of both classical algorithms and nonnegative spectrogram decomposition algorithms. Experi- ments show that it significantly outperforms four categories of classical algorithms in non-stationary noise environments.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-181"
  },
  "varnet12_interspeech": {
   "authors": [
    [
     "Léo",
     "Varnet"
    ],
    [
     "Julien",
     "Meyer"
    ],
    [
     "Michel",
     "Hoen"
    ],
    [
     "Fanny",
     "Meunier"
    ]
   ],
   "title": "Phoneme resistance during speech-in-speech comprehension",
   "original": "i12_0599",
   "page_count": 4,
   "order": 185,
   "p1": "599",
   "pn": "602",
   "abstract": [
    "This study investigates masking effects occurring during speech comprehension in the presence of concurrent speech signals. We examined the differential effects of 4- to 8-talker babble (natural speech) or babble-like noise (reversed speech) on word identification. We measured phoneme identification rates. Results showed that different types of linguistic information can interfere with speech recognition and that different resistances are observed for different phonemes depending on interfering noise.\n",
    "Index Terms: Speech-in-speech; Energetic masking; Informational masking; Phoneme resistance\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-182"
  },
  "quene12_interspeech": {
   "authors": [
    [
     "Hugo",
     "Quené"
    ],
    [
     "Will",
     "Schuerman"
    ]
   ],
   "title": "<i>smile</i> with a smile",
   "original": "i12_0603",
   "page_count": 4,
   "order": 186,
   "p1": "603",
   "pn": "606",
   "abstract": [
    "Smiling during talking yields speech with higher formants, and hence larger formant dispersion. Previous studies have shown that motor resonance during perception of words related to smiling can activate muscles responsible for the smiling action. If word perception causes smiling activation for such smile-related words, then this motor resonance may occur also during production, resulting in larger formant dispersion in these smile-related words. This paper reports on formant measurements from tokens of the Corpus of Spoken Dutch. Formant values of smile-related word tokens were compared to semantically different but phonetically similar word tokens. Results suggest that formant dispersion is indeed larger in smile-related words than in control words, although the predicted difference was observed only for female speakers. These findings suggest that motor resonance originating from a word's meaning may affect the articulatory and acoustic realization of affective spoken words. Female speakers tend to produce the word smile with a smile.\n",
    "Index Terms: smiling; affect; motor resonance; formants; formant dispersion\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-183"
  },
  "lunsford12_interspeech": {
   "authors": [
    [
     "Rebecca",
     "Lunsford"
    ],
    [
     "Peter A.",
     "Heeman"
    ],
    [
     "Jan P. H. van",
     "Santen"
    ]
   ],
   "title": "Interactions between turn-taking gaps, disfluencies and social obligation",
   "original": "i12_0607",
   "page_count": 4,
   "order": 187,
   "p1": "607",
   "pn": "610",
   "abstract": [
    "Speakers strive to minimize inter-turn gaps when engaged in a dialogue. However, little work has addressed what impact this might have on the fluency of the following speech. In this paper we explore whether there are interactions between turn-taking gaps and turn-initial disfluencies and if the social pressure to respond to questions plays a role in that interaction. Our results indicate that child speakers are more likely to become disfluent both after a question and as the gap length increases, and that the two interact to further increase the likelihood. We also compared the speech of children with Typical Development (TD) to those with Autism Spectrum Disorder (ASD) or Developmental Language Disorder (DLD), where we found that those with ASD were less likely to become disfluent after a question. This finding suggests that the trade-off between timing and disfluencies is driven by social obligation, and that speakers are willing to tolerate disfluencies so as to maintain a short delay.\n",
    "Index Terms: turn-taking gaps, disfluencies, social pressure, language impairments\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-184"
  },
  "garnier12_interspeech": {
   "authors": [
    [
     "Maëva",
     "Garnier"
    ],
    [
     "Lucie",
     "Ménard"
    ],
    [
     "Gabrielle",
     "Richard"
    ]
   ],
   "title": "Effect of being seen on the production of visible speech cues. a pilot study on lombard speech",
   "original": "i12_0611",
   "page_count": 4,
   "order": 188,
   "p1": "611",
   "pn": "614",
   "abstract": [
    "Speech produced in noise (or Lombard speech) is characterized by increased vocal effort, but also by amplified lip gestures. The current study examines whether this enhancement of visible speech cues may be sought by the speaker, even unconsciously, in order to improve his visual intelligibility. One subject played an interactive game in a quiet situation and then in 85dB of cocktail-party noise, for three conditions of interaction: without interaction, in face-to-face interaction, and in a situation of audio interaction only. The audio signal was recorded simultaneously with articulatory movements, using 3D electromagnetic articulography.   The results showed that acoustic modifications of speech in noise were greater when the interlocutor could not see the speaker. Furthermore, tongue movements that are hardly visible were not particularly amplified in noise. Lip movements that are very visible were not more enhanced in noise when the interlocutors could see each other. Actually, they were more enhanced in the situation of audio interaction only. These results support the idea that this speaker did not make use of the visual channel to improve his intelligibility, and that his hyperarticulation was just an indirect correlate of increased vocal effort.\n",
    "Index Terms: Lombard speech, hyper-articulation, audiovisual intelligibility, multimodality\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-185"
  },
  "wodarczak12_interspeech": {
   "authors": [
    [
     "Marcin",
     "Włodarczak"
    ],
    [
     "Juraj",
     "Šimko"
    ],
    [
     "Petra",
     "Wagner"
    ]
   ],
   "title": "Temporal entrainment in overlapped speech: cross-linguistic study",
   "original": "i12_0615",
   "page_count": 4,
   "order": 189,
   "p1": "615",
   "pn": "618",
   "abstract": [
    "In a previous paper we investigated how onsets of overlapped speech in English are timed with respect to syllable boundaries of the current speaker [1]. Overlap initiations were found to be more frequent around syllable boundaries than at other locations within the syllable. In this paper we extend the previous analysis by reporting on results from two other corpora in two different languages (French and German). We found similar trends in all three datasets with an increased likelihood of an overlap initiation shortly before vowel onsets in the interlocutor's speech.\n",
    "Index Terms: dialogue rhythm, temporal entrainment, overlapped speech, turn-taking\n",
    "",
    "",
    "M. Włodarczak, J. Šimko, and P. Wagner, “Syllable boundary effect: temporal entrainment in overlapped speech,” in Proceedings of Speech Prosody 2012\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-186"
  },
  "lee12b_interspeech": {
   "authors": [
    [
     "Chi-Chun",
     "Lee"
    ],
    [
     "Athanasios",
     "Katsamanis"
    ],
    [
     "Panayiotis G.",
     "Georgiou"
    ],
    [
     "Shrikanth S.",
     "Narayanan"
    ]
   ],
   "title": "Based on isolated saliency or causal integration? toward a better understanding of human annotation process using multiple instance learning and sequential probability ratio test",
   "original": "i12_0619",
   "page_count": 4,
   "order": 190,
   "p1": "619",
   "pn": "622",
   "abstract": [
    "Human perception is capable of integrating local events to generate an overall impression at the global level; this is evident in daily life and is utilized repeatedly in behavioral science studies to bring objective measures into studies of human behavior. In this work, we explore two hypotheses considering whether it is the isolated-saliency or the causal-integration of information that can trigger the global perceptual behavioral ratings as trained annotators engage in tasks of observational coding. We carry out analyses using Multiple Instance Learning and Sequential Probability Ratio Test in a corpus of real and spontaneous distressed couples' interaction with global session-level abstract behavioral coding done by trained human annotators. We present various analyses based on different behavioral detection schemes demonstrating the potential of utilizing these algorithms in bringing insights into the human annotation process. We further show that while annotating behaviors with more positive impression, annotators gather information throughout the session compared to behaviors with more negative impression, where a single salient instance is enough to trigger the final global decision.\n",
    "Index Terms: multiple instance learning, sequential probability ratio test, behavior annotation, perception, observational coding\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-187"
  },
  "levow12_interspeech": {
   "authors": [
    [
     "Gina-Anne",
     "Levow"
    ],
    [
     "Susan",
     "Duncan"
    ]
   ],
   "title": "Contrasting cues to verbal and non-verbal backchannels in multi-lingual dyadic rapport",
   "original": "i12_0835",
   "page_count": 4,
   "order": 191,
   "p1": "835",
   "pn": "838",
   "abstract": [
    "Diverse multi-modal behaviors provide important cues in establishing and maintaining interactional rapport. However, these behaviors are often subtle and culture-specific. In this paper, we focus on two forms of backchannel behavior: vocal backchannels and non-verbal headnods. We employ a corpus of quasi-monologic story-telling interactions elicited from three distinct language/cultural groups: American English, Mexican Spanish, and Iraqi Arabic speakers. Through this corpus, we investigate prosodic cues associated with these two different types of verbal feedback. We identify both similarities and differences in the cues exploited by the speakers of these diverse language/cultural groups. Although both typically classed as backchannels, we observe substantial differences in cues associated with verbal and non-verbal feedback head nods across these languages. These contrasts argue for a more fine-grained analysis of the use and role of diverse social resonance behaviors.\n",
    "Index Terms: prosody, backchannels, multilingual analysis, multimodal\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-188"
  },
  "strombergsson12_interspeech": {
   "authors": [
    [
     "Sofia",
     "Strömbergsson"
    ],
    [
     "Jens",
     "Edlund"
    ],
    [
     "David",
     "House"
    ]
   ],
   "title": "Prosodic measurements and question types in the spontal corpus of Swedish dialogues",
   "original": "i12_0839",
   "page_count": 4,
   "order": 192,
   "p1": "839",
   "pn": "842",
   "abstract": [
    "Studies of questions present strong evidence that there is no one-to-one relationship between intonation and interrogative mode. In this paper, we describe some aspects of prosodic variation in the Spontal corpus of 120 half-hour spontaneous dialogues in Swedish. The study is part of ongoing work aimed at extracting a database of 600 questions from the corpus, complete with categorization and prosodic descriptions. We report on coding and annotation of question typology and present results concerning prosodic correlates related to question type for 438 of the questions. A prosodically salient distinction was found between the two categories termed, in our typology, forward and backward looking questions.\n",
    "Index Terms: speech prosody, spontaneous speech, question intonation, interrogative intonation\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-189"
  },
  "truong12_interspeech": {
   "authors": [
    [
     "Khiet P.",
     "Truong"
    ],
    [
     "Dirk",
     "Heylen"
    ]
   ],
   "title": "Measuring prosodic alignment in cooperative task-based conversations",
   "original": "i12_0843",
   "page_count": 4,
   "order": 193,
   "p1": "843",
   "pn": "846",
   "abstract": [
    "In this paper, we investigate prosodic alignment in task-based conversations. We use the HCRC Map Task Corpus and investigate how familiarity affects prosodic alignment and how task success is related to prosodic alignment. A variety of existing alignment measures is used and applied to our data. In particular, a windowed cross-correlation procedure, that has been used previously in visual behavior resaearch, is applied to our prosodic data. In addition, we address the issue of how to separate genuine observed alignment as a result from speaker-specific behavior in the data from alignment as a result of random coincidental behavior. Using these measures, we find some indications of prosodic convergence and synchrony in our task-based conversations. Alignment tendencies are strongest for intensity, and familiarity seems to play a role in convergence. Finally, weak evidence was found for a correlation between prosodic alignment measures and task succcess.\n",
    "Index Terms: prosodic alignment, convergence, synchrony, familiarity\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-190"
  },
  "laskowski12_interspeech": {
   "authors": [
    [
     "Kornel",
     "Laskowski"
    ],
    [
     "Mattias",
     "Heldner"
    ],
    [
     "Jens",
     "Edlund"
    ]
   ],
   "title": "On the dynamics of overlap in multi-party conversation",
   "original": "i12_0847",
   "page_count": 4,
   "order": 194,
   "p1": "847",
   "pn": "850",
   "abstract": [
    "Overlap, although short in duration, occurs frequently in multiparty conversation. We show that its duration is approximately log-normal, and inversely proportional to the number of simultaneously speaking parties. Using a simple model, we demonstrate that simultaneous talk tends to end simultaneously less frequently than in begins simultaneously, leading to an arrow of time in chronograms constructed from speech activity alone. The asymmetry is significant and discriminative. It appears to be due to dialog acts which do not carry propositional content, and those which are not brought to completion.\n",
    "Index Terms: multi-party conversation, overlap, turn-taking.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-191"
  },
  "truong12b_interspeech": {
   "authors": [
    [
     "Khiet P.",
     "Truong"
    ],
    [
     "Jürgen",
     "Trouvain"
    ]
   ],
   "title": "On the acoustics of overlapping laughter in conversational speech",
   "original": "i12_0851",
   "page_count": 4,
   "order": 195,
   "p1": "851",
   "pn": "854",
   "abstract": [
    "The social nature of laughter invites people to laugh together. This joint vocal action often results in overlapping laughter. In this paper, we show that the acoustics of overlapping laughter are different from non-overlapping laughter. We found that overlapping laughs are stronger prosodically marked than non-overlapping ones, in terms of higher values for duration, mean F0, mean and maximum intensity, and the amount of voicing. This effect is intensified by the number of people joining in the laughter event, which suggests that entrainment is at work. We also found that group size affects the amount of overlapping laughs which illustrates the contagious nature of laughter. Finally, people appear to join laughter simultaneously at a delay of approximately 500 ms: this means that spoken dialogue systems have some time to decide how to respond to a user's laugh.\n",
    "Index Terms: laughter, conversation, overlap, entrainment\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-192"
  },
  "gravano12_interspeech": {
   "authors": [
    [
     "Agustín",
     "Gravano"
    ],
    [
     "Julia",
     "Hirschberg"
    ]
   ],
   "title": "A corpus-based study of interruptions in spoken dialogue",
   "original": "i12_0855",
   "page_count": 4,
   "order": 196,
   "p1": "855",
   "pn": "858",
   "abstract": [
    "We examine interruptions in a corpus of spontaneous task-oriented dialogue. We present evidence that interruptions occur at particular places in conversation. They are likely to occur during or after speech with certain acoustic/prosodic properties. We also examine the speech of interruptions themselves and find a number of significant differences between interrupting and non-interrupting turns.\n",
    "Index Terms: interruption, turn-taking, prosody, dialogue\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-193"
  },
  "syrdal12_interspeech": {
   "authors": [
    [
     "Ann K.",
     "Syrdal"
    ],
    [
     "H. Timothy",
     "Bunnell"
    ],
    [
     "Susan R.",
     "Hertz"
    ],
    [
     "Taniya",
     "Mishra"
    ],
    [
     "Murray",
     "Spiegel"
    ],
    [
     "Corine",
     "Bickley"
    ],
    [
     "Deborah",
     "Rekart"
    ],
    [
     "Matthew J.",
     "Makashay"
    ]
   ],
   "title": "Text-to-speech intelligibility across speech rates",
   "original": "i12_0623",
   "page_count": 4,
   "order": 197,
   "p1": "623",
   "pn": "626",
   "abstract": [
    "A web-based listening test measured intelligibility across speech rate of 8 TTS systems and a linearly time-compressed human speech reference voice. Four synthesis methods were compared: formant, diphone concatenation, unit selection concatenation, and HMM synthesis. For each TTS method, a female and a male American English voice from each of 2 independent synthesis engines were tested. Semantically unpredictable sentences were presented at 6 speech rates from 200 to 450 words per minute. In an open response format, listeners typed what they heard. Listener transcriptions were automatically scored at the word level, and a normalized edit distance per speech rate was calculated for each of 355 listeners. There were significant differences among the TTS systems. The two unit selection TTS systems were the most intelligible across speech rates; one was equivalent to human speech. Listeners' native language, TTS familiarity, and audio equipment were also significant factors.\n",
    "Index Terms: speech synthesis, text-to-speech, intelligibility, speech rate\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-194"
  },
  "wang12_interspeech": {
   "authors": [
    [
     "Linfang",
     "Wang"
    ],
    [
     "Lijuan",
     "Wang"
    ],
    [
     "Yan",
     "Teng"
    ],
    [
     "Zhe",
     "Geng"
    ],
    [
     "Frank K.",
     "Soong"
    ]
   ],
   "title": "Objective intelligibility assessment of text-to-speech system using template constrained generalized posterior probability",
   "original": "i12_0627",
   "page_count": 4,
   "order": 198,
   "p1": "627",
   "pn": "630",
   "abstract": [
    "Speech intelligibility is one of the most important measures in evaluating text-to-speech (TTS) synthesizer. For fast comparing, developing, and deploying TTS systems, automatic objective intelligibility measurement is desired, as human listening test is label intensive, inconsistent, and with expensive cost. In this work, we propose an automatic objective intelligibility measure for synthesized speech using template constrained generalized posterior probability (TCGPP). TCGPP is a posterior probability based confidence measure, which has the advantage to identify errors in synthesized speech at small granularity level. Moreover, the TCGPP scores over a testing set can be summarized into an overall objective intelligibility metric to compare two synthesizers, or rank multiple TTS systems. We conducted the experiments using the synthesized test sentences from all the participants of EH1 English task in Blizzard Challenge 2010. The results show the proposed measure has high correlation (corr=0.9) with subjective scores and ranking.\n",
    "Index Terms: speech synthesis, objective intelligibility, Template constrained generalized posterior probability\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-195"
  },
  "valentinibotinhao12_interspeech": {
   "authors": [
    [
     "Cassia",
     "Valentini-Botinhao"
    ],
    [
     "Junichi",
     "Yamagishi"
    ],
    [
     "Simon",
     "King"
    ]
   ],
   "title": "Mel cepstral coefficient modification based on the glimpse proportion measure for improving the intelligibility of HMM-generated synthetic speech in noise",
   "original": "i12_0631",
   "page_count": 4,
   "order": 199,
   "p1": "631",
   "pn": "634",
   "abstract": [
    "We propose a method that modifies the Mel cepstral coefficients of HMM-generated synthetic speech in order to increase the intelligibility of the generated speech when heard by a listener in the presence of a known noise. This method is based on an approximation we previously proposed for the Glimpse Proportion measure. Here we show how to update the Mel cepstral coefficients using this measure as an optimization criterion and how to control the amount of distortion by limiting the frequency resolution of the modifications. To evaluate the method we built eight different voices from normal read-text speech data from a male speaker. Some voices were also built from Lombard speech data produced by the same speaker. Listening experiments with speech-shaped noise and with a single competing talker indicate that our method significantly improves intelligibility when compared to unmodified synthetic speech. The voices built from Lombard speech outperformed the proposed method particularly for the competing talker case. However, compared to a voice using only the spectral parameters from Lombard speech, the proposed method obtains similar or higher performance.\n",
    "Index Terms: intelligibility of speech in noise, Mel cepstral coefficients, HMM-based speech synthesis\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-196"
  },
  "zorila12_interspeech": {
   "authors": [
    [
     "Tudor-Catalin",
     "Zorila"
    ],
    [
     "Varvara",
     "Kandia"
    ],
    [
     "Yannis",
     "Stylianou"
    ]
   ],
   "title": "Speech-in-noise intelligibility improvement based on spectral shaping and dynamic range compression",
   "original": "i12_0635",
   "page_count": 4,
   "order": 200,
   "p1": "635",
   "pn": "638",
   "abstract": [
    "In this paper, we suggest a non-parametric way to improve the intelligibility of speech in noise. The signal is enhanced before presented in a noisy environment, under the constraint of equal global signal power before and after modifications. Two systems are combined in a cascade form to enhance the quality of the signal first in frequency (spectral shaping) and then in time (dynamic range compression). Experiments with speech shaped (SSN) and competed speaker (CS) types of noise at various low SNR values, show that the suggested approach outperforms state-of-the art methods in terms of the Speech Intelligibility Index (SII). In terms of SNR gain there is an improvement of 4 dB (SSN) and 8 dB (CS). A large formal listening test confirm the efficiency of the suggested system in enhancing speech intelligibility in noise.\n",
    "Index Terms: speech-in-noise enhancement, speech intelligibility, spectral shaping, dynamic range compression\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-197"
  },
  "erro12b_interspeech": {
   "authors": [
    [
     "Daniel",
     "Erro"
    ],
    [
     "Yannis",
     "Stylianou"
    ],
    [
     "Eva",
     "Navas"
    ],
    [
     "Inma",
     "Hernáez"
    ]
   ],
   "title": "Implementation of simple spectral techniques to enhance the intelligibility of speech using a harmonic model",
   "original": "i12_0639",
   "page_count": 4,
   "order": 201,
   "p1": "639",
   "pn": "642",
   "abstract": [
    "We have designed a system that increases the intelligibility of speech signals in noise by manipulating the parameters of a harmonic speech model. The system performs the transformation in two steps: in the first step, it modifies the spectral slope, which is closely related to the vocal effort; in the second step, it amplifies low-energy parts of the signal using dynamic range compression techniques. Objective and subjective measures involving speech-shaped noise confirm the effectiveness of these simple methods. As the harmonic model has been used in previous works to implement the waveform generation module of high-quality statistical synthesizers, the system presented here can provide the synthesis engine with a higher degree of control on the intelligibility of the resulting artificial speech.\n",
    "Index Terms: speech intelligibility in noise, harmonic model, speech synthesis, spectral tilt, dynamic range compression\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-198"
  },
  "mohammadi12_interspeech": {
   "authors": [
    [
     "Seyed Hamidreza",
     "Mohammadi"
    ],
    [
     "Alexander",
     "Kain"
    ],
    [
     "Jan P. H. van",
     "Santen"
    ]
   ],
   "title": "Making conversational vowels more clear",
   "original": "i12_0643",
   "page_count": 4,
   "order": 202,
   "p1": "643",
   "pn": "646",
   "abstract": [
    "Previously, it has been shown that using clear speech short-term spectra improves the intelligibility of conversational speech. In this paper, a speech transformation method is used to map the spectral features of conversational speech to resemble clear speech. A joint-density Gaussian mixture model is used as the mapping function. The transformation is studied in both the formant frequency and the line spectral frequency domains. Listening test results show that in noisier environments, the transformed speech signal improves vowel intelligibility signif- icantly compared to the original conversational speech. There is also an increase in vowel intelligibility in less noisy environ- ments, but the increase is not statistically significant. The significance tests are performed by Tukey comparison and planned one-tail t-test methods.\n",
    "Index Terms: conversational speech, clear speech, speech transformation, intelligibility\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-199"
  },
  "tsurutani12_interspeech": {
   "authors": [
    [
     "Chiharu",
     "Tsurutani"
    ],
    [
     "Shunichi",
     "Ishihara"
    ]
   ],
   "title": "Naturalness judgement of prosodic variation of Japanese utterances with prosody modified stimuli",
   "original": "i12_0647",
   "page_count": 4,
   "order": 203,
   "p1": "647",
   "pn": "650",
   "abstract": [
    "This study aims to identify the crucial prosodic factor for native speakers' naturalness judgement of L2 pronunciation. Prosodic features are known to have more impact on the naturalness of L2 learners' pronunciation than segmental features do. Among prosodic features, timing and pitch are looked at in this study as major prosodic factors which affect native speakers' naturalness judgement of L2 learners' pronunciation. To examine the relative importance of timing and pitch, we synthesized stimuli using STRAGHT to produce well-controlled timing and pitch errors. Native Japanese listeners assessed the naturalness of these stimuli and the result was compared with the one obtained using natural speech stimuli. The current study obtained the same result as the previous study: that timing is more important than pitch in improving the naturalness of L2 speech.\n",
    "Index Terms: naturalness, morphing, prosodic features, Japanese\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-200"
  },
  "avanzi12b_interspeech": {
   "authors": [
    [
     "Mathieu",
     "Avanzi"
    ],
    [
     "Pauline",
     "Dubosson"
    ],
    [
     "Sandra",
     "Schwab"
    ]
   ],
   "title": "Effects of dialectal origin on articulation rate in French",
   "original": "i12_0651",
   "page_count": 4,
   "order": 204,
   "p1": "651",
   "pn": "654",
   "abstract": [
    "This paper compares the articulation rate of 4 varieties of French: Parisian French (PA); Swiss French spoken in Neuchatel (NE) and French spoken by Swiss German speakers (BE and ZH) who have been living in a French-speaking environment (in Neuchatel) for 20 years at least. The objective is twofold: to assess the existence of differences in articulation rate between native French speakers of a standard variety (PA) and native French speakers of a regional variety (NE); to address whether the non-native speakers (BE and ZH) exhibit a different behaviour regarding articulation rate compared with the native speakers of the corresponding variety (NE). Besides the \"regional\" factor, this study takes into account further factors that may have an influence on articulation rate: gender and age of speakers, speech style (reading or conversation) and number of syllables within the Accentual Phrase.\n",
    "Index Terms: prosody, articulation rate, standard French, nonnative French, regional French, accentual phrase.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-201"
  },
  "hsieh12_interspeech": {
   "authors": [
    [
     "Chiao-Hua",
     "Hsieh"
    ],
    [
     "Chen-Yu",
     "Chiang"
    ],
    [
     "Yih-Ru",
     "Wang"
    ],
    [
     "Hsiu-Min",
     "Yu"
    ],
    [
     "Sin-Horng",
     "Chen"
    ]
   ],
   "title": "A new approach of speaking rate modeling for Mandarin speech prosody",
   "original": "i12_0655",
   "page_count": 4,
   "order": 205,
   "p1": "655",
   "pn": "658",
   "abstract": [
    "In this paper, a new approach of Mandarin-speech prosody modeling to consider the effects of speaking rate is proposed. The approach is a modification of our previous prosody labeling and modeling method to take speaking rate as a continuous independent variable and let prosodic-acoustic features and some parameters of prosodic models depend on it in order to count its influences. A speaking rate-dependent hierarchical prosodic model is hence constructed from four speech corpora of a single female speaker with fast, normal, medium and slow speaking rates. An analysis of the effects of speaking rate on the model parameters showed that they agreed well with our prior knowledge. So, the proposed approach provides a systematic and effective way to quantify the effects of speaking rate on Mandarin-speech prosody.\n",
    "Index Terms: speaking rate, prosody modeling\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-202"
  },
  "doukhan12_interspeech": {
   "authors": [
    [
     "David",
     "Doukhan"
    ],
    [
     "Albert",
     "Rilliard"
    ],
    [
     "Sophie",
     "Rosset"
    ],
    [
     "Christophe",
     "D'Alessandro"
    ]
   ],
   "title": "Modelling pause duration as a function of contextual length",
   "original": "i12_0659",
   "page_count": 4,
   "order": 206,
   "p1": "659",
   "pn": "662",
   "abstract": [
    "Effects of contextual length are known to affect pause durations in neutral speech. The present study investigates these effects on an expressive corpus of read tales in French. Computational models of intra-sentence, and inter-sentence pause durations, as functions of contextual lengths are proposed. These models are aimed at improving Text-To-Speech synthesis systems, and provide clues for synthesizing prosodic instructions above the level of the sentence. They are also aimed to help in the prosodic analysis of pause durations, which may be biased by contextual length effects. We find the phoneme to be the best unit for measuring contextual length. Inter-sentence pauses durations were more influenced by the length of the preceding sentences. Intra-sentence pauses durations were more influenced by the length of the following pseudo-clauses.\n",
    "Index Terms: pause duration, prosodic analysis, Text-To-Speech Synthesis\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-203"
  },
  "wang12b_interspeech": {
   "authors": [
    [
     "Bei",
     "Wang"
    ],
    [
     "Chenxia",
     "Li"
    ],
    [
     "Qian",
     "Wu"
    ],
    [
     "Xiaxia",
     "Zhang"
    ],
    [
     "Baofeng",
     "Wang"
    ],
    [
     "Yi",
     "Xu"
    ]
   ],
   "title": "Production and perception of focus in PFC and non-PFC languages: comparing beijing Mandarin and hainan tsat",
   "original": "i12_0663",
   "page_count": 4,
   "order": 207,
   "p1": "663",
   "pn": "666",
   "abstract": [
    "Prosodic marking of focus has been found to be typologically different in terms of existence of post-focus compression in F0 and intensity (PFC). In the production experiment, PFC showed in Mandarin, but not in Tsat (a language spoken in Hainan, China) or in Tsat-Mandarin (Mandarin spoken by Tsat people). The perception experiments further showed the effectiveness of PFC on focus perception. Focus perception in Tsat-Mandarin and Tsat was relatively low. An interesting finding is that the perception of focus in Mandarin by Tsat listeners was much lower than that by Mandarin listeners (54.8% vs. 75.6%). It shows the difficulty of speakers from non-PFC languages associating PFC with focus perception. It can partly explain why PFC is hard to be passed through language contact.\n",
    "Index Terms: post-focus compression (PFC), language contact, focus\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-204"
  },
  "zhang12b_interspeech": {
   "authors": [
    [
     "Xiaxia",
     "Zhang"
    ],
    [
     "Bei",
     "Wang"
    ],
    [
     "Qian",
     "Wu"
    ],
    [
     "Yi",
     "Xu"
    ]
   ],
   "title": "Prosodic realization of focus in statement and question in tibetan (lhasa dialect)",
   "original": "i12_0667",
   "page_count": 4,
   "order": 208,
   "p1": "667",
   "pn": "670",
   "abstract": [
    "The present study investigated prosodic realization of focus in statement and yes/no question in Tibetan (Lhasa dialect). Two target sentences were uttered by 8 speakers in both statement and yes/no question under four focus conditions, i.e., initial, medial, final and neutral focus. Systematic acoustic analysis showed that: (1) On-focus words exhibited significant F0 rising, pitch range expansion, and duration lengthening, while pitch range of pre-focus words remained largely intact. Importantly, focus led to post-focus F0 lowering and compression and this is true for both statement and question. (2) Interrogative intonation had higher F0 than statement in the unfocused words. In general, the prosodic realization of focus in Tibetan is comparable to English and Beijing Mandarin. We will also discuss the impact of the new finding on our knowledge about the distribution and origin of post-focus compression (PFC).\n",
    "Index Terms: Tibetan, focus, post-focus compression, PFC\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-205"
  },
  "vainio12_interspeech": {
   "authors": [
    [
     "Martti",
     "Vainio"
    ],
    [
     "Daniel",
     "Aalto"
    ],
    [
     "Antti",
     "Suni"
    ],
    [
     "Anja",
     "Arnhold"
    ],
    [
     "Tuomo",
     "Raitio"
    ],
    [
     "Henri",
     "Seijo"
    ],
    [
     "Juhani",
     "Järvikivi"
    ],
    [
     "Paavo",
     "Alku"
    ]
   ],
   "title": "Effect of noise type and level on focus related fundamental frequency changes",
   "original": "i12_0671",
   "page_count": 4,
   "order": 209,
   "p1": "671",
   "pn": "674",
   "abstract": [
    "Speech in noise, or Lombard speech, is characterized by increased intensity and higher fundamental frequency as well as lengthened segmental durations as speakers try to maintain a beneficial signal-to-noise ratio to fill both communicative and self-monitoring requirements. The phenomenon has been studied with regard to different noise types and different noise levels, as well as with respect to different communicative tasks (e.g., reading out loud vs. speaking to a real listener). However, there are no studies where the effect has been measured with different noises keeping the loudness levels equal. Here we study the Lombard effect with three different noise types at three levels with equal loudness while varying focus structure to elicit different pitch contours. The results show that people adapt their intonation contours depending on both noise level and type even when the noises are similar with respect to their perceived loudness. This points to a special role for pitch in Lombard speech.\n",
    "Index Terms: Lombard speech, prosody, focus marking\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-206"
  },
  "warsi12_interspeech": {
   "authors": [
    [
     "Anal",
     "Warsi"
    ],
    [
     "Tulika",
     "Basu"
    ],
    [
     "Debasis",
     "Mazumdar"
    ]
   ],
   "title": "Role of prosody in automatic modality recognition of bangla speech",
   "original": "i12_0675",
   "page_count": 4,
   "order": 210,
   "p1": "675",
   "pn": "678",
   "abstract": [
    "During expressive speech, the voice is enriched to convey not only the intended semantic message but also the speaker's state of mind and intention. Our goal is to design a tool which can be used in Speech-to-Speech translation system for automatically classifying utterances of Bangla into three modalities namely Statement, Question and Command. Although pitch and intensity features have been commonly used to recognize sentence modality, it is not clear what aspects of the pitch and intensity contour are salient for recognizing sentence modality in Bangla. A set of 30 features derived from 680 speech samples are analyzed to identify the most discriminative set of features for Bangla. Three well-known classification algorithms viz. Decision Tree J48, Support Vector Machine and k-Nearest Neighbor (k-NN) are tested with both the full set and reduced subset of features. A global accuracy of 96.08 % of correct classification has been achieved by k-NN using the reduced subset of features.\n",
    "Index Terms: sentence modality, recognition, prosodic features\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-207"
  },
  "braun12_interspeech": {
   "authors": [
    [
     "Bettina",
     "Braun"
    ]
   ],
   "title": "Where to associate stressed additive particles? evidence from speech prosody",
   "original": "i12_0679",
   "page_count": 4,
   "order": 211,
   "p1": "679",
   "pn": "682",
   "abstract": [
    "Theoretical approaches mostly associate stressed additive particles (e.g., auch in German) with contrastive topics. Empirical data show that associated constituents are produced more prominently than unassociated ones but not that they are contrastive topics. This paper compares the prosodic realizations of associated constituents with contrastive and non-contrastive topics. We found no differences in accent types but later alignment for contrastive than non-contrastive topics; associated constituents lie in-between. An unrestricted sentence completion task tested whether listeners produce more additive particles upon hearing fragments with contrastive compared to non-contrastive topics. Completions containing additive particles were generally very infrequent (<4%), but crucially more frequent in sentences with a contrastive topic compared to a non-contrastive topic. Stressed additive particles seem to associate with prominent accents, which may often be contrastive topics.\n",
    "Index Terms: intonation, additive particles, contrastive topic, alignment, information structure\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-208"
  },
  "benton12_interspeech": {
   "authors": [
    [
     "Matthew",
     "Benton"
    ]
   ],
   "title": "From PVI to perception: a return to the roots of rhythm in broadcast news",
   "original": "i12_0683",
   "page_count": 4,
   "order": 212,
   "p1": "683",
   "pn": "686",
   "abstract": [
    "In the mid 1900s, scholars such as Lloyd James (1940), Pike (1945), and Abercrombie (1967) popularized the theory of syllable-timed and stresstimed languages using impressionistic judgments. At that time, the ears of the researchers determined rhythm types. However, in current years, there has been a shift towards an empirical investigation of speech rhythm based on speakers' production of particular languages. A notable lack of research on the perception of rhythm exists since the mid twentieth century. This study tested perception of rhythmic differences between languages using data previously shown to be statistically different based on rhythm metrics. Using what was thought to be prototypical speech rhythm samples of Chinese and English Broadcast news, this work has shown that 60 listeners of various language backgrounds were able to perceive the difference between these two languages, using only acoustic correlate based rhythm cues, with accuracy that was better than chance.\n",
    "Index Terms: speech rhythm perception, PVI, non-laboratory speech rhythm\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-209"
  },
  "meyer12b_interspeech": {
   "authors": [
    [
     "Julien",
     "Meyer"
    ],
    [
     "Laure",
     "Dentel"
    ],
    [
     "Frank",
     "Seifart"
    ]
   ],
   "title": "A methodology for the study of rhythm in drummed forms of languages: application to Bora Manguare of Amazon",
   "original": "i12_0687",
   "page_count": 4,
   "order": 213,
   "p1": "687",
   "pn": "690",
   "abstract": [
    "This study presents a new methodology adapted to the analysis of word rhythmic cues in drummed forms of languages. The semi-automatic beat detection procedure applied to the Manguare drummed form of the Bora language enabled us to measure inter-beat durations. These were found to correspond to V-to-V intervals of the associated speech utterances and to differ as a function of the vowel duration and of the presence/absence of consonant(s) in the V-to-V cluster.\n",
    "Index Terms: drummed language, rhythm, prosody, Bora language, semi-automatic detection, phoneme clustering\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-210"
  },
  "wayland12_interspeech": {
   "authors": [
    [
     "Ratree",
     "Wayland"
    ],
    [
     "Donruethai",
     "Laphasradakul"
    ],
    [
     "Edith",
     "Kaan"
    ],
    [
     "Rui",
     "Cao"
    ]
   ],
   "title": "Perception of pitch contours among native tone listeners",
   "original": "i12_1946",
   "page_count": 3,
   "order": 214,
   "p1": "1946",
   "pn": "1948",
   "abstract": [
    "Level, rising and falling pitch contours were presented to ten native speakers of Thai and fifteen native speakers of Mandarin Chinese for pairwised discrimination in a same-different categorial discrimination task. Their performance was comparable across all contrasts. Both groups were more successful at discriminating the rising from the falling and the level contours, but failed to discriminate the level contour from the falling contour and vice versa. Experience with the native tone systems may partially explain the results. However, the hypothesis that a rising contour may be inherently more psychoacoustically salient than either a falling or a level contour cannot be ruled out.\n",
    "Index Terms: pitch contour perception, Mandarin, Thai\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-211"
  },
  "igarashi12_interspeech": {
   "authors": [
    [
     "Yosuke",
     "Igarashi"
    ],
    [
     "Hanae",
     "Koiso"
    ]
   ],
   "title": "Pitch range control of Japanese boundary pitch movements",
   "original": "i12_1949",
   "page_count": 4,
   "order": 215,
   "p1": "1949",
   "pn": "1952",
   "abstract": [
    "Boundary pitch movements (BPMs) are tones that occur at the end of the prosodic phrases in Japanese and contribute to the pragmatic interpretation of the utterance, such as questioning, continuation, and emphasis. The present paper investigated pitch range control of BPMs, which has been little studied in previous works. Specifically, it examined whether downstep is observed in BPMs through the analysis of Corpus of Spontaneous Japanese. The results revealed that ordinal downstep was not found in BPMs, and that pitch range control of BPMs differed across their types. The results suggest that currently accepted frameworks of Japanese intonation cannot deal with pitch range control of BPMs.\n",
    "Index Terms: downstep, pitch range, boundary pitch movement\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-212"
  },
  "kuo12_interspeech": {
   "authors": [
    [
     "Grace",
     "Kuo"
    ]
   ],
   "title": "Perceived prosodic boundaries in taiwanese and their acoustic correlates",
   "original": "i12_1953",
   "page_count": 4,
   "order": 216,
   "p1": "1953",
   "pn": "1956",
   "abstract": [
    "This paper investigates boundary strength detection in Taiwanese and Swedish by Taiwanese listeners. Earlier production studies have suggested that the Taiwanese tone sandhi group (TSG) is an independent prosodic domain, yet no previous study has reported perceptual data to support this claim. This study presents listeners' perceptual rating data on different prosodic domain boundaries and demonstrates correlations between acoustic measures of the stimuli and the perception ratings. The perceptual rating results show that the Taiwanese tone sandhi group is distinct from other prosodic domains. Significant correlations are found between ratings and pitch and between ratings and acoustic voice quality measures.\n",
    "Index Terms: Taiwanese tone sandhi, boundary strength, prosodic domain\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-213"
  },
  "hon12_interspeech": {
   "authors": [
    [
     "Laying",
     "Hon"
    ],
    [
     "Yuan",
     "Jia"
    ],
    [
     "Aijun",
     "Li"
    ]
   ],
   "title": "Phonetic foreignization of Mandarin for dubbing in imported western movies",
   "original": "i12_1957",
   "page_count": 4,
   "order": 217,
   "p1": "1957",
   "pn": "1960",
   "abstract": [
    "Through a multi-discipline approach, the present study conducts a pioneering exploration of an artistic Mandarin, namely, the Mandarin for Dubbing in imported western movies. Two hypotheses were made based on explicit phonetic cues: (1) Mandarin for dubbing has developed as a particular variety of Mandarin. (2) The main distinctive features come from the original language of imported movies. A perceptual experiment was first carried out to establish the \"variety\" status of the Mandarin for Dubbing. Then the supra-segmental acoustic features were compared among Mandarin for dubbing, Standard Mandarin and English. In this session, a case study to a matched set and comparisons in F0 and rhythm were carried out. The results reveal: (1) A high percentage in which that native Chinese speakers distinguish the Mandarin for Dubbing from standard mandarin. (2) An overall tendency of the parameters representing pitch and rhythm of Mandarin for dubbing towards the source language. All the results support the two hypotheses. On the basis of these results, a genetic, historical explanation was given in the perspective of language contact and language transfer.\n",
    "Index Terms: artistic speech, foreignization, F0, rhythm, variety, language transfer\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-214"
  },
  "moniz12_interspeech": {
   "authors": [
    [
     "Helena",
     "Moniz"
    ],
    [
     "Fernando",
     "Batista"
    ],
    [
     "Isabel",
     "Trancoso"
    ],
    [
     "Ana Isabel",
     "Mata"
    ]
   ],
   "title": "Prosodic contex-based analysis of disfluencies.",
   "original": "i12_1961",
   "page_count": 4,
   "order": 218,
   "p1": "1961",
   "pn": "1964",
   "abstract": [
    "This work explores prosodic cues of disfluencies in a corpus of university lectures. Results show three significant (p< 0.001) trends: pitch and energy slopes are significantly different between the disfluency and the onset of fluency; those features are also relevant to disfluency type differentiation; and they do not seem to be a speaker-effect. The best combination of linguistic features one can use to better predict the onset of fluency are pitch and energy resets as well as the presence of a silent pause immediately before a repair. Our results, thus, point out to a strategy of prosodic contrast rather than of parallelism. With this work we hope to contribute to the analysis of the prosodic behaviors in the production of the so called disfluencies and in the fluency repair in European Portuguese.\n",
    "Index Terms: prosody, disfluencies, university lectures.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-215"
  },
  "lintfert12_interspeech": {
   "authors": [
    [
     "Britta",
     "Lintfert"
    ],
    [
     "Bernd",
     "Möbius"
    ]
   ],
   "title": "Describing the development of intonational categories using a target-oriented parametric approach",
   "original": "i12_1965",
   "page_count": 4,
   "order": 219,
   "p1": "1965",
   "pn": "1968",
   "abstract": [
    "In this paper we analyze the relation between adults' intona- tional categories as described in the ToBI framework and chil- dren's intonation contours, using a parametric approach and cluster evaluation methods. In the field of prosody, an increas- ing number of studies on the development of intonation apply the intonational categories of adult speech described as a se- quence of high (H) and low (L) tones to child speech. However, the categories described by ToBI or by its language- specific variants are developed for adult speakers. Instead of imposing adult category representations on the description of de- velopmental stages in L1 intonation acquisition, we propose and validate a parametric approach and cluster evaluation methods that are applicable to both adult and child speech. First, we show that clusters of parametrized contours obtained from German adult-directed and child-directed speech correlate well with GToBI(S) categories. We then assess how well clusters at different stages of intonation acquisition correspond to adult tar- get categories. Our results indicate that the proposed methodol- ogy is capable of demonstrating, in qualitative and quantitative terms, the continuous development of intonational categories at early ages towards adult target categories.\n",
    "Index Terms: development of intonation, F0 parametrization, GToBI(S), clustering, adult targets\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-216"
  },
  "pohjalainen12b_interspeech": {
   "authors": [
    [
     "Jouni",
     "Pohjalainen"
    ],
    [
     "Tuomo",
     "Raitio"
    ],
    [
     "Hannu",
     "Pulakka"
    ],
    [
     "Paavo",
     "Alku"
    ]
   ],
   "title": "Automatic detection of high vocal effort in telephone speech",
   "original": "i12_0691",
   "page_count": 4,
   "order": 220,
   "p1": "691",
   "pn": "694",
   "abstract": [
    "A system is proposed for the automatic detection of high vocal effort in speech. The system is evaluated using both PCM-coded speech and AMRcoded telephone speech. In addition, the effect of far-end noise in the telephone conditions is studied using both matched-condition training and cases with additive noise mismatch. The proposed system is based on Bayesian classification of mel-frequency cepstral feature vectors. Concerning the MFCC feature extraction process, the substitution of a spectrum analysis method emphasizing the fine structure improves the results in the noisy cases.\n",
    "Index Terms: vocal effort detection, speech analysis\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-217"
  },
  "gomathi12_interspeech": {
   "authors": [
    [
     "D.",
     "Gomathi"
    ],
    [
     "Sathya Adithya",
     "Thati"
    ],
    [
     "Karthik Venkat",
     "Sridaran"
    ],
    [
     "Bayya",
     "Yegnanarayana"
    ]
   ],
   "title": "Analysis of mimicry speech",
   "original": "i12_0695",
   "page_count": 4,
   "order": 221,
   "p1": "695",
   "pn": "698",
   "abstract": [
    "In this paper, mimicry speech is analysed using features at suprasegmental, segmental and subsegmental levels. The possibility of the imitator getting close at each of these levels is examined here. The imitator cannot duplicate all features of the target, as imitation depends on the target speaker, utterance chosen, and his ability to imitate. To study the variation of features in the case of best and poor imitations, the source and system features are observed for different target speakers and for different utterances. Features such as pitch contour, duration, Itakura distance, strength of excitation and loudness measure are used for this analysis. Perceptual evaluation is performed to determine the closeness of imitation to the target. The closeness of features for best imitated and poorly imitated utterances is presented here.\n",
    "Index Terms: Mimicry, imitation, suprasegmental, segmental, and subsegmental features\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-218"
  },
  "kasess12_interspeech": {
   "authors": [
    [
     "Christian H.",
     "Kasess"
    ],
    [
     "Wolfgang",
     "Kreuzer"
    ],
    [
     "Ewald",
     "Enzinger"
    ],
    [
     "Nadja",
     "Kerschhofer-Puhalo"
    ]
   ],
   "title": "Estimation of the vocal tract shape of nasals using a Bayesian scheme",
   "original": "i12_0699",
   "page_count": 4,
   "order": 222,
   "p1": "699",
   "pn": "702",
   "abstract": [
    "For nasal stops and nasalized vowels, one-tube models offer only an inadequate representation. To model the spectral components of nasal speech signals, a minimum of two connected tubes is necessary. Typically, the estimation of branched-tube area functions is based on a polezero model. The present paper introduces a variational Bayesian scheme under Gaussian assumptions to estimate the tube areas directly from the log-spectrum of the speech signal. Probabilistic priors are used to enforce smoothness of the tubes. The method is tested on recorded tokens of /m/ from several speakers using different prior variances. Results show that mild smoothness assumptions yield the best results in terms of model error and marginal likelihood. Furthermore, while yielding comparable fits, the estimated reflection coefficients from the Bayesian scheme show less intra-subject variability between tokens than an unregularized non-linear solver.\n",
    "Index Terms: vocal tract, estimation, nasal stops, Bayesian statistics\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-219"
  },
  "birkholz12_interspeech": {
   "authors": [
    [
     "Peter",
     "Birkholz"
    ],
    [
     "Philippe",
     "Dächert"
    ],
    [
     "Christiane",
     "Neuschaefer-Rube"
    ]
   ],
   "title": "Advances in combined electro-optical palatography",
   "original": "i12_0703",
   "page_count": 4,
   "order": 223,
   "p1": "703",
   "pn": "706",
   "abstract": [
    "This paper describes the development of a device that combines the electropalatographic measurement of tongue-palate contact with optical distance sensing to measure the mid-sagittal contour of the tongue and the position of the lips. The device consists of a thin acrylic pseudopalate that contains both contact sensors and optical reflective sensors. Application areas are, for example, experimental phonetics, speech therapy, and silent speech interfaces. With regard to the latter, the prototype of the system was applied to the recognition of vowels from the sensor signals. It was shown that a classifier using the combined input data from both the contact sensors and the optical sensors had a higher recognition rate than classifiers based on only one type of sensory input.\n",
    "Index Terms: electropalatography, glossometry, silent speech interfaces\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-220"
  },
  "lee12c_interspeech": {
   "authors": [
    [
     "Byung Suk",
     "Lee"
    ],
    [
     "Daniel P. W.",
     "Ellis"
    ]
   ],
   "title": "Noise robust pitch tracking by subband autocorrelation classification",
   "original": "i12_0707",
   "page_count": 4,
   "order": 224,
   "p1": "707",
   "pn": "710",
   "abstract": [
    "Pitch tracking algorithms have a long history in various applications such as speech coding and extracting information, as well as other domains such as bioacoustics and music signal processing. While autocorrelation is a useful technique for detecting periodicity, autocorrelation peaks suffer ambiguity, leading to the classic \"octave error\" in pitch tracking. Moreover, additive noise can affect autocorrelation in ways that are difficult to model. Instead of explicitly using the most obvious features of autocorrelation, we present a trained classifier-based approach which we call Subband Autocorrelation Classification (SAcC). A multi-layer perceptron classifier is trained on the principal components of the autocorrelations of subbands from an auditory filterbank. Training on bandlimited and noisy speech (processed to simulate a low-quality radio channel) leads to a great increase in performance over state-of-the-art algorithms, according to both the traditional GPE measure, and a proposed novel Pitch Tracking Error which more fully reflects the accuracy of both pitch extraction and voicing detection in a single measure.\n",
    "Index Terms: speech, pitch tracking, machine learning, subband, autocorrelation, principal components\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-221"
  },
  "sepulveda12_interspeech": {
   "authors": [
    [
     "Alexander",
     "Sepulveda"
    ],
    [
     "Rodrigo",
     "Capobianco-Guido"
    ],
    [
     "German",
     "Castellanos-Dominguez"
    ]
   ],
   "title": "Inference of critical articulator position for fricative consonants",
   "original": "i12_0711",
   "page_count": 4,
   "order": 225,
   "p1": "711",
   "pn": "714",
   "abstract": [
    "Inversion aims to estimate the articulatory movements which support an acoustic speech signal. Within the acoustic-to-articulatory mapping framework, time frequency atoms had been also employed. The main focus of present work is estimating the relevant acoustic information, in terms of statistical association, for the inference of critical articulators position; in particular, those involved on production of fricatives. The chi2 information measure is used as the measure statistical dependence. The relevant time-frequency features are calculated for the MOCHA-TIMIT database, where the articulatory information is represented by trajectories of specific positions in the vocal tract. Relevant features are estimated on fricative phones, for which tongue tip and lower lip are known to be critical. The usefulness of the relevant maps is tested in an acoustic-to-articulatory mapping system based on Gaussian mixture models. In addition, it is shown that relevant features offer potential usefulness on solving the speaker-independent articulatory inversion problem.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-222"
  },
  "bruckl12_interspeech": {
   "authors": [
    [
     "Markus",
     "Brückl"
    ]
   ],
   "title": "Vocal tremor measurement based on autocorrelation of contours",
   "original": "i12_0715",
   "page_count": 4,
   "order": 226,
   "p1": "715",
   "pn": "718",
   "abstract": [
    "An algorithm to measure vocal tremor is presented, validated, and applied. The expected input is a sound file that captures a sustained phonation. The 6 output values are the frequency of frequency and amplitude tremor, intensity indices of frequency and amplitude tremor, and power indices of frequency and amplitude tremor. Basic principles of the algorithm are (1) autocorrelations of pitch and amplitude contours that are based on an autocorrelation of the input signal, (2) the correction for declination of (natural) contours as well as (3) a contour peakpicking and -averaging method for the determination of tremor intensities. The tremor power indices are new measures that weight tremor intensities by tremor frequency in order to receive bio- and psychologically more significant measures of tremor magnitude. The algorithm is implemented as a script of an open-source speech analysis program providing an most accurate pitch-detection (autocorrelation) method.\n",
    "Index Terms: acoustic voice analysis, vocal tremor, vibrato, modulation\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-223"
  },
  "hansakunbuntheung12_interspeech": {
   "authors": [
    [
     "Chatchawarn",
     "Hansakunbuntheung"
    ],
    [
     "Ananlada",
     "Chotimongkol"
    ],
    [
     "Sumonmas",
     "Thatphithakkul"
    ],
    [
     "Patcharika",
     "Chootrakool"
    ]
   ],
   "title": "Model-based duration-difference approach on accent evaluation of L2 learner",
   "original": "i12_0719",
   "page_count": 4,
   "order": 227,
   "p1": "719",
   "pn": "722",
   "abstract": [
    "This paper aims at using a model-based duration-difference approach to analyze L2 learners' duration-aspect accent, and, segmental duration characteristics. We use the durational differences deviated from native-English speech duration as an objective measure to evaluate the learner's timing characteristics. The use of model-based approach provides flexible evaluation without the need to collect any additional English reference speech. The proposed evaluation method was tested on English speech data uttered by native English speakers and Thainative English learners with different English-study experiences. The experimental results show speaker clusters grouped by English accents and L2 learners' English-study experiences. These results support the effectiveness of the proposed model-based objective evaluation.\n",
    "Index Terms: speech timing, quantitative evaluation, second language\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-224"
  },
  "hueber12_interspeech": {
   "authors": [
    [
     "Thomas",
     "Hueber"
    ],
    [
     "Gérard",
     "Bailly"
    ],
    [
     "Bruce",
     "Denby"
    ]
   ],
   "title": "Continuous articulatory-to-acoustic mapping using phone-based trajectory HMM for a silent speech interface",
   "original": "i12_0723",
   "page_count": 4,
   "order": 228,
   "p1": "723",
   "pn": "726",
   "abstract": [
    "The article presents an HMM-based mapping approach for converting ultrasound and video images of the vocal tract into an audible speech signal, for a silent speech interface application. The proposed technique is based on the joint modeling of articulatory and spectral features, for each phonetic class, using Hidden Markov Models (HMM) and multivariate Gaussian distributions with full covariance matrices. The articulatoryto- acoustic mapping is achieved in 2 steps: 1) finding the most likely HMM state sequence from the articulatory observations; 2) inferring the spectral trajectories from both the decoded state sequence and the articulatory observations. The proposed technique is compared to our previous approach, in which only the decoded state sequence was used for the inference of the spectral trajectories, independently from the articulatory observations. Both objective and perceptual evaluations show that this new approach leads to a better estimation of the spectral trajectories.\n",
    "Index Terms: silent speech interface, handicap, HMM-based speech synthesis, audiovisual speech processing\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-225"
  },
  "kawahara12b_interspeech": {
   "authors": [
    [
     "Tatsuya",
     "Kawahara"
    ],
    [
     "Takuma",
     "Iwatate"
    ],
    [
     "Katsuya",
     "Takanashi"
    ]
   ],
   "title": "Prediction of turn-taking by combining prosodic and eye-gaze information in poster conversations",
   "original": "i12_0727",
   "page_count": 4,
   "order": 229,
   "p1": "727",
   "pn": "730",
   "abstract": [
    "We investigate turn-taking behaviors in conversations in poster sessions. While the poster presenter holds most of the turns during sessions, the audience's utterances are more important and should not be missed. In this paper, therefore, prediction of turn-taking by the audience is addressed. It is classified into two sub-tasks: prediction of speaker change and prediction of the next speaker. We made analysis on eye-gaze information and its relationship with turn-taking, introducing joint eye-gaze events by the presenter and audience. We also parameterize backchannel patterns of the audience. As a result of machine learning with these features, it is found that combination of prosodic features of the presenter and the joint eye-gaze features is effective for predicting speaker change, while eye-gaze duration and backchannels preceding the speaker change are useful for predicting the next speaker among the audience.\n",
    "Index Terms: multi-party interaction, turn-taking, prosody, eye-gaze\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-226"
  },
  "wechsung12_interspeech": {
   "authors": [
    [
     "Ina",
     "Wechsung"
    ],
    [
     "Klaus-Peter",
     "Engelbrecht"
    ],
    [
     "Sebastian",
     "Möller"
    ]
   ],
   "title": "Using quality ratings to predict modality choice in multimodal systems",
   "original": "i12_0731",
   "page_count": 4,
   "order": 230,
   "p1": "731",
   "pn": "734",
   "abstract": [
    "A standardized procedure to evaluate the perceived quality of multimodal systems is still lacking. Previous research has however shown that the quality ratings for a multimodal system are equal to the weighted sum of the quality ratings of its individual modalities, with the modality that is more frequently used having a stronger influence. These findings suggest, that if the choice of modality can be predicted, an estimation of the quality of the multimodal systems is possible based solely on an evaluation of its component modalities. Accordingly, the current study investigates the prediction of modality choice based on quality ratings of the component modalities in order to achieve accurate quality predictions for multimodal systems. It is shown that predictions of modality choice as well as the overall system quality are possible. Furthermore, an age effects is observed: If older adults are included predictions are less precise.\n",
    "Index Terms: Multimodal systems, evaluation, prediction\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-227"
  },
  "fang12_interspeech": {
   "authors": [
    [
     "Fuming",
     "Fang"
    ],
    [
     "Takahiro",
     "Shinozaki"
    ],
    [
     "Yasuo",
     "Horiuchi"
    ],
    [
     "Shingo",
     "Kuroiwa"
    ],
    [
     "Sadaoki",
     "Furui"
    ],
    [
     "Toshimitsu",
     "Musha"
    ]
   ],
   "title": "HMM based continuous EOG recognition for eye-input speech interface",
   "original": "i12_0735",
   "page_count": 4,
   "order": 231,
   "p1": "735",
   "pn": "738",
   "abstract": [
    "To provide an efficient means of communication for those who cannot move muscles of the whole body except eyes due to amyotrophic lateral sclerosis (ALS), we are developing a speech synthesis interface that is based on electrooculogram (EOG) input. EOG is an electrical signal that is observed through electrodes attached on the skin around eyes and reflects eye position. A key component of the system is a continuous recognizer for the EOG signal. In this paper, we propose and investigate a hidden Markov model (HMM) based EOG recognizer applying continuous speech recognition techniques. In the experiments, we evaluate the recognition system both in user dependent and independent conditions. It is shown that 96.1% of recognition accuracy is obtained for five classes of eye actions by a user dependent system using six channels. While it is difficult to obtain good performance by a user independent system, it is shown that maximum likelihood linear regression (MLLR) adaptation helps for EOG recognition.\n",
    "Index Terms: electrooculogram, hidden Markov model, amyotrophic lateral sclerosis, continuous speech recognition, maximum likelihood linear regression\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-228"
  },
  "lilley12_interspeech": {
   "authors": [
    [
     "Jason",
     "Lilley"
    ],
    [
     "Amanda",
     "Stent"
    ],
    [
     "Ilija",
     "Zeljkovic"
    ]
   ],
   "title": "A random, semantically appropriate sentence generator for speaker verification",
   "original": "i12_0739",
   "page_count": 4,
   "order": 232,
   "p1": "739",
   "pn": "742",
   "abstract": [
    "In this paper, we describe two systems for automatically generating English sentences, and evaluate the suitability of their output for speaker verification. The first system, SUSGen, generates grammatical but semantically anomalous sentences of controlled length, vocabulary and phonetic content. The second system, SASGen, extends SUSGen to generate a greater variety of sentences and ones which are, for the most part, semantically acceptable. We demonstrate that sentences generated by SASGen are significantly more readable and meaningful than those generated by SUSGen. While sentences generated by SASGen were not judged to be as readable or meaningful as human-generated sentences, the additional control SASGen provides for sentence length, vocabulary and phonetic content make it more suitable for speaker verification and other voice collection purposes.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-229"
  },
  "maciasgalindo12_interspeech": {
   "authors": [
    [
     "Daniel",
     "Macias-Galindo"
    ],
    [
     "Wilson",
     "Wong"
    ],
    [
     "Lawrence",
     "Cavedon"
    ],
    [
     "John",
     "Thangarajah"
    ]
   ],
   "title": "Coherent topic transition in a conversational agent",
   "original": "i12_0743",
   "page_count": 4,
   "order": 233,
   "p1": "743",
   "pn": "746",
   "abstract": [
    "A conversational agent for entertainment and engagement requires the ability to maintain coherent conversations. We describe the use of semantic relatedness to select the next conversational fragment that an agent utters, to maximise dialogue coherence or to possibly suggest new directions for a dialogue. We compare our approach, using a specific semantic relatedness metric, to an existing nearest-context mechanism based on TF❌IDF for selecting fragments to continue a conversation. Evaluation with human judges shows that use of semantic relatedness provides improved coherence across a sample collection of generated conversations.\n",
    "Index Terms: Spoken dialogue, dialogue coherence, semantic relatedness\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-230"
  },
  "heeman12_interspeech": {
   "authors": [
    [
     "Peter A.",
     "Heeman"
    ],
    [
     "Jordan",
     "Fryer"
    ],
    [
     "Rebecca",
     "Lunsford"
    ],
    [
     "Andrew",
     "Rueckert"
    ],
    [
     "Ethan",
     "Selfridge"
    ]
   ],
   "title": "Using reinforcement learning for dialogue management Policies: towards understanding MDP violations and convergence",
   "original": "i12_0747",
   "page_count": 4,
   "order": 234,
   "p1": "747",
   "pn": "750",
   "abstract": [
    "Reinforcement learning is becoming a popular tool for building dialogue managers. This paper addresses two issues in using RL. First, we propose two methods for finding MDP violations. Both methods make use of computing Q scores when testing the policy. Second, we investigate how convergence happens. To do this, we use a dialogue task in which the only source of variability is the dialogue policy itself. This allows us to study how and when convergence happens as training progresses. The work in this paper should help dialogue designers build effective policies and understand how much training is necessary.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-231"
  },
  "lopezcozar12_interspeech": {
   "authors": [
    [
     "Ramón",
     "López-Cózar"
    ],
    [
     "Zoraida",
     "Callejas"
    ],
    [
     "David",
     "Griol"
    ]
   ],
   "title": "Enhancing speech understanding in spoken dialogue systems by means of a new frame-correction technique",
   "original": "i12_0751",
   "page_count": 4,
   "order": 235,
   "p1": "751",
   "pn": "754",
   "abstract": [
    "This paper proposes a new technique to enhance speech understanding in spoken dialogue systems, which aims to replace semantic frames incorrectly generated by the systems with the correct ones. To do so, it relies on a training procedure that takes into account previous system misunderstandings for each dialogue state. Experiments have been carried out employing two systems (Saplen and Viajero) previously developed in our lab, which employ a prompt-independent language model and several prompt-dependent language models for ASR. The results show that the technique enhances system performance for both kinds of language model, especially for the prompt-independent language model. Using this technique, Saplen increases sentence understanding by 19.54%, task completion by 26.25% and word accuracy by 7.53%, whereas for Viajero these figures increase by 14.93%, 18.06% and 6.98%, respectively.\n",
    "Index Terms: Spoken dialogue systems, speech recognition, speech understanding, dialogue management\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-232"
  },
  "litman12_interspeech": {
   "authors": [
    [
     "Diane",
     "Litman"
    ],
    [
     "Heather",
     "Friedberg"
    ],
    [
     "Kate",
     "Forbes-Riley"
    ]
   ],
   "title": "Prosodic cues to disengagement and uncertainty in physics tutorial dialogues",
   "original": "i12_0755",
   "page_count": 4,
   "order": 236,
   "p1": "755",
   "pn": "758",
   "abstract": [
    "This paper focuses on the analysis and prediction of student disengagement and uncertainty, using a corpus of dialogues collected with a spoken tutorial dialogue system in the STEM domain of qualitative physics. We first compare and contrast the prosodic characteristics of dialogue turns exhibiting disengagement or not, and those exhibiting uncertainty or not. We then compare the utility of using multiple prosodic features to predict both disengagement and uncertainty.\n",
    "Index Terms: spoken dialogue systems, educational applications, emotion detection, prosody\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-233"
  },
  "ward12_interspeech": {
   "authors": [
    [
     "Wayne H.",
     "Ward"
    ],
    [
     "Daniel",
     "Bolanos"
    ],
    [
     "Ronald A.",
     "Cole"
    ]
   ],
   "title": "Spoken dialogs with a virtual science tutor",
   "original": "i12_0759",
   "page_count": 4,
   "order": 237,
   "p1": "759",
   "pn": "762",
   "abstract": [
    "My Science Tutor (MyST) is an intelligent tutoring system designed to improve science learning by elementary school students through conversational dialogs with a virtual science tutor in an interactive multimedia environment. Marni, a lifelike 3-D character, attempts to elicit self-expression from students, process their spoken explanations to assess understanding, and scaffold learning by asking open-ended questions accompanied by illustrations, animations or interactive simulations. MyST uses automatic speech recognition, natural language processing and dialog modeling technologies to interpret student responses and manage the dialog.\n",
    "Index Terms: spoken dialog, virtual tutors\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-234"
  },
  "cerva12_interspeech": {
   "authors": [
    [
     "Petr",
     "Cerva"
    ],
    [
     "Jan",
     "Silovsky"
    ],
    [
     "Jindrich",
     "Zdansky"
    ],
    [
     "Jan",
     "Nouza"
    ],
    [
     "Jiri",
     "Malek"
    ]
   ],
   "title": "Real-time lecture transcription using ASR for Czech hearing impaired or deaf students",
   "original": "i12_0763",
   "page_count": 4,
   "order": 238,
   "p1": "763",
   "pn": "766",
   "abstract": [
    "This paper describes a client-server system developed to enable hearing impaired persons to participate in lectures by providing real-time displayed transcripts. The core of this system is formed by an ASR module running on a recognition server and processing the input audio-video stream. This engine utilizes a large lexicon, topic-specific language models mixed properly from various sources (e.g. transcripts of spontaneous utterances, theses, web discussions) and unsupervised incremental speaker adaptation methods to cope with spontaneous lecture speech in highly inflective Czech language. The raw output of the ASR module is converted into a more readable form using a developed post-processing module based on finite state transducers. The resulting formated text (i.e. containing punctuation marks, digit strings, etc) is then displayed on the screen of each client device (e.g. notebook or tablet) in the lecture room.\n",
    "Index Terms: real-time lecture transcription, applications for handicapped persons, applications in learning\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-235"
  },
  "chen12c_interspeech": {
   "authors": [
    [
     "Lei",
     "Chen"
    ],
    [
     "Su-Youn",
     "Yoon"
    ]
   ],
   "title": "Application of structural events detected on ASR outputs for automated speaking assessment",
   "original": "i12_0767",
   "page_count": 4,
   "order": 239,
   "p1": "767",
   "pn": "770",
   "abstract": [
    "We investigated the features reflecting utterance structure and disfluency profile to improve the automated scoring of spontaneous speech responses by non-native speakers of English. On both human annotated structural events (SEs), e.g., clause structure and disfluencies, and automatically detected SEs on speech transcriptions, several features were derived and showed promisingly high correlations to the human proficiency scores. However, the usefulness of these SE-derived features on ASR hypotheses was still unknown.   In this paper, we reported our studies related to the detection of SEs from noisy ASR outputs and the application of the detected SEs for automated speech scoring. We found that clause boundary (CB) detection was impacted much less compared to interruption point (IP) (of speech disfluencies) detection when facing ASR errors. Next, several features derived from the detected SEs were evaluated by considering their correlation to human scores and their relative importance in a linear regression model.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-236"
  },
  "saz12_interspeech": {
   "authors": [
    [
     "Oscar",
     "Saz"
    ],
    [
     "Maxine",
     "Eskenazi"
    ]
   ],
   "title": "Addressing confusions in spoken language in ESL pronunciation tutors",
   "original": "i12_0771",
   "page_count": 4,
   "order": 240,
   "p1": "771",
   "pn": "774",
   "abstract": [
    "This paper presents a new approach for developing pronunciation tutors in Second Language (L2) learning. Applying the Basic Identification of Confusable Contexts (BICC) procedure we automatically generate curriculum that is rich in possible confusion contexts which can be practiced by L2 students in read-aloud tasks. This is the basis of a new pronunciation tutor where the student interacts orally in a scripted conversation with a virtual avatar. The student's utterances are automatically evaluated to detect mispronunciations and the tutor provides feedback on the errors which produce a confusion in the message. An assessment was carried out with a group of English as Second Language (ESL) speakers to evaluate the tutor interface as well as to determine their motivation and engagement in practicing English with it.\n",
    "Index Terms: pronunciation, intelligent tutors, error detection\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-237"
  },
  "qian12c_interspeech": {
   "authors": [
    [
     "Xiaojun",
     "Qian"
    ],
    [
     "Helen",
     "Meng"
    ],
    [
     "Frank K.",
     "Soong"
    ]
   ],
   "title": "The use of DBN-HMMs for mispronunciation detection and diagnosis in L2 English to support computer-aided pronunciation training",
   "original": "i12_0775",
   "page_count": 4,
   "order": 241,
   "p1": "775",
   "pn": "778",
   "abstract": [
    "This paper investigates acoustic modeling using the hybrid DBN-HMM framework in mispronunciation detection and diagnosis of L2 English. This is one of the first efforts that compare the performance of DBN-HMM with that of the best-tuned GMM-HMM trained in ML and MWE on the same set of features. Previous work in ASR has also shown the necessity of unsupervised pre-training for DBNs to work well. We explore further the effect of training our ASR engine in an unsupervised manner with additional unannotated L2 data from the test speakers. This is compared with the original ASR that has been trained with annotated data in a supervised manner. Experiments show that DBN-HMM can give significant improvement (between 13-18% relative in word pronunciation error rate) but is computationally more expensive.\n",
    "Index Terms: mispronunciation detection and diagnosis, restricted boltzmann machine, deep belief network\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-238"
  },
  "cucchiarini12_interspeech": {
   "authors": [
    [
     "Catia",
     "Cucchiarini"
    ],
    [
     "Joost van",
     "Doremalen"
    ],
    [
     "Helmer",
     "Strik"
    ]
   ],
   "title": "Practice and feedback in L2 speaking: an evaluation of the DISCO CALL system",
   "original": "i12_0779",
   "page_count": 4,
   "order": 242,
   "p1": "779",
   "pn": "782",
   "abstract": [
    "In this paper we report on the ASR-based CALL system DISCO: Development and Integration of Speech technology into COurseware for language learning. The DISCO system automatically detects pronunciation and grammar errors in Dutch L2 speaking and generates appropriate, detailed feedback on the errors detected. We briefly introduce DISCO and present the results of a first evaluation of the complete system.\n",
    "Index Terms: Computer Assisted Language Learning, ASR, speaking proficiency\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-239"
  },
  "hueber12b_interspeech": {
   "authors": [
    [
     "Thomas",
     "Hueber"
    ],
    [
     "Atef",
     "Ben-Youssef"
    ],
    [
     "Gérard",
     "Bailly"
    ],
    [
     "Pierre",
     "Badin"
    ],
    [
     "Frédéric",
     "Elisei"
    ]
   ],
   "title": "Cross-speaker acoustic-to-articulatory inversion using phone-based trajectory HMM for pronunciation training",
   "original": "i12_0783",
   "page_count": 4,
   "order": 243,
   "p1": "783",
   "pn": "786",
   "abstract": [
    "The article presents a statistical mapping approach for cross-speaker acoustic-to-articulatory inversion. The goal is to estimate the most likely articulatory trajectories for a reference speaker from the speech audio signal of another speaker. This approach is developed in the framework of our system of visual articulatory feedback developed for computer-assisted pronunciation training applications (CAPT). The proposed technique is based on the joint modeling of articulatory and acoustic features, for each phonetic class, using full-covariance trajectory HMM. The acousticto- articulatory inversion is achieved in 2 steps: 1) finding the most likely HMM state sequence from the acoustic observations; 2) inferring the articulatory trajectories from both the decoded state sequence and the acoustic observations. The problem of speaker adaptation is addressed using a voice conversion approach, based on trajectory GMM.\n",
    "Index Terms: acoustic-to-articulatory inversion, intelligent tutoring systems, pronunciation training, trajectory HMM, voice conversion, talking head\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-240"
  },
  "kintzley12_interspeech": {
   "authors": [
    [
     "Keith",
     "Kintzley"
    ],
    [
     "Aren",
     "Jansen"
    ],
    [
     "Hynek",
     "Hermansky"
    ]
   ],
   "title": "MAP estimation of whole-word acoustic models with dictionary priors",
   "original": "i12_0787",
   "page_count": 4,
   "order": 244,
   "p1": "787",
   "pn": "790",
   "abstract": [
    "The intrinsic advantages of whole-word acoustic modeling are offset by the problem of data sparsity. To address this, we present several parametric approaches to estimating intra-word phonetic timing models under the assumption that relative timing is independent of word duration. We show evidence that the timing of phonetic events is well described by the Gaussian distribution. We explore the construction of models in the absence of keyword examples (dictionary-based), when keyword examples are abundant (Gaussian mixture models), and also present a Bayesian approach which unifies the two. Applying these techniques in a point process model keyword spotting framework, we demonstrate a 55% relative improvement in performance for models constructed from few examples.\n",
    "Index Terms: phonetic timing, whole-word modeling, keyword spotting, point process model\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-241"
  },
  "thomas12_interspeech": {
   "authors": [
    [
     "Samuel",
     "Thomas"
    ],
    [
     "Sriram",
     "Ganapathy"
    ],
    [
     "Aren",
     "Jansen"
    ],
    [
     "Hynek",
     "Hermansky"
    ]
   ],
   "title": "Data-driven posterior features for low resource speech recognition applications",
   "original": "i12_0791",
   "page_count": 4,
   "order": 245,
   "p1": "791",
   "pn": "794",
   "abstract": [
    "In low resource settings, with very few hours of training data,state-of-the-art speech recognition systems that require large amounts of task specific training data perform very poorly. We address this issue by building data-driven speech recognition front-ends on significant amounts of task independent data from different languages and genres collected in similar acoustic conditions as data provided in the low resource scenario. We show that features derived from these trained front-ends perform significantly better and can alleviate the effect of reduced task specific training data in low resource settings. The proposed features provide a absolute improvement of about 12% (18% relative) in an low-resource LVCSR setting with only one hour of training data. We also demonstrate the usefulness of these features for zero-resource speech applications like spoken term discovery, which operate without any transcribed speech to train systems. The proposed features provide significant gains over conventional acoustic features on various information retrieval metrics for this task.\n",
    "Index Terms: Low-resource speech recognition, spoken term discovery, posterior features.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-242"
  },
  "cui12_interspeech": {
   "authors": [
    [
     "Xiaodong",
     "Cui"
    ],
    [
     "Mohamed",
     "Afify"
    ],
    [
     "George",
     "Saon"
    ],
    [
     "Vaibhava",
     "Goel"
    ]
   ],
   "title": "Sparse Bayesian factor analysis for stereo-based stochastic mapping",
   "original": "i12_0795",
   "page_count": 4,
   "order": 246,
   "p1": "795",
   "pn": "798",
   "abstract": [
    "This paper investigates a factor analysis scheme in the joint channel space of stereo-based stochastic mapping (SSM) for noise robust automatic speech recognition. A mixture of Bayesian factor analyzers is used to describe the generative factors in the multi-conditional training scenario in terms of noise type and signal-to-noise ratio. Sparsity-promoting prior is applied on the matrix of factor loadings to automatically learn the effective factors from a redundant dictionary in a particular soft cluster. Experiments carried out on large vocabulary continuous speech recognition tasks show that this sparse Bayesian factor analysis scheme leads to superior SSM performance for noise robustness.\n",
    "Index Terms: Bayesian factor analysis, sparsity learning, stereo-based stochastic mapping, noise robust automatic speech recognition\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-243"
  },
  "vanhainen12_interspeech": {
   "authors": [
    [
     "Niklas",
     "Vanhainen"
    ],
    [
     "Giampiero",
     "Salvi"
    ]
   ],
   "title": "Word discovery with beta process factor analysis",
   "original": "i12_0799",
   "page_count": 4,
   "order": 247,
   "p1": "799",
   "pn": "802",
   "abstract": [
    "We propose the application of a recently developed non-parametric Bayesian method for factor analysis to the problem of word discovery from continuous speech. The method, based on Beta Process priors, has a number of advantages compared to previously proposed methods, such as Non-negative Matrix Factorisation (NMF). Beta Process Factor Analysis (BPFA) is able to estimate the size of the basis, and therefore the number of recurring patterns, or word candidates, found in the data. We compare the results obtained with BPFA and NMF on the TIDigits database, showing that our method is capable of not only finding the correct words, but also the correct number of words. We also show that the method can infer the approximate number of words for different vocabulary sizes by testing on randomly generated sequences of words.\n",
    "Index Terms: word discovery, beta process factor analysis, Bayesian nonparametric method, non-negative matrix factorisation\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-244"
  },
  "hahm12_interspeech": {
   "authors": [
    [
     "Seong-Jun",
     "Hahm"
    ],
    [
     "Atsunori",
     "Ogawa"
    ],
    [
     "Masakiyo",
     "Fujimoto"
    ],
    [
     "Takaaki",
     "Hori"
    ],
    [
     "Atsushi",
     "Nakamura"
    ]
   ],
   "title": "Speaker adaptation using variational Bayesian linear regression in normalized feature space",
   "original": "i12_0803",
   "page_count": 4,
   "order": 248,
   "p1": "803",
   "pn": "806",
   "abstract": [
    "The purpose of this paper is tuning-free SMAPLR approach. The one of the important issues of SMAPLR approach is deciding occupancy threshold for the tree structure according to the number of adaptation data and control parameter for the hierarchical prior setting for appropriately incorporating prior information. For that purpose, we employ variational Bayesian linear regression (VBLR) approach. VBLR uses variational lower bound as an objective function. Using variational lower bound, model structure and contribution of the prior information can be decided without deciding experimentally. In this paper, we employ the VBLR adaptation in normalized feature-space. We first perform the feature space SMAPLR (fSMAPLR) to normalize the feature space. Then, VBLR is performed in previously normalized feature space. Experiments on large vocabulary continuous speech recognition using the Corpus of Spontaneous Japanese (CSJ) corpus confirm the effectiveness of the proposed method compared with other conventional adaptation methods.\n",
    "Index Terms: speaker adaptation, SMAPLR, VBLR, normalized feature space\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-245"
  },
  "krueger12_interspeech": {
   "authors": [
    [
     "Alexander",
     "Krueger"
    ],
    [
     "Oliver",
     "Walter"
    ],
    [
     "Volker",
     "Leutnant"
    ],
    [
     "Reinhold",
     "Haeb-Umbach"
    ]
   ],
   "title": "Bayesian feature enhancement for ASR of noisy reverberant real-world data",
   "original": "i12_0807",
   "page_count": 4,
   "order": 249,
   "p1": "807",
   "pn": "810",
   "abstract": [
    "In this contribution we investigate the effectiveness of Bayesian feature enhancement (BFE) on a medium-sized recognition task containing real-world recordings of noisy reverberant speech. BFE employs a very coarse model of the acoustic impulse response (AIR) from the source to the microphone, which has been shown to be effective if the speech to be recognized has been generated by artificially convolving nonreverberant speech with a constant AIR. Here we demonstrate that the model is also appropriate to be used in feature enhancement of true recordings of noisy reverberant speech. On the Multi-Channel Wall Street Journal Audio Visual corpus (MC-WSJ-AV) the word error rate is cut in half to 41.9% compared to the ETSI Standard Front-End using as input the signal of a single distant microphone with a single recognition pass.\n",
    "Index Terms: bayesian feature enhancement, dereverberation, denoising\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-246"
  },
  "yilmaz12_interspeech": {
   "authors": [
    [
     "Emre",
     "Yilmaz"
    ],
    [
     "Dirk van",
     "Compernolle"
    ],
    [
     "Hugo",
     "Van hamme"
    ]
   ],
   "title": "Robust tracking for automatic reading tutors",
   "original": "i12_0811",
   "page_count": 4,
   "order": 250,
   "p1": "811",
   "pn": "814",
   "abstract": [
    "Reading tutor software uses automatic speech recognition technology to support children in developing their reading skills. In many forms of exercise and evaluation, tracking the reading position is a relevant task or even a prerequisite, e.g. to provide assistance on the pronunciation of a word or to advance the screen to the next page. In this paper, we introduce a new robust tracking algorithm, which measures the similarity between the recognized phones and the phonetic transcription of words displayed on a screen using an efficient dynamic programming algorithm. The criteria for accepting a word reading attempt and thus advancing the cursor can hence be expressed phonetically. In addition, the most likely state of the Hidden Markov Model (HMM) used to decode the speech serves as a fallback for cases of phone matching failure. The new tracker's performance is compared with two other trackers which use either the most likely HMM state or phone matching. The evaluation metrics quantify both the frequency of timely movements and loss of tracking synchronicity. The proposed approach performs significantly better than the others achieving a Timing Accuracy of Tracking of 81.03% compared to 50.63% of the phone matching approach and 32.36% of the state-based approach.\n",
    "Index Terms: automatic reading tutors, reading position tracking, tracking evaluation\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-247"
  },
  "huang12c_interspeech": {
   "authors": [
    [
     "Hao",
     "Huang"
    ],
    [
     "Jianming",
     "Wang"
    ],
    [
     "Halidan",
     "Abudureyimu"
    ]
   ],
   "title": "Maximum F1-score discriminative training for automatic mispronunciation detection in computer-assisted language learning",
   "original": "i12_0815",
   "page_count": 4,
   "order": 251,
   "p1": "815",
   "pn": "818",
   "abstract": [
    "In this paper, we propose and evaluate a novel discriminative training criterion for hidden Markov model (HMM) based automatic mispronunciation detection in computer-assisted pronunciation training. The objective function is formulated as a smooth form of the F1- score on the annotated non-native speech database. The objective function maximization is achieved by using extended Baum Welch form like HMM updating equations based on the weak-sense auxiliary function method. Simultaneous updating of acoustic model and phone threshold parameters is proposed to ensure objective improvement. Mispronunciation detection experiments have shown the method is effective in increasing the F1-score, Precision, Recall and detection accuracy on both the training data and evaluation data.\n",
    "Index Terms: automatic mispronunciation detection, F1-score, discriminative training, computer-assisted language learning\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-248"
  },
  "wang12c_interspeech": {
   "authors": [
    [
     "Yow-Bang",
     "Wang"
    ],
    [
     "Lin-Shan",
     "Lee"
    ]
   ],
   "title": "Error pattern detection integrating generative and discriminative learning for computer-aided pronunciation training",
   "original": "i12_0819",
   "page_count": 4,
   "order": 252,
   "p1": "819",
   "pn": "822",
   "abstract": [
    "Computer-aided language learning tries to have computers serve as virtual language tutors to help people in learning non-native languages in the globalized world nowadays. In this paper we propose a framework to incorporate specially designed discriminative models with carefully trained generative models for the task of pronunciation error pattern detection. For each phoneme we train one or more SVMs with varying targets and different weights to integrate with HMM/GMMs for optimizing the detection performance from different aspects. Experiments show this integration framework effectively enhance mispronunciation detection performance.\n",
    "Index Terms: Error Pattern, Viterbi Decoding, Discriminative Score, SVM\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-249"
  },
  "honig12_interspeech": {
   "authors": [
    [
     "Florian",
     "Hönig"
    ],
    [
     "Tobias",
     "Bocklet"
    ],
    [
     "Korbinian",
     "Riedhammer"
    ],
    [
     "Anton",
     "Batliner"
    ],
    [
     "Elmar",
     "Nöth"
    ]
   ],
   "title": "The automatic assessment of non-native prosody: combining classical prosodic analysis with acoustic modelling",
   "original": "i12_0823",
   "page_count": 4,
   "order": 253,
   "p1": "823",
   "pn": "826",
   "abstract": [
    "In earlier studies, we assessed the degree of non-nativeness employing prosodic information. In this paper, we combine prosodic information with (1) features derived from a Gaussian Mixture Model used as Universal Background Model (GMM-UBM), a powerful approach used in speaker identification, and (2) openSMILE, a standard open-source toolkit for extracting acoustic features. We evaluate our approach with English speech from 94 non-native speakers. GMM-UBM or openSMILE modelling alone yields lower performance than our prosodic feature vector; however, adding information from the GMM-UBM modelling or openSMILE by late fusion improves results.\n",
    "Index Terms: computer-assisted language learning, non-native prosody, rhythm, automatic assessment\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-250"
  },
  "stanley12_interspeech": {
   "authors": [
    [
     "Theban",
     "Stanley"
    ],
    [
     "Kadri",
     "Hacioglu"
    ]
   ],
   "title": "Improving L1-specific phonological error diagnosis in computer assisted pronunciation training",
   "original": "i12_0827",
   "page_count": 4,
   "order": 254,
   "p1": "827",
   "pn": "830",
   "abstract": [
    "With the increasing use of technology in classrooms, computer assisted pronunciation training (CAPT) is becoming a vital tool in language learning. In this paper, we present a system that takes advantage of data from learners of a specific L1 to better model phonological errors at various levels in the system. At the lexical level, a statistical machine translation approach is used to model common phonological errors produced by a specific L1 population. At the acoustic level, L1-dependent maximum likelihood (ML) nonnative models and discriminative training are explored. In our experiments, use of a Korean language dependent nonnative lexicon gives us diagnostic abilities that did not exist in our baseline configuration. Replacing the native ML acoustic model with the L1-dependent nonnative model produces relative improvements of 27.37% in precision for phone detection/identification tasks. We also propose a constrained variant of minimum phone error (MPE) training which is better adapted to phone detection/diagnosis. This technique produces 5.6% relative improvement in precision in comparison to ML nonnative acoustic models.\n",
    "Index Terms: language learning, phonological error modeling, machine translation, minimum phone error training\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-251"
  },
  "gemmeke12_interspeech": {
   "authors": [
    [
     "Jort F.",
     "Gemmeke"
    ],
    [
     "Janneke van de",
     "Loo"
    ],
    [
     "Guy de",
     "Pauw"
    ],
    [
     "Joris",
     "Driesen"
    ],
    [
     "Hugo",
     "Van hamme"
    ],
    [
     "Walter",
     "Daelemans"
    ]
   ],
   "title": "A self-learning assistive vocal interface based on vocabulary learning and grammar induction",
   "original": "i12_0831",
   "page_count": 4,
   "order": 255,
   "p1": "831",
   "pn": "834",
   "abstract": [
    "This paper introduces research within the ALADIN project, which aims to develop an assistive vocal interface for people with a physical impairment. In contrast to existing approaches, the vocal interface is self-learning which means it can be used with any language, dialect, vocabulary and grammar. The paper describes the overall learning framework, and the two components that will provide vocabulary learning and grammar induction. In addition, the paper describes encouraging results of early implementations of these vocabulary and grammar learning components, applied to recorded sessions of a vocally guided card game, patience.\n",
    "Index Terms: language acquisition, word finding, grammar induction, non-negative matrix factorization, concept tagging\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-252"
  },
  "iribe12_interspeech": {
   "authors": [
    [
     "Yurie",
     "Iribe"
    ],
    [
     "Takurou",
     "Mori"
    ],
    [
     "Kouichi",
     "Katsurada"
    ],
    [
     "Goh",
     "Kawai"
    ],
    [
     "Tsuneo",
     "Nitta"
    ]
   ],
   "title": "Real-time visualization of English pronunciation on an IPA chart based on articulatory feature extraction",
   "original": "i12_1271",
   "page_count": 4,
   "order": 256,
   "p1": "1271",
   "pn": "1274",
   "abstract": [
    "In recent years, Computer Assisted Pronunciation Technology (CAPT) systems have been developed that can help Japanese learners to study foreign languages. We have been developing a pronunciation training system to evaluate and correct learner's pronunciation by extracting articulatory-features (AFs). In this paper, we propose a novel pronunciation training system that can plot the place and manner of articulation of learner's pronunciation on an International Phonetic Alphabet (IPA) chart in real time. First, the proposed system converts input speech into AF-sequences by using multi-layer neural networks (MLNs). Then, the AF-sequences are converted into x-y coordinates and plotted on an IPA chart to show his/her articulation in real time. Lastly, we investigate plotting accuracies on the IPA chart through experimental evaluation.\n",
    "Index Terms: pronunciation training, articulatory feature, IPA chart\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-253"
  },
  "jeon12_interspeech": {
   "authors": [
    [
     "Je Hun",
     "Jeon"
    ],
    [
     "Su-Youn",
     "Yoon"
    ]
   ],
   "title": "Acoustic feature-based non-scorable response detection for an automated speaking proficiency assessment",
   "original": "i12_1275",
   "page_count": 4,
   "order": 257,
   "p1": "1275",
   "pn": "1278",
   "abstract": [
    "This study provides a method that increases the robustness of automated speech scoring. Responses with sub-optimal characteristics such as background noises, volume problems, non-English speech, whispered speech, and non-responses make automated scoring more difficult. For instance, loud background noises distort the spectral characteristics of speech, and the performance of the prosody and pronunciation features are significantly degraded. Finally, the automated scores of these responses become less reliable.   In order to address this problem, the automated scoring system in this study first filters out non-scorable responses using a filtering model and then predicts the proficiency scores of the remaining responses using a scoring model. In addition to automatic speech recognition-based (ASR) filter, which demonstrated promising performances in previous studies, a new filter was implemented in this study using acoustic features. The acoustic-based filter achieved a comparable performance to the ASR-based filter, and the combination of the two models achieved further improvement. The combined filter was evaluated on two actual test products and it achieved an accuracy rate of over 98% with an F-score of 86%.\n",
    "Index Terms: automated speech scoring, speech recognition, acoustic features, filtering models, scorable responses\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-254"
  },
  "wuth12_interspeech": {
   "authors": [
    [
     "Jorge",
     "Wuth"
    ],
    [
     "Néstor Becerra",
     "Yoma"
    ],
    [
     "Leopoldo",
     "Benavides"
    ],
    [
     "Hiram",
     "Vivanco"
    ]
   ],
   "title": "Pronunciation quality evaluation of sentences by combining word based scores",
   "original": "i12_1279",
   "page_count": 4,
   "order": 258,
   "p1": "1279",
   "pn": "1282",
   "abstract": [
    "The problem of pronunciation evaluation of sentences is defined as the combination of word based subjective pronunciation scores. The mean subjective word score criterion is proposed and modeled with the combination of word-based objective assessment. The word objective metric requires no a priori studies of common mistakes, and it makes use of class based language models to incorporate wrong and correct pronunciations. Wrong pronunciations are automatically generated by employing competitive lexicon, and students' native language phonetic rules. Subjective-objective sentence score correlations greater than 0.5 can be achieved when the proposed sentence based pronunciation criterion is approximated with the combinations of word-based scores. Finally, the subjective-objective sentence score correlations reported here are very comparable with those published elsewhere with methods that require a priori studies of pronunciation errors.\n",
    "Index Terms: Computer-aided pronunciation training, subjective criterion, second language learning, ASR\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-255"
  },
  "bell12_interspeech": {
   "authors": [
    [
     "Peter",
     "Bell"
    ],
    [
     "Myroslava",
     "Dzikovska"
    ],
    [
     "Amy",
     "Isard"
    ]
   ],
   "title": "Designing a spoken language interface for a tutorial dialogue system",
   "original": "i12_1283",
   "page_count": 4,
   "order": 259,
   "p1": "1283",
   "pn": "1286",
   "abstract": [
    "We describe our work in building a spoken language interface for a tutorial dialogue system. Our goal is to allow natural, unrestricted student interaction with the computer tutor, which has been shown to improve the student's learning gain, but presents challenges for speech recognition and spoken language understanding. We discuss the choice of system components and present the results of development experiments in both acoustic and language modelling for speech recognition in this domain.\n",
    "Index Terms: spoken dialogue system, speech recognition, computer tutoring, adaptation\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-256"
  },
  "zhang12c_interspeech": {
   "authors": [
    [
     "Long",
     "Zhang"
    ],
    [
     "Haifeng",
     "Li"
    ],
    [
     "Lin",
     "Ma"
    ]
   ],
   "title": "Automatic pronunciation error detection based on extended pronunciation space using the unsupervised clustering of pronunciation errors",
   "original": "i12_1287",
   "page_count": 4,
   "order": 260,
   "p1": "1287",
   "pn": "1290",
   "abstract": [
    "Posterior probability calculated within a standard pronunciation space (SPS) is a common method in automatic pronunciation error detection (APED). However, if pronunciation errors are not within the SPS, the method is only able to find an approximate solution, that may be not right in many cases. This paper expands the SPS to include more pronunciation errors, proposes an unsupervised clustering of pronunciation errors based on Bhattacharyya distance, and then refines more detailed acoustic models for APED within the extended pronunciation space (EPS). The relationship between the performance of APED system and the number of cluster or the size of the EPS is also discussed. The experimental results show that, compared with the APED based on the SPS, the one based on the EPS using adaptive unsupervised clustering of pronunciation errors has better performance and the average scoring error rate (ASER) decreases from 0.412 to 0.301, relatively 26.94%.\n",
    "Index Terms: automatic pronunciation error detection, pronunciation space, unsupervised clustering of pronunciation errors, Bhattacharyya distance\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-257"
  },
  "pellegrini12_interspeech": {
   "authors": [
    [
     "Thomas",
     "Pellegrini"
    ],
    [
     "Ângela",
     "Costa"
    ],
    [
     "Isabel",
     "Trancoso"
    ]
   ],
   "title": "Less errors with TTS? a dictation experiment with foreign language learners",
   "original": "i12_1291",
   "page_count": 4,
   "order": 261,
   "p1": "1291",
   "pn": "1294",
   "abstract": [
    "This article reports a contrastive study about the use of Text-To-Speech (TTS) synthesis instead of pre-recorded utterances in a dictation exercise submitted to students of European Portuguese as a second language (PSL). Fourty sentences were extracted from a PSL student book. Twenty of them were synthesized and the other twenty ones directly taken from the pre-recorded audio documents of the book. The learners were asked to orthographically transcribe the audio sentences presented in a random order. It appeared that the synthetic utterances were easier to transcribe than the human ones, with word error rates of 26.6% and 33.9% respectively. This result was somehow surprising since the synthetic voice was not built for learning purposes. Potential explaining factors were the lower speech rate and the less-reduced pronunciation that characterized the TTS voice.\n",
    "Index Terms: Computer-Assisted Language Learning, speech synthesis, dictation, European Portuguese\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-258"
  },
  "chen12d_interspeech": {
   "authors": [
    [
     "Liang-Yu",
     "Chen"
    ],
    [
     "Jyh-Shing Roger",
     "Jang"
    ]
   ],
   "title": "Improvement in automatic pronunciation scoring using additional basic scores and learning to rank",
   "original": "i12_1295",
   "page_count": 4,
   "order": 262,
   "p1": "1295",
   "pn": "1298",
   "abstract": [
    "This paper proposes the adoption of different word-level scores in the framework of automatic pronunciation scoring using learning to rank. Six types of phone-level scores are first computed and converted to word-level scores by using average-based, vowel-based, and consonantbased methods. Different score combination methods are then used to combine these word-level scores to obtain the final combined score for the utterance under inspection. The experimental result shows that the learning to rank methods perform better than most of the existing methods, while using all types of word-level scores can lead to some improvement over the original average-based scores.\n",
    "Index Terms: automatic pronunciation scoring, computer-assisted pronunciation training, learning to rank\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-259"
  },
  "cheng12_interspeech": {
   "authors": [
    [
     "Jian",
     "Cheng"
    ]
   ],
   "title": "Automatic tone assessment of non-native Mandarin speakers",
   "original": "i12_1299",
   "page_count": 4,
   "order": 263,
   "p1": "1299",
   "pn": "1302",
   "abstract": [
    "In this paper, we discuss the methods used to assess non-native Mandarin speakers' tone automatically. A context-dependent syllable-level tone modeling method for tone assessment is proposed. A direct comparison between a speaker's contours and ideal contours in energy and pitch using a syllable-level normalization technique provides a strong prediction of the speaker's tone as rated by humans. By combining features from energy and pitch with other features such as duration and spectral likelihoods at the phoneme level, we achieved a humanmachine correlation coefficient of 0.77 at the response level and 0.85 at the participant level. As a comparison, the correlation coefficient between human raters was 0.66 at the response level. The results support both the new proposed method and also the use of Read Aloud as a task to assess non-native Mandarin speaker's tone automatically.\n",
    "Index Terms: Chinese, Mandarin, tone, prosody, assessment, proficiency test\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-260"
  },
  "kafentzis12_interspeech": {
   "authors": [
    [
     "George P.",
     "Kafentzis"
    ],
    [
     "Olivier",
     "Rosec"
    ],
    [
     "Yannis",
     "Stylianou"
    ]
   ],
   "title": "On the modeling of voiceless stop sounds of speech using adaptive quasi-harmonic models",
   "original": "i12_0859",
   "page_count": 4,
   "order": 264,
   "p1": "859",
   "pn": "862",
   "abstract": [
    "In this paper, the performance of the recently proposed adaptive signal models on modeling speech voiceless stop sounds is presented. Stop sounds are transient parts of speech that are highly non-stationary in time. State-of-the-art sinusoidal models fail to model them accurately and efficiently, thus introducing an artifact known as the pre-echo effect. The adaptive QHM and the extended adaptive QHM (eaQHM) are tested to confront this effect and it is shown that highly accurate, pre-echo-free representations of stop sounds are possible using adaptive schemes. Results on a large database of voiceless stops show that, on average, eaQHM improves by 100% the Signal to Reconstruction Error Ratio (SRER) obtained by the standard sinusoidal model.\n",
    "Index Terms: Extended adaptive Quasi-Harmonic Model, Stop sounds, Speech analysis, Sinusoidal Modeling, Pre-echo effect\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-261"
  },
  "ng12_interspeech": {
   "authors": [
    [
     "Raymond W. M.",
     "Ng"
    ],
    [
     "Thomas",
     "Hain"
    ],
    [
     "Keikichi",
     "Hirose"
    ]
   ],
   "title": "An alignment matching method to explore pseudosyllable properties across different corpora",
   "original": "i12_0863",
   "page_count": 4,
   "order": 265,
   "p1": "863",
   "pn": "866",
   "abstract": [
    "A pseudosyllable unit was derived for English read speech recognition. It is a question whether the pseudosyllable unit can be extracted in a robust manner and how this unit could help in the speech recognition process by providing some indications to the error pattern. In this study, an evaluation method which maps every hypothesis phoneme to every reference is proposed. Analysis is done to the pseudosyllables extracted from two different sets of speech data. Mutual information is used to look at the relationship between different pseudosyllable aspects and error pattern of the hypothesis phoneme. It was shown that the pseudosyllable extraction algorithm is robust and gives units with consistent nature. Pseudosyllables which have a phone triplet structure tends to have lower insertion. Pseudosyllables which overlap with their neighbours are places where more insertion errors may occur.\n",
    "Index Terms: pseudosyllable, error analysis, mutual information, speech recognition\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-262"
  },
  "uria12_interspeech": {
   "authors": [
    [
     "Benigno",
     "Uria"
    ],
    [
     "Iain",
     "Murray"
    ],
    [
     "Steve",
     "Renals"
    ],
    [
     "Korin",
     "Richmond"
    ]
   ],
   "title": "Deep architectures for articulatory inversion",
   "original": "i12_0867",
   "page_count": 4,
   "order": 266,
   "p1": "867",
   "pn": "870",
   "abstract": [
    "We implement two deep architectures for the acoustic-articulatory inversion mapping problem: a deep neural network and a deep trajectory mixture density network. We find that in both cases, deep architectures produce more accurate predictions than shallow architectures and that this is due to the higher expressive capability of a deep model and not a consequence of adding more adjustable parameters. We also find that a deep trajectory mixture density network is able to obtain better inversion accuracies than smoothing the results of a deep neural network. Our best model obtained an average root mean square error of 0.885 mm on the MNGU0 test dataset.\n",
    "Index Terms: Articulatory inversion, deep neural network, deep belief network, deep regression network, pretraining\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-263"
  },
  "henry12_interspeech": {
   "authors": [
    [
     "Katharine",
     "Henry"
    ],
    [
     "Morgan",
     "Sonderegger"
    ],
    [
     "Joseph",
     "Keshet"
    ]
   ],
   "title": "Automatic measurement of positive and negative voice onset time",
   "original": "i12_0871",
   "page_count": 4,
   "order": 267,
   "p1": "871",
   "pn": "874",
   "abstract": [
    "Previous work on automatic VOT measurement has focused on positive-valued VOT. However, in many languages VOT can be either positive or negative (“prevoiced”). We present a discriminative algorithm that simultaneously decides whether a stop is prevoiced and measures its VOT. The algorithm operates on feature functions designed to locate the burst and voicing onsets in the positive and negative VOT cases. Tested on a database of positive- and negative-VOT voiced stops, the algorithm predicts prevoicing with >90% accuracy, and gives good agreement between automatic and manual measurements.\n",
    "Index Terms: voice onset time, automatic phonetic measurement, discriminative methods, structured prediction\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-264"
  },
  "khanagha12_interspeech": {
   "authors": [
    [
     "Vahid",
     "Khanagha"
    ],
    [
     "Khalid",
     "Daoudi"
    ]
   ],
   "title": "Efficient multipulse approximation of speech excitation using the most singular manifold",
   "original": "i12_0875",
   "page_count": 4,
   "order": 268,
   "p1": "875",
   "pn": "878",
   "abstract": [
    "We propose a novel approach to find the locations of the multipulse sequence that approximates the speech source excitation. This approach is based on the notion of Most Singular Manifold (MSM) which is associated to the set of less predictable events. The MSM is formed by identifying (directly from the speech waveform) multiscale singularities which may correspond to significant impulsive excitations of the vocal tract. This identification is done through a multiscale measure of local predictability and the estimation of its associated singularity exponents. Once the pulse locations are found using the MSM, their amplitudes are computed using the second stage of the classical MultiPulse Excitation (MPE) coder. The multipulse sequence is then fed to the classical LPC synthesizer to reconstruct speech. The resulting MSM-based algorithm is shown to be significantly more efficient than MPE. We evaluate our algorithm using 1 hour of speech from the TIMIT database and compare its performances to MPE and a recent approach based on compressed sensing (CS). The results show that our algorithm yields similar perceptual quality than MPE and outperforms the CS method when the number of pulses is low.\n",
    "Index Terms: Multipulse speech coding, source excitation approximation, multiscale signal processing, singularity exponents.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-265"
  },
  "jansen12_interspeech": {
   "authors": [
    [
     "Aren",
     "Jansen"
    ],
    [
     "Samuel",
     "Thomas"
    ],
    [
     "Hynek",
     "Hermansky"
    ]
   ],
   "title": "Intrinsic spectral analysis for zero and high resource speech recognition",
   "original": "i12_0879",
   "page_count": 4,
   "order": 269,
   "p1": "879",
   "pn": "882",
   "abstract": [
    "The constraints of the speech production apparatus imply that our vocalizations are approximately restricted to a low-dimensional manifold embedded in a high-dimensional space. Manifold learning algorithms provide a means to recover the approximate embedding from untranscribed data and enable use of the manifold's intrinsic distance metric to characterize acoustic similarity for downstream automatic speech applications. In this paper, we consider a previously unevaluated nonlinear out-of-sample extension for intrinsic spectral analysis (ISA), investigating its performance in both unsupervised and supervised tasks. In the zero resource regime, where the lack of transcribed resources forces us to rely solely on the phonetic salience of the acoustic features themselves, ISA provides substantial gains relative to canonical acoustic front-ends. When large amounts of transcribed speech for supervised acoustic model training are also available, we find that the data-driven intrinsic spectrogram matches the performance of and is complementary to these signal processing derived counterparts.\n",
    "Index Terms: intrinsic spectral analysis, manifold learning, speech recognition, zero resource\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-266"
  },
  "scharenborg12b_interspeech": {
   "authors": [
    [
     "Odette",
     "Scharenborg"
    ],
    [
     "Marijt",
     "Witteman"
    ],
    [
     "Andrea",
     "Weber"
    ]
   ],
   "title": "Computational modelling of the recognition of foreign-accented speech",
   "original": "i12_0883",
   "page_count": 4,
   "order": 270,
   "p1": "883",
   "pn": "886",
   "abstract": [
    "In foreign-accented speech, pronunciation typically deviates from the canonical form to some degree. For native listeners, it has been shown that word recognition is more difficult for strongly-accented words than for less strongly-accented words. Furthermore recognition of strongly-accented words becomes easier with additional exposure to the foreign accent. In this paper, listeners' behaviour was simulated with Fine-tracker, a computational model of word recognition that uses real speech as input. The simulations showed that, in line with human listeners, 1) Fine-Tracker's recognition outcome is modulated by the degree of accentedness and 2) it improves slightly after brief exposure with the accent. On the level of individual words, however, Fine-tracker failed to correctly simulate listeners' behaviour, possibly due to differences in overall familiarity with the chosen accent (German-accented Dutch) between human listeners and Fine-Tracker.\n",
    "Index Terms: foreign-accented speech, accent strength, word recognition, computational modelling, German-accented Dutch\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-267"
  },
  "meister12_interspeech": {
   "authors": [
    [
     "Lya",
     "Meister"
    ],
    [
     "Einar",
     "Meister"
    ]
   ],
   "title": "The production and perception of Estonian quantity degrees by native and non-native speakers",
   "original": "i12_0887",
   "page_count": 4,
   "order": 271,
   "p1": "887",
   "pn": "890",
   "abstract": [
    "The paper studies the production and perception of Estonian quantity oppositions by native Estonians (L1 subjects) and non-native speakers with Russian-language background (L2 subjects). Estonian quantity system involves three contrastive prosodic patterns referred to as short (Q1), long (Q2) and overlong (Q3) quantity degrees. These phonological contrasts are manifested by a complex interaction of durational and tonal cues in a disyllabic foot. For L2 subjects the Estonian quantity contrasts constitute a difficult issue since there are no similar patterns in Russian to rely on.   The production of quantity contrasts was examined in two-syllable CVCV-words read in sentence context; for the perception experiments a stimulus set involving isolated CVCV-words with varying duration of vowels in both syllables and varying F0 contour was created. Ten L1 and ten L2 subjects participated in both the reading of test sentences and the perception experiment.   The results showed that L2 subjects were successful in distinguishing Q1 and Q2 patterns in production and perception, but failed in distinguishing Q2 and Q3 patterns in both tasks.\n",
    "Index Terms: L2 speech, quantity degrees, categorical perception, category boundary, Estonian, Russian\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-268"
  },
  "sadakata12_interspeech": {
   "authors": [
    [
     "Makiko",
     "Sadakata"
    ],
    [
     "Mizuki",
     "Shingai"
    ],
    [
     "Alex",
     "Brandmeyer"
    ],
    [
     "Kaoru",
     "Sekiyama"
    ]
   ],
   "title": "Perception of the moraic obstruent /q/: a cross-linguistic study",
   "original": "i12_0891",
   "page_count": 4,
   "order": 272,
   "p1": "891",
   "pn": "894",
   "abstract": [
    "Japanese natives segment speech into morae. The current study tested whether this extends to perception of geminate consonants: do Japanese natives rely on the moraic obstruent /Q/, and if so, which acoustic feature is perceived to characterize /Q/? Based on an informal interview, we hypothesized that Japanese natives rely on /Q/ that is represented as a silent duration. If so, it should be more difficult for them to distinguish 'geminate fricative consonants /ss/' and 'a silent duration plus singleton fricative consonant /_s/'. Two experiments with Japanese and Dutch natives compared discrimination and categorization accuracy of pseudo words including /ss/ and /_s/. Japanese natives discriminated them well while they poorly categorized them. Dutch natives performed both tasks relatively well. These results are in line with our hypothesis. This provides further support for the language specific listening.\n",
    "Index Terms: geminate consonant, moraic obstruent, perception\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-269"
  },
  "nariai12_interspeech": {
   "authors": [
    [
     "Tomoko",
     "Nariai"
    ],
    [
     "Kazuyo",
     "Tanaka"
    ],
    [
     "Tatsuya",
     "Kawahara"
    ]
   ],
   "title": "Comparative analysis of intensity between native speakers and Japanese speakers of English",
   "original": "i12_0895",
   "page_count": 4,
   "order": 273,
   "p1": "895",
   "pn": "898",
   "abstract": [
    "Intensity has been reported as a reliable acoustical correlate of stress accent for English language, but not of pitch accent for Japanese language. This difference between English and Japanese languages is presumed to shape the characteristics of intensity in English spoken by Japanese (Japanese English, henceforth). Based on this presumption, the intensity of words in sentence utterances for Japanese English is compared to that for native speakers' English (native English, henceforth). Statistical analysis shows that nouns for Japanese English are produced with less intensity, whereas most function words are with more intensity than those for native English. A correlation is recognized between the above results and the proficiency in Japanese English.\n",
    "Index Terms: power patterns, amplitude, prominence, sentence stress, second language production\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-270"
  },
  "koniaris12_interspeech": {
   "authors": [
    [
     "Christos",
     "Koniaris"
    ],
    [
     "Olov",
     "Engwall"
    ],
    [
     "Giampiero",
     "Salvi"
    ]
   ],
   "title": "Auditory and dynamic modeling paradigms to detect L2 mispronunciations",
   "original": "i12_0899",
   "page_count": 4,
   "order": 274,
   "p1": "899",
   "pn": "902",
   "abstract": [
    "This paper expands our previous work on automatic pronunciation error detection that exploits knowledge from psychoacoustic auditory models. The new system has two additional important features, i.e., auditory and acoustic processing of the temporal cues of the speech signal, and classification feedback from a trained linear dynamic model. We also perform a pronunciation analysis by considering the task as a classification problem. Finally, we evaluate the proposed methods conducting a listening test on the same speech material and compare the judgment of the listeners and the methods. The automatic analysis based on spectro-temporal cues is shown to have the best agreement with the human evaluation, particularly with that of language teachers, and with previous plenary linguistic studies.\n",
    "Index Terms: L2 pronunciation error, auditory model, linear dynamic model, distortion measure, phoneme\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-271"
  },
  "li12c_interspeech": {
   "authors": [
    [
     "Sheng",
     "Li"
    ],
    [
     "Lan",
     "Wang"
    ]
   ],
   "title": "Cross linguistic comparison of Mandarin and English EMA articulatory data",
   "original": "i12_0903",
   "page_count": 4,
   "order": 275,
   "p1": "903",
   "pn": "906",
   "abstract": [
    "This paper aims at effectively identifying common English mispronunciations by Mandarin speakers and incorporating this knowledge into language learning software to improve the learner's accented English. For this purpose, English and Mandarin multi-channel EMA articulatory datasets collected from native English and native Mandarin speakers respectively have been used to uncover cross-linguistic distinctions. The Procrustes based speaker normalization is used to eliminate the variability which comes from speaker-specific vocal-tract anatomies and other individual biomechanical properties. Then the English phonemes missing from Mandarin and their Mandarin confusing equivalents are identified using phonological knowledge. These English and Mandarin phoneme pairs may be hard to distinguish in acoustics, but by extracting useful information from the changing on tongue positions and shapes of the lips while speaking can be good cross linguistic phoneme level comparison metrics both empirical and quantified. With this method, the same analysis can be done between languages, or different accents within the same language in the future and this knowledge can also be incorporated into language learning software.\n",
    "Index Terms: mispronunciation, articulatory data, cross linguistic comparison\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-272"
  },
  "zeroual12_interspeech": {
   "authors": [
    [
     "Chakir",
     "Zeroual"
    ],
    [
     "Diamantis",
     "Gafos"
    ],
    [
     "Phil",
     "Hoole"
    ],
    [
     "John",
     "Esling"
    ]
   ],
   "title": "Physiological and acoustic study of word initial post-lexical gemination in Moroccan Arabic",
   "original": "i12_0907",
   "page_count": 4,
   "order": 276,
   "p1": "907",
   "pn": "910",
   "abstract": [
    "With the EMA technique we showed that post-lexical word initial [dd] of Moroccan Arabic is produced by our three speakers with a longer acoustic duration and articulatory gesture compared to its initial [d] cognate. The tongue tip gesture during initial [dd] and [d] generally have statistically similar height and amplitude. For two speakers, no significant velocity differences were recorded between [dd] and [d]. Our articulatory observations also show no C-center effect between [d(d)] and [u] in the [d(d)uda] items, which is in accord with the study of Shaw et al. (2009) on word initial [Ci], [CjCi] and [CkCjCi] clusters in MA which they use as an argument against complex onset in this language. Our results seem to show that initial hetero-morphemic geminate [dd] is a cluster of two hetero-syllabic consonants.\n",
    "Index Terms: EMA, Moroccan Arabic, germination, kinematic, temporal coordination, C-center effect.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-273"
  },
  "tyler12b_interspeech": {
   "authors": [
    [
     "Michael D.",
     "Tyler"
    ],
    [
     "Sarah",
     "Fenwick"
    ]
   ],
   "title": "Perceptual assimilation of Arabic voiceless fricatives by English monolinguals",
   "original": "i12_0911",
   "page_count": 4,
   "order": 277,
   "p1": "911",
   "pn": "914",
   "abstract": [
    "Native language experience strongly influences non-native speech discrimination. According to the Perceptual Assimilation Model (PAM; Best, 1995), discrimination is most accurate when two non-native sounds map onto different native phonemes (Two-Category assimilation), poorer when they differ in goodness-of-fit to the same native phoneme (Category-Goodness assimilation), and worst when perceived as equally good or poor versions of the same native phoneme (Single-Category assimilation). The Three Factor Model (Werker & Logan, 1985) suggests that discrimination accuracy is poorer at a 1500ms inter-stimulus interval (ISI), when only phonemic information is available, than at 500ms, when both phonemic and phonetic information is available. To test the models, monolingual English participants assigned Arabic fricatives to English categories, and discriminated contrasting fricative pairs in an AXB task with a 500ms or 1500ms ISI. PAM discrimination predictions were upheld, but there was no influence of ISI.\n",
    "Index Terms: cross-language speech perception, perceptual assimilation, human speech discrimination\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-274"
  },
  "rasanen12_interspeech": {
   "authors": [
    [
     "Okko",
     "Räsänen"
    ]
   ],
   "title": "Non-auditory cognitive capabilities in computational modeling of early language acquisition",
   "original": "i12_0915",
   "page_count": 4,
   "order": 278,
   "p1": "915",
   "pn": "918",
   "abstract": [
    "Computational models of early language acquisition (LA) play an important role in understanding the acquisition and processing of spoken language. Since language is an extremely complex phenomenon, computational studies typically address only a specific aspect of the LA at a time. This calls for a huge number of assumptions regarding the other cognitive processes of the learning system, and these assumptions can have significant consequences to the ecological plausibility of the simulations. In this paper, we review the developmental status of a number of cognitive processes during the first year of infant's life that are typically involved in the computational simulations of LA. How these findings are related to the plausibility of different simplifications and assumptions in computational models are also discussed.\n",
    "Index Terms: language acquisition, computational modeling, statistical learning\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-275"
  },
  "rasanen12b_interspeech": {
   "authors": [
    [
     "Okko",
     "Räsänen"
    ],
    [
     "Heikki",
     "Rasilo"
    ],
    [
     "Unto K.",
     "Laine"
    ]
   ],
   "title": "Modeling spoken language acquisition with a generic cognitive architecture for associative learning",
   "original": "i12_0919",
   "page_count": 4,
   "order": 279,
   "p1": "919",
   "pn": "922",
   "abstract": [
    "Human neo-cortex can be viewed as a modality invariant system for pattern discovery and associative learning. Similarly, research in the field of distributional learning suggests that much of human language acquisition can be explained by generic statistical learning mechanisms. The current paper argues that pattern processing capabilities of the human brain can be better understood if the process of early language acquisition is modeled using an entire cognitive architecture capable of unsupervised pattern discovery and associative learning. A high-level motivation and description for generic processing principles in such architecture are given, followed by examples of our current work in the field.\n",
    "Index Terms: language acquisition, computational modeling, statistical learning, associative learning, multimodality, memory architectures\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-276"
  },
  "wang12d_interspeech": {
   "authors": [
    [
     "Dongmei",
     "Wang"
    ],
    [
     "Philipos C.",
     "Loizou"
    ]
   ],
   "title": "Pitch estimation based on long frame harmonic model and short frame average correlation coefficient",
   "original": "i12_0923",
   "page_count": 4,
   "order": 280,
   "p1": "923",
   "pn": "926",
   "abstract": [
    "We propose a pitch estimation method based on long frame harmonic model (LFHM) and short frame average correlation coefficient (SFACC). The long frame used here is three times as long as the short frame. Our approach consists of two main steps: pitch candidates extraction based on LFHM, and final pitch selection based on the SFACC. The long frame analysis for speech signal is able to achieve a higher harmonic resolution and ensures the inclusion of the true pitch in the pitch candidate pool. The SFACC based pitch selection method is robust to noise and keeps the short-time stationary features of speech. Experimental results show that our method outperforms several state-of-the-art algorithms under both clean and noisy acoustic environments.\n",
    "Index Terms: pitch estimation, long frame harmonic model, average correlation coefficient\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-277"
  },
  "moller12_interspeech": {
   "authors": [
    [
     "Sebastian",
     "Möller"
    ],
    [
     "Marcel",
     "Wältermann"
    ],
    [
     "Nicolas",
     "Côté"
    ]
   ],
   "title": "Diagnostic prediction of transmitted speech quality: a new framework for signal-based and parametric models",
   "original": "i12_0927",
   "page_count": 4,
   "order": 281,
   "p1": "927",
   "pn": "930",
   "abstract": [
    "In this paper, we present a new framework for the diagnostic prediction of transmitted speech quality. The idea is to extract perceptually relevant feature estimations from the speech signal, and combine them with an overall quality metric in order to obtain more reliable as well as more diagnostic predictions of speech quality. We implement this framework in two complementary ways: In terms of a signal-based model which can be used for online and offline measurement, and in terms of a parametric model which can be used for network planning. The implementations are compared to standard state-of-the-art models and show a similar level of reliability, while providing additional diagnostic value.\n",
    "Index Terms: speech quality assessment, speech transmission, quality prediction, Quality of Experience\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-278"
  },
  "backstrom12_interspeech": {
   "authors": [
    [
     "Tom",
     "Bäckström"
    ]
   ],
   "title": "Enumerative algebraic coding for ACELP",
   "original": "i12_0931",
   "page_count": 4,
   "order": 282,
   "p1": "931",
   "pn": "934",
   "abstract": [
    "Speech coding algorithms based on Algebraic Code Excited Linear Prediction (ACELP) quantize and code the residual signal with a fixed algebraic codebook. This paper generalizes the conventional encoding to provide optimal bit consumption for all codebook designs by enumerating all possible states. The codebook designed is aimed for constant bit-rate applications applied on mobile devices.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-279"
  },
  "saha12_interspeech": {
   "authors": [
    [
     "Atanu",
     "Saha"
    ],
    [
     "Tetsuya",
     "Shimamura"
    ]
   ],
   "title": "Speech enhancement with bivariate gamma model",
   "original": "i12_0935",
   "page_count": 4,
   "order": 283,
   "p1": "935",
   "pn": "938",
   "abstract": [
    "In this paper, we propose a family of Bayesian estimators for single channel speech enhancement. Three different kinds of enhancement estimators are derived from the statistics of bivariate Gamma model. In the Gamma model, it is assumed that the spectral components are correlated with each other. The experimental results show that the proposed estimators are effective for speech enhancement compared to state-of-the-art estimators.\n",
    "Index Terms: speech enhancement, bivariate Gamma model, Gaussian distribution, correlated spectral components\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-280"
  },
  "trawicki12_interspeech": {
   "authors": [
    [
     "Marek",
     "Trawicki"
    ],
    [
     "Michael",
     "Johnson"
    ]
   ],
   "title": "Improvements of the beta-order minimum mean-square error (MMSE) spectral amplitude estimator using chi priors",
   "original": "i12_0939",
   "page_count": 4,
   "order": 284,
   "p1": "939",
   "pn": "942",
   "abstract": [
    "In this paper, the authors propose the Beta-Order Minimum Mean-Square Error (MMSE) Spectral Amplitude estimator with Chi statistical models for the speech priors. The new estimator incorporates both a shape parameter on the distribution and cost function parameter. The performance of the MMSE Beta-Order Spectral Amplitude estimator with Chi speech prior is evaluated using the Segmental Signal-to- Noise Ratio (SSNR) and Perceptual Evaluation of Speech Quality (PESQ) objective quality measures. From the experimental results, the new estimator provides gains of 0-3 dB and 0-0.3 in SSNR and PESQ improvements over the corresponding MMSE Beta-Order MMSE Spectral Amplitude estimator with the standard Rayleigh statistical models for the speech prior.\n",
    "Index Terms: speech enhancement, amplitude estimation, phase estimation, parameter estimation\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-281"
  },
  "harding12_interspeech": {
   "authors": [
    [
     "Philip",
     "Harding"
    ],
    [
     "Ben",
     "Milner"
    ]
   ],
   "title": "Enhancing speech by reconstruction from robust acoustic features",
   "original": "i12_0943",
   "page_count": 4,
   "order": 285,
   "p1": "943",
   "pn": "946",
   "abstract": [
    "A method of speech enhancement is developed that reconstructs clean speech from a set of acoustic features using a sinusoidal model of speech. This is a significant departure from traditional filtering-based methods of speech enhancement. A major challenge with this approach is to estimate accurately the acoustic features (voicing, fundamental frequency, spectral envelope) from noisy speech. This is achieved using maximum a-posteriori estimation methods that operate on the noisy speech. Objective results are presented to optimise the proposed system and a set of subjective tests compare the approach with traditional enhancement methods.\n",
    "Index Terms: speech enhancement, MAP, sinusoidal model\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-282"
  },
  "chetupally12_interspeech": {
   "authors": [
    [
     "Srikanth Raj",
     "Chetupally"
    ],
    [
     "Thippur V.",
     "Sreenivas"
    ]
   ],
   "title": "Joint pitch-analysis formant-synthesis framework for CS recovery of speech",
   "original": "i12_0947",
   "page_count": 4,
   "order": 286,
   "p1": "947",
   "pn": "950",
   "abstract": [
    "A joint analysis-synthesis framework is developed for the compressive sensing recovery of speech signals. The signal is assumed to be sparse in the residual domain with the linear prediction filter used as the sparse transformation. Importantly this transform is not known apriori, since estimating the predictor filter requires the knowledge of the signal. Two prediction filters, one comb filter for pitch and another all pole formant filter are needed to induce maximum sparsity. An iterative method is proposed for the estimation of both the prediction filters and the signal itself. Formant prediction filter is used as the synthesis transform, while the pitch filter is used to model the periodicity in the residual excitation signal, in the analysis mode. Significant improvement in the LLR measure is seen over the previously reported formant filter estimation.\n",
    "Index Terms: Linear prediction, Analysis/Synthesis, Compressive Sensing\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-283"
  },
  "liang12_interspeech": {
   "authors": [
    [
     "Shan",
     "Liang"
    ],
    [
     "Wei",
     "Jiang"
    ],
    [
     "Wenju",
     "Liu"
    ]
   ],
   "title": "A new noise-tracking algorithm for generalizing binary time-frequency (t-f) masking to ratio masking",
   "original": "i12_0951",
   "page_count": 4,
   "order": 287,
   "p1": "951",
   "pn": "954",
   "abstract": [
    "In this paper, we attempt to generalize the ideal binary mask (IBM) estimation to the ideal ratio mask (IRM) estimation. Under binary masking, the error in IBM estimation may greatly distort the original speech spectrum. The main purpose of this paper is using ratio mask to smooth this negative impact. Since the key issue is the noise tracking, we firstly use exponential distributions to model the distribution of noise power with binary mask and mixture power as condition. Then, we use a Gaussian distribution to model the correlation of noise estimation between adjacent T-F units. As the IBM of majority units can be estimated correctly, the correlation model could reduce the impact introduced by the error in IBM estimation. Systematic experiments show that our algorithm outperforms a common binary masking based method in terms of SNR gain and PESQ scores.\n",
    "Index Terms: Ideal Binary Mask, Ideal Ratio Mask, Markov Chain Monte Carlo, Bayesian rule\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-284"
  },
  "tang12_interspeech": {
   "authors": [
    [
     "Yan",
     "Tang"
    ],
    [
     "Martin",
     "Cooke"
    ]
   ],
   "title": "Optimised spectral weightings for noise-dependent speech intelligibility enhancement",
   "original": "i12_0955",
   "page_count": 4,
   "order": 288,
   "p1": "955",
   "pn": "958",
   "abstract": [
    "Natural or synthetic speech is increasingly used in less-than-ideal listening conditions. Maximising the likelihood of correct message reception in such situations often leads to a strategy of loud and repetitive renditions of output speech. An alternative approach is to modify the speech signal in ways which increase intelligibility in noise without increasing signal level or duration. The current study focused on the design of stationary spectral modifications whose effect is to reallocate speech energy across frequency bands. Frequency band weights were selected using a genetic algorithm-based optimisation procedure, with glimpse proportion as the objective intelligibility metric, for a range of noise types and levels. As expected, a clear dependence of noise type and global signal-to-noise ratio on energy reallocation was found. One unanticipated outcome was the consistent discovery of sparse, highly-selective spectral energy weightings, particularly in high noise conditions. In a subjective test using stationary noise and competing speech maskers, listeners were able to identify significantly more words in sentences as a result of spectral weighting, with increases of up to 15 percentage points. These findings suggest that context- dependent speech output can be used to maintain intelligibility at lower sound output levels.\n",
    "Index Terms: speech intelligibility, noise, optimisation, genetic algorithm, glimpse proportion\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-285"
  },
  "chen12e_interspeech": {
   "authors": [
    [
     "Langzhou",
     "Chen"
    ],
    [
     "Mark J. F.",
     "Gales"
    ],
    [
     "Vincent",
     "Wan"
    ],
    [
     "Javier",
     "Latorre"
    ],
    [
     "Masami",
     "Akamine"
    ]
   ],
   "title": "Exploring rich expressive information from audiobook data using cluster adaptive training",
   "original": "i12_0959",
   "page_count": 4,
   "order": 289,
   "p1": "959",
   "pn": "962",
   "abstract": [
    "Audiobook data is a freely available source of rich expressive speech data. To accurately generate speech of this form, expressiveness must be incorporated into the synthesis system. This paper investigates two parts of this process: the representation of expressive information in a statistical parametric speech synthesis system; and whether discrete expressive state labels can sufficiently represent the full diversity of expressive speech. Initially a discrete form of expressive information was used. A new form of expressive representation, where each condition maps to a point in an expressive speech space, is described. This cluster adaptively trained (CAT) system is compared to incorporating information in the decision tree construction and a transform based system using CMLLR and CSMAPLR. Experimental results indicate that the CAT system outperformed the contrast systems in both expressiveness and voice quality. The CAT-style representation yields a continuous expressive speech space. Thus, it is possible to treat utterance-level expressiveness as a point in this continuous space, rather than as one of a discrete states. This continuous-space representation outperformed discrete clusters, indicating limitations of discrete labels for expressiveness in audiobook data.\n",
    "Index Terms: expressive speech synthesis, hidden Markov model, cluster adaptive training, audiobook\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-286"
  },
  "he12_interspeech": {
   "authors": [
    [
     "Ji",
     "He"
    ],
    [
     "Yao",
     "Qian"
    ],
    [
     "Frank K.",
     "Soong"
    ],
    [
     "Sheng",
     "Zhao"
    ]
   ],
   "title": "Turning a monolingual speaker into multilingual for a mixed-language TTS",
   "original": "i12_0963",
   "page_count": 4,
   "order": 290,
   "p1": "963",
   "pn": "966",
   "abstract": [
    "We propose an approach to render speech sentences of different languages out of a speaker's monolingual recordings for building mixed-coded TTS systems. The difference between two monolingual speakers' corpora, e.g. English and Chinese, are firstly equalized by warping spectral frequency, removing F0 variation and adjusting speaking rate across speakers and languages. The English speaker's Chinese speech is then rendered by a trajectory tilling approach. The Chinese speaker's parameter trajectories, which are equalized towards English speaker, are used to guide the search for the best sequence of 5ms waveform \"tiles\" in English speaker's recordings. The rendered English speaker's Chinese speech together with her own English recordings is finally used to train a mixed-language (English-Chinese) HMM-based TTS. Experimental results show that the proposed approach can synthesize high quality of mixed-language speech, which is also confirmed in both objective and subjective evaluations.\n",
    "Index Terms: Mixed-language TTS, HMM-based TTS, Unit Selection, Trajectory Tiling\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-287"
  },
  "veaux12_interspeech": {
   "authors": [
    [
     "Christophe",
     "Veaux"
    ],
    [
     "Junichi",
     "Yamagishi"
    ],
    [
     "Simon",
     "King"
    ]
   ],
   "title": "Using HMM-based speech synthesis to reconstruct the voice of individuals with degenerative speech disorders",
   "original": "i12_0967",
   "page_count": 4,
   "order": 291,
   "p1": "967",
   "pn": "970",
   "abstract": [
    "When individuals lose the ability to produce their own speech, due to degenerative diseases such as motor neuron disease (MND) or Parkinson's, they lose not only a functional means of communication but also a display of their individual and group identity. In order to build personalized synthetic voices, attempts have been made to capture the voice before it is lost, using a process known as voice banking. But, for some patients, the speech deterioration frequently coincides or quickly follows diagnosis. Using HMM-based speech synthesis, it is now possible to build personalized synthetic voices with minimal data recordings and even disordered speech. In this approach, the patient's recordings are used to adapt an average voice model pre-trained on many speakers. The structure of the voice model allows some reconstruction of the voice by substituting some components from the average voice in order to compensate for the disorders found in the patient's speech. In this paper, we compare different substitution strategies and introduce a context-dependent model substitution to improve the intelligibility of the synthetic speech while retaining the vocal identity of the patient. A subjective evaluation of the reconstructed voice for a patient with MND shows promising results for this strategy.\n",
    "Index Terms: HTS, Voice Cloning, Voice Reconstruction, Assistive Technologies\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-288"
  },
  "latorre12_interspeech": {
   "authors": [
    [
     "Javier",
     "Latorre"
    ],
    [
     "Vincent",
     "Wan"
    ],
    [
     "Mark J. F.",
     "Gales"
    ],
    [
     "Langzhou",
     "Chen"
    ],
    [
     "K. K.",
     "Chin"
    ],
    [
     "Kate",
     "Knill"
    ],
    [
     "Masami",
     "Akamine"
    ]
   ],
   "title": "Speech factorization for HMM-TTS based on cluster adaptive training",
   "original": "i12_0971",
   "page_count": 4,
   "order": 292,
   "p1": "971",
   "pn": "974",
   "abstract": [
    "This paper presents a novel approach to factorize and control different speech factors in HMM-based TTS systems. In this paper cluster adaptive training (CAT) is used to factorize speaker identity and expressiveness (i.e. emotion). Within a CAT framework, each speech factor can be modelled by a different set of clusters. Users can control speaker identity and expressiveness independently by modifying the weights associated with each set. These weights are defined in a continuous space, so variations of speaker and emotion are also continuous. Additionally, given a speaker which has only neutral-style training data, the approach is able to synthesise speech with that speaker's voice and different expressions. Lastly, the paper discusses how generalization of the basic factorization concept could allow the production of expressive speech from neutral voices for other HMM-TTS systems not based on CAT.\n",
    "Index Terms: speech synthesis, cluster adaptive training, expressive synthesis, speech factorization\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-289"
  },
  "sung12_interspeech": {
   "authors": [
    [
     "June Sig",
     "Sung"
    ],
    [
     "Doo Hwa",
     "Hong"
    ],
    [
     "Hyun Woo",
     "Koo"
    ],
    [
     "Nam Soo",
     "Kim"
    ]
   ],
   "title": "Factored MLLR adaptation algorithm for HMM-based expressive TTS",
   "original": "i12_0975",
   "page_count": 4,
   "order": 293,
   "p1": "975",
   "pn": "978",
   "abstract": [
    "One of the most popular approaches to parameter adaptation in hidden Markov model (HMM) based systems is the maximum likelihood linear regression (MLLR) technique. In our previous work, we proposed factored MLLR (FMLLR) where an MLLR parameter is defined as a function of a control parameter vector. We presented a method to train the FMLLR parameters based on a general framework of the expectation-maximization (EM) algorithm. To show the effectiveness, we applied the FMLLR to adapt the spectral envelope feature of the reading-style speech to those of the singing voice. In this paper, we apply the FMLLR to the HMM-based expressive speech synthesis task and compare its performance with conventional approaches. In a series of experimental results, the FMLLR shows better performance than conventional methods.\n",
    "Index Terms: MLLR, MRHSMM, Factored MLLR, expressive speech synthesis, HMM-based speech synthesis\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-290"
  },
  "schabus12_interspeech": {
   "authors": [
    [
     "Dietmar",
     "Schabus"
    ],
    [
     "Michael",
     "Pucher"
    ],
    [
     "Gregor",
     "Hofer"
    ]
   ],
   "title": "Speaker-adaptive visual speech synthesis in the HMM-framework",
   "original": "i12_0979",
   "page_count": 4,
   "order": 294,
   "p1": "979",
   "pn": "982",
   "abstract": [
    "In this paper we apply speaker-adaptive and speaker-dependent training of hidden Markov models (HMMs) to visual speech synthesis. In speaker-dependent training we use data from one speaker to train a visual and acoustic HMM. In speaker-adaptive training, first a visual background model (average voice) from multiple speakers is trained. This background model is then adapted to a new target speaker using (a small amount of) data from the target speaker. This concept has been successfully applied to acoustic speech synthesis. This paper demonstrates how model adaption is applied to the visual domain to synthesize animations of talking faces. A perceptive evaluation is performed, showing that speaker-adaptive modeling outperforms speaker-dependent models for small amounts of training / adaptation data.\n",
    "Index Terms: Visual speech synthesis, speaker-adaptive training, facial animation\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-291"
  },
  "oliveira12_interspeech": {
   "authors": [
    [
     "Viviane de Franca",
     "Oliveira"
    ],
    [
     "Sayaka",
     "Shiota"
    ],
    [
     "Yoshihiko",
     "Nankaku"
    ],
    [
     "Keiichi",
     "Tokuda"
    ]
   ],
   "title": "Cross-lingual speaker adaptation for HMM-based speech synthesis based on perceptual characteristics and speaker interpolation",
   "original": "i12_0983",
   "page_count": 4,
   "order": 295,
   "p1": "983",
   "pn": "986",
   "abstract": [
    "The language mapping performed in a cross-lingual speaker adaptation task may not produce sufficient results if a bilingual database is not available. In order to overcome this problem, this work proposes a new method in which a correspondence between speakers in two different databases, speaking different languages, is established based on the perceptual characteristics of their voices. The proposed approach uses a language-independent space of voice characteristics obtained by performing subjective listening tests. This new space is used in the speaker adaptation process, making it possible to represent the input speaker in a different language while keeping his/her voice characteristics, without a bilingual database. Furthermore, the method is potentially able to adapt the prosodic information from the target speaker, such as long-term changes in F0 and durations. From the evaluation listening tests, we confirmed that the proposed framework generates speech that sounds similar to the target speaker voice, with better speech quality than the previously proposed method.\n",
    "Index Terms: HMM-based speech synthesis, cross-lingual speaker adaptation, eigenvoices, perceptual characteristics, speaker interpolation\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-292"
  },
  "nicolao12_interspeech": {
   "authors": [
    [
     "Mauro",
     "Nicolao"
    ],
    [
     "Javier",
     "Latorre"
    ],
    [
     "Roger K.",
     "Moore"
    ]
   ],
   "title": "C2h: a computational model of H&h-based phonetic contrast in synthetic speech",
   "original": "i12_0987",
   "page_count": 4,
   "order": 296,
   "p1": "987",
   "pn": "990",
   "abstract": [
    "This paper presents a computational model of human speech production based on the hypothesis that low energy attractors for a human speech production system can be identified, and that interpolation/extrapolation along the key dimension of hypo/hyper-articulation can be motivated by energetic considerations of phonetic contrast. An HMM-based speech synthesiser along with continuous adaptation of its statistical models was used to implement the model. Two adaptation methods were proposed for vowel and consonant models and their effectiveness was tested by showing that such hypo/hyper-articulation control can manipulate successfully the intelligibility of synthetic speech in noise. Objective evaluations with the ANSI Speech Intelligibility Index indicate that intelligibility in various types of noise is effectively controlled. In particular, in the hyper-articulation transforms, the improvement with respect to unadapted speech is above 25 %.\n",
    "Index Terms: reactive speech synthesis, hypo/hyper-articulated speech, intelligibility enhancement\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-293"
  },
  "ling12_interspeech": {
   "authors": [
    [
     "Zhen-Hua",
     "Ling"
    ],
    [
     "Korin",
     "Richmond"
    ],
    [
     "Junichi",
     "Yamagishi"
    ]
   ],
   "title": "Vowel creation by articulatory control in HMM-based parametric speech synthesis",
   "original": "i12_0991",
   "page_count": 4,
   "order": 297,
   "p1": "991",
   "pn": "994",
   "abstract": [
    "This paper presents a method to produce a new vowel by articulatory control in hidden Markov model (HMM) based parametric speech synthesis. A multiple regression HMM (MRHMM) is adopted to model the distribution of acoustic features, with articulatory features used as external auxiliary variables. The dependency between acoustic and articulatory features is modelled by a group of linear transforms that are either estimated context-dependently or determined by the distribution of articulatory features. Vowel identity is removed from the set of context features used to ensure compatibility between the context-dependent model parameters and the articulatory features of a new vowel. At synthesis time, acoustic features are predicted according to the input articulatory features as well as context information. With an appropriate articulatory feature sequence, a new vowel can be generated even when it does not exist in the training set. Experimental results show this method is effective in creating the English vowel [ʌ] by articulatory control without using any acoustic samples of this vowel.\n",
    "Index Terms: Speech synthesis, articulatory features, multiple-regression hidden Markov model\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-294"
  },
  "dall12_interspeech": {
   "authors": [
    [
     "Rasmus",
     "Dall"
    ],
    [
     "Christophe",
     "Veaux"
    ],
    [
     "Junichi",
     "Yamagishi"
    ],
    [
     "Simon",
     "King"
    ]
   ],
   "title": "Analysis of speaker clustering strategies for HMM-based speech synthesis",
   "original": "i12_0995",
   "page_count": 4,
   "order": 298,
   "p1": "995",
   "pn": "998",
   "abstract": [
    "This paper describes a method for speaker clustering, with the application of building average voice models for speaker-adaptive HMMbased speech synthesis that are a good basis for adapting to specific target speakers. Our main hypothesis is that using perceptually similar speakers to build the average voice model will be better than use unselected speakers, even if the amount of data available from perceptually similar speakers is smaller. We measure the the perceived similarities among a group of 30 female speakers in a listening test and then apply multiple linear regression to automatically predict these listener judgements of speaker similarity and thus to identify similar speakers automatically. We then compare a variety of average voice models trained on either speakers who were perceptually judged to be similar to the target speaker, or speakers selected by the multiple linear regression, or a large global set of unselected speakers. We find that the average voice model trained on perceptually similar speakers provides better performance than the global model, even though the latter is trained on more data, confirming our main hypothesis. However, the average voice model using speakers selected automatically by the multiple linear regression does not reach the same level of performance.\n",
    "Index Terms: Statistical parametric speech synthesis, hidden Markov models, speaker adaptation\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-295"
  },
  "chen12f_interspeech": {
   "authors": [
    [
     "Kuan-Yu",
     "Chen"
    ],
    [
     "Hao-Chin",
     "Chang"
    ],
    [
     "Berlin",
     "Chen"
    ],
    [
     "Hsin-Min",
     "Wang"
    ]
   ],
   "title": "Word relevance modeling for speech recognition",
   "original": "i12_0999",
   "page_count": 4,
   "order": 299,
   "p1": "999",
   "pn": "1002",
   "abstract": [
    "Language models for speech recognition tend to be brittle across domains, since their performance is vulnerable to changes in the genre or topic of the text on which they are trained. A number of adaptation methods, discovering either lexical co-occurrence or topic cues, have been developed to mitigate this problem with varying degrees of success. Among them, a more recent thread of work is the relevance modeling approach, which has shown promise to capture the lexical co-occurrence relationship between the search history and an upcoming word. However, a potential downside to such an approach is the need of resorting to a retrieval procedure to obtain relevance information; this is usually complex and time-consuming for practical applications. In this paper, we propose a word relevance modeling framework, which introduces a novel use of relevance information for dynamic language model adaptation in speech recognition. It not only inherits the merits of several existing techniques but also provides a flexible but systematic way to render the lexical and topical relationships between the search history and the upcoming word. Experiments on large vocabulary continuous speech recognition seem to demonstrate the performance merits of the methods instantiated from this framework when compared to some methods.\n",
    "Index Terms: language model, relevance, lexical co-occurrence, topic cues, adaptation\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-296"
  },
  "duckhorn12_interspeech": {
   "authors": [
    [
     "Frank",
     "Duckhorn"
    ],
    [
     "Rüdiger",
     "Hoffmann"
    ]
   ],
   "title": "Using context-free grammars for embedded speech recognition with weighted finite-state transducers",
   "original": "i12_1003",
   "page_count": 4,
   "order": 300,
   "p1": "1003",
   "pn": "1006",
   "abstract": [
    "In this paper we propose an extension to weighted finite-state transducers in order to enable them to model context-free grammars. Classical finite-state transducers are restricted to modeling regular grammars. However, for some tasks it is necessary to use more general context-free grammars. Even some regular grammar models can be scaled down using context-free rules. The paper extents the transducers to pushdown weighted finite-state transducers and explains the decoding procedure. We apply the method to an embedded speech dialog system. Speech recognition results show that more than 80% in network size can be saved. Additionally pushdown weighted finite-state transducers clearly outperform the classic ones in terms of best recognition performance and low computation time. Altogether this extension has enabled our recognition task to be executed on a digital signal processor.\n",
    "Index Terms: Weighted finite-state transducer, WFST, Language Models, Automatic speech recognition\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-297"
  },
  "dufour12_interspeech": {
   "authors": [
    [
     "Richard",
     "Dufour"
    ],
    [
     "Géraldine",
     "Damnati"
    ],
    [
     "Delphine",
     "Charlet"
    ],
    [
     "Frédéric",
     "Béchet"
    ]
   ],
   "title": "Automatic transcription error recovery for person name recognition",
   "original": "i12_1007",
   "page_count": 4,
   "order": 301,
   "p1": "1007",
   "pn": "1010",
   "abstract": [
    "Person Name Recognition from transcriptions of TV shows spoken content is a crucial step towards multimedia document indexing. Recognizing Person Names implies the combination of three main modules: Automatic Speech Recognition, Named-Entity Recognition and Entity Linking to associate the recognized surface form to a normalized Person Name. The three modules are potentially error prone. Hence, beyond each module's intrinsic complexity, the Person Names issue suffers from the highly dynamic evolution of vocabularies and occurrence contexts that are correlated to various dimensions (such as actuality, topic of the showc). This paper focuses on the first module and proposes an approach to recover from transcription errors made on Person Names. An error correction method is applied on the textual ASR output and we show that it is all the more efficient that it is coupled with a specific error region detection system. Experiments on the French REPERE database show that Person Names transcription can be efficiently corrected while preserving the overall transcription quality and thus increasing the performance of the whole Person Name Recognition process.\n",
    "Index Terms: transcription error detection, transcription error correction, person name entity recognition\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-298"
  },
  "kobashikawa12_interspeech": {
   "authors": [
    [
     "Satoshi",
     "Kobashikawa"
    ],
    [
     "Takaaki",
     "Hori"
    ],
    [
     "Yoshikazu",
     "Yamaguchi"
    ],
    [
     "Taichi",
     "Asami"
    ],
    [
     "Hirokazu",
     "Masataki"
    ],
    [
     "Satoshi",
     "Takahashi"
    ]
   ],
   "title": "Efficient beam width control to suppress excessive speech recognition computation time based on prior score range normalization",
   "original": "i12_1011",
   "page_count": 4,
   "order": 302,
   "p1": "1011",
   "pn": "1014",
   "abstract": [
    "This paper proposes a method that efficiently controls the beam width and so yields computation times that permit the practical automatic transcription of massive volumes of speech data. In particular, we focus on the fact that a lot of time is wasted in attempting to recognize poor quality speech samples which will yield erroneous transcripts and thus provide no useful data for subsequent text processing. To stabilize the recognition time regardless of speech quality, our proposal controls the score beam width efficiently based on overall score spread against each target speech sample on the premise of stored speech; it formulates the prior score range within the beam width and maximizes computation efficiency by normalizing the range associated with the survival rate of hypotheses. The technique proposed herein can rapidly estimate the range by using just monophones prior to speech recognition decoding. Experiments with several SNRs and real call-center speech sets confirm a reduction in computation time while matching the accuracy of existing techniques.\n",
    "Index Terms: speech recognition, decoding parameter optimization, beam search\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-299"
  },
  "nolden12_interspeech": {
   "authors": [
    [
     "David",
     "Nolden"
    ],
    [
     "Ralf",
     "Schlüter"
    ],
    [
     "Hermann",
     "Ney"
    ]
   ],
   "title": "Search space pruning based on anticipated path recombination in LVCSR",
   "original": "i12_1015",
   "page_count": 4,
   "order": 303,
   "p1": "1015",
   "pn": "1018",
   "abstract": [
    "In this paper we introduce a well-motivated abstract pruning criterion for LVCSR decoders based on the anticipated recombination of HMM state alignment paths. We show that several heuristical pruning methods common in dynamic network decoders are approximations of this pruning criterion. The abstract criterion is too complex to be applied directly in an efficient manner, so we derive approximations which can be applied efficiently.   Our new pruning methods allow much more exhaustive pruning of the search space than previous methods. We show that the size of the search space can be reduced by up to 50% at equal precision over the previous state of the art, and the RTF by 20%.   The abstract pruning criterion can be considered a guide to derive effective pruning methods for any kind of time synchronous decoder.\n",
    "Index Terms: speech recognition, search, pruning\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-300"
  },
  "mcgraw12_interspeech": {
   "authors": [
    [
     "Ian",
     "McGraw"
    ],
    [
     "Alexander",
     "Gruenstein"
    ]
   ],
   "title": "Estimating word-stability during incremental speech recognition",
   "original": "i12_1019",
   "page_count": 4,
   "order": 304,
   "p1": "1019",
   "pn": "1022",
   "abstract": [
    "Many speech user interfaces can be improved by incrementally displaying or interpreting a speech recognizer's current best path as a user speaks. This gives rise to a problem of instability, whereby the best path may change frequently, particularly with respect to the words most recently spoken. Introducing a lag between the audio most recently processed and the portion of the best path shown to the user can lead to a more usable incremental results. In the ideal case, the lag introduced would vary to recover exactly the longest stable prefix of the best path. In this paper, we introduce a framework for estimating a stability statistic for each word, and explore the tradeoff of stability and lag by thresholding stability statistics estimated using a variety of features.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-301"
  },
  "ziegler12_interspeech": {
   "authors": [
    [
     "Stefan",
     "Ziegler"
    ],
    [
     "Bogdan",
     "Ludusan"
    ],
    [
     "Guillaume",
     "Gravier"
    ]
   ],
   "title": "Using broad phonetic classes to guide search in automatic speech recognition",
   "original": "i12_1023",
   "page_count": 4,
   "order": 305,
   "p1": "1023",
   "pn": "1026",
   "abstract": [
    "This work presents a novel framework to guide the Viterbi decoding process of a hidden Markov model based speech recognition system by means of broad phonetic classes. In a first step, decision trees are employed, along with frame and segment based attributes, in order to detect broad phonetic classes in the speech signal. Then, the detected phonetic classes are used to reinforce paths in the search process, either at every frame or at phonetically significant landmarks. Results obtained on French broadcast news data show a relative improvement in word error rate of about 2% with respect to the baseline.\n",
    "Index Terms: Viterbi decoding, broad phonetic classes, landmarks\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-302"
  },
  "miranda12_interspeech": {
   "authors": [
    [
     "João",
     "Miranda"
    ],
    [
     "João Paulo da Silva",
     "Neto"
    ],
    [
     "Alan W.",
     "Black"
    ]
   ],
   "title": "Parallel combination of multilingual speech streams for improved ASR",
   "original": "i12_1027",
   "page_count": 4,
   "order": 306,
   "p1": "1027",
   "pn": "1030",
   "abstract": [
    "In a growing number of applications, such as simultaneous interpretation, audio or text may be available conveying the same information in different languages. These different views contain redundant information that can be explored to enhance the performance of speech and language processing applications. We propose a method that directly integrates ASR word graphs or lattices and phrase tables from an SMT system to combine such parallel speech data and improve ASR performance. We apply this technique to speeches from four European Parliament committees and obtain a 16.6% relative improvement (20.8% after a second iteration) in WER, when Portuguese and Spanish interpreted versions are combined with the original English speeches. Our results indicate that further improvements may be possible by including additional languages.\n",
    "Index Terms: multistream combination, speech recognition, machine translation\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-303"
  },
  "bougares12_interspeech": {
   "authors": [
    [
     "Fethi",
     "Bougares"
    ],
    [
     "Mickael",
     "Rouvier"
    ],
    [
     "Yannick",
     "Estève"
    ],
    [
     "Georges",
     "Linarès"
    ]
   ],
   "title": "Low latency combination of parallelized single-pass LVCSR systems",
   "original": "i12_1031",
   "page_count": 4,
   "order": 307,
   "p1": "1031",
   "pn": "1034",
   "abstract": [
    "Recent progresses in hardware technology, especially multi-cores CPUs, offer new perspectives to accelerate software in parallelizing the computing process. But, it is still considered as a difficult task to parallelize the execution of an ASR system, as the different steps are usually sequential. This paper addresses some opportunities offered by parallelism for ASR system combination: the proposed approach consists in making these ASR systems exchange information during the decoding process, on the fly, whereas classical approach consists in only combining final outputs. This approach is particularly relevant for applications which need a very low latency response. This paper presents some preliminary results which show a 14% relative reduction in word error rate with a limited impact on the latency due to ASR system combination.\n",
    "Index Terms: LVCSR system harnessing, driven decoding, local ROVER combination\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-304"
  },
  "kim12c_interspeech": {
   "authors": [
    [
     "Jungsuk",
     "Kim"
    ],
    [
     "Jike",
     "Chong"
    ],
    [
     "Ian",
     "Lane"
    ]
   ],
   "title": "Efficient on-the-fly hypothesis rescoring in a hybrid GPU/CPU-based large vocabulary continuous speech recognition engine",
   "original": "i12_1035",
   "page_count": 4,
   "order": 308,
   "p1": "1035",
   "pn": "1038",
   "abstract": [
    "Effectively exploiting the resources available on modern multicore and manycore processors for tasks such as large vocabulary continuous speech recognition (LVCSR) is far from trivial. While prior works have demonstrated the effectiveness of manycore graphic processing units (GPU) for high-throughput, limited vocabulary speech recognition, they are unsuitable for recognition with large acoustic and language models due to the limited 1-6GB of memory on GPUs. To overcome this limitation, we introduce a novel architecture for WFST-based LVCSR that jointly leverages manycore graphic processing units (GPU) and multicore processors (CPU) to efficiently perform recognition even when large acoustic and language models are applied. In the proposed approach, recognition is performed on the GPU using an H-level WFST, composed using a unigram language model. During decoding partial hypotheses generated over this network are rescored on-the-fly using a large language model stored on the CPU. By maintaining N-best hypotheses during decoding our proposed architecture obtains comparable accuracy to a standard CPU-based WFST decoder while improving decoding speed by over a factor of 10.9x.\n",
    "Index Terms: Large Vocabulary Continuous Speech Recognition, WFST, On-The-Fly Rescoring, Graphics Processing Units\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-305"
  },
  "lehr12_interspeech": {
   "authors": [
    [
     "Maider",
     "Lehr"
    ],
    [
     "Emily",
     "Prud'hommeaux"
    ],
    [
     "Izhak",
     "Shafran"
    ],
    [
     "Brian",
     "Roark"
    ]
   ],
   "title": "Fully automated neuropsychological assessment for detecting mild cognitive impairment",
   "original": "i12_1039",
   "page_count": 4,
   "order": 309,
   "p1": "1039",
   "pn": "1042",
   "abstract": [
    "The ability to screen a large population and identify symptoms of Mild Cognitive Impairment (MCI), the earliest stage of dementia, is becoming increasingly important as the aged population grows and research gains are made in delaying the progression of cognitive degeneration. In this paper we present an end-to-end system for automatically scoring spoken responses to a narrative recall test commonly administered to seniors as part of clinical neuropsychological assessment. In this test, a patient listens to a brief narrative, immediately retells it, then retells it again later in the session, after some time has elapsed. ASR transcripts of retellings are automatically aligned to the source narrative, and features are extracted that replicate the published clinical scoring method, which are then used for automatic assessment using a classifier. On a test corpus of 72 subjects, we empirically evaluate different ASR adaptation strategies and analyze the errors and their relationship to clinical assessment accuracy. Despite imperfect recognition, the system presented here yields classification accuracy comparable to that of scores derived from manual transcripts. Our results show that automatic scoring of neuropsychological assessment such as Wechsler Logical Memory (WLM) is practical for screening large cohorts.\n",
    "Index Terms: clinical diagnostics, classifying mild cognitive impairment\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-306"
  },
  "bone12b_interspeech": {
   "authors": [
    [
     "Daniel",
     "Bone"
    ],
    [
     "Matthew P.",
     "Black"
    ],
    [
     "Chi-Chun",
     "Lee"
    ],
    [
     "Marian E.",
     "Williams"
    ],
    [
     "Pat",
     "Levitt"
    ],
    [
     "Sungbok",
     "Lee"
    ],
    [
     "Shrikanth",
     "Narayanan"
    ]
   ],
   "title": "Spontaneous-speech acoustic-prosodic features of children with autism and the interacting psychologist",
   "original": "i12_1043",
   "page_count": 4,
   "order": 310,
   "p1": "1043",
   "pn": "1046",
   "abstract": [
    "Atypical prosody, often reported in children with Autism Spectrum Disorders, is described by a range of qualitative terms that reflect the eccentricities and variability among persons in the spectrum. We investigate various word- and phonetic-level features from spontaneous speech that may quantify the cues reflecting prosody. Furthermore, we introduce the importance of jointly modeling the psychologist's vocal behavior in this dyadic interaction. We demonstrate that acoustic-prosodic features of both participants correlate with the children's rated autism severity. For increasing perceived atypicality, we find children's prosodic features that suggest emonotonicf speech, variable volume, atypical voice quality, and slower rate of speech. Additionally, we find the psychologist's features inform their perception of a child's atypical behavior. e.g., the psychologist's pitch slope and jitter are increasingly variable and their speech rate generally decreases.\n",
    "Index Terms: atypical prosody, autism spectrum disorder, intonation, psychologist, voice quality, ADOS\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-307"
  },
  "kaland12_interspeech": {
   "authors": [
    [
     "Constantijn",
     "Kaland"
    ],
    [
     "Emiel",
     "Krahmer"
    ],
    [
     "Marc",
     "Swerts"
    ]
   ],
   "title": "Contrastive intonation in autism: the effect of speaker- and listener-perspective",
   "original": "i12_1047",
   "page_count": 4,
   "order": 311,
   "p1": "1047",
   "pn": "1050",
   "abstract": [
    "To indicate that a referent is minimally disinguishable from a previously mentioned antecedent speakers can use contrastive intonation. Commonly, the antecedent is shared with the listener. However, in natural discourse interlocutors may not share all information. In a previous study we found that typically developing speakers can account for such perspective differences when producing contrastive intonation. It is known that in autism the ability to account for another's mental state is impaired and prosody is atypical. In the current study we investigate to what extent speakers with an autism spectrum disorder account for their listeners when producing contrastive intonation. Results show that typical and autistic speakers produce contrastive intonation similarly although they sound prosodically different.\n",
    "Index Terms: prosody, contrastive intonation, autism, prominence\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-308"
  },
  "hagedorn12_interspeech": {
   "authors": [
    [
     "Christina",
     "Hagedorn"
    ],
    [
     "Michael",
     "Proctor"
    ],
    [
     "Louis",
     "Goldstein"
    ],
    [
     "Maria Luisa Gorno",
     "Tempini"
    ],
    [
     "Shrikanth S.",
     "Narayanan"
    ]
   ],
   "title": "Characterizing covert articulation in apraxic speech using real-time MRI",
   "original": "i12_1051",
   "page_count": 4,
   "order": 312,
   "p1": "1051",
   "pn": "1054",
   "abstract": [
    "We aimed to test whether real-time magnetic resonance imaging (rtMRI) could be profitably employed to shed light on apraxic speech, particularly by revealing covert articulations. Our pilot data show that covert (silent) gestural intrusion errors (employing an intrinsically simple 1:1 mode of coupling) are made more frequently by the apraxic subject than by normal subjects. Further, we find that covert intrusion errors are pervasive in non-repetitious speech. We demonstrate that what is usually an acoustically silent period before the initiation of apraxic speech oftentimes contains completely covert gestures that occur frequently with multigestural segments. Further, we find that covert gestures corresponding to entire words are produced. Using rtMRI to investigate covert articulatory gestures, we are able to gather information about apraxic speech that traditional methods of transcription based on acoustic data are not at all able to capture.\n",
    "Index Terms: Apraxia, speech production, covert articulation, speech error, real-time MRI, disordered speech, gestures\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-309"
  },
  "abad12_interspeech": {
   "authors": [
    [
     "Alberto",
     "Abad"
    ],
    [
     "Anna",
     "Pompili"
    ],
    [
     "Angela",
     "Costa"
    ],
    [
     "Isabel",
     "Trancoso"
    ]
   ],
   "title": "Automatic word naming recognition for treatment and assessment of aphasia",
   "original": "i12_1055",
   "page_count": 4,
   "order": 313,
   "p1": "1055",
   "pn": "1058",
   "abstract": [
    "VITHEA is an on-line platform designed to act as a \"virtual therapist\" for the treatment of Portuguese speaking aphasic patients. Concretely, the system integrates automatic speech recognition technology to provide word naming exercises to individuals with lost or reduced word naming ability. In this paper, we present the solution adopted for the word naming task, which is based on a keyword spotting approach with hybrid HMM/MLP speech recognizer. Furthermore, we explore a simple cross-validation method that makes use of the patients measured word naming ability to automatically adapt to their speech particularities. A corpus with word naming therapy sessions of aphasic Portuguese native speakers has been collected to test the utility of the approach for both global evaluation and treatment. In spite of the different patient characteristics and speech quality conditions of the collected data, encouraging results have been obtained.\n",
    "Index Terms: speech disorders, aphasia, word naming\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-310"
  },
  "quatieri12_interspeech": {
   "authors": [
    [
     "Thomas F.",
     "Quatieri"
    ],
    [
     "Nicolas",
     "Malyska"
    ]
   ],
   "title": "Vocal-source biomarkers for depression: a link to psychomotor activity",
   "original": "i12_1059",
   "page_count": 4,
   "order": 314,
   "p1": "1059",
   "pn": "1062",
   "abstract": [
    "A hypothesis in characterizing human depression is that change in the brain's basal ganglia results in a decline of motor coordination. Such a neuro-physiological change may therefore affect laryngeal control and dynamics. Under this hypothesis, toward the goal of objective monitoring of depression severity, we investigate vocal-source biomarkers for depression; specifically, source features that may relate to precision in motor control, including vocal-fold shimmer and jitter, degree of aspiration, fundamental frequency dynamics, and frequencydependence of variability and velocity of energy. We use a 35-subject database collected by Mundt et al. in which subjects were treated over a six-week period, and investigate correlation of our features with clinical (HAMD), as well as self-reported (QIDS) Total subject assessment. To explicitly address the motor aspect of depression, we compute correlations with the Psychomotor Retardation component of clinical and selfreported Total assessments. For our longitudinal database, most correlations point to statistical relationships of our vocal-source biomarkers with psychomotor activity, as well as with depression severity.\n",
    "Index Terms: major depressive disorder, motor coordination, laryngeal control, vocal biomarkers\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-311"
  },
  "drugman12_interspeech": {
   "authors": [
    [
     "Thomas",
     "Drugman"
    ],
    [
     "Jerome",
     "Urbain"
    ],
    [
     "Nathalie",
     "Bauwens"
    ],
    [
     "Ricardo",
     "Chessini"
    ],
    [
     "Anne-Sophie",
     "Aubriot"
    ],
    [
     "Patrick",
     "Lebecque"
    ],
    [
     "Thierry",
     "Dutoit"
    ]
   ],
   "title": "Audio and contact microphones for cough detection",
   "original": "i12_1303",
   "page_count": 4,
   "order": 315,
   "p1": "1303",
   "pn": "1306",
   "abstract": [
    "In the framework of assessing the pathology severity in chronic cough diseases, medical literature underlines the lack of tools for allowing the automatic, objective and reliable detection of cough events. This paper describes a system based on two microphones which we developed for this purpose. The proposed approach relies on a large variety of audio descriptors, an efficient algorithm of feature selection based on their mutual information and the use of artificial neural networks. First, the possible use of a contact microphone (placed on the patient's thorax or trachea) in complement to the audio signal is investigated. This study underlines that this contact microphone suffers from reliability issues, and conveys little new relevant information compared to the audio modality. Secondly, the proposed audio-only approach is compared to a commercially available system using four sensors on a database with different sound categories often misdetected as coughs, and produced in various conditions. With average sensitivity and specificity of 94.7% and 95% respectively, the proposed method achieves better cough detection performance than the commercial system.\n",
    "Index Terms: Audio Processing, Audio Event Detection, Cough Detection, Cystic Fibrosis\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-312"
  },
  "chen12g_interspeech": {
   "authors": [
    [
     "Nancy F.",
     "Chen"
    ],
    [
     "Wade",
     "Shen"
    ],
    [
     "Joseph P.",
     "Campbell"
    ]
   ],
   "title": "Analyzing and interpreting automatically learned rules across dialects",
   "original": "i12_1307",
   "page_count": 4,
   "order": 316,
   "p1": "1307",
   "pn": "1310",
   "abstract": [
    "In this paper, we demonstrate how informative dialect recognition systems such as acoustic pronunciation model (APM) can help speech scientists locate and analyze phonetic rules efficiently. In particular, we analyze dialect-specific characteristics automatically learned from APM across two American English dialects. We show that unsupervised rule retrieval performs similarly to supervised retrieval, indicating that APM is useful in practical scenarios, where word transcriptions are often unavailable. We also demonstrate that the top-ranking rules learned from APM generally correspond to the linguistic literature, and can even pinpoint potential research directions to refine existing linguistic knowledge. The APM system can help phoneticians analyze rules efficiently by characterizing large amounts of data to postulate rule candidates, so phoneticians can save time and manual effort to conduct more targeted investigations.Potential applications of informative dialect recognition systems include forensic phonetics and diagnosis of spoken language disorders.\n",
    "Index Terms: informative dialect recognition, rule retrieval, phonological rules, forensic phonetics\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-313"
  },
  "raev12_interspeech": {
   "authors": [
    [
     "Andrey",
     "Raev"
    ],
    [
     "Yuri",
     "Matveev"
    ],
    [
     "Tatiana",
     "Goloshchapova"
    ]
   ],
   "title": "The effect of use of drugs on speaker's fundamental frequency and formants",
   "original": "i12_1311",
   "page_count": 4,
   "order": 317,
   "p1": "1311",
   "pn": "1314",
   "abstract": [
    "In this paper we investigate speech recordings before and after speaker's drug-abuse treatment, and show that there is no statistically significant dependency between distortions of speaker's fundamental frequency and formants on the one side, and different groups of drugs and on the degree of drug intoxication on the other. Changes of the fundamental frequency are not regular and do not have a general nature. The main reasons for these changes are changes in the emotional state of speakers, rather than a drug addiction treatment. Exploring the effect of the duration of narcotic drugs usage on the speaker's fundamental frequency showed that voices of speakers with prolonged use of drugs of the heroin group tend to decrease the fundamental frequency by about 3% per year.\n",
    "Index Terms: fundamental frequency, formants, drug intoxication, speaker\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-314"
  },
  "swerts12b_interspeech": {
   "authors": [
    [
     "Marc",
     "Swerts"
    ],
    [
     "Cees de",
     "Bie"
    ]
   ],
   "title": "On the assessment of audiovisual cues to speaker confidence by preteens with typical development (TD) and a-typical development (AD)",
   "original": "i12_1315",
   "page_count": 4,
   "order": 318,
   "p1": "1315",
   "pn": "1318",
   "abstract": [
    "This paper looks into how preteens with autism (Asperger, PDD-NOS) compare to healthy controls (matched in terms of age, IQ and educational level) in the way they interpret audiovisual expressions produced by adult or child speakers. In previous research, we had recorded utterances from those groups of speakers as they were responding to easy and difficult questions in a quiz-like experiment, so that they were not always equally confident about the correctness of a given answer. The task given to the preteens in the current study was to judge how certain a speaker appeared in his/her response to a question, where they could base such judgments both on auditory and visual properties of the speakers. Results reveal that all groups of preteens are able to estimate a speaker's confidence level on the basis of audiovisual properties, albeit that the preteens diagnosed with PDD-NOS performed significantly worse at this than the other two groups. Moreover, in line with previous results, participants found the data coming from child speakers harder to judge than those produced by adult speakers.\n",
    "Index Terms: nonverbal communication, prosody, cues to speaker confidence, autism\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-315"
  },
  "chaspari12_interspeech": {
   "authors": [
    [
     "Theodora",
     "Chaspari"
    ],
    [
     "Chi-Chun",
     "Lee"
    ],
    [
     "Shrikanth",
     "Narayanan"
    ]
   ],
   "title": "Interplay between verbal response latency and physiology of children with autism during ECA interactions",
   "original": "i12_1319",
   "page_count": 4,
   "order": 319,
   "p1": "1319",
   "pn": "1322",
   "abstract": [
    "The affective state of children with autism is not always expressed or discernible through observational cues, a phenomenon which is further confounded by vast variability across individuals on the autism spectrum. Electrodermal Activity (EDA) is a physiological signal indicative of a person's arousal and thus affording us new insights into a child's inner affective state. In this work we study EDA cues of children with autism while interacting with an Embodied Conversational Agent (ECA). EDA is affected by both cognitive and social factors. In this paper, we consider the child's verbal response latency as the overt behavioral cue and link it with his/her physiology. A classification experiment was performed to differentiate between physiological cues of high and low verbal response latency intervals, based on the assumption that different kinds of mechanisms are triggered in each case. Our results indicate that physiological patterns between short and long verbal response latencies are more discriminative for some children than others, suggesting the existence of multiple levels of cognitive and social efforts across children. They also show variable levels of arousal response, which can provide a complementary view of the observational cues.\n",
    "Index Terms: Electrodermal response, verbal response latency, affective state, cognitive and social activity, autism\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-316"
  },
  "kim12d_interspeech": {
   "authors": [
    [
     "Myung Jong",
     "Kim"
    ],
    [
     "Hoirin",
     "Kim"
    ]
   ],
   "title": "Combination of multiple speech dimensions for automatic assessment of dysarthric speech intelligibility",
   "original": "i12_1323",
   "page_count": 4,
   "order": 320,
   "p1": "1323",
   "pn": "1326",
   "abstract": [
    "This paper focuses on the problem of automatically assessing the speech intelligibility of patients with dysarthria, which is a motor speech disorder. To effectively capture the characteristics of the speech disorder, various features are extracted in three speech dimensions such as phonetic quality, prosody, and voice quality. Then, we find the best feature set satisfying a new feature selection criterion that the selected features produce small prediction errors as well as low mutual dependency among them. Finally, the selected features are combined using support vector regression. Evaluation of the proposed method on a database of 94 speakers with dysarthria yielded an root mean square error of 8.1 to subjectively rated scores in the range of 0 to 100, which is a promising performance that the system can be successfully applied to help a speech therapist diagnosing the degree of speech disorder.\n",
    "Index Terms: Dysarthria, feature selection, speech dimension, speech intelligibility, support vector regression\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-317"
  },
  "wang12e_interspeech": {
   "authors": [
    [
     "Jun",
     "Wang"
    ],
    [
     "Ashok",
     "Samal"
    ],
    [
     "Jordan R.",
     "Green"
    ],
    [
     "Frank",
     "Rudzicz"
    ]
   ],
   "title": "Whole-word recognition from articulatory movements for silent speech interfaces",
   "original": "i12_1327",
   "page_count": 4,
   "order": 321,
   "p1": "1327",
   "pn": "1330",
   "abstract": [
    "Articulation-based silent speech interfaces convert silently produced speech movements into audible words. These systems are still in their experimental stages, but have significant potential for facilitating oral communication in persons with laryngectomy or speech impairments. In this paper, we report the result of a novel, real-time algorithm that recognizes whole-words based on articulatory movements. This approach differs from prior work that has focused primarily on phoneme-level recognition based on articulatory features. On average, our algorithm missed 1.93 words in a sequence of twenty-five words with an average latency of 0.79 seconds for each word prediction using a data set of 5,500 isolated word samples collected from ten speakers. The results demonstrate the effectiveness of our approach and its potential for building a real-time articulation-based silent speech interface for health applications.\n",
    "Index Terms: silent speech recognition, speech impairment, laryngectomy, support vector machine\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-318"
  },
  "yin12_interspeech": {
   "authors": [
    [
     "Shou-Chun",
     "Yin"
    ],
    [
     "Richard C.",
     "Rose"
    ],
    [
     "Yun",
     "Tang"
    ]
   ],
   "title": "Verifying session level pronunciation accuracy in a speech therapy application",
   "original": "i12_1331",
   "page_count": 4,
   "order": 322,
   "p1": "1331",
   "pn": "1334",
   "abstract": [
    "This paper investigates a new pronunciation verification (PV) approach obtained from the subspace based Gaussian mixture model (SGMM) based pronunciation model. A single SGMM model is trained from disabled speakers' utterances and reference speakers' utterances.The PV scores are computed directly from distances between disabled and reference speaker projection vectors. Both session level and utterance level PV scenarios are presented and evaluated. The PV performance is compared with respect to an approach based on the lattice posterior probabilities.\n",
    "Index Terms: confidence measure, speech therapy\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-319"
  },
  "mehta12_interspeech": {
   "authors": [
    [
     "Daryush D.",
     "Mehta"
    ],
    [
     "Rebecca Woodbury",
     "Listfield"
    ],
    [
     "Harold A.",
     "Cheyne II"
    ],
    [
     "James T.",
     "Heaton"
    ],
    [
     "Shengran W.",
     "Feng"
    ],
    [
     "Matías",
     "Zañartu"
    ],
    [
     "Robert E.",
     "Hillman"
    ]
   ],
   "title": "Duration of ambulatory monitoring needed to accurately estimate voice use",
   "original": "i12_1335",
   "page_count": 4,
   "order": 323,
   "p1": "1335",
   "pn": "1338",
   "abstract": [
    "Voice use is considered to play a major role in the development of many voice disorders, and clinicians focus on evaluating and modifying how patients use their voices throughout the day. Some voice monitoring devices have used neck-mounted accelerometers to unobtrusively and confidentially track voice use.related measures, such as phonation time, fundamental frequency, and sound intensity. Guidelines for the clinical use of such monitoring devices have yet to be established. This is a preliminary investigation to establishing initial benchmarks for obtaining robust estimates of long-term average voice use that may be used to begin examining basic relationships between vocal loading and voice use.related pathology. As expected, adequate monitoring durations depend on the inherent variability of the parameter of interest, with much of the error decreased after 26 hours of monitoring. Investigations are currently under way to take advantage of a smartphone-based voice monitoring system that is designed to enhance device wearability and enable the derivation of new clinically relevant measures.\n",
    "Index Terms: ambulatory voice monitoring, voice use, voice disorders, accelerometer\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-320"
  },
  "hassanali12_interspeech": {
   "authors": [
    [
     "Khairun-nisa",
     "Hassanali"
    ],
    [
     "Yang",
     "Liu"
    ],
    [
     "Thamar",
     "Solorio"
    ]
   ],
   "title": "Evaluating NLP features for automatic prediction of language impairment using child speech transcripts",
   "original": "i12_1339",
   "page_count": 4,
   "order": 324,
   "p1": "1339",
   "pn": "1342",
   "abstract": [
    "Language impairment (LI) in children is pervasive in all walks of life. Automatic prediction of LI is useful as a first pass for speech language pathologists in identifying prospective children with LI. Previous work in the automatic prediction of LI has explored various features, mostly shallow and surface level features. In this paper, we evaluate deeper NLP features such as syntactic, semantic and entity grid model features, along with narrative structure and quality features in the prediction of language impairment using child language transcripts. Our experiments show that narrative structure and quality features along with a combination of other features are helpful in the prediction of language impairment in storytelling narratives. Tue.SS5.11 16:00.18:00 Quantitative Analysis of Pitch in Speech of Children with Neurodevelopmental Disorders Geza Kiss, Jan P.H. van Santen, Emily Tucker Prudfhommeaux, Lois M. Black We analyzed the prosody of children with Autism Spectrum Disorders, Developmental Language Disorders, and Typical Development in conversational speech, using the CSLU ADOS speech corpus. We found several significant differences in the pitch characteristics of these diagnostic groups, and report automatic classification utilizing these features that are significantly better than chance. We show that the choice of pitch tracker, its parameters, and the potential pitch correction can substantially affect the results, thus the scientific relevance of work on prosody.\n",
    "Index Terms: language impairment, machine learning, natural language processing\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-321"
  },
  "kiss12_interspeech": {
   "authors": [
    [
     "Géza",
     "Kiss"
    ],
    [
     "Jan P. H. van",
     "Santen"
    ],
    [
     "Emily",
     "Prud'hommeaux"
    ],
    [
     "Lois M.",
     "Black"
    ]
   ],
   "title": "Quantitative analysis of pitch in speech of children with neurodevelopmental disorders",
   "original": "i12_1343",
   "page_count": 4,
   "order": 325,
   "p1": "1343",
   "pn": "1346",
   "abstract": [
    "We analyzed the prosody of children with Autism Spectrum Disorder, Developmental Language Disorder, and typical development in conversational speech, using the CSLU ADOS speech corpus. We found several significant differences in the pitch characteristics of these diagnostic groups, and report automatic classification utilizing these features that are well above chance level. We show that the choice of pitch tracker, its parameters, and the pitch correction method can substantially affect the results, thus the scientific relevance of studies on prosody, and may be one of the reasons for conflicting findings.\n",
    "Index Terms: prosody, pitch, F0, disorder, autism, language impairment, ASD, HFA, ALI, ALN, SLI, DLD\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-322"
  },
  "jyothi12_interspeech": {
   "authors": [
    [
     "Preethi",
     "Jyothi"
    ],
    [
     "Eric",
     "Fosler-Lussier"
    ],
    [
     "Karen",
     "Livescu"
    ]
   ],
   "title": "Discriminatively learning factorized finite state pronunciation models from dynamic Bayesian networks",
   "original": "i12_1063",
   "page_count": 4,
   "order": 326,
   "p1": "1063",
   "pn": "1066",
   "abstract": [
    "This paper describes an approach to efficiently derive, and discriminatively train, a weighted finite state transducer (WFST) representation for an articulatory feature-based model of pronunciation. This model is originally implemented as a dynamic Bayesian network (DBN). The work is motivated by a desire to (1) incorporate such a pronunciation model in WFST-based recognizers, and to (2) learn discriminative models that are more general than the DBNs. The approach is quite general, though here we show how it applies to a specific model. We use the conditional independence assumptions imposed by the DBN to efficiently convert it into a sequence of WFSTs (factor FSTs) which, when composed, yield the same model as the DBN. We then introduce a linear model of the arc weights of the factor FSTs and discriminatively learn its weights using the averaged perceptron algorithm. We demonstrate the approach using a lexical access task in which we recognize a word given its surface realization. Our experimental results using a phonetically transcribed subset of the Switchboard corpus show that the discriminatively learned model performs significantly better than the original DBN.\n",
    "Index Terms: articulatory features, discriminative training, finite state transducers, dynamic Bayesian networks\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-323"
  },
  "deoras12_interspeech": {
   "authors": [
    [
     "Anoop",
     "Deoras"
    ],
    [
     "Ruhi",
     "Sarikaya"
    ],
    [
     "Gokhan",
     "Tur"
    ],
    [
     "Dilek",
     "Hakkani-Tür"
    ]
   ],
   "title": "Joint decoding for speech recognition and semantic tagging",
   "original": "i12_1067",
   "page_count": 4,
   "order": 327,
   "p1": "1067",
   "pn": "1070",
   "abstract": [
    "Most conversational understanding (CU) systems today employ a cascade approach, where the best hypothesis from automatic speech recognizer (ASR) is fed into spoken language understanding (SLU) module, whose best hypothesis is then fed into other systems such as interpreter or dialogue manager. In such approaches, errors from one statistical module irreversibly propagates into another module causing a serious degradation in the overall performance of the conversational understanding system. Thus it is desirable to jointly optimize all the statistical modules together. As a first step towards this, in this paper, we propose a joint decoding framework in which we predict the optimal word as well as slot (semantic tag) sequence jointly given the input acoustic stream. On Microsoft's CU system, we show 1.3% absolute reduction in word error rate (WER) and 1.2% absolute improvement in F measure for slot prediction when compared to a very strong cascade baseline comprising of the state-of-the-art recognizer followed by a slot sequence tagger.\n",
    "Index Terms: ME, CRF, SLU, CU, ASR\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-324"
  },
  "bashashaik12_interspeech": {
   "authors": [
    [
     "M. Ali",
     "Basha Shaik"
    ],
    [
     "Amr El-Desoky",
     "Mousa"
    ],
    [
     "Ralf",
     "Schlüter"
    ],
    [
     "Hermann",
     "Ney"
    ]
   ],
   "title": "Investigation of maximum entropy hybrid language models for open vocabulary German and Polish LVCSR",
   "original": "i12_1071",
   "page_count": 4,
   "order": 328,
   "p1": "1071",
   "pn": "1074",
   "abstract": [
    "For languages like German and Polish, higher numbers of word inflections lead to high out-of-vocabulary (OOV) rates and high language model (LM) perplexities. Thus, one of the main challenges in large vocabulary continuous speech recognition (LVCSR) is recognizing an open vocabulary. In this paper, we investigate the use of mixed type of sub-word units in the same recognition lexicon. Namely, morphemic or syllabic units combined with pronunciations called graphones, normal graphemic morphemes or syllables, along with full-words. In addition, we investigate the suitability of hybrid mixed-unit N-grams as features for Maximum Entropy LM along with adaptation. We achieve significant improvements in recognizing OOVs and word error rate reductions for German and Polish LVCSR compared to the conventional full-word approach and state-of-the-art N-gram mixed type hybrid LM.\n",
    "Index Terms: open vocabulary, maximum entropy\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-325"
  },
  "dixon12_interspeech": {
   "authors": [
    [
     "Paul R.",
     "Dixon"
    ],
    [
     "Chiori",
     "Hori"
    ],
    [
     "Hideki",
     "Kashioka"
    ]
   ],
   "title": "A specialized WFST approach for class models and dynamic vocabulary",
   "original": "i12_1075",
   "page_count": 4,
   "order": 329,
   "p1": "1075",
   "pn": "1078",
   "abstract": [
    "In this paper we describe a specialized Weighted Finite State Transducer (WFST) framework for handling class language models and dynamic vocabulary in automatic speech recognition. The proposed framework has several important features. A fused composition algorithm that substantially reduces the memory usage in comparison to generic WFST operations, and an efficient dynamic vocabulary scheme that allows for arbitrary new words to be added to class based language models on-the-fly without requiring any changes to the pre-compiled transducers. The dynamic vocabulary approach achieves very low run-time costs by representing the dynamic vocabulary items inserted into the language model from an optimum set of existing lexicon items. Experimental results on a voice search task illustrate the low runtime costs of the proposed approach.\n",
    "Index Terms: speech recognition, decoding, WFST\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-326"
  },
  "novak12_interspeech": {
   "authors": [
    [
     "Josef R.",
     "Novak"
    ],
    [
     "Nobuaki",
     "Minematsu"
    ],
    [
     "Keikichi",
     "Hirose"
    ]
   ],
   "title": "Dynamic grammars with lookahead composition for WFST-based speech recognition",
   "original": "i12_1079",
   "page_count": 4,
   "order": 330,
   "p1": "1079",
   "pn": "1082",
   "abstract": [
    "Automatic Speech Recognition (ASR) applications often employ a mixture of static and dynamic grammar components, and can thus benefit from the ability to efficiently modify the system vocabulary and other parameters in an on-line mode. This paper presents a novel, generic approach to dynamic grammar handling in the context of the Weighted Finite-State Transducer (WFST) paradigm. The method relies on a straightforward extension of the lexicon and underlying grammar components, and leverages the ideas of on-the-fly composition and delayed construction to efficiently generate the recognition search space on-the-fly. The alternative partitioning of component models that this approach implies can also result in significant storage savings. In contrast to previous works in this area, the proposed method relies only on generic WFST operations and the context-dependency, lexicon and grammar components that form the basis of standard ASR cascades.\n",
    "Index Terms: WFST, ASR, Dynamic vocabulary, Spoken dialog systems\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-327"
  },
  "shore12_interspeech": {
   "authors": [
    [
     "Todd",
     "Shore"
    ],
    [
     "Friedrich",
     "Faubel"
    ],
    [
     "Hartmut",
     "Helmke"
    ],
    [
     "Dietrich",
     "Klakow"
    ]
   ],
   "title": "Knowledge-based word lattice rescoring in a dynamic context",
   "original": "i12_1083",
   "page_count": 4,
   "order": 331,
   "p1": "1083",
   "pn": "1086",
   "abstract": [
    "Recent advances in automatic speech recognition (ASR) technology continue to be based heavily on data-driven methods, meaning that the full benefits of such research are often not enjoyed in domains for which there is little training data. Moreover, tractability is often an issue with these methods when conditioning for long-distance dependencies, entailing that many higher-level knowledge sources such as situational knowledge cannot be easily utilized in classification. This paper describes an effort to circumvent this problem by using dynamic contextual knowledge to rescore ASR lattice output using a dynamic weighted constraint satisfaction function. With this method, it was possible to achieve a roughly 80% reduction in WER for ASR in the context of an air traffic control scenario.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-328"
  },
  "mcclanahan12_interspeech": {
   "authors": [
    [
     "Richard D.",
     "McClanahan"
    ],
    [
     "Phillip L. De",
     "Leon"
    ]
   ],
   "title": "Mixture component clustering for efficient speaker verification",
   "original": "i12_1087",
   "page_count": 4,
   "order": 332,
   "p1": "1087",
   "pn": "1090",
   "abstract": [
    "In speaker verification (SV) systems based on a support vector machine (SVM) using Gaussian mixture model (GMM) supervectors, a large portion of the test-stage computational load is the calculation of the a posteriori probabilities of the feature vectors for the given universal background model (UBM). Furthermore, the calculation of the sufficient statistics for the mean also contributes substantially to computational load. In this paper, we propose several methods to cluster the GMM-UBM mixture components in order to reduce the computational load and speed up the verification. In the adaptation stage, we compare the feature vectors to the clusters and calculate the a posteriori probabilities and update the statistics exclusively for mixture components belonging to appropriate clusters. Our results, demonstrate that (on average) we can, reduce the number of a posteriori probability calculations by a factor up to 2.8x without loss in accuracy.\n",
    "Index Terms: speaker recognition, clustering methods\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-329"
  },
  "hasan12_interspeech": {
   "authors": [
    [
     "Taufiq",
     "Hasan"
    ],
    [
     "John H. L.",
     "Hansen"
    ]
   ],
   "title": "Front-end channel compensation using mixture-dependent feature transformations for i-vector speaker recognition",
   "original": "i12_1091",
   "page_count": 4,
   "order": 333,
   "p1": "1091",
   "pn": "1094",
   "abstract": [
    "State-of-the-art session variability compensation for speaker recognition are generally based on various linear statistical models of the Gaussian Mixture Model (GMM) mean super-vectors, while front-end features are only processed by standard normalization techniques. In this study, we propose a front-end channel compensation frame-work using mixture-localized linear transforms that operate before super-vector domain modeling begins. In this approach, local linear transforms are trained for each Gaussian component of a Universal Background Model (UBM), and are applied to acoustic features according to their mixture-wise probabilistic alignment, yielding an operation that is globally non-linear. We examine Principal Component Analysis (PCA), whitening, Linear Discriminant Analysis (LDA) and Nuisance Attribute Projection (NAP) as front-end feature transformations. We also propose a method, Nuisance Attribute Elimination (NAE), which is similar to NAP but performs dimensionality reduction in addition to channel compensation. We show that the proposed frame-work can be readily integrated with a standard i-Vector system by simply applying the transformations on the first order Baum-Welch statistics and transforming the UBM. Experiments performed on the telephone trials of the NIST SRE 2010 demonstrate significant performance gain from the proposed frame-work, especially using LDA as the front-end transformation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-330"
  },
  "campbell12_interspeech": {
   "authors": [
    [
     "William M.",
     "Campbell"
    ],
    [
     "Elliot",
     "Singer"
    ]
   ],
   "title": "Query-by-example using speaker content graphs",
   "original": "i12_1095",
   "page_count": 4,
   "order": 334,
   "p1": "1095",
   "pn": "1098",
   "abstract": [
    "We describe methods for constructing and using content graphs for query-by-example speaker recognition tasks within a large speech corpus. This goal is achieved as follows: First, we describe an algorithm for constructing speaker content graphs, where nodes represent speech signals and edges represent speaker similarity. Speech signal similarity can be based on any standard vector-based speaker comparison method, and the content graph can be constructed using an efficient incremental method for streaming data. Second, we apply random walk methods to the content graph to find matching examples to an unlabeled query set of speech signals. The content-graph based method is contrasted to a more traditional approach that uses supervised training and stack detectors. Performance is compared in terms of information retrieval measures and computational complexity. The new content-graph based method is shown to provide a promising low-complexity scalable alternative to standard speaker recognition methods.\n",
    "Index Terms: speaker recognition, graphs\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-331"
  },
  "sun12b_interspeech": {
   "authors": [
    [
     "Hanwu",
     "Sun"
    ],
    [
     "Bin",
     "Ma"
    ]
   ],
   "title": "Unsupervised NAP training data design for speaker recognition",
   "original": "i12_1099",
   "page_count": 4,
   "order": 335,
   "p1": "1099",
   "pn": "1102",
   "abstract": [
    "The Nuisance Attribute Projection (NAP) with labeled data provides an effective approach for improving the speaker recognition performance in the state-of-art speaker recognition system by removing unwanted speaker channel and handsets variation. However, the requirement for the labeled NAP training data may limit its practical application. In this paper, we propose an unsupervised clustering strategy to design NAP training data without labeled information about channel and speaker utterances. A fast clustering and purifying algorithm is introduced to group the unlabeled NAP training data into speaker dependent clusters to drive the NAP training data. The GMM-SVM based speaker recognition system is adopted to evaluate the performance. The system with the unsupervised NAP training data design achieves a similar performance with that using labeled NAP training data on both SRE06 1conv-1conv all English trials and SRE08 short2-short3 Tel-Tel All English trials subtasks.\n",
    "Index Terms: speaker recognition, speaker diarization, speaker cluster, Nuisance Attribute Projection\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-332"
  },
  "doddington12_interspeech": {
   "authors": [
    [
     "George",
     "Doddington"
    ]
   ],
   "title": "The role of score calibration in speaker recognition",
   "original": "i12_1103",
   "page_count": 4,
   "order": 336,
   "p1": "1103",
   "pn": "1106",
   "abstract": [
    "The performance of speaker recognition, as measured by NIST in its periodic evaluation programs of speaker detection technology, is affected by the predictive accuracy of system decisions as well as the inherent discriminating power of the underlying algorithms. This paper analyses the score calibration accuracy of a few systems that exhibited good performance in the NIST 2010 speaker recognition evaluation (SRE10) and proposes a modified cost function that better represents the accuracy of score calibration in the low false alarm decision region, which is the region of interest for intelligence applications.\n",
    "Index Terms: speaker recognition, speaker detection, score calibration\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-333"
  },
  "hattori12_interspeech": {
   "authors": [
    [
     "Takafumi",
     "Hattori"
    ],
    [
     "Kei",
     "Hashimoto"
    ],
    [
     "Yoshihiko",
     "Nankaku"
    ],
    [
     "Keiichi",
     "Tokuda"
    ]
   ],
   "title": "A Bayesian approach to speaker recognition based on GMMs using multiple model structures",
   "original": "i12_1107",
   "page_count": 4,
   "order": 337,
   "p1": "1107",
   "pn": "1110",
   "abstract": [
    "This paper proposes a speaker recognition technique using multiple model structures based on the Bayesian approach. In recent speaker recognition, many sophisticated statistical models have been proposed, e.g., Joint Factor Analysis and i-Vector based method. However, since most of them are based on Gaussian Mixture Models (GMMs), therefore improving estimation accuracy of generative models, i.e. GMMs, with limited amount of training data is still an important problem in speaker recognition. For this purpose, a Bayesian approach which marginalizes all possible model parameters has been applied to the GMM based speaker recognition. This paper extends it to the model structure marginalization. The proposed method can improve the estimation accuracy by integrating multiple GMMs with different numbers of mixtures within the Bayesian framework. Experimental results show that the proposed method improved the identification rates from the conventional method using a single model structure.\n",
    "Index Terms: speaker recognition, GMM, Bayesian approach, model structure\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-334"
  },
  "wang12f_interspeech": {
   "authors": [
    [
     "Jianglin",
     "Wang"
    ],
    [
     "Michael",
     "Johnson"
    ]
   ],
   "title": "Residual phase cepstrum coefficients with application to cross-lingual speaker verification",
   "original": "i12_1556",
   "page_count": 4,
   "order": 338,
   "p1": "1556",
   "pn": "1559",
   "abstract": [
    "Speaker identification and verification has received a great deal of attention from the speech community, and significant gains in robustness and accuracy have been obtained over the past decade. However, the features used for identification are still primarily representations of overall spectral characteristics, and thus the models are primarily phonetic in nature, differentiating speakers based on overall pronunciation patterns. This creates difficulties in terms of the amount of enrollment data and complexity of the models required to cover the phonetic space, especially in tasks such as cross-lingual verification where enrollment and testing data may not have similar phonetic coverage. This paper introduces the use of a new feature for speaker verification, residual phase cepstral coefficients (RPCC), to capture speaker characteristics from their vocal excitation patterns. Results on a cross-lingual speaker verification task taken from the NIST 2004 SRE demonstrate that these RPCC features are significantly more accurate than traditional mel-frequency cepstral coefficients (MFCC) when the amount of enrollment data available for training is limited. Additionally, because of the significant differences in the nature of the features, combining MFCC and RPCC features shows an improvement in verification results over MFCCs alone.\n",
    "Index Terms: speaker verification, glottal source excitation, residual phase cepstrum, GMM, UBM\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-335"
  },
  "liang12b_interspeech": {
   "authors": [
    [
     "Chunyan",
     "Liang"
    ],
    [
     "Jinchao",
     "Yang"
    ],
    [
     "Lin",
     "Yang"
    ],
    [
     "Yonghong",
     "Yan"
    ]
   ],
   "title": "Speaker veri.cation using neighborhood preserving embedding",
   "original": "i12_1560",
   "page_count": 4,
   "order": 339,
   "p1": "1560",
   "pn": "1563",
   "abstract": [
    "In this paper, we adopt a new factor analysis of neighborhood preserving embedding (NPE) for speaker verification under the support vector machine (SVM) framework. NPE aims at preserving the local neighborhood structure on the data and defines a low-dimensional speaker space called neighborhood preserving embedding space. We compare the proposed method with the state-of-the-art total variability approach on the telephone-telephone core condition of the NIST 2008 Speaker recognition evaluation (SRE) dataset. The experimental results indicate that the proposed NPE method outperforms the total variability approach, providing up to 24% relative improvement.\n",
    "Index Terms: speaker verification, neighborhood preserving embedding, total variability, support vector machine\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-336"
  },
  "liang12c_interspeech": {
   "authors": [
    [
     "Chunyan",
     "Liang"
    ],
    [
     "Xiang",
     "Zhang"
    ],
    [
     "Lin",
     "Yang"
    ],
    [
     "Yonghong",
     "Yan"
    ]
   ],
   "title": "Discriminative decision function based scoring method in joint factor analysis for speaker verification",
   "original": "i12_1564",
   "page_count": 4,
   "order": 340,
   "p1": "1564",
   "pn": "1567",
   "abstract": [
    "This paper introduces a discriminative decision function scoring method for speaker recognition with the Joint Factor Analysis (JFA) system. In the scoring module of JFA system, an approximate form of the decision function is proposed. Based on the approximation, we present a discriminative decision function by re-estimating the contribution of each speech sound unit to the decision function. The discriminative decision function is used to exploit the individual Gaussian component for better classification. The experiments are carried out on the telephone-telephone core condition of NIST SRE2010. The experimental results show that the proposed scoring method outperforms the conventional frame-by-frame and integration scoring strategies, respectively providing up to 19% and 15% relative improvement.\n",
    "Index Terms: speaker verification, joint Factor Analysis, discriminative decision function\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-337"
  },
  "hasan12b_interspeech": {
   "authors": [
    [
     "Taufiq",
     "Hasan"
    ],
    [
     "John H. L.",
     "Hansen"
    ]
   ],
   "title": "Integrated feature normalization and enhancement for robust speaker recognition using acoustic factor analysis",
   "original": "i12_1568",
   "page_count": 4,
   "order": 341,
   "p1": "1568",
   "pn": "1571",
   "abstract": [
    "State-of-the-art factor analysis based channel compensation methods for speaker recognition are based on the assumption that speaker/ utterance dependent Gaussian Mixture Model (GMM) mean super-vectors can be constrained to lie in a lower dimensional subspace, which does not consider the fact that conventional acoustic features may also be constrained in a similar way in the feature space. In this study, motivated by the low-rank covariance structure of cepstral features, we propose a factor analysis model in the acoustic feature space instead of the super-vector domain and derive a mixture dependent feature transformation. We demonstrate that, the proposed Acoustic Factor Analysis (AFA) transformation performs feature dimensionality reduction, de-correlation, variance normalization and enhancement at the same time. The transform applies a square-root Wiener gain on the acoustic feature eigenvector directions, and is similar to the signal sub-space based speech enhancement schemes. We also propose several methods of adaptively selecting the AFA parameter for each mixture. The proposed feature transform is applied using a probabilistic mixture alignment, and is integrated with a conventional i-Vector system. Experimental results on the telephone trials of the NIST SRE 2010 demonstrate the effectiveness of the proposed scheme.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-338"
  },
  "machlica12_interspeech": {
   "authors": [
    [
     "Lukáš",
     "Machlica"
    ],
    [
     "Zbyněk",
     "Zajic"
    ]
   ],
   "title": "Factor analysis and nuisance attribute projection revisited",
   "original": "i12_1572",
   "page_count": 4,
   "order": 342,
   "p1": "1572",
   "pn": "1575",
   "abstract": [
    "In the paper Factor Analysis (FA) and Nuisance Attribute Projection (NAP) are reviewed, analyzed and compared. Since nowadays FA become a part of most state-of-the-art recognition systems (used e.g. in the concept of i-vectors or PLDA models) it is of relevance to examine different insights into the problem. NAP was chosen as a counterpart to FA as an advanced PCA like method often utilized in speaker recognition systems along with FA. It is demonstrated how can be both FA and NAP expressed as solutions of Least Squares (LS), the consequences of the LS formulation are discussed, and it is shown in what extent does the solutions of NAP and FA overlap.\n",
    "Index Terms: NAP, FA, PCA, least squares\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-339"
  },
  "chen12h_interspeech": {
   "authors": [
    [
     "Sheng",
     "Chen"
    ],
    [
     "Mingxing",
     "Xu"
    ]
   ],
   "title": "Compensation of intrinsic variability with factor analysis modeling for robust speaker verification",
   "original": "i12_1576",
   "page_count": 4,
   "order": 343,
   "p1": "1576",
   "pn": "1579",
   "abstract": [
    "Performances of speaker verification systems are adversely affected by intrinsic variability in the real world applications. In this paper, factor analysis approaches of Joint Factor Analysis (JFA) and i-vector modeling are used to address the effects of intrinsic variations for robust speaker verification. The speaker variability and intrinsic variability are modeled with the speaker and session factors respectively in the JFA approach. In the i-vector framework, a low-dimensional space is defined to model the total variability and intrinsic variations are compensated with a variety of techniques including Linear Discriminant Analysis (LDA), Within-Class Covariance Normalization (WCCN) and Nuisance Attribute Projection (NAP). Experiments in the intrinsic variation corpus show that factor analysis approaches of JFA and i-vector framework perform much better than the GMM-UBM paradigm in modeling the intrinsic variability. Relative reductions in Error Equal Rate (EER) of around 39.85% and 36.76% are obtained respectively for JFA and i-Vector+LDA+WCCN speaker verification systems, compared to the GMM-UBM baseline system.\n",
    "Index Terms: speaker verification, intrinsic variability, joint factor analysis, i-vector, LDA, WCCN, NAP\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-340"
  },
  "larcher12_interspeech": {
   "authors": [
    [
     "Anthony",
     "Larcher"
    ],
    [
     "Kong Aik",
     "Lee"
    ],
    [
     "Bin",
     "Ma"
    ],
    [
     "Haizhou",
     "Li"
    ]
   ],
   "title": "RSR2015: database for text-dependent speaker verification using multiple pass-phrases",
   "original": "i12_1580",
   "page_count": 4,
   "order": 344,
   "p1": "1580",
   "pn": "1583",
   "abstract": [
    "This paper describes a new speech corpus, the RSR2015 database designed for text-dependent speaker recognition with scenario based on fixed pass-phrases. This database consists of over 71 hours of speech recorded from English speakers covering the diversity of accents spoken in Singapore. Acquisition has been done using a set of six portable devices including smart phones and tablets. The pool of speakers consists of 298 participants (142 female and 156 male speakers) from 17 to 42 years old. We propose a protocol for the case of user-dependent passphrases in text-dependent speaker recognition and we also report speaker recognition experiments on RSR2015 database.\n",
    "Index Terms: database, speaker recognition, text-dependent\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-341"
  },
  "dellwo12_interspeech": {
   "authors": [
    [
     "Volker",
     "Dellwo"
    ],
    [
     "Adrian",
     "Leemann"
    ],
    [
     "Marie-José",
     "Kolly"
    ]
   ],
   "title": "Speaker idiosyncratic rhythmic features in the speech signal",
   "original": "i12_1584",
   "page_count": 4,
   "order": 345,
   "p1": "1584",
   "pn": "1587",
   "abstract": [
    "Speakers' voices are to a high degree individual. In the present paper we report about an ongoing research project in which we study how temporal characteristics of human speech (e.g. segmental or prosodic timing patterns, speech rhythmic characteristics and durational patterns of voicing) contribute to speaker individuality. We report about the creation of the TEVOID-Corpus (Temporal Voice Idiosyncrasy) that we are currently creating in our lab at Zurich University. 8 speakers producing 16 spontaneous sentences each are currently in the database which is rapidly growing. The paper gives an overview of the general ideas for the data collection and first results showing that there are significant rhythmic differences (%V, %VO, VarcoPeak) in spontaneously produced sentences between speakers of Zurich German.\n",
    "Index Terms: speech rhythm, speaker idiosyncratic features, speaker identification, forensic phonetics\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-342"
  },
  "lei12_interspeech": {
   "authors": [
    [
     "Yun",
     "Lei"
    ],
    [
     "Lukáš",
     "Burget"
    ],
    [
     "Nicolas",
     "Scheffer"
    ]
   ],
   "title": "Bilinear factor analysis for i-vector based speaker verification",
   "original": "i12_1588",
   "page_count": 4,
   "order": 346,
   "p1": "1588",
   "pn": "1591",
   "abstract": [
    "The combination of iVector extraction and Probabilistic Linear Discriminant Analysis (PLDA) model forms a basis of the current state of the art speaker verification. The PLDA model makes an assumption that the within-speaker (or inter-session) variability in the iVector space is independent of speaker identity. In this work we propose a new model, which can be seen as an extension of PLDA, relaxing this assumption and allowing the within-speaker variability to be different for different locations of speakers in the iVector space. The potential of the proposed model is demonstrated in preliminary experiments.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-343"
  },
  "poignant12_interspeech": {
   "authors": [
    [
     "Johann",
     "Poignant"
    ],
    [
     "Hervé",
     "Bredin"
    ],
    [
     "Viet Bac",
     "Le"
    ],
    [
     "Laurent",
     "Besacier"
    ],
    [
     "Claude",
     "Barras"
    ],
    [
     "Georges",
     "Quénot"
    ]
   ],
   "title": "Unsupervised speaker identification using overlaid texts in TV broadcast",
   "original": "i12_2650",
   "page_count": 4,
   "order": 347,
   "p1": "2650",
   "pn": "2653",
   "abstract": [
    "We propose an approach for unsupervised speaker identification in TV broadcast videos, by combining acoustic speaker diarization with person names obtained via video OCR from overlaid texts. Three methods for the propagation of the overlaid names to the speech turns are compared, taking into account the co-occurence duration between the speaker clusters and the names provided by the video OCR and using a task-adapted variant of the TF-IDF information retrieval coefficient. These methods were tested on the REPERE dry-run evaluation corpus, containing 3 hours of annotated videos. Our best unsupervised system reaches a F-measure of 69.1% when considering all the speakers, and 80.9% if anchor speakers are left out. By comparison, a mono-modal, supervised speaker identification system with 535 speaker models trained on matching development data and additional TV and radio data only provided a 57.5% F-measure when considering all the speakers and 45.9% without anchor.\n",
    "Index Terms: unsupervised speaker identification, multimodal fusion, speaker diarization, optical character recognition, reproducible results\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-344"
  },
  "zhao12_interspeech": {
   "authors": [
    [
     "Yali",
     "Zhao"
    ],
    [
     "Lie",
     "Xie"
    ],
    [
     "Zhonghua",
     "Fu"
    ]
   ],
   "title": "Mask estimation and refinement for MFT-based robust speaker verification",
   "original": "i12_2654",
   "page_count": 4,
   "order": 348,
   "p1": "2654",
   "pn": "2657",
   "abstract": [
    "Missing feature theory (MFT) has been proposed to effectively improve speaker recognition performance in noisy environments. For MFTbased speaker recognition, the binary mask is required to identify those reliable and unreliable feature components. In this paper, a dualmicrophone based semi-blind Degenerate Unmixing Estimation Technique (DUET) approach is proposed to estimate the binary mask. Using the spatial information instead of the conventional statistics of noises, our proposed approach has a good mask estimation, especially when the noises are non-stationary, e.g., interfering speech or music. Experimental results show that the proposed method achieve significant improvements over alternative approaches. We further refine the estimated binary mask by removing the unreliable time frames and nondiscriminate frequency subbands. Experiments demonstrate that the refined binary mask enhances the performance of MFT-based speaker verification, and represents a promising dire ction for MFT-based applications.\n",
    "Index Terms: speaker verification, missing feature theory, dual-microphone, binary mask estimation\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-345"
  },
  "yang12_interspeech": {
   "authors": [
    [
     "Hai",
     "Yang"
    ],
    [
     "Chunyan",
     "Liang"
    ],
    [
     "Yunfei",
     "Xu"
    ],
    [
     "Lin",
     "Yang"
    ],
    [
     "Yonghong",
     "Yan"
    ]
   ],
   "title": "Sparse probabilistic linear discriminant analysis for speaker verification",
   "original": "i12_2658",
   "page_count": 4,
   "order": 349,
   "p1": "2658",
   "pn": "2661",
   "abstract": [
    "This paper introduces an approach based on a generative model named Sparse Probabilistic Linear Discriminant Analysis in speaker verification. The model provides an alternative approach to deal with the non-Gaussian behavior of the latent variables, directly assuming they are based on Laplace prior. This distribution encourages the model to set many latent variables to zero. An expectation-maximization algorithm is derived to train model with a variational approximation to a range of heavy-tailed distributions whose limit is the Laplace. The variational approximation is also used to compute of likelihood ratio. This approach performed well on the tel-tel extended condition of the NIST 2010 Speaker Recognition Evaluation, and is competitive compared to the Gaussian Probabilistic Linear Discriminant Analysis, in terms of normalized Decision Cost Function and Equal Error Rate.\n",
    "Index Terms: speaker verification, i-vectors, sparse, Laplace prior\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-346"
  },
  "sarkar12_interspeech": {
   "authors": [
    [
     "Achintya Kumar",
     "Sarkar"
    ],
    [
     "Driss",
     "Matrouf"
    ],
    [
     "Pierre Michel",
     "Bousquet"
    ],
    [
     "Jean-François",
     "Bonastre"
    ]
   ],
   "title": "Study of the effect of i-vector modeling on short and mismatch utterance duration for speaker verification",
   "original": "i12_2662",
   "page_count": 4,
   "order": 350,
   "p1": "2662",
   "pn": "2665",
   "abstract": [
    "It is well known that state-of-the-art speaker verification system using i-vector concept shows prominent performance when target speakers training and test utterances are fixed conditions: long-long as per NIST evaluation. However, most of the real-time applications of speaker verification systems are limited to different training and test durations of the speech segments. State-of-the-art speaker verification system needs to estimate some statistical parameters. The aim of this paper is to explore how to train the statistical model parameter of the state-of-the-art system while speakers training and test data are on mismatch durations. Experimental results are shown on NIST 2008 SRE for various duration of target training and test speech segments, such as 5 seconds, 10 seconds and full (5 minutes).\n",
    "Index Terms: short segment, i-vector, length normalization, PLDA, speaker verification\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-347"
  },
  "huang12d_interspeech": {
   "authors": [
    [
     "Chien-Lin",
     "Huang"
    ],
    [
     "Chiori",
     "Hori"
    ],
    [
     "Hideki",
     "Kashioka"
    ],
    [
     "Bin",
     "Ma"
    ]
   ],
   "title": "Ensemble classifiers using unsupervised data selection for speaker recognition",
   "original": "i12_2666",
   "page_count": 4,
   "order": 351,
   "p1": "2666",
   "pn": "2669",
   "abstract": [
    "This paper presents an approach with ensemble classifiers using unsupervised data selection for speaker recognition. Ensemble learning is a type of machine learning that applies a combination of several weak learners to achieve accurate and improved performance than a single learner. Based on its acoustic characteristics, the speech utterance is divided into several subsets using unsupervised data selection methods. The ensemble classifiers are then trained with these non-overlapping subsets of speech data to improve the recognition accuracy. Our experiments on the 2008 and 2010 NIST Speaker Recognition Evaluation datasets show that using ensemble classifiers substantially reduces DCF.\n",
    "Index Terms: speaker recognition, ensemble classifier, unsupervised data selection\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-348"
  },
  "hyon12_interspeech": {
   "authors": [
    [
     "Songgun",
     "Hyon"
    ],
    [
     "Hongcui",
     "Wang"
    ],
    [
     "Chen",
     "Zhao"
    ],
    [
     "Jianguo",
     "Wei"
    ],
    [
     "Jianwu",
     "Dang"
    ]
   ],
   "title": "A method of speaker identification based on phoneme mean F-Ratio contribution",
   "original": "i12_2670",
   "page_count": 4,
   "order": 352,
   "p1": "2670",
   "pn": "2673",
   "abstract": [
    "This paper proposes a new method for speaker identification, which based on the non-uniformly distributed speaker information in frequency bands. In order to discard the linguistic information effectively, in this study, we adopt an improved Fisherfs F-ratio called the phoneme mean F-ratio to measure the dependences between frequency components and individual characteristics. Then we adopt an adaptive frequency filter to extract more discriminative feature. The experiment shows that the recognition rate using the proposed feature is increased by 0.62% compared with the F-ratio feature, and increased by 3.46% compared with the MFCC feature. The results confirmed that emphasizing the features from highly individual dependent frequency bands is valid for improving speaker recognition performance.\n",
    "Index Terms: speaker identification, frequency warping, F-ratio\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-349"
  },
  "remus12_interspeech": {
   "authors": [
    [
     "Jeremiah J.",
     "Remus"
    ],
    [
     "Jenniffer M.",
     "Estrada"
    ],
    [
     "Stephanie A. C.",
     "Schuckers"
    ]
   ],
   "title": "Mitigating effects of recording condition mismatch in speaker recognition using partial least squares",
   "original": "i12_2674",
   "page_count": 4,
   "order": 353,
   "p1": "2674",
   "pn": "2677",
   "abstract": [
    "Speaker recognition systems have been shown to work well when recordings are collected in conditions with relatively limited mismatch. Thus, a significant focus of the current research is techniques for robust system performance when greater variability is present. This study considers a diverse data set with recordings collected in multiple different rooms with different types of microphones. A technique recently introduced to the speaker recognition community, called partial least squares (PLS), is considered for decomposing the features and mitigating the degradation in performance due to room and/or microphone mismatch. Results of this study suggest that PLS decomposition can provide substantial improvements in performance in the presence of mismatched recording conditions. The outcomes of this study provide further validation for the partial least squares decomposition and encourage further consideration of PLS for reducing session and environment variability in speaker recognition tasks.\n",
    "Index Terms: speaker recognition, partial least squares, subspace decomposition\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-350"
  },
  "marklund12_interspeech": {
   "authors": [
    [
     "Ellen",
     "Marklund"
    ],
    [
     "Francisco",
     "Lacerda"
    ],
    [
     "Iris-Corinna",
     "Schwarz"
    ],
    [
     "Ulla",
     "Sundberg"
    ]
   ],
   "title": "Similarities in fundamental frequency in infant speech segmentation models",
   "original": "i12_1111",
   "page_count": 4,
   "order": 354,
   "p1": "1111",
   "pn": "1114",
   "abstract": [
    "The present study investigates fundamental frequency as a potential basis for segmentation in models of infant speech segmentation. Pairs of segments that were similar either in terms of fundamental frequency envelop or in terms of transcribed content were found in three different speech styles; speech directed to three-month-olds, speech directed to twelve-month-olds and speech directed to adults. Spectral distance between the segments was calculated for each pair and used as a rough measure of spectral similarity. In both the infant-directed speech style conditions, the spectral distance was smaller when fundamental frequency was used as basis for segmentation, compared to when transcriptions were used. In the adult-directed speech style condition, no difference was found between different bases of segmentation.\n",
    "Index Terms: speech segmentation, fundamental frequency, language acquisition, infant-directed speech\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-351"
  },
  "marklund12b_interspeech": {
   "authors": [
    [
     "Ulrika",
     "Marklund"
    ],
    [
     "Ulla",
     "Sundberg"
    ],
    [
     "Iris-Corinna",
     "Schwarz"
    ],
    [
     "Francisco",
     "Lacerda"
    ]
   ],
   "title": "Phonological complexity and vocabulary size in 30-month-old Swedish children",
   "original": "i12_1115",
   "page_count": 4,
   "order": 355,
   "p1": "1115",
   "pn": "1118",
   "abstract": [
    "This study addresses the relationship between phonological complexity and vocabulary size in fifteen 30-month-old Swedish children, selected to cover a wide range of reported lexical development. It is a follow-up of earlier studies indicating a relationship between phonological and lexical development in children and how it is affected by the ambient linguistic input. Phonological complexity of content words uttered by the children was computed by using a Swedish adaptation of Stoel-Gammons Word Complexity Measure. Speech data from recordings of parent-child conversations were related to parent-reported vocabulary size data collected with the Swedish version of McArthur-Bates CDI. The results indicate that phonological complexity increases with vocabulary size. Implications for intervention to children with language impairment are discussed.\n",
    "Index Terms: phonological development, lexical development, phonological complexity, parental input, parent-child interaction\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-352"
  },
  "kim12e_interspeech": {
   "authors": [
    [
     "Jeesun",
     "Kim"
    ],
    [
     "Chris",
     "Davis"
    ],
    [
     "Christine",
     "Kitamura"
    ]
   ],
   "title": "Auditory-visual speech to infants and adults: signals and correlations",
   "original": "i12_1119",
   "page_count": 4,
   "order": 356,
   "p1": "1119",
   "pn": "1122",
   "abstract": [
    "We investigated how the properties of Infant Directed Speech (IDS) and Adult Directed Speech (ADS) differed in acoustics and in speech-related articulation. Both the degree to which auditory and motion properties changed as a function of speech style (IDS vs. ADS) as well as how the correlation between properties was affected by this change were examined. The acoustic properties of 13 sentences uttered by six mothers either to their infant or to an adult and the corresponding 3D motion of face and head markers were measured. Mean speech duration was longer and mean pitch higher for IDS; the IDS vowel space was also expanded compared to ADS; and all face and head motions were greater in IDS. Moderate correlations were found between speech acoustics and face and head motion. These correlations were consistently larger for ADS compared to IDS.\n",
    "Index Terms: Infant Directed Speech; Auditory-Visual speech\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-353"
  },
  "xu12b_interspeech": {
   "authors": [
    [
     "Dongxin",
     "Xu"
    ],
    [
     "Jill",
     "Gilkerson"
    ],
    [
     "Jeffery A.",
     "Richards"
    ]
   ],
   "title": "Objective child vocal development measurement with naturalistic daylong audio recording",
   "original": "i12_1123",
   "page_count": 4,
   "order": 357,
   "p1": "1123",
   "pn": "1126",
   "abstract": [
    "Child vocal development is a subject that touches many areas. Its measurement is based mainly on subjective approaches. This study demonstrates an objective and unobtrusive measurement and monitoring approach using daylong audio recordings of the natural environment. Our previous study had shown significant result in automatic child vocalization analysis and childhood autism identification. However, there remains the question of why it works. The previous focus was on the overall performance and data-driven modeling without regard to the meaning of underlying features. Even with a good performance, the information about child vocal behavior that contributes to the result was not explored. This study attempts to explore the underlying features and uncover additional information about child vocal development buried within the audio streams. It was found that child vocal behavior can be measured automatically by applying signal processing and pattern recognition technologies to daylong recordings. By combining such features, a correlation of 0.84 between the estimated vocalization age and the chronological age for children of typical development and 94% accuracy for autism identification are achieved. Similar to many emerging non-invasive and telemonitoring technologies in health care, this approach is believed to have great potential in child development research, clinical practice and parenting.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-354"
  },
  "nagao12_interspeech": {
   "authors": [
    [
     "Kyoko",
     "Nagao"
    ],
    [
     "Mark",
     "Paullin"
    ],
    [
     "Vilena",
     "Livinsky"
    ],
    [
     "James B.",
     "Polikoff"
    ],
    [
     "Linda D.",
     "Vallino"
    ],
    [
     "Thierry G.",
     "Morlet"
    ],
    [
     "N. Carolyn",
     "Schanen"
    ],
    [
     "H. Timothy",
     "Bunnell"
    ]
   ],
   "title": "Speech production-perception relationships in children with speech delay",
   "original": "i12_1127",
   "page_count": 4,
   "order": 358,
   "p1": "1127",
   "pn": "1130",
   "abstract": [
    "This study examines the relationship between speech perception and speech production in children with speech delay (SD). Sixty-three children who participated in the Nemours Genetics of Speech Delay Project were categorized as either typically developing (TD) or SD. The children with SD were subgrouped by their articulation errors on the /s/ and /ʃ/ sounds tested in a perception experiment. An identification task was used to assess children's perceptual ability to identify common error sounds using four sets of a 9-step synthetic continuum. The current study did not observe large boundary shifts in children with SD compared with TD children for the /s/-/ʃ/ contrast. However, one group of children who had articulation errors on both contrasting sounds exhibited a non-categorical perception pattern, whereas the other group exhibited functions similar to the control group. Furthermore, boundary shifts were observed in the perception of /k/-/g/ contrast in the children who made articulation errors on /ʃ/. The results suggest that children with delayed speech can be classified into various subgroups based on speech production as well as speech perception measures. A group of children might have poor perceptual abilities to identify phonemic sounds even though they do not exhibit articulation errors.\n",
    "Index Terms: speech sound disorder, speech perception, speech production, categorical perception\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-355"
  },
  "strombergsson12b_interspeech": {
   "authors": [
    [
     "Sofia",
     "Strömbergsson"
    ]
   ],
   "title": "Synthetic correction of deviant speech – children's perception of phonologically modified recordings of their own speech",
   "original": "i12_1131",
   "page_count": 4,
   "order": 359,
   "p1": "1131",
   "pn": "1134",
   "abstract": [
    "This report describes preliminary data from a study of how children with phonological impairment (PI) perceive automatically corrected versions of their own deviant speech. The results from 8 children with PI are compared to results of a group of 20 children with typical speech and language (nPI). The results indicate group differences only in tasks where the children make judgments of their own recorded (original or modified) speech; here, the children in the nPI group perform significantly better than the children with PI. In tasks where the children judge the phonological accuracy of recordings of other children (original or modified), however, the two groups perform equally well. Furthermore, the results indicate that sub-phonemic modifications of recordings are too subtle for the children in both groups to detect. Technical and clinical implications of these findings are discussed.\n",
    "Index Terms: speech perception, children, phonological impairment, unit selection\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-356"
  },
  "wan12_interspeech": {
   "authors": [
    [
     "Vincent",
     "Wan"
    ],
    [
     "Javier",
     "Latorre"
    ],
    [
     "K. K.",
     "Chin"
    ],
    [
     "Langzhou",
     "Chen"
    ],
    [
     "Mark J. F.",
     "Gales"
    ],
    [
     "Heiga",
     "Zen"
    ],
    [
     "Kate",
     "Knill"
    ],
    [
     "Masami",
     "Akamine"
    ]
   ],
   "title": "Combining multiple high quality corpora for improving HMM-TTS",
   "original": "i12_1135",
   "page_count": 4,
   "order": 360,
   "p1": "1135",
   "pn": "1138",
   "abstract": [
    "The most reliable way to build synthetic voices for end-products is to start with high quality recordings from professional voice talents. This paper describes the application of average voice models (AVMs) and a novel application of cluster adaptive training (CAT) to combine a small number of these high quality corpora to make best use of them and improve overall voice quality in hidden Markov model based text-tospeech (HMMTTS) systems. It is shown that integrated training by both CAT and AVM approaches, yields better sounding voices than speaker dependent modelling. It is also shown that CAT has an advantage over AVMs when adapting to a new speaker. Given a limited amount of adaptation data CAT maintains a much higher voice quality even when adapted to tiny amounts of speech.\n",
    "Index Terms: speech synthesis, cluster adaptive training, speaker adaptation, average voice models\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-357"
  },
  "takamichi12_interspeech": {
   "authors": [
    [
     "Shinnosuke",
     "Takamichi"
    ],
    [
     "Tomoki",
     "Toda"
    ],
    [
     "Yoshinori",
     "Shiga"
    ],
    [
     "Hisashi",
     "Kawai"
    ],
    [
     "Sakriani",
     "Sakti"
    ],
    [
     "Satoshi",
     "Nakamura"
    ]
   ],
   "title": "An evaluation of parameter generation methods with rich context models in HMM-based speech synthesis",
   "original": "i12_1139",
   "page_count": 4,
   "order": 361,
   "p1": "1139",
   "pn": "1142",
   "abstract": [
    "In this paper, we propose parameter generation methods using rich context models in HMM-based speech synthesis to improve quality of synthetic speech while keeping capability of flexibly modeling acoustic features. In traditional HMM-based speech synthesis, generated speech parameters tend to be excessively smoothed and the use of them causes muffled sounds in synthetic speech. To alleviate this problem, some hybrid methods combining HMM-based speech synthesis and unit selection synthesis have been proposed. Rich context modeling is one of the hybrid methods of representing acoustic inventories with probability density functions. To make it as flexible as original HMM-based speech synthesis, a novel parameter generation methods using rich context modeling is proposed. Rich context models are reformed as GMMs and the parameter generation based on the maximum likelihood criterion is performed. We conduct several experimental evaluations of the proposed methods from various perspectives. The experimental results demonstrate that the proposed methods yield significant improvements in quality of synthetic speech.\n",
    "Index Terms: HMM-based speech synthesis, over-smoothing, rich context model, parameter generation\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-358"
  },
  "lu12c_interspeech": {
   "authors": [
    [
     "Heng",
     "Lu"
    ],
    [
     "Simon",
     "King"
    ]
   ],
   "title": "Using Bayesian networks to find relevant context features for HMM-based speech synthesis",
   "original": "i12_1143",
   "page_count": 4,
   "order": 362,
   "p1": "1143",
   "pn": "1146",
   "abstract": [
    "Speech units are highly context-dependent, so taking contextual features into account is essential for speech modelling. Context is employed in HMM-based Text-to-Speech speech synthesis systems via context-dependent phone models. A very wide context is taken into account, represented by a large set of contextual factors. However, most of these factors probably have no significant influence on the speech, most of the time. To discover which combinations of features should be taken into account, decision tree-based context clustering is used. But the space of contextdependent models is vast, and the number of contexts seen in the training data is only a tiny fraction of this space, so the task of the decision tree is very hard: to generalise from observations of a tiny fraction of the space to the rest of the space, whilst ignoring uninformative or redundant context features. The structure of the context feature space has not been systematically studied for speech synthesis. In this paper we discover a dependency structure by learning a Bayesian Network over the joint distribution of the features and the speech. We demonstrate that it is possible to discard the majority of context features with minimal impact on quality, measured by a perceptual test.\n",
    "Index Terms: HMM-based speech synthesis, Bayesian Networks, context information\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-359"
  },
  "yin12b_interspeech": {
   "authors": [
    [
     "Xiang",
     "Yin"
    ],
    [
     "Zhen-Hua",
     "Ling"
    ],
    [
     "Ming",
     "Lei"
    ],
    [
     "Lirong",
     "Dai"
    ]
   ],
   "title": "Considering global variance of the log power spectrum derived from mel-cepstrum in HMM-based parametric speech synthesis",
   "original": "i12_1147",
   "page_count": 4,
   "order": 363,
   "p1": "1147",
   "pn": "1150",
   "abstract": [
    "This paper utilizes global variance (GV) of the log power spectrum (LPS) derived from mel-cepstum to improve hidden Markov model (HMM) based parametric speech synthesis. In order to alleviate the over-smoothing effect on the generated spectral structures, an LPS-GV modeling method using line spectral pairs (LSPs) has been proposed in our previous work, where the estimated distribution of LPS-GV was combined with the trained acoustic models to determine the optimal spectral features at synthesis time. In this paper, we extend this method to the condition where mel-cepstral coefficients are used as spectral features. Further, a method of integrating LPS-GV distortions into the criterion of minimum generation error (MGE) model training is proposed in order to avoid high computational complexity of the parameter generation algorithm considering GV model. Experimental results show that the parameter generation algorithm using LPS-GV model produces more natural acoustic features than the conventional GV modeling method when mel-cepstrum features are used. Besides, integrating LPS-GV distortions into model training criterion achieves similar performance as applying LPS-GV model at synthesis time.\n",
    "Index Terms: Speech synthesis, hidden Markov model, global variance, log power spectrum\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-360"
  },
  "chunwijitra12_interspeech": {
   "authors": [
    [
     "Vataya",
     "Chunwijitra"
    ],
    [
     "Takashi",
     "Nose"
    ],
    [
     "Takao",
     "Kobayashi"
    ]
   ],
   "title": "A speech parameter generation algorithm using local bariance for HMM-based speech synthesis",
   "original": "i12_1151",
   "page_count": 4,
   "order": 364,
   "p1": "1151",
   "pn": "1154",
   "abstract": [
    "This paper proposes a parameter generation algorithm using local variance (LV) constraint of spectral parameter trajectory for HMM-based speech synthesis. In the parameter generation process, we take account of both the HMM likelihood of speech feature vectors and a likelihood for LVs. To model LV precisely, we use dynamic features of LV with context-dependent HMMs. The objective experimental results show that the proposed technique can generate a better spectral trajectory in terms of the spectral and LV distortions than a conventional technique with global variance (GV) constraint. The subjective experimental results also show that the proposed technique significantly improve the reproducibility of the synthetic speech than the conventional one.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-361"
  },
  "ohtani12_interspeech": {
   "authors": [
    [
     "Yamato",
     "Ohtani"
    ],
    [
     "Masatsune",
     "Tamura"
    ],
    [
     "Masahiro",
     "Morita"
    ],
    [
     "Takehiko",
     "Kagoshima"
    ],
    [
     "Masami",
     "Akamine"
    ]
   ],
   "title": "Histogram-based spectral equalization for HMM-based speech synthesis using mel-LSP",
   "original": "i12_1155",
   "page_count": 4,
   "order": 365,
   "p1": "1155",
   "pn": "1158",
   "abstract": [
    "We propose a statistical spectral parameter emphasis technique for HMM-based speech synthesis. In the proposed method, the cumulative distribution function (CDF) is calculated from the histogram of spectral parameters which are extracted from training speech data. In the same manner, CDF of spectral parameters which are generated from HMMs is constructed. Then the emphasis rule is trained by relating CDF of training data to that of generated parameters. After generating an arbitrary spectral parameter sequence from HMMs,it is emphasized by a conversion to bring the histogram of the generated spectral parameters closer to that of the spectral parameters included in the training data. The subjective experimental result demonstrates that our proposed method can improve speech quality.\n",
    "Index Terms: speech synthesis, hidden Markov model, parameter emphasis, mel-LSP, histogram equalization\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-362"
  },
  "raitio12_interspeech": {
   "authors": [
    [
     "Tuomo",
     "Raitio"
    ],
    [
     "Antti",
     "Suni"
    ],
    [
     "Martti",
     "Vainio"
    ],
    [
     "Paavo",
     "Alku"
    ]
   ],
   "title": "Wideband parametric speech synthesis using warped linear prediction",
   "original": "i12_1420",
   "page_count": 4,
   "order": 366,
   "p1": "1420",
   "pn": "1423",
   "abstract": [
    "This paper studies the use of warped linear prediction (WLP) for wideband parametric speech synthesis. As the sampling frequency is increased from the usual 16 kHz, linear frequency resolution of conventional linear prediction (LP) cannot efficiently model the speech spectrum. By using frequency warping that weights perceptually the most important formant information, spectral models with better accuracy and lower model orders can be utilized. In this work, WLP is embedded in a parametric speech synthesizer to efficiently create wideband synthetic speech. Experiments show that WLP-based wideband synthetic speech is rated better compared to narrowband speech and wideband LP-based speech.\n",
    "Index Terms: statistical parametric speech synthesis, wideband, warped linear prediction, WLP\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-363"
  },
  "drugman12b_interspeech": {
   "authors": [
    [
     "Thomas",
     "Drugman"
    ],
    [
     "John",
     "Kane"
    ],
    [
     "Christer",
     "Gobl"
    ]
   ],
   "title": "Modeling the creaky excitation for parametric speech synthesis",
   "original": "i12_1424",
   "page_count": 4,
   "order": 367,
   "p1": "1424",
   "pn": "1427",
   "abstract": [
    "In order to produce natural sounding output, speech synthesis systems need to be able to properly model the acoustic variability in the corpus. Creaky voice is a voice quality frequently produced in many languages, in both read and conversational speech settings. However, the creaky excitation displays different acoustic characteristics than modal excitations and is, hence, not suitably modelled by standard vocoders. This study presents an analysis of the creaky excitation which is used to derive an extension of the Deterministic plus Stochastic Model of the residual signal. This proposed model is designed for an appropriate modeling of creaky voice and is integrated into a vocoder for parametric speech synthesis. Analysis-synthesis versions of short speech segments containing creaky voice were used in a subjective listening test which revealed clearly better rendering of the voice quality than a standard vocoder.\n",
    "Index Terms: Voice quality, speech synthesis, creak, vocal fry\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-364"
  },
  "wen12b_interspeech": {
   "authors": [
    [
     "Zhengqi",
     "Wen"
    ],
    [
     "Jianhua",
     "Tao"
    ]
   ],
   "title": "Amplitude spectrum based excitation model for HMM-based speech synthesis",
   "original": "i12_1428",
   "page_count": 4,
   "order": 368,
   "p1": "1428",
   "pn": "1431",
   "abstract": [
    "This paper describes an excitation model based on amplitude spectrum for HMM-based speech synthesis system (HTS). Residual signal obtained from inverse filtering is decomposed into periodic and aperiodic spectrums in frequency domain. Amplitude spectrum of half pitch period length is reserved as periodic component in synthesis stage and zero-phase criterion and pitch synchronous overlap add method (PSOLA) are adopted to reconstruct the residual signal. Before integrating this excitation model into HTS, these periodic spectrums are normalized and Linde-Buzo-Gray (LBG) algorithm is adopted to construct codebooks for every Mandarin final . Then index parameters from these codebooks which are indicated as excitation information are taken into HTS training together with spectral, F0 and aperiodic parameters. Listening test showed that for female voice the analysis-synthesis result of the vocoder based on proposed excitation model is comparable with that of STRAIGHT and when integrating into HTS, the quality of generated speech is also improved.\n",
    "Index Terms: speech synthesis, HMM-based speech synthesis, excitation model, amplitude spectrum\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-365"
  },
  "nishizawa12_interspeech": {
   "authors": [
    [
     "Nobuyuki",
     "Nishizawa"
    ],
    [
     "Tsuneo",
     "Kato"
    ]
   ],
   "title": "Speech synthesis using a non-maximally decimated filter bank for embedded systems",
   "original": "i12_1432",
   "page_count": 4,
   "order": 369,
   "p1": "1432",
   "pn": "1435",
   "abstract": [
    "A novel speech waveform generation method using a non-maximally decimated filter bank is proposed, where spectral features of synthetic sounds are created by amplitude modification of subband samples that are pre-decomposed from impulse or noise waveforms. The proposed method uses two synthesis banks of the maximally decimated pseudo quadrature mirror filter (QMF) bank structure which is similar to that in the MPEG audio decoder. Consequently, the computational complexity of the proposed method is O(log N) per sample, while that of the conventional method based on the source-filter model with an auto-regressive (AR) filter or mel log spectrum approximation (MLSA) filter is O(N) per sample. A MOS test for resynthesized speech sounds from the results of analyzing natural speech sounds showed the proposed method achieved scores similar to those of the conventional method using the MLSA filter for a female narrator.\n",
    "Index Terms: HMM-based speech synthesis, pseudo quadrature mirror filter bank, non-maximal decimation, embedded systems\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-366"
  },
  "silen12_interspeech": {
   "authors": [
    [
     "Hanna",
     "Silén"
    ],
    [
     "Elina",
     "Helander"
    ],
    [
     "Jani",
     "Nurminen"
    ],
    [
     "Moncef",
     "Gabbouj"
    ]
   ],
   "title": "Ways to implement global variance in statistical speech synthesis",
   "original": "i12_1436",
   "page_count": 4,
   "order": 370,
   "p1": "1436",
   "pn": "1439",
   "abstract": [
    "Hidden Markov model-based speech synthesis is prone to over-smoothing of spectral parameter trajectories. The maximum-likelihood parameter generation favors smooth tracks and the utterance-level variance of each parameter trajectory is significantly reduced compared to the original recordings. This results in muffled speech. To retain the natural variance, statistical global variance modeling has been used in parameter generation. The modeling increases the utterance-level variance in synthesis, but it is computationally demanding: there is no closed-form solution and an iterative approach is used. In this paper, we analyze the performance of two simple alternative approaches for retaining the natural variance of spectral parameters in synthesis, namely variance scaling and histogram equalization. Both methods apply analytically solvable parameter generation and impose the natural variance afterwards as an efficient post-processing step. Subjective evaluations carried out on English data confirm that the achieved synthesis quality is higher compared to simple post-filtering and similar to the standard global variance modeling.\n",
    "Index Terms: statistical speech synthesis, global variance, variance scaling, histogram equalization\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-367"
  },
  "ohtani12b_interspeech": {
   "authors": [
    [
     "Yamato",
     "Ohtani"
    ],
    [
     "Masatsune",
     "Tamura"
    ],
    [
     "Masahiro",
     "Morita"
    ],
    [
     "Takehiko",
     "Kagoshima"
    ],
    [
     "Masami",
     "Akamine"
    ]
   ],
   "title": "HMM-based speech synthesis using sub-band basis spectrum model",
   "original": "i12_1440",
   "page_count": 4,
   "order": 371,
   "p1": "1440",
   "pn": "1443",
   "abstract": [
    "In this paper, we propose a HMM-based text-to-speech (TTS) using sub-band basis spectrum model (SBM). SBM can represent vocal tract spectra and phase characteristics by liner combination of sub-band basis vectors. Some reports suggest that analysis-synthesized speech based on SBM is close to the natural speech and SBM can perform effectively in the text-to-speech. Therefore, SBM framework is expected to improve speech quality to have good effects on the HMM-based TTS. Subjective experimental results show that the proposed method improves speech quality in some conditions.\n",
    "Index Terms: speech synthesis, hidden Markov model, sub-band basis spectrum model, phase feature\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-368"
  },
  "imseng12_interspeech": {
   "authors": [
    [
     "David",
     "Imseng"
    ],
    [
     "John",
     "Dines"
    ],
    [
     "Petr",
     "Motlicek"
    ],
    [
     "Philip N.",
     "Garner"
    ],
    [
     "Hervé",
     "Bourlard"
    ]
   ],
   "title": "Comparing different acoustic modeling techniques for multilingual boosting",
   "original": "i12_1191",
   "page_count": 4,
   "order": 372,
   "p1": "1191",
   "pn": "1194",
   "abstract": [
    "In this paper, we explore how different acoustic modeling techniques can benefit from data in languages other than the target language. We propose an algorithm to perform decision tree state clustering for the recently proposed Kullback-Leibler divergence based hidden Markov models (KL-HMM) and compare it to subspace Gaussian mixture modeling (SGMM). KL-HMM can exploit multilingual information in the form of universal phoneme posterior features and SGMM benefits from a universal background model that can be trained on multilingual data. Taking the Greek SpeechDat(II) data as an example, we show that KL-HMM performs best for small amounts of target language data.\n",
    "Index Terms: Speech recognition, multilingual acoustic modeling, under-resourced languages\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-369"
  },
  "wang12g_interspeech": {
   "authors": [
    [
     "Yongqiang",
     "Wang"
    ],
    [
     "Mark J. F.",
     "Gales"
    ]
   ],
   "title": "Model-based approaches to adaptive training in reverberant environments",
   "original": "i12_1195",
   "page_count": 4,
   "order": 373,
   "p1": "1195",
   "pn": "1198",
   "abstract": [
    "Adaptive training is a powerful approach for building speech recognition systems using non-homogeneous data. This work presents an extension of model-based adaptive training to handle reverberant environments. The recently proposed Reverberant VTS-Joint (RVTSJ) adaptation is used to factor out unwanted additive and reverberant noise variations in multiconditional training data, yielding a canonical model neutral to noise conditions. An maximum likelihood estimation of the canonical model parameters is described. An initialisation scheme that uses the VTS-based adaptive training to initialise the model parameters is also presented. Experiments are conducted on a reverberant simulated AURORA4 task.\n",
    "Index Terms: reverberant noise robustness, vector Taylor series, adaptive training\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-370"
  },
  "gales12_interspeech": {
   "authors": [
    [
     "Mark J. F.",
     "Gales"
    ],
    [
     "Federico",
     "Flego"
    ]
   ],
   "title": "Model-based approaches for degraded channel modelling in robust ASR",
   "original": "i12_1199",
   "page_count": 4,
   "order": 374,
   "p1": "1199",
   "pn": "1202",
   "abstract": [
    "Speech is usually observed after passing through some form of \"channel\" that results in distortions. For some scenarios it is possible to build explicit models of this channel distortion and hence compensate the acoustic models. However the accuracy of the distortion model is sometimes poor and more general adaptation approaches are required. This paper investigates these model-based approaches for communication channel, link, modelling. In particular the paper examines the interaction of link models with speaker adaptation and adaptive training. CMLLR link models with multiple transforms can yield multiple inconsistent feature-spaces When combined with speaker adaptation with very few transforms this inconsistency can limit adaptation performance gains. In contrast using a front-end CMLLR (FE-CMLLR) transform yields a consistent space for speaker adaptation. These schemes are compared on communication channel distorted dialect Arabic conversational speech. Preliminary results on this task indicate the benefits of performing adaptation in a consistent feature-space.\n",
    "Index Terms: acoustic model adaptation, adaptive training\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-371"
  },
  "hartmann12_interspeech": {
   "authors": [
    [
     "William",
     "Hartmann"
    ],
    [
     "Eric",
     "Fosler-Lussier"
    ]
   ],
   "title": "Improved model selection for the ASR-driven binary mask",
   "original": "i12_1203",
   "page_count": 4,
   "order": 375,
   "p1": "1203",
   "pn": "1206",
   "abstract": [
    "In a previous study, we proposed an alternative masking criterion for binary mask estimation based on the underlying linguistic information. We estimated this mask by selecting from a set of candidate masks at each frame based on the hypotheses from an ASR system. Our previous system provided an 8% reduction in WER. In this work, we present an improved method for selecting the correct candidate mask at each frame, increasing the reduction in WER to 14%. Our new method uses a discriminative sequence model and provides a framework that can incorporate other mask estimations as features.\n",
    "Index Terms: speech recognition, binary mask estimation\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-372"
  },
  "wiesler12_interspeech": {
   "authors": [
    [
     "Simon",
     "Wiesler"
    ],
    [
     "Ralf",
     "Schlüter"
    ],
    [
     "Hermann",
     "Ney"
    ]
   ],
   "title": "Accelerated batch learning of convex log-linear models for LVCSR",
   "original": "i12_1207",
   "page_count": 4,
   "order": 376,
   "p1": "1207",
   "pn": "1210",
   "abstract": [
    "This paper describes a log-linear modeling framework suitable for large-scale speech recognition tasks. We introduce modifications to our training procedure that are required for extending our previous work on log-linear models to larger tasks. We give a detailed description of the training procedure with a focus on aspects that impact computational efficiency. The performance of our approach is evaluated on the English Quaero corpus, a challenging broadcast conversations task. The log-linear model consistenly outperforms the maximum likelihood baseline system. Comparable performance to a system with minimum-phone-error training is achieved.\n",
    "Index Terms: acoustic modeling, discriminative models\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-373"
  },
  "pylkkonen12_interspeech": {
   "authors": [
    [
     "Janne",
     "Pylkkönen"
    ],
    [
     "Mikko",
     "Kurimo"
    ]
   ],
   "title": "Improving discriminative training for robust acoustic models in large vocabulary continuous speech recognition",
   "original": "i12_1211",
   "page_count": 4,
   "order": 377,
   "p1": "1211",
   "pn": "1214",
   "abstract": [
    "This paper studies the robustness of discriminatively trained acoustic models for large vocabulary continuous speech recognition. Popular discriminative criteria maximum mutual information (MMI), minimum phone error (MPE), and minimum phone frame error (MPFE), are used in the experiments, which include realistic mismatched conditions from Finnish Speecon corpus and English Wall Street Journal corpus. A simple regularization method for discriminative training is proposed and it is shown to improve the robustness of acoustic models gaining consistent improvements in noisy conditions.\n",
    "Index Terms: speech recognition, discriminative training, robustness\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-374"
  },
  "novotney12_interspeech": {
   "authors": [
    [
     "Scott",
     "Novotney"
    ],
    [
     "Ivan",
     "Bulyko"
    ],
    [
     "Richard",
     "Schwartz"
    ],
    [
     "Sanjeev",
     "Khudanpur"
    ],
    [
     "Owen",
     "Kimball"
    ]
   ],
   "title": "Semi-supervised methods for improving keyword search of unseen terms",
   "original": "i12_1215",
   "page_count": 4,
   "order": 378,
   "p1": "1215",
   "pn": "1218",
   "abstract": [
    "We present a semi-supervised language modeling technique to improve search performance on terms without training data. Probabilities estimated from automatic transcripts of a large corpus of in-domain audio are added to an existing LM. Requiring no development data or external resources, our method achieves 70% of the possible gain for manual transcription of the same audio. This is in sharp contrast to the modest gains of previous semisupervised LM experiments. We compare the value of additional resources (labor or data) to semi-supervised learning. If human effort is available, we describe a transcription regime to efficiently close the remaining performance gap.\n",
    "Index Terms: KWS, language modeling, CTS\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-375"
  },
  "li12d_interspeech": {
   "authors": [
    [
     "Xiangang",
     "Li"
    ],
    [
     "Dan",
     "Su"
    ],
    [
     "Zaihu",
     "Pang"
    ],
    [
     "Xihong",
     "Wu"
    ]
   ],
   "title": "Probabilistic speaker-class based acoustic modeling for large vocabulary continuous speech recognition",
   "original": "i12_1219",
   "page_count": 4,
   "order": 379,
   "p1": "1219",
   "pn": "1222",
   "abstract": [
    "In this paper, a probabilistic speaker-class (PSC) based acoustic modeling method is proposed for taking into account speaker variability influence in HMM-based speech recognition systems. Firstly, within the context of speaker-class based speech recognition, an experiment is conducted to investigate the performance of speaker-class recognition based on hard-cut speaker clustering. Then, in the proposed method, through introducing the probabilistic latent speaker analysis, the speaker-class dependent acoustic models are trained based on a softdecision speaker clustering method, and combined by the distribution of speaker-class in the decoding phase. The experiments were conducted on a 600-hour speech corpus, and showed improvement in a large vocabulary continuous speech recognition task.\n",
    "Index Terms: speech recognition, probabilistic latent speaker analysis, speaker clustering, speaker-class\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-376"
  },
  "yao12_interspeech": {
   "authors": [
    [
     "Xiao",
     "Yao"
    ],
    [
     "Takatoshi",
     "Jitsuhiro"
    ],
    [
     "Chiyomi",
     "Miyajima"
    ],
    [
     "Norihide",
     "Kitaoka"
    ],
    [
     "Kazuya",
     "Takeda"
    ]
   ],
   "title": "Classification of stressed speech using physical parameters derived from two-mass model",
   "original": "i12_1223",
   "page_count": 4,
   "order": 380,
   "p1": "1223",
   "pn": "1226",
   "abstract": [
    "In this study, we investigate physical parameters which can be used to classify speech as either stressed or neutral based on a two-mass vocal fold model. The model attempts to characterize the behavior of the vocal folds and fluid airflow properties when stress is present. The two-mass model is fitted to real speech to estimate the values of physical parameters that represent the stiffness of vocal folds, vocal fold viscosity loss, and subglottal pressure coming from the lungs. The estimated parameters can be used to distinguish stressed speech from neutral speech because these parameters can represent the mechanisms of vocal folds under stress. We propose combinations of physical parameters as features for classification. Experimental results show that our proposed features achieved better classification performance than features derived from traditional methods.\n",
    "Index Terms: physical parameters, two-mass model, speech under stress, stress classification\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-377"
  },
  "du12_interspeech": {
   "authors": [
    [
     "Jun",
     "Du"
    ],
    [
     "Qiang",
     "Huo"
    ]
   ],
   "title": "IVN-based joint training of GMM and HMMs using an improved VTS-based feature compensation for noisy speech recognition",
   "original": "i12_1227",
   "page_count": 4,
   "order": 381,
   "p1": "1227",
   "pn": "1230",
   "abstract": [
    "In our previous work, we proposed a feature compensation approach using high-order vector Taylor series approximation for noisy speech recognition. In this paper, first we improve the feature compensation in both efficiency and accuracy by boosted mixture learning of GMM, applying higher order information of VTS approximation only to the noisy speech mean parameters, acoustic context expansion, and modeling the convolutional distortion as a single Gaussian. Then we design a procedure to perform irrelevant variability normalization based joint training of GMM and HMM using VTS-based feature compensation. The effectiveness of our proposed approach is confirmed by experiments on Aurora3 database.\n",
    "Index Terms: irrelevant variability normalization, feature compensation, vector Taylor series.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-378"
  },
  "moritz12_interspeech": {
   "authors": [
    [
     "Niko",
     "Moritz"
    ],
    [
     "Jörn",
     "Anemüller"
    ],
    [
     "Birger",
     "Kollmeier"
    ]
   ],
   "title": "Amplitude modulation filters as feature sets for robust ASR: constant absolute or relative bandwidth?",
   "original": "i12_1231",
   "page_count": 4,
   "order": 382,
   "p1": "1231",
   "pn": "1234",
   "abstract": [
    "Many research efforts in the field of feature extraction for automatic speech recognition are focused on analyzing slow amplitude fluctuations of speech. In this study the importance of spectral and temporal resolution for the amplitude modulation frequency analysis are investigated in order to provide guidance for the appropriate filter design. Therefore, different wavelet and Fourier transform like filter time scales are examined, i.e. the importance of time and frequency separation is compared. The results demonstrate that analyzing three separate amplitude modulation frequency bands of constant bandwidth that cover the range from about 2 to 16 Hz are sufficient for automatic speech recognition.\n",
    "Index Terms: amplitude modulation, speech recognition, wavelet transform, feature extraction\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-379"
  },
  "demir12_interspeech": {
   "authors": [
    [
     "Cemil",
     "Demir"
    ],
    [
     "A. Taylan",
     "Cemgil"
    ],
    [
     "Murat",
     "Saraçlar"
    ]
   ],
   "title": "Effect of speech priors in single-channel speech-music separation for ASR",
   "original": "i12_1235",
   "page_count": 4,
   "order": 383,
   "p1": "1235",
   "pn": "1238",
   "abstract": [
    "In this study, we extend the catalog-based single-channel speech-music separation method such that it incorporate prior speech information to enhance the separation performance of the method. We developed the inference method that enable us to use the speech prior model which is obtained using pre-obtained speech signals. Complex Gaussian observation model which uses the Inverse-Gamma distribution as a prior model are used to develop the inference method. We compare the separation performance of the catalog-based method with and without prior speech model in both complex Gaussian and Poisson observation models. It is shown that for both observation models incorporating prior speech information improves the separation performance of the catalog-based method.\n",
    "Index Terms: speech-music separation, prior speech model, speech recognition\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-380"
  },
  "narayanan12_interspeech": {
   "authors": [
    [
     "Arun",
     "Narayanan"
    ],
    [
     "DeLiang",
     "Wang"
    ]
   ],
   "title": "On the role of binary mask pattern in automatic speech recognition",
   "original": "i12_1239",
   "page_count": 4,
   "order": 384,
   "p1": "1239",
   "pn": "1242",
   "abstract": [
    "Processing noisy signals using the ideal binary mask has been shown to improve automatic speech recognition (ASR) performance. In this paper, we present the first study that investigates the role of mask patterns in ASR under varying signal-to-noise ratios (SNR), noise conditions and mask definitions. Binary masks are typically computed either by comparing the local SNR within a time-frequency unit of a mixture signal with a threshold termed the local criterion (LC), or by comparing the local target energy with the long-term average energy of speech. Results show that: (i) Akin to human speech recognition, binary masking can significantly improve ASR even when the mixture SNR is as low as -60 dB. (ii) The difference between the LC and the mixture SNR is more correlated to the recognition accuracy than LC. (iii) The performance profiles in ASR are qualitatively similar to those obtained for human speech recognition. (iv) The LC at which the peak performance is obtained is lower than 0 dB, which is the optimal threshold as far as the SNR gain of processed signals is concerned. This indicates that maximizing SNR gain may not be the optimal criterion to improve either human or machine recognition of noisy speech.\n",
    "Index Terms: computational auditory scene analysis, ideal binary mask, automatic speech recognition, mask pattern\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-381"
  },
  "gomez12_interspeech": {
   "authors": [
    [
     "Randy",
     "Gomez"
    ],
    [
     "Tatsuya",
     "Kawahara"
    ]
   ],
   "title": "Dereverberation based on wavelet packet filtering for robust automatic speech recognition",
   "original": "i12_1243",
   "page_count": 4,
   "order": 385,
   "p1": "1243",
   "pn": "1246",
   "abstract": [
    "This paper describes a multiple-resolution signal analysis to suppress late reflection of reverberation for robust automatic speech recognition (ASR). Wavelet packet tree (WPT) decomposition offers a finer resolution to discriminate the late reflection subspace from the speech subspace. By selecting appropriate wavelet basis in the WPT for speech and late reflection, we can effectively estimate the Wiener gain directly from the observed reverberant data. Moreover, the selection procedure is performed in accordance with the acoustic model likelihood used by the speech recognizer for improved ASR performance. Dereverberation is realized by filtering the wavelet packet coefficients with the Wiener gain to suppress the effects of the late reflection. Experimental evaluations with large vocabulary continuous speech recognition (LVCSR) in real reverberant conditions show that the proposed method outperforms conventional wavelet-based methods and other dereverberation techniques.\n",
    "Index Terms: Speech recognition, Robustness, Dereverberation, Wavelet Packets\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-382"
  },
  "kristjansson12_interspeech": {
   "authors": [
    [
     "Trausti",
     "Kristjansson"
    ],
    [
     "Thad",
     "Hughes"
    ]
   ],
   "title": "Spectral intersections for non-stationary signal separation",
   "original": "i12_1247",
   "page_count": 4,
   "order": 386,
   "p1": "1247",
   "pn": "1250",
   "abstract": [
    "We describe a new method for non-stationary noise suppression that is simple to implement yet has performance rivaling far more complex algorithms.   Spectral Intersections is a model based MMSE signal separation method that uses a new simple approximation to the observation likelihood. Furthermore, Spectral Intersections uses an efficient approximation to the expectation integral of the MMSE estimate that could be described as unscented importance sampling.   We apply the new method to the task of separating speech mixed with music. We report results on the Google Voice Search task where the new method provides a 7% relative reduction in WER at 10dB SNR. Interestingly, the new method provides considerably greater reduction in average WER than the MAX method and approaches the performance of the more complex Algonquin algorithm.\n",
    "Index Terms: Speech Recognition, Noise Robustness, Noise Suppression, Spectral Subtraction, Algonquin\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-383"
  },
  "odani12_interspeech": {
   "authors": [
    [
     "Kyohei",
     "Odani"
    ],
    [
     "Longbiao",
     "Wang"
    ],
    [
     "Atsuhiko",
     "Kai"
    ]
   ],
   "title": "Speech recognition by denoising and dereverberation based on spectral subtraction in a real noisy reverberant environment",
   "original": "i12_1251",
   "page_count": 4,
   "order": 387,
   "p1": "1251",
   "pn": "1254",
   "abstract": [
    "A blind dereverberation method based on spectral subtraction (SS) using a multi-channel least mean squares (MCLMS) algorithm was previously proposed. The results of a large vocabulary continuous speech recognition (LVCSR) task showed that this method achieved significant improvements over the conventional method based on cepstral mean normalization (CMN) and beamforming in a simulated reverberant environment without additive noise. In this paper, we evaluate the blind dereverberation method in a real noisy reverberant environment. We present a denoising and dereverberation method based on power SS or generalized SS (GSS), and evaluate our proposed method using speech in a real environment.The GSS-based method achieves an average relative word error reduction rate of 39.1% and 11.5% compared to the conventional CMN and power-SS based methods, respectively.\n",
    "Index Terms: hands-free speech recognition, blind dere- verberation, noise reduction, spectral subtraction, real en- vironment\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-384"
  },
  "pardede12_interspeech": {
   "authors": [
    [
     "Hilman F.",
     "Pardede"
    ],
    [
     "Koichi",
     "Shinoda"
    ],
    [
     "Koji",
     "Iwano"
    ]
   ],
   "title": "Q-Gaussian based spectral subtraction for robust speech recognition",
   "original": "i12_1255",
   "page_count": 4,
   "order": 388,
   "p1": "1255",
   "pn": "1258",
   "abstract": [
    "Spectral subtraction (SS) is derived using maximum likelihood estimation assuming both noise and speech follow Gaussian distributions and are independent from each other. Under this assumption, noisy speech, speech contaminated by noise, also follows a Gaussian distribution. However, it is well known that noisy speech observed in real situations often follows a heavy-tailed distribution, not a Gaussian distribution. In this paper, we introduce a q-Gaussian distribution in non-extensive statistics to represent the distribution of noisy speech and derive a new spectral subtraction method based on it. In our analysis, the q-Gaussian distribution fits the noisy speech distribution better than the Gaussian distribution does. Our speech recognition experiments showed that the proposed method, q-spectral subtraction (q-SS), outperformed the conventional SS method using the Aurora-2 database.\n",
    "Index Terms: robust speech recognition, spectral subtraction, Gaussian distribution, q-Gaussian, maximum likelihood\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-385"
  },
  "meyer12c_interspeech": {
   "authors": [
    [
     "Bernd T.",
     "Meyer"
    ],
    [
     "Constantin",
     "Spille"
    ],
    [
     "Birger",
     "Kollmeier"
    ],
    [
     "Nelson",
     "Morgan"
    ]
   ],
   "title": "Hooking up spectro-temporal filters with auditory-inspired representations for robust automatic speech recognition",
   "original": "i12_1259",
   "page_count": 4,
   "order": 389,
   "p1": "1259",
   "pn": "1262",
   "abstract": [
    "Spectro-temporal filtering has been shown to result in features that can help to increase the robustness of automatic speech recognition (ASR) in the past. We replace the spectro-temporal representation used in previous work with spectrograms that incorporate knowledge about the signal processing of the human auditory system and which are derived from Power-Normalized Cepstral Coefficients (PNCCs). 2D-Gabor filters are applied to these spectrograms to extract features evaluated on a noisy digit recognition task. The filter bank is adapted to the new representation by optimizing the spectral modulation frequencies associated with each Gabor function. A comparison of optimized parameters and the spectral modulation of vowels shows a good match between optimized and expected range of frequencies. When processed with a non-linear neural net and combined with PNCCs, Gabor features decrease the error rate compared to the baseline and PNCCs by at least 19%.\n",
    "Index Terms: automatic speech recognition, spectrotemporal features, power-normalized features\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-386"
  },
  "li12e_interspeech": {
   "authors": [
    [
     "Qi Peter",
     "Li"
    ],
    [
     "Xie",
     "Sun"
    ]
   ],
   "title": "Feature extraction based on hearing system signal processing for robust large vocabulary speech recognition",
   "original": "i12_1263",
   "page_count": 4,
   "order": 390,
   "p1": "1263",
   "pn": "1266",
   "abstract": [
    "A new auditory-based feature extraction algorithm for robust speech recognition is developed from modeling the signal processing functions in the hearing system. Usually, the performance of acoustic models trained in clean speech drops significantly when tested on noisy speech; thus recognition systems cannot work robustly in the field even when they have good performance in labs. To address the problem, we have developed features based on a set of modules to simulate the signal processing functions in the cochlea, such as auditory transform, hair cells, and equal-loudness functions. The features are then applied to the Wall Street Journal task. To simulate the performance in the field, the training data is near clean speech while the testing data are with added white and babble noise. As shown in our experiments, without added noise, the proposed features have a similar performance as MFCC, RASTA-PLP, and PLP features. When we added noise and tested at different SNR levels, the performance of the proposed auditory features is significantly better than others. For example, at 10 dB SNR level which is often encountered in real applications, the performance of the proposed auditory features is 65.53% while the best from others is 36.33% from the RASTA-PLP. The proposed features provide an absolute gain on recognition accuracy of 29.20%. Overall, our experiments show that the proposed auditory features have strong robustness in the mismatched and noisy situations in speech recognition.\n",
    "Index Terms: Speech feature extraction, auditory-based feature, robust speech recognition, cochlea, auditory transform\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-387"
  },
  "arsikere12_interspeech": {
   "authors": [
    [
     "Harish",
     "Arsikere"
    ],
    [
     "Gary K. F.",
     "Leung"
    ],
    [
     "Steven M.",
     "Lulich"
    ],
    [
     "Abeer",
     "Alwan"
    ]
   ],
   "title": "Automatic estimation of the first two subglottal resonances in children's speech with application to speaker normalization in limited-data conditions",
   "original": "i12_1267",
   "page_count": 4,
   "order": 391,
   "p1": "1267",
   "pn": "1270",
   "abstract": [
    "This paper proposes an automatic algorithm for estimating the first two subglottal resonances (SGRs) - Sg1 and Sg2 - from continuous speech of children, and applies it to automatic speaker normalization in mismatched, limited-data conditions. The proposed algorithm is based on the observation that Sg1 and Sg2 form phonological vowel feature boundaries, and is motivated by our recent SGR estimation algorithm for adults. The algorithm is trained and evaluated, respectively, on 25 and 9 children, aged between 7 and 18 years. The average RMS errors incurred in estimating Sg1 and Sg2 are 55 and 144 Hz, respectively. By applying the proposed algorithm to a connected digits speech recognition task, it is shown that: 1) a linear frequency warping using Sg1 or Sg2 is comparable to or better than maximum likelihood-based vocal tract length normalization (ML-VTLN), 2) the performance of SGR-based frequency warping is less content dependent than that of ML-VTLN, and 3) SGRbased frequency warping can be integrated into ML-VTLN to yield a statistically-significant improvement in performance.\n",
    "Index Terms: subglottal resonances, children's speech, automatic estimation, limited data, speaker normalization\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-388"
  },
  "carlin12b_interspeech": {
   "authors": [
    [
     "Michael A.",
     "Carlin"
    ],
    [
     "Kailash",
     "Patil"
    ],
    [
     "Sridhar Krishna",
     "Nemala"
    ],
    [
     "Mounya",
     "Elhilali"
    ]
   ],
   "title": "Robust phoneme recognition based on biomimetic speech contours",
   "original": "i12_1348",
   "page_count": 4,
   "order": 392,
   "p1": "1348",
   "pn": "1351",
   "abstract": [
    "It has been previously suggested that ensembles of central auditory neurons optimize a sustained firing criterion as part of the underlying code for representing sound. Moreover, computational studies have shown that optimizing such a criterion yields ensembles of spectro-temporal receptive fields akin to those observed in physiological studies. In this study we show that these emergent receptive fields contour the high energy modulations in speech. A simple 2D filter thus derived is shown to improve upon the performance of state-of-the-art phoneme recognition systems under both additive noise conditions and reverberation by 6.2% absolute on average.\n",
    "Index Terms: robust feature extraction, bio-inspired features, sustained neural firings\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-389"
  },
  "yao12b_interspeech": {
   "authors": [
    [
     "Kaisheng",
     "Yao"
    ],
    [
     "Yifan",
     "Gong"
    ],
    [
     "Chaojun",
     "Liu"
    ]
   ],
   "title": "A feature space transformation method for personalization using generalized i-vector clustering",
   "original": "i12_1352",
   "page_count": 4,
   "order": 393,
   "p1": "1352",
   "pn": "1355",
   "abstract": [
    "We present a feature space transformation method for personalization. This method includes a generalization of i-vector based clustering that allows parameter tying of sub-loading matrices. This method trains i-vector parameters from utterances of a device, uncovering a low dimension space for clustering variability within a device. We show through empirical results impacts of parameters of the generalized i-vector method. We conducted recognition experiments on an internal large vocabulary voice search system for gaming. The method achieved significant reductions of word error rates by 28%, compared to a per utterance adaptation system.\n",
    "Index Terms: speech recognition, personalization, adaptation, i-vector\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-390"
  },
  "tsai12_interspeech": {
   "authors": [
    [
     "T. J.",
     "Tsai"
    ],
    [
     "Nelson",
     "Morgan"
    ]
   ],
   "title": "Longer features: they do a speech detector good",
   "original": "i12_1356",
   "page_count": 4,
   "order": 394,
   "p1": "1356",
   "pn": "1359",
   "abstract": [
    "We have incorporated spectrotemporal features in a speech activity detection (SAD) task for the Speech in Noisy Environments 2 (SPINE2) data set. The features were generated by applying 2D Gabor filters to the mel spectrogram in order to measure the strength of various spectral and temporal modulation frequencies in different patches of the spectrogram. Using several different back-ends, the Gabor features significantly outperformed MFCCs, yielding relative reductions in equal error rate (EER) of between 40 and 50%. Compared to the other backends, Adaboost with tree stumps performed particularly well with Gabor features and particularly poorly with MFCCs. An investigation into the reasons for this disparity suggests that the most useful features for SAD incorporate information over longer time scales.\n",
    "Index Terms: spectrotemporal features, speech activity detection\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-391"
  },
  "alam12_interspeech": {
   "authors": [
    [
     "Md Jahangir",
     "Alam"
    ],
    [
     "Patrick",
     "Kenny"
    ],
    [
     "Douglas",
     "O'Shaughnessy"
    ]
   ],
   "title": "Robust feature extraction for speech recognition by enhancing auditory spectrum",
   "original": "i12_1360",
   "page_count": 4,
   "order": 395,
   "p1": "1360",
   "pn": "1363",
   "abstract": [
    "The goal of this work is to improve the robustness of speech recognition systems in additive noise and real-time reverberant environments. In this paper we present a compressive gammachirp filter-bank-based feature extractor that incorporates a method for the enhancement of auditory spectrum and a short-time feature normalization technique, which, by adjusting the scale and mean of cepstral features, reduces the difference of cepstra between the training and test environments. For performance evaluation, in the context of speech recognition, of the proposed feature extractor we use the standard noisy AURORA-2 corpus and the meeting recorder digits (MRDs) subset of the AURORA-5 corpus, which represent additive noise and reverberant acoustic conditions, respectively. The ETSI advanced front-end (ETSI-AFE), the recently proposed power normalized cepstral coefficients (PNCC) and conventional MFCC features are used for comparison purposes. Experimental speech recognition results depict that the proposed method is robust against both additive and reverberant environments. The proposed method provides comparable results to that of ESTI-AFE and PNCC on the AURORA-2 corpus and provides considerable improvements with respect to the other feature extractors on the AURORA-5 corpus.\n",
    "Index Terms: speech recognition, compressive gammchirp, auditory spectrum enhancement, feature normalization\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-392"
  },
  "muller12_interspeech": {
   "authors": [
    [
     "Florian",
     "Müller"
    ],
    [
     "Alfred",
     "Mertins"
    ]
   ],
   "title": "Enhancing vocal tract length normalization with elastic registration for automatic speech recognition",
   "original": "i12_1364",
   "page_count": 4,
   "order": 396,
   "p1": "1364",
   "pn": "1367",
   "abstract": [
    "Vocal tract length normalization (VTLN) is commonly applied utterance-wise with a warping function which makes the assumption of a linear dependence between the vocal tract length and the location of the formants. In this work we propose a data-driven method for enhancing the performance of systems that already use standard VTLN. The method is based on elastic registration to estimate optimal nonparametric transformations to further reduce inter-speaker variabilities. Results show that the proposed method can increase the performance of monophone systems such that it reaches that of a triphone system.\n",
    "Index Terms: automatic speech recognition, vocal tract length normalization, elastic registration\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-393"
  },
  "pessentheiner12_interspeech": {
   "authors": [
    [
     "Hannes",
     "Pessentheiner"
    ],
    [
     "Stefan",
     "Petrik"
    ],
    [
     "Harald",
     "Romsdorfer"
    ]
   ],
   "title": "Beamforming using uniform circular arrays for distant speech recognition in reverberant environments and double talk scenarios",
   "original": "i12_1368",
   "page_count": 4,
   "order": 397,
   "p1": "1368",
   "pn": "1371",
   "abstract": [
    "Beamforming is crucial for hands-free mobile terminals and voice-enabled automated home environments based on distant-speech interaction to mitigate causes of system degradation, e.g., interfering noise sources or competing speakers. This paper presents an adaptation of the most common state-of-the-art broadband beamformers to uniformcircular arrays, such that competing speakers are attenuated sufficiently for distant speech recognition. As a result, a new beamformer is presented. Finally, the speech quality of the beamformers' enhanced signals is evaluated with different objective speech quality measures and a word recognizer as a measure for the attenuation of competing speakers.\n",
    "Index Terms: array signal processing, beamforming, uniform circular array, reverberant environment, double-talk, distant speech recognition\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-394"
  },
  "prazak12_interspeech": {
   "authors": [
    [
     "Aleš",
     "Pražák"
    ],
    [
     "Zdeněk",
     "Loos"
    ],
    [
     "Jan",
     "Trmal"
    ],
    [
     "Josef V.",
     "Psutka"
    ],
    [
     "Josef",
     "Psutka"
    ]
   ],
   "title": "Novel approach to live captioning through re-speaking: tailoring speech recognition to re-speaker's needs",
   "original": "i12_1372",
   "page_count": 4,
   "order": 398,
   "p1": "1372",
   "pn": "1375",
   "abstract": [
    "A novel approach to the live captioning through re-speaking is introduced in this paper. We describe our concept of re-speaking using only one respeaker with enhanced re-speaker tasks fully integrated to the recognition system and captioning software. New techniques for instant correction of recognition output, punctuation mark introduction or new word addition are presented. Our real-time recognition system of the Czech language with a vocabulary containing more than one million words is described and an architecture of captioning system that we operate is illustrated. Last part of the paper is dedicated to the re-speaker training methodology and a three-level evaluation method of final re-speaker's skills is proposed.\n",
    "Index Terms: live captioning, speech recognition, re-speaking\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-395"
  },
  "kolar12_interspeech": {
   "authors": [
    [
     "Jáchym",
     "Kolář"
    ],
    [
     "Lori",
     "Lamel"
    ]
   ],
   "title": "Development and evaluation of automatic punctuation for French and English speech-to-text",
   "original": "i12_1376",
   "page_count": 4,
   "order": 399,
   "p1": "1376",
   "pn": "1379",
   "abstract": [
    "Automatic punctuation of speech is important to make speech-to-text output more readable and easier for downstream language processing. We describe the development of an automatic punctuation system for French and English. The punctuation model using both textual information and acoustic (prosodic) information is based on adaptive boosting. The system is evaluated on a difficult speech database under real-application conditions using output from a state-of-the-art speech-to-text system and automatic audio segmentation and speaker diarization. Unlike previous work, we score automatic punctuation based on two independent manual references. We also compare the two languages and the performance of the automatic system with inter-annotator agreement.\n",
    "Index Terms: automatic punctuation, rich transcription, prosody\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-396"
  },
  "ikbal12_interspeech": {
   "authors": [
    [
     "Shajith",
     "Ikbal"
    ],
    [
     "Sachindra",
     "Joshi"
    ],
    [
     "Ashish",
     "Verma"
    ],
    [
     "Om D.",
     "Deshmukh"
    ]
   ],
   "title": "Spoken document clustering using word confusion networks",
   "original": "i12_1380",
   "page_count": 4,
   "order": 400,
   "p1": "1380",
   "pn": "1383",
   "abstract": [
    "In this paper, we propose a word confusion network (WCN) based approach to perform clustering of the spoken documents and analyze its ability to handle the influence of speech recognition errors. WCN compactly represents multiple confidence weighted recognition hypotheses. Thus it provides scope for improving the clustering accuracy as a result of the likely presence of the correct transcription in the alternative hypotheses for those cases where 1-best transcripts are erroneous. On the other hand, several of the remaining hypotheses are incorrect and hence could pose a challenge during the clustering. In our approach, we extract TF-IDF vectors from the WCNs to perform clustering using K-Means algorithm. The components of TF-IDF vectors are further weighted with the word posterior probabilities. This is to potentially down-weight those vector components that are contributed by the incorrect hypotheses of low posterior probabilities. The experimental results obtained using switchboard data illustrate the usefulness of rich information in the WCN for clustering, showing up to 4% absolute improvement in normalized mutual information metric.\n",
    "Index Terms: spoken document clustering, word confusion network, posterior weighted TF-IDF vector, k-means clustering\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-397"
  },
  "wang12h_interspeech": {
   "authors": [
    [
     "Xuancong",
     "Wang"
    ],
    [
     "Hwee Tou",
     "Ng"
    ],
    [
     "Khe Chai",
     "Sim"
    ]
   ],
   "title": "Dynamic conditional random fields for joint sentence boundary and punctuation prediction",
   "original": "i12_1384",
   "page_count": 4,
   "order": 401,
   "p1": "1384",
   "pn": "1387",
   "abstract": [
    "The use of dynamic conditional random fields (DCRF) has been shown to outperform linear-chain conditional random fields (L-CRF) for punctuation prediction on conversational speech texts. In this paper, we combine lexical, prosodic, and modified n-gram score features into the DCRF framework for a joint sen-tence boundary and punctuation prediction task on TDT3 En-glish broadcast news. We show that the joint prediction method outperforms the conventional two-stage method using L-CRF or maximum entropy model (MaxEnt). We show the im-portance of various features using DCRF, LCRF, MaxEnt, and hidden-event n-gram model (HEN) respectively. In addition, we address the practical issue of feature explosion by introduc-ing lexical pruning, which reduces model size and improves the F1-measure. We adopt incremental local training to overcome memory size limitation without incurring significant per-formance penalty. Our results show that adding prosodic and n-gram score features gives ~20% relative error reduction in all cases. Overall, DCRF gives the best accuracy, followed by LCRF, MaxEnt, and HEN.\n",
    "Index Terms: punctuation, dynamic conditional random fields, sentence boundary detection\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-398"
  },
  "brugnara12_interspeech": {
   "authors": [
    [
     "Fabio",
     "Brugnara"
    ],
    [
     "Daniele",
     "Falavigna"
    ],
    [
     "Diego",
     "Giuliani"
    ],
    [
     "Roberto",
     "Gretter"
    ]
   ],
   "title": "Analysis of the characteristics of talk-show TV programs",
   "original": "i12_1388",
   "page_count": 4,
   "order": 402,
   "p1": "1388",
   "pn": "1391",
   "abstract": [
    "We examined the content of 6 talk-show TV programs in order to better understand the challenges posed by this program genre to automatic transcription. The selected programs were first segmented, transcribed and annotated by experts. Most of the speech content was found in conversational style with a significant portion of overlapped speech, about 18%. Then, automatic speech recognition experiments were conducted showing that recognition performance on talk-show programs is much worse 28.3% word error rate (WER), in comparison with that achieved on broadcast news programs, 10.9% WER. For talk-shows performance varied tangibly between non-overlapped speech, 21.8% WER, and overlapped speech, 58.5% WER. On clean, non-overlapped speech a 18.7% WER is achieved, this result is significantly worse than the result achieved for the dominant condition in broadcast news programs represented by clean read/planned speech from the anchormen, 7.6% WER.\n",
    "Index Terms: broadcast conversations, overlap speakers, spontaneous speech, automatic transcription\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-399"
  },
  "rosenberg12b_interspeech": {
   "authors": [
    [
     "Andrew",
     "Rosenberg"
    ]
   ],
   "title": "Rethinking the corpus: moving towards dynamic linguistic resources",
   "original": "i12_1392",
   "page_count": 4,
   "order": 403,
   "p1": "1392",
   "pn": "1395",
   "abstract": [
    "The corpus is an invaluable resource in Spoken and Natural Language Processing. Consistent data sets has allowed for empirical evaluation of competing algorithms. The sharing of high-quality annotated linguistic data has enabled participation and experimentation by a wide range of researchers. However, despite dubbing these annotations as \"gold-standard\", many corpora contain labeling errors and idiosyncrasies. The current view of the corpus as a static resource make correction of errors and other modifications prohibitively difficult. In this paper, a perspective of the corpus as dynamically changing is advanced. Version control software can provide a mechanism to facilitate this. We highlight the problems of the static view of the corpus through case studies of the Penn Treebank, Switchboard, Hub-4 and Boston University Radio News Corpus.\n",
    "Index Terms: Linguistic Resources, Opinion paper\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-400"
  },
  "safavi12_interspeech": {
   "authors": [
    [
     "Saeid",
     "Safavi"
    ],
    [
     "Maryam",
     "Najafian"
    ],
    [
     "Abualsoud",
     "Hanani"
    ],
    [
     "Martin",
     "Russell"
    ],
    [
     "Peter",
     "Jančovič"
    ],
    [
     "Michael",
     "Carey"
    ]
   ],
   "title": "Speaker recognition for children's speech",
   "original": "i12_1836",
   "page_count": 4,
   "order": 404,
   "p1": "1836",
   "pn": "1839",
   "abstract": [
    "This paper presents results on Speaker Recognition (SR) for children's speech, using the OGI Kids corpus and GMM-UBM and GMM-SVM SR systems. Regions of the spectrum containing important speaker information for children are identified by conducting SR experiments over 21 frequency bands. As for adults, the spectrum can be split into four regions, with the first (containing primary vocal tract resonance information) and third (corresponding to highfrequency speech sounds) being most useful for SR. However, the frequencies at which these regions occur are from 11% to 38% higher for children. It is also noted that subband SR rates are lower for younger children. Finally results are presented of SR experiments to identify a child in a class (30 children, similar age) and school (288 children, varying ages). Class performance depends on age, with accuracy varying from 90% for young children to 99% for older children. The identification rate achieved for a child in a school is 81%.\n",
    "Index Terms: speaker verification, speaker identification, child speech, gaussian mixture model, support vector machine, bandwidth.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-401"
  },
  "bordel12_interspeech": {
   "authors": [
    [
     "Germán",
     "Bordel"
    ],
    [
     "Mikel",
     "Penagarikano"
    ],
    [
     "Luis Javier",
     "Rodriguez-Fuentes"
    ],
    [
     "Amparo",
     "Varona"
    ]
   ],
   "title": "A simple and efficient method to align very long speech signals to acoustically imperfect transcriptions",
   "original": "i12_1840",
   "page_count": 4,
   "order": 405,
   "p1": "1840",
   "pn": "1843",
   "abstract": [
    "In the framework of a contract with the Basque Parliament for subtitling the videos of bilingual plenary sessions, which basically consisted of aligning very long (around 3 hours long) audio tracks with syntactically correct but acoustically inaccurate text transcriptions (since all the disfluencies, mistakes, etc. were edited), a very simple and efficient procedure (avoiding the need for language nor lexical models, which was key because of the mix of languages) was developed as a first approach, before trying more complex schemes found in the literature. Since it worked pretty well and the output was quite satisfactory for the intended application, that simple approach was finally chosen. In this paper, we describe the approach in detail and apply it to a widely known annotated dataset (specifically, to the 1997 Hub4 task), to allow the comparison to a reference approach. Results demonstrate that our approach provides only slightly worse segmentations at a much lower computational cost and requiring much fewer resources. Moreover, if the resource to be segmented includes speech in two or more languages and speakers conmute between them at any time, applying a speech recognizer becomes unfeasible in practice, whereas our approach can be still applied with no additional cost.\n",
    "Index Terms: speech-to-text alignment, automatic video subtitling, multimedia information retrieval, multilingual speech.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-402"
  },
  "takashima12_interspeech": {
   "authors": [
    [
     "Ryoichi",
     "Takashima"
    ],
    [
     "Tetsuya",
     "Takiguchi"
    ],
    [
     "Yasuo",
     "Ariki"
    ]
   ],
   "title": "Estimation of talker's head orientation based on discrimination of the shape of cross-power spectrum phase coefficients",
   "original": "i12_1844",
   "page_count": 4,
   "order": 406,
   "p1": "1844",
   "pn": "1847",
   "abstract": [
    "This paper presents a talker's head orientation estimation method using 2-channel microphones. In recent research, some approaches based on a network of microphone arrays have been proposed in order to estimate the talker's head orientation. In those methods, the talker's head orientation is estimated using the sound amplitude or peak value of CSP (Cross-power Spectrum Phase) coefficients obtained from each microphone array. However, microphone array network systems need many microphone arrays to be set along the walls of a given room so that sub-microphone arrays surround the user. In this paper, we focus on the shape of the CSP coefficients affected by the reverberation, which depends on the talker's position and the head orientation. In our proposed method, we use not only the peak value but also the other values of the CSP coefficients as feature vectors, and the talker's position and the head orientation are estimated by discriminating the CSP vector. The effectiveness of this method has been confirmed by talker localization and head orientation estimation experiments performed in a real environment.\n",
    "Index Terms: microphone array, talker localization, head orientation estimation, acoustic transfer function, CSP coefficients\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-403"
  },
  "lee12d_interspeech": {
   "authors": [
    [
     "Ann",
     "Lee"
    ],
    [
     "James",
     "Glass"
    ]
   ],
   "title": "Sentence detection using multiple annotations",
   "original": "i12_1848",
   "page_count": 4,
   "order": 407,
   "p1": "1848",
   "pn": "1851",
   "abstract": [
    "In this paper, we develop a sentence boundary detection system which incorporates a prosodic model, word and preterminal-level language models, and a global sentence-length model. An important aspect of this research was the investigation of crowdsourced punctuation annotations as a source of multiple references for evaluation purposes. In order to evaluate the system we propose a BLUE-like metric which compares a hypothesis to multiple references. Experiments on both transcription and ASR output show that the global sentence length model can improve the performance by 7.2% on reference transcripts and 3.8% on ASR output.\n",
    "Index Terms: sentence boundary detection, prosody, finite-state transducer, amazon mechanical turk\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-404"
  },
  "charlet12_interspeech": {
   "authors": [
    [
     "Delphine",
     "Charlet"
    ],
    [
     "Geraldine",
     "Damnati"
    ]
   ],
   "title": "A speaker-role based approach for detecting Politicians in TV broadcast news",
   "original": "i12_1852",
   "page_count": 4,
   "order": 408,
   "p1": "1852",
   "pn": "1855",
   "abstract": [
    "Politician speaker turn detection in TV Broadcast News shows is addressed in this paper. Politician speech model combines acoustical and lexical cues as well as contextual information, and does not use any specific politician model (person-independent). Politician speaker turn detection is coupled with an automatic role labeling step, which determines the contextual information and the set on which politician detector is applied. On a set of 101 TV broadcast news shows, experiments show that the politician speaker turns, which represent only 3% of the whole set of speaker turns in the corpus, are detected with a maximal F-measure of 65.6%.\n",
    "Index Terms: politician speech detection, speaker role\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-405"
  },
  "mai12_interspeech": {
   "authors": [
    [
     "Guangting",
     "Mai"
    ]
   ],
   "title": "Relative importance of temporal envelope and fine structure cues in low- and high- order harmonic regions for Mandarin lexical-tone recognition",
   "original": "i12_1856",
   "page_count": 4,
   "order": 409,
   "p1": "1856",
   "pn": "1859",
   "abstract": [
    "Importance of speech temporal envelope (TE) and fine structure (TFS) cues for lexical-tone recognition has been investigated in normal-hearing subjects. The present study explores the relative importance of TE and TFS cues in low- (LH) versus high-order harmonic (HH) regions, using \"acoustic chimeras\" with Mandarin monosyllables divided into 8 and 16 frequency channels that the current multichannel cochlear prosthesis can provide. The results show: (1) TE in both LH and HH regions make contributions to lexical-tone recognition without the existence of TFS of the original speech, but their relative importance is modulated by the number of channels; (2) TFS in LH region takes the major role in recognition, but is not enough for perfect performances; (3) TE in both LH and HH regions and TFS in HH region make significant but different complementary contributions based on the presence of TFS in LH region. Current results further address potential implications for cochlear implant stimulations for lexical-tones with combination of newly-developed encoding strategies.\n",
    "Index Terms: Mandarin lexical-tone recognition, temporal envelope and fine structure, low- and high-order harmonics, acoustic chimera, cochlear implants\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-406"
  },
  "tiwari12_interspeech": {
   "authors": [
    [
     "Nitya",
     "Tiwari"
    ],
    [
     "Prem C.",
     "Pandey"
    ],
    [
     "Pandurangarao N.",
     "Kulkarni"
    ]
   ],
   "title": "Real-time implementation of multi-band frequency compression for listeners with moderate sensorineural impairment",
   "original": "i12_1860",
   "page_count": 4,
   "order": 410,
   "p1": "1860",
   "pn": "1863",
   "abstract": [
    "Widening of auditory filters in persons with sensorineural hearing impairment leads to increased spectral masking and degraded speech perception. Multi-band frequency compression of the complex spectral samples using pitch-synchronous processing has been reported to increase speech perception by persons with moderate sensorineural loss. It is shown that implementation of multi-band frequency compression using fixed-frame processing along with least-squares error based signal estimation reduces the processing delay and the speech output is indistinguishable from pitch-synchronous processing. For real-time operation, the processing is implemented using a fraction of the computing capacity of the 16-bit fixed point processor TMS320C5515.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-407"
  },
  "mishra12_interspeech": {
   "authors": [
    [
     "Taniya",
     "Mishra"
    ],
    [
     "Vivek Rangarajan",
     "Sridhar"
    ],
    [
     "Alistair",
     "Conkie"
    ]
   ],
   "title": "Word prominence detection using robust yet simple prosodic features",
   "original": "i12_1864",
   "page_count": 4,
   "order": 411,
   "p1": "1864",
   "pn": "1867",
   "abstract": [
    "Automatic detection of word prominence can provide valuable information for downstream applications such as spoken language understanding. Prior work on automatic word prominence detection exploit a variety of lexical, syntactic, and prosodic features and model the task as a sequence labeling problem (independently or using context). While lexical and syntactic features are highly correlated with the notion of word prominence, the output of speech recognition is typically noisy and hence these features are less reliable than the acousticprosodic feature stream. In this work, we address the automatic detection of word prominence through novel prosodic features that capture the changes in F0 curve shape and magnitude in conjunction with duration and energy. We contrast the utility of these features with aggregate statistics of F0, duration and energy used in prior work. Our features are simple to compute yet robust to the inherent difficulties associated with identifying salient points (such as F0 peaks) in the F0 contour. Feature analysis demonstrates that these novel features are significantly more predictive than the standard aggregation-based prosodic features. Experimental results on a corpus of spontaneous speech indicate that prominence detection accuracy using only the new prosodic features is better than using both lexical and syntactic features.\n",
    "Index Terms: Word prominence detection, prosodic features\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-408"
  },
  "srivastava12_interspeech": {
   "authors": [
    [
     "Amit",
     "Srivastava"
    ],
    [
     "Saurabh",
     "Khanwalkar"
    ],
    [
     "Gretchen",
     "Markiewicz"
    ],
    [
     "Guruprasad",
     "Saikumar"
    ]
   ],
   "title": "Online story segmentation of multilingual streaming broadcast news",
   "original": "i12_1868",
   "page_count": 4,
   "order": 412,
   "p1": "1868",
   "pn": "1871",
   "abstract": [
    "We present an online story segmentation approach for Broadcast News (BN) that is built upon and integrated into BBN COTS multilingual Broadcast Monitoring System (BMS). We take a discriminative model-based approach, using Support Vector Machines to segment BN transcriptions into thematically coherent stories within the real-time constraints defined by BMS. We extract lexical, topical and story boundary cue features from source language transcriptions, machine translated (MT) English and metadata generated by BMS. We leverage BBN's Topic Classification technique to extract topic persistence features, and incorporate topic supporting words and topic clusters to encode thematic transitions. Using the discriminative model-based approach, we get a relative gain of 27.9% on English BN and 22.0% on Arabic BN over a rule-based system. We also demonstrate a relative improvement of 11.8% in segmentation performance using features extracted from MT English compared to Arabic source features. We highlight the impact of topic model training in our story segmentation approach by varying corpus size to achieve a 13.7% relative gain with increase in number of topics.\n",
    "Index Terms: story segmentation, topic classification, topic modeling\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-409"
  },
  "rasanen12c_interspeech": {
   "authors": [
    [
     "Okko",
     "Räsänen"
    ]
   ],
   "title": "Average spectrotemporal structure of continuous speech matches with the frequency resolution of human hearing",
   "original": "i12_1444",
   "page_count": 4,
   "order": 413,
   "p1": "1444",
   "pn": "1447",
   "abstract": [
    "The main goal of the auditory system is to detect and identify incoming sound patterns that are distributed in time and frequency. Since a priori knowledge of the spectrotemporal structure of these patterns is not available, the optimal strategy for the auditory system is to integrate incoming signals in frequency and time according to the average spectrotemporal structure of ecologically relevant stimuli. In the current work, we measure the average spectrotemporal dependencies of continuous speech and show that the dependency structure can be interpreted as an optimal filter matched to the structure of speech, and that the characteristics of the obtained filters are notably similar to the critical bands of human hearing. This result provides further evidence that speech and the auditory system are matched for optimal signaling performance and that the dependency structure is learnable with a single Hebbian-like learning mechanism.\n",
    "Index Terms: speech perception, auditory perception, statistical learning, sensory plasticity\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-410"
  },
  "saratxaga12_interspeech": {
   "authors": [
    [
     "Ibon",
     "Saratxaga"
    ],
    [
     "Inma",
     "Hernáez"
    ],
    [
     "Michael",
     "Pucher"
    ],
    [
     "Eva",
     "Navas"
    ],
    [
     "Iñaki",
     "Sainz"
    ]
   ],
   "title": "Perceptual importance of the phase related information in speech",
   "original": "i12_1448",
   "page_count": 4,
   "order": 414,
   "p1": "1448",
   "pn": "1451",
   "abstract": [
    "The importance of phase information in the perceptual quality of the speech signals is studied in this paper. Many speech synthesisers do not use the original phase information of the signals assuming their contribution is almost inaudible. The Relative Phase Shift, (RPS) representation of the phases allows straightforward phase structure analysis, manipulation and resynthesis, and we use these features to do a comparative evaluation of some phase modifications usually found in speech models. The final intention of this study is to get an answer to the question of whether phases deserve elaborate models to get high quality synthetic speech, or their subtle effects justify overlooking them.\n",
    "Index Terms: phase perception, RPS, speech synthesis\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-411"
  },
  "grigorescu12_interspeech": {
   "authors": [
    [
     "Andrea",
     "Grigorescu"
    ],
    [
     "Marek",
     "Rudnicki"
    ],
    [
     "Michael",
     "Isik"
    ],
    [
     "Werner",
     "Hemmert"
    ],
    [
     "Stefano",
     "Rini"
    ]
   ],
   "title": "Improving the entropy estimate of neuronal firings of modeled cochlear nucleus neurons",
   "original": "i12_1452",
   "page_count": 4,
   "order": 415,
   "p1": "1452",
   "pn": "1455",
   "abstract": [
    "In this correspondence information theoretical tools are used to investigate the statistical properties of modeled cochlear nucleus globular bushy cell spike trains. The firing patterns are obtained from a simulation software that generates sample spike trains from any auditory input. Here we analyze for the first time the responses of globular bushy cells to voiced and unvoiced speech sounds. Classical entropy estimates, such as the direct method, are improved upon by considering a time-varying and time-dependent entropy estimate. With this method we investigated the relationship between the variability of the neuronal response and the frequency content in the auditory signals. The analysis quantifies the temporal precision of the neuronal coding and the memory in the neuronal response.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-412"
  },
  "nagao12b_interspeech": {
   "authors": [
    [
     "Kyoko",
     "Nagao"
    ],
    [
     "Mark",
     "Paullin"
    ],
    [
     "James B.",
     "Polikoff"
    ],
    [
     "Jason",
     "Lilley"
    ],
    [
     "H. Timothy",
     "Bunnell"
    ]
   ],
   "title": "Perception of synthetic speech in adult users of cochlear implants",
   "original": "i12_1456",
   "page_count": 4,
   "order": 416,
   "p1": "1456",
   "pn": "1459",
   "abstract": [
    "Text-to-speech (TTS) synthesis technology has great potential to be implemented in aural rehabilitation because of the flexibility it affords for stimulus generation. However, little is known about how listeners with cochlear implants (CI) perceive and respond to synthetic speech. This study investigated how adult CI users perceive and respond to synthetic speech of varying qualities in order to determine if they respond to synthetic speech in a different manner than individuals with normal hearing (NH). Sentence recognition performance of 16 adult CI users and 30 NH listeners was compared on a semantically unpredictable sentence task using synthetic speech generated at different levels of quality. The results indicated that while the CI group had an overall lower performance than the NH group, their pattern of responding was similar to NH listeners across the speech stimuli. These parallel sentence recognition results of synthetic stimuli in both groups are promising for the application of a concatenative TTS synthesis system for generating speech training material in auditory rehabilitation software for CI users.\n",
    "Index Terms: speech synthesis, semantically unpredictable sentences, cochlear implants, speech perception, intelligibility, aural rehabilitation\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-413"
  },
  "scharenborg12c_interspeech": {
   "authors": [
    [
     "Odette",
     "Scharenborg"
    ],
    [
     "Esther",
     "Janse"
    ]
   ],
   "title": "Hearing loss and the use of acoustic cues in phonetic categorisation of fricatives",
   "original": "i12_1460",
   "page_count": 4,
   "order": 417,
   "p1": "1460",
   "pn": "1463",
   "abstract": [
    "Aging often affects sensitivity to the higher frequencies, which results in the loss of sensitivity to phonetic detail in speech. Hearing loss may therefore interfere with the categorisation of two consonants that have most information to differentiate between them in those higher frequencies and less in the lower frequencies, e.g., /f/ and /s/. We investigate two acoustic cues, i.e., formant transitions and fricative intensity, that older listeners might use to differentiate between /f/ and /s/. The results of two phonetic categorisation tasks on 38 older listeners (aged 60+) with varying degrees of hearing loss indicate that older listeners seem to use formant transitions as a cue to distinguish /s/ from /f/. Moreover, this ability is not impacted by hearing loss. On the other hand, listeners with increased hearing loss seem to rely more on intensity for fricative identification. Thus, progressive hearing loss may lead to gradual changes in perceptual cue weighting.\n",
    "Index Terms: fricative perception, aging, hearing loss, acoustic cues\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-414"
  },
  "hodoshima12_interspeech": {
   "authors": [
    [
     "Nao",
     "Hodoshima"
    ],
    [
     "Takayuki",
     "Arai"
    ],
    [
     "Kiyohiro",
     "Kurisu"
    ]
   ],
   "title": "Intelligibility of speech spoken in noise/reverberation for older adults in reverberant environments",
   "original": "i12_1464",
   "page_count": 4,
   "order": 418,
   "p1": "1464",
   "pn": "1467",
   "abstract": [
    "Speech intelligibility is in general lower for older adults than young adults in reverberant environments such as train stations or airports. We aim at to make speech announcements intelligible in public spaces. Speech spoken in noise, i.e., noise-induced speech, is usually more intelligible than speech spoken in a quiet environment for young people when they are heard in noise, a phenomenon called the Lombard effect. The current study applied this effect for an input of a sound reinforcement system in public spaces. The results of the listening tests conducted by 24 older adults showed that noise/reverberation-induced speech was more intelligible than speech spoken in a quiet environment when they were in reverberant environments (reverberation time of 1.4 s and 2.4 s). The results also showed that the effect of noise/reverberationinduced speech was observed when the recording and listening condition were different. For example, different reverberation times were used between the two conditions and noise-induced speech was intelligible in reverberation. The results suggest that using noise/reverberationinduced speech as an input of a sound reinforcement system might yields higher intelligibility in public spaces.\n",
    "Index Terms: older adults, speech intelligibility, Lombard effect, reverberation\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-415"
  },
  "hines12_interspeech": {
   "authors": [
    [
     "Andrew",
     "Hines"
    ],
    [
     "Naomi",
     "Harte"
    ]
   ],
   "title": "Improved speech intelligibility with a chimaera hearing aid algorithm",
   "original": "i12_1468",
   "page_count": 4,
   "order": 419,
   "p1": "1468",
   "pn": "1471",
   "abstract": [
    "It is recognised that current hearing aid fitting algorithms can corrupt fine timing cues in speech. This paper presents a fitting algorithm that aims to improve speech intelligibility, while preserving the temporal fine structure. The algorithm combines the signal envelope amplification from a standard hearing aid fitting algorithm with the fine timing information available to unaided listeners. The proposed \"chimaera aid\" is evaluated with computer simulated listener tests to measure its speech intelligibility for 3 sample hearing losses. In addition, the experiment demonstrates the potential application of auditory nerve models in the development of new hearing aid algorithm designs using the previously developed Neurogram Similarity Index Measure (NSIM) to predict speech intelligibility. The results predict that the new aid restores envelope without degrading fine timing information.\n",
    "Index Terms: auditory periphery model, NSIM, speech intelligibility, Hearing Aid\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-416"
  },
  "godoy12_interspeech": {
   "authors": [
    [
     "Elizabeth",
     "Godoy"
    ],
    [
     "Yannis",
     "Stylianou"
    ]
   ],
   "title": "Unsupervised acoustic analyses of normal and lombard speech, with spectral envelope transformation to improve intelligibility",
   "original": "i12_1472",
   "page_count": 4,
   "order": 420,
   "p1": "1472",
   "pn": "1475",
   "abstract": [
    "The \"Lombard effect\" describes how humans modify their speech in noisy environments to make it more intelligible. The present work analyzes Normal and Lombard speech from multiple speakers in an unsupervised context, using meaningful acoustic criteria for speech classification (according to voicing and stationarity) and evaluation (using loudness and intelligibility). These acoustic analyses using generalized classes offer alternative and informative interpretations of the Lombard effect. For example, the Lombard increase in intelligibility is shown to be isolated primarily to voiced speech. Also, while transients are shown to be less intelligible overall, the Lombard effect does not appear to distinguish between stationary and transient speech. In addition to these analyses, following recently published results illustrating that Lombard spectral modifications account for the largest increases in intelligibility, this work also examines spectral envelope transformation to improve speech intelligibility. In particular, speaker-dependent Normal-to-Lombard correction filters are estimated and, when applied in transformation, shown to yield higher overall objective intelligibility than Normal, and even Lombard, speech.\n",
    "Index Terms: Lombard effect, spectral transformation\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-417"
  },
  "amanokusumoto12_interspeech": {
   "authors": [
    [
     "Akiko",
     "Amano-Kusumoto"
    ],
    [
     "Justin M.",
     "Aronoff"
    ],
    [
     "Motokuni",
     "Itoh"
    ],
    [
     "Sigfrid D.",
     "Soli"
    ]
   ],
   "title": "The effect of dichotic processing on the perception of binaural cues",
   "original": "i12_1476",
   "page_count": 4,
   "order": 421,
   "p1": "1476",
   "pn": "1479",
   "abstract": [
    "Hearing impaired individuals often have difficulty hearing in noise because of reduced spectral resolution. Previous research suggests that dichotic processing, where information from neighboring frequency regions is sent to opposite ears, may benefit those individuals. However, dichotic processing can degrade binaural cues, reducing patial release from masking and localization accuracy. In this study, an eight-channel filter bank was used to create diotic and dichotic stimuli as well as partial dichotic stimuli that used a combination of diotic and dichotic filters. To test the effect of dichotic processing on binaural cues, speech intelligibility in noise and sound localization were evaluated in normal hearing subjects. Results showed that dichotic processing degrades speech intelligibility in spatially separated noise and sound localization, but that degradation can be minimized by using partial dichotic filtering.\n",
    "Index Terms: dichotic listening, upward spread of masking, release from masking, sound localization\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-418"
  },
  "mesgarani12_interspeech": {
   "authors": [
    [
     "Nima",
     "Mesgarani"
    ],
    [
     "Edward",
     "Chang"
    ]
   ],
   "title": "Speech and speaker separation in human auditory cortex",
   "original": "i12_1480",
   "page_count": 4,
   "order": 422,
   "p1": "1480",
   "pn": "1483",
   "abstract": [
    "Humans possess a remarkable ability to attend to a single speaker's voice in a multi-talker background. How the auditory system manages to extract intelligible speech under such acoustically complex and adverse listening conditions is not known, and indeed, it is not clear how attended speech is internally represented. Here, using multi-electrode surface recordings from the cortex of subjects engaged in a listening task with two simultaneous speakers, we demonstrate that population responses in non-primary human auditory cortex faithfully encode critical features of attended speech: speech spectrograms reconstructed based on cortical responses to the mixture of speakers reveal salient spectral and temporal features of the attended speaker, as if listening to that speaker alone. A simple classifier trained solely on examples of single speakers can decode both attended words and speaker identity. We find that task performance is well predicted by a rapid increase in attention-modulated neural selectivity across both local single- electrode and population-level cortical responses. These findings demonstrate that the cortical representation of speech does not merely reflect the external acoustic environment, but instead gives rise to the perceptual aspects relevant for the listener's intended goal.\n",
    "Index Terms: speech synthesis, unit selection, joint costs\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-419"
  },
  "edlund12_interspeech": {
   "authors": [
    [
     "Jens",
     "Edlund"
    ],
    [
     "Mattias",
     "Heldner"
    ],
    [
     "Joakim",
     "Gustafson"
    ]
   ],
   "title": "On the effect of the acoustic environment on the accuracy of perception of speaker orientation from auditory cues alone",
   "original": "i12_1484",
   "page_count": 4,
   "order": 423,
   "p1": "1484",
   "pn": "1487",
   "abstract": [
    "The ability of people, and of machines, to determine the position of a sound source in a room is well studied. The related ability to determine the orientation of a directed sound source, on the other hand, is not, but the few studies there are show people to be surprisingly skilled at it. This has bearing for studies of face-to-face interaction and of embodied spoken dialogue systems, as sound source orientation of a speaker is connected to the head pose of the speaker, which is meaningful in a number of ways. The feature most often implicated for detection of sound source orientation is the inter-aural level difference - a feature which it is assumed is more easily exploited in anechoic chambers than in everyday surroundings. We expand here on our previous studies and compare detection of speaker orientation within and outside of the anechoic chamber. Our results show that listeners find the task easier, rather than harder, in everyday surroundings, which suggests that interaural level differences is not the only feature at play.\n",
    "Index Terms: turn-taking, head pose, gaze, acoustic directionality\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-420"
  },
  "gonzalez12_interspeech": {
   "authors": [
    [
     "Sira",
     "Gonzalez"
    ],
    [
     "Mike",
     "Brookes"
    ]
   ],
   "title": "Sibilant speech detection in noise",
   "original": "i12_1488",
   "page_count": 4,
   "order": 424,
   "p1": "1488",
   "pn": "1491",
   "abstract": [
    "We present an algorithm for identifying the location of sibilant phones in noisy speech. Our algorithm does not attempt to identify sibilant onsets and offsets directly but instead detects a sustained increase in power over the entire duration of a sibilant phone. The normalized estimate of the sibilant power in each of 14 frequency bands forms the input to two Gaussian mixture models that are trained on sibilant and non-sibilant frames respectively. The likelihood ratio of the two models is then used to classify each frame. We evaluate the performance of our algorithm on the TIMIT database and demonstrate that the classification accuracy is over 80% at 0 dB signal to noise ratio for additive white noise.\n",
    "Index Terms: sibilant speech, spectrographic mask estimation, speech classification, speech segregation\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-421"
  },
  "thambiratnam12_interspeech": {
   "authors": [
    [
     "Kit",
     "Thambiratnam"
    ],
    [
     "Weiwu",
     "Zhu"
    ],
    [
     "Frank",
     "Seide"
    ]
   ],
   "title": "Voice activity detection using speech recognizer feedback",
   "original": "i12_1492",
   "page_count": 4,
   "order": 425,
   "p1": "1492",
   "pn": "1495",
   "abstract": [
    "This paper demonstrates how feedback from a speech recognizer can be leveraged to improve Voice Activity Detection (VAD) for online speech recognition. First, reliably transcribed segments of audio are fed back by the recognizer as supervision for VAD model adaptation. This allows the much stronger LVCSR acoustic models to be harnessed without adding computation. Second, when to make a VAD decision is dictated by the recognizer not the VAD module, allowing an implicit dynamic look-ahead for VAD. This improves robustness but can be gracefully reduced to meet latency requirements if necessary without requiring retraining/retuning of the VAD module. Experiments on telephone conversations yielded a 6.7% abs. reduction in frame classification error rate when feedback was applied to HMM-based VAD and a 4.2% abs. reduction over the best baseline system. Furthermore, a 3.0% abs. WER reduction was achieved over the best baseline in speech recognition experiments.\n",
    "Index Terms: voice activity detection (VAD), speech segmentation, speech recognition\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-422"
  },
  "sharma12_interspeech": {
   "authors": [
    [
     "Dushyant",
     "Sharma"
    ],
    [
     "Gaston",
     "Hilkhuysen"
    ],
    [
     "Patrick A.",
     "Naylor"
    ],
    [
     "Nikolay D.",
     "Gaubitch"
    ],
    [
     "Mark",
     "Huckvale"
    ],
    [
     "Mike",
     "Brookes"
    ]
   ],
   "title": "Descriptive vocabulary development for degraded speech",
   "original": "i12_1496",
   "page_count": 4,
   "order": 426,
   "p1": "1496",
   "pn": "1499",
   "abstract": [
    "This paper presents the development of a compact vocabulary for describing the audible characteristics of degraded speech. An experiment was conducted with 51 English-speaking subjects who were tasked with assigning one of a list of given text descriptors to 220 degradation conditions. Exploratory data analysis using hierarchical clustering resulted in a compact vocabulary of 10 classes, which was further validated by a bootstrap cluster analysis.\n",
    "Index Terms: speech degraded vocabulary, speech quality, correspondence analysis\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-423"
  },
  "yokoyama12_interspeech": {
   "authors": [
    [
     "Ryo",
     "Yokoyama"
    ],
    [
     "Yu",
     "Nasu"
    ],
    [
     "Koichi",
     "Shinoda"
    ],
    [
     "Koji",
     "Iwano"
    ]
   ],
   "title": "Overlapped speech detection in meeting using cross-channel spectral subtraction and spectrum similarity",
   "original": "i12_1500",
   "page_count": 4,
   "order": 427,
   "p1": "1500",
   "pn": "1503",
   "abstract": [
    "We propose an overlapped speech detection method for speech recognition and speaker diarization of meetings, where each speaker wears a lapel microphone. Two novel features are utilized as inputs for a GMM-based detector. One is speech power after cross-channel spectral subtraction which reduces the power from the other speakers. The other is an amplitude spectral cosine correlation coefficient which effectively extracts the correlation of spectral components in a rather quiet condition. We evaluated our method using a meeting speech corpus of four persons. The accuracy of our proposed method, 74.1%, was significantly better than that of the conventional method, 67.0%, which uses raw speech power and power spectral Pearson's correlation coefficient.\n",
    "Index Terms: overlap speech detection, spectral subtraction, cosine distance\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-424"
  },
  "lu12d_interspeech": {
   "authors": [
    [
     "Xugang",
     "Lu"
    ],
    [
     "Shigeki",
     "Matsuda"
    ],
    [
     "Chiori",
     "Hori"
    ],
    [
     "Hideki",
     "Kashioka"
    ]
   ],
   "title": "Speech restoration based on deep learning autoencoder with layer-wised pretraining",
   "original": "i12_1504",
   "page_count": 4,
   "order": 428,
   "p1": "1504",
   "pn": "1507",
   "abstract": [
    "Neural network can be used to “remember” speech patterns by encoding speech statistical regularity in network parameters. Clean speech can be “recalled” when noisy speech is input to the network. Adding more hidden layers can increase network capacity. But when the hidden layer size increases (deep network), the network is easily to be trapped to a local solution when traditional training strategy is used. Therefore, the performance of using a deep network sometimes is even worse than using a shallow network. In this study, we explore the greedy layer-wised pretraining strategy to train a deep autoencoder (DAE) for speech restoration, and apply the restored speech for noisy robust speech recognition. The DAE is first pretrained using quasi-Newton optimization algorithm layer by layer in which each layer is regarded as a shallow autoencoder. And the output of the preceding layer is served as the input to the next layer. The pretrained layers are stacked and “unrolled” to be a DAE. The pretrained parameters are served as initial parameters of the DAE which are used to refine training. The trained DAE is used as a filter for speech restoration when noisy speech is given. Noisy robust speech recognition experiments were done to examine the performance of the trained deep network. Experimental results show that the DAE trained with pretraining process significantly improved the performance of speech restoration from noisy input.\n",
    "Index Terms: Deep learning, autoencoder, noise reduction, speech recognition\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-425"
  },
  "chakraborty12_interspeech": {
   "authors": [
    [
     "Rupayan",
     "Chakraborty"
    ],
    [
     "Climent",
     "Nadeu"
    ],
    [
     "Taras",
     "Butko"
    ]
   ],
   "title": "Detection and positioning of overlapped sounds in a room environment",
   "original": "i12_1508",
   "page_count": 4,
   "order": 429,
   "p1": "1508",
   "pn": "1511",
   "abstract": [
    "The description of the acoustic activity in a room environment has to face the problem of overlapped sounds, i.e. those which occur simultaneously. That problem can be tackled by carrying out some kind of source signal separation followed by the detection and recognition of the identity of each of the overlapped sounds. An alternative approach relies on modeling all possible overlapping combinations of acoustic events. For a spatial scene description, there is still the need of assigning each of the detected acoustic events to one of the estimated source positions. Both detection approaches are tested in our work for the case of two simultaneous sources, one of which is speech, and an array of three microphones. Blind source separation based on the deflation method and null steering beamforming are used for signal separation. Also a position assignment system is developed and tested in the same experimental scenario. It is based on the above mentioned beamformer and takes the decision based on a likelihood ratio. Both signal-level fusion and likelihood fusion are tried to combine the information from the two pairs of microphones. The reported experimental results illustrate the possibilities of the various implemented techniques.\n",
    "Index Terms: Acoustic event detection, source separation, null steering beamforming, source position assignment\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-426"
  },
  "deepak12_interspeech": {
   "authors": [
    [
     "K. T.",
     "Deepak"
    ],
    [
     "Biswajit Dev",
     "Sarma"
    ],
    [
     "S. R. Mahadeva",
     "Prasanna"
    ]
   ],
   "title": "Foreground speech segmentation using zero frequency filtered signal",
   "original": "i12_1512",
   "page_count": 4,
   "order": 430,
   "p1": "1512",
   "pn": "1515",
   "abstract": [
    "A method for the robust segmentation of foreground speech in the presence of background degradation using zero frequency filtered signal (ZFFS) is proposed. The speech signal from the desired speaker collected over a mobile phone is termed as foreground speech and the acoustic background picked by the same sensor that includes both speech and non-speech sources is termed as background degradation. The zero frequency filtering (ZFF) of speech allows only information around the zero frequency to pass through. The features from the resulting ZFFS, namely, the normalized first order autocorrelation coefficient and the strength of excitation of ZFFS are observed to be different for foreground speech and background degradation. A method for foreground speech segmentation is developed using these two features. The evaluation using utterances containing isolated words of foreground speech and background degradation collected in a real environment shows a robust foreground speech segmentation.\n",
    "Index Terms: Foreground speech, background degradation, ZFFS, segmentation\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-427"
  },
  "reidy12_interspeech": {
   "authors": [
    [
     "Patrick",
     "Reidy"
    ],
    [
     "Mary",
     "Beckman"
    ]
   ],
   "title": "The effect of spectral estimator on common spectral measures for sibilant fricatives",
   "original": "i12_1516",
   "page_count": 4,
   "order": 431,
   "p1": "1516",
   "pn": "1519",
   "abstract": [
    "Recently, speech researchers have begun to base spectral analyses of sibilant fricatives on modern spectral estimators that promise reduced error in the estimation of the spectrum of the acoustic waveform. In this paper we look at the effect that the choice of spectral estimator has on the estimation of spectral properties of English voiceless sibilant fricatives.\n",
    "Index Terms: spectral estimation, multitaper spectrum, sibilants\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-428"
  },
  "grais12_interspeech": {
   "authors": [
    [
     "Emad M.",
     "Grais"
    ],
    [
     "Hakan",
     "Erdogan"
    ]
   ],
   "title": "Gaussian mixture gain priors for regularized nonnegative matrix factorization in single- channel source separation",
   "original": "i12_1520",
   "page_count": 4,
   "order": 432,
   "p1": "1520",
   "pn": "1523",
   "abstract": [
    "We propose a new method to incorporate statistical priors on the solution of the nonnegative matrix factorization (NMF) for single-channel source separation (SCSS) applications. The Gaussian mixture model (GMM) is used as a log-normalized gain prior model for the NMF solution. The normalization makes the prior models energy independent. In NMF based SCSS, NMF is used to decompose the spectra of the observed mixed signal as a weighted linear combination of a set of trained basis vectors. In this work, the NMF decomposition weights are enforced to consider statistical prior information on the weight combination patterns that the trained basis vectors can jointly receive for each source in the observed mixed signal. The NMF solutions for the weights are encouraged to increase the log likelihood with the trained gain prior GMMs while reducing the NMF reconstruction error at the same time.\n",
    "Index Terms: Nonnegative matrix factorization, single-channel source separation, Gaussian mixture models\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-429"
  },
  "ranjan12_interspeech": {
   "authors": [
    [
     "Shivesh",
     "Ranjan"
    ],
    [
     "Karen L.",
     "Payton"
    ],
    [
     "Pejman",
     "Mowlaee"
    ]
   ],
   "title": "Speaker independent single channel source separation using sinusoidal features",
   "original": "i12_1524",
   "page_count": 4,
   "order": 433,
   "p1": "1524",
   "pn": "1527",
   "abstract": [
    "Model-based approaches to achieve Single Channel Source Separation (SCSS) have been reasonably successful at separating two sources. However, most of the currently used model-based approaches require pre-trained speaker specific models in order to perform the separation. Often, insufficient or no prior training data may be available to develop such speaker specific models, necessitating the use of a speaker independent approach to SCSS. This paper proposes a speaker independent approach to SCSS using sinusoidal features. The algorithm develops speaker models for novel speakers from the speech mixtures under test, using prior training data available from other speakers. An iterative scheme improves the models with respect to the novel speakers present in the test mixtures. Experimental results indicate improved separation performance as measured by the Perceptual Evaluation of Speech Quality (PESQ) scores of the separated sources.\n",
    "Index Terms: single channel, source separation, speaker independent, sinusoidal features\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-430"
  },
  "wang12i_interspeech": {
   "authors": [
    [
     "Yuxuan",
     "Wang"
    ],
    [
     "DeLiang",
     "Wang"
    ]
   ],
   "title": "Boosting classification based speech separation using temporal dynamics",
   "original": "i12_1528",
   "page_count": 4,
   "order": 434,
   "p1": "1528",
   "pn": "1531",
   "abstract": [
    "Significant advances in speech separation have been made by formulating it as a classification problem, where the desired output is the ideal binary mask (IBM). Previous work does not explicitly model the correlation between neighboring time-frequency units and standard binary classifiers are used. As one of the most important characteristics of speech signal is its temporal dynamics, the IBM contains highly structured, instead of, random patterns. In this study, we incorporate temporal dynamics into classification by employing structured output learning. In particular, we use linear-chain structured perceptrons to account for the interactions of neighboring labels in time. However, the performance of structured perceptrons largely depends on the linear separability of features. To address this problem, we employ pretrained deep neural networks to automatically learn effective feature functions for structured perceptrons. The experiments show that the proposed system significantly outperforms previous IBM estimation systems.\n",
    "Index Terms: Monaural speech separation, temporal dynamics, structured perceptron, deep neural networks\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-431"
  },
  "wang12j_interspeech": {
   "authors": [
    [
     "Yuxuan",
     "Wang"
    ],
    [
     "Kun",
     "Han"
    ],
    [
     "DeLiang",
     "Wang"
    ]
   ],
   "title": "Acoustic features for classification based speech separation",
   "original": "i12_1532",
   "page_count": 4,
   "order": 435,
   "p1": "1532",
   "pn": "1535",
   "abstract": [
    "Speech separation can be effectively formulated as a binary classification problem. A classification based system produces a binary mask using acoustic features in each time-frequency unit. So far, only pitch and amplitude modulation spectrogram have been used as unit level features. In this paper, we study other acoustic features and show that they can significantly improve both voiced and unvoiced speech separation performance. To further explore complementarity in terms of discriminative power, we propose a group Lasso approach for feature combination. The final combined feature set yields promising results in both matched and unmatched test conditions.\n",
    "Index Terms: Speech separation, binary classification, feature combination, group Lasso\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-432"
  },
  "grais12b_interspeech": {
   "authors": [
    [
     "Emad M.",
     "Grais"
    ],
    [
     "Hakan",
     "Erdogan"
    ]
   ],
   "title": "Hidden Markov models as priors for regularized nonnegative matrix factorization in single-channel source separation",
   "original": "i12_1536",
   "page_count": 4,
   "order": 436,
   "p1": "1536",
   "pn": "1539",
   "abstract": [
    "We propose a new method to incorporate rich statistical priors, modeling temporal gain sequences in the solutions of nonnegative matrix factorization (NMF). The proposed method can be used for single-channel source separation (SCSS) applications. In NMF based SCSS, NMF is used to decompose the spectra of the observed mixed signal as a weighted linear combination of a set of trained basis vectors. In this work, the NMF decomposition weights are enforced to consider statistical and temporal prior information on the weight combination patterns that the trained basis vectors can jointly receive for each source in the observed mixed signal. The Hidden Markov Model (HMM) is used as a lognormalized gain \"weights\" prior model for the NMF solution. The normalization makes the prior models energy independent. HMM is used as a rich model that characterizes the statistics of sequential data. The NMF solutions for the weights are encouraged to increase the log-likelihood with the trained gain prior HMMs while reducing the NMF reconstruction error at the same time.\n",
    "Index Terms: Nonnegative matrix factorization, single-channel source separation, Hidden Markov Models\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-433"
  },
  "ji12_interspeech": {
   "authors": [
    [
     "Ming",
     "Ji"
    ],
    [
     "Ramji",
     "Srinivasan"
    ],
    [
     "Danny",
     "Crookes"
    ]
   ],
   "title": "Unconstrained speech separation by composition of longest segments",
   "original": "i12_1540",
   "page_count": 4,
   "order": 437,
   "p1": "1540",
   "pn": "1543",
   "abstract": [
    "A data-driven approach is presented for improving the performance of separating single-channel mixed speech signals, assuming unknown, arbitrary temporal dynamics. The new approach seeks and separates the longest mixed speech segments which can be accurately matched by composite training segments. Lengthening the mixed speech segments to match reduces the uncertainty of the matching constituent training segments, and hence the error of separation. Experiments are conducted on the Wall Street Journal database, for separating mixtures of largevocabulary speech utterances. The results are evaluated using various objective and subjective measures, including the challenge of largevocabulary continuous speech recognition. It is shown that the new separation approach leads to significant improvement in all these measures.\n",
    "Index Terms: Temporal dynamics, longest matching segment, speech separation, speech recognition\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-434"
  },
  "zhang12d_interspeech": {
   "authors": [
    [
     "Yi",
     "Zhang"
    ],
    [
     "Yunxin",
     "Zhao"
    ]
   ],
   "title": "Modulation domain blind source separation for noisy speech mixture",
   "original": "i12_1544",
   "page_count": 4,
   "order": 438,
   "p1": "1544",
   "pn": "1547",
   "abstract": [
    "In this paper, we propose a noise-robust blind speech separation (BSS) method by using two microphones. We first use modulation domain real and imaginary spectral subtraction (MRISS) to enhance both magnitude and phase spectra of the speech mixture inputs. We then estimate the direction of arrivals (DOAs) of the speech sources and perform time-acoustic-modulation frequency masking to recover the source signals. Our experimental results in five types of noise conditions have showed the superior performance of the proposed method in comparison with the conventional acoustic-domain DOA based separation method.\n",
    "Index Terms: time-frequency masking, direction of arrival, modulation frequency, blind speech separation\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-435"
  },
  "mowlaee12_interspeech": {
   "authors": [
    [
     "Pejman",
     "Mowlaee"
    ],
    [
     "Rahim",
     "Saeidi"
    ],
    [
     "Rainer",
     "Martin"
    ]
   ],
   "title": "Phase estimation for signal reconstruction in single-channel source separation",
   "original": "i12_1548",
   "page_count": 4,
   "order": 439,
   "p1": "1548",
   "pn": "1551",
   "abstract": [
    "Single-channel speech separation algorithms frequently ignore the issue of accurate phase estimation while reconstructing the enhanced signal. Instead, they directly employ the mixed-signal phase for signal reconstruction which leads to undesired traces of the interfering source in the target signal. In this paper, as- suming a given knowledge of signal spectrum amplitude, we present a solution to estimate the phase information for signal reconstruction of the sources from a single-channel mixture ob- servation. We first investigate the effectiveness of the proposed phase estimation method employing known magnitude spectra of sources as an ideal case. We further relax the ideal signal spectra assumption by perturbing the clean signal spectra via Gaussian noise. The results show that for both scenarios, ideal and noisy magnitude signal spectra, the proposed phase estima- tion approach offers improved signal reconstruction accuracy, segmental SNR and PESQ compared to benchmark methods, and those neglecting the phase information.\n",
    "Index Terms: Phase estimation, signal reconstruction, single-channel speech separation\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-436"
  },
  "chien12_interspeech": {
   "authors": [
    [
     "Jen-Tzung",
     "Chien"
    ],
    [
     "Hsin-Lung",
     "Hsieh"
    ]
   ],
   "title": "Bayesian group sparse learning for nonnegative matrix factorization",
   "original": "i12_1552",
   "page_count": 4,
   "order": 440,
   "p1": "1552",
   "pn": "1555",
   "abstract": [
    "Nonnegative matrix factorization (NMF) is developed for parts-based representation of nonnegative data with the sparseness constraint. The degree of sparseness plays an important role for model regularization. This paper presents Bayesian group sparse learning for NMF and applies it for single-channel source separation. This method establishes the common bases and individual bases to characterize the shared information and residual noise in observed signals, respectively. Laplacian scale mixture distribution is introduced for sparse coding given a sparseness control parameter. A Markov chain Monte Carlo procedure is presented to infer two groups of parameters and their hyperparameters through a sampling procedure based on the conditional posterior distributions. Experiments on separating the single-channel audio signals into rhythmic and harmonic source signals show that the proposed method outperforms baseline NMF, Bayesian NMF and other group-based NMF in terms of signal-to-interference ratio.\n",
    "Index Terms: Bayesian sparse learning, group sparsity, nonnegative matrix factorization, source separation\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-437"
  },
  "drugman12c_interspeech": {
   "authors": [
    [
     "Thomas",
     "Drugman"
    ],
    [
     "John",
     "Kane"
    ],
    [
     "Christer",
     "Gobl"
    ]
   ],
   "title": "Resonator-based creaky voice detection",
   "original": "i12_1592",
   "page_count": 4,
   "order": 441,
   "p1": "1592",
   "pn": "1595",
   "abstract": [
    "Creaky voice is used by speakers for a variety of interactive, expressive and stylistic reasons. As a result the accurate detection of creaky regions in speech can yield important information not captured within the propositional content of spoken utterances. We, hence, describe a new method for automatically detecting creaky regions following the observation that secondary peaks occur in the LP-residual signal. The proposed approach is shown through an objective evaluation on a range of speech databases to significantly outperform the state-of-the-art.\n",
    "Index Terms: Voice quality, glottal source, creak, vocal fry\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-438"
  },
  "mittal12_interspeech": {
   "authors": [
    [
     "V. K.",
     "Mittal"
    ],
    [
     "N.",
     "Dhananjaya"
    ],
    [
     "Bayya",
     "Yegnanarayana"
    ]
   ],
   "title": "Effect of tongue tip trilling on the glottal excitation source",
   "original": "i12_1596",
   "page_count": 4,
   "order": 442,
   "p1": "1596",
   "pn": "1599",
   "abstract": [
    "Recent studies have indicated changes in the glottal excitation source characteristics apart from vocal tract resonances due to tongue tip trilling. In this paper we study the significance of changing vocal tract system and the associated glottal excitation source characteristics due to trilling, from perception point of view. These studies are made by generating speech signal by either retaining the features of the vocal tract system or of the glottal excitation source of trill sounds. Experiments are conducted to understand the perceptual significance of the excitation source characteristics on production of different trill sounds. Speech sounds of long trill and approximant pair, and apical trills produced by four different places of articulation are considered. Features of the vocal tract system are extracted using linear prediction analysis, and those of the source by zero frequency filtering.\n",
    "Index Terms: glottal excitation source, trill synthesis, zero frequency filtering, apical trills, tongue tip trilling\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-439"
  },
  "chen12i_interspeech": {
   "authors": [
    [
     "Gang",
     "Chen"
    ],
    [
     "Yen-Liang",
     "Shue"
    ],
    [
     "Jody",
     "Kreiman"
    ],
    [
     "Abeer",
     "Alwan"
    ]
   ],
   "title": "Estimating the voice source in noise",
   "original": "i12_1600",
   "page_count": 4,
   "order": 443,
   "p1": "1600",
   "pn": "1603",
   "abstract": [
    "Estimation of the glottal source has applications in many areas of speech processing. Therefore, a noise-robust automatic source estimation algorithm is proposed in this paper. The source signal is estimated using a codebook search approach and formant estimation is not required. The glottal area waveforms extracted from high-speed recordings of the glottis is converted to the glottal flow signals in order to evaluate the performance of the proposed source estimation algorithm. Results in clean and noisy conditions, on average, show that the proposed algorithm provides more accurate estimation than the software toolkit Aparat as well as an earlier approach.\n",
    "Index Terms: voice source, source estimation, speech analysis\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-440"
  },
  "pinheiro12_interspeech": {
   "authors": [
    [
     "Alan",
     "Pinheiro"
    ],
    [
     "Tuomo",
     "Raitio"
    ],
    [
     "Danyane",
     "Gomes"
    ],
    [
     "Paavo",
     "Alku"
    ]
   ],
   "title": "Voice source analysis using biomechanical modeling and glottal inverse filtering",
   "original": "i12_1604",
   "page_count": 4,
   "order": 444,
   "p1": "1604",
   "pn": "1607",
   "abstract": [
    "This paper studies the use of glottal inverse filtering together with a biomechanical model of the vocal folds to simulate the glottal flow waveform. The glottal flow waveform is first estimated by inverse filtering the acoustic speech pressure signal of natural speech. The estimated glottal flow is used as a template in an optimization process which searches for a set of parameters for a deterministic vocal fold model such that the model output reproduces the estimated glottal flow. The results indicate that the method can reproduce the main deterministic components of the glottal flow signal with good accuracy.\n",
    "Index Terms: vocal folds, glottal flow, biomechanical simulation, glottal inverse filtering\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-441"
  },
  "drioli12_interspeech": {
   "authors": [
    [
     "Carlo",
     "Drioli"
    ],
    [
     "Andrea",
     "Calanca"
    ]
   ],
   "title": "Speech modeling and processing by low-dimensional dynamic glottal models",
   "original": "i12_1608",
   "page_count": 4,
   "order": 445,
   "p1": "1608",
   "pn": "1611",
   "abstract": [
    "We discuss the use of low-dimensional physical models of the voice source for speech coding and processing applications. A class of waveform-adaptive dynamic glottal models and parameter tracking procedures are illustrated. The model and analysis procedures are assessed by addressing signal transformations on recorded speech, achievable by fitting the model to the data, and then acting on the physically-oriented parameters of the voice source. The class of models proposed provides in principle a tool for both the estimation of glottal source signals, and the encoding of the speech signal for transformation purposes. The application of this model to time stretching and to frequency control (pitch shifting) is also illustrated. The experiments show that copy synthesis is perceptually almost indistin- guishable form the target, and that time stretching and ”pitch extrapolation” effects can be obtained by simple control strategies.\n",
    "Index Terms: speech synthesis, glottal modeling, speech coding, physical modeling\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-442"
  },
  "alku12_interspeech": {
   "authors": [
    [
     "Paavo",
     "Alku"
    ],
    [
     "Jouni",
     "Pohjalainen"
    ],
    [
     "Martti",
     "Vainio"
    ],
    [
     "Anne-Maria",
     "Laukkanen"
    ],
    [
     "Brad",
     "Story"
    ]
   ],
   "title": "Improved formant frequency estimation from high-pitched vowels by downgrading the contribution of the glottal source with weighted linear prediction",
   "original": "i12_1612",
   "page_count": 4,
   "order": 446,
   "p1": "1612",
   "pn": "1615",
   "abstract": [
    "Since performance of conventional linear prediction (LP) deteriorates in formant estimation of high-pitched voices, several all-pole modeling methods robust to F0 have been developed. This study compares five such previously known methods and proposes a new technique, Weighted Linear Prediction with Attenuated Main Excitation (WLP-AME). WLP-AME utilizes weighted linear prediction in which the square of the prediction error is multiplied with a weighting function that downgrades the contribution of the glottal source in the model optimization. Consequently, the resulting all-pole model is affected more by the vocal tract characteristics, which leads to more accurate formant estimates. By using synthetic vowels created with a physical modeling approach, the study shows that WLP-AME yields improved formant frequency estimates for high-pitched vowels in comparison to the previously known methods.\n",
    "Index Terms: formants, linear prediction\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-443"
  },
  "sasou12_interspeech": {
   "authors": [
    [
     "Akira",
     "Sasou"
    ]
   ],
   "title": "Automatic topology generation of glottal source HMM",
   "original": "i12_1616",
   "page_count": 4,
   "order": 447,
   "p1": "1616",
   "pn": "1619",
   "abstract": [
    "We previously proposed the Auto-Regressive Hidden Markov Model (AR-HMM) for speech signal analysis, where the HMM was introduced as a non-stationary glottal source model. In this paper, we propose a novel method that can automatically generate the topology of the Glottal Source Hidden Markov Model (GS-HMM), as well as estimate the AR-HMM parameter obtained by combining the AR-HMM parameter estimation method and the Minimum Description Length-based Successive State Splitting (MDL-SSS) algorithm. In the experiments, we apply the proposed method to analyze the laryngeal and esophageal voices. The topology generated from the laryngeal voices tended to form a ring state, compared with the topology of the esophageal voices; this result indicates that the glottal sources of the laryngeal voices exhibit clearer periodicity than the sound sources of the esophageal voices. We also compared the vocal tract characteristics estimated by the proposed method and a conventional LP method. From these results, we were able to confirm the feasibility and the validity of the proposed method.\n",
    "Index Terms: glottal source, AR-HMM, MDL-SSS\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-444"
  },
  "lorenzotrueba12_interspeech": {
   "authors": [
    [
     "Jaime",
     "Lorenzo-Trueba"
    ],
    [
     "Roberto",
     "Barra-Chicote"
    ],
    [
     "Tuomo",
     "Raitio"
    ],
    [
     "Nicolas",
     "Obin"
    ],
    [
     "Paavo",
     "Alku"
    ],
    [
     "Junichi",
     "Yamagishi"
    ],
    [
     "Juan M.",
     "Montero"
    ]
   ],
   "title": "Towards glottal source controllability in expressive speech synthesis",
   "original": "i12_1620",
   "page_count": 4,
   "order": 448,
   "p1": "1620",
   "pn": "1623",
   "abstract": [
    "In order to obtain more human like sounding human-machine interfaces we must first be able to give them expressive capabilities in the way of emotional and stylistic features so as to closely adequate them to the intended task. If we want to replicate those features it is not enough to merely replicate the prosodic information of fundamental frequency and speaking rhythm. The proposed additional layer is the modification of the glottal model, for which we make use of the GlottHMM parameters. This paper analyzes the viability of such an approach by verifying that the expressive nuances are captured by the aforementioned features, obtaining 95% recognition rates on styled speaking and 82% on emotional speech. Then we evaluate the effect of speaker bias and recording environment on the source modeling in order to quantify possible problems when analyzing multi-speaker databases. Finally we propose a speaking styles separation for Spanish based on prosodic features and check its perceptual significance.\n",
    "Index Terms: expressive speech synthesis, speaking style, glottal source modeling\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-445"
  },
  "alpan12_interspeech": {
   "authors": [
    [
     "Ali",
     "Alpan"
    ],
    [
     "Jean",
     "Schoentgen"
    ],
    [
     "Francis",
     "Grenez"
    ]
   ],
   "title": "Combining temporal and cepstral features for the automatic perceptual categorization of disordered connected speech",
   "original": "i12_1624",
   "page_count": 4,
   "order": 449,
   "p1": "1624",
   "pn": "1627",
   "abstract": [
    "The objective of the presentation is to report experiments involving the automatic classification of disordered connected speech into multiple (modal, moderately hoarse, severely hoarse) categories. Support vector machines, used for the classification, have been fed with temporal signal-todysperiodicity ratios, the first rahmonic amplitude as well as mel-frequency cepstral coefficients. The signal-to-dysperiodicity ratio complements the first rahmonic amplitude when categorizing voice samples according to the degree of hoarseness yielding 77% of correct classification.\n",
    "Index Terms: automatic perceptual categorization of disordered connected speech, variogram analysis, signal-to-dysperiodicity ratio, first amplitude rahmonic, mel-frequency cepstral coefficients, support vector machine\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-446"
  },
  "sun12c_interspeech": {
   "authors": [
    [
     "Rui",
     "Sun"
    ],
    [
     "Elliot",
     "Moore II"
    ]
   ],
   "title": "A preliminary study on cross-databases emotion recognition using the glottal features in speech",
   "original": "i12_1628",
   "page_count": 4,
   "order": 450,
   "p1": "1628",
   "pn": "1631",
   "abstract": [
    "While the majority of traditional research in emotional speech recognition has focused on the use of a single database for assessment, it is clear that the lack of large databases has presented a significant challenge in generalizing results for the purposes of building a robust emotion classification system. Recently, work has been reported on cross-training emotional databases to examine consistency and reliability of acoustic measures in performing emotional assessment. This paper presents preliminary results on the use of glottal-based features in cross-testing (i.e., training on one database and testing on another) across 3 databases for emotion recognition of neutral, angry, happy, and sad. A comparative study is also presented using pitch-based features. The results suggest that the glottal features are more robust to the 4-class emotion classification system developed in this study and are able to perform well above chance for several of the cross-testing experiments.\n",
    "Index Terms: emotion recognition, cross-databases, glottal features, pitch\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-447"
  },
  "maia12_interspeech": {
   "authors": [
    [
     "Ranniery",
     "Maia"
    ],
    [
     "Masami",
     "Akamine"
    ]
   ],
   "title": "Analysis on the importance of short-term speech parameterizations for emotional statistical parametric speech synthesis",
   "original": "i12_1632",
   "page_count": 4,
   "order": 451,
   "p1": "1632",
   "pn": "1635",
   "abstract": [
    "This paper presents a study on the importance of short-term spectral and excitation parameterizations for emotional hidden Markov model (HMM)-based speech synthesis. The analysis is performed through an emotion classification task by using two methods: K-means emotion clustering and Gaussian Mixture Models (GMMs)-based emotion identification. Two known forms of parameterization for the short-term speech spectral envelope, the mel-cepstrum and the mel- line spectrum pairs are utilized while features derived from the complex cepstrum and group delay, and band-aperiodicity coefficients are used as excitation parameters. The emotion-dependent features according to the classification performance are then selected to train emotion-dependent HMM-based synthesizers. Listening tests are then performed to verify the impact of the parameters on the similarity of the synthesized speech with its natural version.\n",
    "Index Terms: speech synthesis, statistical parametric speech synthesis, expressive speech synthesis\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-448"
  },
  "mertens12_interspeech": {
   "authors": [
    [
     "Christophe",
     "Mertens"
    ],
    [
     "Francis",
     "Grenez"
    ],
    [
     "Jean",
     "Schoentgen"
    ]
   ],
   "title": "Analysis of vocal tremor and jitter by empirical mode decomposition of glottal cycle length time series",
   "original": "i12_1636",
   "page_count": 4,
   "order": 452,
   "p1": "1636",
   "pn": "1639",
   "abstract": [
    "The presentation concerns a method for tracking cycle lengths in voiced speech and breaking up vocal cycle length fluctuations in cycle length jitter and cycle length tremor. The tracking of the cycle lengths is based on a dynamic programming algorithm, which does not request that the signal is locally periodic and that the average period length is known a priori. The cycle length time series are decomposed into a sum of intrinsic mode functions by means of empirical mode decomposition. These mode functions are then assigned to three phenomena, which are cycle length jitter, cycle length tremor and trend owing to intonation and physiological tremor. We report tests of the proposed analysis by means of synthetic disordered speech sounds and illustrate slow and fast cycle length perturbations in modal and essential tremor speakers.\n",
    "Index Terms: vocal frequency, vocal tremor, vocal jitter, speech salience analysis, empirical mode decomposition\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-449"
  },
  "auvinen12_interspeech": {
   "authors": [
    [
     "Harri",
     "Auvinen"
    ],
    [
     "Tuomo",
     "Raitio"
    ],
    [
     "Samuli",
     "Siltanen"
    ],
    [
     "Paavo",
     "Alku"
    ]
   ],
   "title": "Utilizing Markov chain Monte Carlo (MCMC) method for improved glottal inverse filtering",
   "original": "i12_1640",
   "page_count": 4,
   "order": 453,
   "p1": "1640",
   "pn": "1643",
   "abstract": [
    "This paper presents a new glottal inverse filtering (GIF) method that utilizes Markov chain Monte Carlo (MCMC) algorithm. First, initial estimates of the vocal tract and glottal flow are evaluated by an existing GIF method, the iterative adaptive inverse filtering (IAIF). Simultaneously, the initially estimated glottal flow is synthesized using the Klatt model and filtered with the estimated vocal tract filter. In the MCMC estimation process, the first few poles of the initial vocal tract model and the Klatt parameter are refined in order to minimize the error between the original and the synthetic signals. MCMC converges to the optimal result, and the final estimate of the vocal tract is found by averaging the parameter values of the Markov chain. Experiments show that the MCMC-based GIF method gives more accurate results compared to the original IAIF method.\n",
    "Index Terms: glottal inverse filtering, Markov chain Monte Carlo, MCMC\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-450"
  },
  "huber12_interspeech": {
   "authors": [
    [
     "Stefan",
     "Huber"
    ],
    [
     "Axel",
     "Roebel"
    ],
    [
     "Gilles",
     "Degottex"
    ]
   ],
   "title": "Glottal source shape parameter estimation using phase minimization variants",
   "original": "i12_1644",
   "page_count": 4,
   "order": 454,
   "p1": "1644",
   "pn": "1647",
   "abstract": [
    "The glottal shape parameter Rd provides a one-dimensional parameterisation of the Liljencrants-Fant (LF) model which describes the deterministic component of the glottal source. In this paper we first propose to estimate the Rd parameter by means of extending a stateof- the- art method based on the phase minimization criterion. Then we propose an adaption of the standard Rd parameter regression which enables us to coherently assess the normal and the upper Rd range. By evaluating the confusion matrices depicting the error surfaces of the involved different Rd parameter estimation methods and by objective measurement tests we verify the overall improvement of one new method compared to the state-of-the-art baseline approach.\n",
    "Index Terms: glottal excitation source, shape parameter, voice quality, confusion matrices, Rd regression\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-451"
  },
  "godin12_interspeech": {
   "authors": [
    [
     "Keith W.",
     "Godin"
    ],
    [
     "Taufiq",
     "Hasan"
    ],
    [
     "John H. L.",
     "Hansen"
    ]
   ],
   "title": "Glottal waveform analysis of physical task stress speech",
   "original": "i12_1648",
   "page_count": 4,
   "order": 455,
   "p1": "1648",
   "pn": "1651",
   "abstract": [
    "Physical task stress affects the acoustic speech wave in various ways. Motivated by observations that fundamental frequency and open quotient are affected by physical task stress, this study examines the effects of physical task stress on parameters of the estimated glottal volume velocity waveform. It is shown that, in contrast to other types of phonation such as soft, loud, breathy, or pressed voice, physical task stress has little effect on the glottal waveform parameters chosen for analysis. Further, the use of glottal waveform parameters in a stress detection system does not improve the system accuracy, again in contrast to other types of non-neutral speech. These results suggest that a medium level of physical task stress does not greatly perturb vocal fold behavior, and the search for explanations for the spectral perturbations that make stress detection possible must turn to other directions.\n",
    "Index Terms: glottal waveform, physical task stress, stress detection\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-452"
  },
  "torres12_interspeech": {
   "authors": [
    [
     "Juan Félix",
     "Torres"
    ],
    [
     "Elliot",
     "Moore"
    ]
   ],
   "title": "Speaker discrimination ability of glottal waveform features",
   "original": "i12_1652",
   "page_count": 4,
   "order": 456,
   "p1": "1652",
   "pn": "1655",
   "abstract": [
    "To measure the extent to which individual glottal features obtained via inverse filtering can be used to discriminate between the speech of different speakers, we test a set of 16 glottal parameters on a 50-speaker subset of the TIMIT corpus in a frame-level pairwise speaker discrimination task. In addition, we compare a vector of glottal parameters to a set of spectral envelope features in the same task. The results showed higher discrimination ability between pairs of male speakers, and an overall inconsistency in feature rankings between genders. Glottal feature vectors were able to discriminate speakers at a much higher rate, and showed some ability to complement spectral envelope features.\n",
    "Index Terms: glottal waveform, voice source, speaker identification, feature extraction\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-453"
  },
  "liu12_interspeech": {
   "authors": [
    [
     "Xunying",
     "Liu"
    ],
    [
     "Mark J. F.",
     "Gales"
    ],
    [
     "Phillip C.",
     "Woodland"
    ]
   ],
   "title": "Paraphrastic language models",
   "original": "i12_1656",
   "page_count": 4,
   "order": 457,
   "p1": "1656",
   "pn": "1659",
   "abstract": [
    "Natural languages have layered structures, the meaning and the surface word sequence. The mapping from the former to the latter is often one-to-many. This dramatically increases the sparsity when directly modelling the surface word sequence, for example, using n-gram language models (LM). To handle this issue, this paper presents a novel form of language model, paraphrastic LMs. A phrase level transduction model statistically learnt from standard text data is used to generate paraphrase variants. LM probabilities are then estimated by maximizing their marginal probability. Significant error rate reductions of 0.5%-0.6% absolute were obtained on a state-of-the-art conversational telephone speech recognition task using a paraphrastic multi-level LM modelling both word and phrase sequences.\n",
    "Index Terms: language model, paraphrase, speech recognition\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-454"
  },
  "rastrow12_interspeech": {
   "authors": [
    [
     "Ariya",
     "Rastrow"
    ],
    [
     "Mark",
     "Dredze"
    ],
    [
     "Sanjeev",
     "Khudanpur"
    ]
   ],
   "title": "Efficient structured language modeling for speech recognition",
   "original": "i12_1660",
   "page_count": 4,
   "order": 458,
   "p1": "1660",
   "pn": "1663",
   "abstract": [
    "The structured language model (SLM) of [1] was one of the first to successfully integrate syntactic structure into language models. We extend the SLM framework in two new directions. First, we propose a new syntactic hierarchical interpolation that improves over previous approaches. Second, we develop a general information-theoretic algorithm for pruning the underlying Jelinek-Mercer interpolated LM used in [1], which substantially reduces the size of the LM, enabling us to train on large data. When combined with hill-climbing [2] the SLM is an accurate model, space-efficient and fast for rescoring large speech lattices. Experimental results on broadcast news demonstrate that the SLM outperforms a large 4-gram LM.\n",
    "s\n",
    "C. Chelba and F. Jelinek, “Structured language modeling,” Computer Speech and Language, vol. 14, no. 4, pp. 283–332, 2000.\n",
    "A. Rastrow, M. Dreyer, A. Sethy, S. Khudanpur, B. Ramabhadran, and M. Dredze, “Hill climbing on speech lattices: A new rescoring framework,” in Proceeding of ICASSP, 2011\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-455"
  },
  "shi12_interspeech": {
   "authors": [
    [
     "Yangyang",
     "Shi"
    ],
    [
     "Pascal",
     "Wiggers"
    ],
    [
     "Catholijn M.",
     "Jonker"
    ]
   ],
   "title": "Towards recurrent neural networks language models with linguistic and contextual features",
   "original": "i12_1664",
   "page_count": 4,
   "order": 459,
   "p1": "1664",
   "pn": "1667",
   "abstract": [
    "Recent studies show that recurrent neural network language models (RNNLM) perform better than traditional language models such as smoothed n-grams. For traditional models it is known that the addition of for example part-of-speech information and topical information can improve performance. In this paper we investigate the usefulness of additional features for RNNLM. We look at four types of features: POS tags, lemmas, and the topics and the socio-situational setting of a conversation. In our experiments, almost all RNNLM models that make use of extra information outperform our baseline RNNLM model in terms of both perplexity and word prediction accuracy. Whereas the baseline model has a perplexity of 114.79, the model that uses a combination of POS tags, socio-situational settings and lemmas achieves the lowest perplexity result of 83.59, and the combination of all 4 types of features, using a network with 500 hidden neurons, achieves the highest word prediction accuracy of 23.11%.\n",
    "Index Terms: socio-situational setting, part of speech, lemma, topic, recurrent neural networks.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-456"
  },
  "lecorve12b_interspeech": {
   "authors": [
    [
     "Gwénolé",
     "Lecorvé"
    ],
    [
     "Petr",
     "Motlicek"
    ]
   ],
   "title": "Conversion of recurrent neural network language models to weighted finite state transducers for automatic speech recognition",
   "original": "i12_1668",
   "page_count": 4,
   "order": 460,
   "p1": "1668",
   "pn": "1671",
   "abstract": [
    "Recurrent neural network language models (RNNLMs) have recently shown to outperform the venerable n-gram language models (LMs). However, in automatic speech recognition (ASR), RNNLMs were not yet used to directly decode a speech signal. Instead, RNNLMs are rather applied to rescore N-best lists generated from word lattices. To use RNNLMs in earlier stages of the speech recognition, our work proposes to transform RNNLMs into weighted finite state transducers approximating their underlying probability distribution. While the main idea consists in discretizing continuous representations of word histories, we present a first implementation of the approach using clustering techniques and entropy-based pruning. Achieved experimental results on LM perplexity and on ASR word error rates are encouraging since the performance of the discretized RNNLMs is comparable to the one of n-gram LMs.\n",
    "Index Terms: Language model, recurrent neural network, weighted finite state transducer, speech decoding\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-457"
  },
  "kuo12b_interspeech": {
   "authors": [
    [
     "Hong-Kwang",
     "Kuo"
    ],
    [
     "Ebru",
     "Arısoy"
    ],
    [
     "Ahmad",
     "Emami"
    ],
    [
     "Paul",
     "Vozila"
    ]
   ],
   "title": "Large scale hierarchical neural network language models",
   "original": "i12_1672",
   "page_count": 4,
   "order": 461,
   "p1": "1672",
   "pn": "1675",
   "abstract": [
    "Feed-forward neural network language models (NNLMs) are known to improve both perplexity and word error rate performance for speech recognition compared with conventional n-gram language models. We present experimental results showing how much the WER can be improved by increasing the scale of the NNLM, in terms of model size and training data. However, training time can become very long. We implemented a hierarchical NNLM approximation to speed up the training, through splitting up events and parallelizing training as well as reducing the output vocabulary size of each sub-network. The training time was reduced by about 20 times, e.g. from 50 days to 2 days, with no degradation in WER. Using English Broadcast News data (350M words), we obtained significant improvements over the baseline n-gram language model, competitive with recently published recurrent neural network language model (RNNLM) results.\n",
    "Index Terms: neural network language models\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-458"
  },
  "hutchinson12_interspeech": {
   "authors": [
    [
     "Brian",
     "Hutchinson"
    ],
    [
     "Mari",
     "Ostendorf"
    ],
    [
     "Maryam",
     "Fazel"
    ]
   ],
   "title": "A sparse plus low rank maximum entropy language model",
   "original": "i12_1676",
   "page_count": 4,
   "order": 462,
   "p1": "1676",
   "pn": "1679",
   "abstract": [
    "This work introduces a new maximum entropy language model that decomposes the model parameters into a low rank component that learns regularities in the training data and a sparse component that learns exceptions (e.g. keywords). The low rank solution corresponds to a continuous-space language model. This model generalizes the standard l1-regularized maximum entropy model, and has an efficient accelerated first-order training algorithm. In conversational speech language modeling experiments, we see perplexity reductions of 2-5%.\n",
    "Index Terms: language modeling, maximum entropy, sparse plus low rank decomposition\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-459"
  },
  "jiang12_interspeech": {
   "authors": [
    [
     "Ye",
     "Jiang"
    ],
    [
     "Kong Aik",
     "Lee"
    ],
    [
     "Zhenmin",
     "Tang"
    ],
    [
     "Bin",
     "Ma"
    ],
    [
     "Anthony",
     "Larcher"
    ],
    [
     "Haizhou",
     "Li"
    ]
   ],
   "title": "PLDA modeling in i-vector and supervector space for speaker verification",
   "original": "i12_1680",
   "page_count": 4,
   "order": 463,
   "p1": "1680",
   "pn": "1683",
   "abstract": [
    "In this paper, we advocate the use of uncompressed form of i-vector. We employ the probabilistic linear discriminant analysis (PLDA) to handle speaker and session variability for speaker verification task. An i-vector is a low-dimensional vector containing both speaker and channel information acquired from a speech segment. When PLDA is used on i-vector, dimension reduction is performed twice . first in the i-vector extraction process and second in the PLDA model. Keeping the full dimensionality of i-vector in the supervector space for PLDA modeling and scoring would avoid unnecessary loss of information. The drawback of using PLDA on uncompressed i-vector is the inversion of large matrices, which we show can be solved rather efficiently by portioning large matrix into smaller blocks. We also introduce the Gaussianized rank-norm, as an alternative to whitening, for feature normalization prior to PLDA modeling.\n",
    "Index Terms: speaker verification, i-vector, probabilistic LDA\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-460"
  },
  "simonchik12_interspeech": {
   "authors": [
    [
     "Konstantin",
     "Simonchik"
    ],
    [
     "Timur",
     "Pekhovsky"
    ],
    [
     "Andrey",
     "Shulipa"
    ],
    [
     "Anton",
     "Afanasyev"
    ]
   ],
   "title": "Supervized mixture of PLDA models for cross-channel speaker verification",
   "original": "i12_1684",
   "page_count": 4,
   "order": 464,
   "p1": "1684",
   "pn": "1687",
   "abstract": [
    "This paper presents a development of previous research by P.Kenny, which deals with using a supervised PLDA mixture of two gender-dependent speaker verification systems under the conditions of gender uncertainty. We propose using PLDA mixtures for speaker verification in different channels. However, in contrast to creating a gender-independent mixture, the optimal decision for training a channel-independent mixture for two channels in our task was mixing three channel-dependent PLDA systems. The experiments conducted on different conditions of NIST 2010 showed the superior robustness of the PDLA system mixture compared to each of its component PDLA subsystems not only in EER value but also in the stability of the decision threshold. The latter fact is very significant for using this approach not just for obtaining a good NIST SRE actual cost but also for commercial applications.\n",
    "Index Terms: speaker verification, i-vector, length normalization, supervized mixture PLDA\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-461"
  },
  "alegre12_interspeech": {
   "authors": [
    [
     "Federico",
     "Alegre"
    ],
    [
     "Ravichander",
     "Vipperla"
    ],
    [
     "Nicholas",
     "Evans"
    ]
   ],
   "title": "Spoofing countermeasures for the protection of automatic speaker recognition systems against attacks with artificial signals",
   "original": "i12_1688",
   "page_count": 4,
   "order": 465,
   "p1": "1688",
   "pn": "1691",
   "abstract": [
    "The vulnerability of automatic speaker recognition systems to imposture or spoofing is widely acknowledged. This paper shows that extremely high false alarm rates can be provoked by simple spoofing attacks with artificial, non-speech-like signals and highlights the need for spoofing countermeasures. We show that two new, but trivial countermeasures based on higher-level, dynamic features and voice quality assessment offer varying degrees of protection and that further work is needed to develop more robust spoofing countermeasure mechanisms. Finally, we show that certain classifiers are inherently more robust to such attacks than others which strengthens the case for fused-system approaches to automatic speaker recognition.\n",
    "Index Terms: automatic speaker verification, biometrics, spoofing, imposture, countermeasures\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-462"
  },
  "stafylakis12_interspeech": {
   "authors": [
    [
     "Themos",
     "Stafylakis"
    ],
    [
     "Patrick",
     "Kenny"
    ],
    [
     "Mohammed",
     "Senoussaoui"
    ],
    [
     "Pierre",
     "Dumouchel"
    ]
   ],
   "title": "PLDA using Gaussian restricted boltzmann machines with application to speaker verification",
   "original": "i12_1692",
   "page_count": 4,
   "order": 466,
   "p1": "1692",
   "pn": "1695",
   "abstract": [
    "A novel approach to supervised dimensionality reduction is introduced, based on Gaussian Restricted Boltzmann Machines. The proposed model should be considered as the analogue of the probabilistic LDA, using undirected graphical models. The training algorithm of the model is presented while its close relation to the cosine distance is underlined. For the problem of speaker verification, we applied it to i-vectors and attained a significant improvement compared to the Fisher's Discriminant LDA projection using less than half of the number of eigenvectors required by LDA.\n",
    "Index Terms: Speaker Recognition, Restricted Boltzmann Machines\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-463"
  },
  "sadjadi12_interspeech": {
   "authors": [
    [
     "Seyed Omid",
     "Sadjadi"
    ],
    [
     "Taufiq",
     "Hasan"
    ],
    [
     "John H. L.",
     "Hansen"
    ]
   ],
   "title": "Mean hilbert envelope coefficients (MHEC) for robust speaker recognition",
   "original": "i12_1696",
   "page_count": 4,
   "order": 467,
   "p1": "1696",
   "pn": "1699",
   "abstract": [
    "The recently introduced mean Hilbert envelope coefficients (MHEC) have been shown to be an effective alternative to MFCCs for robust speaker identification under noisy and reverberant conditions in relatively small tasks. In this study, we investigate the effectiveness of these acoustic features in the context of a state-of-the-art speaker recognition system. The i-vectors are used to represent the acoustic space of speakers, while modeling is performed via probabilistic linear discriminant analysis (PLDA). We report speaker verification performance on the NIST SRE-2010 extended telephone and microphone trials for both female and male genders. Experimental results confirm consistent superiority of MHECs to traditional MFCCs within i-vector speaker verification, particularly under microphone and telephone training-test mismatch conditions. In addition, fusion of subsystems trained with the individual front-ends proves that the two acoustic features (i.e., MHEC and MFCC) provide complimentary information for recognizing speakers.\n",
    "Index Terms: Mean Hilbert Envelope Coefficients (MHEC), mismatch conditions, NIST SRE, speaker recognition\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-464"
  },
  "wu12c_interspeech": {
   "authors": [
    [
     "Zhizheng",
     "Wu"
    ],
    [
     "Eng Siong",
     "Chng"
    ],
    [
     "Haizhou",
     "Li"
    ]
   ],
   "title": "Detecting converted speech and natural speech for anti-spoofing attack in speaker recognition",
   "original": "i12_1700",
   "page_count": 4,
   "order": 468,
   "p1": "1700",
   "pn": "1703",
   "abstract": [
    "Voice conversion techniques present a threat to speaker verification systems. To enhance the security of speaker verification systems, We study how to automatically distinguish natural speech and synthetic/converted speech. Motivated by the research on phase spectrum in speech perception, in this study, we propose to use features derived from phase spectrum to detect converted speech. The features are tested under three different training situations: a) only Gaussian mixture model (GMM) based converted speech data are available; b) only unit-selection based converted speech data are available; c) no converted speech data are available. Experiments conducted on the National Institute of Standards and Technology (NIST) 2006 speaker recognition evaluation (SRE) corpus show that the performance of the features derived from phase spectrum outperform the mel-frequency cepstral coefficients (MFCCs) tremendously: even without converted speech for training, the equal error rate (EER) is reduced from 20.20 of MFCCs to 2.35.\n",
    "Index Terms: Speaker verification, voice conversion, anti-spoofing attack, synthetic speech detection, phase spectrum\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-465"
  },
  "villegas12_interspeech": {
   "authors": [
    [
     "Julián",
     "Villegas"
    ],
    [
     "Martin",
     "Cooke"
    ]
   ],
   "title": "Maximising objective speech intelligibility by local <i>F</i><sub>0</sub> modulation",
   "original": "i12_1704",
   "page_count": 4,
   "order": 469,
   "p1": "1704",
   "pn": "1707",
   "abstract": [
    "We investigated the effect on objective speech intelligibility of scaling the fundamental frequency (F0) of voiced regions in a set of utterances. The frequency scaling was driven by max- imising the glimpse proportion in voiced epochs, inspired by musical consonance maximisation techniques. Results show that depending on the energetic masker and the signal to noise ratio, F0 modifications increased the mean glimpse proportion by up to 15 %. On average, lower mean F0 changes resulted in greater glimpse proportions. It was also found that the glimpse proportion could be a good predictor of music consonance.\n",
    "Index Terms: roughness, glimpse proportion, objective speech intelligibility, musical consonance, fundamental frequency\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-466"
  },
  "mayo12_interspeech": {
   "authors": [
    [
     "Catherine",
     "Mayo"
    ],
    [
     "Vincent",
     "Aubanel"
    ],
    [
     "Martin",
     "Cooke"
    ]
   ],
   "title": "Effect of prosodic changes on speech intelligibility",
   "original": "i12_1708",
   "page_count": 4,
   "order": 470,
   "p1": "1708",
   "pn": "1711",
   "abstract": [
    "Talkers adopt different speech styles in response to factors such as the perceived needs of the interlocutor, environmental noise and explicit instruction. Some styles have been shown to be beneficial for listeners but many aspects of the relationship between speech modifications and intelligibility remain unclear, particularly for prosodic changes. The current study measures the relative intelligibility in noise of speech spoken in 5 speech styles . plain, infant-, computer- and foreigner-directed, and shouted . and relates listener scores to acoustic/prosodic parameters and quantitative estimates of energetic masking. Intelligibility changes over plain speech correlated well with durational modifications, which included elongations of all segments as well as increases in the number of unfilled pauses. Both mean fundamental frequency and its range displayed great variation across styles but with no clear intelligibility benefits. Energetic masking per unit time was similar in each style but the total amount of speech which escaped masking was a good predictor of word identification rate. These findings suggest that much of the prosody-related intelligibility gain is derived from durational increases.\n",
    "Index Terms: speech styles, prosody, intelligibility\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-467"
  },
  "kawase12_interspeech": {
   "authors": [
    [
     "Saya",
     "Kawase"
    ],
    [
     "Yue",
     "Wang"
    ]
   ],
   "title": "Effects of visual speech information on native listener judgments of L2 consonant intelligibility",
   "original": "i12_1712",
   "page_count": 4,
   "order": 471,
   "p1": "1712",
   "pn": "1715",
   "abstract": [
    "The present study examines how visual information in nonnative phonemes affects perceptual accuracy of second language (L2) speech production. Native Canadian English listeners perceived three English phonemic contrasts produced by native speakers of Japanese as well as native speakers of Canadian English as controls, under audiovisual (AV), audio-only (AO), and visual-only (VO) conditions. The phonemes include /v, Θ, l, ɹ/, which are not existent in Japanese (L2 phonemes) as well as /b, s/ that are shared in both Japanese and English consonant inventories. The results showed that the English listeners perceived the Japanese productions of the phonemes /b, v, s, Θ/ as significantly more intelligible when presented with the AV condition compared to the AO condition, indicating facilitative effects of visual speech information on their perceptual accuracy of nonnative production. However, the Japanese production of /ɹ/ was perceived as less intelligible in the AV condition compared to the AO condition, indicating that nonnative speakers' incorrect articulatory configurations may decrease the degree of intelligibility. These results suggest that listener judgments of L2 productions may be either positively or negatively affected by additional visual speech information.\n",
    "Index Terms: audiovisual speech, perception of L2 consonants, Japanese learners of English.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-468"
  },
  "brown12_interspeech": {
   "authors": [
    [
     "Guy J.",
     "Brown"
    ],
    [
     "Amy V.",
     "Beeston"
    ],
    [
     "Kalle J.",
     "Palomäki"
    ]
   ],
   "title": "Perceptual compensation for the effects of reverberation on consonant identification: a comparison of human and machine performance",
   "original": "i12_1716",
   "page_count": 4,
   "order": 472,
   "p1": "1716",
   "pn": "1719",
   "abstract": [
    "Human listeners are able to perceptually compensate for the effects of reverberation on speech recognition, by exploiting information gleaned from prior exposure to the reverberant environment. We present a computer model of perceptual compensation for reverberation implemented within a hidden Markov model speech recogniser, in which different reverberant speech models are selected depending on the acoustic context preceding a test word. During decoding, observation state likelihoods were computed from two reverberant acoustic models in parallel, and weighted according to the amount of reverberation in the first 500 ms of each utterance. The confusions made by the computer model closely corresponded with those made by listeners in a consonant identification task, and showed a perceptual compensation effect.\n",
    "Index Terms: reverberation, speech perception, computer model\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-469"
  },
  "fitzpatrick12_interspeech": {
   "authors": [
    [
     "Michael",
     "Fitzpatrick"
    ],
    [
     "Jeesun",
     "Kim"
    ],
    [
     "Chris",
     "Davis"
    ]
   ],
   "title": "The intelligibility of lombard speech: communicative setting matters",
   "original": "i12_1720",
   "page_count": 4,
   "order": 473,
   "p1": "1720",
   "pn": "1723",
   "abstract": [
    "Recently we reported that talkers modify their speech production strategies in noise as a function of whether their interlocutor could or could not be seen, i.e. face-to-face (FTF) or non-visual conditions (NV). Participants made greater auditory speech modifications (e.g. in terms of amplitude and F0) in NV condition, and greater visual speech modifications (in terms of inter-lip area) in FTF condition. The current study examined whether such modifications will lead to corresponding differences in speech intelligibility in the different settings. In the experiment, participants were presented with a set of consonant-vowel-consonant (CVC) phonemes in noise at a fixed SNR in auditory-only, visual-only and auditory-visual conditions. The CVC stimuli were drawn from speech recordings in quiet and in noise conditions, and also during NV and FTF conditions. The results showed that the speech in noise tokens produced in the FTF conditions had a greater AV benefit than for tokens produced in the NV conditions. Also, the AV benefit was greater for speech tokens produced in noise than for speech produced in quiet. The results were discussed in terms of efficient talker and listener strategies.\n",
    "Index Terms: Lombard speech, AV speech, speech production.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-470"
  },
  "santos12_interspeech": {
   "authors": [
    [
     "João Felipe",
     "Santos"
    ],
    [
     "Stefano",
     "Cosentino"
    ],
    [
     "Oldooz",
     "Hazrati"
    ],
    [
     "Philipos C.",
     "Loizou"
    ],
    [
     "Tiago H.",
     "Falk"
    ]
   ],
   "title": "Performance comparison of intrusive objective speech intelligibility and quality metrics for cochlear implant users",
   "original": "i12_1724",
   "page_count": 4,
   "order": 474,
   "p1": "1724",
   "pn": "1727",
   "abstract": [
    "In this paper, we evaluate the performance of six intrusive objective measures as intelligibility predictors of degraded speech for cochlear implant (CI) users. Three practical environmental degradation scenarios are considered: reverberation alone, additive noise alone, and noise-plus-reverberation. A subjective intelligibility test was performed with eleven cochlear implant users and objective measures were evaluated using three performance metrics: Pearson, Spearman rank, and sigmoid-fitted correlation coefficients. It was observed that existing metrics performed well in the noise-alone scenarios, but obtained lower performance in the reverberation-alone scenario and in many cases, unacceptable results in the noise-plus-reverberation scenario. It is concluded that further work is still needed in order to accurately predict speech intelligibility ratings for CI users, particularly in environments corrupted by reverberation.\n",
    "Index Terms: Objective Measures, Speech Intelligibility, Reverberation, Noise, Cochlear Implants\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-471"
  },
  "chaudhuri12_interspeech": {
   "authors": [
    [
     "Sourish",
     "Chaudhuri"
    ],
    [
     "Rita",
     "Singh"
    ],
    [
     "Bhiksha",
     "Raj"
    ]
   ],
   "title": "Exploiting temporal sequence structure for semantic analysis of multimedia",
   "original": "i12_1728",
   "page_count": 4,
   "order": 475,
   "p1": "1728",
   "pn": "1731",
   "abstract": [
    "In this paper, we explore the hypothesis that the ability to accurately associate semantics to scenes requires processing of sequences of such scenes rather than individual snapshots in time. We build on work that seeks to represent audio as a sequence of descriptors, each spanning multiple frames, by exploring and comparing different ways of obtaining such a lexicon of descriptors. We then present an extension of such an unsupervised learning scheme to video, and report results on experiments with the Multimedia Event Detection, 2011 dataset. We find that learning the set of descriptors automatically from data significantly outperforms the vector quantization-based systems and systems using library based descriptors.\n",
    "Index Terms: multimedia analysis, semantic labels, unsupervised lexicon learning, audiovisual data retrieval\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-472"
  },
  "liu12b_interspeech": {
   "authors": [
    [
     "Hong",
     "Liu"
    ],
    [
     "Xiaofei",
     "Li"
    ]
   ],
   "title": "Time delay estimation for speech signal based on FOC-spectrum",
   "original": "i12_1732",
   "page_count": 4,
   "order": 476,
   "p1": "1732",
   "pn": "1735",
   "abstract": [
    "Higher-order statistics can be used for time delay estimation (TDE) to suppress spatially correlated Gaussian noise, since the higher-order cumulant of Gaussian signal is always zero. However, third-order statistics is invalid for those signals with zero skewness, speech signal as a typical one. In this paper, the fourth-order cumulant (FOC) spectrum is derived, based on which a TDE algorithm that is valid for speech signal and immune to spatially correlated Gaussian noise is proposed. This method can estimate the time delay between two sensor signals or simultaneously estimate the time delays between one sensor signal and other three. In addition, just like generalized cross correlation method, this spectrum domain algorithm is more robust than time domain FOC-based TDE algorithm, especially for speech signal due to its periodicity. Experiments verify the effectiveness of this TDE method for speech signal with spatially correlated Gaussian noise.\n",
    "Index Terms: time delay estimation, FOC-spectrum, spatially correlated Gaussian noise, speech signal\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-473"
  },
  "shi12b_interspeech": {
   "authors": [
    [
     "Ziqiang",
     "Shi"
    ],
    [
     "Tieran",
     "Zheng"
    ],
    [
     "Jiqing",
     "Han"
    ],
    [
     "Shiwen",
     "Deng"
    ]
   ],
   "title": "Low-rank audio signal classification under soft margin and trace norm constraints",
   "original": "i12_1736",
   "page_count": 4,
   "order": 477,
   "p1": "1736",
   "pn": "1739",
   "abstract": [
    "We propose an algorithm to classify low-rank matrix representative audio data. Conventionally, the low-rank matrix data can be represented by a vector in high dimensional space. Some learning algorithms are then applied in such a vector space for matrix data classification. Particularly, maximum margin classifiers, such as support vector machine (SVM) etc. have received a lot of attentions due to their effectiveness. In this paper, we classify the data directly in the matrix space. Our methodology is built on recent studies about matrix classification with the trace norm constrained weight matrix and SVM's large-margin linear discrimination principle. The resulting low-rank SVM is then designed to maximize the margin between classes whilst minimizing the complexity of the classifier in both original and low-rank space. We compared our proposed algorithm with SVM and other state-of-the-art matrix classification methods. Experimental studies on real life audio signal classification show the effectiveness of our algorithm.\n",
    "Index Terms: speech/non-speech, matrix classification, maximum margin, trace norm regularization, low-rank feature\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-474"
  },
  "segura12_interspeech": {
   "authors": [
    [
     "Carlos",
     "Segura"
    ],
    [
     "Javier",
     "Hernando"
    ]
   ],
   "title": "GCC-PHAT based head orientation estimation",
   "original": "i12_1740",
   "page_count": 4,
   "order": 478,
   "p1": "1740",
   "pn": "1743",
   "abstract": [
    "This work presents a novel two-step algorithm to estimate the orientation of speakers in a smart-room environment equipped with microphone arrays. First the position of the speaker is estimated by the SRP-PHAT algorithm, and the time delay of arrival for each microphone pair with respect to the detected position is computed. In the second step, the value of the cross-correlation at the estimated time delay is used as the fundamental characteristic from where to derive the speaker orientation. The proposed method performs consistently better than other state-of-the-art acoustic techniques with a purposely recorded database and the CLEAR head pose database.\n",
    "Index Terms: Head pose; speaker orientation; acoustic source localization\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-475"
  },
  "de12_interspeech": {
   "authors": [
    [
     "Soham",
     "De"
    ],
    [
     "Indradyumna",
     "Roy"
    ],
    [
     "Tarunima",
     "Prabhakar"
    ],
    [
     "Kriti",
     "Suneja"
    ],
    [
     "Sourish",
     "Chaudhuri"
    ],
    [
     "Rita",
     "Singh"
    ],
    [
     "Bhiksha",
     "Raj"
    ]
   ],
   "title": "Plagiarism detection in polyphonic music using monaural signal separation",
   "original": "i12_1744",
   "page_count": 4,
   "order": 479,
   "p1": "1744",
   "pn": "1747",
   "abstract": [
    "With the rapid increase in the number of music documents being registered for copyright every year, detection of music plagiarism has become critical and parallelly more difficult. While most current research focuses on monophonic music, we attempt to design a method for finding similarities between polyphonic music documents, which can subsequently be used for the automatic identification of music plagiarism. For extracting features for each song, we use the non-negative matrix factorization (NMF) method, which has recently been effectively used in signal separation. We also find a number of features directly based on the music content, which when combined with the NMF, lead to accurate results. A modified version of the dynamic time warping algorithm is used for comparing the features obtained between two songs. A database of almost 3000 songs is created to train a random forest classifier, and the method is tested on successful plagiarism suits obtained from the Music Copyright Infringement Resource of the UCLA School of Law. Preliminary results show an accuracy of 78.4%.\n",
    "Index Terms: music plagiarism detection, polyphonic music, similarity measures, compositional models, monaural signal separation\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-476"
  },
  "bouafif12_interspeech": {
   "authors": [
    [
     "Mariem",
     "Bouafif"
    ],
    [
     "Zied",
     "Lachiri"
    ]
   ],
   "title": "TDOA estimation for multiple speakers in underdetermined case",
   "original": "i12_1748",
   "page_count": 4,
   "order": 480,
   "p1": "1748",
   "pn": "1751",
   "abstract": [
    "In this paper we address the issue of estimating the time delay of arrival in underdetermined case. We develop a method using the excitation characteristics of the speech production. This method is based on the cross correlation of the Hilbert Envelops of linear prediction residuals derived from two microphones signals. The method has been applied to real data obtained by recording many sources captured by a pair of microphones. Experiments show that reverberation distorts the input signals, each reverberation causes an extra peak in the crosscorrelation. This makes it difficult to determine which peak is the central time-delay peak and which are just reverberation sidelobes. An alternative time delay estimation method has been implemented and compared to spectrum angular methods.\n",
    "Index Terms: TDOA, Linear prediction, Hilbert Envelope\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-477"
  },
  "nakashika12_interspeech": {
   "authors": [
    [
     "Toru",
     "Nakashika"
    ],
    [
     "Christophe",
     "Garcia"
    ],
    [
     "Tetsuya",
     "Takiguchi"
    ]
   ],
   "title": "Local-feature-map integration using convolutional neural networks for music genre classification",
   "original": "i12_1752",
   "page_count": 4,
   "order": 481,
   "p1": "1752",
   "pn": "1755",
   "abstract": [
    "A map-based approach, which treats 2-dimensional acoustic features using image analysis, has recently attracted attention in music genre classification. While this is successful at extracting local music-patterns compared with other frame-based methods, in most works the extracted features are not sufficient for music genre classification. In this paper, we focus on appropriate feature extraction and proper classification by integrating automatically learnt image feature. For the musical feature extraction, we build gray level co-occurrence matrix (GLCM) descriptors with different offsets from a short-term mel spectrogram. These feature maps are integratively classified using convolutional neural networks (ConvNets). In our experiments, we obtained a large improvement of more than 10 points in classification accuracy on the GTZAN database, compared with other ConvNets-based methods.\n",
    "Index Terms: music genre classification, music information retrieval, music feature extraction, convolutional neural networks\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-478"
  },
  "berry12_interspeech": {
   "authors": [
    [
     "Jeff",
     "Berry"
    ],
    [
     "Ian",
     "Fasel"
    ],
    [
     "Luciano",
     "Fadiga"
    ],
    [
     "Diana",
     "Archangeli"
    ]
   ],
   "title": "Training deep nets with imbalanced and unlabeled data",
   "original": "i12_1756",
   "page_count": 4,
   "order": 482,
   "p1": "1756",
   "pn": "1759",
   "abstract": [
    "Training deep belief networks (DBNs) is normally done with large data sets. Our goal is to predict traces of the surface of the tongue in ultrasound images of hu- man speech. Hand-tracing is labor-intensive; the dataset is highly imbalanced since many images are extremely similar. We propose a bootstrapping method which han- dles this imbalance by iteratively selecting a small subset of images to be handtraced (thereby reducing human la- bor time), then (re)training the DBN, making use of an entropy-based diversity measure for the initial selection, thereby achieving over a two-fold reduction in human time required for tracing with human-level accuracy.\n",
    "Index Terms: deep belief networks, ultrasound imaging, tongue imaging, speech processing, bootstrapping, class imbalance problem\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-479"
  },
  "asami12_interspeech": {
   "authors": [
    [
     "Taichi",
     "Asami"
    ],
    [
     "Satoshi",
     "Kobashikawa"
    ],
    [
     "Hirokazu",
     "Masataki"
    ],
    [
     "Osamu",
     "Yoshioka"
    ],
    [
     "Satoshi",
     "Takahashi"
    ]
   ],
   "title": "Speech data clustering based on phoneme error trend for unsupervised acoustic model adaptation",
   "original": "i12_1760",
   "page_count": 4,
   "order": 483,
   "p1": "1760",
   "pn": "1763",
   "abstract": [
    "Unsupervised cluster adaptive training of acoustic models offers promise in improving recognition accuracy, especially for speech recognition systems that store massive sets of speech samples from unknown people. How to classify the variety of acoustic characteristics is an important problem in adaptation sample clustering. We propose a novel speech sample clustering method that focuses on the phoneme error trend in each speech sample. The proposed method classifies adaptation samples in terms of the trend of phoneme discrimination in each sample, and represents each sample as a compact phoneme error trend vector whose dimension is at most the number of phonemes. Experiments illustrate that the phoneme error trend vectors have enough expressiveness to classify acoustic characteristics effectively, and are compact enough to provide robustness against unknown data.\n",
    "Index Terms: speech recognition, acoustic model adaptation, data clustering, phoneme error trend\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-480"
  },
  "kim12f_interspeech": {
   "authors": [
    [
     "Wooil",
     "Kim"
    ],
    [
     "John H. L.",
     "Hansen"
    ]
   ],
   "title": "Gaussian map based acoustic model adaptation using untranscribed data for speech recognition in severely adverse environments",
   "original": "i12_1764",
   "page_count": 4,
   "order": 484,
   "p1": "1764",
   "pn": "1767",
   "abstract": [
    "This study proposes an acoustic model adaptation scheme to improve speech recognition in severely adverse environments utilizing untranscribed data. In the proposed method, a clean GMM is estimated from clean training data, and a noise corrupted GMM is obtained by MAP adaptation over the adaptation data. The Gaussian component of the adapted HMMs is obtained using the transform of the most similar Gaussian component of the GMM. The proposed mixture-selective model adaptation method is evaluated using an LDC corpus which represents severely adverse communication channel environments. The experimental results show the proposed adaptation method is comparable or improves performance compared to conventional MLLR adaptation. The proposed method is also effective at improving speech recognition using independent adaptation data sets. Performance results demonstrate that the proposed adaptation method is significantly more effective at improving speech recognition in severely noise conditions, where transcribed data is unavailable and baseline ASR fails to accurately transcribe the adaptation data due to acoustic condition mismatch.\n",
    "Index Terms: model adaptation, untranscribed data, Gaussian mapping, adverse environments, robust speech recognition\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-481"
  },
  "jiang12b_interspeech": {
   "authors": [
    [
     "Danning",
     "Jiang"
    ],
    [
     "Dimitri",
     "Kanevsky"
    ],
    [
     "Vaibhava",
     "Goel"
    ],
    [
     "Yong",
     "Qin"
    ]
   ],
   "title": "Investigating performance of the discriminative methods for long-term speaker adaptation",
   "original": "i12_1768",
   "page_count": 4,
   "order": 485,
   "p1": "1768",
   "pn": "1771",
   "abstract": [
    "Many of today's speech recognition applications can benefit from long-term speaker adaptation using speaker logs, and discriminative methods present a promising approach for that given their previous successes. This paper carries out large-vocabulary speech recognition experiments to investigate performance of feature-space and model-space discriminative adaptation methods for long-term speaker adaptation. The experimental results suggest that though on average discriminative adaptation does not obtain a big gain over ML adaptation, there are still a number of test speakers that show significant improvements. Motivated by this observation, we further propose an efficient method to automatically select speakers which can obtain big improvements in discriminative adaptation. When 35%~65% of the whole test population are selected for discriminative adaptation, the relative WER reduction over ML adaptation can reach 4%~5% if only these speakers' performance is inspected.\n",
    "Index Terms: discriminative speaker adaptation, CDLT, DLT, DMAP, performance prediction\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-482"
  },
  "li12f_interspeech": {
   "authors": [
    [
     "Bo",
     "Li"
    ],
    [
     "Khe Chai",
     "Sim"
    ]
   ],
   "title": "A two-stage speaker adaptation approach for subspace Gaussian mixture model based nonnative speech recognition",
   "original": "i12_1772",
   "page_count": 4,
   "order": 486,
   "p1": "1772",
   "pn": "1775",
   "abstract": [
    "Nonnative speech recognition is becoming more and more important as many speech applications are deployed world wide. Meanwhile, due to the large population of nonnative speakers, speaker adaptation remains the most practical way for providing high performance speech services. Subspace Gaussian Mixture Model (SGMM) has recently been shown to yield superior performance on various native speech recognition tasks. In this paper, we investigated different speaker adaptation techniques of SGMM for nonnative speech recognition. A two-stage direct model adaptation approach has been proposed based on the analysis of SGMM model parameter functionalities. Our initial experiments have also verified that the proposed approach is much more effective than the traditional feature-space Maximum Likelihood Linear Regression(MLLR) on SGMM based nonnative speaker adaptation tasks.\n",
    "Index Terms: Speaker Adaptation, Nonnative Speech Recognition, Subspace Gaussian Mixture Model\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-483"
  },
  "christensen12_interspeech": {
   "authors": [
    [
     "Heidi",
     "Christensen"
    ],
    [
     "Stuart",
     "Cunningham"
    ],
    [
     "Charles",
     "Fox"
    ],
    [
     "Phil",
     "Green"
    ],
    [
     "Thomas",
     "Hain"
    ]
   ],
   "title": "A comparative study of adaptive, automatic recognition of disordered speech",
   "original": "i12_1776",
   "page_count": 4,
   "order": 487,
   "p1": "1776",
   "pn": "1779",
   "abstract": [
    "Speech-driven assistive technology can be an attractive alternative to conventional interfaces for people with physical disabilities. However, often the lack of motor-control of the speech articulators results in disordered speech, as condition known as dysarthria. Dysarthric speakers can generally not obtain satisfactory performances with off-the-shelf automatic speech recognition (ASR) products and disordered speech ASR is an increasingly active research area. Sparseness of suitable data is a big challenge. The experiments described here use UAspeech, one of the largest dysarthric databases available, which is still easily an order of magnitude smaller than typical speech databases. This study investigates how far state-of-the-art training and adaptation techniques developed in the LVCSR community can take us. A variety of ASR systems using maximum likelihood and MAP adaptation strategies are established with all speakers obtaining significant improvements compared to the baseline system regardless of the severity of their condition. The best systems show on average 34% relative improvement on known published results. An analysis of the correlation between intelligibility of the speaker and the type of system which would represent an optimal operating point in terms of performance shows that for less severely dysarthric speakers, there is a wider choice of \"best\" system.\n",
    "Index Terms: dysarthric speech, speech recognition, speaker adaptation\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-484"
  },
  "uluskan12_interspeech": {
   "authors": [
    [
     "Seçkin",
     "Uluskan"
    ],
    [
     "John H. L.",
     "Hansen"
    ]
   ],
   "title": "Phoneme class based adaptation for mismatch acoustic modeling of distant noisy speech",
   "original": "i12_1780",
   "page_count": 4,
   "order": 488,
   "p1": "1780",
   "pn": "1783",
   "abstract": [
    "A new adaptation strategy for distant noisy speech is created by phoneme class based approaches for context-independent acoustic models. Unlike the previous approaches such as MLLR-MAP adaptation which adapts acoustic model to the features, our phoneme-class based adaptation (PCBA) adapts the distant data features to our acoustic model which has trained on close microphone TIMIT sentences. The essence of PCBA is to create a transformation strategy which makes the distribution of phoneme-classes of distant noisy speech be similar to those of close microphone acoustic model in thirteen dimensional MFCC space (mostly in c0-c1 plane). It creates a mean, orientation and variance adaptation scheme for each phoneme class to compensate the mismatch. New adapted features, and new and improved acoustic models which are produced by PCBA are outperforming those created by MLLR-MAP adaptation for ASR and KWS. And PCBA offers a new powerful understanding in acoustic-modeling of distant speech.\n",
    "Index Terms: phoneme class, distant noisy speech, mismatch acoustic modeling, feature adaptation\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-485"
  },
  "roupakia12_interspeech": {
   "authors": [
    [
     "Zoi",
     "Roupakia"
    ],
    [
     "Anton",
     "Ragni"
    ],
    [
     "Mark J. F.",
     "Gales"
    ]
   ],
   "title": "Rapid nonlinear speaker adaptation for large-vocabulary continuous speech recognition",
   "original": "i12_1784",
   "page_count": 4,
   "order": 489,
   "p1": "1784",
   "pn": "1787",
   "abstract": [
    "Recently, kernel eigenvoices were revisited using kernel representations of distributions for rapid nonlinear speaker adaptation. These representations reassure the validity of the adapted distribution functions and enable expectation-maximisation training. Though gains have been shown in terms of word error rate for rapid speaker adaptation, this approach leads to an increase in decoding cost as the number of likelihood evaluations is amplified. The present paper addresses this issue by providing a coherent framework for systematic probabilistic approaches aimed at reducing the recognition cost and yet yielding equally powerful adapted models. The common denominator of such approaches is the use of probabilistic criteria, such as Kullback-Leibler divergence. However, in the general case, the resulting adapted models have full covariance matrices. In order to overcome this issue, the use of predictive semi-tied transforms to yield diagonal covariances for decoding is investigated in this paper. Experimental results are presented on a large-vocabulary conversational telephone task.\n",
    "Index Terms: kernel eigenvoices, compact nonlinear adaptation, Kullback Leibler divergence\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-486"
  },
  "chen12j_interspeech": {
   "authors": [
    [
     "I-Fan",
     "Chen"
    ],
    [
     "Chin-Hui",
     "Lee"
    ]
   ],
   "title": "A study on using word-level HMMs to improve ASR performance over state-of-the-art phone-level acoustic modeling for LVCSR",
   "original": "i12_1788",
   "page_count": 4,
   "order": 490,
   "p1": "1788",
   "pn": "1791",
   "abstract": [
    "In this paper, we propose word-level hidden Markov models (HMMs) to supplement state-of-the-art phone-based acoustic modeling in order to enhance the performance of automatic speech recognition (ASR) system. Each word in a vocabulary is initially modeled by well-trained triphone models. Maximum a posteriori adaptation is then applied to generate models for words with a large number of occurrences in the training set so that the acoustic distribution of the words can be modeled more precisely. Experimental results show that the proposed word-based systems outperform phone-based systems on the TIMIT task with a small training corpus. While in tasks with plenty of training data, word-based systems still show improvements over phone-based systems, such as the WSJ task. Furthermore the word-based systems have a better discriminating ability on short words and homophones. They are also more robust to language model weight variation than conventional phone-based systems.\n",
    "Index Terms: word-level HMM, automatic speech recognition, detection-based ASR, language model weight, homophone\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-487"
  },
  "seltzer12_interspeech": {
   "authors": [
    [
     "Michael",
     "Seltzer"
    ],
    [
     "Alex",
     "Acero"
    ]
   ],
   "title": "Factored adaptation using a combination of feature-space and model-space transforms",
   "original": "i12_1792",
   "page_count": 4,
   "order": 491,
   "p1": "1792",
   "pn": "1795",
   "abstract": [
    "Acoustic model adaptation can mitigate the degradation in recognition accuracy caused by speaker or environment mismatch. While there are many methods for speaker or environment adaptation, far less attention has been focused on methods that compensate for both causes simultaneously. We recently proposed an algorithm called factored adaptation which jointly estimates speaker and environment transforms in a manner which facilitates the reuse of transforms across sessions. For example, a speaker transform estimated in one environment can later be used even if the speaker's environment changes. In this paper, we introduce a new factored adaptation algorithm that uses a combination of feature-space and model-space transforms. We describe an iterative EM algorithm for transform estimation that also incorporates speaker and environment clustering in cases where the speaker or environment labels are unknown. On a large vocabulary voice search task, the proposed method consistently outperforms conventional adaptation.\n",
    "Index Terms: speaker adaptation, environment adaptation, robustness, factored transforms, acoustic factorization\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-488"
  },
  "huang12e_interspeech": {
   "authors": [
    [
     "Heyun",
     "Huang"
    ],
    [
     "Louis ten",
     "Bosch"
    ],
    [
     "Bert",
     "Cranen"
    ],
    [
     "Lou",
     "Boves"
    ]
   ],
   "title": "Exploring discriminative speech trajectory structures",
   "original": "i12_1796",
   "page_count": 4,
   "order": 492,
   "p1": "1796",
   "pn": "1799",
   "abstract": [
    "The articulators of the human speech production mechanism can only move relatively sluggishly. This results in speech sounds of which the acoustic speech properties mostly change continuously and gradually over time. However, such continuity constraints are seldom exploited for the purpose of discriminating different phones. In order to explore to what extent incorporating continuity information can help to improve phone discrimination, we investigated a multi-frame MFCC representation in combination with a supervised dimensionality reduction method which is aimed at finding a low-dimensional representation that best separates the different phones. The speech continuity information is encoded by a second-order smoothness regularizer. Experimental results on TIMIT phone classification show that the regularizer is helpful in better distinguishing vowels, but fails to improve the discrimination of consonants.\n",
    "Index Terms: Dimensionality Reduction; Contextual Representation; TIMIT; regularization; Laplacian smoothing\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-489"
  },
  "variani12_interspeech": {
   "authors": [
    [
     "Ehsan",
     "Variani"
    ],
    [
     "Hynek",
     "Hermansky"
    ]
   ],
   "title": "Estimating classifier performance in unknown noise",
   "original": "i12_1800",
   "page_count": 4,
   "order": 493,
   "p1": "1800",
   "pn": "1803",
   "abstract": [
    "We propose and investigate methods for identifying regions of speech that have unexpected distortions not seen in training data. The methods do not require knowledge of correct labels and rely only on divergence between statistics of test and training data. We propose two metrics with and without probabilistic assumptions. Our experiments show that the proposed non-probabilistic method requires a relatively small amount of test data of the order of several seconds to stabilize, and correlates well with recognition error observed on the test data.\n",
    "Index Terms: Unexpected distortions, confidence estimation, machine recognition of speech\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-490"
  },
  "jalalvand12_interspeech": {
   "authors": [
    [
     "Azarakhsh",
     "Jalalvand"
    ],
    [
     "Fabian",
     "Triefenbach"
    ],
    [
     "Jean-Pierre",
     "Martens"
    ]
   ],
   "title": "Continuous digit recognition in noise: reservoirs can do an excellent job!",
   "original": "i12_1804",
   "page_count": 4,
   "order": 494,
   "p1": "1804",
   "pn": "1807",
   "abstract": [
    "In this paper a formerly proposed continuous digit recognition system based on Reservoir Computing (RC) is improved in two respects: (1) the single reservoir is substituted by a stack of reservoirs, and (2) the straightforward mapping of reservoir outputs to state likelihoods is replaced by a trained non-parametric mapping. Furthermore, it is shown that a reservoir-based method can improve a model trained on clean speech to work better in a noisy condition from which it has a number of unknown digit string recordings available. The first two improvements have lead to a system that outperforms a HMM-based system with the same noise robust features as input. The model adaptation offers a significant supplementary gain when the noise level is not too high.\n",
    "Index Terms: Reservoir Computing, Acoustic Modeling, Model Adaptation, Noise Robustness\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-491"
  },
  "pylkkonen12b_interspeech": {
   "authors": [
    [
     "Janne",
     "Pylkkönen"
    ],
    [
     "Mikko",
     "Kurimo"
    ]
   ],
   "title": "Optimization-based control for the extended baum-welch algorithm",
   "original": "i12_1808",
   "page_count": 4,
   "order": 495,
   "p1": "1808",
   "pn": "1811",
   "abstract": [
    "The extended Baum-Welch (EBW) is the most popular algorithm for discriminative training of speech recognition acoustic models. The EBW algorithm is usually controlled with heuristic rules, which are used to determine the smoothing parameters of the algorithm. In this paper we propose a control method for EBW which is based on optimization of an error measure over a small control set. The large vocabulary speech recognition experiments show this to have clear benefits over the heuristic methods.\n",
    "Index Terms: speech recognition, discriminative training, extended Baum-Welch, optimization\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-492"
  },
  "schadler12_interspeech": {
   "authors": [
    [
     "Marc René",
     "Schädler"
    ],
    [
     "Birger",
     "Kollmeier"
    ]
   ],
   "title": "Normalization of spectro-temporal Gabor filter bank features for improved robust automatic speech recognition systems",
   "original": "i12_1812",
   "page_count": 4,
   "order": 496,
   "p1": "1812",
   "pn": "1815",
   "abstract": [
    "Physiologically motivated feature extraction methods based on 2D-Gabor filters have already been used successfully in robust automatic speech recognition (ASR) systems. Recently it was shown that a Mel Frequency Cepstral Coefficients (MFCC) baseline can be improved with physiologically motivated features extracted by a 2D-Gabor filter bank (GBFB). Besides physiologically inspired approaches to improve ASR systems technical ones, such as mean and variance normalization (MVN) or histogram equalization (HEQ), exist which aim to reduce undesired information from the speech representation by normalization. In this study we combine the physiologically inspired GBFB features with MVN and HEQ in comparison to MFCC features. Additionaly, MVN is applied at different stages of MFCC feature extraction in order to evaluate its effect to spectral, temporal or spectro-temporal patterns. We find that MVN/HEQ dramatically improve the robustness of MFCC and GBFB features on the Aurora~2 ASR task. While normalized MFCCs perform best with clean condition training, normalized GBFBs improve the ETSI MFCCs features with multi-condition training by 48%, outperforming the ETSI advanced front-end (AFE). The MVN, which may be interpreted as a normalization of modulation depth works best when applied to spectro-temporal patterns. HEQ was not found to perform better than MVN.\n",
    "Index Terms: robust ASR, physiological Gabor filter bank features, modulation depth, normalization\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-493"
  },
  "li12g_interspeech": {
   "authors": [
    [
     "Feipeng",
     "Li"
    ],
    [
     "Sri Harish",
     "Mallidi"
    ],
    [
     "Hynek",
     "Hermansky"
    ]
   ],
   "title": "Phone recognition in critical bands using sub-band temporal modulations",
   "original": "i12_1816",
   "page_count": 4,
   "order": 497,
   "p1": "1816",
   "pn": "1819",
   "abstract": [
    "Researches on human speech perception indicate that temporal envelopes of speech signal are the main carrier of linguistic information. In automatic speech recognition (ASR), the long-term temporal envelopes of subband signals are replaced with short-time spectral envelopes to characterize the linguistic information in speech signal. Past studies have repeatedly shown that temporal fluctuation of spectral trajectory beyond the range of [1, 12]Hz can be harmful to speech recognition. This study investigates the significance of temporal modulation for phoneme identification in machine system. Both long-term temporal envelopes and short-term spectral envelopes are used as the front-end features. Results indicate that temporal modulations above 16 Hz have significant contribution to phoneme identification in clean and noisy conditions, in long-term analysis case. Whereas in short-term analysis case, modulations above 16 Hz are not robust.\n",
    "Index Terms: multistream, temporal modulations, phone recognition\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-494"
  },
  "rasipuram12_interspeech": {
   "authors": [
    [
     "Ramya",
     "Rasipuram"
    ],
    [
     "Mathew M.",
     "Doss"
    ]
   ],
   "title": "Combining acoustic data driven G2p and letter-to-sound rules for under resource lexicon generation",
   "original": "i12_1820",
   "page_count": 4,
   "order": 498,
   "p1": "1820",
   "pn": "1823",
   "abstract": [
    "In a recent work, we proposed an acoustic data-driven grapheme-to-phoneme (G2P) conversion approach, where the probabilistic relationship between graphemes and phonemes learned through acoustic data is used along with the orthographic transcription of words to infer the phoneme sequence. In this paper, we extend our studies to under-resourced lexicon development problem. More precisely, given a small amount of transcribed speech data consisting of few words along with its pronunciation lexicon, the goal is to build a pronunciation lexicon for unseen words. In this framework, we compare our G2P approach with standard letter-to-sound (L2S) rule based conversion approach. We evaluated the generated lexicons on PhoneBook 600 words task in terms of pronunciation errors and ASR performance. The G2P approach yields a best ASR performance of 14.0% word error rate (WER), while L2S approach yields a best ASR performance of 13.7% WER. A combination of G2P approach and L2S approach yields a best ASR performance of 9.3% WER.\n",
    "Index Terms: Kullback-Leibler divergence based HMM, Lex- icon, grapheme, phoneme, grapheme-to-phoneme converter, letter-to-sound rules, multilayer perceptron.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-495"
  },
  "alshareef12_interspeech": {
   "authors": [
    [
     "Sarah",
     "Al-Shareef"
    ],
    [
     "Thomas",
     "Hain"
    ]
   ],
   "title": "CRF-based diacritisation of colloquial Arabic for automatic speech recognition",
   "original": "i12_1824",
   "page_count": 4,
   "order": 499,
   "p1": "1824",
   "pn": "1827",
   "abstract": [
    "Most of the available resources of colloquial Arabic speech are transcribed without diacritics. Those diacritics provide short vowels and other pronunciation information and by omitting them a considerable amount of ambiguity is introduced. In this paper, we propose the use of an automatic diacritisation method as front-end for training of automatic speech recognition systems of colloquial Arabic. The system used is based on conditional random fields that are trained on speaker and contextual information. This method outperforms other reported methods in diacritisation colloquial Arabic by 13.2% relative. The empirical experiments show that applying this method on acoustic model training transcriptions and assisting it with a normalised unigram pronunciation probabilities improves the recognition performance.\n",
    "Index Terms: conditional random fields, automatic speech recognition, automatic diacritisation, conversational speech recognition, colloquial Arabic\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-496"
  },
  "ganapathy12_interspeech": {
   "authors": [
    [
     "Sriram",
     "Ganapathy"
    ],
    [
     "Hynek",
     "Hermansky"
    ]
   ],
   "title": "Analysis of temporal resolution in frequency domain linear prediction",
   "original": "i12_1828",
   "page_count": 4,
   "order": 500,
   "p1": "1828",
   "pn": "1831",
   "abstract": [
    "Frequency domain linear prediction (FDLP) is a technique for auto-regressive (AR) modeling of Hilbert envelopes of the signal. The model is derived by the application of linear prediction on the discrete cosine transform (DCT) of the signal. We analyze resolution properties of the FDLP model using synthetic signals with peaks that are closely spaced in time. The temporal resolution of the FDLP model is defined as the inverse of a critical time span, which is the duration between two temporal peaks in the signal below which the resulting peaks of the AR model cannot be resolved. We study several factors that affect this resolution, such as the location of the input peaks within the analysis segment, type of window applied in the DCT of the signal, and order of the FDLP model. The results of this analysis suggest ways to improve the performance of the FDLP analysis on phoneme recognition of speech. The improved FDLP features outperform MFCC features in both the clean and the noisy conditions.\n",
    "Index Terms: Frequency Domain Linear Prediction, Resolution Analysis, Feature Extraction, Phoneme Recognition\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-497"
  },
  "zhang12e_interspeech": {
   "authors": [
    [
     "Bing",
     "Zhang"
    ],
    [
     "Richard",
     "Schwartz"
    ],
    [
     "Stavros",
     "Tsakalidis"
    ],
    [
     "Long",
     "Nguyen"
    ],
    [
     "Spyros",
     "Matsoukas"
    ]
   ],
   "title": "White listing and score normalization for keyword spotting of noisy speech",
   "original": "i12_1832",
   "page_count": 4,
   "order": 501,
   "p1": "1832",
   "pn": "1835",
   "abstract": [
    "We present a method that avoids the problem of a large vocabulary recognition system missing keywords due to pruning errors or degraded speech. The method, called white listing, assures that all tokens of all of the keywords are found by the recognizer, albeit with a low score. We show that this method far outperforms methods that attempt to increase recall by using subword models. In addition, we introduce a simple score normalization technique based on mapping the decoding score for a keyword to the probability of false alarm for that keyword. This method has the advantage that it can be estimated for all keywords with reliability, even though there might not be any examples of those keywords in the training or tuning set. This makes the scores of all keywords consistent at all ranges, which allows us to use a single consistent score for all keywords. We show that this method reduces the average miss rate by about a factor of 2 for the same false alarm rate. The method can also be used for combining multiple keyword spotting systems.\n",
    "Index Terms: keyword search, noise robustness, white list, score normalization\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-498"
  },
  "diehl12_interspeech": {
   "authors": [
    [
     "Frank",
     "Diehl"
    ],
    [
     "Phillip C.",
     "Woodland"
    ]
   ],
   "title": "Complementary phone error training",
   "original": "i12_2610",
   "page_count": 4,
   "order": 502,
   "p1": "2610",
   "pn": "2613",
   "abstract": [
    "This paper introduces a novel method for the training of a complementary acoustic model with respect to set of given acoustic models. The method is based upon an extension of the Minimum Phone Error (MPE) criterion and aims at producing a model that makes complementary phone errors to those already trained. The technique is therefore called Complementary Phone Error (CPE) training. The method is evaluated using an Arabic large vocabulary continuous speech recognition task. Reductions in word error rate (WER) after combination with a CPE-trained system were obtained with up to 0.7% absolute for a system trained on 172 hours of acoustic data and up to 0.2% absolute for the final system trained on nearly 2000 hours of Arabic data.\n",
    "Index Terms: Speech recognition, acoustic model training, discriminant training, complementary models, system combination\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-499"
  },
  "nussbaumthom12_interspeech": {
   "authors": [
    [
     "Markus",
     "Nussbaum-Thom"
    ],
    [
     "Zoltan",
     "Tuske"
    ],
    [
     "Georg",
     "Heigold"
    ],
    [
     "Ralf",
     "Schlüter"
    ],
    [
     "Hermann",
     "Ney"
    ]
   ],
   "title": "Posterior-scaled MPE: novel discriminative training criteria",
   "original": "i12_2614",
   "page_count": 4,
   "order": 503,
   "p1": "2614",
   "pn": "2617",
   "abstract": [
    "We recently discovered novel discriminative training criteria following a principled approach. In this approach training criteria are developed from error bounds on the classification error for pattern classification tasks that depend on non-trivial loss functions. Automatic speech recognition (ASR) is a prominent example for such a task depending on the non-trivial Levenshtein loss. In this context, the posterior-scaled Minimum Phoneme Error (MPE) training criterion, which is the state-of-the-art discriminative training criterion in ASR, was shown to be an approximation to one of the novel criteria.   Here, we describe the implementation of the posterior-scaled MPE criterion in a transducer-based framework, and compare this criterion to other discriminative training criteria on an ASR task. This comparison indicates that the posteriorscaled MPE criterion performs better than other discriminative criteria including MPE.\n",
    "Index Terms: error bounds, discriminative training criteria, margin, MPE\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-500"
  },
  "ding12b_interspeech": {
   "authors": [
    [
     "Pei",
     "Ding"
    ],
    [
     "Liqiang",
     "He"
    ]
   ],
   "title": "Improve the implementation of pitch features for Mandarin digit string recognition task",
   "original": "i12_2618",
   "page_count": 4,
   "order": 504,
   "p1": "2618",
   "pn": "2621",
   "abstract": [
    "Mandarin digit string recognition (MDSR) is a difficult task in the field of automatic speech recognition (ASR) and using pitch feature can significantly increase the performance. In conventional methods of pitch feature extraction, random value is commonly used as pitch output in unvoiced (UV) frames, which causes serious statistical confusion between voiced (V) and UV units and incurs abnormal likelihood in decoding. In this paper we propose to normalize the distribution of random values assigned in UV frames to avoid the above side-effects and introduce extra discrimination information in statistics. Besides, voice-level (VL), which is an intermedial parameter used in pitch estimation for V/UV decision, is adopted to expand the acoustic feature stream. VL features indicate the intensity of periodicity of speech frames and provide complementary information for ASR. In the experiments the proposed methods significantly improve the accuracy of MDSR tasks and achieve the sentence error reduction rate (ERR) of 13.3% and 15.1% versus the baseline in the evaluation on free-length and 6-digit testing set, respectively.\n",
    "Index Terms: Mandarin digit string recognition, automatic speech recognition, pitch feature extraction\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-501"
  },
  "hsieh12b_interspeech": {
   "authors": [
    [
     "Hsin-Ju",
     "Hsieh"
    ],
    [
     "Jeih-weih",
     "Hung"
    ],
    [
     "Berlin",
     "Chen"
    ]
   ],
   "title": "Exploring joint equalization of spatial-temporal contextual statistics of speech features for robust speech recognition",
   "original": "i12_2622",
   "page_count": 4,
   "order": 505,
   "p1": "2622",
   "pn": "2625",
   "abstract": [
    "Histogram equalization (HEQ) of speech features has recently become an active focus of much research in the field of robust speech recognition due to its inherent neat formulation and remarkable performance. Our work in this paper continues this general line of research in two significant aspects. First, a novel framework for joint equalization of spatial-temporal contextual statistics of speech features is proposed. For this idea to work, we leverage simple differencing and averaging operations to render the contextual relationships of feature vector components, not only between different dimensions but also between consecutive speech frames, for speech feature normalization. Second, we exploit a polynomial-fitting scheme to efficiently approximate the inverse of the cumulative density function of training speech, so as to work in conjunction with the presented normalization framework. As such, it provides the advantages of lower storage and time consumption when compared with the conventional HEQ methods. All experiments were carried out on the Aurora-2 database and task. The performance of the methods deduced from our proposed framework was thoroughly tested and verified by comparisons with other popular robustness methods, which suggests the utility of our methods.\n",
    "Index Terms: noise robustness, histogram equalization, feature contextual statistics\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-502"
  },
  "matsuda12_interspeech": {
   "authors": [
    [
     "Shigeki",
     "Matsuda"
    ],
    [
     "Naoya",
     "Ito"
    ],
    [
     "Kosuke",
     "Tsujino"
    ],
    [
     "Hideki",
     "Kashioka"
    ],
    [
     "Shigeki",
     "Sagayama"
    ]
   ],
   "title": "Speaker-dependent voice activity detection robust to background speech noise",
   "original": "i12_2626",
   "page_count": 4,
   "order": 506,
   "p1": "2626",
   "pn": "2629",
   "abstract": [
    "In this paper, we proposed a speaker-dependent VAD algorithm that extract speech period uttered by a target user only. Based on our survey on recognition error of a real speech data collected in \"VoiceTra\" that is a speech-to-speech translation system for smart phones, we found a lot of word insertion errors caused by background speakers' speech. Our VAD that consists of the three GMMs (noise GMM and speech GMM as used in traditional GMM-based VAD, and speaker adapted GMM) can be easily used for speech detection of the target speaker. Experiments using test utterances with background speakers' speech demonstrated that an ASR system using our proposed VAD achieved better ASR performance compared with an ASR system using the conventional VAD.\n",
    "Index Terms: voice activity detection, speech recognition\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-503"
  },
  "gonzalez12b_interspeech": {
   "authors": [
    [
     "Jose A.",
     "González"
    ],
    [
     "Antonio M.",
     "Peinado"
    ],
    [
     "Angel M.",
     "Gómez"
    ],
    [
     "Ning",
     "Ma"
    ]
   ],
   "title": "Log-spectral feature reconstruction based on an occlusion model for noise robust speech recognition",
   "original": "i12_2630",
   "page_count": 4,
   "order": 507,
   "p1": "2630",
   "pn": "2633",
   "abstract": [
    "This paper addresses the problem of feature compensation in the log-spectral domain for speech recognition in noise by means of minimum mean square error (MMSE) estimation assuming an occlusion speech/noise model. Under this model, the usual non-linear mismatch function that represents the speech distortion due to additive noise can be reasonably well approximated by the maximum of the two mixing sources (speech and noise). Using this approximation, we propose to enhance the degraded speech features by means of a novel MMSE estimator. The resulting technique shows clear similarities with soft-mask missing-data (MD) reconstruction, although the experimental results on both Aurora-2 and Aurora-4 databases show the effectiveness of the proposed technique in comparison with MD.\n",
    "Index Terms: Feature compensation, MMSE estimation, Missing data imputation, speech recognition\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-504"
  },
  "abdelaziz12_interspeech": {
   "authors": [
    [
     "Ahmed Hussen",
     "Abdelaziz"
    ],
    [
     "Dorothea",
     "Kolossa"
    ]
   ],
   "title": "Decoding of uncertain features using the posterior distribution of the clean data for robust speech recognition",
   "original": "i12_2634",
   "page_count": 4,
   "order": 508,
   "p1": "2634",
   "pn": "2637",
   "abstract": [
    "The emerging field of uncertainty-of-observation techniques has recently been successful in improving performance of automatic speech recognition in non-stationary noisy environments by considering the preprocessed feature vectors not as deterministic but rather as random variables containing noisy observations of the underlying, hidden, clean speech features. A number of resulting modifications to the speech decoding rule have been proposed in this framework, and two of these rules, uncertainty decoding and modified imputation, are especially straightforward in their implementation and have shown good success in many environments. In the following, we will present a new decoding rule that shares the simplicity of these two strategies, but results in consistently better accuracy over a wide range of non-stationary noise conditions. In addition, we provide a unifying view of these three strategies using a simple Bayesian network, from which insights into the relationships among them are deduced.\n",
    "Index Terms: Robust speech recognition, uncertainty decoding, modified imputation\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-505"
  },
  "ma12_interspeech": {
   "authors": [
    [
     "Ning",
     "Ma"
    ],
    [
     "Jon",
     "Barker"
    ]
   ],
   "title": "Coupling identification and reconstruction of missing features for noise-robust automatic speech recognition",
   "original": "i12_2638",
   "page_count": 4,
   "order": 509,
   "p1": "2638",
   "pn": "2641",
   "abstract": [
    "The standard missing feature imputation approach to noise-robust automatic speech recognition requires that a single foreground/background segmentation mask is identified prior to reconstruction. This paper presents a novel imputation approach which more closely couples the identification and reconstruction of missing features by using a probabilistic framework based on the speech fragment decoding technique. Using fragment decoding, the most joint-likely state sequence and segmentation hypothesis is identified with which the missing data region is imputed. Crucially, however, imputation can exploit the speech state sequence recovered by the fragment decoding. Further, using N-best decodings allows the clean spectrogram to be estimated as a weighted combination of reconstructions which provides some allowance for uncertainty in the estimates. Experiments on the PASCAL CHiME Challenge task show that system performance is highly dependent on the complexity of the speech models used for segmentation and imputation, and by exploiting the temporal constraint of speech the system significantly outperforms those that ignore the constraint.\n",
    "Index Terms: Missing feature reconstruction, noise-robust speech recognition, feature compensation, fragment decoding\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-506"
  },
  "ludusan12_interspeech": {
   "authors": [
    [
     "Bogdan",
     "Ludusan"
    ],
    [
     "Stefan",
     "Ziegler"
    ],
    [
     "Guillaume",
     "Gravier"
    ]
   ],
   "title": "Integrating stress information in large vocabulary continuous speech recognition",
   "original": "i12_2642",
   "page_count": 4,
   "order": 510,
   "p1": "2642",
   "pn": "2645",
   "abstract": [
    "In this paper we propose a novel method for integrating stress information in the decoding step of a speech recognizer. A multiscale rhythm model was used to determine the stress scores for each syllable, which are further used to reinforce paths during search. Two strategies for integrating the stress were employed: the first one reinforces paths through all the syllables with a value proportional to the their stress score, while the second one enhances paths passing only through stressed syllables, but with a constant value. The former strategy slightly outperforms the later, bringing a relative improvement of more than 2% over the baseline. Furthermore, the stress information proved to be a robust feature, by performing well even for foreign-accented speech.\n",
    "Index Terms: speech recognition, stress, rhythm\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-507"
  },
  "chien12b_interspeech": {
   "authors": [
    [
     "Jen-Tzung",
     "Chien"
    ],
    [
     "Cheng-Chun",
     "Chiang"
    ]
   ],
   "title": "Group sparse hidden Markov models for speech recognition",
   "original": "i12_2646",
   "page_count": 4,
   "order": 511,
   "p1": "2646",
   "pn": "2649",
   "abstract": [
    "This paper presents the group sparse hidden Markov models (GS-HMMs) where a sequence of acoustic features is driven by Markov chain and each feature vector is represented by two groups of basis vectors. The group of common bases represents the features across states within a HMM. The group of individual bases compensates the intra-state residual information. Importantly, the sparse prior for sensing weights is controlled by the Laplacian scale mixture (LSM) distribution which is obtained by multiplying Laplacian variable with an inverse Gamma variable. The scale mixture parameter in LSM makes the distribution even sparser. This parameter serves as an automatic relevance determination for selecting relevant bases from two groups. The weights and two sets of bases in GS-HMMs are estimated via Bayesian learning. We apply this framework for acoustic modeling and show the robustness of GS-HMMs for speech recognition in presence of different noises types and SNRs.\n",
    "Index Terms: Bayesian learning, group sparsity, hidden Markov model, speech recognition\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-508"
  },
  "metze12_interspeech": {
   "authors": [
    [
     "Florian",
     "Metze"
    ],
    [
     "Eric",
     "Fosler-Lussier"
    ]
   ],
   "title": "The speech recognition virtual kitchen: an initial prototype",
   "original": "i12_1872",
   "page_count": 2,
   "order": 512,
   "p1": "1872",
   "pn": "1873",
   "abstract": [
    "This paper describes a \"kitchen\" environment to promote community sharing of research techniques, foster innovative experimentation, and provide solid reference systems as a tool for education, research, and evaluation with a focus on, but not re- stricted to, speech and language research. The core of the research infrastructure is the use of virtual machines (VMs) that provide a consistent environment for experimentation. We liken the virtual machines to a \"kitchen\" because they provide the infrastructure into which one can install \"appliances\" (e.g., speech recognition toolkits), \"recipes\" (scripts for creating state-of-the art systems), and \"ingredients\" (language data). In this demo, we show an initial proposal for a community framework based on VM technology used for teaching classes at CMU and OSU.\n",
    "Index Terms: speech recognition, virtualization, educational tools, research infrastructure\n",
    ""
   ]
  },
  "reichel12b_interspeech": {
   "authors": [
    [
     "Uwe D.",
     "Reichel"
    ]
   ],
   "title": "Perma and Balloon: tools for string alignment and text processing",
   "original": "i12_1874",
   "page_count": 4,
   "order": 513,
   "p1": "1874",
   "pn": "1877",
   "abstract": [
    "Two research tools available as webservices are presented in this paper: PermA, a general-purpose string aligner which can for example be used for grapheme-to-phoneme and phoneme-to-phoneme alignment, and Balloon, a text processing toolkit for German and English providing components for part-of-speech tagging, morphological analyses, and grapheme-to-phoneme conversion including syllabification and wordstress assignment. In this paper the general architectures of these tools are introduced with a focus on recent enhancements concerning the alignment cost function derivation and word stress assignment.\n",
    "Index Terms: alignment, grapheme-to-phoneme conversion, part-of-speech tagging, morphology, word-stress assignment, tools\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-509"
  },
  "ouni12_interspeech": {
   "authors": [
    [
     "Slim",
     "Ouni"
    ],
    [
     "Loïc",
     "Mangeonjean"
    ],
    [
     "Ingmar",
     "Steiner"
    ]
   ],
   "title": "Visartico: a visualization tool for articulatory data",
   "original": "i12_1878",
   "page_count": 4,
   "order": 514,
   "p1": "1878",
   "pn": "1881",
   "abstract": [
    "In this paper, we present VisArtico, a visualization tool for articulatory data acquired using the AG500 3D electromagnetic articulograph (EMA). This software allows displaying the positions of the EMA coils that are simultaneously animated with playback of the acoustic speech signal. It is also possible to display contours for the tongue and lips. The software helps to find the midsagittal plane of the speaker and offers databased palate shape discovery. In addition, VisArtico allows labeling the articulatory data into phonetic segments. Our main goal is to provide an efficient, easy-to-use tool to visualize articulatory data for researchers working in the field of speech production.\n",
    "Index Terms: speech production, articulatory data, vocal tract, visualization, electromagnetic articulography\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-510"
  },
  "lenkiewicz12_interspeech": {
   "authors": [
    [
     "Przemyslaw",
     "Lenkiewicz"
    ],
    [
     "Dieter van",
     "Uytvanck"
    ],
    [
     "Peter",
     "Wittenburg"
    ],
    [
     "Sebastian",
     "Drude"
    ]
   ],
   "title": "Towards automated annotation of audio and video recordings by application of advanced web-services",
   "original": "i12_1882",
   "page_count": 4,
   "order": 515,
   "p1": "1882",
   "pn": "1885",
   "abstract": [
    "In this paper we describe audio and video processing algorithms that are developed in the scope of AVATecH project. The purpose of these algorithms is to shorten the time taken by manual annotation of audio and video recordings by extracting features from media files and creating semi-automated annotations. We show that the use of such supporting algorithms can shorten the annotation time to 30-50% of the time necessary to perform a fully manual annotation of the same kind.\n",
    "Index Terms: signal processing, automated annotation\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-511"
  },
  "ashby12_interspeech": {
   "authors": [
    [
     "Simone",
     "Ashby"
    ],
    [
     "Sílvia",
     "Barbosa"
    ],
    [
     "Silvia",
     "Brandão"
    ],
    [
     "José Pedro",
     "Ferreira"
    ],
    [
     "Maarten",
     "Janssen"
    ],
    [
     "Catarina",
     "Silva"
    ],
    [
     "Mário Eduardo",
     "Viaro"
    ]
   ],
   "title": "A rule based pronunciation generator and regional accent databank for Portuguese",
   "original": "i12_1886",
   "page_count": 2,
   "order": 516,
   "p1": "1886",
   "pn": "1887",
   "abstract": [
    "In this paper, we describe the work of the LUPo (Portuguese Unisyn Lexicon) project to model standard and non-standard varieties of spoken Portuguese from around the globe, and: (1) deliver a free, open-source tool for the automatic generation of accent-specific pronunciation lexica within the existing online lexical knowledge base, the Portal da Lingua Portuguesa; and (2) provide the research and speech technology communities with a free, online, searchable database, the Portuguese RADbank, dedicated to the description of regional varieties of spoken Portuguese.\n",
    ""
   ]
  },
  "chappel12_interspeech": {
   "authors": [
    [
     "Roger",
     "Chappel"
    ],
    [
     "Kuldip",
     "Paliwal"
    ]
   ],
   "title": "Speech enhancement for android (SEA): a speech processing demonstration tool for android based smart phones and tablets",
   "original": "i12_1888",
   "page_count": 4,
   "order": 517,
   "p1": "1888",
   "pn": "1891",
   "abstract": [
    "This paper presents a speech processing platform which can be used to demonstrate and investigate speech enhancement methods. This platform is called Speech Enhancement for Android (SEA), and has been developed on the Android operating system, available to students, teaching staff and researches through the Android market. SEA can be used as an additional teaching tool in undergraduate courses as well as a useful tool to aid researches in gaining an intuitive understanding of speech enhancement methods. The focus of this platform is to present advanced speech processing concepts in a quick and interactive way on a personal phone or tablet. This paper outlines the operation of SEA along with how it can be used to engage students and speech processing professionals.\n",
    "Index Terms: Digital signal processing (DSP), analysis–modification–synthesis (AMS), Android operating system\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-512"
  },
  "okamoto12_interspeech": {
   "authors": [
    [
     "Jacob",
     "Okamoto"
    ],
    [
     "Serguei",
     "Pakhomov"
    ],
    [
     "Elizabeth",
     "Shriberg"
    ],
    [
     "Andreas",
     "Stolcke"
    ]
   ],
   "title": "ProTK: an improved prosody toolkit",
   "original": "i12_1892",
   "page_count": 2,
   "order": 518,
   "p1": "1892",
   "pn": "1893",
   "abstract": [
    "We present an improvement to our previous work to create a toolkit for integrating automated speech recognition, prosodic feature analysis, and machine learning to create models for identifying and classifying speech characteristics such as filled pauses. The toolkit provides a modular and extensible platform for intaking, analyzing, and formatting data for use in a wide variety of other tools.\n",
    "Index Terms: speech recognition, machine learning, toolkit, prosody\n",
    ""
   ]
  },
  "boyce12_interspeech": {
   "authors": [
    [
     "Suzanne",
     "Boyce"
    ],
    [
     "Harriet",
     "Fell"
    ],
    [
     "Joel",
     "MacAuslan"
    ]
   ],
   "title": "Speechmark: landmark detection tool for speech analysis",
   "original": "i12_1894",
   "page_count": 4,
   "order": 519,
   "p1": "1894",
   "pn": "1897",
   "abstract": [
    "Landmark-based software tools are particularly suited to fast, automatic analysis of small, non-lexical differences in production of the same speech material by the same speaker. We are building a tool set that provides fast, automatic summary statistics for measures of speech acoustics based on Stevens's paradigm of landmarks, points in an utterance around which information about articulatory events can be extracted. This will be achieved by extending existing shareware software platforms with \"plug-ins\" that perform specific measures and report results to the user.\n",
    "Index Terms: speech production, articulation, landmark, software\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-513"
  },
  "bell12b_interspeech": {
   "authors": [
    [
     "Peter",
     "Bell"
    ],
    [
     "Myroslava",
     "Dzikovska"
    ],
    [
     "Amy",
     "Isard"
    ]
   ],
   "title": "A tutorial dialogue system with unrestricted spoken input",
   "original": "i12_2113",
   "page_count": 2,
   "order": 520,
   "p1": "2113",
   "pn": "2114",
   "abstract": [
    "We present our work in building a spoken language interface for a tutorial dialogue system. Our goal is to allow natural, unrestricted student interaction with the computer tutor, which has been shown to improve the student's learning gain, but presents challenges for speech recognition and spoken language understanding. Here we describe the system design, focusing on the components used for speech recognition.\n",
    "Index Terms: spoken dialogue system, speech recognition, computer tutoring\n",
    ""
   ]
  },
  "sun12d_interspeech": {
   "authors": [
    [
     "Xie",
     "Sun"
    ],
    [
     "Qi Peter",
     "Li"
    ],
    [
     "Manli",
     "Zhu"
    ],
    [
     "Qiru",
     "Zhou"
    ]
   ],
   "title": "Integrating adaptive beam-forming and auditory features for robust large vocabulary speech recognition",
   "original": "i12_2115",
   "page_count": 2,
   "order": 521,
   "p1": "2115",
   "pn": "2116",
   "abstract": [
    "We demonstrate a system to integrate adaptive beam-forming and auditory features in order to improve speech recognition accuracy in noisy environments. Adaptive beam-forming based on a microphone array can utilize spatial information to improve the sound recording signal-to-noise ratio (SNR) on a focused speaker for robust speech recognition. Auditory features based on modeling the signal processing functions in the hearing system have shown to largely improve speech recognition accuracy under noisy conditions. According to our experiments, when both adaptive beam-forming and the auditory features are integrated, an absolute gain of more than 50% over a baseline on speech recognition accuracy is achieved when 5dB white noise is added.\n",
    "Index Terms: adaptive beam-forming, auditory features, robust speech recognition, SNR\n",
    ""
   ]
  },
  "hofmann12_interspeech": {
   "authors": [
    [
     "Hansjörg",
     "Hofmann"
    ],
    [
     "Ute",
     "Ehrlich"
    ],
    [
     "Klaus",
     "Bader"
    ],
    [
     "Ilona",
     "Nothelfer"
    ],
    [
     "André",
     "Berton"
    ]
   ],
   "title": "A natural in-car speech interface to internet services using hybrid ASR",
   "original": "i12_2117",
   "page_count": 2,
   "order": 522,
   "p1": "2117",
   "pn": "2118",
   "abstract": [
    "The manual use of smartphone's Internet access while driving endangers the drivers safety. Therefore, an intuitive in-car speech interface to the Internet is crucial. In this paper we present an in-car speech dialog system prototype which allows the user to access several Internet services by speech. In order to reduce driver distraction the system is controlled by natural speech and the visual feedback is very limited. The speech dialog system is based on hybrid automatic speech recognition technologies used for different types of recognition tasks.\n",
    "Index Terms: Speech dialog system, hybrid automatic speech recognition, Internet services\n",
    ""
   ]
  },
  "cole12_interspeech": {
   "authors": [
    [
     "Ronald A.",
     "Cole"
    ],
    [
     "Daniel",
     "Bolanos"
    ],
    [
     "Wayne H.",
     "Ward"
    ],
    [
     "J. T.",
     "Carmer"
    ],
    [
     "Eric",
     "Borts"
    ],
    [
     "Edward",
     "Svirsky"
    ]
   ],
   "title": "How marni helps English language learners acquire oral reading fluency",
   "original": "i12_2119",
   "page_count": 2,
   "order": 523,
   "p1": "2119",
   "pn": "2120",
   "abstract": [
    "We will demonstrate Interactive Books that children English can use independently to learn to read English texts fluently and expressively. The books feature Marni, a bilingual virtual reading teacher. Marni first familiarizes children with text by narrating it in the child's first language, (currently, either Chinese or Spanish), followed by narration and read along activities in English. Following the familiarization activities, students attempt to read the text by themselves, but are able to click on words to hear Marni say them. The system then provides the child with feedback on their overall fluency score in terms of accuracy, reading rate and how expressively they read the text, and highlights all words in the text scored as errors. The student then practices listening to and watching Marni pronounce words classified as reading errors, and can elect to record and listen to their own production of the word and compare it to Marni's. The child then listens to Marni produce the sentence, records themselves saying the sentence, and compare their production to the model produced by Marni. This process.independent reading, feedback on oral reading fluency, and practice listening to, reading and saying words and sentences, proceeds through two to three repeated readings.\n",
    ""
   ]
  },
  "finomorejr12_interspeech": {
   "authors": [
    [
     "Victor",
     "Finomore Jr"
    ],
    [
     "John",
     "Stewart"
    ],
    [
     "Rita",
     "Singh"
    ],
    [
     "Bhiksha",
     "Raj"
    ],
    [
     "Ron",
     "Dallman"
    ]
   ],
   "title": "Demonstration of advanced multi-modal, network-centric communication management suite",
   "original": "i12_2121",
   "page_count": 2,
   "order": 524,
   "p1": "2121",
   "pn": "2122",
   "abstract": [
    "The Multi Modal Communication tool was developed to improve mission effectiveness for Command and Control operators. This communication management suite combines mature communication technology to provide the operator with an intuitive display that allows on-demand access to mission critical communication data.\n",
    "Index Terms: Multi-Modal Display, Communication Interface Automatic Speech Recognition\n",
    ""
   ]
  },
  "pelemans12_interspeech": {
   "authors": [
    [
     "Joris",
     "Pelemans"
    ],
    [
     "Kris",
     "Demuynck"
    ],
    [
     "Patrick",
     "Wambacq"
    ]
   ],
   "title": "Dutch automatic speech recognition on the web: towards a general purpose system",
   "original": "i12_2123",
   "page_count": 4,
   "order": 525,
   "p1": "2123",
   "pn": "2126",
   "abstract": [
    "In this paper we present our state-of-the art automatic speech recognition system for Dutch that we made available on the web. The free, online disclosure of our software aims at allowing non-specialists to adopt ASR technology effortlessly. Access is possible via a standard web browser or as a web service in automated tools. We discuss the way the web application was built and focus on usability criteria - especially interoperability. To overcome user differences we provide input conversion and basic parameter selection. Extensions of the current system and a path to robust, general purpose ASR are suggested.\n",
    "Index Terms: robust speech recognition, web service, automatic model selection, speech processing tools\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-514"
  },
  "tejedor12_interspeech": {
   "authors": [
    [
     "Javier",
     "Tejedor"
    ],
    [
     "Fernando",
     "López-Colino"
    ],
    [
     "Jordi",
     "Porta"
    ],
    [
     "José",
     "Colás"
    ]
   ],
   "title": "An on-line, cloud-based Spanish-Spanish sign language translation system",
   "original": "i12_2127",
   "page_count": 2,
   "order": 526,
   "p1": "2127",
   "pn": "2128",
   "abstract": [
    "An on-line Spanish-Spanish Sign Language (LSE) translation system is presented in which Spanish speech content is translated into LSE to provide Spanish deaf people access to the speech information. It is cloud-based, built over a speech recognition module, a transfer-based machine translation module and a Sign Language synthesis module that employs an avatar-like technology to present the signed content.\n",
    "Index Terms: Spanish Sign Language, machine translation, speech recognition, deaf people\n",
    ""
   ]
  },
  "he12b_interspeech": {
   "authors": [
    [
     "Yanzhang",
     "He"
    ],
    [
     "Eric",
     "Fosler-Lussier"
    ]
   ],
   "title": "Efficient segmental conditional random fields for one-pass phone recognition",
   "original": "i12_1898",
   "page_count": 4,
   "order": 527,
   "p1": "1898",
   "pn": "1901",
   "abstract": [
    "Segmental models have been shown effective on speech recognition recently. However, a first-pass baseline system such as HMMs is required to provide a constrained set of candidate segmentations and label sequences for most segmental models to make inference on. This paper explores one-pass segmental models based on continuous feature space for phone recognition and make the first direct comparison between a frame-based system and segmental system using the same base features. We also show that transition features can be very beneficial for segmental models, particularly the ones surrounding the segment boundaries. In order to efficiently incorporate such features, we propose the Boundary-Factored SCRF, which reduces the time complexity of a SCRF to that of a frame-level CRF.\n",
    "Index Terms: Segmental Conditional Random Fields, Phone Recognition\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-515"
  },
  "nallasamy12_interspeech": {
   "authors": [
    [
     "Udhyakumar",
     "Nallasamy"
    ],
    [
     "Florian",
     "Metze"
    ],
    [
     "Tanja",
     "Schultz"
    ]
   ],
   "title": "Enhanced polyphone decision tree adaptation for accented speech recognition",
   "original": "i12_1902",
   "page_count": 4,
   "order": 528,
   "p1": "1902",
   "pn": "1905",
   "abstract": [
    "State-of-the-art Automatic Speech Recognition (ASR) models struggle to handle accented speech, particularly if the target accent is under-represented in the training data. The acoustic variations presented by an unfamiliar accent, render the ASR polyphone decision tree (PDT) and its associated Gaussian mixture models (GMM) misfit to the test data. In this paper, we improve on the previous work of adapting the polyphone decision tree, using a semi-continuous model based approach to address the problem of data sparsity. We extend the existing PDT to introduce additional states with shared parameters, corresponding to the new contextual variations identified in the adaptation data, while still robustly estimating the state based parameters on a small adaptation set. We conduct ASR experiments on Arabic and English accents and show that our technique performs better than Maximum A-Posteriori (MAP) adaptation and a previous implementation of polyphone decision tree specialization (PDTS). Compared to MAP adaptation, we obtain 7% relative improvement for Dialectal Arabic and 13.8% relative improvement for Accented English.\n",
    "Index Terms: automatic speech recognition, accent adaptation\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-516"
  },
  "li12h_interspeech": {
   "authors": [
    [
     "Jinyu",
     "Li"
    ],
    [
     "Michael L.",
     "Seltzer"
    ],
    [
     "Yifan",
     "Gong"
    ]
   ],
   "title": "Efficient VTS adaptation using jacobian approximation",
   "original": "i12_1906",
   "page_count": 4,
   "order": 529,
   "p1": "1906",
   "pn": "1909",
   "abstract": [
    "By explicitly modeling the distortion sources of speech signals, model adaptation based on vector Taylor series (VTS) approaches have been shown to significantly improve the robustness of speech recognizers to environmental noise. However, the computational cost of VTS model adaptation (MVTS) methods hinders them from being widely used. In this paper, we propose to reduce the computation cost of standard MVTS by approximating the Jacobian matrix as a diagonal one (DJ-MVTS). We verified this approximation by showing that the diagonal elements of Jacobian matrixes provide dominant information and the model distortion introduced by this approximation is very small. DJ-MVTS gives similar accuracy as the standard MVTS method with significant computation cost reduction. With the setup in this paper, the proposed DJ-MVTS method achieves higher accuracy with lower computation cost than a featured-based VTS method.\n",
    "Index Terms: vector Taylor series, Jacobian matrix, robust ASR\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-517"
  },
  "cernak12_interspeech": {
   "authors": [
    [
     "Miloš",
     "Cerňak"
    ],
    [
     "David",
     "Imseng"
    ],
    [
     "Hervé",
     "Bourlard"
    ]
   ],
   "title": "Robust triphone mapping for acoustic modeling",
   "original": "i12_1910",
   "page_count": 4,
   "order": 530,
   "p1": "1910",
   "pn": "1913",
   "abstract": [
    "In this paper we revisit the recently proposed triphone mapping as an alternative to decision tree state clustering. We generalize triphone mapping to Kullback-Leibler based hidden Markov models for acoustic modeling and propose a modified training procedure for the Gaussian mixture model based acoustic modeling. We compare the triphone mapping to decision tree state clustering on the Wall Street journal task as well as in the context of an under-resourced language by using Greek data from the SpeechDat(II) corpus. Experiments reveal that triphone mapping has the best overall performance and is robust against varying the acoustic modeling technique as well as variable amounts of training data.\n",
    "Index Terms: Speech recognition, acoustic modeling, triphone mapping, Kullback-Leibler divergence\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-518"
  },
  "zhang12f_interspeech": {
   "authors": [
    [
     "Weibin",
     "Zhang"
    ],
    [
     "Pascale",
     "Fung"
    ]
   ],
   "title": "Sparse banded precision matrices for low resource speech recognition",
   "original": "i12_1914",
   "page_count": 4,
   "order": 531,
   "p1": "1914",
   "pn": "1917",
   "abstract": [
    "We propose to use sparse banded precision matrices for speech recognition when there is insufficient training data. Previously we proposed a method to drive the structure of precision matrices to sparse under the HMM framework during training. The recognition accuracy of this compact model is shown to be better than full covariance or diagonal covariance systems. In this paper we propose to modify the penalization to automatically learn sparse banded precision matrices. This will drive the models trained even more compact. We demonstrate the importance of the order of features to the success of our proposed method. Using our proposed feature order, we can substantially reduce the right halfbandwidth of the sparse banded matrices without sacrificing the recognition accuracy. This saves memory and computation.\n",
    "Index Terms: low resource speech recognition, sparse precision matrix, sparse banded precision matrix\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-519"
  },
  "mohammed12_interspeech": {
   "authors": [
    [
     "Abdul Waheed",
     "Mohammed"
    ],
    [
     "Marco",
     "Matassoni"
    ],
    [
     "Harikrishna",
     "Maganti"
    ],
    [
     "Maurizio",
     "Omologo"
    ]
   ],
   "title": "Semi-blind model adaptation using piece-wise energy decay curve for large reverberant environments",
   "original": "i12_1918",
   "page_count": 4,
   "order": 532,
   "p1": "1918",
   "pn": "1921",
   "abstract": [
    "This work presents semi-blind acoustic model adaptation based on a piece-wise energy decay curve. The dual slope representation of the piecewise curve accurately captures the early and late reflection decay that helps in precisely modeling the smearing effect caused due to reverberation. The slopes are estimated in a semi-blind fashion, late reflection slope is estimated blindly by finding the highest likelihood obtained after matching the test features with Gaussian mixture models trained on reverberant data, while the early reflection slope is empirically computed. Adaptation using piece-wise decay curve leads to robust acoustic models consequently improving the recognition performance. The approach is tested on connected digits recognition task in a large room with various reverberation times. The performance is compared with the exponential decay approach and incremental MLLR, where the proposed approach is found to provide robust and consistent gains across all the cases.\n",
    "Index Terms: reverberation, acoustic model adaptation, GMM\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-520"
  },
  "bispo12_interspeech": {
   "authors": [
    [
     "Bruno C.",
     "Bispo"
    ],
    [
     "Diamantino S.",
     "Freitas"
    ]
   ],
   "title": "Developments of a hybrid pre-processor based on frequency shifting for stereophonic acoustic echo cancellation",
   "original": "i12_1922",
   "page_count": 4,
   "order": 533,
   "p1": "1922",
   "pn": "1925",
   "abstract": [
    "In a multi-channel hands-free communication, decorrelation algorithms are necessary, along with adaptive filters, to efficiently remove the acoustic echo in real time applications. But the quality of the signals and the spatial position of the sound source must not be perceptually affected by those algorithms. This paper proposes a new hybrid solution that uses frequency shifts to improve the performance of a state-ofart solution based on addition of half-wave rectified signals. The results show that the new method achieves a significant improvement in the identification process of the real echo paths as well as in the global perceptual quality of the processed signals.\n",
    "Index Terms: stereophonic acoustic echo cancellation, frequency shifting, speech decorrelation\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-521"
  },
  "kinoshita12_interspeech": {
   "authors": [
    [
     "Keisuke",
     "Kinoshita"
    ],
    [
     "Marc",
     "Delcroix"
    ],
    [
     "Mehrez",
     "Souden"
    ],
    [
     "Tomohiro",
     "Nakatani"
    ]
   ],
   "title": "Example-based speech enhancement with joint of spatial, spectral & temporal cues of speech and noise",
   "original": "i12_1926",
   "page_count": 4,
   "order": 534,
   "p1": "1926",
   "pn": "1929",
   "abstract": [
    "This paper proposes a multichannel speech enhancement technique that leverages three essential cues embedded in the observed signal, i.e., spatial, spectral and temporal cues, for differentiating underlying clean speech component from noise. The proposed method estimates clean speech and noise features in a single optimization criterion by integrating two approaches, namely, example- and model-based multichannel speech enhancement approaches: The former utilizes spectral and temporal cues, while the latter spatial and spectral cues. In the experiment, we show the superiority of the proposed method over the conventional methods in terms of the automatic keyword recognition performance in adverse and highly non-stationary noisy environment.\n",
    "Index Terms: example-based speech enhancement, model-based approach, speech recognition, blind source separation\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-522"
  },
  "zhao12b_interspeech": {
   "authors": [
    [
     "Shengkui",
     "Zhao"
    ],
    [
     "Douglas L.",
     "Jones"
    ]
   ],
   "title": "A fast-converging adaptive frequency-domain MVDR beamformer for speech enhancement",
   "original": "i12_1930",
   "page_count": 4,
   "order": 535,
   "p1": "1930",
   "pn": "1933",
   "abstract": [
    "In this paper, we present a fast-converging adaptive frequency-domain minimum-variance-distortionless-response (MVDR) beamformer (FMV) for speech enhancement. The well-known FMV solution is optimum in the microphone array processing. However, the direct computation of the optimum FMV solution is often undesirable due to the the inversion of the spatio-spectral correlation matrix which is often unstable and is expensive for large arrays. To avoid the matrix inversion, we develop a fast-converging conjugate gradient (CG) algorithm for iteratively computing the FMV solution. Compared to the existing steepest descent (SD) algorithm, the CG algorithm can dramatically improve the convergence speed for the case of multiple interfering signals in speech enhancement. Therefore, the computational load and processing time can be significantly reduced. The speech enhancement experiments using a four-channel acoustic-vector-sensor (AVS) microphone array are demonstrated for the target speech signal corrupted by two and five interfering speech signals and superior performance are achieved.\n",
    "Index Terms: speech enhancement, microphone arrays, correlation, convergence, adaptive signal processing\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-523"
  },
  "singh12_interspeech": {
   "authors": [
    [
     "Rita",
     "Singh"
    ],
    [
     "Kenichi",
     "Kumatani"
    ],
    [
     "John",
     "McDonough"
    ],
    [
     "Chen",
     "Liu"
    ]
   ],
   "title": "A signal-separation-based array postfilter for distant speech recognition",
   "original": "i12_1934",
   "page_count": 4,
   "order": 536,
   "p1": "1934",
   "pn": "1937",
   "abstract": [
    "In standard microphone array processing for distant speech recognition, the beamformed output is postfiltered to reduce residual noise. Postfiltering is usually performed through a weiner filter whose parameters are estimated from both the beamformer output and the signals captured at the microphones themselves. Conventional postfiltering methods assume diffuse or incoherent noise at the various microphones in order to estimate these parameters. When the noise does not conform to this assumption they perform poorly. We propose an alternate postfiltering mechanism that attenuates noise by estimating and separating out the contributions of speech and noise explicitly. Experiments on a corpus of in-car two-channel recordings show that the proposed postfiltering algorithm outperforms conventional postfilters significantly under many noise conditions.\n",
    "Index Terms: Microphone arrays, postfiltering, beamforming, com- positional models, signal separation\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-524"
  },
  "yu12c_interspeech": {
   "authors": [
    [
     "Meng",
     "Yu"
    ],
    [
     "Frank K.",
     "Soong"
    ]
   ],
   "title": "Constrained multichannel speech dereverberation",
   "original": "i12_1938",
   "page_count": 4,
   "order": 537,
   "p1": "1938",
   "pn": "1941",
   "abstract": [
    "We propose a multi-channel speech dereverberation approach based on cross-channel cancellation and spectrogram decomposition. The reverberation is modeled as a convolution operation in the spectrogram magnitude domain. Using the Itakura divergence (I-divergence), we decompose reverberant spectrogram into clean spectrogram convolved with a deconvolution filter. The speech spectrogram is constrained and regularized by non-negativity and sparsity, respectively, while the deconvolution filter is constrained by non-negativity and cross-channel cancellation. Spectrogram decomposition of individual channels and cross-channel cancellation are jointly optimized by a multiplicative algorithm to achieve multi-channel speech dereverberation. We confirm through experiment that the proposed multi-channel method outperforms the related other single/multi-channel methods.\n",
    "Index Terms: Multichannel dereverberation, Spectral decomposition, I-divergence, Sparsity, Cross-channel cancellation\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-525"
  },
  "ritch12_interspeech": {
   "authors": [
    [
     "Ryan",
     "Ritch"
    ],
    [
     "Meng",
     "Yu"
    ],
    [
     "Jack",
     "Xin"
    ]
   ],
   "title": "A triple-microphone real-time speech enhancement algorithm based on approximate array analytical solutions",
   "original": "i12_1942",
   "page_count": 4,
   "order": 538,
   "p1": "1942",
   "pn": "1945",
   "abstract": [
    "A novel triple microphone array speech enhancement algorithm is developed based on closed form solutions to an approximate algebraic system of equations on noise directivity and speech spectral energy. The system is derived from the ensemble averaged spectral energy equations of a first order differential microphone array. For sufficiently small microphone spacing, closed form analytical solutions are derived using suitable approximations for the normal speech frequency range. The resulting algorithm is simple to implement and efficient for real time noise reduction in a reverberant environment. The algorithm's limitation, the case of spatially almost overlapping sources, is analyzed mathematically, leading to an effective alternative solution utilizing array rotation. The algorithm is evaluated and compared with well-established beamforming algorithms. Results indicate robust gains in objective measures of speech relative to the baseline algorithms in the presence of multiple/mobile noise sources.\n",
    "Index Terms: triple-microphone array, spectrum estimation, noise directivity, closed form solutions, real-time algorithm\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-526"
  },
  "ng12b_interspeech": {
   "authors": [
    [
     "Tim",
     "Ng"
    ],
    [
     "Bing",
     "Zhang"
    ],
    [
     "Long",
     "Nguyen"
    ],
    [
     "Spyros",
     "Matsoukas"
    ],
    [
     "Xinhui",
     "Zhou"
    ],
    [
     "Nima",
     "Mesgarani"
    ],
    [
     "Karel",
     "Veselý"
    ],
    [
     "Pavel",
     "Matějka"
    ]
   ],
   "title": "Developing a speech activity detection system for the DARPA RATS program",
   "original": "i12_1969",
   "page_count": 4,
   "order": 539,
   "p1": "1969",
   "pn": "1972",
   "abstract": [
    "This paper describes the speech activity detection (SAD) system developed by the Patrol team for the first phase of the DARPA RATS (Robust Automatic Transcription of Speech) program, which seeks to advance state of the art detection capabilities on audio from highly degraded communication channels. We present two approaches to SAD, one based on Gaussian mixture models, and one based on multi-layer perceptrons. We show that significant gains in SAD accuracy can be obtained by careful design of acoustic front end, feature normalization, incorporation of long span features via data-driven dimensionality reducing transforms, and channel dependent modeling. We also present a novel technique for normalizing detection scores from different systems for the purpose of system combination.\n",
    "Index Terms: speech activity detection, noisy speech\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-527"
  },
  "omar12_interspeech": {
   "authors": [
    [
     "Mohamed Kamal",
     "Omar"
    ]
   ],
   "title": "Speech activity detection for noisy data using adaptation techniques",
   "original": "i12_1973",
   "page_count": 4,
   "order": 540,
   "p1": "1973",
   "pn": "1976",
   "abstract": [
    "Automatic detection of speech in audio streams has become an important preprocessing step for speech recognition, speaker recognition, and audio data mining. In many applications, the speech activity detection has to be performed on highly degraded audio streams. We present here our work to address the challenge of speech activity detection for highly degraded channel conditions. We present two two-pass modified cumulative sum (CUSUM) approaches based on maximum a posteriori (MAP) adaptation and regularized feature-based maximum likelihood linear regression (RFMLLR) adaption. In this paper, we compare the two approaches to a single-pass modified CUSUM baseline system with Gaussian mixture models (GMM) of speech and non-speech classes. The systems are evaluated on two test sets. Each consists of data from eight highly degraded channels. Our two-pass MAP adaptation system reduces the total error by 27%-54% relative compared to the single-pass baseline system. We present also experiments showing additional gains of 3%-25% relative by using channel-specific GMM models for speech and non-speech instead of a single channel-indpendent GMM model for each.\n",
    "Index Terms: speech activity detection, adaptation\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-528"
  },
  "misra12_interspeech": {
   "authors": [
    [
     "Ananya",
     "Misra"
    ]
   ],
   "title": "Speech/nonspeech segmentation in web videos",
   "original": "i12_1977",
   "page_count": 4,
   "order": 541,
   "p1": "1977",
   "pn": "1980",
   "abstract": [
    "Speech transcription of web videos requires first detecting segments with transcribable speech. We refer to this as segmentation. Commonly used segmentation techniques are inadequate for domains such as YouTube, where videos may have a large variety of background and recording conditions. In this work, we investigate alternative audio features and a discriminative classifier, which together yield a lower frame error rate (25.3%) on YouTube videos compared to the commonly used Gaussian mixture models trained on cepstral features (30.6%). The alternative audio features perform particularly well in noisy conditions.\n",
    "Index Terms: segmentation, speech detection, voice activity detection, video\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-529"
  },
  "harding12b_interspeech": {
   "authors": [
    [
     "Philip",
     "Harding"
    ],
    [
     "Ben",
     "Milner"
    ]
   ],
   "title": "On the use of machine learning methods for speech and voicing classification",
   "original": "i12_1981",
   "page_count": 4,
   "order": 542,
   "p1": "1981",
   "pn": "1984",
   "abstract": [
    "This work examines the effectiveness of machine learning (ML) classifiers on the problems of voice activity detection and voicing classification. A wide range of ML classifiers are considered and include parametric, probabilistic and non-probabilistic, artificial neural networks and regression. Evaluations are carried out in both stationary and non-stationary noise types at signal-to-noise ratios down to 0dB. In comparison to conventional methods the ML methods are found to be significantly more robust with multilayer perceptrons, Gaussian mixture models and Rotation Forest giving consistently best performance.\n",
    "Index Terms: voice activity detection, mfcc, machine learning\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-530"
  },
  "thomas12b_interspeech": {
   "authors": [
    [
     "Samuel",
     "Thomas"
    ],
    [
     "Sri Harish",
     "Mallidi"
    ],
    [
     "Thomas",
     "Janu"
    ],
    [
     "Hynek",
     "Hermansky"
    ],
    [
     "Nima",
     "Mesgarani"
    ],
    [
     "Xinhui",
     "Zhou"
    ],
    [
     "Shihab",
     "Shamma"
    ],
    [
     "Tim",
     "Ng"
    ],
    [
     "Bing",
     "Zhang"
    ],
    [
     "Long",
     "Nguyen"
    ],
    [
     "Spyros",
     "Matsoukas"
    ]
   ],
   "title": "Acoustic and data-driven features for robust speech activity detection",
   "original": "i12_1985",
   "page_count": 4,
   "order": 543,
   "p1": "1985",
   "pn": "1988",
   "abstract": [
    "In this paper we evaluate different features for speech activity detection (SAD). Several signal processing techniques are used to derive acoustic features that capture attributes of speech useful in differentiating speech segments in noise. The acoustic features include shortterm spectral features, long-term modulation features both derived using Frequency Domain Linear Prediction (FDLP), and joint spectrotemporal features extracted using 2D filters on a cortical representation of speech. Posteriors of speech and non-speech from a trained multi-layer perceptron are also used as data-driven features for this task. These feature extraction techniques form part of an elaborate feature extraction front-end where information spanning several hundreds of milliseconds of the signal are used along with heteroscedastic linear discriminant analysis for dimensionality reduction. Processed feature outputs from the proposed front-end are used to train SAD systems based on Gaussian mixture models for processing of speech from multiple languages transmitted over noisy radio communication channels under the ongoing DARPA Robust Automatic Transcription of Speech (RATS)program. The proposed front-end performs significantly better than standard acoustic feature extraction techniques in these noisy conditions.\n",
    "Index Terms: Speech Activity Detection, Features for SAD\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-531"
  },
  "wang12k_interspeech": {
   "authors": [
    [
     "Shuo",
     "Wang"
    ],
    [
     "Wenjun",
     "Wu"
    ]
   ],
   "title": "A two-step NMF based algorithm for single channel speech separation",
   "original": "i12_1989",
   "page_count": 4,
   "order": 544,
   "p1": "1989",
   "pn": "1992",
   "abstract": [
    "Nonnegative Matrix Factorization (NMF) has become an increasingly popular method in the field of non-stationary speech denoising. However most NMF based algorithms assume prior knowledge about the background noise, which is often not available in the time-varying and mobile environments. In this paper, we propose a two-step NMF based speech-noise separation algorithm to address this issue. This algorithm takes the outcome of the first NMF separation as the dataset to train the basis vectors for the background noise, which will be used for the second-step NMF separation with fixed speech and noise basis vectors. Experimental results show that the proposed algorithm could achieve better results than other NMF algorithms for speech-noise separation.\n",
    "Index Terms: single channel speech separation, nonnegative matrix factorization, voiced/unvoiced sound classification, Wiener filter.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-532"
  },
  "yip12_interspeech": {
   "authors": [
    [
     "Michael C. W.",
     "Yip"
    ]
   ],
   "title": "Meaning inhibition and sentence processing in Chinese: evidence from negative priming",
   "original": "i12_1993",
   "page_count": 4,
   "order": 545,
   "p1": "1993",
   "pn": "1996",
   "abstract": [
    "The present study was designed to further examine the inhibitory processes of spoken word recognition of Chinese homophones during sentence processing. In this study, we employed the negative priming paradigm in a cross-modal naming experiment. In the experiment, all the native Cantonese listeners were asked to name aloud a visual probe as quick and accurate as they could after hearing a sentence, which ended with a homophone. Results suggested that preceding sentence context has an early effect on selecting the appropriate meaning among all the other alternative meanings of the homophone. Furthermore, negative priming effects were observed that the contextually inappropriate meanings of the homophone were inhibited rapidly during sentence processing.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-533"
  },
  "ijima12_interspeech": {
   "authors": [
    [
     "Yusuke",
     "Ijima"
    ],
    [
     "Mitsuaki",
     "Isogai"
    ],
    [
     "Hideyuki",
     "Mizuno"
    ]
   ],
   "title": "Similar speaker selection technique based on distance metric learning with perceptual voice quality similarity",
   "original": "i12_1997",
   "page_count": 4,
   "order": 546,
   "p1": "1997",
   "pn": "2000",
   "abstract": [
    "This paper describes a similar speaker selection technique based on distance metric learning. Our aim is selection of a perceptually similar speaker using acoustic features from a multi-speaker database. A novel point of the proposed technique is training a transform matrix using the perceptual voice quality similarity between many speakers obtained from a subjective evaluation to convert acoustic feature space. Given an input speech, acoustic features of the input speech are transformed using a trained transform matrix, after which speaker selection is performed based on the Euclidean distance on the transformed acoustic feature space. We perform speaker selection experiments and evaluate the performance results by comparing them with those of speaker selection on acoustic feature space without feature space transformation. The results indicate that transformation based on distance metric learning provides about 60% of the error reduction rate.\n",
    "Index Terms: speaker selection, perceptual similarity, voice quality, distance metric learning\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-534"
  },
  "babel12_interspeech": {
   "authors": [
    [
     "Molly",
     "Babel"
    ],
    [
     "Grant",
     "McGuire"
    ]
   ],
   "title": "Gendered sound symbolism and masking effects in speech processing",
   "original": "i12_2001",
   "page_count": 4,
   "order": 547,
   "p1": "2001",
   "pn": "2004",
   "abstract": [
    "Sound symbolism is the non-arbitrary association of sound and meaning. Experiment 1 demonstrates that sound symbolic associations facilitate the online processing of male and female voices when the target words contain the vowels /a/ and /i/ for male and female voices, respectively, when listeners are engaged in a speeded gender identification task. Experiment 2 reveals that when listeners are attending to a vowel identification task, there is no effect of talker gender. This suggests support of models of perception where activation of linguistic structure masks indexical bottom-up information.\n",
    "Index Terms: speech perception, speech processing, sound symbolism, adaptive resonance theory\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-535"
  },
  "bosch12_interspeech": {
   "authors": [
    [
     "Louis ten",
     "Bosch"
    ],
    [
     "Odette",
     "Scharenborg"
    ]
   ],
   "title": "Modeling cue trading in human word recognition",
   "original": "i12_2005",
   "page_count": 4,
   "order": 548,
   "p1": "2005",
   "pn": "2008",
   "abstract": [
    "Classical phonetic studies have shown that acoustic-articulatory cues can be interchanged without affecting the resulting phoneme percept (ecue tradingf). Cue trading has so far mainly been investigated in the context of phoneme identification. In this study, we investigate cue trading in word recognition, because words are the units of speech through which we communicate. This paper aims to provide a method to quantify cue trading effects by using a computational model of human word recognition. This model takes the acoustic signal as input and represents speech using articulatory feature streams. Importantly, it allows cue trading and underspecification. Its set-up is inspired by the functionality of Fine-Tracker, a recent computational model of human word recognition. This approach makes it possible, for the first time, to quantify cue trading in terms of a trade-off between features and to investigate cue trading in the context of a word recognition task.\n",
    "Index Terms: cue trading, human word recognition, computational modeling, articulatory features\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-536"
  },
  "li12i_interspeech": {
   "authors": [
    [
     "David Cheng-Huan",
     "Li"
    ],
    [
     "Elsi",
     "Kaiser"
    ]
   ],
   "title": "Accounting for speech rate in spoken word recognition",
   "original": "i12_2009",
   "page_count": 4,
   "order": 549,
   "p1": "2009",
   "pn": "2012",
   "abstract": [
    "To test whether speech rate influences the interpretation of lexically-ambiguous forms, we conducted a visual-world eye-tracking study. We varied the speech rate of carrier phrases, to see if this influences how listeners perceive sequences with a coronal/labial ambiguity. Our results suggest that listeners' interpretations are sensitive to speech rate: Even with identical acoustic materials, listeners' real-time processing of ambiguous words is influenced by the speech rate of the surrounding carrier sentence. This work highlights the importance of incorporating speech rate into existing models of spoken word recognition.\n",
    "Index Terms: spoken word recognition, speech rate, visual-world eye-tracking\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-537"
  },
  "hanique12_interspeech": {
   "authors": [
    [
     "Iris",
     "Hanique"
    ],
    [
     "Mirjam",
     "Ernestus"
    ]
   ],
   "title": "The processes underlying two frequent casual speech phenomena in Dutch: a production experiment",
   "original": "i12_2013",
   "page_count": 4,
   "order": 550,
   "p1": "2013",
   "pn": "2016",
   "abstract": [
    "This study investigated whether a shadowing task can provide insights in the nature of reduction processes that are typical of casual speech. We focused on the shortening and presence versus absence of schwa and /t/ in Dutch past participles. Results showed that the absence of these segments was affected by the same variables as their shortening, suggesting that absence mostly resulted from extreme gradient shortening. This contrasts with results based on recordings of spontaneous conversations. We hypothesize that this difference is due to non-casual fast speech elicited by a shadowing task.\n",
    "Index Terms: pronunciation variation, acoustic reduction, shadowing task, experimental methodology\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-538"
  },
  "birkholz12b_interspeech": {
   "authors": [
    [
     "Peter",
     "Birkholz"
    ],
    [
     "Phil",
     "Hoole"
    ]
   ],
   "title": "Intrinsic velocity differences of lip and jaw movements: preliminary results",
   "original": "i12_2017",
   "page_count": 4,
   "order": 551,
   "p1": "2017",
   "pn": "2020",
   "abstract": [
    "The observed kinematics of speech movements are the result of both the control by the brain and the biomechanical properties of the peripheral speech apparatus. For many kinematic phenomena, it is not clear whether they are actively controlled or intrinsic to the biomechanical system. This pilot study investigated the movement of sensors on the lips and the jaw in cyclical vowel transitions at specific speaking rates to identify possible intrinsic differences in the velocities of the articulators. Thereby, the lower lip was found to be significantly faster in approaching its targets than the upper lip, the mouth corners, and the jaw. Furthermore, for the mouth corners, backward movements were significantly faster than forward movements.\n",
    "Index Terms: Speech kinematics, intrinsic velocity differences, articulatory control\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-539"
  },
  "viebahn12_interspeech": {
   "authors": [
    [
     "Malte C.",
     "Viebahn"
    ],
    [
     "Mirjam",
     "Ernestus"
    ],
    [
     "James M.",
     "McQueen"
    ]
   ],
   "title": "Co-occurrence of reduced word forms in natural speech",
   "original": "i12_2021",
   "page_count": 4,
   "order": 552,
   "p1": "2021",
   "pn": "2024",
   "abstract": [
    "This paper presents a corpus study that investigates the co-occurrence of reduced word forms in natural speech. We extracted Dutch past participles from three different speech registers and investigated the influence of several predictor variables on the presence and duration of schwas in prefixes and /t/s in suffixes. Our results suggest that reduced word forms tend to co-occur even if we partial out the effect of speech rate. The implications of our findings for episodic and abstractionist models of lexical representation are discussed.\n",
    "Index Terms: speech production, spontaneous speech, speech reduction\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-540"
  },
  "yoshinaga12_interspeech": {
   "authors": [
    [
     "Ikuyo",
     "Yoshinaga"
    ],
    [
     "Jiangping",
     "Kong"
    ]
   ],
   "title": "Voice production mechanisms of vibrato in Noh",
   "original": "i12_2025",
   "page_count": 4,
   "order": 553,
   "p1": "2025",
   "pn": "2028",
   "abstract": [
    "Vibrato used in Noh play was investigated using electroglottographic and acoustical analyses. Laryngeal movements were successfully obtained from the EGG signal and its derivative in order to study how the peculiar and expressive voice qualities were produced during vibrato. The slow wide vibrato of Noh was achieved not only by the F0 modulation but also by changing the degree of constriction at the vocal folds and the supraglottal structures. These complicated movements at the end of the resonator seemed to add color and variety to the voice quality, and produce an extremely unique vibrato.\n",
    "Index Terms: vibrato, Noh, supraglottal constriction\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-541"
  },
  "orozcoarroyave12_interspeech": {
   "authors": [
    [
     "Juan Rafael",
     "Orozco-Arroyave"
    ],
    [
     "Julian David",
     "Arias-Londoño"
    ],
    [
     "Jesús Francisco",
     "Vargas-Bonilla"
    ],
    [
     "Elmar",
     "Nöth"
    ]
   ],
   "title": "Automatic detection of hypernasal speech signals using nonlinear and entropy measurements",
   "original": "i12_2029",
   "page_count": 4,
   "order": 554,
   "p1": "2029",
   "pn": "2032",
   "abstract": [
    "Automatic hypernasality detection in children with Cleft Lip and Palate is classically performed by means of acoustic analysis; however, recent findings indicate that nonlinear dynamics features could be useful for this task. In order to continue deepening in this issue, in this paper the discriminant capability of 4 different nonlinear dynamics features along with a set of 6 entropy measurements is studied. The whole set of features is optimized using an automatic feature selection technique based on principal component analysis. The decision about the presence or absence of hypernality is made by employing a support vector machine. The system is tested over two databases, one considers the five Spanish vowels and the words /coco/ and /gato/, and the other one considers different German words. The performance of the system is presented in terms of accuracy, sensitivity, specificity and receiver operating curves. According to the results, the accuracy of system increases when nonlinear and entropy measures are combined.\n",
    "Index Terms: Hypernasality, Cleft Lip and Palate, nonlinear dynamics, entropy measures\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-542"
  },
  "aubanel12_interspeech": {
   "authors": [
    [
     "Vincent",
     "Aubanel"
    ],
    [
     "Martin",
     "Cooke"
    ],
    [
     "Emma",
     "Foster"
    ],
    [
     "Maria Luisa",
     "Garcia Lecumberri"
    ],
    [
     "Catherine",
     "Mayo"
    ]
   ],
   "title": "Effects of the availability of visual information and presence of competing conversations on speech production",
   "original": "i12_2033",
   "page_count": 4,
   "order": 555,
   "p1": "2033",
   "pn": "2036",
   "abstract": [
    "How do talkers maintain intelligibility when speaking in the presence of a background conversation? The current study identified acoustic and temporal modifications of speech manifested by interlocutors in the face of competing speech, with and without visual contact. Pairs of talkers held free conversations either alone or in the presence of a second pair. Regardless of the availability of visual information, speaking simultaneously with another talker resulted in overall increases in energy, F0, F1 and a decrease in speech rate. Overlapping with the background pair resulted in an increase in energy but no change in the two prosodic parameters F0 and speech rate. By contrast, within-pair overlap led to an increase in F0 and a decrease in rate, and no change in speech level. The absence of visual cues produced a significant reduction in withinpair overlap, which tended to be greater when the background pair was present. These findings emphasize the need to distinguish between Lombard and interactional influences on acoustic parameters, and suggest that adverse conditions such as competing speech or absence of visual cues cause interlocutors to adopt more careful dialogue strategies, perhaps with the aim of reducing energetic and informational masking at the ears of the listener.\n",
    "Index Terms: simultaneous conversations, speech production modifications, speech in noise\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-543"
  },
  "huang12f_interspeech": {
   "authors": [
    [
     "Shuai",
     "Huang"
    ],
    [
     "Glen A.",
     "Coppersmith"
    ],
    [
     "Damianos",
     "Karakos"
    ]
   ],
   "title": "Constrained maximum mutual information dimensionality reduction for language identification",
   "original": "i12_2037",
   "page_count": 4,
   "order": 556,
   "p1": "2037",
   "pn": "2040",
   "abstract": [
    "In this paper we propose Constrained Maximum Mutual Information dimensionality reduction (CMMI), an information-theoretic based dimensionality reduction technique. CMMI tries to maximize the mutual information between the class labels and the projected (lower dimensional) features, optimized via gradient ascent. Supervised and semi-supervised CMMI are introduced and compared with a state of the art dimensionality reduction technique (Minimum/Maximum Rényi's Mutual Information using the Stochastic Information Gradient; MRMISIG) for a language identification (LID) task using CallFriend corpus, with favorable results. CMMI also deals with higher dimensional data more gracefully than MRMI-SIG, permitting application to datasets for which MRMI-SIG is computationally prohibitive.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-544"
  },
  "benzeghiba12_interspeech": {
   "authors": [
    [
     "Mohamed Faouzi",
     "BenZeghiba"
    ],
    [
     "Jean-Luc",
     "Gauvain"
    ],
    [
     "Lori",
     "Lamel"
    ]
   ],
   "title": "Phonotactic language recognition using MLP features",
   "original": "i12_2041",
   "page_count": 4,
   "order": 557,
   "p1": "2041",
   "pn": "2044",
   "abstract": [
    "PPRLM language recognition systems using context-dependent (CD) phone recognizers outperform significantly those using contextindependent (CI) phone recognizers, but computationally are less efficient. This papers describes a very efficient PPRLM system both in terms of performances and processing speed. The system uses CI phone recognizers trained with MLP features concatenated with the conventional PLP and pitch features. MLP features have some interesting properties that make them suitable to build such a system, in particular the temporal context provided to the inputs of the MLP and the discriminative criterion used to learn MLP parameters. Results of preliminary experiments conducted on the NIST LRE 2005 for closed-set task show significant improvements (for the three conditions) obtained by the proposed system compared to a PPRLM system using CI phone models trained with PLP features. More ever, the proposed system performs equally compared to the PPRLM using CD phone models, while running 6 times faster.\n",
    "Index Terms: Language recognition, Phonotactic approach, MLP features\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-545"
  },
  "penagarikano12_interspeech": {
   "authors": [
    [
     "Mikel",
     "Penagarikano"
    ],
    [
     "Amparo",
     "Varona"
    ],
    [
     "Luis Javier",
     "Rodriguez-Fuentes"
    ],
    [
     "Mireia",
     "Diez"
    ],
    [
     "German",
     "Bordel"
    ]
   ],
   "title": "The EHU systems for the NIST 2011 language recognition evaluation",
   "original": "i12_2045",
   "page_count": 4,
   "order": 558,
   "p1": "2045",
   "pn": "2048",
   "abstract": [
    "This paper describes the systems developed by the Software Technologies Working Group of the University of the Basque Country (EHU) for the NIST 2011 Language Recognition Evaluation (LRE). One primary and three contrastive systems were submitted, all of them fusing five component subsystems: a Linearized Eigenchannel GMM (LE-GMM) subsystem, an iVector subsystem and three phone-lattice-SVM subsystems based on the publicly available BUT decoders for Czech, Hungarian an Russian. The four submitted systems were identical except for the backend approach and the development dataset used to estimate the backend and fusion parameters. Multiclass discriminative fusion was performed separately for each nominal duration. A development set was defined, including the evaluation sets of LRE07 and LRE09 and the development data provided by NIST for 9 additional languages in the 2011 campaign. The official results, which were among the best submitted to the evaluation, are presented and briefly discussed. Post-key analyses are also addressed in the paper, including the performance attained by component subsystems and a study of their contribution to fusion performance by means of a greedy selection procedure.\n",
    "Index Terms: Spoken Language Recognition, NIST 2011 LRE, Gaussian Backend, Multiclass Discriminative Fusion\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-546"
  },
  "penagarikano12b_interspeech": {
   "authors": [
    [
     "Mikel",
     "Penagarikano"
    ],
    [
     "Amparo",
     "Varona"
    ],
    [
     "Mireia",
     "Diez"
    ],
    [
     "Luis Javier",
     "Rodriguez-Fuentes"
    ],
    [
     "German",
     "Bordel"
    ]
   ],
   "title": "Study of different backends in a state-of-the-art language recognition system",
   "original": "i12_2049",
   "page_count": 4,
   "order": 559,
   "p1": "2049",
   "pn": "2052",
   "abstract": [
    "State of the art language recognition systems usually add a backend prior to the linear fusion of the subsystems scores. The backend plays a dual role. When the set of languages for which models have been trained does not match the set of target languages, the backend maps the available scores to the space of target languages. On the other hand, the backend serves as a precalibration stage that adapts the amorphous space of scores. In this work, well known backends (Generative Gaussian Backend, Discriminative Gaussian Backend and Logistic Regression Backend) and newer proposals (Fully Bayesian Gaussian Backend and Gaussian Mixture Backend) are analyzed and compared. The effect of applying a T-Norm or a ZT-Norm is also analyzed. Finally the effect of discarding development signals, those with the highest scores, is also studied. Experiments have been carried out on the NIST 2009 LRE database, using a state-of-the-art Language Recognition System consisting of the fusion of five subsystems: A Linearized Eigenchannel GMM (LE-GMM) subsystem, an iVector subsystem and three phone-lattice-SVM subsystems.\n",
    "Index Terms: Spoken Language Recognition, Gaussian Backend, Gaussian Mixture Backend, Discriminative Gaussian Backend\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-547"
  },
  "yaman12_interspeech": {
   "authors": [
    [
     "Sibel",
     "Yaman"
    ],
    [
     "Jason",
     "Pelecanos"
    ],
    [
     "Mohamed Kamal",
     "Omar"
    ]
   ],
   "title": "On the use of non-linear polynomial kernel SVMs in language recognition",
   "original": "i12_2053",
   "page_count": 4,
   "order": 560,
   "p1": "2053",
   "pn": "2056",
   "abstract": [
    "Low-dimensional representations have been shown to outperform their supervector counterparts in a variety of speaker recognition tasks. In this paper, we show that non-linear polynomial kernel support vector machines (SVMs) trained with low-dimensional representations almost halve the equal-error rate (EER) of the best performing SVMs trained with supervectors. Non-linear kernel SVMs implicitly transform the input features onto higher-dimensional spaces, a mechanism known to be generally effective when the number of instances is much larger than the feature dimension. Contrary to linear kernels, non-linear kernels exploit the dependencies among different input feature dimensions in the resulting high-dimensional spaces. Our experiments demonstrate that fifth-order polynomial kernel SVMs trained with low-dimensional representations reduce the EER by 56% relative when compared to standard linear SVMs trained with supervectors. They reduce the EER by 40% relative to the best performing SVMs trained with supervectors.\n",
    "Index Terms: language recognition, support vector machines\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-548"
  },
  "jiang12c_interspeech": {
   "authors": [
    [
     "Bing",
     "Jiang"
    ],
    [
     "Yan",
     "Song"
    ],
    [
     "Wu",
     "Guo"
    ],
    [
     "Lirong",
     "Dai"
    ]
   ],
   "title": "Exemplar-based sparse representation for language recognition on i-vectors",
   "original": "i12_2057",
   "page_count": 4,
   "order": 561,
   "p1": "2057",
   "pn": "2060",
   "abstract": [
    "In this paper, a new automatic language identification method using sparse representation on ivectors in low-dimensional total variability space is proposed. It is mainly based on the recently proposed i-vector based language recognition systems. In our proposed method, an over-complete dictionary is first constructed by randomly sampling of the low-dimensional total variability space after Within-Class Covariance Normalization (WCCN) and Linear Discriminate Analysis (LDA). And then for each test sample, the classification score is derived from sparse linear representation with respect to the over-complete dictionary. Furthermore, a random subspace method, which combines different sparse representation classifiers, is introduced to address the possible over-fitting issue and to improve the robustness of the estimation. Evaluations on NIST LRE 2007 dataset show that the proposed method outperforms the state-of-the-art i-vector based language recognition system. Especially for 30s test condition, our proposed method achieves relative reduction of 29.6% on Equal Error Rate (EER) compared with the baseline system.\n",
    "Index Terms: language recognition, ivector, sparse representation\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-549"
  },
  "shih12_interspeech": {
   "authors": [
    [
     "Yu-Chin",
     "Shih"
    ],
    [
     "Hung-Shin",
     "Lee"
    ],
    [
     "Hsin-Min",
     "Wang"
    ],
    [
     "Shyh-Kang",
     "Jeng"
    ]
   ],
   "title": "Subspace-based feature representation and learning for language recognition",
   "original": "i12_2061",
   "page_count": 4,
   "order": 562,
   "p1": "2061",
   "pn": "2064",
   "abstract": [
    "This paper presents a novel subspace-based approach for phonotactic language recognition. The whole framework is divided into two parts: the speech feature representation and the subspace-based learning algorithm. First, the phonetic information as well as the contextual relationship, possessed by spoken utterances, are more abundantly retrieved by likelihood computation and feature concatenation through the decoding processed by an automatic speech recognizer. It is assumed that the extracted phone frames reside in a lower dimensional eigen-subspace, in which the structure of data can be approximately captured. Each utterance is further represented by a fixed-dimensional linear subspace. Second, to measure the similarity between two utterances, suitable non-Euclidean metrics are explored and applied to non-linear discriminant analysis in a kernel fashion, followed by a back-end classifier, such as the k-nearest neighbor (K-NN) classifier. The results of experiments on the OGI-TS database demonstrate that the proposed framework outperforms the well-known vector space modeling based method with relative reductions of 38.90% and 27.13% on the 1-to-50-second and 3-second data sets respectively in equal error rate (EER).\n",
    "Index Terms: language recognition, subspace-based learning\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-550"
  },
  "you12_interspeech": {
   "authors": [
    [
     "Changhuai",
     "You"
    ],
    [
     "Haizhou",
     "Li"
    ],
    [
     "Bin",
     "Ma"
    ],
    [
     "Kong Aik",
     "Lee"
    ]
   ],
   "title": "Effect of relevance factor of maximum a posteriori adaptation for GMM-SVM in speaker and language recognition",
   "original": "i12_2065",
   "page_count": 4,
   "order": 563,
   "p1": "2065",
   "pn": "2068",
   "abstract": [
    "Gaussian mixture model - support vector machine (GMM-SVM) with nuisance attribute projection (NAP) has been found to be effective and reliable for speaker and language recognition. In maximum a posteriori (MAP) adaptation of GMM, the relevance factor is the parameter that regulates how much the adaptation data affect the base model, which impacts the final recognition performance. In our previous work, the data-dependent relevance factor and adaptive relevance factor have been introduced. In this paper, we provide insights into different types of relevance factor for MAP in the context of application as formulated under Speaker Recognition Evaluation (SRE) and Language Recognition Evaluation (LRE) by the National Institute of Standards and Technology (NIST).\n",
    "Index Terms: maximum a posteriori, supervector, Gaussian mixture model, support vector machine\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-551"
  },
  "varona12_interspeech": {
   "authors": [
    [
     "Amparo",
     "Varona"
    ],
    [
     "Mikel",
     "Penagarikano"
    ],
    [
     "Luis Javier",
     "Rodriguez-Fuentes"
    ],
    [
     "German",
     "Bordel"
    ],
    [
     "Mireia",
     "Diez"
    ]
   ],
   "title": "Using time-synchronous phone co-occurrences in a SVM-phonotactic dialect recognition system",
   "original": "i12_2069",
   "page_count": 4,
   "order": 564,
   "p1": "2069",
   "pn": "2072",
   "abstract": [
    "This paper presents a simple approach to phonotactic dialect recognition which uses lattices of time-synchronous phone co-occurrences at the frame level. In previous works, we successfully applied cross-decoder phone co-occurrences to improve performance in a language recognition experiments on the 2007 NIST LRE database. We call phone co-occurrence to the simultaneous (time-synchronous) presence of two phone units coming from two different phone decoders. In this work, the approach is ported to a Dialect Recognition task based on the assumption that co-occurrences can better represent the tiny differences among the dialects. Besides, a slightly different approach is presented, based on the simultaneous presence of two phone units in the lattice produced by a single decoder (intra-decoder phone co-occurrences). For evaluating the approach, a choice of open software (Brno University of Technology phone decoders, HTK, SRILM, LIBLINEAR and FoCal) was used, and experiments were carried out on the Arabic dialects of the 2011 NIST LRE database. The proposed cross-decoder approach outperformed the baseline phonotactic systems, yielding around 7% relative improvement. The fusion of the baseline system with the proposed approach yielded 7.31% EER and CLLR=0.497 meaning 19% relative improvement.\n",
    "Index Terms: Phonotactic Dialect Recognition, Phone Cooccurrences, Phone Lattices, Support Vector Machines\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-552"
  },
  "mehrabani12_interspeech": {
   "authors": [
    [
     "Mahnoosh",
     "Mehrabani"
    ],
    [
     "Joseph",
     "Tepperman"
    ],
    [
     "Emily",
     "Nava"
    ]
   ],
   "title": "Nativeness classification with suprasegmental features on the accent group level",
   "original": "i12_2073",
   "page_count": 4,
   "order": 565,
   "p1": "2073",
   "pn": "2076",
   "abstract": [
    "We present a novel approach to discriminating native and nonnative utterances based on suprasegmental features extracted at the Accent Group (AG) level. Past studies have shown modeling a set of shared intonation patterns across AGs to be effective in predicting local F0 contour shapes. Here we demonstrate that AG level prosodic features are also effective in nativeness classification. The proposed suprasegmental feature set is very low dimensional, and is derived from F0 and energy contours across the AG, as well as normalized duration of the syllables within each AG. A Random Forest back end classifier is used to combine AG level scores from GMM and Decision Tree models, producing nativeness scores at the utterance level. The proposed prosodic nativeness classifier achieves 83.3% accuracy for 2-AG utterances and 89.1% accuracy for 3-AG utterances, exceeding a baseline Gaussian Supervector system's performance by more than 10% absolute. The vastly lower dimensionality of the proposed feature set relative to the baseline method suggests the importance of suprasegmental features over traditional spectral cues in contributing to the perceived nativeness of a learner's language.\n",
    "Index Terms: nativeness, prosody, intonation, rhythm\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-553"
  },
  "lee12e_interspeech": {
   "authors": [
    [
     "Huny-yi",
     "Lee"
    ],
    [
     "Po-wei",
     "Chou"
    ],
    [
     "Lin-shan",
     "Lee"
    ]
   ],
   "title": "Open-vocabulary retrieval of spoken content with shorter/longer queries considering word/subword-based acoustic feature similarity",
   "original": "i12_2077",
   "page_count": 4,
   "order": 566,
   "p1": "2077",
   "pn": "2080",
   "abstract": [
    "Acoustic feature similarity between utterances has been shown to be very helpful for spoken term detection using pseudo-relevance feedback (PRF) and graph-based re-ranking. Both cases are based on the concept that utterances similar to those utterances with higher relevance scores in acoustic features should have higher scores, while graph-based re-ranking further considers the similarity structure between many utterances globally with a graph. In this paper, we extend these approaches to consider acoustic feature similarity between utterances over both word and subword lattices, and offer a complete formulation for the general problem of open vocabulary retrieval of spoken content with shorter or longer queries. All these are verified by significant improvements in preliminary experiments with both in vocabulary (IV) and OOV queries.\n",
    "Index Terms: Spoken Content Retrieval, Pseudo-relevance Feedback, Random Walk\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-554"
  },
  "byun12_interspeech": {
   "authors": [
    [
     "Byungki",
     "Byun"
    ],
    [
     "Ilseo",
     "Kim"
    ],
    [
     "Sabato Marco",
     "Siniscalchi"
    ],
    [
     "Chin-Hui",
     "Lee"
    ]
   ],
   "title": "Consumer-level multimedia event detection through unsupervised audio signal modeling",
   "original": "i12_2081",
   "page_count": 4,
   "order": 567,
   "p1": "2081",
   "pn": "2084",
   "abstract": [
    "In this work, a novel acoustic characterization approach to multimedia event detection (MED) task for unconstrained and unstructured consumer-level videos through audio signal modeling is proposed. The key idea is to characterize the acoustic space of interest with a set of fundamental acoustic units around which a set of acoustic segment models (ASMs) is built. A vector space modeling technique to address MED is here adopted, where an incoming audio signal is first decoded into a sequence of acoustic segments. Then, a feature vector is generated by using co-occurrence statistics of acoustic units, and the MED final decision is implemented with a vector space language classifier. Experimental evidence on the TRECVID2011 MED demonstrates the viability of the proposed approach. Furthermore, it better accounts for temporal dependencies than previously proposed MFCC bag-of-word approaches.\n",
    "Index Terms: multimedia event detection, unsupervised audio modeling, acoustic segment models\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-555"
  },
  "jin12_interspeech": {
   "authors": [
    [
     "Qin",
     "Jin"
    ],
    [
     "Peter",
     "Schulam"
    ],
    [
     "Shourabh",
     "Rawat"
    ],
    [
     "Susanne",
     "Burger"
    ],
    [
     "Duo",
     "Ding"
    ],
    [
     "Florian",
     "Metze"
    ]
   ],
   "title": "Event-based video retrieval using audio",
   "original": "i12_2085",
   "page_count": 4,
   "order": 568,
   "p1": "2085",
   "pn": "2088",
   "abstract": [
    "Multimedia Event Detection (MED) is an annual task in the NIST TRECVID evaluation, and requires participants to build indexing and retrieval systems for locating videos in which certain predefined events are shown. Typical systems focus heavily on the use of visual data. Audio data, however, also contains rich information that can be effectively used for video retrieval, and MED could benefit from the attention of researchers in audio analysis. We present several systems for performing MED using only audio data, report the results of each system on the TRECVID MED 2011 development dataset, and compare the strengths and weaknesses of each approach.\n",
    "Index Terms: multimedia event detection, audio processing, video retrieval\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-556"
  },
  "zhuang12_interspeech": {
   "authors": [
    [
     "Xiaodan",
     "Zhuang"
    ],
    [
     "Stavros",
     "Tsakalidis"
    ],
    [
     "Shuang",
     "Wu"
    ],
    [
     "Pradeep",
     "Natarajan"
    ],
    [
     "Rohit",
     "Prasad"
    ],
    [
     "Prem",
     "Natarajan"
    ]
   ],
   "title": "Compact audio representation for event detection in consumer media",
   "original": "i12_2089",
   "page_count": 4,
   "order": 569,
   "p1": "2089",
   "pn": "2092",
   "abstract": [
    "Local audio-visual descriptors are often compactly stored using representations such as the soft quantization histogram [1]. Typically, classification performance with histogram representations is improved through the use of large codeword sets. Unfortunately, this approach runs into overfitting and scalability challenges when applied to richly diverse real-world collections.   A novel “i-vector” approach was recently proposed for the speaker-verification task [2]. In this work, we study the relative effectiveness of the i-vector as a compact representation of local audio descriptors (e.g., MFCC's) within a multimedia event detection system. Specifically, we model the local audio descriptors using a Guassian Mixture Model (GMM). Following [2], we constrain theGMMparameters to a low-dimensional subspace while preserving most of the variability (i.e., information) in the descriptors. The GMM parameters in the subspace constitute a compact representation that exhibits robustness in the face of sparse data.   We evaluate the method by performing the multimedia event detection (MED) task using only audio information within consumer (e.g., YouTube) videos. Experiments with the 2011 TRECVID MED data show that the i-vector provides superior performance and lower dimensionality than the bag-of-words soft quantization histograms used in the state-of-the-art BBN VISER system in the 2011 TRECVID MED Evaluation.\n",
    "s\n",
    "J. C. van Gemert, C. J. Veenman, A. W. M. Smeulders, and J. M. Geusebroek, “Visual word ambiguity,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 32, no. 7, pp. 1271–1283, 2010.\n",
    "N. Dehak, P. Kenny, R. Dehak, P. Dumouchel, and P. Ouellet, “Front-end factor analysis for speaker verification,” Audio, Speech, and Language Processing, IEEE Transactions on, vol. 19, no. 4, pp. 788–798, may 2011\n",
    "",
    "",
    "Index Terms: multimedia event detection, factor analysis\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-557"
  },
  "liu12c_interspeech": {
   "authors": [
    [
     "Chao",
     "Liu"
    ],
    [
     "Dong",
     "Wang"
    ],
    [
     "Javier",
     "Tejedor"
    ]
   ],
   "title": "N-gram FST indexing for spoken term detection",
   "original": "i12_2093",
   "page_count": 4,
   "order": 570,
   "p1": "2093",
   "pn": "2096",
   "abstract": [
    "An efficient indexing scheme is essentially important for spoken term detection (STD) on large databases, particularly for phone-based systems that have been widely adopted to achieve vocabulary-independent detection. While the finite state transducer (FST) composition provides a standard indexing approach, the n-gram reverse indexing is more flexible in connectivity representation and confidence measuring and therefore may result in better performance than searching within the original lattices or the equivalent FSTs. In this paper we present an n-gram FST indexing approach which combines the flexibility of n-gram indexing and the efficiency of FST indexing. Specifically, we employ the n-gram indexing to relax the connectivity in original lattices and then formalize the indices into an FST for online search. We demonstrate this approach with a phone-based STD task where the lattice is sparse due to strong language models. The results show that the n-gram FST indexing provides not only better detection performance but a faster detection speed than both the conventional n-gram and FST indexing.\n",
    "Index Terms: spoken term indexing, finite state transducer, spoken term detection, speech recognition\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-558"
  },
  "majima12_interspeech": {
   "authors": [
    [
     "Haruka",
     "Majima"
    ],
    [
     "Rafael",
     "Torres"
    ],
    [
     "Yoko",
     "Fujita"
    ],
    [
     "Hiromichi",
     "Kawanami"
    ],
    [
     "Tomoko",
     "Matsui"
    ],
    [
     "Hiroshi",
     "Saruwatari"
    ],
    [
     "Kiyohiro",
     "Shikano"
    ]
   ],
   "title": "Spoken inquiry discrimination using bag-of-words for speech-oriented guidance system",
   "original": "i12_2097",
   "page_count": 4,
   "order": 571,
   "p1": "2097",
   "pn": "2100",
   "abstract": [
    "We investigate a discrimination method for invalid and valid spoken inquiries, received by a speech-oriented guidance system operating in a real environment. Invalid spoken inquiries include background voices, which are not directly uttered to the system, and nonsense utterances. Such spoken inquiries should be rejected beforehand. By now, we have reported a method using the likelihood values of Gaussian mixture models (GMMs) to discriminate invalid spoken inquiries from valid ones. In this paper, we improve the performance by utilizing not only the likelihood values but also other information in spoken inquiries such as bag-of-words (BOW), utterance duration, and signal-to-noise ratio (SNR). To deal with these multiple information, we use support vector machine (SVM) with radial basis function (RBF) kernel and maximum entropy (ME) method and compare the performance. In the experiments, we achieve 86.6% of F-measure for SVM and 84.2% for ME, while F-measure for GMM-based method is 81.7%.\n",
    "Index Terms: speech-oriented guidance system, spoken inquiry discrimination, support vector machine, maximum entropy, bag-of-words\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-559"
  },
  "tsakalidis12_interspeech": {
   "authors": [
    [
     "Stavros",
     "Tsakalidis"
    ],
    [
     "Xiaodan",
     "Zhuang"
    ],
    [
     "Roger",
     "Hsiao"
    ],
    [
     "Shuang",
     "Wu"
    ],
    [
     "Pradeep",
     "Natarajan"
    ],
    [
     "Rohit",
     "Prasad"
    ],
    [
     "Prem",
     "Natarajan"
    ]
   ],
   "title": "Robust event detection from spoken content in consumer domain videos",
   "original": "i12_2101",
   "page_count": 4,
   "order": 572,
   "p1": "2101",
   "pn": "2104",
   "abstract": [
    "In this paper, we propose an innovative integrated approach to leveraging available spoken content while detecting events in consumergenerated multimedia data (i.e., YouTube videos). Spoken content in consumer videos exhibits several challenges. For example, unlike Broadcast News, the spoken audio is typically not labeled. Also, the audio track in consumer videos tends to be noisy and the spoken content is often sporadic.   Here, we describe three recent improvements that are specifically targeted at overcoming the challenges in consumer videos: robust data-driven keyword selection, automatic discovery of word-classes for keyword expansion, and a keyword spotting approach for improving recall in noisy conditions. These improvements were integrated into the audio analysis component of the BBN VISER system that demonstrated top performance in the 2011 TRECVID Multimedia Event Detection (MED) task. Experimental results on the 2011 TRECVID MED task clearly demonstrate the effectiveness of the three improvements.\n",
    "Index Terms: multimedia event detection, keyword selection, keyword expansion, keyword spotting.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-560"
  },
  "pancoast12_interspeech": {
   "authors": [
    [
     "Stephanie",
     "Pancoast"
    ],
    [
     "Murat",
     "Akbacak"
    ]
   ],
   "title": "Bag-of-audio-words approach for multimedia event classification",
   "original": "i12_2105",
   "page_count": 4,
   "order": 573,
   "p1": "2105",
   "pn": "2108",
   "abstract": [
    "With the popularity of online multimedia videos, there has been much interest in recent years in acoustic event detection and classification for the improvement of online video search. The audio component of a video has the potential to contribute significantly to multimedia event classification. Recent research in audio document classification has drawn parallels to text and image document retrieval by employing what is referred to as the bag-of-audio words (BoAW) method. Compared to supervised approaches where audio concept detectors are trained using annotated data and extracted labels are used as lowlevel features for multimedia event classification. The BoAW approach extracts audio concepts in an unsupervised fashion. Hence this method has the advantage that it can be employed easily for a new set of audio concepts in multimedia videos without going through a laborious annotation effort. In this paper, we explore variations of the BoAW method and present results on NIST 2011 multimedia event detection (MED) dataset.\n",
    "Index Terms: Bag-of-audio-words, multimedia event detection\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-561"
  },
  "iso12_interspeech": {
   "authors": [
    [
     "Ken-ichi",
     "Iso"
    ],
    [
     "Edward",
     "Whittaker"
    ],
    [
     "Tadashi",
     "Emori"
    ],
    [
     "Junpei",
     "Miyake"
    ]
   ],
   "title": "Improvements in Japanese voice search",
   "original": "i12_2109",
   "page_count": 4,
   "order": 574,
   "p1": "2109",
   "pn": "2112",
   "abstract": [
    "This paper describes work on Japanese voice-search at Yahoo! Japan. We first describe several implementation details of our WFST-based internal decoder which make the voice-search task more efficient including a simple, but effective, compressed WFST arc representation. We then describe a baseline system and make a comparison between our internal decoder and two open-source decoders, Juicer and Julius. We also describe our initial attempts to adapt the baseline system through simple language model adaptation using manually transcribed anonymized voice queries. To achieve this we present a sequence of WFST operations which preserve consistency of segmentation between the manual and automatic transcriptions. We show that even using this simple adaptation method we obtain a reduction in sentence error rate of up to 4.64% relative.\n",
    "Index Terms: ASR, Japanese, voice search, WFST\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-562"
  },
  "liu12d_interspeech": {
   "authors": [
    [
     "Jingjing",
     "Liu"
    ],
    [
     "Scott",
     "Cyphers"
    ],
    [
     "Panupong",
     "Pasupat"
    ],
    [
     "Ian",
     "McGraw"
    ],
    [
     "James",
     "Glass"
    ]
   ],
   "title": "A conversational movie search system based on conditional random fields",
   "original": "i12_2454",
   "page_count": 4,
   "order": 575,
   "p1": "2454",
   "pn": "2457",
   "abstract": [
    "Online streaming companies such as Netflix have become dominant in the media distribution sector. However, such media delivery services often support very rudimentary search, especially for natural language queries. To provide a more natural search interface, we have developed a conversational movie search system, which parses the recognition hypothesis of a spoken query into semantic classes using conditional random fields (CRFs), and then searches an indexed database with the identified semantics. Topic modeling on user-generated content (e.g., movie reviews) is employed for query expansion. Thirteen searching schemas are supported (such as genre, plot, character and soundtrack search). A crowd-sourcing platform was utilized to automatically collect large-scale annotated data for incremental CRF training.\n",
    "Index Terms: conditional random fields, spoken dialogue system\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-563"
  },
  "wen12c_interspeech": {
   "authors": [
    [
     "Tsung-Hsien",
     "Wen"
    ],
    [
     "Hung-Yi",
     "Lee"
    ],
    [
     "Lin-Shan",
     "Lee"
    ]
   ],
   "title": "Interactive spoken content retrieval with different types of actions optimized by a Markov decision process",
   "original": "i12_2458",
   "page_count": 4,
   "order": 576,
   "p1": "2458",
   "pn": "2461",
   "abstract": [
    "Interaction with user is specially important for spoken content retrieval, not only because of the recognition uncertainty, but because the retrieved spoken content items are difficult to be shown on the screen and difficult to be scanned and selected by the user. The user cannot playback and go through all the retrieved items and then find out they are not what he is looking for. In this paper, we propose a new approach for interactive spoken content retrieval, in which the system can estimate the quality of the retrieved results, and take different types of actions to clarify the user's intention based on an intrinsic policy. The policy is optimized by a Markov Decision Process (MDP) trained with Reinforcement Learning based on a set of pre-defined rewards considering the extra burden given to the user.\n",
    "Index Terms: Interactive SDR, MDP, Reinforcement Learning\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-564"
  },
  "allauzen12_interspeech": {
   "authors": [
    [
     "Cyril",
     "Allauzen"
    ],
    [
     "Edward",
     "Benson"
    ],
    [
     "Ciprian",
     "Chelba"
    ],
    [
     "Michael",
     "Riley"
    ],
    [
     "Johan",
     "Schalkwyk"
    ]
   ],
   "title": "Voice query refinement",
   "original": "i12_2462",
   "page_count": 4,
   "order": 577,
   "p1": "2462",
   "pn": "2465",
   "abstract": [
    "We describe a system for the refinement of spoken search queries. Given an initial query (\"Northern Italian restaurants in New York\"), instead of requiring a fully-specified followup query (\"Korean restaurants in New York\"), a more natural, abbreviated update query (\"Korean instead\") may be spoken. The system consists of a parsing step to identify the type and arguments of the refinement, a candidate generation step to enumerate the possible refinements, and a model classification step to select the best refinement. We present results on test query refinements given both to this system and to human judges that show the automated system outperforms the human judges on that data set.\n",
    "Index Terms: spoken dialog systems, voice search, query refinement\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-565"
  },
  "jansen12b_interspeech": {
   "authors": [
    [
     "Aren",
     "Jansen"
    ],
    [
     "Benjamin Van",
     "Durme"
    ]
   ],
   "title": "Indexing raw acoustic features for scalable zero resource search",
   "original": "i12_2466",
   "page_count": 4,
   "order": 578,
   "p1": "2466",
   "pn": "2469",
   "abstract": [
    "We present a new speech indexing and search scheme called Randomized Acoustic Indexing and Logarithmic-time Search (RAILS) that enables scalable query-by-example spoken term detection in the zero resource regime. RAILS is derived from our recent investigation into the application of randomized hashing and approximate nearest neighbor search algorithms to raw acoustic features. Our approach permits an approximate search through hundreds of hours of speech audio in a matter of seconds, and may be applied to any language without the need of a training corpus, acoustic model, or pronunciation lexicon. The fidelity of the approximation is controlled through a small number of easily interpretable parameters that allow a trade-off between search accuracy and speed.\n",
    "Index Terms: speech indexing, zero resource, query-byexample, spoken term detection, locality sensitive hashing\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-566"
  },
  "fayolle12_interspeech": {
   "authors": [
    [
     "Julien",
     "Fayolle"
    ],
    [
     "Murat",
     "Saraçlar"
    ],
    [
     "Fabienne",
     "Moreau"
    ],
    [
     "Christian",
     "Raymond"
    ],
    [
     "Guillaume",
     "Gravier"
    ]
   ],
   "title": "Lexical-phonetic automata for spoken utterance indexing and retrieval",
   "original": "i12_2470",
   "page_count": 4,
   "order": 579,
   "p1": "2470",
   "pn": "2473",
   "abstract": [
    "This paper presents a method for indexing spoken utterances which combines lexical and phonetic hypotheses in a hybrid index built from automata. The retrieval is realised by a lexical-phonetic and semi-imperfect matching whose aim is to improve the recall. A feature vector, containing edit distance scores and a confidence measure, weights each transition to help the filtering of the candidate utterance list for a more precise search. Experiment results show the complementarity of the lexical and phonetic representations, and compare the hybrid search with the state-of-the-art cascaded search to retrieve named entity queries.\n",
    "Index Terms: information retrieval, speech indexing, lexical-phonetic automata, confidence measures, edit distances, supervised learning\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-567"
  },
  "mcgraw12b_interspeech": {
   "authors": [
    [
     "Ian",
     "McGraw"
    ],
    [
     "Scott",
     "Cyphers"
    ],
    [
     "Panupong",
     "Pasupat"
    ],
    [
     "Jingjing",
     "Liu"
    ],
    [
     "James",
     "Glass"
    ]
   ],
   "title": "Automating crowd-supervised learning for spoken language systems",
   "original": "i12_2474",
   "page_count": 4,
   "order": 580,
   "p1": "2474",
   "pn": "2477",
   "abstract": [
    "Spoken language systems often rely on static speech recognizers. When the underlying models are dynamic, training is usually performed using unsupervised methods. In this work, we explore an alternative approach that uses human computation to provide on-the-fly crowd-supervised training. Although the framework we describe is applicable to any stochastic model for which the training data can be generated by nonexperts, we demonstrate its utility on the lexicon and language model of a speech recognizer in a cinema voice-search domain. We show how an initially shaky system can achieve over a 10#328% absolute improvement in word error rate (WER) - entirely without expert intervention. We then analyze how these gains were made.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-568"
  },
  "sainath12_interspeech": {
   "authors": [
    [
     "Tara N.",
     "Sainath"
    ],
    [
     "David",
     "Nahamoo"
    ],
    [
     "Dimitri",
     "Kanevsky"
    ],
    [
     "Bhuvana",
     "Ramabhadran"
    ]
   ],
   "title": "Enhancing exemplar-based posteriors for speech recognition tasks",
   "original": "i12_2130",
   "page_count": 4,
   "order": 581,
   "p1": "2130",
   "pn": "2133",
   "abstract": [
    "Posteriors generated from exemplar-based sparse representation methods are often learned to minimize reconstruction error of the feature vectors. These posteriors are not learned through a discriminative process linked to the word error rate (WER) objective of a speech recognition task. In this paper, we explore modeling exemplar-based posteriors to address this issue. We first explore posterior modeling by training a Neural Network using exemplar-based posteriors as inputs. This produces a new set of posteriors which have been learned to minimize a cross-entropy measure, and indirectly frame error rate. Second, we take the new NN posteriors and apply a tied mixture smoothing technique to these posteriors, making them more suited for a speech recognition task. On the TIMIT task, we show that using a NN model, we can improve the performance of our sparse representations by 1.3% absolute, achieving a PER of 19.0% by modeling SR posteriors with a NN. Furthermore, taking these NN posteriors and applying further smoothing techniques, we improve the PER to 18.7%, one of the best results reported in the literature on TIMIT.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-569"
  },
  "gemmeke12b_interspeech": {
   "authors": [
    [
     "Jort F.",
     "Gemmeke"
    ],
    [
     "Hugo",
     "Van hamme"
    ]
   ],
   "title": "Advances in noise robust digit recognition using hybrid exemplar-based techniques",
   "original": "i12_2134",
   "page_count": 4,
   "order": 582,
   "p1": "2134",
   "pn": "2137",
   "abstract": [
    "Expressing noisy speech spectra as a linear combination of speech and noise exemplars has been shown to be a powerful tool to achieve noise robust ASR. Such a model has been used both to do feature enhancement (FE) and to directly provide noise robust speech state probabilities using a method called sparse classification (SC). The goal of this work is threefold: First, we integrate various SC advances recently proposed in literature, second, we improve upon the results obtained with FE through retraining and multi-condition training of the acoustic models used in the recognizer and finally, we propose the use of a single hybrid SC-FE system. In our experiments on AURORA-2 we obtain an impressive 3% and 5% average WER on matched and on mismatched noise types, respectively.\n",
    "Index Terms: noise robustness, exemplar-based speech recognition, multi-stream decoding\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-570"
  },
  "hurmalainen12_interspeech": {
   "authors": [
    [
     "Antti",
     "Hurmalainen"
    ],
    [
     "Rahim",
     "Saeidi"
    ],
    [
     "Tuomas",
     "Virtanen"
    ]
   ],
   "title": "Group sparsity for speaker identity discrimination in factorisation-based speech recognition",
   "original": "i12_2138",
   "page_count": 4,
   "order": 583,
   "p1": "2138",
   "pn": "2141",
   "abstract": [
    "Spectrogram factorisation using a dictionary of spectro-temporal atoms has been successfully employed to separate a mixed audio signal into its source components. When atoms from multiple sources are included in a combined dictionary, the relative weights of activated atoms reveal likely sources as well as the content of each source. Enforcing sparsity on the activation weights produces solutions, where only a small number of atoms are active at a time. In this paper we propose using group sparsity to restrict simultaneous activation of sources, allowing us to discover the identity of an unknown speaker from multiple candidates, and further to recognise the phonetic content more reliably with a narrowed down subset of atoms belonging to the most likely speakers. An evaluation on the CHiME corpus shows that the use of group sparsity improves the results of noise robust speaker identification and speech recognition using speaker-dependent models.\n",
    "Index Terms: group sparsity, speech recognition, speaker identification, spectrogram factorization\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-571"
  },
  "sun12e_interspeech": {
   "authors": [
    [
     "Yang",
     "Sun"
    ],
    [
     "Bert",
     "Cranen"
    ],
    [
     "Jort F.",
     "Gemmeke"
    ],
    [
     "Lou",
     "Boves"
    ],
    [
     "Louis ten",
     "Bosch"
    ],
    [
     "Mathew M.",
     "Doss"
    ]
   ],
   "title": "Using sparse classification outputs as feature observations for noise-robust ASR",
   "original": "i12_2142",
   "page_count": 4,
   "order": 584,
   "p1": "2142",
   "pn": "2145",
   "abstract": [
    "Sparse Classification (SC) is an exemplar-based approach to Automatic Speech Recognition. By representing noisy speech as a sparse linear combination of speech and noise exemplars, SC allows separating speech from noise. The approach has shown its robustness in noisy conditions, but at the cost of degradation in clean conditions. In this work, rather than using the state probability estimates obtained with SC directly in a Viterbi decoding, the probability distributions of SC are modeled by Gaussian Mixture Models (GMMs), for which purpose we introduce a novel whitening transformation. Results on the AURORA-2 task show that our proposed approach is especially effective in clean speech and in the matched noise conditions in test set A. Except in the -5 dB SNR condition we also find substantial improvements in the non-matched noise conditions in test set B.\n",
    "Index Terms: template-based ASR, noise robustness, speech modeling\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-572"
  },
  "soldo12_interspeech": {
   "authors": [
    [
     "Serena",
     "Soldo"
    ],
    [
     "Mathew",
     "Magimai-Doss"
    ],
    [
     "Hervé",
     "Bourlard"
    ]
   ],
   "title": "Synthetic references for template-based ASR using posterior features",
   "original": "i12_2146",
   "page_count": 4,
   "order": 585,
   "p1": "2146",
   "pn": "2149",
   "abstract": [
    "Recently, the use of phoneme class-conditional probabilities as features (posterior features) for template-based ASR has been proposed. These features have been found to generalize well to unseen data and yield better systems than standard spectral-based features. In this paper, motivated by the high quality of current text-to-speech systems and the robustness of posterior features toward undesired variability, we investigate the use of synthetic speech to generate reference templates. The use of synthetic speech in template-based ASR not only allows to address the issue of in-domain data collection but also expansion of vocabulary. Using 75- and 600-word task-independent and speakerindependent setup on Phonebook database, we investigate different synthetic voices produced by the Festival HTSbased synthesizer trained on CMU ARCTIC databases. Our study shows that synthetic speech templates can yield performance comparable to the natural speech templates, especially with synthetic voices that have high intelligibility.\n",
    "Index Terms: Speech recognition, template-based approach, posterior features, synthetic reference templates\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-573"
  },
  "wang12l_interspeech": {
   "authors": [
    [
     "Dong",
     "Wang"
    ],
    [
     "Javier",
     "Tejedor"
    ]
   ],
   "title": "Heterogeneous convolutive non-negative sparse coding",
   "original": "i12_2150",
   "page_count": 4,
   "order": 586,
   "p1": "2150",
   "pn": "2153",
   "abstract": [
    "Convolutive non-negative matrix factorization (CNMF) and its sparse version, convolutive non-negative sparse coding (CNSC), exhibit great success in speech processing. A particular limitation of the current CNMF/CNSC approaches is that the convolution ranges of the bases in learning are identical, resulting in patterns covering the same time-span. This is obvious unideal as most of sequential signals, for example speech, involve patterns with a multitude of time spans. This paper extends the CNMF/CNSC algorithm and presents a heterogeneous learning approach which can learn bases with non-uniformed convolution ranges. The validity of this extension is demonstrated with a simple speech separation task.\n",
    "Index Terms: non-negative matrix factorization, sparse coding, speech processing\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-574"
  },
  "geiger12_interspeech": {
   "authors": [
    [
     "Jürgen T.",
     "Geiger"
    ],
    [
     "Ravichander",
     "Vipperla"
    ],
    [
     "Simon",
     "Bozonnet"
    ],
    [
     "Nicholas",
     "Evans"
    ],
    [
     "Björn",
     "Schuller"
    ],
    [
     "Gerhard",
     "Rigoll"
    ]
   ],
   "title": "Convolutive non-negative sparse coding and new features for speech overlap handling in speaker diarization",
   "original": "i12_2154",
   "page_count": 4,
   "order": 587,
   "p1": "2154",
   "pn": "2157",
   "abstract": [
    "The effective handling of overlapping speech is at the limits of the current state-of-the-art in speaker diarization. This paper presents our latest work in overlap detection. We report the combination of features derived through convolutive non-negative sparse coding and new energy, spectral and voicing-related features within a conventional HMM system. Overlap detection results are fully integrated into our topdown diarization system through the application of overlap exclusion and overlap labelling. Experiments on a subset of the AMI corpus show that the new system delivers significant reductions in missed speech and speaker error. Through overlap exclusion and labelling the overall diarization error rate is shown to improve by 6.4% relative.\n",
    "Index Terms: speech overlap detection, convolutive nonnegative sparse coding, speaker diarization\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-575"
  },
  "martinezgonzalez12_interspeech": {
   "authors": [
    [
     "Beatriz",
     "Martínez-González"
    ],
    [
     "José M.",
     "Pardo"
    ],
    [
     "Julián D.",
     "Echeverry-Correa"
    ],
    [
     "José A.",
     "Vallejo-Pinto"
    ],
    [
     "Roberto",
     "Barra-Chicote"
    ]
   ],
   "title": "Selection of TDOA parameters for MDM speaker diarization",
   "original": "i12_2158",
   "page_count": 4,
   "order": 588,
   "p1": "2158",
   "pn": "2161",
   "abstract": [
    "Several methods to improve multiple distant microphone (MDM) speaker diarization based on Time Delay of Arrival (TDOA) features are evaluated in this paper. All of them avoid the use of a single reference channel to calculate the TDOA values and, based on different criteria, select among all possible pairs of microphones a set of pairs that will be used to estimate the TDOA's. The evaluated methods have been named the \"Dynamic Margin\" (DM), the \"Extreme Regions\" (ER), the \"Most Common\" (MC), the \"Cross Correlation\" (XCorr) and the \"Principle Component Analysis\" (PCA). It is shown that all methods improve the baseline results for the development set and four of them improve also the results for the evaluation set. Improvements of 3.49% and 10.77% DER relative are obtained for DM and ER respectively for the test set. The XCorr and PCA methods achieve an improvement of 36.72% and 30.82% DER relative for the test set. Moreover, the computational cost for the XCorr method is 20% less than the baseline.\n",
    "Index Terms: Speaker diarization, speaker localization, speaker identification, speaker segmentation\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-576"
  },
  "toledoronen12_interspeech": {
   "authors": [
    [
     "Orith",
     "Toledo-Ronen"
    ],
    [
     "Hagai",
     "Aronowitz"
    ]
   ],
   "title": "Confidence for speaker diarization using PCA spectral ratio",
   "original": "i12_2162",
   "page_count": 4,
   "order": 589,
   "p1": "2162",
   "pn": "2165",
   "abstract": [
    "Confidence scoring is an important component in speaker diarization systems, both for offline speech analytics and for online diarization that are require to produce the speaker segmentation from very little audio. This paper proposes a confidence measure for speaker diarization based on the spectral ratio of the eigenvalues of the Principal Component Analysis (PCA) transformation computed on the pre-segmented audio before diarization is performed on the conversation. We tested our method on two-speaker data and our results show the effectiveness of the PCA's spectral ratio confidence measure for both offline and online diarization. We compare and contrast our proposed confidence measure with other clustering validation methods that provide a quantitative measure of the segmentation quality but are calculated on the segmented data after diarization is performed, and with a related approach that extracts a confidence from the PCA of the pre-segmented audio.\n",
    "Index Terms: speaker diarization, principle component analysis, confidence measure\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-577"
  },
  "tawara12_interspeech": {
   "authors": [
    [
     "Naohiro",
     "Tawara"
    ],
    [
     "Tetsuji",
     "Ogawa"
    ],
    [
     "Shinji",
     "Watanabe"
    ],
    [
     "Atsushi",
     "Nakamura"
    ],
    [
     "Tetsunori",
     "Kobayashi"
    ]
   ],
   "title": "Fully Bayesian speaker clustering based on hierarchically structured utterance-oriented Dirichlet process mixture model",
   "original": "i12_2166",
   "page_count": 4,
   "order": 590,
   "p1": "2166",
   "pn": "2169",
   "abstract": [
    "We proposed a novel Bayesian speaker clustering method based on a nonparametric Bayesian model which has a hierarchical structure. We carried out preliminary speaker clustering experiments with the conventional hierarchical agglomerative clustering based on Bayesian information criterion (AHC-BIC). Experimental result showed that the proposed method was effective to the data in which the number of utterances varied from speaker to speaker, while the conventional method caused significant degradation in clustering accuracy for these data.\n",
    "Index Terms Speaker clustering, nonparametric Bayesian model, Gibbs sampling, utterance-oriented Dirichlet process mixture model.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-578"
  },
  "vijayasenan12_interspeech": {
   "authors": [
    [
     "Deepu",
     "Vijayasenan"
    ],
    [
     "Fabio",
     "Valente"
    ]
   ],
   "title": "Diartk: an open source toolkit for research in multistream speaker diarization and its application to meetings recordings",
   "original": "i12_2170",
   "page_count": 4,
   "order": 591,
   "p1": "2170",
   "pn": "2173",
   "abstract": [
    "The speaker diarization task consists in inferring \"who spoke when\" in an audio stream without any prior knowledge and has been object of several NIST international evaluation campaigns is last years. A common trend for improving performances has been the use of several different feature streams as diverse as speaker location features, visual features or noise robust acoustic features. This paper describes an open source toolkit released under GPL license aiming at facilitating research in multistream speaker diarization and reproducing state-of-the-art results. In contrary to other related diarization toolkits, it is explicitly designed to handle an arbitrary number of features with very different statistics while limiting the computational complexity. The release includes a set of recipes scripts to replicate benchmark results on previous NIST evaluations and is intended to provide an easy to use software to study and include novel features into diarization systems.\n",
    "Index Terms: Open Source toolkit, Speaker Diarization, multistream features, NIST Rich Transcription\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-579"
  },
  "dupuy12_interspeech": {
   "authors": [
    [
     "Grégor",
     "Dupuy"
    ],
    [
     "Mickael",
     "Rouvier"
    ],
    [
     "Sylvain",
     "Meignier"
    ],
    [
     "Yannick",
     "Estève"
    ]
   ],
   "title": "I-vectors and ILP clustering adapted to cross-show speaker diarization",
   "original": "i12_2174",
   "page_count": 4,
   "order": 592,
   "p1": "2174",
   "pn": "2177",
   "abstract": [
    "We propose to study speaker diarization from a collection of audio documents. The goal is to detect speakers appearing in several shows. In our approach, each show of the collection is processed separately before being processed collectively, to group speakers involved in several shows. Two clustering methods are studied for the overall processing of the collection: one uses the NCLR metric and the other is inspired by techniques based on i-vectors, mainly used in the speaker verification field. Both methods were evaluated on the whole training corpus of ESTER 2. The method based on the use of i-vectors achieves error rates similar to those obtained by the NCLR method, however, the computation time is on average 7.46 times faster. Therefore, this method is suitable f or processing large volumes of data.\n",
    "Index Terms: speaker diarization, cross-show diarization, i-vectors, ilp clustering.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-580"
  },
  "israel12_interspeech": {
   "authors": [
    [
     "Assaf",
     "Israel"
    ],
    [
     "Michael",
     "Proctor"
    ],
    [
     "Louis",
     "Goldstein"
    ],
    [
     "Khalil",
     "Iskarous"
    ],
    [
     "Shrikanth",
     "Narayanan"
    ]
   ],
   "title": "Emphatic segments and emphasis spread in Lebanese Arabic: a real-time magnetic resonance imaging study",
   "original": "i12_2178",
   "page_count": 4,
   "order": 593,
   "p1": "2178",
   "pn": "2181",
   "abstract": [
    "Production of emphatic consonants by a speaker of Lebanese Arabic was examined using real-time magnetic resonance imaging (rtMRI). Emphatic consonants were found to be articulated with a more depressed and retracted tongue body than their non-empatic counterparts, with the narrowest emphatic constriction observed in the upper pharynx. Both progressive and regressive emphasis spread was observed, and was not blocked by a high palatal segment ([y]). Emphaticized segments exhibit similar retraction and depression with magnitudes that vary according to the direction of spreading. These data suggest that emphasis spread may operate in a phonetically-complex way not accounted for by phonological theory, and in addition illustrate the advantage of real-time MRI as a method for studying emphasis in Semitic phonology.\n",
    "Index Terms: speech production, Arabic, emphatic, emphasis spread, pharyngealization, real-time MRI\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-581"
  },
  "shosted12_interspeech": {
   "authors": [
    [
     "Ryan K.",
     "Shosted"
    ],
    [
     "Bradley P.",
     "Sutton"
    ],
    [
     "Abbas",
     "Benmamoun"
    ]
   ],
   "title": "Using magnetic resonance to image the pharynx during Arabic speech: static and dynamic aspects",
   "original": "i12_2182",
   "page_count": 4,
   "order": 594,
   "p1": "2182",
   "pn": "2185",
   "abstract": [
    "Magnetic resonance imaging has been applied only recently to the study of Arabic speech production. Arabic has a relatively large number of sounds produced with constrictions in the pharynx, a part of the vocal anatomy well-suited to investigation using MRI. We show that static 3D MRI techniques can be useful in distinguishing the pharyngeal sounds of Arabic and that average pixel intensity in MRI images can be used to track pharyngeal articulations as a function of time.\n",
    "Index Terms: magnetic resonance, pharynx, pharyngeals, Arabic\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-582"
  },
  "vargas12_interspeech": {
   "authors": [
    [
     "Julián Andrés Valdés",
     "Vargas"
    ],
    [
     "Pierre",
     "Badin"
    ],
    [
     "Laurent",
     "Lamalle"
    ]
   ],
   "title": "Articulatory speaker normalisation based on MRI-data using three-way linear decomposition methods",
   "original": "i12_2186",
   "page_count": 4,
   "order": 595,
   "p1": "2186",
   "pn": "2189",
   "abstract": [
    "The aim of this study was to characterise, to model and to compare the different lingual articulatory strategies of a group of speakers. Individual principal component analysis (PCA) and multi-linear decomposition methods have been applied to different representations of the tongue contour extracted from magnetic resonance images (MRI). The corpus consisted of seven speakers articulating 63 French vowels and consonants. On the average, over the seven speakers, the Root Mean Square prediction Error (RMSE) was 0.12 cm accounting for a percentage of variance explanation of 92.6% for the individual PCA, using 4 components. Several Multi-linear decomposition methods, to model the tongue contour with a single set of components, have been performed and compared. The 2-Level-PCA gave the best results among the other techniques. By means of a Student's t-test, at 5% of significance level, we found that 2-level-PCA equals the PCA performance with 11 components to represent 91% of the variance explanation with a RMSE of 0.11 cm. While the same method, with 4 components, represents 75% of the variance explanation with a RMSE of 0.19 cm.\n",
    "Index Terms: Articulatory modelling, speaker normalisation, factor analysis, MRI\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-583"
  },
  "arai12_interspeech": {
   "authors": [
    [
     "Takayuki",
     "Arai"
    ]
   ],
   "title": "Vowels produced by sliding three-tube model with different lengths",
   "original": "i12_2190",
   "page_count": 4,
   "order": 596,
   "p1": "2190",
   "pn": "2193",
   "abstract": [
    "The sliding three-tube (S3T) model, based on Fant's acoustic theory and proposed in our previous studies, has a simple structure, enabling it to produce human-like vowels useful for education in acoustics and speech science. In this study, we changed the size of the S3T model and combined it with sound sources with different fundamental frequencies. We confirmed that the models could produce vowels of different speaker types. We were able to retain good vowel quality for a perceptual study when we simultaneously shortened vocal-tract length and increased fundamental frequency. We also discussed the models in a new way, comparing children's and adults' vowels, especially for educational purposes.\n",
    "Index Terms: physical models of the human vocal tract, vowel production, education in acoustics, speech science\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-584"
  },
  "kaburagi12_interspeech": {
   "authors": [
    [
     "Tokihiko",
     "Kaburagi"
    ],
    [
     "Tetsuro",
     "Takano"
    ],
    [
     "Yuki",
     "Sakamoto"
    ]
   ],
   "title": "Estimating the vocal-tract area function from formants using a sensitivity function and least square",
   "original": "i12_2194",
   "page_count": 4,
   "order": 597,
   "p1": "2194",
   "pn": "2197",
   "abstract": [
    "We present a method for estimating the vocal-tract area function from specified formant frequencies. The method extends the work of Story (J.A.S.A., 119, 715-718, 1996) based on a sensitivity function representing the change in the formant frequency due to a small perturbation of the cross-sectional area of the vocal tract. Our method estimates the vocal-tract shape through an iterative procedure in which the sensitivity function is used as the basis function to gradually optimize the cross-sectional area that produces the target formant frequencies. In addition, the summing weight of sensitivity functions is determined by minimizing an objective function representing the relative frequency error of every format. We conducted numerical experiments using area function data of English vowels. Results showed that our method can estimate the vocal-tract shape with satisfactory accuracy. In addition, the number of iterative calculations is significantly lower than with Story's original method.\n",
    "Index Terms: formant frequency, vocal-tract area function, sensitivity function, inverse estimation\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-585"
  },
  "lucero12_interspeech": {
   "authors": [
    [
     "Jorge C.",
     "Lucero"
    ],
    [
     "Laura L.",
     "Koenig"
    ],
    [
     "Susanne",
     "Fuchs"
    ]
   ],
   "title": "Modeling source-tract interaction in speech production: voicing onset vs. vowel height after a voiceless obstruent",
   "original": "i12_2198",
   "page_count": 4,
   "order": 598,
   "p1": "2198",
   "pn": "2201",
   "abstract": [
    "This paper shows that observed higher values of intraoral pressure at voicing onset of higher vowels vs. low vowels might be consequence of the acoustical coupling between the vocal fold oscillation and the vocal tract. The acoustical coupling is characterized by a simple model based on a lumped description of tissue mechanics, quasi-steady flow and one dimensional acoustics. The model predicts a \"U\"-shaped relation between the transglottal pressure at voicing onset and the ratio of voice fundamental frequency to the first formant, which reproduces the data with good approximation.\n",
    "Index Terms: voicing onset, transglottal pressure, acoustical coupling, vowel height\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-586"
  },
  "bollepalli12_interspeech": {
   "authors": [
    [
     "Bajibabu",
     "Bollepalli"
    ],
    [
     "Alan W.",
     "Black"
    ],
    [
     "Kishore",
     "Prahallad"
    ]
   ],
   "title": "Modelling a noisy-channel for voice conversion using articulatory features",
   "original": "i12_2202",
   "page_count": 4,
   "order": 599,
   "p1": "2202",
   "pn": "2205",
   "abstract": [
    "In this paper, we propose modeling a noisy-channel for the task of voice conversion (VC). We have used the artificial neural networks (ANN) to capture speaker-specific characteristics of a target speaker which avoids the need for any training utterance from a source speaker. We use articulatory features (AFs) as canonical form or speaker-independent representation of speech signal. Our studies show that AFs contain significant amount of speaker information in their trajectories. Suitable techniques are proposed to normalize the speaker-specific information in AF trajectories and the resultant AFs are used in voice conversion. The results of voice conversion evaluated using objective and subjective measures confirm that speaker-specific characteristics of target speaker could be captured.\n",
    "Index Terms: voice conversion, articulatory features, noisy-channel model, speaker-independent representation\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-587"
  },
  "janska12_interspeech": {
   "authors": [
    [
     "Anna C.",
     "Janska"
    ],
    [
     "Erich",
     "Schröger"
    ],
    [
     "Thomas",
     "Jacobsen"
    ],
    [
     "Robert A. J.",
     "Clark"
    ]
   ],
   "title": "Asymmetries in the perception of synthesized speech",
   "original": "i12_2206",
   "page_count": 4,
   "order": 600,
   "p1": "2206",
   "pn": "2209",
   "abstract": [
    "It was previously observed [1] that the order of presentation of paired stimuli influenced the number of different responses in same-different tasks in speech synthesis evaluation. This paper investigates this phenomenon within the context of cognitive psychology and demonstrates that, as the cognitive psychology literature suggests, there is an effect relating to the prototypicality of the stimulus.\n",
    "Index Terms: speech synthesis, evaluation, perception, Blizzard Challenge\n",
    "",
    "",
    "Janska, A.C., “Further Investigation of MDS as a Tool for Evaluation of Speech Quality of Synthesized Speech”, MSc Dissertation, The University of Edinburgh, 2009. Online: www.era.lib.ed.ac.uk/handle/1842/3624\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-588"
  },
  "greene12_interspeech": {
   "authors": [
    [
     "Erica",
     "Greene"
    ],
    [
     "Taniya",
     "Mishra"
    ],
    [
     "Patrick",
     "Haffner"
    ],
    [
     "Alistair",
     "Conkie"
    ]
   ],
   "title": "Predicting character-appropriate voices for a TTS-based storyteller system",
   "original": "i12_2210",
   "page_count": 4,
   "order": 601,
   "p1": "2210",
   "pn": "2213",
   "abstract": [
    "Using distinct and appropriate synthetic voices to voice the characters in a children's story would make a TTS-based digital storyteller system more engaging and entertaining, and also help listeners comprehend the story better. However, automatically predicting appropriate voices for storybook characters is a non-trivial problem.   In this paper, we present a data-driven approach towards predicting the most appropriate voices for different characters in children's stories based on salient character attributes. We use Mechanical Turk to identify the character attributes that are most salient in evoking the listeners' perception that a specific character should have a particular voice, and to label the voices in our collection with attribute tags. Naive Bayes was used to model the attribute-to-voice relationship. Our system was evaluated objectively, and significantly above chance results show our approach to be viable.\n",
    "Index Terms: Speech synthesis, TTS, expressive speech, childdirected speech applications.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-589"
  },
  "sorin12_interspeech": {
   "authors": [
    [
     "Alexander",
     "Sorin"
    ],
    [
     "Slava",
     "Shechtman"
    ],
    [
     "Vincent",
     "Pollet"
    ]
   ],
   "title": "Psychoacoustic segment scoring for multi-form speech synthesis",
   "original": "i12_2214",
   "page_count": 4,
   "order": 602,
   "p1": "2214",
   "pn": "2217",
   "abstract": [
    "In multi-form segment synthesis, output speech is constructed by splicing waveform segments with statistically modeled and regenerated parametric speech segments. The fraction of model-derived segments is called model-template ratio. The motivation of this work is to further increase flexibility of multi-form synthesis maintaining high speech quality for high model-template ratios. An approach is presented where the representation type of a segment is selected per acoustic leaf. We introduce a novel method for leaf representation selection based on a psychoacoustic segment stationarity score. Additionally, refinements in multi-form segment concatenation including boundary constrained statistical parametric synthesis and timedomain alignment based on multi-peak analysis of cross-correlation for high model-template ratio multi-form synthesis are presented.\n",
    "Index Terms: speech synthesis, multi-form segments, speech stationarity, psychoacoustic segment scoring, statistical parametric synthesis, segment concatenation\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-590"
  },
  "bailly12_interspeech": {
   "authors": [
    [
     "Gérard",
     "Bailly"
    ],
    [
     "Cécilia",
     "Gouvernayre"
    ]
   ],
   "title": "Pauses and respiratory markers of the structure of book reading",
   "original": "i12_2218",
   "page_count": 4,
   "order": 603,
   "p1": "2218",
   "pn": "2221",
   "abstract": [
    "The automatic reading of books by text-to-speech synthesizers requires not only the adequate encoding of the many levels of information and discourse structures by acoustic signals but also the proper patterns of breathing, so that to pace information and organize discourse at an ecological rhythm.   We analyze here the locations and durations of more than 2000 pauses produced by voice donor who has read several audiobooks, freely available via the litteratureaudio.com website. Since the voice was recorded by a close microphone, we also characterized the acoustic markers of inhalation and show that the delay between end of phonation and air intake can be considered an additional marker of thematic continuity between the two adjacent speech chunks that complements well-documented prosodic cues such as the preboundary tone or the pause duration.\n",
    "Index Terms: prosody, pause, respiration, prediction of pause locations and durations\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-591"
  },
  "potard12_interspeech": {
   "authors": [
    [
     "Blaise",
     "Potard"
    ],
    [
     "Matthew P.",
     "Aylett"
    ],
    [
     "Christopher J.",
     "Pidcock"
    ]
   ],
   "title": "Proper name splicing in computer games with TTS",
   "original": "i12_2222",
   "page_count": 4,
   "order": 604,
   "p1": "2222",
   "pn": "2225",
   "abstract": [
    "Building high quality synthesis systems with open domain vocabulary and a small audio database is a challenging problem, even when the targeted application is well constrained. Monophone unit concatenation (as opposed to diphone) is an approach that can compensate for the poor unit coverage that a small database implies. However, joining at phone boundaries is a delicate task that requires accurate targeting. In this paper, we present an automatically trained targeting system based on the parametric synthesiser HTS, and compare it to a concatenative monophone system and a baseline concatenative diphone system. We apply a novel evaluation methodology which includes a qualitative component, and allows for fast incremental development of synthesis systems. Preliminary results show that although the hybrid system performed significantly more poorly on out of database items, it is less affected by segmentation errors than the monophone system.\n",
    "Index Terms: hybrid speech synthesis, unit selection, evaluation of TTS systems\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-592"
  },
  "mehrabani12b_interspeech": {
   "authors": [
    [
     "Mahnoosh",
     "Mehrabani"
    ],
    [
     "John H. L.",
     "Hansen"
    ]
   ],
   "title": "Speaker clustering for a mixture of singing and reading",
   "original": "i12_2258",
   "page_count": 4,
   "order": 605,
   "p1": "2258",
   "pn": "2261",
   "abstract": [
    "In this study, we propose a speaker clustering algorithm based on reading and singing speech samples for each speaker. As a speaking style, singing introduces changes in the time-frequency structure of a speaker's voice. The purpose of this study is to introduce advancements into speech systems such as speech indexing and retrieval which improve robustness to intrinsic variations in speech production. Clustering is performed within a GMM mean supervector space. The proposed method includes two stages: first, initial clusters are obtained using traditional clustering techniques such as k-means, and hierarchical. Next, each cluster is refined in a PLDA subspace resulting in a more speaker dependent representation that is less sensitive to speaking style. The proposed algorithm improves the average clustering accuracy of the k-means baseline by +9.3% absolute.\n",
    "Index Terms: speaker clustering, singing\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-593"
  },
  "ghosh12_interspeech": {
   "authors": [
    [
     "Sayan",
     "Ghosh"
    ],
    [
     "Thippur V.",
     "Sreenivas"
    ]
   ],
   "title": "Automatic speech segmentation using probabilistic latent component modeling",
   "original": "i12_2262",
   "page_count": 4,
   "order": 606,
   "p1": "2262",
   "pn": "2265",
   "abstract": [
    "Latent variable methods, such as PLCA (Probabilistic Latent Component Analysis) have been successfully used for analysis of non-negative signal representations.In this paper, we formulate PLCS (Probabilistic Latent Component Segmentation), which models each time frame of a spectrogram as a spectral distribution. Given the signal spectrogram, the segmentation boundaries are estimated using a maximum-likelihood approach. For an efficient solution, the algorithm imposes a hard constraint that each segment is modelled by a single latent component. The hard constraint facilitates the solution of ML boundary estimation using dynamic programming. The PLCS framework does not impose a parametric assumption unlike earlier ML segmentation techniques. PLCS can be naturally extended to model coarticulation between successive phones. Experiments on the TIMIT corpus show that the proposed technique is promising compared to most state of the art speech segmentation algorithms.\n",
    "Index Terms: Speech segmentation, PLCA, Spectrograms, Coarticulation, Dynamic Programming\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-594"
  },
  "dennis12_interspeech": {
   "authors": [
    [
     "Jonathan",
     "Dennis"
    ],
    [
     "Huy Dat",
     "Tran"
    ],
    [
     "Eng Siong",
     "Chng"
    ]
   ],
   "title": "Overlapping sound event recognition using local spectrogram features with the generalised hough transform",
   "original": "i12_2266",
   "page_count": 4,
   "order": 607,
   "p1": "2266",
   "pn": "2269",
   "abstract": [
    "We present a novel approach for recognition of overlapping sound events based on the Generalised Hough Transform (GHT) - a technique commonly used for object recognition in the domain of image processing. Unlike our previous work on image-based sound event classification, where we focussed on global image features, here we extract local features from detected interest-points in the spectrogram. These form a robust representation of the local region, and when the information from all interest-points in the spectrogram are combined using the GHT, we can form hypotheses for the location of one or more overlapping sound events in the image. Our experiments show promising results, and demonstrate the ability of our approach to recognise overlapping sounds.\n",
    "Index Terms: overlapping, sound events, recognition\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-595"
  },
  "kalinli12_interspeech": {
   "authors": [
    [
     "Ozlem",
     "Kalinli"
    ]
   ],
   "title": "Automatic phoneme segmentation using auditory attention features",
   "original": "i12_2270",
   "page_count": 4,
   "order": 608,
   "p1": "2270",
   "pn": "2273",
   "abstract": [
    "Segmentation of speech into phonemes is beneficial for many spoken language processing applications. Here, a novel method which uses auditory attention features for detecting phoneme boundaries from acoustic signal is proposed. The proposed phoneme segmentation method does not require transcription or acoustic models of phonemes. The auditory attention cues are biologically inspired and capture changes in sound characteristics by using 2D spectro-temporal receptive filters. When tested on TIMIT, it is shown that the proposed method successfully predicts phoneme boundaries and performs better than the state-of-the art phoneme segmentation methods.\n",
    "Index Terms: speech segmentation, phoneme boundary detection, auditory attention model.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-596"
  },
  "kua12_interspeech": {
   "authors": [
    [
     "Jia Min Karen",
     "Kua"
    ],
    [
     "Tharmarajah",
     "Thiruvaran"
    ],
    [
     "Eliathamby",
     "Ambikairajah"
    ]
   ],
   "title": "A non-uniform filterbank for speaker recognition",
   "original": "i12_2274",
   "page_count": 4,
   "order": 609,
   "p1": "2274",
   "pn": "2277",
   "abstract": [
    "It is known that speaker-specific information is distributed non-uniformly in the frequency domain. Current speaker recognition systems utilize auditory-motivated scales for extracting acoustic features. These scales, however, are not optimised to exploit the spectral distribution of speaker-specific information and hence may not be the optimal choice for speaker recognition. In this paper, we studied the distribution of speaker-specific information in Spectral Centroid Frequency feature, and a non-uniform filter bank is proposed to capture the speaker-specific information effectively. We used F-ratio and Kullback-Leibler (KL) distance to measure distribution of speaker-specific information and we empirically showed that KL distance is better than F-ratio in measuring discriminative ability. The proposed filterbank emphasises the high KL distance regions by allocating more filters in those regions. Experimental results showed a relative EER reduction of 8.8% over the Mel-scale filterbank on NIST2006 SRE database.\n",
    "Index Terms: speaker recognition, F-ratio, Kullback-Leibler distance, Spectral centroid frequency\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-597"
  },
  "lorenzotrueba12b_interspeech": {
   "authors": [
    [
     "Jaime",
     "Lorenzo-Trueba"
    ],
    [
     "Beatriz",
     "Martinez-Gonzalez"
    ],
    [
     "Veronica",
     "Lopez–Ludeña"
    ],
    [
     "Roberto",
     "Barra-Chicote"
    ],
    [
     "Javier",
     "Ferreiros"
    ],
    [
     "Junichi",
     "Yamagishi"
    ],
    [
     "Juan M.",
     "Montero"
    ]
   ],
   "title": "Towards an unsupervised speaking style voice building framework: multi.style speaker diarization",
   "original": "i12_2278",
   "page_count": 4,
   "order": 610,
   "p1": "2278",
   "pn": "2281",
   "abstract": [
    "Current text.to.speech systems are developed using studio-recorded speech in a neutral style or based on acted emotions. However, the proliferation of media sharing sites would allow developing a new generation of speech.based systems which could cope with spontaneous and styled speech. This paper proposes an architecture to deal with realistic recordings and carries out some experiments on unsupervised speaker diarization. In order to maximize the speaker purity of the clusters while keeping a high speaker coverage, the paper evaluates the F-measure of a diarization module, achieving high scores (>85%) especially when the clusters are longer than 30 seconds, even for the more spontaneous and expressive styles (such as talk shows or sports).\n",
    "Index Terms: expressive speech synthesis, speaker diarization, speaking styles, voice cloning.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-598"
  },
  "mohammadi12b_interspeech": {
   "authors": [
    [
     "Seyed Hamidreza",
     "Mohammadi"
    ],
    [
     "Hossein",
     "Sameti"
    ],
    [
     "Mahsa Sadat Elyasi",
     "Langarani"
    ],
    [
     "Amirhossein",
     "Tavanaei"
    ]
   ],
   "title": "KNNDIST: a non-parametric distance measure for speaker segmentation",
   "original": "i12_2282",
   "page_count": 4,
   "order": 611,
   "p1": "2282",
   "pn": "2285",
   "abstract": [
    "A novel distance measure for distance-based speaker segmentation is proposed. This distance measure is non- parametric, in contrast to common distance measures used in speaker segmentation systems, which often assume a Gaussian distribution when measuring the distance between1two audio segments. This distance measure is essentially a k-nearest- neighbor distance measure. Non-vowel segment removal in pre- processing stage is also proposed. Speaker segmentation performance is tested on artificially created conversations from the TIMIT database and two AMI conversations. For short window lengths, Missed Detection Rated is decreased significantly. For moderate window lengths, a decrease in both Missed Detection and False Alarm Rates occur. The computational cost of the distance measure is high for long window lengths.\n",
    "Index Terms: speaker segmentation, distance measure, k-nearest-neighbor\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-599"
  },
  "feng12_interspeech": {
   "authors": [
    [
     "Wei",
     "Feng"
    ],
    [
     "Xuecheng",
     "Nie"
    ],
    [
     "Liang",
     "Wan"
    ],
    [
     "Lei",
     "Xie"
    ],
    [
     "Jianmin",
     "Jiang"
    ]
   ],
   "title": "Lexical story co-segmentation of Chinese broadcast news",
   "original": "i12_2286",
   "page_count": 4,
   "order": 612,
   "p1": "2286",
   "pn": "2289",
   "abstract": [
    "We present an unsupervised technique, namely story co-segmentation, to automatically extract the common stories on the same topic within a pair of Chinese broadcast news transcripts. Unlike classical topic tracking that usually relies on previously trained topic models, our method is purely data-driven and is able to simultaneously determine the common stories of the input texts. Specifically, we propose an iterative four-step MRF solution to the problem of story co-segmentation using lexical cues only. We first construct a sentence-level graph formulation of the input news transcripts, and initialize foreground and background labeling by lexical clustering. We then update both foreground and background models based on the current labeling. We formalize story co-segmentation as a Gibbs energy minimization problem that balances the optimal objectives of foreground/background likelihood, intra-doc coherence, and inter-doc similarity. Finally, the labeling refinement is obtained by hybrid optimization with QPBO and BP. The effectiveness of our method has been validated on real-world CCTV corpus.\n",
    "Index Terms: story co-segmentation, foreground and background story modeling, lexical clustering, MRF, QP- BO, belief propagation (BP)\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-600"
  },
  "karnjanadecha12_interspeech": {
   "authors": [
    [
     "Montri",
     "Karnjanadecha"
    ],
    [
     "Stephen A.",
     "Zahorian"
    ]
   ],
   "title": "Toward an optimum feature set and HMM model parameters for automatic phonetic alignment of spontaneous speech",
   "original": "i12_2290",
   "page_count": 4,
   "order": 613,
   "p1": "2290",
   "pn": "2293",
   "abstract": [
    "Many speech segmentation techniques have been proposed to automate phonetic alignment. Most of the techniques require, however, labeled data to train, and perform well only for read, high-quality speech. Automatic phonetic alignment, for lower quality varied data with no labeled training data, the subject of this paper, is a much more challenging domain. An HMM-based automatic speech recognizer was used in this study to determine phonetic sequences and boundaries of \"open source\" speech data, retrieved from public websites. The HMM models were initially trained using the TIMIT database and subsequently adapted to each passage. Standard frontend features such as MFCC, LPCC and PLP, and features computed by applying the DCT directly to the short-time spectrum (DCTC) were evaluated using TIMIT data. The \"best\" parameter set was found to be DCTC_78 and these parameters were used to align the speech data of interest.\n",
    "Index Terms: speech segmentation, phonetic alignment, speech recognition\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-601"
  },
  "vertanen12_interspeech": {
   "authors": [
    [
     "Keith",
     "Vertanen"
    ],
    [
     "Per Ola",
     "Kristensson"
    ]
   ],
   "title": "Spelling as a complementary strategy for speech recognition",
   "original": "i12_2294",
   "page_count": 4,
   "order": 614,
   "p1": "2294",
   "pn": "2297",
   "abstract": [
    "We compare a variety of strategies for incorporating spelling to create more robust voice-only speech interfaces. These strategies use different combinations of speaking the word, spelling the word, and spelling the word using a phonetic alphabet. For correcting a single recognition error, spelling the word or speaking and spelling the word reduced error rates substantially. Phonetic-spelling was very accurate with error rates on a 5K task approaching zero. Most importantly, multiple input strategies could be used simultaneously with only a modest degradation in performance compared to allowing only a single input strategy. Thus our work shows that spelling-based input strategies offer the potential of a simple, natural and effective way for users to both avoid and correct recognition errors.\n",
    "Index Terms: speech recognition, error correction\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-602"
  },
  "schlippe12_interspeech": {
   "authors": [
    [
     "Tim",
     "Schlippe"
    ],
    [
     "Sebastian",
     "Ochs"
    ],
    [
     "Ngoc Thang",
     "Vu"
    ],
    [
     "Tanja",
     "Schultz"
    ]
   ],
   "title": "Automatic error recovery for pronunciation dictionaries",
   "original": "i12_2298",
   "page_count": 4,
   "order": 615,
   "p1": "2298",
   "pn": "2301",
   "abstract": [
    "In this paper, we present our latest investigations on pronunciation modeling and its impact on ASR. We propose completely automatic methods to detect, remove, and substitute inconsistent or flawed entries in pronunciation dictionaries. The experiments were conducted on different tasks, namely (1) word-pronunciation pairs from the Czech, English, French, German, Polish, and Spanish Wiktionary [1], a multilingual wiki-based open content dictionary, (2) our GlobalPhone Hausa pronunciation dictionary [2], and (3) pronunciations to complement our Mandarin-English SEAME code-switch dictionary [3]. In the final results, we fairly observed on average an improvement of 2.0% relative in terms of word error rate and even 27.3% for the case of English Wiktionary word-pronunciation pairs.\n",
    "Index Terms: pronunciation dictionaries, automatic error recovery, multilingual speech recognition\n",
    "s “Wiktionary - a wiki-based open content dictionary”, Website, http://www.wiktionary.org. Schlippe, T., Komgang Djomgang, E. G., Vu, N. T., Ochs, S., and Schultz, T., “Hausa Large Vocabulary Continuous Speech Recognition”, SLTU, 2012 Vu, T., Lyu, D.-C., Weiner, J., Telaar, D., Schlippe, T., Blaicher, F., Chng, E.-S., Schultz, T., and Li, H., “A First Speech Recognition System For Mandarin-English Code-Switch Conversational Speech”, ICASSP, 2012.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-603"
  },
  "senay12_interspeech": {
   "authors": [
    [
     "Grégory",
     "Senay"
    ],
    [
     "Georges",
     "Linarès"
    ]
   ],
   "title": "Confidence measure for speech indexing based on latent dirichlet allocation",
   "original": "i12_2302",
   "page_count": 4,
   "order": 616,
   "p1": "2302",
   "pn": "2305",
   "abstract": [
    "This paper presents a confidence measure for speech indexing that aims to predict the indexing quality of a speech document for a Spoken Document Retrieval (SDR) task. We first introduce how the indexing quality of a speech document is evaluated. Then, we present our method to predict the indexing quality of a speech document. It is based on confidence measure provided by an automatic speech recognition system and the detection of semantic outliers implemented with the Latent Dirichlet Allocation (LDA) model. Experiments are conducted on the French Broadcast news campaign ESTER2 in a classical SDR scenario where users submit text-queries to a search engine. Results demonstrate an overall improvement when the detection is done with the LDA model. The detection rate is always above 70%.\n",
    "Index Terms: speech indexing, confidence measure, spoken document retrieval, latent dirichlet allocation\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-604"
  },
  "cerisara12_interspeech": {
   "authors": [
    [
     "Christophe",
     "Cerisara"
    ],
    [
     "Alejandra",
     "Lorenzo"
    ]
   ],
   "title": "Mixed probabilistic and deterministic dependency parsing",
   "original": "i12_2306",
   "page_count": 4,
   "order": 617,
   "p1": "2306",
   "pn": "2309",
   "abstract": [
    "This work describes a new multi-stage dependency parsing framework that relies on stochastic probabilistic models, such as the Maximum- Entropy Markov Model. It proposes an original compromise between locally optimal parsers with global features, and globally optimal models with local features. The main advantage of this framework is its ability to choose the desired compromise over the full range between both extreme models, by modifying the topology of the underlying automaton. Thanks to its probabilistic definition, it further gives access to several powerful classical probabilistic algorithms, and in particular to marginalization and Bayesian inference of, for instance, missing or corrupted observations. The rank-1 model has been evaluated on a French broadcast news parsing task, and has obtained comparable performance to state-of-the-art transition-based parsers.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-605"
  },
  "yamahata12_interspeech": {
   "authors": [
    [
     "Shoko",
     "Yamahata"
    ],
    [
     "Yoshikazu",
     "Yamaguchi"
    ],
    [
     "Atsunori",
     "Ogawa"
    ],
    [
     "Hirokazu",
     "Masataki"
    ],
    [
     "Osamu",
     "Yoshioka"
    ],
    [
     "Satoshi",
     "Takahashi"
    ]
   ],
   "title": "Automatic vocabulary adaptation based on semantic similarity and speech recognition confidence measure",
   "original": "i12_2310",
   "page_count": 4,
   "order": 618,
   "p1": "2310",
   "pn": "2313",
   "abstract": [
    "Out-Of-Vocabulary (OOV) word utterances are unavoidable in speech recognition since the vocabulary size of a recognition dictionary is limited. And therefore, automatic vocabulary adaptation, which selects unregistered (i.e. OOV) words from relevant documents and registers them to a dictionary with their proper probability values, is an important technique. To improve recognition accuracy, a vocabulary adaptation method is required to register only relevant words that will actually be spoken in target utterances and not to register words that will not be spoken (i.e. redundant word entries). In this paper, we propose a novel automatic vocabulary adaptation method that satisfies these requirements based on semantic and acoustic similarities. Acoustic similarity is represented in speech recognition confidence measure. Experiments show that, with our method, the word selection accuracy is improved twice and the recognition accuracy focused on newly registered words is improved 15.1% in F-measure, compared with conventional methods.\n",
    "Index Terms: out-of-vocabulary, vocabulary adaptation, semantic similarity, confidence measure\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-606"
  },
  "ward12b_interspeech": {
   "authors": [
    [
     "Nigel G.",
     "Ward"
    ],
    [
     "Alejandro",
     "Vega"
    ]
   ],
   "title": "Towards empirical dialog-state modeling and its use in language modeling",
   "original": "i12_2314",
   "page_count": 4,
   "order": 619,
   "p1": "2314",
   "pn": "2317",
   "abstract": [
    "Inspired by the goal of modeling the dialog state and the speaker's mental state, moment by moment, we apply Principal Component Analysis to a vector of 76 prosodic features spanning 6 seconds of context. This gives a multidimensional representation of the current state, and we find that word probabilities do vary strongly with several of these dimensions, that the use of this information in a language model gives a 15% reduction in perplexity, and that the dimensions do relate to aspects of mental state and dialog state.\n",
    "Index Terms: prosody, context, principal component analysis, perplexity, dimensions, dialog activities\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-607"
  },
  "kubo12b_interspeech": {
   "authors": [
    [
     "Keigo",
     "Kubo"
    ],
    [
     "Hiromichi",
     "Kawanami"
    ],
    [
     "Hiroshi",
     "Saruwatari"
    ],
    [
     "Kiyohiro",
     "Shikano"
    ]
   ],
   "title": "Evaluation of many-to-many alignment algorithm by automatic pronunciation annotation using web text mining",
   "original": "i12_2318",
   "page_count": 4,
   "order": 620,
   "p1": "2318",
   "pn": "2321",
   "abstract": [
    "The need for robust pronunciation annotation over out-of-vocabulary (OOV) words has been increasing with the development of an application that deals with proper nouns and brand-new words, such as Voice Search. In robust pronunciation annotation over OOV words, the alignment between graphemes and phonemes is vital data. For a many-to-many alignment algorithm between graphemes and phonemes, we describe its problems and methods to overcome them. An evaluation experiment of a many-to-many alignment by automatic pronunciation annotation using Web text mining is also performed. That experimental result shows that the proposed many-to-many alignment produces an alignment that has the high generalization ability for OOV words while avoiding degradation of the accuracy of the pronunciation annotation compared with the conventional approach.\n",
    "Index Terms: string alignment, out-of-vocabulary word, pronunciation annotation\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-608"
  },
  "koco12_interspeech": {
   "authors": [
    [
     "Sokol",
     "Koço"
    ],
    [
     "Cécile",
     "Capponi"
    ],
    [
     "Frédéric",
     "Béchet"
    ]
   ],
   "title": "Applying multiview learning algorithms to human-human conversation classification",
   "original": "i12_2322",
   "page_count": 4,
   "order": 621,
   "p1": "2322",
   "pn": "2325",
   "abstract": [
    "We propose in this paper to use a novel multiview boosting- like algorithm called Mumbo for processing human-human con- versations recorded in a call-senter setting. We present how dia- log classification can be seen as a multiview classification prob- lem and we compare the performance of Mumbo and the one of a standard boosting algorithm. The first results obtained on a subset of the DECODA corpus show that a significant improve- ment in classification performance can be achieved, especially on high Word Error Rate transcriptions.\n",
    "Index Terms: human-human conversation, multiview learning, boosting, spoken language understanding\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-609"
  },
  "akita12_interspeech": {
   "authors": [
    [
     "Yuya",
     "Akita"
    ],
    [
     "Makoto",
     "Watanabe"
    ],
    [
     "Tatsuya",
     "Kawahara"
    ]
   ],
   "title": "Automatic transcription of lecture speech using language model based on speaking- style transformation of proceeding texts",
   "original": "i12_2326",
   "page_count": 4,
   "order": 622,
   "p1": "2326",
   "pn": "2329",
   "abstract": [
    "For language modeling of spontaneous speech recognition, we propose a style transformation approach, which transforms written texts to a spoken-style language model. Since these two styles are largely different and thus direct transformation is difficult, we cascade two transformation methods; rule-based transformation to rewrite written-style texts to intermediate polite-style texts, and statistical transformation of language model from polite style to faithful style which is suitable for ASR. In an experimental evaluation on real lecture speech, the proposed transformation approach realized higher performance than conventional linear interpolation method.\n",
    "Index Terms: automatic speech recognition, lecture speech, language model, style transformation\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-610"
  },
  "li12j_interspeech": {
   "authors": [
    [
     "Chen",
     "Li"
    ],
    [
     "Yang",
     "Liu"
    ]
   ],
   "title": "Normalization of text messages using character- and phone-based machine translation approaches",
   "original": "i12_2330",
   "page_count": 4,
   "order": 623,
   "p1": "2330",
   "pn": "2333",
   "abstract": [
    "There are many abbreviation and non-standard words in SMS and Twitter messages. They are problematic for text-to-speech (TTS) or language processing techniques for these data. A character-based machine translation (MT) approach was previously used for normalization of nonstandard words. In this paper, we propose a two-step translation method to leverage phonetic information, where non-standard words are first translated to possible pronunciations, which are then translated to standard words. We further combine it with the single-step character-based translation module. Our experiments show that our proposed method significantly outperforms previous results in both n-best coverage and 1-best accuracy.\n",
    "Index Terms: text normalization, text-to-speech, abbreviation\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-611"
  },
  "azim12_interspeech": {
   "authors": [
    [
     "Aisha S.",
     "Azim"
    ],
    [
     "Xiaoxuan",
     "Wang"
    ],
    [
     "Sim Khe",
     "Chai"
    ]
   ],
   "title": "A weighted combination of speech with text-based models for Arabic diacritization",
   "original": "i12_2334",
   "page_count": 4,
   "order": 624,
   "p1": "2334",
   "pn": "2337",
   "abstract": [
    "The majority of studies on Arabic diacritization have employed textually inferred features alone. This paper proposes a novel approach, where the weighted combination of speech with a text-based model is used to allow linguistically-insensitive acoustic information to correct and complement the errors generated by the text model's diacritic predictions. The acoustic model is based on Hidden Markov Models and the textual model on Conditional Random Fields. The combination brings significant reduction in error rates across all metrics, especially in case endings, which are the most difficult to predict. The results in this paper are the most accurate reported to date, with diacritic and word error rates of 1.5 and 4.9 inclusive of case endings, and 1.0 and 2.7 exclusive of them.\n",
    "Index Terms: Arabic diacritization, case endings, multimodal systems\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-612"
  },
  "seigel12_interspeech": {
   "authors": [
    [
     "Matthew S.",
     "Seigel"
    ],
    [
     "Phillip C.",
     "Woodland"
    ]
   ],
   "title": "Using sub-word-level information for confidence estimation with conditional random field models",
   "original": "i12_2338",
   "page_count": 4,
   "order": 625,
   "p1": "2338",
   "pn": "2341",
   "abstract": [
    "The task of word-level confidence estimation (CE) for automatic speech recognition (ASR) systems stands to benefit from the combination of suitably defined input features from multiple information sources. However, the information sources of interest may not necessarily operate at the same level of granularity as the underlying ASR system. The research described here builds on previous work on confidence estimation for ASR systems using features extracted from word-level recognition lattices, by incorporating information at the sub-word level. Furthermore, the use of Conditional Random Fields (CRFs) with hidden states is investigated as a technique to combine information for word-level CE. Performance improvements are shown using the sub-word-level information in linear-chain CRFs with appropriately engineered feature functions, as well as when applying the hidden-state CRF model at the word level.\n",
    "Index Terms: confidence estimation, hidden-state conditional random fields, speech recognition, sub-word-level information\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-613"
  },
  "lee12f_interspeech": {
   "authors": [
    [
     "Hung-yi",
     "Lee"
    ],
    [
     "Yu-yu",
     "Chou"
    ],
    [
     "Yow-Bang",
     "Wang"
    ],
    [
     "Lin-shan",
     "Lee"
    ]
   ],
   "title": "Supervised spoken document summarization jointly considering utterance importance and redundancy by structured support vector machine",
   "original": "i12_2342",
   "page_count": 4,
   "order": 626,
   "p1": "2342",
   "pn": "2345",
   "abstract": [
    "In extractive spoken document summarization, it is desired to select important utterances from documents to construct the summary while avoiding redundancy among the selected utterances, but it is not easy to balance the two different goals. In this paper, a supervised spoken document summarization approach is proposed based on structured support vector machine (SVM), in which the above two goals are jointly considered during training. A set of parameters not only describing the ways to evaluate the importance of the utterances but minimizing the redundancy is directly learned from the training set. Encouraging results were obtained on a lecture corpus in the preliminary experiments.\n",
    "Index Terms: speech summarization, structured SVM\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-614"
  },
  "chen12k_interspeech": {
   "authors": [
    [
     "Yun-Nung",
     "Chen"
    ],
    [
     "Florian",
     "Metze"
    ]
   ],
   "title": "Integrating intra-speaker topic modeling and temporal-based inter-speaker topic modeling in random walk for improved multi-party meeting summarization",
   "original": "i12_2346",
   "page_count": 4,
   "order": 627,
   "p1": "2346",
   "pn": "2349",
   "abstract": [
    "This paper proposes an improved approach of summarization for spoken multi-party interaction, in which intra-speaker and inter-speaker topics are modeled in a graph constructed with topical relations. Each utterance is represented as a node of the graph and the edge between two nodes is weighted by the similarity between the two utterances, which is topical similarity evaluated by probabilistic latent semantic analysis (PLSA). We model intra-speaker topics by sharing the topics from the same speaker and inter-speaker topics by partially sharing the topics from the adjacent utterances based on temporal information. We did experiments for ASR and manual transcripts. For both transcripts, experiments showed combining intra-speaker and inter-speaker topic modeling can help include the important utterances to offer the improvement for summarization.\n",
    "Index Terms: summarization, multi-party meeting, topic model, probabilistic latent semantic analysis (PLSA), topic transition, temporal information, random walk\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-615"
  },
  "feng12b_interspeech": {
   "authors": [
    [
     "Junlan",
     "Feng"
    ],
    [
     "Bernard",
     "Renger"
    ]
   ],
   "title": "Language modeling for voice-enabled social TV using tweets",
   "original": "i12_2350",
   "page_count": 4,
   "order": 628,
   "p1": "2350",
   "pn": "2353",
   "abstract": [
    "Social TV is a recent trend that integrates social media access and TV viewing. In this paper, we investigate approaches for building effective language models for a voice-enabled social TV application, where viewers can speak their social media updates while watching TV. We propose to take advantage of social media data, more specifically TV-related Twitter messages (tweets). The challenge is the noisy nature of Twitter data. Our contributions are as follows. First, we collect TV show related tweets and provide a detailed analysis of the style mismatch between written tweets and spoken language. Second, we propose a learning based approach to transforming tweets to be more suitable for language modeling. This transformation considers lexical, phonetic and contextual similarity between the misspellings and the canonical form. Third, we build the language models from normalized TV-related tweets along with other data resources that are weighted to optimize speech recognition performance. The model created via normalized tweets achieved higher performance.\n",
    "Index Terms: Voice-Enabled Social TV, Text Normalization, Language Modeling\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-616"
  },
  "kumar12_interspeech": {
   "authors": [
    [
     "Rohit",
     "Kumar"
    ],
    [
     "Rohit",
     "Prasad"
    ],
    [
     "Sankaranarayanan",
     "Ananthakrishnan"
    ],
    [
     "Aravind Namandi",
     "Vembu"
    ],
    [
     "Dave",
     "Stallard"
    ],
    [
     "Stavros",
     "Tsakalidis"
    ],
    [
     "Prem",
     "Natarajan"
    ]
   ],
   "title": "Detecting OOV named-entities in conversational speech",
   "original": "i12_2354",
   "page_count": 4,
   "order": 629,
   "p1": "2354",
   "pn": "2357",
   "abstract": [
    "A common cause of errors in spoken language systems is the presence of out-of-vocabulary (OOV) words in the input. Named entities (people, places, organizations, etc.) are a particularly important class of OOVs. In this paper we focus on detecting OOV named entities (NEs) for twoway English/Iraqi speech-to-speech translation. Our approach builds on Maximum Entropy (MaxEnt) classifier trained on a suite of contextual features. These features include: n-gram context, part-of-speech tags (both supervised an unsupervised), and a novel word posterior feature computed from the trajectory of the word posteriors within the utterance. Our experimental results show that fusion (both early and late) of these novel word posterior features with rest of the contextual features significantly improves detection accuracy for OOV NEs. However, we also observe that the same features that perform well on OOV NEs can hurt in detecting in-vocabulary NEs. Therefore, the choice of the features should be based on expected occurrence of OOV NEs.\n",
    "Index Terms: named entity detection, ASR confidence estimation, conversational speech, speech to speech translation\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-617"
  },
  "maskey12_interspeech": {
   "authors": [
    [
     "Sameer",
     "Maskey"
    ],
    [
     "Bowen",
     "Zhou"
    ]
   ],
   "title": "Unsupervised deep belief features for speech translation",
   "original": "i12_2358",
   "page_count": 4,
   "order": 630,
   "p1": "2358",
   "pn": "2361",
   "abstract": [
    "We present a novel formalism for introducing deep belief features to Hierarchical Machine Translation Model. The deep features are generated by unsupervised training of a deep belief network built with stacked sets of Restricted Boltzmann Machines. We show that our new deep feature based hierarchical model is significantly better than the baseline hierarchical model with gains for two different languages pairs in two different data size settings. We obtain absolute BLEU score improvement of +1.13 on Dari-to-English and +0.66 on English-to-Dari Transtac Evaluation task. We also observe gains on English-to-Chinese translation task.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-618"
  },
  "perez12_interspeech": {
   "authors": [
    [
     "Alicia",
     "Pérez"
    ],
    [
     "José M.",
     "Alcaide"
    ],
    [
     "María-Inés",
     "Torres"
    ]
   ],
   "title": "Euskoparl: a speech and text Spanish-basque parallel corpus",
   "original": "i12_2362",
   "page_count": 4,
   "order": 631,
   "p1": "2362",
   "pn": "2365",
   "abstract": [
    "The advances in corpus-based approaches and machine learning techniques have promoted the development of minority languages. The aim of this work is to acquire a parallel corpus in Spanish and Basque with both text and speech data. In order to be able to compare the obtained results with those developed for other languages, we took Europarl as a reference. Thus, the data was acquired within the Basque Parliament reports and speeches. The acquisition process shows subtle differences to that of Europarl acquisition. The resulting corpus is described and a few preliminary experiments on machine translation with Moses reported.\n",
    "Index Terms: speech resources, statistical machine translation, under-resourced languages\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-619"
  },
  "ryu12_interspeech": {
   "authors": [
    [
     "Hyuksu",
     "Ryu"
    ],
    [
     "Sunhee",
     "Kim"
    ],
    [
     "Minhwa",
     "Chung"
    ]
   ],
   "title": "Comparing transcription agreement on non-native English speech corpus between native and non-native annotators",
   "original": "i12_2366",
   "page_count": 4,
   "order": 632,
   "p1": "2366",
   "pn": "2369",
   "abstract": [
    "This paper aims to compare transcription agreement on non-native English speech corpus spoken by Korean learners between native and non-native annotators. Ten non-native annotators and three native annotators participate in the transcription of 608 sentences. All annotators are provided with forced-aligned phone sequences, which are to be corrected in case when they are realized differently. The transcription agreement is calculated by counting the number of identically labeled phones for all pairs of annotators. The overall transcription agreement as well as categorical transcription agreement is measured among non-native annotators, among native annotators, and among both native and non-native annotators. As a result, the transcription agreement for the three groups is 88.35%, 88.76%, and 87.83% respectively. Furthermore, vowels show 84.43% among non-natives, 88.38% among natives, and 85.75% between non-natives and natives, whereas consonants show 89.20%, 88.82%, and 88.34%, respectively. In sum, the results indicate that the transcription performed by non-native annotators is close to that performed by native annotators with respect to transcription agreement.\n",
    "Index Terms: non-native speech, corpus, transcription agreement, non-native annotator, native annotator\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-620"
  },
  "ogata12_interspeech": {
   "authors": [
    [
     "Jun",
     "Ogata"
    ],
    [
     "Masataka",
     "Goto"
    ]
   ],
   "title": "Podcastle: collaborative training of language models on the basis of wisdom of crowds",
   "original": "i12_2370",
   "page_count": 4,
   "order": 633,
   "p1": "2370",
   "pn": "2373",
   "abstract": [
    "This paper presents a language-model training method for improving automatic transcription of online spoken contents. Unlike previously studied LVCSR tasks such as broadcast news and lectures, large-sized task-specific corpora for training language models cannot be prepared and used in recognition because of the diversity of topics, vocabularies, and speaking styles. To overcome difficulties in preparing such taskspecific language models in advance, we propose collaborative training of language models on the basis of wisdom of crowds. On our public web service for LVCSR-based spoken document retrieval PodCastle, over half a million recognition errors were corrected by anonymous users. By leveraging such corrected transcriptions, component language models for various topics can be built and dynamically mixed to generate an appropriate language model for each podcast episode in an unsupervised manner. Experimental results with Japanese podcasts showed that the mixed languages models significantly reduced the word error rate.\n",
    "Index Terms: web service, LVCSR, language modeling, wisdom of crowds, error correction\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-621"
  },
  "xie12_interspeech": {
   "authors": [
    [
     "Lei",
     "Xie"
    ],
    [
     "Yinqing",
     "Xu"
    ],
    [
     "Lilei",
     "Zheng"
    ],
    [
     "Qiang",
     "Huang"
    ],
    [
     "Bingfeng",
     "Li"
    ]
   ],
   "title": "Speech pattern discovery using audio-visual fusion and canonical correlation analysis",
   "original": "i12_2374",
   "page_count": 4,
   "order": 634,
   "p1": "2374",
   "pn": "2377",
   "abstract": [
    "In this paper, we propose a speech pattern discovery approach using audio visual information fusion. We first align the audio and visual feature sequences using canonical correlation analysis (CCA) to account for the temporal asynchrony between audio and visual speech modalities. We then search for potential patterns, called paths, using unbounded dynamic time warping (UDTW) on the inter-utterance audio and visual similarity matrices, individually. Audio paths and visual paths are finally integrated and the reliable ones are reserved as the discovered speech patterns. Experiments on an audio-visual corpus has shown for the first time that the performance of speech pattern discovery can be improved by the use of visual information when the speaker's facial information is avaliable. Specifically, the proposed path fusion approach shows superior performance as compared to feature concatenation and similarity weighting. CCA-based audio-visual synchronization plays an important role in the performance improvement.\n",
    "Index Terms: Speech pattern discovery, canonical correlation analysis, audio-visual speech processing, dynamic time warping\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-622"
  },
  "maskey12b_interspeech": {
   "authors": [
    [
     "Sameer",
     "Maskey"
    ],
    [
     "Andrew",
     "Rosenberg"
    ]
   ],
   "title": "Power mean pyramid scores for summarization evaluation",
   "original": "i12_2378",
   "page_count": 4,
   "order": 635,
   "p1": "2378",
   "pn": "2381",
   "abstract": [
    "We present Power Mean Pyramid Scores (PMP), an evaluation metric that extends the Pyramid evaluation scheme for summarization by combining Sentence Content Units (SCU) scores using Power Mean. The Pyramid method generates a summarization score by linearly combining component SCU scores. We find that by combining SCU scores using Power Mean, we can optimize a single parameter, α, leading to significantly improved correlation with human judgements. We demonstrate this result through an empirical study based on TAC-08 evaluation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-623"
  },
  "escuderomancebo12_interspeech": {
   "authors": [
    [
     "David",
     "Escudero-Mancebo"
    ],
    [
     "Eva",
     "Estebas-Vilaplana"
    ]
   ],
   "title": "Visualizing tool for evaluating inter-label similarity in prosodic labeling experiments",
   "original": "i12_2382",
   "page_count": 4,
   "order": 636,
   "p1": "2382",
   "pn": "2385",
   "abstract": [
    "This paper presents a technique that allows us to detect similarities among prosodic labels used to describe pitch accents within the ToBI framework. The inter-label proximity is determined empirically as a result of the evidence obtained in contingency tables of inter-transcriber agreement tests and in the confusion matrices used in automatic prosodic labeling experiments. This tool may be useful to decide which labels can be grouped together when a simplified representation is required.\n",
    "Index Terms: Prosodic labeling, ToBI pitch accents, tonal representation\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-624"
  },
  "wagner12b_interspeech": {
   "authors": [
    [
     "Petra",
     "Wagner"
    ],
    [
     "Fabio",
     "Tamburini"
    ],
    [
     "Andreas",
     "Windmann"
    ]
   ],
   "title": "Objective, subjective and linguistic roads to perceptual prominence.how are they compared and why?",
   "original": "i12_2386",
   "page_count": 4,
   "order": 637,
   "p1": "2386",
   "pn": "2389",
   "abstract": [
    "Prosodic prominence denotes the perceptual salience of linguistic units. There exists no agreement on (1) adequate methods for its subjective measurement, (2) its objective acoustic correlates and (3) its relationship to linguistic structure. A traditional approach for evaluating any of these descriptive layers is an inter-level comparison, e.g. between a perceptual and an acoustic model of prominence. However, (1) there exists no standard procedure for such a comparison, and (2) such a comparison is misleading if both layers are expected to be symmetrical, given the neglected influence of linguistic top-down expectancies. We propose an evaluation procedure for prominence models relying on tripartite correlations of perception, its acoustic correlates and linguistic expectations. We suggest a novel correlation metric and test its usefulness on a prosodic corpus of German.\n",
    "Index Terms: prominence, evaluation, prosody\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-625"
  },
  "heckmann12_interspeech": {
   "authors": [
    [
     "Martin",
     "Heckmann"
    ]
   ],
   "title": "Audio-visual evaluation and detection of word prominence in a human-machine interaction scenario",
   "original": "i12_2390",
   "page_count": 4,
   "order": 638,
   "p1": "2390",
   "pn": "2393",
   "abstract": [
    "This paper investigates the audio-visual correlates and the detection of word prominence in a scenario where subjects were interacting with a computer in a small cartoon game. I set up a Wizard of Oz experiment in which subjects were asked to make corrections for a misunderstanding of the system. As only one word was misunderstood this evoked a narrow focus condition rendering the corrected word highly prominent. I made audio-visual recordings with a distant microphone and without visual markers. From these conditions I expected to elicit natural reactions from the subjects in a human-machine interaction task. As acoustic features I extracted duration, intensity, fundamental frequency and spectral emphasis. From the visual channel I extracted head movements based on the movements of the nose and image transformation based features from the mouth region. First I show that the extracted features are significantly different for the two focus conditions (broad and narrow). Based on classification results I demonstrate that they can be differentiated without knowledge of the word identity. Furthermore, I show that the visual channel by itself yields comparable accuracies as acoustic features and that a combination of both modalities increases performance.\n",
    "Index Terms: prosody, prominence, visual, audio-visual, spectral emphasis, lip movement, head movement\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-626"
  },
  "arnold12_interspeech": {
   "authors": [
    [
     "Denis",
     "Arnold"
    ],
    [
     "Petra",
     "Wagner"
    ],
    [
     "Bernd",
     "Möbius"
    ]
   ],
   "title": "Obtaining prominence judgments from naive listeners.influence of rating scales linguistic levels and normalisation",
   "original": "i12_2394",
   "page_count": 4,
   "order": 639,
   "p1": "2394",
   "pn": "2397",
   "abstract": [
    "In this paper we examine different approaches to obtain judgments of perceptual prominence. We discuss different prominence scales, the influence of the linguistic unit on which prominence is rated and the normalisation of prominence judgments. We propose the use of a multilevel scale for obtaining prominence judgments. It seems that naive listeners can rate word prominence better than syllable prominence, resulting in better correlations to acoustics. It is shown that normalisation should be applied to the obtained ratings.\n",
    "Index Terms: prosody, prominence, methods, normalisation, acoustic correlates\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-627"
  },
  "badino12_interspeech": {
   "authors": [
    [
     "Leonardo",
     "Badino"
    ],
    [
     "Robert A. J.",
     "Clark"
    ],
    [
     "Mirjam",
     "Wester"
    ]
   ],
   "title": "Towards hierarchical prosodic prominence generation in TTS synthesis",
   "original": "i12_2398",
   "page_count": 4,
   "order": 640,
   "p1": "2398",
   "pn": "2401",
   "abstract": [
    "We address the problem of the identification (from text) and generation of pitch accents in HMM-based English TTS synthesis. We show, through a large scale perceptual test, that a large improvement of the binary discrimination between pitch accented and non-accented words has no effect on the quality of the speech generated by the system. On the other side adding a third accent type that emphatically marks words that convey \"contrastive\" focus (automatically identified from text) produces beneficial effects on the synthesized speech. These results support the accounts on prosodic prominence that consider the prosodic patterns of utterances as hierarchical structured and point out the limits of a flattening of such structure resulting from a simple accent/non-accent distinction.\n",
    "Index Terms: speech synthesis, HMM, pitch accents, focus detection\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-628"
  },
  "cutugno12_interspeech": {
   "authors": [
    [
     "Francesco",
     "Cutugno"
    ],
    [
     "Enrico",
     "Leone"
    ],
    [
     "Bogdan",
     "Ludusan"
    ],
    [
     "Antonio",
     "Origlia"
    ]
   ],
   "title": "Investigating syllabic prominence with conditional random fields and latent-dynamic conditional random fields",
   "original": "i12_2402",
   "page_count": 4,
   "order": 641,
   "p1": "2402",
   "pn": "2405",
   "abstract": [
    "The present study performs an investigation on several issues concerning the automatic detection of prominences. Its aim is to offer a better understanding of the prominence phenomenon in order to be able to improve existent prominence detection systems. The study is threefold: first, the presence of hidden dynamics in the sequence of prominent and non-prominent syllables is tested by comparing results obtained with CRFs and LDCRFs. Second, the size of the context to be taken into account when determining prominence was examined and third, a new set of features was investigated. The results obtained show that LDCRFs systematically outperforms CRFs, that a context of three syllables is generally sufficient for prominence detection, that syllable length is a powerful feature to include and that new features concerning pitch movements we introduce here can substitute heuristic measures used in previous works.\n",
    "Index Terms: syllabic prominence, conditional random fields\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-629"
  },
  "samlowski12_interspeech": {
   "authors": [
    [
     "Barbara",
     "Samlowski"
    ],
    [
     "Petra",
     "Wagner"
    ],
    [
     "Bernd",
     "Möbius"
    ]
   ],
   "title": "Disentangling lexical, morphological, syntactic and semantic influences on German prominence - evidence from a production study",
   "original": "i12_2406",
   "page_count": 4,
   "order": 642,
   "p1": "2406",
   "pn": "2409",
   "abstract": [
    "The aim of this paper is to examine effects on syllable prominence exerted by word and phrase boundaries, lexical stress, and sentence focus, and by the interactions between these factors. In a production study, German verb prefixes potentially forming prosodic minimal word pairs were systematically placed in a set of different contexts. Acoustic analyses showed a consistent effect of lexical stress on syllable prominence in both focused and unfocused sentence positions. When the verb was in sentence focus, even unstressed syllables in bisyllabic prefixes changed as a function of lexical stress. Varying sentence stress only had an effect on syllables in lexically stressed prefixes. While no effect of word boundary was found, unbound verb particles preceding phrase boundaries received the highest prominence values. Syllables in lexically stressed prefixes showed greater acoustic similarity with these unbound particles than did syllables in lexically unstressed prefixes.\n",
    "Index Terms: syllables, prominence, duration, stress\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-630"
  },
  "rosenberg12c_interspeech": {
   "authors": [
    [
     "Andrew",
     "Rosenberg"
    ]
   ],
   "title": "Using prominence and phrasing predictions to improve weighted dictionary pronunciation models",
   "original": "i12_2410",
   "page_count": 4,
   "order": 643,
   "p1": "2410",
   "pn": "2413",
   "abstract": [
    "Prosody impacts the pronunciation variation of lexical items in a number of ways. Accented syllables tend to be pronounced with their canonical (dictionary) vowel. Deaccented vowels are more likely to be reduced. Coarticulatory influences rarely span intonational phrase boundaries. In this work, we investigate the use of automatically generated prosodic hypotheses to improve a weighted dictionary pronunciation model. We use the phonemically transcribed, Buckeye Corpus for this investigation. We find that predictions of pitch accent and intonational phrase boundaries can be used to lower pronunciation model perplexity.\n",
    "Index Terms: Pronunciation Modeling, Prosody, Prominence, Phrasing\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-631"
  },
  "goldman12_interspeech": {
   "authors": [
    [
     "Jean-Philippe",
     "Goldman"
    ],
    [
     "Mathieu",
     "Avanzi"
    ],
    [
     "Antoine",
     "Auchlin"
    ],
    [
     "Anne Catherine",
     "Simon"
    ]
   ],
   "title": "A continuous prominence score based on acoustic features",
   "original": "i12_2414",
   "page_count": 4,
   "order": 644,
   "p1": "2414",
   "pn": "2417",
   "abstract": [
    "Up to now, prominence detection has mainly been considered a binary matter, a syllable being considered as prominent or not. This contribution aims at developing an automatic detection procedure of gradual prominence. Based on 4 prosodic parameters (relative duration, relative F0, F0 movement and pause duration), the system provides each syllable with a gradual score of prominence ranging from 0 (non-prominent syllable) to 4 (extra prominent syllable). The automatic detection (ProsoProm) relies on a manually annotated corpus (18 minutes, or 3669 syllables, of speech annotated by three experts) and is cumulative (the relative weight of each parameter is taken into account in order to compute a global score for each syllable). Discussion of the results includes a comparison to a similar software (Analor) and a qualitative analysis of misses and false detections. The agreement between automatic and (median) human annotation reaches a Kappa score of 0.8.\n",
    "Index Terms: prosody, speech, prominence detection, cumulative prominence, automatic prosodic analysis, expert vs automatic prosodic annotation\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-632"
  },
  "sappok12_interspeech": {
   "authors": [
    [
     "Christopher",
     "Sappok"
    ],
    [
     "Denis",
     "Arnold"
    ]
   ],
   "title": "More on the normalization of syllable prominence ratings",
   "original": "i12_2418",
   "page_count": 4,
   "order": 645,
   "p1": "2418",
   "pn": "2421",
   "abstract": [
    "The perception of syllable prominence depends to a limited extent on the acoustic properties of the speech signal in question. Psychoacoustic factors are involved as well. Thus, research often relies on two types of data: subjective prominence ratings collected in perception experiments and acoustic measures. A problem with the rating data is noise resulting from individual approaches to the rating task. This paper addresses the question of how this noise can be reduced by normalization, evaluating 12 normalization methods. In a perception experiment, prominence ratings concerning German read speech were collected. From the raw rating data 12 different emirrorf data-sets were computed according to the 12 methods. Each mirror data-set was correlated with the same set of underlying acoustic data. The multiple regression setup included raw syllable duration and within-syllable maximum F0 and intensity. Adjusted R2-values could be raised considerably with selected methods.\n",
    "Index Terms: perception experiment, inter-rater variability, intra-rater variability, read speech, German, prose, poetry\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-633"
  },
  "mahrt12_interspeech": {
   "authors": [
    [
     "Tim",
     "Mahrt"
    ],
    [
     "Jennifer",
     "Cole"
    ],
    [
     "Margaret",
     "Fleck"
    ],
    [
     "Mark",
     "Hasegawa-Johnson"
    ]
   ],
   "title": "<i>F</i><sub>0</sub> and the perception of prominence",
   "original": "i12_2422",
   "page_count": 4,
   "order": 646,
   "p1": "2422",
   "pn": "2425",
   "abstract": [
    "This study investigates the role F0 plays in the perception of prominence in American English. Raw, log and locally normalized measures of F0 were extracted from words in a 20K word corpus of spontaneous speech. Linear regression analyses were conducted to test the strength of these measures as cues to prominence, with prominence based on judgments made by ordinary listeners in real-time auditory perception. The Bayesian Information Criterion was used to further investigate whether these F0 measures cue prominence in a linear or piece-wise linear function, corresponding to a linguistic model of prominence as a gradient or discrete feature. The results of this study show that F0 measures are similar to intensity measures in both their strength as cues to perceived prominence, and in signaling a discrete prominence distinction that distinguishes non- or weakly-prominent words from words with greater prominence. Our finding that F0 and intensity cue a discrete prominence distinction is compared with our prior finding that duration and word frequency signal gradient prominence distinctions. This apparent discrepancy is discussed in terms of the dual nature of prominence in English, as an expression of layered metrical (stress) structure in phonology, and as an expression of pragmatic focus.\n",
    "Index Terms: speech prosody, prominence, Bayesian Information Criterion, F0\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-634"
  },
  "andreeva12_interspeech": {
   "authors": [
    [
     "Bistra",
     "Andreeva"
    ],
    [
     "William",
     "Barry"
    ],
    [
     "Magdalena",
     "Wolska"
    ]
   ],
   "title": "Language differences in the perceptual weight of prominence-lending properties",
   "original": "i12_2426",
   "page_count": 4,
   "order": 647,
   "p1": "2426",
   "pn": "2429",
   "abstract": [
    "A Bulgarian and a German sentence were presented to Bulgarian and German listeners together with a question which either expected an early narrow focus or a late narrow focus. The answering sentences were manipulated so that the word in the late-focused position ranged from completely de-accented to strongly accented. The early focused position was neutral, allowing late-focus perception with late strong accentuation and early focus with de-accentuation of the late-focus position. Accentuation strength of the late position was varied by changing the duration, intensity and F0 values individually between accented and low de-accented. Subjects were asked to judge the suitability of the answers to the question with. Results show the relative contribution of the three parameters to the acceptability of the word in the late focus position as focally accented or de-accented. Differences between Bulgarian and German in the relative weighting of the parameters are revealed.\n",
    "Index Terms: focal accentuation, perception, parameter weighting, cross-language differences\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-635"
  },
  "li12k_interspeech": {
   "authors": [
    [
     "Haiyang",
     "Li"
    ],
    [
     "Jiqing",
     "Han"
    ],
    [
     "Tieran",
     "Zheng"
    ],
    [
     "Guibin",
     "Zheng"
    ]
   ],
   "title": "A novel confidence measure based on context consistency for spoken term detection",
   "original": "i12_2430",
   "page_count": 4,
   "order": 648,
   "p1": "2430",
   "pn": "2433",
   "abstract": [
    "In this paper, we propose a novel confidence measure to improve the performance of spoken term detection (STD). The proposed confidence measure is based on the context consistency between a hypothesized word and its context in word lattice. When calculating the context consistency of a hypothesized word, the proposed confidence measure considers not only the semantic similarity between words but also the uncertainty of the context. To measure the uncertainty of the context, we employ the word occurrence probability, which is obtained by combining the overlapping hypotheses in word posterior lattice. Additionally, we also use two effective measures of semantic similarity to acquire more accurate context consistency for confidence measure. The experiments conducted on the Hub-4NE Mandarin database show that the proposed confidence measure can achieve improvements over the confidence measure which ignores the word occurrence probability of context word.\n",
    "Index Terms: confidence measure, spoken term detection, context consistency, semantic similarity, word occurrence probability\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-636"
  },
  "karanasou12_interspeech": {
   "authors": [
    [
     "Panagiota",
     "Karanasou"
    ],
    [
     "Lukas",
     "Burget"
    ],
    [
     "Dimitra",
     "Vergyri"
    ],
    [
     "Murat",
     "Akbacak"
    ],
    [
     "Arindam",
     "Mandal"
    ]
   ],
   "title": "Discriminatively trained phoneme confusion model for keyword spotting",
   "original": "i12_2434",
   "page_count": 4,
   "order": 649,
   "p1": "2434",
   "pn": "2437",
   "abstract": [
    "Keyword Spotting (KWS) aims at detecting speech segments that contain a given query within large amounts of audio data. Typically, a speech recognizer is involved in a first indexing step. One of the challenges of KWS is how to handle recognition errors and out-of-vocabulary (OOV) terms. This work proposes the use of discriminative training to construct a phoneme confusion model, which expands the phonemic index of a KWS system by adding phonemic variation to handle the above-mentioned problems. The objective function that is optimized is the Figure of Merit (FOM), which is directly related to the KWS performance. The experiments conducted on English data sets show some improvement on the FOM and are promising for the use of such technique.\n",
    "Index Terms: keyword spotting, confusion model, discriminative training, Figure of Merit\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-637"
  },
  "kintzley12b_interspeech": {
   "authors": [
    [
     "Keith",
     "Kintzley"
    ],
    [
     "Aren",
     "Jansen"
    ],
    [
     "Kenneth",
     "Church"
    ],
    [
     "Hynek",
     "Hermansky"
    ]
   ],
   "title": "Inverting the point process model for fast phonetic keyword search",
   "original": "i12_2438",
   "page_count": 4,
   "order": 650,
   "p1": "2438",
   "pn": "2441",
   "abstract": [
    "Normally, we represent speech as a long sequence of frames and model the keyword with a relatively small set of parameters, commonly with a hidden Markov model (HMM). However, since the input speech is much longer than the keyword, suppose instead that we represent the speech as a relatively sparse set of impulses (roughly one per phoneme) and model the keyword as a filter-bank, where each filter's impulse response relates to the likelihood of a phone at a given position within a word. Evaluating keyword detections can then be seen of as a convolution of an impulse train with an array of filters. This view enables huge speedups; runtime no longer depends on the frame rate and is instead linear in the number of events (impulses). We apply this intuition to redesign the runtime engine behind of the point process model for keyword spotting. We demonstrate impressive real-time speedups (500,000 times faster than real-time) with minimal loss in search accuracy.\n",
    "Index Terms: keyword spotting, point process model\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-638"
  },
  "norouzian12_interspeech": {
   "authors": [
    [
     "Atta",
     "Norouzian"
    ],
    [
     "Aren",
     "Jansen"
    ],
    [
     "Richard C.",
     "Rose"
    ],
    [
     "Samuel",
     "Thomas"
    ]
   ],
   "title": "Exploiting discriminative point process models for spoken term detection",
   "original": "i12_2442",
   "page_count": 4,
   "order": 651,
   "p1": "2442",
   "pn": "2445",
   "abstract": [
    "State-of-the-art spoken term detection (STD) systems are built on top of large vocabulary speech recognition engines, which generate lattices that encode candidate occurrences of each in-vocabulary query. These lattices specifiy start and stop times of hypothesized term occurrences, providing a clear opportunity to return to the acoustics to incorporate novel confidence measures for verification. In this paper, we introduce a novel exemplar distance metric to the recently proposed discriminative point process modeling (DPPM) framework and use the resulting whole word models to generate STD confidence scores. In doing so, we introduce STD to a completely distinct acoustic modeling pipeline, trading Gaussian mixture models (GMM) for multi-layer perceptrons and replacing dictionary-derived hidden Markov models (HMM) with exemplarbased point process models. We find that whole word DPPM scores both perform comparably and are complementary to lattice posterior scores produced by a state-of-the-art speech recognition engine.\n",
    "Index Terms: spoken term detection, point process model, discriminative training, whole word model\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-639"
  },
  "bulyko12_interspeech": {
   "authors": [
    [
     "Ivan",
     "Bulyko"
    ],
    [
     "José",
     "Herrero"
    ],
    [
     "Chris",
     "Mihelich"
    ],
    [
     "Owen",
     "Kimball"
    ]
   ],
   "title": "Subword speech recognition for detection of unseen words",
   "original": "i12_2446",
   "page_count": 4,
   "order": 652,
   "p1": "2446",
   "pn": "2449",
   "abstract": [
    "We present a novel approach to building a subword speech recognizer for the task of phonetic keyword search. The recognizer, which uses short fixed-length phonetic units, is trained with phonetic transcripts that are segmented into all possible substrings of 1, 2 and 3 phones, using a lattice representation to accommodate the overlapping units. We compare the keyword search accuracy of the proposed system with systems that use words, graphones, variable-length phonetic units and context-dependent phones. Experiments with Spanish CTS data show that the proposed subword recognizer outperforms other subword systems in terms of phonetic keyword search accuracy measured on queries that consist of words not present in the training data.\n",
    "Index Terms: speech recognition, keyword search, OOV\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-640"
  },
  "qin12_interspeech": {
   "authors": [
    [
     "Long",
     "Qin"
    ],
    [
     "Alexander",
     "Rudnicky"
    ]
   ],
   "title": "OOV word detection using hybrid models with mixed types of fragments",
   "original": "i12_2450",
   "page_count": 4,
   "order": 653,
   "p1": "2450",
   "pn": "2453",
   "abstract": [
    "This paper presents initial studies to improve the out-of-vocabulary (OOV) word detection performance by using mixed types of fragment units in one hybrid system. Three types of fragment units, subwords, syllables, and graphones, were combined in two different ways to build the hybrid lexicon and language model. The experimental results show that hybrid systems with mixed types of fragment units perform better than hybrid systems using only one type of fragment unit. After comparing the OOV word detection performance with the number and length of fragment units of each system, we proposed future work to better utilize mixed types of fragment units in a hybrid system.\n",
    "Index Terms: OOV word detection, hybrid model, subword, syllable, graphone, mixed types of fragment units\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-641"
  },
  "vosoughi12_interspeech": {
   "authors": [
    [
     "Soroush",
     "Vosoughi"
    ],
    [
     "Deb",
     "Roy"
    ]
   ],
   "title": "An automatic child-directed speech detector for the study of child language development",
   "original": "i12_2478",
   "page_count": 4,
   "order": 654,
   "p1": "2478",
   "pn": "2481",
   "abstract": [
    "In this paper, we present an automatic child-directed speech detection system to be used in the study of child language development. Child-directed speech(CDS) is speech that is directed by caregivers towards infants. It is not uncommon for corpora used in child language development studies to have a combination of CDS and non-CDS. As the size of the corpora used in these studies grow, manual annotation of CDS becomes impractical. Our automatic CDS detector addresses this issue.   The focus of this paper is to propose and evaluate different sets of features for the detection of CDS, using several off-the-shelf classifiers. First, we look at the performance of a set of acoustic features. We continue by combining these acoustic features with several linguistic and eventually contextual features. Using the full set of features, our CDS detector was able to correctly identify CDS with an accuracy of .88 and F1 score of .87 using Naive Bayes.\n",
    "Index Terms: motherese, automatic, child-directed speech, infant-directed speech, adult-directed speech, prosody, language development\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-642"
  },
  "plummer12_interspeech": {
   "authors": [
    [
     "Andrew R.",
     "Plummer"
    ]
   ],
   "title": "Aligning manifolds to model the earliest phonological abstraction in infant-caretaker vocal imitation",
   "original": "i12_2482",
   "page_count": 4,
   "order": 655,
   "p1": "2482",
   "pn": "2485",
   "abstract": [
    "We argue that infants perform an abstraction over their auditory representations of the vowels of individual speakers, by mapping them to a mediating space of speaker-independent representations, guided by vocal imitative interaction with their caretakers, as a first step in the phonological acquisition process. Furthermore, we proffer a methodology for modeling this abstraction which involves the alignment of the cognitive structures, or manifolds, that the infant builds from the auditory representations of the vowels of individual speakers. As a demonstration of the methodology, we show that higher-dimensional \"excitation pattern\" representations facilitate modeling of the influence of the imitative process on perception and abstraction more so than lower-dimensional formant representations.\n",
    "Index Terms: vowel normalization, manifold alignment, vocal imitation, phonological abstraction\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-643"
  },
  "saikachi12_interspeech": {
   "authors": [
    [
     "Yoko",
     "Saikachi"
    ],
    [
     "Mafuyu",
     "Kitahara"
    ],
    [
     "Ken'ya",
     "Nishikawa"
    ],
    [
     "Ai",
     "Kanato"
    ],
    [
     "Reiko",
     "Mazuka"
    ]
   ],
   "title": "The <i>F</i><sub>0</sub> fall delay of lexical pitch accent in Japanese infant-directed speech",
   "original": "i12_2486",
   "page_count": 4,
   "order": 656,
   "p1": "2486",
   "pn": "2489",
   "abstract": [
    "The current study examined the acoustic modifications of the lexical pitch accent in Tokyo Japanese infant-directed speech (IDS), with the focus on the F0 fall delay, where the alignment of the F0 turning points associated with pitch accents were delayed with respect to the accented mora. The RIKEN Mother-Infant Conversation Corpus (R-JMICC) produced by 21 mothers from Tokyo area, was used to investigate the alignment of the F0 turning points. Two-piece linear regression was used to locate the turning points and the frequency of F0 fall delay was computed in IDS and in adult-directed speech (ADS). The results revealed that the frequency of F0 fall delay depended on the syllable structures of the accented syllable as well as the prosodic conditions (the presence of the boundary pitch movements and non-lexical lengthening) typically observed in Japanese IDS. We found significantly more frequent F0 fall delay in IDS compared to ADS, when the prosodic conditions were taken into account. The results indicate that the language-specific prosodic structure should be considered in order to characterize the F0 fall delay of lexical pitch accents in IDS.\n",
    "Index Terms: F0 turning points of pitch accents, Infant-directed speech\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-644"
  },
  "shport12_interspeech": {
   "authors": [
    [
     "Irina A.",
     "Shport"
    ]
   ],
   "title": "Childrenfs productions of multi-syllabic lexical stress patterns in different prosodic positions",
   "original": "i12_2490",
   "page_count": 4,
   "order": 657,
   "p1": "2490",
   "pn": "2493",
   "abstract": [
    "Production of lexical stress patterns in bi- and tri-syllabic words by five-year-old children was investigated. Duration and amplitude, which are the primary acoustic correlates of lexical stress in American English, were examined in words produced in isolation, in utterance-initial and utterance-final positions. In all three prosodic environments, children and adults differentiated lexical stress patterns by varying relative rhyme durations in the words. The difference between children and adults was observed in amplitude patterns within each word type. The amplitude patterns of adults varied as a function of prosodic position, whereas children tended to have similar amplitude patterns in words produced in isolation and in utterance-final position. These results suggest that the position of a word in an utterance influences the intersyllabic amplitude pattern in the word. Children may acquire amplitude patterns in utterance-final words later than in utterance-initial words, possibly due to a larger degree of pattern variation in the former rather than the latter position.\n",
    "Index Terms: child speech, lexical stress patterns, prosodic position, duration, amplitude\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-645"
  },
  "redford12_interspeech": {
   "authors": [
    [
     "Melissa A.",
     "Redford"
    ],
    [
     "Laura C.",
     "Dilley"
    ],
    [
     "Jessica L.",
     "Gamache"
    ],
    [
     "Elizabeth A.",
     "Wieland"
    ]
   ],
   "title": "Prosodic marking of continuation versus completion in childrenfs narratives",
   "original": "i12_2494",
   "page_count": 4,
   "order": 658,
   "p1": "2494",
   "pn": "2497",
   "abstract": [
    "Discourse prosody in school-aged childrenfs narratives was investigated to test for developmental changes in global prosodic structure and to characterize the key contrastive features for marking continuation versus completion. Spontaneous narratives were obtained from 42 children (5 to 7 years old) and 14 adult caregivers. The narratives were prosodically transcribed using a formal annotation system that relies on perceptual and acoustic analyses. Metalinguistic judgments of prosodic function and appropriateness were also obtained. Analyses examined the global prosodic structure emergent from metalinguistic judgments as well as the prosodic cues to continuation and completion. Results were that children marked phrases for completion less often and less appropriately than adults. Children and adults both used phrase-final tones and post-boundary pauses to mark continuation versus completion; however, childrenfs overall higher rate of pausing lessened the extent to which continuation and completion were differentiated through pausing.\n",
    "Index Terms: speech acquisition, speech prosody, discourse prosody, prosodic boundaries\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-646"
  },
  "fogerty12_interspeech": {
   "authors": [
    [
     "Daniel",
     "Fogerty"
    ],
    [
     "Diane",
     "Kewley-Port"
    ],
    [
     "Larry E.",
     "Humes"
    ]
   ],
   "title": "Judging temporal onset differences for concurrent vowels: results for young, middleaged, and older adults",
   "original": "i12_2498",
   "page_count": 4,
   "order": 659,
   "p1": "2498",
   "pn": "2501",
   "abstract": [
    "Temporal processing abilities generally decline with age. These temporal processing declines may reduce the ability of older listeners to use temporal cues for spoken language processing, such as for segregating multiple talkers. A concurrent vowel paradigm was used to examine categorization judgments for young, middle-aged, and older listeners based on temporal onset differences. Listeners categorized vowel pairs varying in temporal asynchrony as one sound, two overlapping sounds, or two sounds separated by a gap. The two boundaries separating the three response categories were determined for each listener. These boundaries were related to the categorization of multiple events (multiplicity) and presence of a silent gap (gap-identification). Compared to young and middle-aged listeners, older listeners required longer temporal offsets for multiplicity judgment. Middle-aged and older listeners also required longer offsets than young listeners for gapidentification. For older listeners, correlations with various temporal processing tasks indicated that vowel temporal-order thresholds were related to multiplicity, while age and non-speech gap-detection thresholds were related to gap-identification.\n",
    "Index Terms: speech perception, speech acoustics, noise vocoding, fundamental frequency, consonant-vowel ratio\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-647"
  },
  "hu12c_interspeech": {
   "authors": [
    [
     "Pengfei",
     "Hu"
    ],
    [
     "Wenju",
     "Liu"
    ],
    [
     "Wei",
     "Jiang"
    ]
   ],
   "title": "Combining frame and segment based models for environmental sound classification",
   "original": "i12_2502",
   "page_count": 4,
   "order": 660,
   "p1": "2502",
   "pn": "2505",
   "abstract": [
    "The paper considers the task of recognizing environmental sounds, which plays a critical role in human's perception of an auditory context in audiovisual materials. A variety of features have been proposed for audio recognition, either frame-based or segmental. Here, we propose a two-stage framework to combine modeling in these two levels. First, the Gaussian Mixture Models(GMMs) are built based on short-term features and pre-classification are performed. Then, in the event that the GMMs are not certain about the result, the system engages Support Vector Machines (SVMs) to refine the output hypothesis. In the next stage, the features are combined by taking posterior estimates of GMMs along with segmental features as SVMs' input features. Experiments on the sound dataset show that the proposed framework makes an improvement over the traditional methods.\n",
    "Index Terms: environmental sound classification, model combination, GMMs, SVMs\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-648"
  },
  "leng12_interspeech": {
   "authors": [
    [
     "Yi Ren",
     "Leng"
    ],
    [
     "Huy Dat",
     "Tran"
    ]
   ],
   "title": "Using blob detection in missing feature linear-frequency cepstral coefficients for robust sound event recognition",
   "original": "i12_2506",
   "page_count": 4,
   "order": 661,
   "p1": "2506",
   "pn": "2509",
   "abstract": [
    "The Missing Feature Linear-Frequency Cepstral Coefficients (MF-LFCC) is a noise robust cepstral feature that transforms both clean and noisy signals into a similar representation. Unlike conventional Missing Feature Techniques, the MF-LFCC does not require spectrogram imputation or classifier modification. To improve the noise mask used in the MF-LFCC, we propose to use the computer vision technique of blob detection to identify the peaks characterizing the sparsity of sound event spectrograms. For single sound event recognition using SVM classifiers, the MF-LFCC is shown to significantly outperform the MFCC baseline and the noise robust ESTI Advanced Front End feature in noisy conditions.\n",
    "Index Terms: blob detection, missing feature, robust recognition, sound event recognition\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-649"
  },
  "patil12_interspeech": {
   "authors": [
    [
     "Kailash",
     "Patil"
    ],
    [
     "Mounya",
     "Elhilali"
    ]
   ],
   "title": "Goal-oriented auditory scene recognition",
   "original": "i12_2510",
   "page_count": 4,
   "order": 662,
   "p1": "2510",
   "pn": "2513",
   "abstract": [
    "How do we understand and interpret complex auditory environments in a way that may depend on some stated goals or intentions? Here, we propose a framework that provides a detailed analysis of the spectrotemporal modulations in the acoustic signal, augmented with a discriminative classifier using multilayer perceptrons. We show that such a representation is successful at capturing the non-trivial commonalties within a sound class and differences between different classes. It not only surpasses performance of current systems in the literature by about 21%, but proves quite robust for processing multi-source cases. In addition, we test the role of feature re-weighting in improving feature selectivity and signal-to-noise ratio in the direction of a sound class of interest.\n",
    "Index Terms: scene understanding, acoustic event recognition, attention, bottom-up, top-down\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-650"
  },
  "ziaei12_interspeech": {
   "authors": [
    [
     "Ali",
     "Ziaei"
    ],
    [
     "Abhijeet",
     "Sangwan"
    ],
    [
     "John H. L.",
     "Hansen"
    ]
   ],
   "title": "Prof-life-log: audio environment detection for naturalistic audio streams",
   "original": "i12_2514",
   "page_count": 4,
   "order": 663,
   "p1": "2514",
   "pn": "2517",
   "abstract": [
    "In this study, we develop a new system for real world audio environment matching. Environment detection within unknown audio streams requires a system that operates in an unsupervised manner since it will be faced with unknown environments with- out prior information. In addition, the overall solution should be computationally efficient for large audio collection. In the pro- posed approach, a Gaussian mixture model(GMM) is trained on large amounts of unlabeled audio data and used as a back- ground acoustic model. Subsequently, an acoustic signature vector (ASV) is computed for each environment. Here, the ASV vector is designed to capture the unique acoustic characteristics of an environment. Using the ASV vectors, we demonstrate that it is possible to compute an effective similarity measure between two acoustic environments. We demonstrate the per- formance of the proposed system on real-world audio data, and compare it to a traditional GMM-UBM (Universal Background Model) system. Experiments show that our system achieves an equal error rate (EER) that is +35% better than a baseline GMM-UBM system.\n",
    "Index Terms: Audio Environment Detection, Acoustic Signature, Real word audio data, Prof-Life-Log\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-651"
  },
  "huang12g_interspeech": {
   "authors": [
    [
     "Po-Sen",
     "Huang"
    ],
    [
     "Jianchao",
     "Yang"
    ],
    [
     "Mark",
     "Hasegawa-Johnson"
    ],
    [
     "Feng",
     "Liang"
    ],
    [
     "Thomas S.",
     "Huang"
    ]
   ],
   "title": "Pooling robust shift-invariant sparse representations of acoustic signals",
   "original": "i12_2518",
   "page_count": 4,
   "order": 664,
   "p1": "2518",
   "pn": "2521",
   "abstract": [
    "In recent years, designing the coding and pooling structures in layered networks has been shown to be a useful method for learning highlevel feature representations for visual data. Yet, such learning structures have not been extensively studied for audio signals. In this paper, we investigate the different pooling strategies based on the sparse coding scheme and propose a temporal pyramid pooling method to extract discriminative and shift-invariant feature representations. We demonstrate the superiority of our new feature representation over traditional features on the acoustic event classification task.\n",
    "Index Terms: sparse coding, pooling, acoustic event classification\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-652"
  },
  "tan12_interspeech": {
   "authors": [
    [
     "Lee Ngee",
     "Tan"
    ],
    [
     "Kantapon",
     "Kaewtip"
    ],
    [
     "Martin L.",
     "Cody"
    ],
    [
     "Charles E.",
     "Taylor"
    ],
    [
     "Abeer",
     "Alwan"
    ]
   ],
   "title": "Evaluation of a sparse representation-based classifier for bird phrase classification under limited data conditions",
   "original": "i12_2522",
   "page_count": 4,
   "order": 665,
   "p1": "2522",
   "pn": "2525",
   "abstract": [
    "This paper evaluates the performance of a sparse representation-based (SR) classifier for a limited data, bird phrase classification task. The evaluation database contains 32 unique phrases segmented from songs of the Cassinfs Vireo (Vireo cassinii). Spectrographic features were extracted from each phrase-segmented audio file, followed by dimension reduction using principal component analysis (PCA). A performance comparison to the nearest subspace (NS) and support vector machine (SVM) classifiers was conducted. The SR classifier outperforms the NS and SVM classifiers, with a maximum absolute improvement of 3.4% observed when there are only four tokens per phrase in the training set.\n",
    "Index Terms: bird phrase classification, limited data, sparse representation, L1 minimization.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-653"
  },
  "novak12b_interspeech": {
   "authors": [
    [
     "Josef R.",
     "Novak"
    ],
    [
     "Paul R.",
     "Dixon"
    ],
    [
     "Nobuaki",
     "Minematsu"
    ],
    [
     "Keikichi",
     "Hirose"
    ],
    [
     "Chiori",
     "Hori"
    ],
    [
     "Hideki",
     "Kashioka"
    ]
   ],
   "title": "Improving WFST-based G2p conversion with alignment constraints and RNNLM n-best rescoring",
   "original": "i12_2526",
   "page_count": 4,
   "order": 666,
   "p1": "2526",
   "pn": "2529",
   "abstract": [
    "This work introduces a modified WFST-based multiple to multiple EM-driven alignment algorithm for Grapheme-to-Phoneme (G2P) conversion, and preliminary experimental results applying a Recurrent Neural Network Language Model (RNNLM) as an N-best rescoring mechanism for G2P conversion. The alignment algorithm leverages the WFST framework and introduces several simple structural constraints which yield a small but consistent improvement in Word Accuracy (WA) on a selection of standard baselines. The RNNLM rescoring further extends these gains and achieves state-of-the-art performance on four standard G2P datasets. The system is also shown to be significantly faster than existing solutions. Finally, the complete WFST-based G2P framework is provided as an open-source toolkit.\n",
    "Index Terms: G2P, Alignment, RNNLM, WFST\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-654"
  },
  "luan12_interspeech": {
   "authors": [
    [
     "Jian",
     "Luan"
    ],
    [
     "Bolei",
     "He"
    ],
    [
     "Hairong",
     "Xia"
    ],
    [
     "Linfang",
     "Wang"
    ],
    [
     "Daniela",
     "Braga"
    ],
    [
     "Sheng",
     "Zhao"
    ]
   ],
   "title": "Expand CRF to model long distance dependencies in prosodic break prediction",
   "original": "i12_2530",
   "page_count": 4,
   "order": 667,
   "p1": "2530",
   "pn": "2533",
   "abstract": [
    "Intonation phrase length distribution is important information for prosodic break prediction. However, existing CRF frameworks cannot make full use of it. An expanded CRF is proposed in this paper to tackle this problem. Its lattice carries the location of previous intonation phrase (L3) break, and consequently makes it possible to support various dynamic features, such as the number of syllables from the previous L3 break and the POS of word after the previous L3 break. Remarkable improvements are obtained with the expanded CRF for L3 break prediction task. It is also promising to benefit other tasks containing long distance dependencies.\n",
    "Index Terms: CRF, intonation phrase, prosodic break prediction, speech prosody\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-655"
  },
  "veilleux12_interspeech": {
   "authors": [
    [
     "Nanette",
     "Veilleux"
    ],
    [
     "Jonathan",
     "Barnes"
    ],
    [
     "Alejna",
     "Brugos"
    ],
    [
     "Stefanie",
     "Shattuck-Hufnagel"
    ]
   ],
   "title": "Perceptual foundations for naturalistic variability in the prosody of synthetic speech",
   "original": "i12_2534",
   "page_count": 4,
   "order": 668,
   "p1": "2534",
   "pn": "2537",
   "abstract": [
    "Recent studies have shown that the Tonal Center of Gravity is a better classifier than F0 Turning Points for at least two contrastively timed pitch accents in American English intonation contours. Within this framework, a binary F0 weighting function derived from the F0 contour can be used instead of the natural F0 contour without a degradation in discrimination performance. This success has important implications for speech synthesis. Just as we can capture the functional equivalence of a multitude of auditorily distinct F0 contour shapes in terms of their mapping to a single parameter (the TCoG) via a set of binary weighting functions, this same mapping could be run in reverse as a source to generate natural-sounding variability in speech synthesis.\n",
    "Index Terms: Tonal Center of Gravity, F0 alignment, pitch accent classification, prosody, speech synthesis\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-656"
  },
  "hahn12_interspeech": {
   "authors": [
    [
     "Stefan",
     "Hahn"
    ],
    [
     "Paul",
     "Vozila"
    ],
    [
     "Maximilian",
     "Bisani"
    ]
   ],
   "title": "Comparison of grapheme-to-phoneme methods on large pronunciation dictionaries and LVCSR tasks",
   "original": "i12_2538",
   "page_count": 4,
   "order": 669,
   "p1": "2538",
   "pn": "2541",
   "abstract": [
    "Grapheme-to-Phoneme conversion (G2P) is usually used within every state-of-the-art ASR system to generalize beyond a fixed set of words. Although the performance is typically already quite good (<10% phoneme error rate) and pronunciations of important words are checked by a linguist, further improvements are still desirable, especially for end user customization.   In this work, we present and compare five methods/tools to tackle the G2P task. Although most of the methods have already been published and/or are available as open source software, the reported experiments are done on large state-of-the-art tasks and the used software is from the actual publications.   Besides an experimental comparison on text data for a range of languages (i.e. measuring the G2P accuracy only), our focus in this paper is measuring the effect of improved G2P modeling on LVCSR performance for a challenging ASR task. Additionally, the effect of using n-Best pronunciation variants instead of single best is investigated briefly.\n",
    "Index Terms: grapheme-to-phoneme conversion, G2P, ASR\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-657"
  },
  "berthommier12_interspeech": {
   "authors": [
    [
     "Frédéric",
     "Berthommier"
    ],
    [
     "Laurent",
     "Girin"
    ],
    [
     "Louis-Jean",
     "Boë"
    ]
   ],
   "title": "A simple hybrid acoustic/morphologically-constrained technique for the synthesis of stop consonants in various vocalic contexts",
   "original": "i12_2542",
   "page_count": 4,
   "order": 670,
   "p1": "2542",
   "pn": "2545",
   "abstract": [
    "The predominant way to synthesize stop consonants is currently to use an articulatory model controlled by vocal tract parameters. We propose a new method to make this synthesis in various vocalic contexts. To generate the formant transitions, the basic principle is to apply an opening function on the (equal-length section) area function derived from the linear predictive (LP) model of speech signals. The definition of this opening function is empirically based on morphological considerations, and the main parameter is the place of articulation. Syllabic sounds with /b d g/ in /a i u/ vowel contexts are generated using LP synthesis with reflections coefficients corresponding to the interpolated area function. We show that the general structure of the formant transitions can be well represented using this model, and provide intelligible sound examples.\n",
    "Index Terms: syllable synthesis, co-articulation, stop consonants, place of articulation, acoustic tube model, formant transitions, reflection coefficients\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-658"
  },
  "prahallad12_interspeech": {
   "authors": [
    [
     "Kishore",
     "Prahallad"
    ],
    [
     "E. Naresh",
     "Kumar"
    ],
    [
     "Venkatesh",
     "Keri"
    ],
    [
     "S.",
     "Rajendran"
    ],
    [
     "Alan W.",
     "Black"
    ]
   ],
   "title": "The IIIT-h indic speech databases",
   "original": "i12_2546",
   "page_count": 4,
   "order": 671,
   "p1": "2546",
   "pn": "2549",
   "abstract": [
    "This paper discusses the efforts in collecting speech databases for Indian languages – Bengali, Hindi, Kannada, Malayalam, Marathi, Tamil and Telugu. We discuss relevant design considerations in collecting these databases, and demonstrate their usage in speech synthesis. By releasing these speech databases in the public domain without any restrictions for non commercial and commercial purposes, we hope to promote research and developmental activities in building speech synthesis systems in Indian languages.\n",
    "Index Terms: speech databases, speech synthesis, Indian languages\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-659"
  },
  "sansegundo12_interspeech": {
   "authors": [
    [
     "Rubén",
     "San-Segundo"
    ],
    [
     "Juan M.",
     "Montero"
    ],
    [
     "Verónica",
     "López-Ludeña"
    ],
    [
     "Simon",
     "King"
    ]
   ],
   "title": "Detecting acronyms from capital letter sequences in Spanish",
   "original": "i12_2550",
   "page_count": 4,
   "order": 672,
   "p1": "2550",
   "pn": "2553",
   "abstract": [
    "This paper presents an automatic strategy to decide how to pronounce a Capital Letter Sequence (CLS) in a Text to Speech system (TTS). If CLS is well known by the TTS, it can be expanded in several words. But when the CLS is unknown, the system has two alternatives: spelling it (abbreviation) or pronouncing it as a new word (acronym). In Spanish, there is a high relationship between letters and phonemes. Because of this, when a CLS is similar to other words in Spanish, there is a high tendency to pronounce it as a standard word. This paper proposes an automatic method for detecting acronyms. Additionaly, this paper analyses the discrimination capability of some features, and several strategies for combining them in order to obtain the best classifier. For the best classifier, the classification error is 8.45%. About the feature analysis, the best features have been the Letter Sequence Perplexity and the Average N-gram order.\n",
    "Index Terms: Capital letter sequence pronunciation, Speech synthesis, Spelling, Spanish, Acronyms, Abbreviations\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-660"
  },
  "lehnen12_interspeech": {
   "authors": [
    [
     "Patrick",
     "Lehnen"
    ],
    [
     "Stefan",
     "Hahn"
    ],
    [
     "Vlad-Andrei",
     "Guta"
    ],
    [
     "Hermann",
     "Ney"
    ]
   ],
   "title": "Hidden conditional random fields with M-to-N alignments for grapheme-to-phoneme conversion",
   "original": "i12_2554",
   "page_count": 4,
   "order": 673,
   "p1": "2554",
   "pn": "2557",
   "abstract": [
    "Conditional Random Fields have been successfully applied to a number of NLP tasks like concept tagging, named entity tagging, or graphemeto- phoneme conversion. When no alignment between source and target side is provided with the training data, it is challenging to build a CRF system with state-of-the-art performance. In this work, we present an approach incorporating an M-to-N alignment as a hidden variable within a transducer-based implementation of CRFs. Including integrated estimation of transition penalties, it was possible to train a state-of-the-art hidden CRF system in reasonable time for an English grapheme-to-phoneme conversion task without using an external model to provide the alignment.\n",
    "Index Terms: CRF, G2P, Alignment, M-N\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-661"
  },
  "rosenberg12d_interspeech": {
   "authors": [
    [
     "Andrew",
     "Rosenberg"
    ],
    [
     "Raul",
     "Fernandez"
    ],
    [
     "Bhuvana",
     "Ramabhadran"
    ]
   ],
   "title": "Phrase boundary assignment from text in multiple domains",
   "original": "i12_2558",
   "page_count": 4,
   "order": 674,
   "p1": "2558",
   "pn": "2561",
   "abstract": [
    "Detecting and modeling proper phrasing from an input text string is an important aspect when producing synthesis that sounds intelligible and natural. Knowledge of proper phrase structure influences, e.g., the placement and length of pauses, and the realization of phrase-final boundary contours, both of which can have an effect in a listener's percepts ranging from naturalness to semantic interpretation. In this work, we look at modeling the occurrence, and types, of phrase breaks from purely textual features, paying close attention to how the performance of the systems generalizes in- and out-of-domain for corpora of various types (such as broadcast news, spontaneous speech, and synthesis databases), and as a function of various subsets of syntactical and lexical features investigated.\n",
    "Index Terms: Prosody Modeling, Prosodic Assignment, Speech Synthesis\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-662"
  },
  "minematsu12_interspeech": {
   "authors": [
    [
     "Nobuaki",
     "Minematsu"
    ],
    [
     "Shumpei",
     "Kobayashi"
    ],
    [
     "Shinya",
     "Shimizu"
    ],
    [
     "Keikichi",
     "Hirose"
    ]
   ],
   "title": "Improved prediction of Japanese word accent sandhi using CRF",
   "original": "i12_2562",
   "page_count": 4,
   "order": 675,
   "p1": "2562",
   "pn": "2565",
   "abstract": [
    "In Japanese, every content word has its own mora-based H/L pitch pattern when it is uttered isolatedly, called accent type. When reading out a written sentence, however, this lexical H/L pattern is often changed according to the context, known as word accent sandhi. In our previous work, an accent sandhi predictor was developed using CRF, and in this paper, the predictor is improved through feature engineering especially focusing on phrases including numerals and those including loanwords. This is because our previous work showed that the prediction performance was relatively low for those phrases. To optimize the features used for CRF, it is critical to take into account the mechanism of word accent sandhi. We review linguistic and technical literatures that attempted to characterize accent sandhi in the phrases including numerals and loanwords and, by reflecting these characteristics, the features are re-designed. Experiments show that the proposed predictor improved the performance relatively by 37% and 41%, respectively.\n",
    "Index Terms: word accent sandhi, accent nucleus, text-to- speech, Japanese education, rule-based, corpus-based, CRF\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-663"
  },
  "toutios12_interspeech": {
   "authors": [
    [
     "Asterios",
     "Toutios"
    ],
    [
     "Shinji",
     "Maeda"
    ]
   ],
   "title": "Articulatory VCV synthesis from EMA data",
   "original": "i12_2566",
   "page_count": 4,
   "order": 676,
   "p1": "2566",
   "pn": "2569",
   "abstract": [
    "This paper reports experiments in synthesizing VCV sequences with French unvoiced stop or fricative consonants, using a time-domain simulation of the vocal-tract system. The necessary dynamics of the vocal-tract shape are derived in two steps: first, time-varying parameters of an articulatory model are calculated automatically from electromagnetic articulography (EMA) data, using a method previously published by the first author; second, semi-automatic corrections are applied to properly account for consonantal events. Time-varying characteristics of the glottis are set using empirical rules. Friction noise is generated along the length of the whole vocal-tract instead of locally at the narrowest constriction. Spectrogams of satisfactory synthesis results are presented alongside those of real speech recorded simultaneously with the articulatory data. Corresponding audio files are available online.\n",
    "Index Terms: speech production, speech synthesis, articulatory model, electromagnetic articulography\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-664"
  },
  "zellou12_interspeech": {
   "authors": [
    [
     "Georgia",
     "Zellou"
    ]
   ],
   "title": "Nasality from Moroccan Arabic nasal and pharyngeal consonants: patterns of airflow and nasalance",
   "original": "i12_2678",
   "page_count": 4,
   "order": 677,
   "p1": "2678",
   "pn": "2681",
   "abstract": [
    "This articulatory study investigates velum activity during the production of nasal and pharyngeal consonants, using airflow, and during the production of vowels adjacent to nasal and pharyngeal consonants, using airflow and nasalance, in Moroccan Arabic (MA). The results indicate that the velum is lowered during the production of pharyngeals and that this overlaps on adjacent vowels. However, nasalance patterns on vowels suggest that nasality is greatest on the portion of the vowel opposite the actual pharyngeal. Examination of both oral and nasal airflow patterns on vowels indicates that actually oral airflow is increasing significantly towards the pharyngeal, due to extreme jaw lowering for this articulation, which accounts for the observed nasality patterns. This study highlights the importance of separating out the relative contribution of the oral and nasal tracts when investigating nasality patterns.\n",
    "Index Terms: nasality, airflow, nasalance, pharyngeals\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-665"
  },
  "delvaux12_interspeech": {
   "authors": [
    [
     "Véronique",
     "Delvaux"
    ],
    [
     "Kathy",
     "Huet"
    ],
    [
     "Myriam",
     "Piccaluga"
    ],
    [
     "Bernard",
     "Harmegnies"
    ]
   ],
   "title": "Inter-gestural timing in French nasal vowels: a comparative study of (liege, tournai) northern French vs. (marseille, toulouse) southern French",
   "original": "i12_2682",
   "page_count": 4,
   "order": 678,
   "p1": "2682",
   "pn": "2685",
   "abstract": [
    "This paper provides a comparative gestural account of nasal vowels in Southern French (Marseille, Toulouse; France) and in Northern French (Liege, Tournai; Belgium). Timing measurements based on acoustic data confirm a different temporal organisation of the gestures between the two regiolects. We also report on statistical comparisons between phonetic events (such as duration of the Southern French enasal appendixf vs. duration of a plain nasal consonant), that help to confront competing hypotheses concerning the underlying phonological representation of SF nasal vowels.\n",
    "Index Terms: nasal vowels, timing, cross-regiolect variation, experimental phonology\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-666"
  },
  "zellou12b_interspeech": {
   "authors": [
    [
     "Georgia",
     "Zellou"
    ],
    [
     "Rebecca",
     "Scarborough"
    ]
   ],
   "title": "Nasal coarticulation and contrastive stress",
   "original": "i12_2686",
   "page_count": 4,
   "order": 679,
   "p1": "2686",
   "pn": "2689",
   "abstract": [
    "This study investigates the effect of contrastive stress on nasal coarticulation in English. There are two opposing findings about the correlation between coarticulation and hyperarticulation in the literature: first, that emphasis results in timing patterns which reduce coarticulation; second, that both increased hyperarticulation and increased coarticulation have been observed in words of similar lexical properties (i.e., words from dense phonological neighborhoods). The current study was designed to explore these correlations further. The results indicate that contrastively stressed words have less coarticulation and longer vowels and consonants than unstressed words; however, the neighborhood density patterns were maintained across stress conditions. We take these findings to suggest that contrastive stress is about duration whereas other (i.e., lexically conditioned) hyperarticulation is about explicit spectral reorganization.\n",
    "Index Terms: contrastive stress, coarticulation, neighborhood density\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-667"
  },
  "oliveira12b_interspeech": {
   "authors": [
    [
     "Catarina",
     "Oliveira"
    ],
    [
     "Paula",
     "Martins"
    ],
    [
     "Samuel",
     "Silva"
    ],
    [
     "António",
     "Teixeira"
    ]
   ],
   "title": "An MRI study of the oral articulation of European Portuguese nasal vowels",
   "original": "i12_2690",
   "page_count": 4,
   "order": 680,
   "p1": "2690",
   "pn": "2693",
   "abstract": [
    "There is increasing evidence that, in addition to velopharyngeal coupling, lingual position may also change during production of phonemic nasal vowels. In order to investigate differences in oral articulation between European Portuguese (EP) nasal vowels and oral counterparts, imaging data (both static and real-time MRI) of several EP speakers (male and female) are used. Superimposition of outlines of the vocal tract profiles, semi-automatically extracted from MRI images, were used to compare the position of tongue and lips during nasal and oral vowel production. The results suggest that lingual and labial differences between nasal vowels and their oral counterparts are quite subtle in EP. Nasal vowels [ɐ~], [õ] exhibited more articulatory adjustments with respect to oral congeners than [i~] and [u~].\n",
    "Index Terms: nasal vowels, Magnetic Resonance Imaging (MRI), European Portuguese\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-668"
  },
  "scarborough12b_interspeech": {
   "authors": [
    [
     "Rebecca",
     "Scarborough"
    ],
    [
     "Georgia",
     "Zellou"
    ]
   ],
   "title": "Acoustic and perceptual similarity in coarticulatorily nasalized vowels",
   "original": "i12_2694",
   "page_count": 4,
   "order": 681,
   "p1": "2694",
   "pn": "2697",
   "abstract": [
    "This study investigates the acoustic and perceptual consequences of nasal coarticulation in English. Nasalized (coarticulated) vowels were found to be closer in the F1-F2 acoustic vowel space than corresponding oral (non-coarticulated) vowels, indicating that contrast is reduced in the nasal vowel space, relative to the oral vowel space. With respect to perception, listeners are, perhaps unsurprisingly, more accurate in identifying oral vowels than nasalized vowels. Interestingly, however, while listeners take longer to identify nasalized vowels than oral vowels when they hear those vowels in isolation, this difference in processing time disappears when the nasalized and oral vowels are heard in lexical contexts. We take these findings to indicate that listeners compensate for nasal coarticulation, albeit sometimes incompletely, attributing the acoustic effects to their consonantal source, and that this compensation takes place instantaneously.\n",
    "Index Terms: nasality, coarticulation, perception, similarity\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-669"
  },
  "rong12_interspeech": {
   "authors": [
    [
     "Panying",
     "Rong"
    ],
    [
     "Ryan K.",
     "Shosted"
    ],
    [
     "David",
     "Kuehn"
    ]
   ],
   "title": "Articulatory differences between oral and nasal vowels based on the simulation of a speaker-adaptive articulatory model",
   "original": "i12_2698",
   "page_count": 4,
   "order": 682,
   "p1": "2698",
   "pn": "2701",
   "abstract": [
    "In this study, a speaker-adaptive articulatory model was constructed by fitting point-wise articulatory positions measured by Electromagnetic Articulography (EMA) to the framework of Childerfs vocal tract model (2000) to customize the standard vocal tract model with speaker-dependent articulatory features. With the speaker-adaptive articulatory model, the area functions of oral and nasal vowel pairs (/a/, /i/, /u/) were simulated. The differences of area functions between oral and nasal vowels were decomposed into orthogonal modes (Story et al., 1998) to account for the primary articulatory changes related to nasalization. The relationship between the principal articulatory modes and the lowest two formant frequencies (F1, F2) was examined to indicate the effect of oropharyngeal articulatory alterations on the acoustics of nasal vowels.\n",
    "Index Terms: articulatory modeling, nasalization, electromagnetic articulography\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2012-670"
  }
 },
 "sessions": [
  {
   "title": "Keynote Papers",
   "papers": [
    "lee12_interspeech",
    "dannenberg12_interspeech",
    "riley12_interspeech",
    "lahvis12_interspeech"
   ]
  },
  {
   "title": "ASR: Deep Neural Networks I, II",
   "papers": [
    "yu12_interspeech",
    "kingsbury12_interspeech",
    "saon12_interspeech",
    "tuske12_interspeech",
    "maas12_interspeech",
    "chen12_interspeech",
    "vinyals12_interspeech",
    "xiao12_interspeech",
    "jaitly12_interspeech",
    "qian12_interspeech",
    "vu12_interspeech",
    "siniscalchi12_interspeech",
    "kubo12_interspeech",
    "deng12_interspeech",
    "qian12b_interspeech",
    "fernandezastudillo12_interspeech"
   ]
  },
  {
   "title": "Language Recognition",
   "papers": [
    "boril12_interspeech",
    "greenberg12_interspeech",
    "rodriguezfuentes12_interspeech",
    "dharo12_interspeech",
    "mccree12_interspeech",
    "matejka12_interspeech"
   ]
  },
  {
   "title": "Communication Disorders and Assistive Technologies",
   "papers": [
    "hu12_interspeech",
    "bechet12_interspeech",
    "paja12_interspeech",
    "kacha12_interspeech",
    "fuchs12_interspeech",
    "richmond12_interspeech"
   ]
  },
  {
   "title": "Voice Conversion",
   "papers": [
    "hwang12_interspeech",
    "li12_interspeech",
    "erro12_interspeech",
    "percybrooks12_interspeech",
    "toda12_interspeech",
    "saito12_interspeech"
   ]
  },
  {
   "title": "Phonetics and Phonology I, II",
   "papers": [
    "weninger12_interspeech",
    "avanzi12_interspeech",
    "jannedy12_interspeech",
    "ishi12_interspeech",
    "kallay12_interspeech",
    "jian12_interspeech",
    "ouyang12_interspeech",
    "dicanio12_interspeech",
    "fujimoto12_interspeech",
    "nadeu12_interspeech",
    "mcauliffe12_interspeech",
    "fox12_interspeech",
    "scarborough12_interspeech",
    "chen12b_interspeech",
    "simonet12_interspeech"
   ]
  },
  {
   "title": "Enhancement",
   "papers": [
    "feher12_interspeech",
    "xue12_interspeech",
    "marinhurtado12_interspeech",
    "yu12b_interspeech",
    "bao12_interspeech",
    "mirbagheri12_interspeech",
    "hazrati12_interspeech",
    "petkov12_interspeech"
   ]
  },
  {
   "title": "Language Modeling",
   "papers": [
    "mousa12_interspeech",
    "yamamoto12_interspeech",
    "alumae12_interspeech",
    "lecorve12_interspeech",
    "tam12_interspeech",
    "wu12_interspeech",
    "sundermeyer12_interspeech",
    "xu12_interspeech",
    "karakos12_interspeech",
    "dikici12_interspeech",
    "thadani12_interspeech"
   ]
  },
  {
   "title": "Spoken Language Understanding and Dialog I, II",
   "papers": [
    "jabaian12_interspeech",
    "griol12_interspeech",
    "sugiyama12_interspeech",
    "meena12_interspeech",
    "komatani12_interspeech",
    "planells12_interspeech",
    "kazemzadeh12_interspeech",
    "seide12_interspeech",
    "engelbrecht12_interspeech",
    "callejas12_interspeech",
    "kretzschmar12_interspeech",
    "witt12_interspeech",
    "hakkanitur12_interspeech",
    "shriberg12_interspeech",
    "tur12_interspeech",
    "fandrianto12_interspeech"
   ]
  },
  {
   "title": "Speaker Trait Challenge I, II (Special Session)",
   "papers": [
    "schuller12_interspeech",
    "polzehl12_interspeech",
    "audhkhasi12_interspeech",
    "chastagnol12_interspeech",
    "pohjalainen12_interspeech",
    "wagner12_interspeech",
    "ivanov12_interspeech",
    "cummins12_interspeech",
    "lu12_interspeech",
    "brueckner12_interspeech",
    "wu12b_interspeech",
    "weiss12_interspeech",
    "sanchez12_interspeech",
    "buisman12_interspeech",
    "attabi12_interspeech",
    "montacie12_interspeech",
    "anumanchipalli12_interspeech",
    "kim12_interspeech",
    "stark12_interspeech",
    "zhou12_interspeech",
    "huang12_interspeech"
   ]
  },
  {
   "title": "ASR: Noise Robustness",
   "papers": [
    "kumatani12_interspeech",
    "weninger12b_interspeech",
    "lu12b_interspeech",
    "sun12_interspeech",
    "li12b_interspeech",
    "bouallegue12_interspeech"
   ]
  },
  {
   "title": "Paralinguistics I-III",
   "papers": [
    "ringeval12_interspeech",
    "wollmer12_interspeech",
    "swerts12_interspeech",
    "cao12_interspeech",
    "zhang12_interspeech",
    "rozgic12_interspeech",
    "weninger12c_interspeech",
    "ding12_interspeech",
    "kim12b_interspeech",
    "reichel12_interspeech",
    "bone12_interspeech",
    "busso12_interspeech",
    "valente12_interspeech",
    "lyu12_interspeech",
    "deng12b_interspeech",
    "xia12_interspeech",
    "obin12_interspeech",
    "fewzee12_interspeech",
    "rosenberg12_interspeech",
    "oertel12_interspeech",
    "fecher12_interspeech",
    "can12_interspeech"
   ]
  },
  {
   "title": "Pitch and Harmonic Analysis",
   "papers": [
    "leon12_interspeech",
    "wen12_interspeech",
    "huang12b_interspeech",
    "degottex12_interspeech",
    "kawahara12_interspeech",
    "yoshizato12_interspeech"
   ]
  },
  {
   "title": "Perceptual Learning and Perceptual Cues to Segments and Tones",
   "papers": [
    "sjerps12_interspeech",
    "scharenborg12_interspeech",
    "hatano12_interspeech",
    "jagbandhu12_interspeech",
    "dubois12_interspeech",
    "kertkeidkachorn12_interspeech",
    "tantibundhit12_interspeech",
    "meyer12_interspeech",
    "cao12b_interspeech",
    "tyler12_interspeech"
   ]
  },
  {
   "title": "Speech Synthesis: Prosody",
   "papers": [
    "morley12_interspeech",
    "baumann12_interspeech",
    "iwata12_interspeech",
    "parlikar12_interspeech",
    "gruber12_interspeech",
    "norrenbrock12_interspeech",
    "hashimoto12_interspeech",
    "koriyama12_interspeech",
    "meng12_interspeech",
    "hoffmann12_interspeech",
    "ohishi12_interspeech"
   ]
  },
  {
   "title": "Speaker Diarization and Age Recognition",
   "papers": [
    "silovsky12_interspeech",
    "shum12_interspeech",
    "knox12_interspeech",
    "yella12_interspeech",
    "bozonnet12_interspeech",
    "kelly12_interspeech",
    "leeuwen12_interspeech",
    "bahari12_interspeech"
   ]
  },
  {
   "title": "ASR: Discriminative Training",
   "papers": [
    "rath12_interspeech",
    "tomar12_interspeech",
    "weng12_interspeech",
    "suzuki12_interspeech",
    "hu12b_interspeech",
    "tahir12_interspeech"
   ]
  },
  {
   "title": "Single Channel Speech Enhancement",
   "papers": [
    "boucheron12_interspeech",
    "koutsogiannaki12_interspeech",
    "carlin12_interspeech",
    "kolossa12_interspeech",
    "jokinen12_interspeech",
    "duan12_interspeech"
   ]
  },
  {
   "title": "Conversation and Interaction I, II",
   "papers": [
    "varnet12_interspeech",
    "quene12_interspeech",
    "lunsford12_interspeech",
    "garnier12_interspeech",
    "wodarczak12_interspeech",
    "lee12b_interspeech",
    "levow12_interspeech",
    "strombergsson12_interspeech",
    "truong12_interspeech",
    "laskowski12_interspeech",
    "truong12b_interspeech",
    "gravano12_interspeech"
   ]
  },
  {
   "title": "Speech Synthesis: Intelligibility",
   "papers": [
    "syrdal12_interspeech",
    "wang12_interspeech",
    "valentinibotinhao12_interspeech",
    "zorila12_interspeech",
    "erro12b_interspeech",
    "mohammadi12_interspeech"
   ]
  },
  {
   "title": "Prosody I, II",
   "papers": [
    "tsurutani12_interspeech",
    "avanzi12b_interspeech",
    "hsieh12_interspeech",
    "doukhan12_interspeech",
    "wang12b_interspeech",
    "zhang12b_interspeech",
    "vainio12_interspeech",
    "warsi12_interspeech",
    "braun12_interspeech",
    "benton12_interspeech",
    "meyer12b_interspeech",
    "wayland12_interspeech",
    "igarashi12_interspeech",
    "kuo12_interspeech",
    "hon12_interspeech",
    "moniz12_interspeech",
    "lintfert12_interspeech"
   ]
  },
  {
   "title": "Speech Analysis",
   "papers": [
    "pohjalainen12b_interspeech",
    "gomathi12_interspeech",
    "kasess12_interspeech",
    "birkholz12_interspeech",
    "lee12c_interspeech",
    "sepulveda12_interspeech",
    "bruckl12_interspeech",
    "hansakunbuntheung12_interspeech"
   ]
  },
  {
   "title": "Dialog Systems",
   "papers": [
    "hueber12_interspeech",
    "kawahara12b_interspeech",
    "wechsung12_interspeech",
    "fang12_interspeech",
    "lilley12_interspeech",
    "maciasgalindo12_interspeech",
    "heeman12_interspeech",
    "lopezcozar12_interspeech"
   ]
  },
  {
   "title": "Speech and Language Technologies for STEM (Special Session)",
   "papers": [
    "litman12_interspeech",
    "ward12_interspeech",
    "cerva12_interspeech",
    "chen12c_interspeech",
    "saz12_interspeech",
    "qian12c_interspeech",
    "cucchiarini12_interspeech",
    "hueber12b_interspeech"
   ]
  },
  {
   "title": "ASR: Bayesian Modeling",
   "papers": [
    "kintzley12_interspeech",
    "thomas12_interspeech",
    "cui12_interspeech",
    "vanhainen12_interspeech",
    "hahm12_interspeech",
    "krueger12_interspeech"
   ]
  },
  {
   "title": "Computer Assisted Language Learning I, II",
   "papers": [
    "yilmaz12_interspeech",
    "huang12c_interspeech",
    "wang12c_interspeech",
    "honig12_interspeech",
    "stanley12_interspeech",
    "gemmeke12_interspeech",
    "iribe12_interspeech",
    "jeon12_interspeech",
    "wuth12_interspeech",
    "bell12_interspeech",
    "zhang12c_interspeech",
    "pellegrini12_interspeech",
    "chen12d_interspeech",
    "cheng12_interspeech"
   ]
  },
  {
   "title": "Speech Analysis and Modeling",
   "papers": [
    "kafentzis12_interspeech",
    "ng12_interspeech",
    "uria12_interspeech",
    "henry12_interspeech",
    "khanagha12_interspeech",
    "jansen12_interspeech"
   ]
  },
  {
   "title": "Language Learning and Cross-Language Production and Perception",
   "papers": [
    "scharenborg12b_interspeech",
    "meister12_interspeech",
    "sadakata12_interspeech",
    "nariai12_interspeech",
    "koniaris12_interspeech",
    "li12c_interspeech",
    "zeroual12_interspeech",
    "tyler12b_interspeech",
    "rasanen12_interspeech",
    "rasanen12b_interspeech"
   ]
  },
  {
   "title": "Enhancement and Coding",
   "papers": [
    "wang12d_interspeech",
    "moller12_interspeech",
    "backstrom12_interspeech",
    "saha12_interspeech",
    "trawicki12_interspeech",
    "harding12_interspeech",
    "chetupally12_interspeech",
    "liang12_interspeech",
    "tang12_interspeech"
   ]
  },
  {
   "title": "Speech Synthesis: Adaptation",
   "papers": [
    "chen12e_interspeech",
    "he12_interspeech",
    "veaux12_interspeech",
    "latorre12_interspeech",
    "sung12_interspeech",
    "schabus12_interspeech",
    "oliveira12_interspeech",
    "nicolao12_interspeech",
    "ling12_interspeech",
    "dall12_interspeech"
   ]
  },
  {
   "title": "Search and Decoding",
   "papers": [
    "chen12f_interspeech",
    "duckhorn12_interspeech",
    "dufour12_interspeech",
    "kobashikawa12_interspeech",
    "nolden12_interspeech",
    "mcgraw12_interspeech",
    "ziegler12_interspeech",
    "miranda12_interspeech",
    "bougares12_interspeech",
    "kim12c_interspeech"
   ]
  },
  {
   "title": "Analysis of Spoken Disorders in Health Applications I, II (Special Session)",
   "papers": [
    "lehr12_interspeech",
    "bone12b_interspeech",
    "kaland12_interspeech",
    "hagedorn12_interspeech",
    "abad12_interspeech",
    "quatieri12_interspeech",
    "drugman12_interspeech",
    "chen12g_interspeech",
    "raev12_interspeech",
    "swerts12b_interspeech",
    "chaspari12_interspeech",
    "kim12d_interspeech",
    "wang12e_interspeech",
    "yin12_interspeech",
    "mehta12_interspeech",
    "hassanali12_interspeech",
    "kiss12_interspeech"
   ]
  },
  {
   "title": "Dynamic Decoding",
   "papers": [
    "jyothi12_interspeech",
    "deoras12_interspeech",
    "bashashaik12_interspeech",
    "dixon12_interspeech",
    "novak12_interspeech",
    "shore12_interspeech"
   ]
  },
  {
   "title": "Speaker Recognition I-III",
   "papers": [
    "mcclanahan12_interspeech",
    "hasan12_interspeech",
    "campbell12_interspeech",
    "sun12b_interspeech",
    "doddington12_interspeech",
    "hattori12_interspeech",
    "wang12f_interspeech",
    "liang12b_interspeech",
    "liang12c_interspeech",
    "hasan12b_interspeech",
    "machlica12_interspeech",
    "chen12h_interspeech",
    "larcher12_interspeech",
    "dellwo12_interspeech",
    "lei12_interspeech",
    "poignant12_interspeech",
    "zhao12_interspeech",
    "yang12_interspeech",
    "sarkar12_interspeech",
    "huang12d_interspeech",
    "hyon12_interspeech",
    "remus12_interspeech"
   ]
  },
  {
   "title": "Development of Speech Production and Perception",
   "papers": [
    "marklund12_interspeech",
    "marklund12b_interspeech",
    "kim12e_interspeech",
    "xu12b_interspeech",
    "nagao12_interspeech",
    "strombergsson12b_interspeech"
   ]
  },
  {
   "title": "HMM Synthesis I, II",
   "papers": [
    "wan12_interspeech",
    "takamichi12_interspeech",
    "lu12c_interspeech",
    "yin12b_interspeech",
    "chunwijitra12_interspeech",
    "ohtani12_interspeech",
    "raitio12_interspeech",
    "drugman12b_interspeech",
    "wen12b_interspeech",
    "nishizawa12_interspeech",
    "silen12_interspeech",
    "ohtani12b_interspeech"
   ]
  },
  {
   "title": "ASR: Robust Modeling",
   "papers": [
    "imseng12_interspeech",
    "wang12g_interspeech",
    "gales12_interspeech",
    "hartmann12_interspeech",
    "wiesler12_interspeech",
    "pylkkonen12_interspeech",
    "novotney12_interspeech",
    "li12d_interspeech",
    "yao12_interspeech",
    "du12_interspeech"
   ]
  },
  {
   "title": "ASR: Robust Features I, II",
   "papers": [
    "moritz12_interspeech",
    "demir12_interspeech",
    "narayanan12_interspeech",
    "gomez12_interspeech",
    "kristjansson12_interspeech",
    "odani12_interspeech",
    "pardede12_interspeech",
    "meyer12c_interspeech",
    "li12e_interspeech",
    "arsikere12_interspeech",
    "carlin12b_interspeech",
    "yao12b_interspeech",
    "tsai12_interspeech",
    "alam12_interspeech",
    "muller12_interspeech",
    "pessentheiner12_interspeech"
   ]
  },
  {
   "title": "Rich Transcription I, II",
   "papers": [
    "prazak12_interspeech",
    "kolar12_interspeech",
    "ikbal12_interspeech",
    "wang12h_interspeech",
    "brugnara12_interspeech",
    "rosenberg12b_interspeech",
    "safavi12_interspeech",
    "bordel12_interspeech",
    "takashima12_interspeech",
    "lee12d_interspeech",
    "charlet12_interspeech",
    "mai12_interspeech",
    "tiwari12_interspeech",
    "mishra12_interspeech",
    "srivastava12_interspeech"
   ]
  },
  {
   "title": "Hearing",
   "papers": [
    "rasanen12c_interspeech",
    "saratxaga12_interspeech",
    "grigorescu12_interspeech",
    "nagao12b_interspeech",
    "scharenborg12c_interspeech",
    "hodoshima12_interspeech",
    "hines12_interspeech",
    "godoy12_interspeech",
    "amanokusumoto12_interspeech",
    "mesgarani12_interspeech",
    "edlund12_interspeech"
   ]
  },
  {
   "title": "Degraded Speech and Enhancement",
   "papers": [
    "gonzalez12_interspeech",
    "thambiratnam12_interspeech",
    "sharma12_interspeech",
    "yokoyama12_interspeech",
    "lu12d_interspeech",
    "chakraborty12_interspeech",
    "deepak12_interspeech",
    "reidy12_interspeech"
   ]
  },
  {
   "title": "Source Separation and Computational Auditory Scene Analysis",
   "papers": [
    "grais12_interspeech",
    "ranjan12_interspeech",
    "wang12i_interspeech",
    "wang12j_interspeech",
    "grais12b_interspeech",
    "ji12_interspeech",
    "zhang12d_interspeech",
    "mowlaee12_interspeech",
    "chien12_interspeech"
   ]
  },
  {
   "title": "Glottal Source Processing: from Analysis to Applications (Special Session)",
   "papers": [
    "drugman12c_interspeech",
    "mittal12_interspeech",
    "chen12i_interspeech",
    "pinheiro12_interspeech",
    "drioli12_interspeech",
    "alku12_interspeech",
    "sasou12_interspeech",
    "lorenzotrueba12_interspeech",
    "alpan12_interspeech",
    "sun12c_interspeech",
    "maia12_interspeech",
    "mertens12_interspeech",
    "auvinen12_interspeech",
    "huber12_interspeech",
    "godin12_interspeech",
    "torres12_interspeech"
   ]
  },
  {
   "title": "Language Modeling: New Models and Features",
   "papers": [
    "liu12_interspeech",
    "rastrow12_interspeech",
    "shi12_interspeech",
    "lecorve12b_interspeech",
    "kuo12b_interspeech",
    "hutchinson12_interspeech"
   ]
  },
  {
   "title": "Speaker Verification",
   "papers": [
    "jiang12_interspeech",
    "simonchik12_interspeech",
    "alegre12_interspeech",
    "stafylakis12_interspeech",
    "sadjadi12_interspeech",
    "wu12c_interspeech"
   ]
  },
  {
   "title": "Speech Intelligibility in Quiet and in Noise",
   "papers": [
    "villegas12_interspeech",
    "mayo12_interspeech",
    "kawase12_interspeech",
    "brown12_interspeech",
    "fitzpatrick12_interspeech",
    "santos12_interspeech"
   ]
  },
  {
   "title": "Audio Analysis, Estimation and Classification",
   "papers": [
    "chaudhuri12_interspeech",
    "liu12b_interspeech",
    "shi12b_interspeech",
    "segura12_interspeech",
    "de12_interspeech",
    "bouafif12_interspeech",
    "nakashika12_interspeech",
    "berry12_interspeech"
   ]
  },
  {
   "title": "Adaptation for ASR",
   "papers": [
    "asami12_interspeech",
    "kim12f_interspeech",
    "jiang12b_interspeech",
    "li12f_interspeech",
    "christensen12_interspeech",
    "uluskan12_interspeech",
    "roupakia12_interspeech",
    "chen12j_interspeech",
    "seltzer12_interspeech"
   ]
  },
  {
   "title": "Robust Speech Recognition I, II",
   "papers": [
    "huang12e_interspeech",
    "variani12_interspeech",
    "jalalvand12_interspeech",
    "pylkkonen12b_interspeech",
    "schadler12_interspeech",
    "li12g_interspeech",
    "rasipuram12_interspeech",
    "alshareef12_interspeech",
    "ganapathy12_interspeech",
    "zhang12e_interspeech",
    "diehl12_interspeech",
    "nussbaumthom12_interspeech",
    "ding12b_interspeech",
    "hsieh12b_interspeech",
    "matsuda12_interspeech",
    "gonzalez12b_interspeech",
    "abdelaziz12_interspeech",
    "ma12_interspeech",
    "ludusan12_interspeech",
    "chien12b_interspeech"
   ]
  },
  {
   "title": "Speech Tools and Systems Demo (Special Session)",
   "papers": [
    "metze12_interspeech",
    "reichel12b_interspeech",
    "ouni12_interspeech",
    "lenkiewicz12_interspeech",
    "ashby12_interspeech",
    "chappel12_interspeech",
    "okamoto12_interspeech",
    "boyce12_interspeech",
    "bell12b_interspeech",
    "sun12d_interspeech",
    "hofmann12_interspeech",
    "cole12_interspeech",
    "finomorejr12_interspeech",
    "pelemans12_interspeech",
    "tejedor12_interspeech"
   ]
  },
  {
   "title": "Adaptation &amp; Robust Modeling",
   "papers": [
    "he12b_interspeech",
    "nallasamy12_interspeech",
    "li12h_interspeech",
    "cernak12_interspeech",
    "zhang12f_interspeech",
    "mohammed12_interspeech"
   ]
  },
  {
   "title": "Multi-Channel Speech Enhancement",
   "papers": [
    "bispo12_interspeech",
    "kinoshita12_interspeech",
    "zhao12b_interspeech",
    "singh12_interspeech",
    "yu12c_interspeech",
    "ritch12_interspeech"
   ]
  },
  {
   "title": "Voice Activity Detection",
   "papers": [
    "ng12b_interspeech",
    "omar12_interspeech",
    "misra12_interspeech",
    "harding12b_interspeech",
    "thomas12b_interspeech",
    "wang12k_interspeech"
   ]
  },
  {
   "title": "Perception and Production",
   "papers": [
    "yip12_interspeech",
    "ijima12_interspeech",
    "babel12_interspeech",
    "bosch12_interspeech",
    "li12i_interspeech",
    "hanique12_interspeech",
    "birkholz12b_interspeech",
    "viebahn12_interspeech",
    "yoshinaga12_interspeech",
    "orozcoarroyave12_interspeech",
    "aubanel12_interspeech"
   ]
  },
  {
   "title": "Language and Accent Recognition",
   "papers": [
    "huang12f_interspeech",
    "benzeghiba12_interspeech",
    "penagarikano12_interspeech",
    "penagarikano12b_interspeech",
    "yaman12_interspeech",
    "jiang12c_interspeech",
    "shih12_interspeech",
    "you12_interspeech",
    "varona12_interspeech",
    "mehrabani12_interspeech"
   ]
  },
  {
   "title": "Voice Search and Spoken Document Retrieval I, II",
   "papers": [
    "lee12e_interspeech",
    "byun12_interspeech",
    "jin12_interspeech",
    "zhuang12_interspeech",
    "liu12c_interspeech",
    "majima12_interspeech",
    "tsakalidis12_interspeech",
    "pancoast12_interspeech",
    "iso12_interspeech",
    "liu12d_interspeech",
    "wen12c_interspeech",
    "allauzen12_interspeech",
    "jansen12b_interspeech",
    "fayolle12_interspeech",
    "mcgraw12b_interspeech"
   ]
  },
  {
   "title": "Sparse, Template-Based Representations",
   "papers": [
    "sainath12_interspeech",
    "gemmeke12b_interspeech",
    "hurmalainen12_interspeech",
    "sun12e_interspeech",
    "soldo12_interspeech",
    "wang12l_interspeech"
   ]
  },
  {
   "title": "Speaker Diarization",
   "papers": [
    "geiger12_interspeech",
    "martinezgonzalez12_interspeech",
    "toledoronen12_interspeech",
    "tawara12_interspeech",
    "vijayasenan12_interspeech",
    "dupuy12_interspeech"
   ]
  },
  {
   "title": "Speech Production: Imaging and Models",
   "papers": [
    "israel12_interspeech",
    "shosted12_interspeech",
    "vargas12_interspeech",
    "arai12_interspeech",
    "kaburagi12_interspeech",
    "lucero12_interspeech"
   ]
  },
  {
   "title": "Speech Synthesis",
   "papers": [
    "bollepalli12_interspeech",
    "janska12_interspeech",
    "greene12_interspeech",
    "sorin12_interspeech",
    "bailly12_interspeech",
    "potard12_interspeech"
   ]
  },
  {
   "title": "Speech and Speaker Segmentation",
   "papers": [
    "mehrabani12b_interspeech",
    "ghosh12_interspeech",
    "dennis12_interspeech",
    "kalinli12_interspeech",
    "kua12_interspeech",
    "lorenzotrueba12b_interspeech",
    "mohammadi12b_interspeech",
    "feng12_interspeech",
    "karnjanadecha12_interspeech"
   ]
  },
  {
   "title": "Spoken Language Understanding",
   "papers": [
    "vertanen12_interspeech",
    "schlippe12_interspeech",
    "senay12_interspeech",
    "cerisara12_interspeech",
    "yamahata12_interspeech",
    "ward12b_interspeech",
    "kubo12b_interspeech",
    "koco12_interspeech",
    "akita12_interspeech",
    "li12j_interspeech",
    "azim12_interspeech",
    "seigel12_interspeech"
   ]
  },
  {
   "title": "Spoken Language Applications",
   "papers": [
    "lee12f_interspeech",
    "chen12k_interspeech",
    "feng12b_interspeech",
    "kumar12_interspeech",
    "maskey12_interspeech",
    "perez12_interspeech",
    "ryu12_interspeech",
    "ogata12_interspeech",
    "xie12_interspeech",
    "maskey12b_interspeech"
   ]
  },
  {
   "title": "Prosodic Prominence: Annotation, Prediction, Applications (Special Session)",
   "papers": [
    "escuderomancebo12_interspeech",
    "wagner12b_interspeech",
    "heckmann12_interspeech",
    "arnold12_interspeech",
    "badino12_interspeech",
    "cutugno12_interspeech",
    "samlowski12_interspeech",
    "rosenberg12c_interspeech",
    "goldman12_interspeech",
    "sappok12_interspeech",
    "mahrt12_interspeech",
    "andreeva12_interspeech"
   ]
  },
  {
   "title": "Spoken Term and Unseen Word Detection",
   "papers": [
    "li12k_interspeech",
    "karanasou12_interspeech",
    "kintzley12b_interspeech",
    "norouzian12_interspeech",
    "bulyko12_interspeech",
    "qin12_interspeech"
   ]
  },
  {
   "title": "Speech and Age Differences",
   "papers": [
    "vosoughi12_interspeech",
    "plummer12_interspeech",
    "saikachi12_interspeech",
    "shport12_interspeech",
    "redford12_interspeech",
    "fogerty12_interspeech"
   ]
  },
  {
   "title": "Acoustic Classification",
   "papers": [
    "hu12c_interspeech",
    "leng12_interspeech",
    "patil12_interspeech",
    "ziaei12_interspeech",
    "huang12g_interspeech",
    "tan12_interspeech"
   ]
  },
  {
   "title": "Speech Synthesis: Selected Topics",
   "papers": [
    "novak12b_interspeech",
    "luan12_interspeech",
    "veilleux12_interspeech",
    "hahn12_interspeech",
    "berthommier12_interspeech",
    "prahallad12_interspeech",
    "sansegundo12_interspeech",
    "lehnen12_interspeech",
    "rosenberg12d_interspeech",
    "minematsu12_interspeech",
    "toutios12_interspeech"
   ]
  },
  {
   "title": "New Trends in Vowel Nasalization: The Articulation of Nasal Vowels (Special Session)",
   "papers": [
    "zellou12_interspeech",
    "delvaux12_interspeech",
    "zellou12b_interspeech",
    "oliveira12b_interspeech",
    "scarborough12b_interspeech",
    "rong12_interspeech"
   ]
  }
 ],
 "doi": "10.21437/Interspeech.2012"
}