{
 "location": "Volterra, Italy",
 "startDate": "1/9/2011",
 "endDate": "2/9/2011",
 "conf": "AVSP",
 "year": "2011",
 "name": "avsp_2011",
 "series": "AVSP",
 "SIG": "AVISA",
 "title": "Auditory-Visual Speech Processing",
 "title1": "Auditory-Visual Speech Processing",
 "date": "1-2 September 2011",
 "booklet": "avsp_2011.pdf",
 "papers": {
  "sjolander11_avsp": {
   "authors": [
    [
     "Sverre",
     "Sjölander"
    ]
   ],
   "title": "Acoustical and visual processing in the animal kingdom",
   "original": "av11_001",
   "page_count": 1,
   "order": 1,
   "p1": "1 (abstract)",
   "pn": "",
   "abstract": [
    "First, it should be pointed out that animals in general only are sensitive to those parts of the available information that is of importance to their species. If the only acoustical signal of importance to a grasshopper is the sound of the male, this is all the female can hear. The only molecules that an animal reacts to are those of any significance to the species — they are “smell”. If colours are of importance, colour vision will develop, but only then. I.e. the information every species receives is the one that evolution has selected as important.\n",
    "Secondly, in the animal kingdom, the question is rarely of some kind of information corresponds to the actual situation in reality. In sending information, whether acoustical, visual, tactical, olfactory, electrical or whatever, the behaviour is directed at creating something favourable for the sender. You do not want to tell the truth about e.g. your strength or your muscular mass: you want the receiver to be impressed, as much as possible. Consequently the animal world is full of bluffing. You increase your body size as much as possible, by inflating, by special feathers or fins, etc. You deepen your voice by e.g. using a big balloon under you jaw, like in frogs, thus sounding bigger than you are.\n",
    "The receiver on his hand has to try to look behind the bluff, to estimate how strong or dangerous the sender actually is. Thus, in animal communication we get a series from bluffing to trying to test the bluff, and ultimately to fight, if nobody gives up. In short, it is just like human party conversation: try to give a favourable impression of yourself, exaggerate your good points and cover up the weak ones, being aware that the receiver the whole time deducts from what she or he receives, to arrive at a truthful impression. Too much boasting is counterproductive, since the receiver will deduct even more.\n",
    "A very important point is that humans, and some degree the great apes (at least)—probably dolphins as well — are aware of their own bluffs and lying, or exaggerations. Other animals do not seem to understand what they are doing, in this sense. But a chimpanzee may produce a simple lie, to get some advantage. It is of interest that human children do not start to lie, more consciously, until about 4 years of age.\n",
    ""
   ]
  },
  "massey11_avsp": {
   "authors": [
    [
     "Colm",
     "Massey"
    ]
   ],
   "title": "From actor to avatar: real world challenges in capturing the human face",
   "original": "av11_003",
   "page_count": 1,
   "order": 2,
   "p1": "3 (abstract)",
   "pn": "",
   "abstract": [
    "In Nov 2009, James Cameron’s seminal movie Avatar raised the bar for what can be achieved with facial animation. For many, the “Uncanny Valley”, the bane of the animation industry for so long seemed to have been comprehensively bridged. But what does it take to create such jaw dropping animations, and is it possible to create them on anything less than a titanic budget with armies of animators? This talk will provide an overview of what it takes to create high end facial animation using motion capture technology. What are the current challenges facing CG studios, attempting to map human performances onto computer game and movie characters? What tools and pipelines are currently used? Can these tools be of use to AVSP researchers? What can AVSP researchers contribute to the goal of making high end facial animation accessible to those of us with shallower pockets?\n",
    ""
   ]
  },
  "paris11_avsp": {
   "authors": [
    [
     "Tim",
     "Paris"
    ],
    [
     "Jeesun",
     "Kim"
    ],
    [
     "Chris",
     "Davis"
    ]
   ],
   "title": "Visual speech influences speeded auditory identification",
   "original": "av11_005",
   "page_count": 4,
   "order": 3,
   "p1": "5",
   "pn": "8",
   "abstract": [
    "Auditory speech perception is faster and more accurate when combined with visual speech. We attempted to replicate previous findings that suggested visual speech facilitates auditory processing when speech is paired with matching video and interferes with processing when paired with mismatched videos. Crucially we employed button presses instead of a vocal response to determine if previous results could be attributed to the specific nature of the task. Stimuli consisted of the sounds 'apa’, 'aka' and 'ata', with matched and mismatched videos that showed the talker’s whole face or upper face (control). The percentage of matched AV videos was set at 85% in the congruent condition and 15% in the incongruent condition. The results show that speeded identification decisions influence auditory processing. Furthermore, this influence is moderated by (a) visual speech acting as a temporal cue to the acoustic signal and (b) resolving the perceived differences between visual and auditory modalities. The current study builds on previous results suggesting visual speech plays a role in the termporal processing of auditory speech\n",
    ""
   ]
  },
  "best11_avsp": {
   "authors": [
    [
     "Catherine T.",
     "Best"
    ],
    [
     "Christian",
     "Kroos"
    ],
    [
     "Julia",
     "Irwin"
    ]
   ],
   "title": "Do infants detect a-v articulator congruency for non-native click consonants?",
   "original": "av11_009",
   "page_count": 6,
   "order": 4,
   "p1": "9",
   "pn": "14",
   "abstract": [
    "In a prior study infants habituated to an audio-only labial or alveolar, native English voiceless or non-native ejective stop, then saw silent videos of stops at each place [1]. 4-month-olds gazed more at congruent videos for native and non-native stops. 11-month-olds preferred congruence for native stops but incongruence for non-native ejectives, suggesting language experience biases but does not block detection of non-native A.V speech relations. But as English adults perceive ejectives as deviant stops [2], we asked whether infants detect A.V congruence in non-native phones adults hear as nonspeech, i.e., click consonants [3-6]. 4-month-olds preferred incongruency; 11-month-olds showed no preference. We posit that infants prefer A.V congruency for phones heard as native-like speech; prefer incongruency for phones heard as speech that deviates from native segments; notice extreme deviance earlier (clicks: 4 mo; ejectives: 11 mo); and later treat very deviant phones as discriminable nonspeech sounds [3, 4] that are unrelated to visual speech. Results are at odds with existing AV models, but may be handled by a hybrid of Amodal Articulatory and Intersensory Narrowing views.\n",
    "",
    "",
    "Index Terms. infant speech perception, cross-modal, articulatory phonology, non-native contrasts, click consonants\n",
    "s Best, C. T., Kroos, C. H. and Irwin, J. “Now I see what you said: Infant sensitivity to place congruency between audio-only and silent-video presentations of native and non-native consonants”, Proceedings of AVSP (AudioVisual Speech Perception). Hakone, Japan, Sept-Oct., 2010. Best, C. T., McRoberts, G. W., and Goodell, E. “American listeners' perception of non-native consonant contrasts varying in perceptual assimilation to English phonology”, J. Acoust. Soc. America, 1097: 775-794, 2001. Best, C. T., McRoberts, G. W., and Sithole, N. M., “Examination of perceptual reorganization for non-native speech contrasts: Zulu click discrimination by English-speaking adults and infants”, J. Exp. Psych.: Human Perception & Performance, 14:45-60, 1988. Best, C. T., McRoberts, G. W., LaFleur, R., and Silver- Isenstadt, J. “Divergent developmental patterns for infants’ perception of two non-native consonant contrasts”, Infant Behavior and Devel., 18:339-350, 1995. Best, C. T., & Avery, R. A., “Left hemisphere advantage for click consonants is determined by linguistic significance”, Psych. Science, 10:65-69, 1999. Brancazio, L., Best, C. T., and Fowler, C. A., “Visual influences on perception of speech and nonspeech vocal-tract events”, Lang. and Speech, 49:21-53, 2006\n",
    ""
   ]
  },
  "cvejic11_avsp": {
   "authors": [
    [
     "Erin",
     "Cvejic"
    ],
    [
     "Jeesun",
     "Kim"
    ],
    [
     "Chris",
     "Davis"
    ]
   ],
   "title": "Perceiving visual prosody from point-light displays",
   "original": "av11_015",
   "page_count": 6,
   "order": 5,
   "p1": "15",
   "pn": "20",
   "abstract": [
    "This study examined the perception of linguistic prosody from augmented point-light displays that were derived from motion tracking six talkers producing different prosodic contrasts. In Experiment 1, we determined perceivers’ ability to use these abstract visual displays to match prosody across modalities (audio to video), when the non-matching visual display was segmentally identical and differed only in prosody. The results showed that perceivers were able to match the auditory speech to these limited face motion prosodic displays at better than chance levels; performance for the stimuli of different talkers varied greatly. A subjective perceptual rating task (Experiment 2) demonstrated that variation across talkers in the acoustic realization of prosodic contrasts may account for some of this difference; however a combination of the salience of acoustic and visual prosodic cues is likely to be driving matching performance.\n",
    "",
    "",
    "Index Terms. visual prosody, focus, phrasing, point-light displays, cross-modal matching\n",
    ""
   ]
  },
  "nahorna11_avsp": {
   "authors": [
    [
     "Olha",
     "Nahorna"
    ],
    [
     "Frédéric",
     "Berthommier"
    ],
    [
     "Jean-Luc",
     "Schwartz"
    ]
   ],
   "title": "Binding and unbinding the Mcgurk effect in audiovisual speech fusion: follow-up experiments on a new paradigm",
   "original": "av11_021",
   "page_count": 4,
   "order": 6,
   "p1": "21",
   "pn": "24",
   "abstract": [
    "The McGurk effect demonstrates the existence of a fusion process in audiovisual speech perception: the combination of the sound \"ba\" with the face of a speaker who pronounces \"ga\" is frequently perceived as \"da\". We assume that in the upstream of this phonetic fusion process, there is a “binding” process, which controls the combination of image and sound, and can block or reduce it in the case of audiovisual incoherencies (conditional binding process), as in the case of a dubbed film. To test and explore this binding hypothesis, we designed various experiments in which a coherent or incoherent audiovisual context is placed before McGurk stimuli, and we show that the incoherent contextual stimulus can significantly reduce the McGurk effect.\n",
    "",
    "",
    "Index Terms. McGurk effect, binding, multisensory fusion, audiovisual speech perception, audiovisual scene analysis\n",
    ""
   ]
  },
  "visser11_avsp": {
   "authors": [
    [
     "Mandy",
     "Visser"
    ],
    [
     "Emiel",
     "Krahmer"
    ],
    [
     "Marc",
     "Swerts"
    ]
   ],
   "title": "Children’s expression of uncertainty in collaborative and competitive contexts",
   "original": "av11_025",
   "page_count": 6,
   "order": 7,
   "p1": "25",
   "pn": "30",
   "abstract": [
    "We studied the effect of two social settings (collaborative versus competitive) on the audiovisual expression of uncertainty of children in two age groups (8 and 11). We conducted an experiment in which children played a quiz game in pairs. They either had to collaborate or compete with each other. We found an interaction effect of age and social setting on children’s Feeling of Knowing: 8 year old children do not seem to be affected by the social setting, contrary to 11 year old children. In a subsequent perception test, adults rated the certainty of children in clips taken from the experiment. We found that the (un)certainty of older children as well as children in competition were rated more accurately than the (un)certainty of younger children and children in collaboration. Moreover, it appeared that younger children were rated more certain in a collaborative setting and older children were rated more certain in competition. We also labeled children’s expressions for various visual and auditory features. We found that children used some of these features to signal uncertainty and that older children exhibited clearer cues than younger children.\n",
    "",
    "",
    "Index Terms. audiovisual expression, (un)certainty, collaboration, competition, social development, Feeling of Knowing.\n",
    ""
   ]
  },
  "fitzpatrick11_avsp": {
   "authors": [
    [
     "Michael",
     "Fitzpatrick"
    ],
    [
     "Jeesun",
     "Kim"
    ],
    [
     "Chris",
     "Davis"
    ]
   ],
   "title": "The effect of seeing the interlocutor on auditory and visual speech production in noise",
   "original": "av11_031",
   "page_count": 5,
   "order": 8,
   "p1": "31",
   "pn": "35",
   "abstract": [
    "Talkers modify their speech production in noisy environments partly as a reflex but also as an intentional communicative strategy to facilitate the transmission of the speech signal to the interlocutor. Previous studies have shown that talkers can adapt both auditory and visual elements of speech produced in noise. The current study examined whether auditory and visual speech production would be affected by being able to see their interlocutor or not. Participants completed an interactive communication game in various quiet and in noise conditions with/without being able to see their interlocutor. The results showed that the amplitude of talkers. speech modifications was significantly lower when interlocutors could see each other. Furthermore, talkers instead increased the saliency of their visual speech production (measured as lip-area) in noisy conditions for face-to-face communication. These results suggest that talkers actively monitor their environment and adopt appropriate speech production for efficient communication.\n",
    "",
    "",
    "Index Terms. Speech production; Speech perception;\n",
    ""
   ]
  },
  "burnham11_avsp": {
   "authors": [
    [
     "Denis",
     "Burnham"
    ],
    [
     "Virginie",
     "Attina"
    ],
    [
     "Benjawan",
     "Kasisopa"
    ]
   ],
   "title": "Auditory-visual discrimination and identification of lexical tone within and across tone languages",
   "original": "av11_037",
   "page_count": 6,
   "order": 9,
   "p1": "37",
   "pn": "42",
   "abstract": [
    "The aim of this research is to investigate the general features of lexical tones that might contribute to their categorisation. Thai tones were presented for (a) discrimination and (b) identification by native Thai and non-native Mandarin tone language participants in auditory-only (AO), visual-only (VO) and auditory-visual (AV) conditions. Discrimination tests revealed: (i) good auditory and auditory-visual discrimination of tone pairs by Thai and Mandarin perceivers, (ii) significant contribution of visual information to tone discrimination in Thai and Mandarin perceivers; (iii) greater AV>AO augmentation at 1500 vs 500 ms interstimulus interval (ISI), showing more use of visual information for tone at phonemic (tonemic) than phonetic (tonetic) levels; and (iv) better overall discrimination – and especially large AV>AO augmentation – of contour-contour than contour-level or level-level tone pairs. Identification tests showed, as expected, that Thai participants were accurate in identifying Thai tones, using both auditory and visual information. Mandarin participants were generally able to categorize the non-native Thai tones into their native tone categories, and also used visual information, especially for contour tones. The discrimination and identification data relationship is discussed as are implications for further studies.\n",
    "",
    "",
    "Index Terms. auditory-visual speech perception, crosslanguage studies, lexical tone.\n",
    ""
   ]
  },
  "borrascomes11_avsp": {
   "authors": [
    [
     "Joan",
     "Borràs-Comes"
    ],
    [
     "Cecilia",
     "Pugliesi"
    ],
    [
     "Pilar",
     "Prieto"
    ]
   ],
   "title": "Audiovisual perception of counter-expectational questions",
   "original": "av11_043",
   "page_count": 5,
   "order": 10,
   "p1": "43",
   "pn": "47",
   "abstract": [
    "The precise nature of the interaction between acoustic and visual information in the perception of prosodic information is a question that still remains unclear. Though the fuzzy logical model of perception (FLMP) has been shown to explain the recognition of segmental information, this model also needs to be tested in the field of suprasegmentals such as facial gestures. The first goal of this paper is to investigate, by means of a computer-generated 3D video character, the interaction between intonational and gestural information in the detection by listeners of counter-expectational questions compared to narrow focus statements. The second goal is to test which specific facial gesture conveys the counter-expectation meaning most clearly. Our results represent a further step for considering an FLMP approach to the analysis of audiovisual prosody.\n",
    "",
    "",
    "Index Terms. audiovisual prosody, intonation, facial gestures, eyebrow, models of perception\n",
    ""
   ]
  },
  "musti11_avsp": {
   "authors": [
    [
     "Utpala",
     "Musti"
    ],
    [
     "Vincent",
     "Colotte"
    ],
    [
     "Asterios",
     "Toutios"
    ],
    [
     "Slim",
     "Ouni"
    ]
   ],
   "title": "Introducing visual target cost within an acoustic-visual unit-selection speech synthesizer",
   "original": "av11_049",
   "page_count": 6,
   "order": 11,
   "p1": "49",
   "pn": "55",
   "abstract": [
    "In this paper, we present a method to take into account visual information during the selection process in an acoustic-visual synthesizer. The acoustic-visual speech synthesizer is based on the selection and concatenation of synchronous bimodal diphone units i.e., speech signal and 3D facial movements of the speaker’s face. The visual speech information is acquired using a stereovision technique. Unit selection for synthesis is based on the classical target cost consisting of linguistic and phonological features. We compare several methods to take into account the visual articulatory context in the target cost. We present an objective evaluation of the synthesis results based on correlation of the actual visual speech trajectory and synthesized visual speech trajectory.\n",
    "",
    "",
    "Index Terms. speech synthesis, unit selection, target costs.\n",
    ""
   ]
  },
  "mattheyses11_avsp": {
   "authors": [
    [
     "Wesley",
     "Mattheyses"
    ],
    [
     "Lukas",
     "Latacz"
    ],
    [
     "Werner",
     "Verhelst"
    ]
   ],
   "title": "Auditory and photo-realistic audiovisual speech synthesis for Dutch",
   "original": "av11_055",
   "page_count": 6,
   "order": 12,
   "p1": "55",
   "pn": "60",
   "abstract": [
    "Both auditory and audiovisual speech synthesis have been the subject of many research projects throughout the years. Unfortunately, in recent years only very few research focuses on synthesis for the Dutch language. Especially for audiovisual synthesis, hardly any available system or resource can be found. In this paper we describe the creation of a new extensive Dutch speech database, containing audiovisual recordings of a single speaker. The database is constructed as such that it can be employed in both auditory and audiovisual speech synthesis systems. Subsequently, we describe how we achieve high-quality auditory speech synthesis by applying the database in our textto- speech framework. In addition, it is explained how we used the new database to attain photorealistic audiovisual text-tospeech synthesis for Dutch. The new database and its applications for synthesis are a significant addition to the resources for Dutch speech synthesis research.\n",
    "",
    "",
    "Index Terms. Dutch speech database, speech synthesis, audiovisual speech synthesis\n",
    ""
   ]
  },
  "wu11_avsp": {
   "authors": [
    [
     "Peng",
     "Wu"
    ],
    [
     "Dongmei",
     "Jiang"
    ],
    [
     "He",
     "Zhang"
    ],
    [
     "Hichem",
     "Sahli"
    ]
   ],
   "title": "Photo-realistic visual speech synthesis based on AAM features and an articulatory DBN model with constrained asynchrony",
   "original": "av11_061",
   "page_count": 6,
   "order": 13,
   "p1": "61",
   "pn": "66",
   "abstract": [
    "This paper presents a photo realistic visual speech synthesis method based on an audio visual articulatory dynamic Bayesian network model (AF_AVDBN) in which the maximum asynchronies between the articulatory features, such as lips, tongue and glottis/velum, can be controlled. Perceptual linear prediction (PLP) features from the audio speech and active appearance model (AAM) features from mouth images of the visual speech are adopted to train the AF_AVDBN model for continuous speech. An EM-based optimal visual feature learning algorithm is deduced given the input auditory speech and the trained AF_AVDBN parameters. Finally, photo realistic mouth images are synthesized from the learned AAM features. In the experiments, mouth animations are synthesized for 30 connected digit audio speech sentences. Objective evaluation results show that the learned visual features using AF_AVDBN track the real parameters much more closely than those from the audio visual state synchronous DBN model (SS_DBN, the DBN implementation of multi-stream Hidden Markov Model), as well the state asynchronous DBN model (SA_DBN). Subjective evaluation results show that by considering the asynchronies between articulatory features in the AF_AVDBN (as well between audio and visual states in the SA_DBN), the synchronization between the audio speech and mouth animations are well obtained. Moreover, since AF_AVDBN captures the dynamic movements of articulatory features and model the pronunciation process more precisely, the accuracy of the mouth animations from the AF_AVDBN is much higher than those from the SA_DBN and the SS_DBN models, very accurate, clear, and natural mouth animations can be obtained through the AF_AVDBN model and AAM features.\n",
    "",
    "",
    "Index Terms. visual speech synthesis, AF_AVDBN, asynchrony, AAM features\n",
    ""
   ]
  },
  "kim11_avsp": {
   "authors": [
    [
     "Jeesun",
     "Kim"
    ],
    [
     "Chris",
     "Davis"
    ]
   ],
   "title": "Audiovisual speech processing in visual speech noise",
   "original": "av11_073",
   "page_count": 4,
   "order": 14,
   "p1": "73",
   "pn": "76",
   "abstract": [
    "When the talker’s face (visual speech) can be seen, speech perception is both facilitated (for congruent visual speech) and interfered with (for incongruent visual speech). The current study investigated whether the degree of these visual speech effects was affected by the presence of an additional irrelevant talking face. In the experiment, auditory speech targets (vCv syllables) were presented in noise for subsequent speech identification. Participants were presented with the full display or upper-half (control) display of a talker’s face uttering single syllables either in central vision (Exp 1) or in the visual periphery (Exp 2). In addition, another talker was presented (silently uttering a sentence) either in the periphery (Exp 1) or in central vision (Exp 2). Participants’ eye-movements were monitored to ensure that participants always fixated centrally. Congruent AV speech facilitation and incongruent McGurk effects were tested by comparing percent correct syllable identification for full face visual speech stimuli compared to upper-face only conditions. The results showed more accurate identification for congruent stimuli and less accurate responses for incongruent ones (full face condition vs. the upper-half face control). The magnitude of the McGurk effect was greater when the face articulating the syllable was presented in central vision (with visual speech noise in the periphery) than when it was presented in the periphery (with central visual speech noise). The size of the congruent AV speech effect, however, did not differ as a function of central or peripheral presentation.\n",
    "",
    "",
    "Index Terms. Visual speech; AV congruency; Peripheral visual speech\n",
    ""
   ]
  },
  "berthommier11_avsp": {
   "authors": [
    [
     "Frédéric",
     "Berthommier"
    ],
    [
     "Jean-Luc",
     "Schwartz"
    ]
   ],
   "title": "Audiovisual streaming in voicing perception: new evidence for a low-level interaction between audio and visual modalities",
   "original": "av11_077",
   "page_count": 4,
   "order": 15,
   "p1": "77",
   "pn": "80",
   "abstract": [
    "Speech Audio-visual (AV) interaction has been considered for redundancy and complementary properties at the phonetic level but a few experiments have shown a significant role in early auditory analysis. A new paradigm is proposed which uses the pre-voicing component (PVC) excised from a true /b/. When the so called target PVC is added up to a /p/ this leads to the clear perception of /b/. Moreover, the amplitude variation of the target PVC allows building of a perceptual continuum between /p/ when amplitude is set at 0 and /b/ at original amplitude. In the audio channel, adding a series of PVC at fixed low amplitude before and after the target allows the creation of a stream of regular sounds, which are not related to visible events. On the contrary, the bilabial aperture of the /p/ is a specific speech gesture visible in the video channel. The target PVC and the visible gesture are also not redundant events. Then, depending on its intensity level, the target PVC added to an audio /p/ could be either embedded to a stream of other PVCs or phonetically fused to perceive /b/. To study the competition between these two alternatives and the role of the AV interaction, we use a 2*2 factorial design to contrast Clear/Stream and Audio/AV conditions with a control of the amplitude of the target PVC. There is no stream of PVCs in the “Clear” condition for providing the baseline. The streaming effect by itself is significant in the audio condition, but the novelty is that we find a strong AV interaction. When a stream of PVCs is present, in the “AV” condition, the rate of perceived /p/ is higher than in the “Audio” condition, suggesting that the video lip opening gestures increases the trend to isolate the formant trajectory towards the vowel from the PVC, hence increasing the perception of unvoiced stimuli. We conclude that the process of low level audio streaming is reinforced when the visual information is not redundant, and that, in this case, the phonetic fusion of the voicing cue is disadvantaged by visual information.\n",
    "",
    "",
    "Index Terms. auditory streaming, voicing perception, multisensory fusion, AV speech perception, scene analysis\n",
    ""
   ]
  },
  "andersen11_avsp": {
   "authors": [
    [
     "Tobias S.",
     "Andersen"
    ]
   ],
   "title": "An ordinal model of the Mcgurk illusion",
   "original": "av11_081",
   "page_count": 6,
   "order": 16,
   "p1": "81",
   "pn": "86",
   "abstract": [
    "Audiovisual information is integrated in speech perception. One manifestation of this is the McGurk illusion in which watching the articulating face alters the auditory phonetic percept. Understanding this phenomenon fully requires a computational model with predictive power. Here, we describe an ordinal model, in which the response categories are ordered cyclically, that can account for the McGurk illusion. We compare this model to the Fuzzy Logical Model of Perception (FLMP), which is not an ordinal model, based on an original data set. While the FLMP fitted the data better than the ordinal model it also employed 30 free parameters where the ordinal model needed only 14. Testing the predictive power of the models using a form of cross-validation we found that, although both models performed rather poorly, the ordinal model performed better than the FLMP. Based on these findings we suggest that ordinal models generally have greater predictive power because they are constrained by a priori information about the adjacency of phonetic categories.\n",
    "",
    "",
    "Index Terms. audiovisual speech perception, ordinal models, FLMP, McGurk illusion\n",
    ""
   ]
  },
  "joosten11_avsp": {
   "authors": [
    [
     "Bart",
     "Joosten"
    ],
    [
     "Marije van",
     "Amelsvoort"
    ],
    [
     "Emiel",
     "Krahmer"
    ],
    [
     "Eric",
     "Postma"
    ]
   ],
   "title": "Thin slices of head movements during problem solving reveal level of difficulty",
   "original": "av11_087",
   "page_count": 5,
   "order": 17,
   "p1": "87",
   "pn": "92",
   "abstract": [
    "This paper reports on the computational analysis of thin slices of video fragments showing children that solve either easy or hard mathematical puzzles. The objective of the analysis is to determine if head movements reveal whether the children consider the puzzle to be easy or hard. Our analysis method combines a facial-expression extraction method with a nonparametric classifier. Training and evaluating the classifier in a leaving-one-out cross-validation procedure on extracted head movements, we obtained a 71% correct classification rate. Children engaged in solving mathematical puzzles tend to make head movements in a prevailing orientation that depends on the experienced level of difficulty of the puzzles, i.e., vertically for easy puzzles and diagonally for hard puzzles. We conclude that (1) computational analysis methods lead to the identification of hitherto unnoticed nonverbal behaviors that reflect the perceived difficulty of mathematical puzzles, and (2) computational analysis methods can be employed in automatic tutoring systems that automatically estimate the experienced difficulty of the problems presented.\n",
    "Index Terms. facial expression analysis, computational analysis, tutoring systems\n",
    ""
   ]
  },
  "arimoto11_avsp": {
   "authors": [
    [
     "Yoshiko",
     "Arimoto"
    ],
    [
     "Kazuo",
     "Okanoya"
    ]
   ],
   "title": "Dimensional mapping of multimodal integration on audiovisual emotion perception",
   "original": "av11_093",
   "page_count": 6,
   "order": 18,
   "p1": "93",
   "pn": "98",
   "abstract": [
    "The aim of this research was to investigate what emotions are perceived from incongruent vocal and facial emotional expressions as an integrated emotional expression. Our approach is unimodal and bimodal perceptual emotional information mapping to dimensions of emotional space created with principal component analysis (PCA). Unimodal perception tests and a bimodal congruent/incongruent perception test were conducted with each stimuli in which professional actors expressed four emotions?anger, joy, fear, and sadness?and observers rated the intensity of six emotions (the four expressed emotions plus disgust and surprise) on a six-point scale. A PCA was performed with the scores of each stimulus to create a perceptual emotional space and to compare the difference between unimodal perception and bimodal perception.\n",
    "The results showed that some incongruent emotional expressions were significantly perceived as inconsistent emotions with expressed emotion.\n",
    "",
    "",
    "Index Terms. emotional speech, facial expression, emotion perception, multimodal integration, principal component analysis\n",
    ""
   ]
  },
  "almoubayed11_avsp": {
   "authors": [
    [
     "Samer",
     "Al Moubayed"
    ],
    [
     "Gabriel",
     "Skantze"
    ]
   ],
   "title": "Turn-taking control using gaze in multiparty human-computer dialogue: effects of 2d and 3d displays",
   "original": "av11_099",
   "page_count": 4,
   "order": 19,
   "p1": "99",
   "pn": "102",
   "abstract": [
    "In a previous experiment we found that the perception of gaze from an animated agent on a two-dimensional display suffers from the Mona Lisa effect, which means that exclusive mutual gaze cannot be established if there is more than one observer. By using a three-dimensional projection surface, this effect can be eliminated. In this study, we investigate whether this difference also holds for the turn-taking behaviour of subjects interacting with the animated agent in a multi-party dialogue. We present a Wizard-of-Oz experiment where five subjects talk to an animated agent in a route direction dialogue. The results show that the subjects to some extent can infer the intended target of the agent’s questions, in spite of the Mona Lisa effect, but that the accuracy of gaze when it comes to selecting an addressee is still significantly lower in the 2D condition, as compared to the 3D condition. The response time is also significantly longer in the 2D condition, indicating that the inference of intended gaze may require additional cognitive efforts.\n",
    "",
    "",
    "Index Terms. Turn-taking, Multi-party Dialogue, Gaze, Facial Interaction, Mona Lisa Effect, Facial Projection, Wizard of Oz\n",
    ""
   ]
  },
  "galatas11_avsp": {
   "authors": [
    [
     "Georgios",
     "Galatas"
    ],
    [
     "Gerasimos",
     "Potamianos"
    ],
    [
     "Dimitrios",
     "Kosmopoulos"
    ],
    [
     "Chris",
     "McMurrough"
    ],
    [
     "Fillia",
     "Makedon"
    ]
   ],
   "title": "Bilingual corpus for AVASR using multiple sensors and depth information",
   "original": "av11_103",
   "page_count": 4,
   "order": 20,
   "p1": "103",
   "pn": "106",
   "abstract": [
    "In this paper we present the Bilingual Audio-Visual Corpus with Depth information (BAVCD). The database contains utterances of connected digits, spoken by 15 subjects in English and 6 subjects in Greek, and collected employing multiple audio-visual sensors. Among them, of particular interest is the use of the Microsoft Kinect device, which is able to capture facial depth images using the structured light technique in addition to the traditional RGB video. The database allows conducting research on multiple aspects of small-vocabulary audio-visual automatic speech recognition, such as the use of visual depth information for speechreading, fusion of multiple video and audio streams, and language dependencies of the task. Preliminary results on the corpus are also presented.\n",
    "",
    "",
    "Index Terms. Audiovisual speech recognition, corpora, multisensory fusion, depth information, languages.\n",
    ""
   ]
  },
  "beskow11_avsp": {
   "authors": [
    [
     "Jonas",
     "Beskow"
    ],
    [
     "Simon",
     "Alexandersson"
    ],
    [
     "Samer",
     "Al Moubayed"
    ],
    [
     "Jens",
     "Edlund"
    ],
    [
     "David",
     "House"
    ]
   ],
   "title": "Kinetic data for large-scale analysis and modeling of face-to-face conversation",
   "original": "av11_107",
   "page_count": 4,
   "order": 21,
   "p1": "107",
   "pn": "110",
   "abstract": [
    "Spoken face to face interaction is a rich and complex form of communication that includes a wide array of phenomena that are not fully explored or understood. While there has been extensive studies on many aspects in face-to-face interaction, these are traditionally of a qualitative nature, relying on hand annotated corpora, typically rather limited in extent, which is a natural consequence of the labour intensive task of multimodal data annotation. In this paper we present a corpus of 60 hours of unrestricted Swedish face-to-face conversations recorded with audio, video and optical motion capture, and we describe a new project setting out to exploit primarily the kinetic data in this corpus in order to gain quantitative knowledge on human face-to-face interaction.\n",
    "",
    "",
    "Index Terms. motion capture, face-to-face conversation, multimodal corpus.\n",
    ""
   ]
  },
  "kuratate11_avsp": {
   "authors": [
    [
     "Takaaki",
     "Kuratate"
    ],
    [
     "Brennard",
     "Pierce"
    ],
    [
     "Gordon",
     "Cheng"
    ]
   ],
   "title": "“Mask-bot” - a life-size talking head animated robot for AV speech and human-robot communication research",
   "original": "av11_111",
   "page_count": 6,
   "order": 22,
   "p1": "111",
   "pn": "116",
   "abstract": [
    "In this paper we introduce our life-size talking head robotic system, Mask-bot, developed to support human-robot communication research. The physical system consists of a semitransparent face mask, a portable LED projector with a fish-eye conversion lens, a pan-tilt unit and a mounting base. Mask-bot uses the mask as a screen for a talking head animation engine that is broadcast from the projector mounted behind the mask. Via this process the head becomes a life-size talking head in real space as opposed to 2D flat screen space or stereo pseudo-3D screen space, affording the means for testing new face models for AV speech synthesis and perception in life-size output without building an actual robotic head.\n",
    "",
    "",
    "Index Terms. talking head, face animation, AV speech synthesis, 3D face model, humanoid robot, robotic head\n",
    ""
   ]
  },
  "saitoh11_avsp": {
   "authors": [
    [
     "Takeshi",
     "Saitoh"
    ]
   ],
   "title": "Development of communication support system using lip reading",
   "original": "av11_117",
   "page_count": 6,
   "order": 23,
   "p1": "117",
   "pn": "122",
   "abstract": [
    "This paper develops an entire communication support system for speech handicaps using lip reading. Our system is a high level system and consists of a face detector using Viola-jones method, lip detector based on active appearance model, utterance section detector, determined phrase recognizer using DP matching, camera controller, and message output. We implemented our system in the commercial laptop, and demonstrated it in the various situations of the sitting position, supine position, and holding position. We confirmed our system was able to be used in various situations.\n",
    "",
    "",
    "Index Terms. communication support system, lip reading, fixed phrase recognition, real-time system.\n",
    ""
   ]
  },
  "leone11_avsp": {
   "authors": [
    [
     "Giuseppe Riccardo",
     "Leone"
    ],
    [
     "Piero",
     "Cosi"
    ]
   ],
   "title": "LUCIA-webGL: a web based Italian MPEG-4 talking head",
   "original": "av11_123",
   "page_count": 4,
   "order": 24,
   "p1": "123",
   "pn": "126",
   "abstract": [
    "In this work we present the reviewing of the activities focused on the development of the WebGL software version of LUCIA talking head, an open source facial animation framework developed at ISTC-CNR of Padua. LUCIA works on standard MPEG-4 Facial Animation Parameters and speaks with the Italian version of FESTIVAL TTS. LUCIA is totally based on true real human data collected by the use of ELITE, a fully automatic movement analyzer for 3D kinematics data acquisition. These informations are used to create lips articulatory model and to drive directly the talking face, generating human facial movements. We are exploiting the use of LUCIA WebGL as a virtual guide in the Wikimemo.it project: The portal of Italian Language and Culture. The easy integration of this technology in websites offers promising future uses for theWebGL Avatars: on-line personal assistant, storyteller for web-books, digital tutor for hearing impaired are only few examples.\n",
    "",
    "",
    "Index Terms. WebGL, talking head, facial animation, mpeg4, 3D avatar, virtual agent, TTS, LUCIA, FESTIVAL\n",
    ""
   ]
  },
  "huang11_avsp": {
   "authors": [
    [
     "Qiang",
     "Huang"
    ],
    [
     "Stephen",
     "Cox"
    ],
    [
     "Fei",
     "Yan"
    ],
    [
     "Teo de",
     "Campos"
    ],
    [
     "David",
     "Windridge"
    ],
    [
     "Josef",
     "Kittler"
    ],
    [
     "William",
     "Christmas"
    ]
   ],
   "title": "Improved detection of ball hit events in a tennis game using multimodal information",
   "original": "av11_127",
   "page_count": 4,
   "order": 25,
   "p1": "127",
   "pn": "130",
   "abstract": [
    "We describe a novel framework to detect ball hits in a tennis game by combining audio and visual information. Ball hit detection is a key step in understanding a game such as tennis, but single-mode approaches are not very successful: audio detection suffers from interfering noise and acoustic mismatch, video detection is made difficult by the small size of the ball and the complex background of the surrounding environment. Our goal in this paper is to improve detection performance by focusing on high-level information (rather than low-level features), including the detected audio events, the ball’s trajectory, and inter-event timing information. Visual information supplies coarse detection of the ball-hits events. This information is used as a constraint for audio detection. In addition, useful gains in detection performance can be obtained by using and inter-ballhit timing information, which aids prediction of the next ball hit. This method seems to be very effective in reducing the interference present in low-level features. After applying this method to a women’s doubles tennis game, we obtained improvements in the F-score of about 30% (absolute) for audio detection and about 10% for video detection.\n",
    "",
    "",
    "Index Terms. Scene analysis, multimodal information integration\n",
    ""
   ]
  },
  "ishi11_avsp": {
   "authors": [
    [
     "Carlos T.",
     "Ishi"
    ],
    [
     "Chaoran",
     "Liu"
    ],
    [
     "Hiroshi",
     "Ishiguro"
    ],
    [
     "Norihiro",
     "Hagita"
    ]
   ],
   "title": "Speech-driven lip motion generation for tele-operated humanoid robots",
   "original": "av11_131",
   "page_count": 5,
   "order": 26,
   "p1": "131",
   "pn": "135",
   "abstract": [
    "In order to tele-operate the lip motion of a humanoid robot (such as android) from the utterances of the operator, we developed a speech-driven lip motion generation method. The proposed method is based on the rotation of the vowel space, given by the first and second formants, around the center vowel, and a mapping to the lip opening degrees. The method requires the calibration of only one parameter for speaker normalization, so that no other training of models is required. In a pilot experiment, the proposed audio-based method was perceived as more natural than vision-based approaches, regardless of the language.\n",
    "",
    "",
    "Index Terms. lip motion, formant, humanoid robot, teleoperation, synchronization\n",
    ""
   ]
  },
  "czap11_avsp": {
   "authors": [
    [
     "László",
     "Czap"
    ]
   ],
   "title": "On the audiovisual asynchrony of speech",
   "original": "av11_137",
   "page_count": 4,
   "order": 27,
   "p1": "137",
   "pn": "140",
   "abstract": [
    "The temporal synchrony of auditory and visual signals is known to affect the perception of audiovisual speech. Several papers have discussed the asymmetry of acoustic and visual timing cues. These results are usually based on subjective intelligibility tests and the reason is remained obscure. It is not clear that the observation is perception or production origin. In this paper the effect of audio-visual asynchrony is studied in an automatic bimodal speech recognition task, eliminating the perception expertise of observers. Results are utilized to improve naturalness of audiovisual speech synthesis.\n",
    "",
    "",
    "Index Terms. speechreading, multimodality, audiovisual asymmetry\n",
    ""
   ]
  },
  "fagel11_avsp": {
   "authors": [
    [
     "Sascha",
     "Fagel"
    ]
   ],
   "title": "Talking heads for elderly and Alzheimer patients (THEA): project report and demonstration",
   "original": "av11_067",
   "page_count": 1,
   "order": 28,
   "p1": "67",
   "pn": "",
   "abstract": [
    "In the present project a talking head is developed especially for the use with mobile devices. Its design targets applications in the area of Ambient Assisted Living services. It shall be easily adapted to various face and head typologies and appearances to represent specific persons known to the user, highly performing on current mobile hardware, and flexible in the interface design to communicate with a range of middleware and service platforms. A Client-Server-Architecture with lightweight client software ensures easy adaptability of the main server logic to various end devices like Android smartphones, iPhones, set-top boxes or standard computers.\n",
    "",
    "",
    "Index Terms. talking head, Ambient Assisted Living, elderly, cognitive impairment, virtual assistant\n",
    ""
   ]
  },
  "czap11b_avsp": {
   "authors": [
    [
     "László",
     "Czap"
    ],
    [
     "János",
     "Mátyás"
    ]
   ],
   "title": "Improving naturalness of visual speech synthesis",
   "original": "av11_069",
   "page_count": 1,
   "order": 29,
   "p1": "69",
   "pn": "",
   "abstract": [
    "Facial animation has progressed significantly over the past few years and a variety of algorithms and techniques now make it possible to create highly realistic characters. Based on the author’s visual feature database for speechreading and the development of 3D modelling, a Hungarian talking head has been created. Our general approach is to use both static and dynamic observations of natural speech to guide the facial animation. A three level dominance model has been introduced that takes co-articulation into account. Each articulation feature has been grouped to dominant, flexible or uncertain classes. Analysis of the standard deviation and the trajectory of features served the evaluation process. The acoustic speech and the articulation are linked to each other by a synchronising process. Natural head movements, eyebrow rising, blinking and expressing emotions are demonstrated.\n",
    "",
    "",
    "Index Terms. AV speech synthesis, improving naturalness, expressing emotions\n",
    ""
   ]
  },
  "almoubayed11b_avsp": {
   "authors": [
    [
     "Samer",
     "Al Moubayed"
    ],
    [
     "Simon",
     "Alexandersson"
    ],
    [
     "Jonas",
     "Beskow"
    ],
    [
     "Björn",
     "Granström"
    ]
   ],
   "title": "A robotic head using projected animated faces",
   "original": "av11_071",
   "page_count": 1,
   "order": 30,
   "p1": "71",
   "pn": "",
   "abstract": [
    "This paper presents a setup which employs virtual animated agents for robotic heads. The system uses a laser projector to project animated faces onto a three dimensional face mask. This approach of projecting animated faces onto a three dimensional head surface as an alternative to using flat, two dimensional surfaces, eliminates several deteriorating effects and illusions that come with flat surfaces for interaction purposes, such as exclusive mutual gaze and situated and multi-partner dialogues. In addition to that, it provides robotic heads with a flexible solution for facial animation which takes into advantage the advancements of facial animation using computer graphics over mechanically controlled heads.\n",
    "",
    "",
    "Index Terms. 3D facial projection, facial animation, gaze direction, situated interaction\n",
    ""
   ]
  }
 },
 "sessions": [
  {
   "title": "Keynote Papers",
   "papers": [
    "sjolander11_avsp",
    "massey11_avsp"
   ]
  },
  {
   "title": "Perception",
   "papers": [
    "paris11_avsp",
    "best11_avsp",
    "cvejic11_avsp",
    "nahorna11_avsp",
    "visser11_avsp",
    "fitzpatrick11_avsp",
    "burnham11_avsp",
    "borrascomes11_avsp"
   ]
  },
  {
   "title": "Synthesis",
   "papers": [
    "musti11_avsp",
    "mattheyses11_avsp",
    "wu11_avsp"
   ]
  },
  {
   "title": "Perception and Modeling",
   "papers": [
    "kim11_avsp",
    "berthommier11_avsp",
    "andersen11_avsp",
    "joosten11_avsp",
    "arimoto11_avsp",
    "almoubayed11_avsp"
   ]
  },
  {
   "title": "Corpora and Applications",
   "papers": [
    "galatas11_avsp",
    "beskow11_avsp",
    "kuratate11_avsp",
    "saitoh11_avsp",
    "leone11_avsp"
   ]
  },
  {
   "title": "Analysis and Recognition",
   "papers": [
    "huang11_avsp",
    "ishi11_avsp",
    "czap11_avsp"
   ]
  },
  {
   "title": "Demo Session",
   "papers": [
    "fagel11_avsp",
    "czap11b_avsp",
    "almoubayed11b_avsp"
   ]
  }
 ]
}