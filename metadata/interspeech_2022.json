{
 "series": "Interspeech",
 "title": "Interspeech 2022",
 "location": "Incheon, Korea",
 "startDate": "18/09/2022",
 "endDate": "22/09/2022",
 "URL": "https://interspeech2022.org/",
 "chair": "Chairs: Hanseok Ko and John H. L. Hansen",
 "intro": "intro.pdf",
 "ISSN": "1990-9772",
 "copyright": "Copyright © 2022 ISCA",
 "conf": "Interspeech",
 "year": "2022",
 "name": "interspeech_2022",
 "SIG": "",
 "title1": "Interspeech 2022",
 "booklet": "interspeech_2022.pdf",
 "date": "18-22 September 2022",
 "papers": {
  "variani22_interspeech": {
   "authors": [
    [
     "Ehsan",
     "Variani"
    ],
    [
     "Michael",
     "Riley"
    ],
    [
     "David",
     "Rybach"
    ],
    [
     "Cyril",
     "Allauzen"
    ],
    [
     "Tongzhou",
     "Chen"
    ],
    [
     "Bhuvana",
     "Ramabhadran"
    ]
   ],
   "title": "On Adaptive Weight Interpolation of the Hybrid Autoregressive Transducer",
   "original": "4",
   "page_count": 5,
   "order": 334,
   "p1": 1646,
   "pn": 1650,
   "abstract": [
    "This paper explores rescoring strategies to improve a two-pass speech recognition system when first-pass is a hybrid autoregressive transducer model and second-pass is a neural language model. The main focus is on the scores provided by each of these models, their quantitative analysis, how to improve them and the best way to integrate them with the objective of better recognition accuracy. Several analyses are presented to emphasise the importance of the choice of the integration weights for combining the first-pass and the second-pass scores. A sequence level combination weight estimation model along with four training criteria are proposed which allows adaptive integration of the scores per acoustic sequence. The effectiveness of this algorithm is demonstrated by constructing and analyzing models on the Librispeech data set."
   ],
   "doi": "10.21437/Interspeech.2022-4"
  },
  "nozaki22_interspeech": {
   "authors": [
    [
     "Jumon",
     "Nozaki"
    ],
    [
     "Tatsuya",
     "Kawahara"
    ],
    [
     "Kenkichi",
     "Ishizuka"
    ],
    [
     "Taiichi",
     "Hashimoto"
    ]
   ],
   "title": "End-to-end Speech-to-Punctuated-Text Recognition",
   "original": "5",
   "page_count": 5,
   "order": 367,
   "p1": 1811,
   "pn": 1815,
   "abstract": [
    "Conventional automatic speech recognition systems do not produce punctuation marks which are important for the readability of the speech recognition results. They are also needed for subsequent natural language processing tasks such as machine translation. There have been a lot of works on punctuation prediction models that insert punctuation marks into speech recognition results as post-processing. However, these studies do not utilize acoustic information for punctuation prediction and are directly affected by speech recognition errors. In this study, we propose an end-to-end model that takes speech as input and outputs punctuated texts. This model is expected to predict punctuation robustly against speech recognition errors while using acoustic information. We also propose to incorporate an auxiliary loss to train the model using the output of the intermediate layer and unpunctuated texts. Through experiments, we compare the performance of the proposed model to that of a cascaded system. The proposed model achieves higher punctuation prediction accuracy than the cascaded system without sacrificing the speech recognition error rate. It is also demonstrated that the multi-task learning using the intermediate output against the unpunctuated text is effective. Moreover, the proposed model has only about 1/7th of the parameters compared to the cascaded system."
   ],
   "doi": "10.21437/Interspeech.2022-5"
  },
  "tae22_interspeech": {
   "authors": [
    [
     "Jaesung",
     "Tae"
    ],
    [
     "Hyeongju",
     "Kim"
    ],
    [
     "Taesu",
     "Kim"
    ]
   ],
   "title": "EdiTTS: Score-based Editing for Controllable Text-to-Speech",
   "original": "6",
   "page_count": 5,
   "order": 85,
   "p1": 421,
   "pn": 425,
   "abstract": [
    "We present EdiTTS, an off-the-shelf speech editing methodology based on score-based generative modeling for text-to-speech synthesis. EdiTTS allows for targeted, granular editing of audio, both in terms of content and pitch, without the need for any additional training, task-specific optimization, or architectural modifications to the score-based model backbone. Specifically, we apply coarse yet deliberate perturbations in the Gaussian prior space to induce desired behavior from the diffusion model while applying masks and softening kernels to ensure that iterative edits are applied only to the target region. Through listening tests and speech-to-text back transcription, we show that EdiTTS outperforms existing baselines and produces robust samples that satisfy user-imposed requirements."
   ],
   "doi": "10.21437/Interspeech.2022-6"
  },
  "kanda22_interspeech": {
   "authors": [
    [
     "Naoyuki",
     "Kanda"
    ],
    [
     "Jian",
     "Wu"
    ],
    [
     "Yu",
     "Wu"
    ],
    [
     "Xiong",
     "Xiao"
    ],
    [
     "Zhong",
     "Meng"
    ],
    [
     "Xiaofei",
     "Wang"
    ],
    [
     "Yashesh",
     "Gaur"
    ],
    [
     "Zhuo",
     "Chen"
    ],
    [
     "Jinyu",
     "Li"
    ],
    [
     "Takuya",
     "Yoshioka"
    ]
   ],
   "title": "Streaming Multi-Talker ASR with Token-Level Serialized Output Training",
   "original": "7",
   "page_count": 5,
   "order": 765,
   "p1": 3774,
   "pn": 3778,
   "abstract": [
    "This paper proposes a token-level serialized output training (t-SOT), a novel framework for streaming multi-talker automatic speech recognition (ASR). Unlike existing streaming multi-talker ASR models using multiple output branches, the t-SOT model has only a single output branch that generates recognition tokens (e.g., words, subwords) of multiple speakers in chronological order based on their emission times. A special token that indicates the change of ``virtual'' output channels is introduced to keep track of the overlapping utterances. Compared to the prior streaming multi-talker ASR models, the t-SOT model has the advantages of less inference cost and a simpler model architecture. Moreover, in our experiments with LibriSpeechMix and LibriCSS datasets, the t-SOT-based transformer transducer model achieves the state-of-the-art word error rates by a significant margin to the prior results. For non-overlapping speech, the t-SOT model is on par with a single-talker ASR model in terms of both accuracy and computational cost, opening the door for deploying one model for both single- and multi-talker scenarios."
   ],
   "doi": "10.21437/Interspeech.2022-7"
  },
  "bousquet22_interspeech": {
   "authors": [
    [
     "Pierre-Michel",
     "Bousquet"
    ],
    [
     "Mickael",
     "Rouvier"
    ],
    [
     "Jean-Francois",
     "Bonastre"
    ]
   ],
   "title": "Reliability criterion based on learning-phase entropy for speaker recognition with neural network",
   "original": "8",
   "page_count": 5,
   "order": 57,
   "p1": 281,
   "pn": 285,
   "abstract": [
    "The reliability of Automatic Speaker Recognition (SR) is of the utmost importance for real-world applications. Even if SR systems obtain spectacular performance during evaluation campaigns, several studies have shown the limits and shortcomings of these systems. Reliability first means knowing where and when a system is performing as expected and a research effort is devoted to building confidence measures, by scanning input signals, representations or output scores. Here, a new reliability criterion is presented, dedicated to the latest SR systems based on deep neural network (DNN). The proposed approach uses the set of anchor speakers that controls the learning phase and takes advantage of the structure of the network itself, in order to derive a criterion making it possible to better assess the reliability of the decision based on the extracted speaker embeddings. The relevance and effectiveness of the proposed confidence measure are tested and demonstrated on widely used datasets."
   ],
   "doi": "10.21437/Interspeech.2022-8"
  },
  "wang22_interspeech": {
   "authors": [
    [
     "Yizhou",
     "Wang"
    ],
    [
     "Rikke",
     "Bundgaard-Nielsen"
    ],
    [
     "Brett",
     "Baker"
    ],
    [
     "Olga",
     "Maxwell"
    ]
   ],
   "title": "Native phonotactic interference in L2 vowel processing: Mouse-tracking reveals cognitive conflicts during identification",
   "original": "12",
   "page_count": 5,
   "order": 1059,
   "p1": 5223,
   "pn": 5227,
   "abstract": [
    "Regularities of phoneme distribution in a listener's native language (L1), i.e., L1 phonotactics, can at times induce interference in their perception of second language (L2) phonemes and phonemic strings. This paper presents a study examining phonological interference experienced by L1 Mandarin listeners in identifying the English /i/ vowel in three consonantal contexts /p, f, w/, which have different distributional patterns in Mandarin phonology: /pi/ is a licit sequence in Mandarin, */fi/ is illicit due to co-occurrence restrictions, and */wi/ is illicit due to Mandarin contextual allophony. L1 Mandarin listeners completed two versions of an identification experiment (keystroke and mouse-tracking), in which they identified vowels in different consonantal contexts. Analysis of error rates, response times, and hand motions in the tasks suggests that L1 co-occurrence restriction and contextual allophony induce different levels of phonological interference in L2 vowel perception compared to the licit control condition. In support of the dynamic theory of linguistic cognition, our results indicate that illicit phonotactic contexts can lead to more identification errors, longer decision processes, and spurious activation of a distractor category. Index Terms: vowel perception, phonotactics, Mandarin"
   ],
   "doi": "10.21437/Interspeech.2022-12"
  },
  "meng22_interspeech": {
   "authors": [
    [
     "Zhong",
     "Meng"
    ],
    [
     "Yashesh",
     "Gaur"
    ],
    [
     "Naoyuki",
     "Kanda"
    ],
    [
     "Jinyu",
     "Li"
    ],
    [
     "Xie",
     "Chen"
    ],
    [
     "Yu",
     "Wu"
    ],
    [
     "Yifan",
     "Gong"
    ]
   ],
   "title": "Internal Language Model Adaptation with Text-Only Data for End-to-End Speech Recognition",
   "original": "13",
   "page_count": 5,
   "order": 530,
   "p1": 2608,
   "pn": 2612,
   "abstract": [
    "Text-only adaptation of an end-to-end (E2E) model remains a challenging task for automatic speech recognition (ASR). Language model (LM) fusion-based approaches require an additional external LM during inference, significantly increasing the computation cost. To overcome this, we propose an internal LM adaptation (ILMA) of the E2E model using text-only data. Trained with audio-transcript pairs, an E2E model implicitly learns an internal LM that characterizes the token sequence probability which is approximated by the E2E model output after zeroing out the encoder contribution. During ILMA, we fine-tune the internal LM, i.e., the E2E components excluding the encoder, to minimize a cross-entropy loss. To make ILMA effective, it is essential to train the E2E model with an internal LM loss besides the standard E2E loss. Furthermore, we propose to regularize ILMA by minimizing the Kullback-Leibler divergence between the output distributions of the adapted and unadapted internal LMs. ILMA is the most effective when we update only the last linear layer of the joint network. ILMA enables a fast text-only adaptation of the E2E model without increasing the run-time computational cost. Experimented with 30K-hour trained transformer transducer models, ILMA achieves up to 34.9% relative word error rate reduction from the unadapted baseline."
   ],
   "doi": "10.21437/Interspeech.2022-13"
  },
  "saeed22_interspeech": {
   "authors": [
    [
     "Aaqib",
     "Saeed"
    ]
   ],
   "title": "Binary Early-Exit Network for Adaptive Inference on Low-Resource Devices",
   "original": "17",
   "page_count": 5,
   "order": 104,
   "p1": 516,
   "pn": 520,
   "abstract": [
    "Deep neural networks have significantly improved performance on a range of tasks with the increasing demand for computational resources, leaving deployment on low-resource devices (with limited memory and battery power) infeasible. Binary neural networks (BNNs) tackle the issue to an extent with extreme compression and speed-up gains compared to real-valued models. We propose a simple but effective method to accelerate inference through unifying BNNs with an early-exiting strategy. Our approach allows simple instances to exit early based on a decision threshold and utilizes output layers added to different intermediate layers to avoid executing the entire binary model. We extensively evaluate our method on three audio classification tasks and across four BNNs architectures. Our method demonstrates favorable quality-efficiency trade-offs while being controllable with an entropy-based threshold specified by the system user. It also results in better speed-ups (latency less than 6ms) with a single model based on existing BNN architectures without retraining for different efficiency levels. It also provides a straightforward way to estimate sample difficulty and better understanding of uncertainty around certain classes within the dataset."
   ],
   "doi": "10.21437/Interspeech.2022-17"
  },
  "xu22_interspeech": {
   "authors": [
    [
     "Hai-tao",
     "Xu"
    ],
    [
     "Jie",
     "Zhang"
    ],
    [
     "Li-rong",
     "Dai"
    ]
   ],
   "title": "Differential Time-frequency Log-mel Spectrogram Features for Vision Transformer Based Infant Cry Recognition",
   "original": "18",
   "page_count": 5,
   "order": 401,
   "p1": 1963,
   "pn": 1967,
   "abstract": [
    "Crying is the main way for babies to communicate with the outside world. Analyzing cry enables not only the identification of babies' needs/thoughts they want to express, but also the prediction of potential diseases. In general, it is much more difficult to recognize special needs and emotions from infant cry than adults, because infant cry does not contain any linguistic information and the emotional expression is not as rich as adults.In this work, we focus on the time-frequency characteristics of infant crying signals and propose a differential time-frequency log-Mel spectrogram features based vision transformer (ViT) approach for infant cry recognition (ICR). We first calculate the deltas of log-Mel spectrogram of infant crying sounds over time frames and frequencies, respectively. The log-Mels and deltas are then combined as a 3-D feature representation and fed into the ViT model for cry classification. Experimental results on the CRIED database show the superiority of the proposed system over comparison methods and that the combination of logMels, the time-frame delta and frequency-bin delta achieves the best performance. The proposed method is further validated on a self-recorded dataset."
   ],
   "doi": "10.21437/Interspeech.2022-18"
  },
  "yoo22_interspeech": {
   "authors": [
    [
     "Eunkyung",
     "Yoo"
    ],
    [
     "Hyeonseop",
     "Song"
    ],
    [
     "Taehyeong",
     "Kim"
    ],
    [
     "Chul",
     "Lee"
    ]
   ],
   "title": "Online Learning of Open-set Speaker Identification by Active User-registration",
   "original": "25",
   "page_count": 5,
   "order": 1025,
   "p1": 5065,
   "pn": 5069,
   "abstract": [
    "Registering each user's identity for voice assistants is burdensome and complex for multi-user environments like a household scenario. This is particularly true when the registration needs to happen on-the-fly with a relatively minimum effort. Most of the prior works for speaker identification (SID) do not seamlessly allow the addition of new speakers as these do not support online updates. To deal with such limitation, we introduce a novel online learning approach to open-set SID that can actively register unknown users in the household setting. Based on MPART (Message Passing Adaptive Resonance Theory), our method performs online active semi-supervised learning for open-set SID by using speaking embedding vectors to infer new speakers and request user's identity. Our method progressively improves the overall SID performance without forgetting, making it attractive for many interactive real-world applications. We evaluate our model for the online learning setting of an open-set SID task where new speakers are added on-the-fly, demonstrating its superior performance."
   ],
   "doi": "10.21437/Interspeech.2022-25"
  },
  "wong22_interspeech": {
   "authors": [
    [
     "Jeremy Heng Meng",
     "Wong"
    ],
    [
     "Huayun",
     "Zhang"
    ],
    [
     "Nancy",
     "Chen"
    ]
   ],
   "title": "Variations of multi-task learning for spoken language assessment",
   "original": "28",
   "page_count": 5,
   "order": 903,
   "p1": 4456,
   "pn": 4460,
   "abstract": [
    "Automatic spoken language assessment often operates within a regime whereby only a limited quantity of training data is available. In other low-resourced tasks, such as in speech recognition, multi-task learning has previously been investigated as a potential approach to regularise the model and maximise the utilisation of the available annotation information during training. This paper applies multi-task learning to spoken language assessment, by assessing three different forms of task diversities. These are, concurrently learning scores at different linguistic levels, different types of scores, and different representations of the same score. Experiments on the speechocean762 dataset suggest that jointly learning from phone and word-level scores yields significant performance gains for the sentence-level score prediction task, and jointly learning from different score types can also be mutually beneficial."
   ],
   "doi": "10.21437/Interspeech.2022-28"
  },
  "peyser22_interspeech": {
   "authors": [
    [
     "Cal",
     "Peyser"
    ],
    [
     "W. Ronny",
     "Huang"
    ],
    [
     "Andrew",
     "Rosenberg"
    ],
    [
     "Tara",
     "Sainath"
    ],
    [
     "Michael",
     "Picheny"
    ],
    [
     "Kyunghyun",
     "Cho"
    ]
   ],
   "title": "Towards Disentangled Speech Representations",
   "original": "30",
   "page_count": 5,
   "order": 729,
   "p1": 3603,
   "pn": 3607,
   "abstract": [
    "The careful construction of audio representations has become a dominant feature in the design of approaches to many speech tasks. Increasingly, such approaches have emphasized \"disentanglement”, which refers to the property of a representation of containing only parts of the speech signal relevant to transcription while discarding irrelevant information. In this paper, we construct a representation learning task based on joint modeling of ASR and TTS, and seek to learn a representation of audio that disentangles that part of the speech signal that is relevant to transcription from that part which is not. We present empirical evidence that arrival at such a representation is tied to the randomness inherent in training. We then make the observation that these desired, disentangled solutions to the optimization problem possess unique statistical properties. Finally, we show that enforcing these properties during training improves WER by 24.5% relative on average for our task. These observations motivate a novel approach to learning effective audio representations."
   ],
   "doi": "10.21437/Interspeech.2022-30"
  },
  "turrisi22_interspeech": {
   "authors": [
    [
     "Rosanna",
     "Turrisi"
    ],
    [
     "Leonardo",
     "Badino"
    ]
   ],
   "title": "Interpretable dysarthric speaker adaptation based on optimal-transport",
   "original": "36",
   "page_count": 5,
   "order": 6,
   "p1": 26,
   "pn": 30,
   "abstract": [
    "This work addresses the mismatch problem between the distribution of training data (source) and testing data (target), in the challenging context of dysarthric speech recognition. We focus on Speaker Adaptation (SA) in command speech recognition, where data from multiple sources (i.e., multiple speakers) are available. Specifically, we propose an unsupervised Multi-Source Domain Adaptation (MSDA) algorithm based on optimal-transport, called MSDA via Weighted Joint Optimal Transport (MSDA-WJDOT). We achieve a Command Error Rate relative reduction of 16% and 7% over the speaker-independent model and the best competitor method, respectively. The strength of the proposed approach is that, differently from any other existing SA method, it offers an interpretable model that can also be exploited, in this context, to diagnose dysarthria without any specific training. Indeed, it provides a closeness measure between the target and the source speakers, reflecting their similarity in terms of speech characteristics. Based on the similarity between the target speaker and the healthy/dysarthric source speakers, we then define the healthy/dysarthric score of the target speaker that we leverage to perform dysarthria detection. This approach does not require any additional training and achieves a 95% accuracy in the dysarthria diagnosis."
   ],
   "doi": "10.21437/Interspeech.2022-36"
  },
  "huang22_interspeech": {
   "authors": [
    [
     "W. Ronny",
     "Huang"
    ],
    [
     "Shuo-Yiin",
     "Chang"
    ],
    [
     "David",
     "Rybach"
    ],
    [
     "Tara",
     "Sainath"
    ],
    [
     "Rohit",
     "Prabhavalkar"
    ],
    [
     "Cal",
     "Peyser"
    ],
    [
     "Zhiyun",
     "Lu"
    ],
    [
     "Cyril",
     "Allauzen"
    ]
   ],
   "title": "E2E Segmenter: Joint Segmenting and Decoding for Long-Form ASR",
   "original": "38",
   "page_count": 5,
   "order": 1011,
   "p1": 4995,
   "pn": 4999,
   "abstract": [
    "Improving the performance of end-to-end ASR models on long utterances ranging from minutes to hours is an ongoing challenge in speech recognition. A common solution is to segment the audio in advance using a separate voice activity detector (VAD) that decides segment boundary locations based purely on acoustic speech/non-speech information. VAD segmenters, however, may be sub-optimal for real-world speech where, e.g., a complete sentence that should be taken as a whole may contain hesitations in the middle (``set an alarm for... 5 o'clock''). We propose to replace the VAD with an end-to-end ASR model capable of predicting segment boundaries in a streaming fashion, allowing the segmentation decision to be conditioned not only on better acoustic features but also on semantic features from the decoded text with negligible extra computation. In experiments on real world long-form audio (YouTube) with lengths of up to 30 minutes long, we demonstrate 8.5% relative WER improvement and 250 ms reduction in median end-of-segment latency compared to the VAD baseline on a state-of-the-art Conformer RNN-T model."
   ],
   "doi": "10.21437/Interspeech.2022-38"
  },
  "li22_interspeech": {
   "authors": [
    [
     "Shaokai",
     "Li"
    ],
    [
     "Peng",
     "Song"
    ],
    [
     "Keke",
     "Zhao"
    ],
    [
     "Wenjing",
     "Zhang"
    ],
    [
     "Wenming",
     "Zheng"
    ]
   ],
   "title": "Coupled Discriminant Subspace Alignment for Cross-database Speech Emotion Recognition",
   "original": "40",
   "page_count": 5,
   "order": 951,
   "p1": 4695,
   "pn": 4699,
   "abstract": [
    "Speech emotion recognition (SER) is a long-standing important research problem in speech signal processing. In practice, the training and test data are often collected in different scenarios, e.g., different languages, different collecting devices, which would severely degrade the recognition performance. To tackle this problem, in this letter, we propose a novel transfer learning algorithm, named coupled discriminant subspace alignment (CDSA), for cross-database SER. In CDSA, we first conduct linear discriminant analysis (LDA) in source and target databases, respectively. Meanwhile, we learn a latent common subspace, where the target samples are represented by the combination of source samples. Furthermore, we align the projection subspace of source and target databases to make the model more robust. Extensive experiments are carried out on four benchmark databases, and the results demonstrate the effectiveness of the proposed method."
   ],
   "doi": "10.21437/Interspeech.2022-40"
  },
  "schmidt22_interspeech": {
   "authors": [
    [
     "Nicolás",
     "Schmidt"
    ],
    [
     "Jordi",
     "Pons"
    ],
    [
     "Marius",
     "Miron"
    ]
   ],
   "title": "PodcastMix: A dataset for separating music and speech in podcasts",
   "original": "41",
   "page_count": 5,
   "order": 47,
   "p1": 231,
   "pn": 235,
   "abstract": [
    "We introduce PodcastMix, a dataset formalizing the task of separating background music and foreground speech in podcasts. We aim at defining a benchmark suitable for training and evaluating (deep learning) source separation models. To that end, we release a large and diverse training dataset based on programatically generated podcasts. However, current (deep learning) models can incur into generalization issues, specially when trained on synthetic data. To target potential generalization issues, we release an evaluation set based on real podcasts for which we design objective and subjective tests. Out of our experiments with real podcasts, we find that current (deep learning) models may have generalization issues. Yet, these can perform competently, e.g., our best baseline separates speech with a mean opinion score of 3.84 (rating ``overall separation quality\" from 1 to 5). The dataset and baselines are accessible online."
   ],
   "doi": "10.21437/Interspeech.2022-41"
  },
  "liang22_interspeech": {
   "authors": [
    [
     "Yunhao",
     "Liang"
    ],
    [
     "Yanhua",
     "Long"
    ],
    [
     "Yijie",
     "Li"
    ],
    [
     "Jiaen",
     "Liang"
    ]
   ],
   "title": "Selective Pseudo-labeling and Class-wise Discriminative Fusion for Sound Event Detection",
   "original": "42",
   "page_count": 5,
   "order": 304,
   "p1": 1496,
   "pn": 1500,
   "abstract": [
    "In recent years, exploring effective sound separation (SSep) techniques to improve overlapping sound event detection (SED) attracts more and more attention. Creating accurate separation signals to avoid the catastrophic error accumulation during SED model training is very important and challenging. In this study, we first propose a novel selective pseudo-labeling approach, termed SPL, to produce high confidence separated target events from blind sound separation outputs. These target events are then used to fine-tune the original SED model that pre-trained on the sound mixtures in a multi-objective learning style. Then, to further leverage the SSep outputs, a class-wise discriminative fusion is proposed to improve the final SED performances, by combining multiple frame-level event predictions of both sound mixtures and their separated signals. All experiments are performed on the public DCASE 2021 Task 4 dataset, and results show that our approaches significantly outperforms the official baseline, the collar-based F 1, PSDS1 and PSDS2 performances are improved from 44.3%, 37.3% and 54.9% to 46.5%, 44.5% and 75.4%, respectively."
   ],
   "doi": "10.21437/Interspeech.2022-42"
  },
  "ge22_interspeech": {
   "authors": [
    [
     "Xiaofeng",
     "Ge"
    ],
    [
     "Jiangyu",
     "Han"
    ],
    [
     "Yanhua",
     "Long"
    ],
    [
     "Haixin",
     "Guan"
    ]
   ],
   "title": "PercepNet+: A Phase and SNR Aware PercepNet for Real-Time Speech Enhancement",
   "original": "43",
   "page_count": 5,
   "order": 187,
   "p1": 916,
   "pn": 920,
   "abstract": [
    "PercepNet, a recent extension of the RNNoise, an efficient, high-quality and real-time full-band speech enhancement technique, has shown promising performance in various public deep noise suppression tasks. This paper proposes a new approach, named PercepNet+, to further extend the PercepNet with four significant improvements. First, we introduce a phase-aware structure to leverage the phase information into PercepNet, by adding the complex features and complex subband gains as the deep network input and output respectively. Then, a signal-to-noise ratio (SNR) estimator and an SNR-switched post-processing are specially designed to alleviate the over attenuation (OA) that appears in high SNR conditions of the original PercepNet. Moreover, the GRU layer is replaced by TF-GRU to model both temporal and frequency dependencies. Finally, we propose to integrate the loss of complex subband gain, SNR, pitch filtering strength, and an OA loss in a multi-objective learning manner to further improve the speech enhancement performance. Experimental results show that, the proposed PercepNet+ outperforms the original PercepNet significantly in terms of both PESQ and STOI, without increasing the model size too much."
   ],
   "doi": "10.21437/Interspeech.2022-43"
  },
  "han22_interspeech": {
   "authors": [
    [
     "Seungu",
     "Han"
    ],
    [
     "Junhyeok",
     "Lee"
    ]
   ],
   "title": "NU-Wave 2: A General Neural Audio Upsampling Model for Various Sampling Rates",
   "original": "45",
   "page_count": 5,
   "order": 892,
   "p1": 4401,
   "pn": 4405,
   "abstract": [
    "Conventionally, audio super-resolution models fixed the initial and the target sampling rates, which necessitate the model to be trained for each pair of sampling rates. We introduce NU-Wave 2, a diffusion model for neural audio upsampling that enables the generation of 48 kHz audio signals from inputs of various sampling rates with a single model. Based on the architecture of NU-Wave, NU-Wave 2 uses short-time Fourier convolution (STFC) to generate harmonics to resolve the main failure modes of NU-Wave, and incorporates bandwidth spectral feature transform (BSFT) to condition the bandwidths of inputs in the frequency domain. We experimentally demonstrate that NU-Wave 2 produces high-resolution audio regardless of the sampling rate of input while requiring fewer parameters than other models. The official code and the audio samples are available at \\url{https://mindslab-ai.github.io/nuwave2}."
   ],
   "doi": "10.21437/Interspeech.2022-45"
  },
  "cho22_interspeech": {
   "authors": [
    [
     "Hyunjae",
     "Cho"
    ],
    [
     "Wonbin",
     "Jung"
    ],
    [
     "Junhyeok",
     "Lee"
    ],
    [
     "Sang Hoon",
     "Woo"
    ]
   ],
   "title": "SANE-TTS: Stable And Natural End-to-End Multilingual Text-to-Speech",
   "original": "46",
   "page_count": 5,
   "order": 1,
   "p1": 1,
   "pn": 5,
   "abstract": [
    "In this paper, we present SANE-TTS, a stable and natural end-to-end multilingual TTS model. By the difficulty of obtaining multilingual corpus for given speaker, training multilingual TTS model with monolingual corpora is unavoidable. We introduce speaker regularization loss that improves speech naturalness during cross-lingual synthesis as well as domain adversarial training, which is applied in other multilingual TTS models. Furthermore, by adding speaker regularization loss, replacing speaker embedding with zero vector in duration predictor stabilizes cross-lingual inference. With this replacement, our model generates speeches with moderate rhythm regardless of source speaker in cross-lingual synthesis. In MOS evaluation, SANE-TTS achieves naturalness score above 3.80 both in cross-lingual and intralingual synthesis, where the ground truth score is 3.99. Also, SANE-TTS maintains speaker similarity close to that of ground truth even in cross-lingual inference. Audio samples are available on our web page."
   ],
   "doi": "10.21437/Interspeech.2022-46"
  },
  "lee22_interspeech": {
   "authors": [
    [
     "Yuna",
     "Lee"
    ],
    [
     "Seung Jun",
     "Baek"
    ]
   ],
   "title": "Keyword Spotting with Synthetic Data using Heterogeneous Knowledge Distillation",
   "original": "47",
   "page_count": 5,
   "order": 284,
   "p1": 1397,
   "pn": 1401,
   "abstract": [
    "It is crucial that Keyword Spotting (KWS) systems learn to understand new classes of user-defined keywords, which however is a challenging task requiring high-quality audio datasets. We propose KWS with Heterogeneous Embedding Knowledge Distillation (HEKD) which uses only synthetic data of unseen keyword classes. In HEKD, a reference model transfers the heterogeneous knowledge on seen classes to the student model for classifying keywords of unseen classes. By mimicking the embedding function of reference model trained on real data via a contrastive learning approach, we show that student model can learn to discriminate unseen keyword classes guided by synthetic data. In addition, we propose to maximize the dispersion of embedding clusters of unseen keywords with approximation guarantees in order to enhance the inter-class variability. Experiments show that HEKD outperforms baseline schemes using few-shot learning and those pre-trained on a large volume of data, demonstrating its effectiveness and efficiency."
   ],
   "doi": "10.21437/Interspeech.2022-47"
  },
  "wang22b_interspeech": {
   "authors": [
    [
     "Yu",
     "Wang"
    ],
    [
     "Xinsheng",
     "Wang"
    ],
    [
     "Pengcheng",
     "Zhu"
    ],
    [
     "Jie",
     "Wu"
    ],
    [
     "Hanzhao",
     "Li"
    ],
    [
     "Heyang",
     "Xue"
    ],
    [
     "Yongmao",
     "Zhang"
    ],
    [
     "Lei",
     "Xie"
    ],
    [
     "Mengxiao",
     "Bi"
    ]
   ],
   "title": "Opencpop: A High-Quality Open Source Chinese Popular Song Corpus for Singing Voice Synthesis",
   "original": "48",
   "page_count": 5,
   "order": 860,
   "p1": 4242,
   "pn": 4246,
   "abstract": [
    "This paper introduces Opencpop, a publicly available high-quality Mandarin singing corpus designed for singing voicesynthesis (SVS). The corpus consists of 100 popular Mandarinsongs performed by a female professional singer. Audio filesare recorded with studio quality at a sampling rate of 44,100 Hzand the corresponding lyrics and musical scores are provided.All singing recordings have been phonetically annotated withphoneme boundaries and syllable (note) boundaries. To demon-strate the reliability of the released data and to provide a baselinefor future research, we built baseline deep neural network-basedSVS models and evaluated them with both objective metrics andsubjective mean opinion score (MOS) measure. Experimentalresults show that the best SVS model trained on our databaseachieves 3.70 MOS, indicating the reliability of the providedcorpus. Opencpop is released to the open-source community WeNet, and the corpus, as well as synthesized demos, can befound on the project homepage."
   ],
   "doi": "10.21437/Interspeech.2022-48"
  },
  "strom22_interspeech": {
   "authors": [
    [
     "Nikko",
     "Strom"
    ],
    [
     "Haidar",
     "Khan"
    ],
    [
     "Wael",
     "Hamza"
    ]
   ],
   "title": "Squashed Weight Distribution for Low Bit Quantization of Deep Models",
   "original": "50",
   "page_count": 5,
   "order": 802,
   "p1": 3953,
   "pn": 3957,
   "abstract": [
    "Inference with large deep learning models in resource-constrained settings is increasingly a bottleneck in real-world applications of state-of-the-art AI. Here we address this by low-precision weight quantization. We achieve very low accuracy degradation by re-parametrizing the weights in a way that leaves the weight distribution approximately uniform. We show lower bit-width quantization and less accuracy degradation than previously reported in experiments on GLUE benchmarks (3-bit, 0.2% rel. degradation), and on internal intent/slot-filling datasets (2-bit, 0.4% rel. degradation)."
   ],
   "doi": "10.21437/Interspeech.2022-50"
  },
  "guo22_interspeech": {
   "authors": [
    [
     "Haohan",
     "Guo"
    ],
    [
     "Hui",
     "Lu"
    ],
    [
     "Xixin",
     "Wu"
    ],
    [
     "Helen",
     "Meng"
    ]
   ],
   "title": "A Multi-Scale Time-Frequency Spectrogram Discriminator for GAN-based Non-Autoregressive TTS",
   "original": "52",
   "page_count": 5,
   "order": 318,
   "p1": 1566,
   "pn": 1570,
   "abstract": [
    "The generative adversarial network (GAN) has shown its outstanding capability in improving Non-Autoregressive TTS (NAR-TTS) by adversarially training it with an extra model that discriminates between the real and the generated speech. To maximize the benefits of GAN, it is crucial to find a powerful discriminator that can capture rich distinguishable information. In this paper, we propose a multi-scale time-frequency spectrogram discriminator to help NAR-TTS generate high-fidelity Mel-spectrograms. It treats the spectrogram as a 2D image to exploit the correlation among different components in the time-frequency domain. And a U-Net-based model structure is employed to discriminate at different scales to capture both coarse-grained and fine-grained information. We conduct subjective tests to evaluate the proposed approach. Both multi-scale and time-frequency discriminating bring significant improvement in the naturalness and fidelity. When combining the neural vocoder, it is shown more effective and concise than fine-tuning the vocoder. Finally, we visualize the discriminating maps to compare their difference to verify the effectiveness of multi-scale discriminating."
   ],
   "doi": "10.21437/Interspeech.2022-52"
  },
  "bae22_interspeech": {
   "authors": [
    [
     "Hanbin",
     "Bae"
    ],
    [
     "Young-Sun",
     "Joo"
    ]
   ],
   "title": "Enhancement of Pitch Controllability using Timbre-Preserving Pitch Augmentation in FastPitch",
   "original": "55",
   "page_count": 5,
   "order": 2,
   "p1": 6,
   "pn": 10,
   "abstract": [
    "The recently developed pitch-controllable text-to-speech (TTS) model, i.e. FastPitch, was conditioned for the pitch contours. However, the quality of the synthesized speech degraded considerably for pitch values that deviated significantly from the average pitch; i.e. the ability to control vocal pitch was limited. To address this issue, we propose two algorithms to improve the robustness of FastPitch. First, we propose a novel timbre-preserving pitch-shifting algorithm for natural pitch augmentation. Pitch-shifted speech samples sound more natural when using the proposed algorithm because the speaker's vocal timbre is maintained. Moreover, we propose a training algorithm that defines FastPitch using pitch-augmented speech datasets with different pitch ranges for the same sentence. The experimental results demonstrate that the proposed algorithms improve the pitch controllability of FastPitch."
   ],
   "doi": "10.21437/Interspeech.2022-55"
  },
  "tsiamas22_interspeech": {
   "authors": [
    [
     "Ioannis",
     "Tsiamas"
    ],
    [
     "Gerard I.",
     "Gállego"
    ],
    [
     "José A. R.",
     "Fonollosa"
    ],
    [
     "Marta R.",
     "Costa-jussà"
    ]
   ],
   "title": "SHAS: Approaching optimal Segmentation for End-to-End Speech Translation",
   "original": "59",
   "page_count": 5,
   "order": 22,
   "p1": 106,
   "pn": 110,
   "abstract": [
    "Speech translation models are unable to directly process long audios, like TED talks, which have to be split into shorter segments. Speech translation datasets provide manual segmentations of the audios, which are not available in real-world scenarios, and existing segmentation methods usually significantly reduce translation quality at inference time. To bridge the gap between the manual segmentation of training and the automatic one at inference, we propose Supervised Hybrid Audio Segmentation (SHAS), a method that can effectively learn the optimal segmentation from any manually segmented speech corpus. First, we train a classifier to identify the included frames in a segmentation, using speech representations from a pre-trained wav2vec 2.0. The optimal splitting points are then found by a probabilistic Divide-and-Conquer algorithm that progressively splits at the frame of lowest probability until all segments are below a pre-specified length. Experiments on MuST-C and mTEDx show that the translation of the segments produced by our method approaches the quality of the manual segmentation on 5 languages pairs. Namely, SHAS retains 95-98% of the manual segmentation's BLEU score, compared to the 87-93% of the best existing methods. Our method is additionally generalizable to different domains and achieves high zero-shot performance in unseen languages."
   ],
   "doi": "10.21437/Interspeech.2022-59"
  },
  "xu22b_interspeech": {
   "authors": [
    [
     "Qiantong",
     "Xu"
    ],
    [
     "Alexei",
     "Baevski"
    ],
    [
     "Michael",
     "Auli"
    ]
   ],
   "title": "Simple and Effective Zero-shot Cross-lingual Phoneme Recognition",
   "original": "60",
   "page_count": 5,
   "order": 431,
   "p1": 2113,
   "pn": 2117,
   "abstract": [
    "Recent progress in self-training, self-supervised pretraining and unsupervised learning enabled well performing speech recognition systems without any labeled data. However, in many cases there is labeled data available for related languages which is not utilized by these methods. This paper extends previous work on zero-shot cross-lingual transfer learning by fine-tuning a multilingually pretrained wav2vec 2.0 model to transcribe unseen languages. This is done by mapping phonemes of the training languages to the target language using articulatory features. Experiments show that this simple method significantly outperforms prior work which introduced task-specific architectures and used only part of a monolingually pretrained model."
   ],
   "doi": "10.21437/Interspeech.2022-60"
  },
  "kim22_interspeech": {
   "authors": [
    [
     "Byeonggeun",
     "Kim"
    ],
    [
     "Seunghan",
     "Yang"
    ],
    [
     "Jangho",
     "Kim"
    ],
    [
     "Hyunsin",
     "Park"
    ],
    [
     "Juntae",
     "Lee"
    ],
    [
     "Simyung",
     "Chang"
    ]
   ],
   "title": "Domain Generalization with Relaxed Instance Frequency-wise Normalization for Multi-device Acoustic Scene Classification",
   "original": "61",
   "page_count": 5,
   "order": 487,
   "p1": 2393,
   "pn": 2397,
   "abstract": [
    "While using two-dimensional convolutional neural networks (2D-CNNs) in image processing, it is possible to manipulate domain information using channel statistics, and instance normalization has been a promising way to get domain-invariant features. Unlike image processing, we analyze that domain-relevant information in an audio feature is dominant in frequency statistics rather than channel statistics. Motivated by our analysis, we introduce Relaxed Instance Frequency-wise Normalization (RFN): a plug-and-play, explicit normalization module along the frequency axis which can eliminate instance-specific domain discrepancy in an audio feature while relaxing undesirable loss of useful discriminative information. Empirically, simply adding RFN to networks shows clear margins compared to previous domain generalization approaches on acoustic scene classification and yields improved robustness for multiple audio-devices. Especially, the proposed RFN won the DCASE2021 challenge TASK1A, low-complexity acoustic scene classification with multiple devices, with a clear margin, and this work is extended version of the work."
   ],
   "doi": "10.21437/Interspeech.2022-61"
  },
  "luo22_interspeech": {
   "authors": [
    [
     "Jian",
     "Luo"
    ],
    [
     "Jianzong",
     "Wang"
    ],
    [
     "Ning",
     "Cheng"
    ],
    [
     "Edward",
     "Xiao"
    ],
    [
     "Xulong",
     "Zhang"
    ],
    [
     "Jing",
     "Xiao"
    ]
   ],
   "title": "Tiny-Sepformer: A Tiny Time-Domain Transformer Network For Speech Separation",
   "original": "66",
   "page_count": 5,
   "order": 1077,
   "p1": 5313,
   "pn": 5317,
   "abstract": [
    "Time-domain Transformer neural networks have proven their superiority in speech separation tasks. However, these models usually have a large number of network parameters, thus often encountering the problem of GPU memory explosion. In this paper, we proposed Tiny-Sepformer, a tiny version of Transformer network for speech separation. We presented two techniques to reduce the model parameters and memory consumption: (1) Convolution-Attention (CA) block, spliting the vanilla Transformer to two paths, multi-head attention and 1D depthwise separable convolution, (2) parameter sharing, sharing the layer parameters within the CA block. In our experiments, Tiny-Sepformer could greatly reduce the model size, and achieves comparable separation performance with vanilla Sepformer on WSJ0-2/3Mix datasets."
   ],
   "doi": "10.21437/Interspeech.2022-66"
  },
  "munakata22_interspeech": {
   "authors": [
    [
     "Hokuto",
     "Munakata"
    ],
    [
     "Ryu",
     "Takeda"
    ],
    [
     "Kazunori",
     "Komatani"
    ]
   ],
   "title": "Training Data Generation with DOA-based Selecting and Remixing for Unsupervised Training of Deep Separation Models",
   "original": "69",
   "page_count": 5,
   "order": 176,
   "p1": 861,
   "pn": 865,
   "abstract": [
    "We propose a simple and easy-to-apply unsupervised training method for multi-channel deep separation models used in sound source separation. Such models require a large amount of training data, i.e., source signals and their mixtures. A previous method uses pseudo-target source signals, which can be obtained as the outputs of blind source separation (BSS) based on statistical models in place of ground-truth source signals. However, the model performance of the previous method is degraded by some pseudo-targets that are inadequately separated by BSS. To exploit the reliable part of BSS, we select and remix well-separated signals included in the BSS result. In the selection step, we choose well-separated signals using the direction of arrival (DOA). As a criterion that addresses the quality of the separated signals, we adopted the minimum angular difference of DOA between source signals. In the remixing step, we introduce resampling of the DOA, which generates mixtures composed of source signals with both wide and narrow angular differences. These mixtures are not simply given by BSS and allow the deep separation model to learn both spectral and spatial information. In our experiment, our method's model performance was improved for mixture signals composed of the sources from various angles."
   ],
   "doi": "10.21437/Interspeech.2022-69"
  },
  "pan22_interspeech": {
   "authors": [
    [
     "Jiahui",
     "Pan"
    ],
    [
     "Shuai",
     "Nie"
    ],
    [
     "Hui",
     "Zhang"
    ],
    [
     "Shulin",
     "He"
    ],
    [
     "Kanghao",
     "Zhang"
    ],
    [
     "Shan",
     "Liang"
    ],
    [
     "Xueliang",
     "Zhang"
    ],
    [
     "Jianhua",
     "Tao"
    ]
   ],
   "title": "Speaker recognition-assisted robust audio deepfake detection",
   "original": "72",
   "page_count": 5,
   "order": 852,
   "p1": 4202,
   "pn": 4206,
   "abstract": [
    "Audio deepfake detection is usually formulated as a binary classification between genuine and fake speech for an entire utterance. Environmental clues such as background and device noise can be used as the classification features, but they are easy to be attacked, e.g. by simply adding real noise to the fake speech. While speech spectral discrimination are more robust features, which have been used in speaker recognition models to authenticate the speaker identity. In the study, we propose a speaker recognition-assisted audio deepfake detector. Feature representation extracted by a speaker recognition model is introduced into multiple layers of deepfake detector to fully exploit the inherent spectral discrimination of speech. Speaker recognition and audio deepfake detection models are jointly optimized by a multi-objective learning method. Systematic experiments on the ASVspoof 2019 logical access corpus demonstrate the proposed approach outperforms existing single systems and significantly improves the robustness to noise."
   ],
   "doi": "10.21437/Interspeech.2022-72"
  },
  "fietkau22_interspeech": {
   "authors": [
    [
     "Arne-Lukas",
     "Fietkau"
    ],
    [
     "Simon",
     "Stone"
    ],
    [
     "Peter",
     "Birkholz"
    ]
   ],
   "title": "Relationship between the acoustic time intervals and tongue movements of German diphthongs",
   "original": "73",
   "page_count": 5,
   "order": 148,
   "p1": 734,
   "pn": 738,
   "abstract": [
    "This study investigated the relationship between tongue movements during the production of German diphthongs and their acoustic time intervals. To this end, five subjects produced a set of logatomes that contained German primary, secondary, and peripheral diphthongs in the context of bilabial and labiodental consonants at three different speaking rates. During the utterances, tongue movements were measured by means of optical palatography (OPG), i.e. by optical distance sensing in the oral cavity, along with the acoustic speech signal. The analysis of the movement signals revealed that the diphthongs have s-shaped tongue trajectories that strongly resemble half cosine periods. In addition, acoustic and articulatory diphthong durations have a linear, but not proportional, relationship. Finally, the peak velocity and midpoint between the two targets of a diphthong are reached in the middle of both the acoustic and articulatory diphthong time intervals, regardless of the duration and type of diphthong. These results can help to model realistic tongue movements for diphthongs in articulatory speech synthesis."
   ],
   "doi": "10.21437/Interspeech.2022-73"
  },
  "chen22_interspeech": {
   "authors": [
    [
     "Weidong",
     "Chen"
    ],
    [
     "Xiaofen",
     "Xing"
    ],
    [
     "Xiangmin",
     "Xu"
    ],
    [
     "Jianxin",
     "Pang"
    ],
    [
     "Lan",
     "Du"
    ]
   ],
   "title": "SpeechFormer: A Hierarchical Efficient Framework Incorporating the Characteristics of Speech",
   "original": "74",
   "page_count": 5,
   "order": 70,
   "p1": 346,
   "pn": 350,
   "abstract": [
    "Transformer has obtained promising results on cognitive speech signal processing field, which is of interest in various applications ranging from emotion to neurocognitive disorder analysis. However, most works treat speech signal as a whole, leading to the neglect of the pronunciation structure that is unique to speech and reflects the cognitive process. Meanwhile, Transformer has heavy computational burden due to its full attention operation. In this paper, a hierarchical efficient framework, called SpeechFormer, which considers the structural characteristics of speech, is proposed and can be served as a general-purpose backbone for cognitive speech signal processing. The proposed SpeechFormer consists of frame, phoneme, word and utterance stages in succession, each performing a neighboring attention according to the structural pattern of speech with high computational efficiency. SpeechFormer is evaluated on speech emotion recognition (IEMOCAP & MELD) and neurocognitive disorder detection (Pitt & DAIC-WOZ) tasks, and the results show that SpeechFormer outperforms the standard Transformer-based framework while greatly reducing the computational cost. Furthermore, our SpeechFormer achieves comparable results to the state-of-the-art approaches."
   ],
   "doi": "10.21437/Interspeech.2022-74"
  },
  "deja22_interspeech": {
   "authors": [
    [
     "Kamil",
     "Deja"
    ],
    [
     "Ariadna",
     "Sanchez"
    ],
    [
     "Julian",
     "Roth"
    ],
    [
     "Marius",
     "Cotescu"
    ]
   ],
   "title": "Automatic Evaluation of Speaker Similarity",
   "original": "75",
   "page_count": 5,
   "order": 478,
   "p1": 2348,
   "pn": 2352,
   "abstract": [
    "We introduce a new automatic evaluation method for speaker similarity assessment, that is consistent with human perceptual scores. Modern neural text-to-speech models require a vast amount of clean training data, which is why many solutions switch from single speaker models to solutions trained on examples from many different speakers. Multi-speaker models bring new possibilities, such as a faster creation of new voices, but also a new problem - speaker leakage, where the speaker identity of a synthesized example might not match those of the target speaker. Currently, the only way to discover this issue is through costly perceptual evaluations. In this work, we propose an automatic method for assessment of speaker similarity. For that purpose, we extend the recent work on speaker verification systems and evaluate how different metrics and speaker embeddings models reflect Multiple Stimuli with Hidden Reference and Anchor (MUSHRA) scores. Our experiments show that we can train a model to predict speaker similarity MUSHRA scores from speaker embeddings with 0.96 accuracy and significant correlation up to 0.78 Pearson score at the utterance level."
   ],
   "doi": "10.21437/Interspeech.2022-75"
  },
  "mateju22_interspeech": {
   "authors": [
    [
     "Lukas",
     "Mateju"
    ],
    [
     "Frantisek",
     "Kynych"
    ],
    [
     "Petr",
     "Cerva"
    ],
    [
     "Jiri",
     "Malek"
    ],
    [
     "Jindrich",
     "Zdansky"
    ]
   ],
   "title": "Overlapped Speech Detection in Broadcast Streams Using X-vectors",
   "original": "81",
   "page_count": 5,
   "order": 933,
   "p1": 4606,
   "pn": 4610,
   "abstract": [
    "A new approach to overlapped speech detection (OSD) is introduced in this work. It is designed for real-time processing of streamed data and utilizes x-vectors as its input features. It thus allows us to reduce computational demands within the entire streaming data processing chain, where the same x-vectors can also be used for the related task of speaker diarization. Within our method, the x-vectors are extracted using a feed-forward sequential memory network (FSMN) and then fed into a simple neural classifier (speech or cross-talk), whose output is smoothed by a decoder based on weighted finite-state transducers (WFSTs). The evaluation is done on a Czech/Slovak broadcast dataset (we make this data public) and on the AMI meeting corpus. Our online method yields a solid performance while operating with a 2-second latency."
   ],
   "doi": "10.21437/Interspeech.2022-81"
  },
  "liu22_interspeech": {
   "authors": [
    [
     "Yuchen",
     "Liu"
    ],
    [
     "Apu",
     "Kapadia"
    ],
    [
     "Donald",
     "Williamson"
    ]
   ],
   "title": "Preventing sensitive-word recognition using self-supervised learning to preserve user-privacy for automatic speech recognition",
   "original": "85",
   "page_count": 5,
   "order": 853,
   "p1": 4207,
   "pn": 4211,
   "abstract": [
    "Smart voice assistants that rely on automatic speech recognition (ASR) are widely used by people for multiple reasons. These devices, however, feature \"always on\" microphones that enable sensitive and private user information to be maliciously or inadvertently collected. In this paper, we develop an end-to-end approach that generates utterance-specific perturbations that obscure a set of words that have been deemed sensitive. In particular, spoken digits, which may be contained in credit card or social security numbers, have been chosen as the words that an ASR system should not be able to recognize, though all other words should be recognized accordingly. Our approach consists of a self-supervised learning feature extractor and U-Net style network for generating noise perturbations. The proposed approach shows promising performance that will help address privacy concerns, without affecting the main functionality of an ASR model."
   ],
   "doi": "10.21437/Interspeech.2022-85"
  },
  "choi22_interspeech": {
   "authors": [
    [
     "Jeong-Hwan",
     "Choi"
    ],
    [
     "Joon-Young",
     "Yang"
    ],
    [
     "Ye-Rin",
     "Jeoung"
    ],
    [
     "Joon-Hyuk",
     "Chang"
    ]
   ],
   "title": "Improved CNN-Transformer using Broadcasted Residual Learning for Text-Independent Speaker Verification",
   "original": "88",
   "page_count": 5,
   "order": 453,
   "p1": 2223,
   "pn": 2227,
   "abstract": [
    "This study proposes a novel speaker embedding extractor architecture that effectively combines convolutional neural networks (CNNs) and Transformers. Based on the recently proposed CNNs-meet-vision-Transformers (CMT) architecture, we propose two strategies for efficient speaker embedding extraction modeling. First, we apply broadcast residual learning techniques to the building blocks of the CMT, allowing us to extract frequency-aware temporal features shared across frequency dimensions with a reduced set of parameters. Second, frequency-statistics-dependent attentive statistics pooling is proposed to aggregate attentive temporal statistics acquired from the means and standard deviations of input feature maps weighted along the frequency axis using an attention mechanism. The experimental results on the VoxCeleb-1 dataset show that the proposed model outperforms several CNN- and Transformer-based models with a similar number of model parameters. Moreover, the effectiveness of the proposed modifications to the CMT architecture is validated through ablation studies."
   ],
   "doi": "10.21437/Interspeech.2022-88"
  },
  "luo22b_interspeech": {
   "authors": [
    [
     "Mingqiong",
     "Luo"
    ]
   ],
   "title": "Mandarin nasal place assimilation revisited: an acoustic study",
   "original": "89",
   "page_count": 5,
   "order": 1060,
   "p1": 5228,
   "pn": 5232,
   "abstract": [
    "Two types of nasal place assimilation (NPA) have been proved to take place in Mandarin Chinese (MC) disyllabic words: categorical NPA and gradient NPA. The former mainly happens in /n.C/ clusters and the latter /ŋ.C/ clusters. Previous studies show that in fast speech, the nasal coda may be completely lost, indicating a possible effect of speech rate on NPA patterns. Previous studies on NPA in other languages show that not only does the consonant have anticipatory effect on the preceding nasal, but the nasal also has carryover effect on the following consonant. However, no agreement has been reached concerning speech-rate effect on NPA patterns. This study aims to examine whether the nasal coda in MC N.C clusters has carryover effect on C and how speech rate affects N.C coarticulatory patterns. A production experiment was carried out and results show that (i) coda /n/ does not have carryover effect on Cs, but the fronted /a/ does; (ii) coda /ŋ/ does have carryover effect on the consonantal onset; (iii) speech rate does not have significant effect on the overall N.C coarticulatory patterns, but does for certain Cs. The results were explained in light of the degree of articulatory constraint (DAC) model."
   ],
   "doi": "10.21437/Interspeech.2022-89"
  },
  "zhao22_interspeech": {
   "authors": [
    [
     "Zifeng",
     "Zhao"
    ],
    [
     "Rongzhi",
     "Gu"
    ],
    [
     "Dongchao",
     "Yang"
    ],
    [
     "Jinchuan",
     "Tian"
    ],
    [
     "Yuexian",
     "Zou"
    ]
   ],
   "title": "Speaker-Aware Mixture of Mixtures Training for Weakly Supervised Speaker Extraction",
   "original": "96",
   "page_count": 5,
   "order": 1078,
   "p1": 5318,
   "pn": 5322,
   "abstract": [
    "Dominant researches adopt supervised training for speaker extraction, while the scarcity of ideally clean corpus and channel mismatch problem are rarely considered. To this end, we propose speaker-aware mixture of mixtures training (SAMoM), utilizing the consistency of speaker identity among target source, enrollment utterance and target estimate to weakly supervise the training of a deep speaker extractor. In SAMoM, the input is constructed by mixing up different speaker-aware mixtures (SAMs), each contains multiple speakers with their identities known and enrollment utterances available. Informed by enrollment utterances, target speech is extracted from the input one by one, such that the estimated targets can approximate the original SAMs after a remix in accordance with the identity consistency. Moreover, using SAMoM in a semi-supervised setting with a certain amount of clean sources enables application in noisy scenarios. Extensive experiments on Libri2Mix show that the proposed method achieves promising results without access to any clean sources (11.06 dB SI-SDRi). With a domain adaptation, our approach even outperformed supervised framework in a cross-domain evaluation on AISHELL-1."
   ],
   "doi": "10.21437/Interspeech.2022-96"
  },
  "masuyama22_interspeech": {
   "authors": [
    [
     "Yoshiki",
     "Masuyama"
    ],
    [
     "Kouei",
     "Yamaoka"
    ],
    [
     "Nobutaka",
     "Ono"
    ]
   ],
   "title": "Joint Optimization of Sampling Rate Offsets Based on Entire Signal Relationship Among Distributed Microphones",
   "original": "97",
   "page_count": 5,
   "order": 142,
   "p1": 704,
   "pn": 708,
   "abstract": [
    "In this paper, we propose to simultaneously estimate all the sampling rate offsets (SROs) of multiple devices. In a distributed microphone array, the SRO is inevitable, which deteriorates the performance of array signal processing. Most of the existing SRO estimation methods focused on synchronizing two microphones. When synchronizing more than two microphones, we select one reference microphone and estimate the SRO of each non-reference microphone independently. Hence, the relationship among signals observed by non-reference microphones is not considered. To address this problem, the proposed method jointly optimizes all SROs based on a probabilistic model of a multichannel signal. The SROs and model parameters are alternately updated to increase the log-likelihood based on an auxiliary function. The effectiveness of the proposed method is validated on mixtures of various numbers of speakers."
   ],
   "doi": "10.21437/Interspeech.2022-97"
  },
  "shi22_interspeech": {
   "authors": [
    [
     "Bowen",
     "Shi"
    ],
    [
     "Wei-Ning",
     "Hsu"
    ],
    [
     "Abdelrahman",
     "Mohamed"
    ]
   ],
   "title": "Robust Self-Supervised Audio-Visual Speech Recognition",
   "original": "99",
   "page_count": 5,
   "order": 432,
   "p1": 2118,
   "pn": 2122,
   "abstract": [
    "Audio-based automatic speech recognition (ASR) degrades significantly in noisy environments and is particularly vulnerable to interfering speech, as the model cannot determine which speaker to transcribe. Audio-visual speech recognition (AVSR) systems improve robustness by complementing the audio stream with the visual information that is invariant to noise and helps the model focus on the desired speaker. However, previous AVSR work focused solely on the supervised learning setup; hence the progress was hindered by the amount of labeled data available. In this work, we present a self-supervised AVSR framework built upon Audio-Visual HuBERT (AV-HuBERT), a state-of-the-art audio-visual speech representation learning model. On the largest available AVSR benchmark dataset LRS3, our approach outperforms prior state-of-the-art by ~50% (28.0% vs. 14.1%) using less than 10% of labeled data (433hr vs. 30hr) in the presence of babble noise, while reducing the WER of an audio-based model by over 75% (25.8% vs. 5.8%) on average."
   ],
   "doi": "10.21437/Interspeech.2022-99"
  },
  "kitamura22_interspeech": {
   "authors": [
    [
     "Tatsuya",
     "Kitamura"
    ],
    [
     "Naoki",
     "Kunimoto"
    ],
    [
     "Hideki",
     "Kawahara"
    ],
    [
     "Shigeaki",
     "Amano"
    ]
   ],
   "title": "Perceptual Evaluation of Penetrating Voices through a Semantic Differential Method",
   "original": "100",
   "page_count": 5,
   "order": 621,
   "p1": 3063,
   "pn": 3067,
   "abstract": [
    "Some speakers have penetrating voices that can be popped out and heard clearly, even in loud noise or from a long distance. This study investigated the voice quality of the penetrating voices using factor analysis. Eleven participants scored how the voices of 124 speakers popped out from the babble noise. By assuming the score as an index of penetration, ten each of high- and low-scored speakers were selected for a rating experiment with a semantic differential method. Forty undergraduate students rated a Japanese sentence produced by these speakers using 14 bipolar 7-point scales concerning voice quality. A factor analysis was conducted using the data of 13 scales (i.e., excluding one scale of penetrating from 14 scales). Three main factors were obtained: (1) powerful and metallic, (2) feminine, and (3) esthetic. The first factor (powerful and metallic) highly correlated with the ratings of penetrating. These results suggest that penetrating voices have multi-dimensional voice quality and that the characteristics of penetrating voice related to powerful and metallic aspects of voices."
   ],
   "doi": "10.21437/Interspeech.2022-100"
  },
  "kunihara22_interspeech": {
   "authors": [
    [
     "Takuya",
     "Kunihara"
    ],
    [
     "Chuanbo",
     "Zhu"
    ],
    [
     "Nobuaki",
     "Minematsu"
    ],
    [
     "Noriko",
     "Nakanishi"
    ]
   ],
   "title": "Gradual Improvements Observed in Learners' Perception and Production of L2 Sounds Through Continuing Shadowing Practices on a Daily Basis",
   "original": "101",
   "page_count": 5,
   "order": 265,
   "p1": 1303,
   "pn": 1307,
   "abstract": [
    "Shadowing was proposed originally in psycholinguistics to investigate listeners' process of perceiving speech, and now it is used widely in language education in Japan as a training method for learners to acquire a better skill in perceiving and producing L2 sounds. Recently, a shadowing-based assessment method was proposed to quantify learners' skill in perception and production. By integrating the two methods, this study aims at tracking gradual improvements of learners' skill during shadowing practices. We held a special program of 42-day Shadowing Marathon, where Japanese learners of English participated in shadowing practices every day for six weeks. Four new oral passages were presented daily, each of which was shadowed repeatedly. From the obtained data, we analyzed gradual improvements in learners' perception and production of both segments and prosody through repeating shadowing within a day, and through continuing the shadowing practices over 42 days. The results of analysis showed that, while learners' perception of segments and prosody improved significantly even with no explicit instructions, their production did not show significant improvement in a self-learning condition. The data also showed in what way learners depend on written input and auditory input when they learn perception and production of L2 sounds."
   ],
   "doi": "10.21437/Interspeech.2022-101"
  },
  "rui22_interspeech": {
   "authors": [
    [
     "Tao",
     "Rui"
    ],
    [
     "Yan",
     "Long"
    ],
    [
     "Ouchi",
     "Kazushige"
    ],
    [
     "Xiangdong",
     "Wang"
    ]
   ],
   "title": "Couple learning for semi-supervised sound event detection",
   "original": "103",
   "page_count": 5,
   "order": 488,
   "p1": 2398,
   "pn": 2402,
   "abstract": [
    "The recently proposed Mean Teacher method, which exploits large-scale unlabeled data in a self-ensembling manner, has achieved state-of-the-art results in several semi-supervised learning benchmarks. Spurred by current achievements, this paper proposes an effective Couple Learning method that combines a well-trained model and a Mean Teacher model. The suggested pseudo-labels generated model (PLG) increases strongly- and weakly-labeled data to improve the Mean Teacher method's performance. Moreover, the Mean Teacher's consistency cost reduces the noise impact in the pseudo-labels introduced by detection errors. The experimental results on Task 4 of the DCASE2020 challenge demonstrate the superiority of the proposed method, achieving about 44.25% F1-score on the validation set without post-processing, significantly outperforming the baseline system's 32.39%. furthermore, this paper also propose a simple and effective experiment called the Variable Order Input (VOI) experiment, which proves the significance of the Couple Learning method. Our developed Couple Learning code is available on GitHub."
   ],
   "doi": "10.21437/Interspeech.2022-103"
  },
  "stan22_interspeech": {
   "authors": [
    [
     "Adriana",
     "Stan"
    ]
   ],
   "title": "The ZevoMOS entry to VoiceMOS Challenge 2022",
   "original": "105",
   "page_count": 5,
   "order": 915,
   "p1": 4516,
   "pn": 4520,
   "abstract": [
    "This paper introduces the ZevoMOS entry to the main track of the VoiceMOS Challenge 2022. The ZevoMOS submission is based on a two-step finetuning of pretrained self-supervised learning (SSL) speech models. The first step uses a task of classifying natural versus synthetic speech, while the second step's task is to predict the MOS scores associated with each training sample. The results of the finetuning process are then combined with the confidence scores extracted from an automatic speech recognition model, as well as the raw embeddings of the training samples obtained from a wav2vec SSL speech model. The team id assigned to the ZevoMOS system within the VoiceMOS Challenge is T01. The submission was placed on the 14th place with respect to the system-level SRCC, and on the 9th place with respect to the utterance-level MSE. The paper also introduces additional evaluations of the intermediate results."
   ],
   "doi": "10.21437/Interspeech.2022-105"
  },
  "yang22_interspeech": {
   "authors": [
    [
     "Qu",
     "Yang"
    ],
    [
     "Qi",
     "Liu"
    ],
    [
     "Haizhou",
     "Li"
    ]
   ],
   "title": "Deep residual spiking neural network for keyword spotting in low-resource settings",
   "original": "107",
   "page_count": 5,
   "order": 613,
   "p1": 3023,
   "pn": 3027,
   "abstract": [
    "We propose a practical solution for the implementation of keyword spotting (KWS) system on portable devices, that features all three properties required for battery-powered portable scenarios: low power usage, small footprint, and high accuracy. In particular, we study an end-to-end KWS system with deep residual Spiking Neural Network (SNN), perform experiments on Google Speech Commands Dataset, and compare with both state-of-the-art ANN and SNN models. First, the proposed solution outperforms its ANN counterpart and other SNN in terms of energy efficiency. Second, it requires a smaller footprint (86.5K) than other ANN and SNN (210K) models. Third, in terms of classification accuracy, it outperforms the existing power-efficient SNN benchmark by 4% to 17%. The proposed solution is an example of the unparalleled performance of spiking neural network in real-world applications."
   ],
   "doi": "10.21437/Interspeech.2022-107"
  },
  "muller22_interspeech": {
   "authors": [
    [
     "Nicolas",
     "Müller"
    ],
    [
     "Pavel",
     "Czempin"
    ],
    [
     "Franziska",
     "Diekmann"
    ],
    [
     "Adam",
     "Froghyar"
    ],
    [
     "Konstantin",
     "Böttinger"
    ]
   ],
   "title": "Does Audio Deepfake Detection Generalize?",
   "original": "108",
   "page_count": 5,
   "order": 565,
   "p1": 2783,
   "pn": 2787,
   "abstract": [
    "Current text-to-speech algorithms produce realistic fakes of human voices, making deepfake detection a much-needed area of research. While researchers have presented various techniques for detecting audio spoofs, it is often unclear exactly why these architectures are successful: Preprocessing steps, hyperparameter settings, and the degree of fine-tuning are not consistent across related work. Which factors contribute to success, and which are accidental? In this work, we address this problem: We systematize audio spoofing detection by re-implementing and uniformly evaluating architectures from related work. We identify overarching features for successful audio deepfake detection, such as using cqtspec or logspec features instead of melspec features, which improves performance by 37% EER on average, all other factors constant. Additionally, we evaluate generalization capabilities: We collect and publish a new dataset consisting of 37.9 hours of found audio recordings of celebrities and politicians, of which 17.2 hours are deepfakes. We find that related work performs poorly on such real-world data (performance degradation of up to one thousand percent). This may suggest that the community has tailored its solutions too closely to the prevailing ASVSpoof benchmark and that deepfakes are much harder to detect outside the lab than previously thought."
   ],
   "doi": "10.21437/Interspeech.2022-108"
  },
  "ahn22_interspeech": {
   "authors": [
    [
     "Youngdo",
     "Ahn"
    ],
    [
     "Sung Joo",
     "Lee"
    ],
    [
     "Jong Won",
     "Shin"
    ]
   ],
   "title": "Multi-Corpus Speech Emotion Recognition for Unseen Corpus Using Corpus-Wise Weights in Classification Loss",
   "original": "111",
   "page_count": 5,
   "order": 27,
   "p1": 131,
   "pn": 135,
   "abstract": [
    "Since each of the currently available emotional speech corpora is rather small to deal with personal or cultural diversity, multiple emotional speech corpora can be jointly used to train a speech emotion recognition (SER) model robust to unseen corpora. Each corpus has different characteristics, including whether acted or spontaneous, in which environment it was recorded, and what lexical contents it contains. Depending on the characteristics, the emotion recognition accuracy and time required to train a model for it are different. If we train the SER model utilizing multiple corpora equally, the classification performance for each training corpus would be different. The performance for unseen corpora may be enhanced if the model is trained to show similar recognition accuracy for each training corpus that covers different characteristics. In this study, we propose to adopt corpus-wise weights in the classification loss, which are functions of the recognition accuracy for each of the training corpus. We also adopt pseudo-emotion labels for the unlabeled speech corpus to further enhance the performance. Experimental results showed that the proposed method outperformed previously proposed approaches in the out-of-corpus SER using three emotional corpora for training and one corpus for evaluation."
   ],
   "doi": "10.21437/Interspeech.2022-111"
  },
  "baas22_interspeech": {
   "authors": [
    [
     "Matthew",
     "Baas"
    ],
    [
     "Herman",
     "Kamper"
    ]
   ],
   "title": "Voice Conversion Can Improve ASR in Very Low-Resource Settings",
   "original": "112",
   "page_count": 5,
   "order": 711,
   "p1": 3513,
   "pn": 3517,
   "abstract": [
    "Voice conversion (VC) could be used to improve speech recognition systems in low-resource languages by using it to augment limited training data. However, VC has not been widely used for this purpose because of practical issues such as compute speed and limitations when converting to and from unseen speakers. Moreover, it is still unclear whether a VC model trained on one well-resourced language can be applied to speech from another low-resource language for the aim of data augmentation. In this work we assess whether a VC system can be used cross-lingually to improve low-resource speech recognition. We combine several recent techniques to design and train a practical VC system in English, and then use this system to augment data for training speech recognition models in several low-resource languages. When using a sensible amount of VC augmented data, speech recognition performance is improved in all four low-resource languages considered. We also show that VC-based augmentation is superior to SpecAugment (a widely used signal processing augmentation method) in the low-resource languages considered."
   ],
   "doi": "10.21437/Interspeech.2022-112"
  },
  "feinberg22_interspeech": {
   "authors": [
    [
     "David",
     "Feinberg"
    ]
   ],
   "title": "VoiceLab: Software for Fully Reproducible Automated Voice Analysis",
   "original": "113",
   "page_count": 5,
   "order": 71,
   "p1": 351,
   "pn": 355,
   "abstract": [
    "There's a problem with acoustic analyses because you often need to hand adjust parameters meaning you can only process them individually or in small batches. This creates two key problems. First, it compromises the reproducibility of measurements because setting parameters by hand requires specialist knowledge and is often poorly documented. Second, it means that you can't easily process large samples of voices, which creates bottlenecks in workflows. This issue is compounded by researchers looking to use increasingly large and diverse samples. To address these issues, VoiceLab software offers automated acoustical analysis and automatically logs analysis parameters. VoiceLab analyses are fully reproducible and require little to no knowledge about acoustical analysis from the user. Analysis parameters can also be manually adjusted by experts. VoiceLab is used primarily by researchers studying person perception, creating reproducible voice manipulations, developing voices for conversational agents, and creating feature sets for machine learning."
   ],
   "doi": "10.21437/Interspeech.2022-113"
  },
  "pesoparada22_interspeech": {
   "authors": [
    [
     "Pablo",
     "Peso Parada"
    ],
    [
     "Agnieszka",
     "Dobrowolska"
    ],
    [
     "Karthikeyan",
     "Saravanan"
    ],
    [
     "Mete",
     "Ozay"
    ]
   ],
   "title": "pMCT: Patched Multi-Condition Training for Robust Speech Recognition",
   "original": "117",
   "page_count": 5,
   "order": 766,
   "p1": 3779,
   "pn": 3783,
   "abstract": [
    "We propose a novel Patched Multi-Condition Training (pMCT) method for robust Automatic Speech Recognition (ASR). pMCT employs Multi-condition Audio Modification and Patching (MAMP) via mixing patches of the same utterance extracted from clean and distorted speech. Training using patchmodified signals improves robustness of models in noisy reverberant scenarios. Our proposed pMCT is evaluated on the LibriSpeech dataset showing improvement over using vanilla Multi-Condition Training (MCT). For analyses on robust ASR, we employed pMCT on the VOiCES dataset which is a noisy reverberant dataset created using utterances from LibriSpeech. In the analyses, pMCT achieves 23.1% relative WER reduction compared to the MCT."
   ],
   "doi": "10.21437/Interspeech.2022-117"
  },
  "shor22_interspeech": {
   "authors": [
    [
     "Joel",
     "Shor"
    ],
    [
     "Subhashini",
     "Venugopalan"
    ]
   ],
   "title": "TRILLsson: Distilled Universal Paralinguistic Speech Representations",
   "original": "118",
   "page_count": 5,
   "order": 72,
   "p1": 356,
   "pn": 360,
   "abstract": [
    "Recent advances in self-supervision have dramatically improved the quality of speech representations. However, deployment of state-of-the-art embedding models on devices has been restricted due to their limited public availability and large resource footprint. Our work addresses these issues by publicly releasing a collection of paralinguistic speech models that are small and near state-of-the-art performance. Our approach is based on knowledge distillation, and our models are distilled on public data only. We explore different architectures and thoroughly evaluate our models on the NonSemantic Speech (NOSS) benchmark. Our largest distilled model achieves over 96% the accuracy on 6 of 7 tasks, is less than 15% the size of the original model (314MB vs 2.2GB), and is trained on 6.5% the data. The smallest model achieves over 90% the accuracy on 6 of 7 tasks and is 1% in size (22MB). Our models outperform the 1.2GB open source Wav2Vec 2.0 model on 5 of 7 tasks despite being less than a third the size, and one of our models outperforms Wav2Vec 2.0 on both emotion recognition tasks despite being less than 4% the size."
   ],
   "doi": "10.21437/Interspeech.2022-118"
  },
  "ciccarelli22_interspeech": {
   "authors": [
    [
     "Gregory",
     "Ciccarelli"
    ],
    [
     "Jarred",
     "Barber"
    ],
    [
     "Arun",
     "Nair"
    ],
    [
     "Israel",
     "Cohen"
    ],
    [
     "Tao",
     "Zhang"
    ]
   ],
   "title": "Challenges and Opportunities in Multi-device Speech Processing",
   "original": "119",
   "page_count": 5,
   "order": 143,
   "p1": 709,
   "pn": 713,
   "abstract": [
    "We review current solutions and technical challenges for automatic speech recognition, keyword spotting, device arbitration, speech enhancement, and source localization in multi-device home environments to provide context for the INTERSPEECH 2022 special session, \"Challenges and opportunities for signal processing and machine learning for multiple smart devices''. We also identify the datasets needed to support these research areas. Based on the review and our research experience in the multi-device domain, we conclude with an outlook on the future evolution of multiple device signal processing and machine learning."
   ],
   "doi": "10.21437/Interspeech.2022-119"
  },
  "liu22b_interspeech": {
   "authors": [
    [
     "Zhengyuan",
     "Liu"
    ],
    [
     "Nancy",
     "Chen"
    ]
   ],
   "title": "Dynamic Sliding Window Modeling for Abstractive Meeting Summarization",
   "original": "121",
   "page_count": 5,
   "order": 1042,
   "p1": 5150,
   "pn": 5154,
   "abstract": [
    "Summarizing spoken content using neural approaches has raised emerging research interest lately, as sequence-to-sequence approaches have improved abstractive summarization performance. However, summarizing long meeting transcripts remains challenging. Meetings are multi-party spoken discussions where information is topically diffuse, making it harder for neural models to distill and cover essential content. Such meeting summarization tasks cannot readily benefit from pre-trained language models, which typically have input length limitations. In this work, we take advantage of the intuition that the topical structure of meetings tends to correlate with the meeting agendas. Inspired by this phenomenon, we propose a dynamic sliding window strategy to elegantly decompose the long source content of meetings to smaller contextualized semantic chunks for more resourceful modeling, and propose two methods without additional trainable parameters for context boundary prediction. Experimental results show that the proposed framework achieves state-of-the-art abstractive summarization performance on the AMI corpus and obtains higher factual consistency on competitive baselines."
   ],
   "doi": "10.21437/Interspeech.2022-121"
  },
  "vuho22_interspeech": {
   "authors": [
    [
     "Tuan",
     "Vu Ho"
    ],
    [
     "Maori",
     "Kobayashi"
    ],
    [
     "Masato",
     "Akagi"
    ]
   ],
   "title": "Speak Like a Professional: Increasing Speech Intelligibility by Mimicking Professional Announcer Voice with Voice Conversion",
   "original": "124",
   "page_count": 5,
   "order": 35,
   "p1": 171,
   "pn": 175,
   "abstract": [
    "In most of practical scenarios, the announcement system must deliver speech messages in a noisy environment, in which the background noise cannot be cancelled out. The local noise reduces speech intelligibility and increases listening effort of the listener, hence hamper the effectiveness of announcement system. There has been reported that voices of professional announcers are clearer and more comprehensive than that of non-expert speakers in noisy environment. This finding suggests that the speech intelligibility might be related to the speaking style of professional announcer, which can be adapted using voice conversion method. Motivated by this idea, this paper proposes a speech intelligibility enhancement in noisy environment by applying voice conversion method on non-professional voice. We discovered that the professional announcers and non-professional speakers are clusterized into different clusters on the speaker embedding plane. This implies that the speech intelligibility can be controlled as an independent feature of speaker individuality. To examine the advantage of converted voice in noisy environment, we experimented using test words masked in pink noise at different SNR levels. The results of objective and subjective evaluations confirm that the speech intelligibility of converted voice is higher than that of original voice in low SNR conditions."
   ],
   "doi": "10.21437/Interspeech.2022-124"
  },
  "jung22_interspeech": {
   "authors": [
    [
     "Jee-weon",
     "Jung"
    ],
    [
     "Youjin",
     "Kim"
    ],
    [
     "Hee-Soo",
     "Heo"
    ],
    [
     "Bong-Jin",
     "Lee"
    ],
    [
     "Youngki",
     "Kwon"
    ],
    [
     "Joon Son",
     "Chung"
    ]
   ],
   "title": "Pushing the limits of raw waveform speaker recognition",
   "original": "126",
   "page_count": 5,
   "order": 454,
   "p1": 2228,
   "pn": 2232,
   "abstract": [
    "In recent years, speaker recognition systems based on raw waveform inputs have received increasing attention. However, the performance of such systems are typically inferior to the state-of-the-art handcrafted feature-based counterparts, which demonstrate equal error rates under 1% on the popular VoxCeleb1 test set. This paper proposes a novel speaker recognition model based on raw waveform inputs. The model incorporates recent advances in machine learning and speaker verification, including the Res2Net backbone module and multi-layer feature aggregation. Our best model achieves an equal error rate of 0.89%, which is competitive with the state-of-the-art models based on handcrafted features, and outperforms the best model based on raw waveform inputs by a large margin. We also explore the application of the proposed model in the context of self-supervised learning framework. Our self-supervised model outperforms single phase-based existing works in this line of research. Finally, we show that self-supervised pre-training is effective for the semi-supervised scenario where we only have a small set of labelled training data, along with a larger set of unlabelled examples."
   ],
   "doi": "10.21437/Interspeech.2022-126"
  },
  "muller22b_interspeech": {
   "authors": [
    [
     "Nicolas",
     "Müller"
    ],
    [
     "Franziska",
     "Diekmann"
    ],
    [
     "Jennifer",
     "Williams"
    ]
   ],
   "title": "Attacker Attribution of Audio Deepfakes",
   "original": "129",
   "page_count": 5,
   "order": 566,
   "p1": 2788,
   "pn": 2792,
   "abstract": [
    "Deepfakes are synthetically generated media often devised with malicious intent. They have become increasingly more convincing with large training datasets advanced neural networks. These fakes are readily being misused for slander, misinformation and fraud. For this reason, intensive research for developing countermeasures is also expanding. However, recent work is almost exclusively limited to deepfake detection - predicting if audio is real or fake. This is despite the fact that attribution (who created which fake?) is an essential building block of a larger defense strategy, as practiced in the field of cybersecurity for a long time. This paper considers the problem of deepfake attacker attribution in the domain of audio. We present several methods for creating attacker signatures using low-level acoustic descriptors and machine learning embeddings. We show that speech signal features are inadequate for characterizing attacker signatures. However, we also demonstrate that embeddings from a recurrent neural network can successfully characterize attacks from both known and unknown attackers. Our attack signature embeddings result in distinct clusters, both for seen and unseen audio deepfakes. We show that these embeddings can be used in downstream-tasks to high-effect, scoring 97.10% accuracy in attacker-id classification."
   ],
   "doi": "10.21437/Interspeech.2022-129"
  },
  "chen22b_interspeech": {
   "authors": [
    [
     "Jie",
     "Chen"
    ],
    [
     "Changhe",
     "Song"
    ],
    [
     "Deyi",
     "Tuo"
    ],
    [
     "Xixin",
     "Wu"
    ],
    [
     "Shiyin",
     "Kang"
    ],
    [
     "Zhiyong",
     "Wu"
    ],
    [
     "Helen",
     "Meng"
    ]
   ],
   "title": "Improving Mandarin Prosodic Structure Prediction with Multi-level Contextual Information",
   "original": "131",
   "page_count": 5,
   "order": 86,
   "p1": 426,
   "pn": 430,
   "abstract": [
    "For text-to-speech (TTS) synthesis, prosodic structure prediction (PSP) plays an important role in producing natural and intelligible speech. Although inter-utterance linguistic information can influence the speech interpretation of the target utterance, previous works on PSP mainly focus on utilizing intrautterance linguistic information of the current utterance only. This work proposes to use inter-utterance linguistic information to improve the performance of PSP. Multi-level contextual information, which includes both inter-utterance and intrautterance linguistic information, is extracted by a hierarchical encoder from character level, utterance level and discourse level of the input text. Then a multi-task learning (MTL) decoder predicts prosodic boundaries from multi-level contextual information. Objective evaluation results on two datasets show that our method achieves better F1 scores in predicting prosodic word (PW), prosodic phrase (PPH) and intonational phrase (IPH). It demonstrates the effectiveness of using multi-level contextual information for PSP. Subjective preference tests also indicate the naturalness of synthesized speeches are improved."
   ],
   "doi": "10.21437/Interspeech.2022-131"
  },
  "goswami22_interspeech": {
   "authors": [
    [
     "Nabarun",
     "Goswami"
    ],
    [
     "Tatsuya",
     "Harada"
    ]
   ],
   "title": "SATTS: Speaker Attractor Text to Speech, Learning to Speak by Learning to Separate",
   "original": "133",
   "page_count": 5,
   "order": 245,
   "p1": 1203,
   "pn": 1207,
   "abstract": [
    "The mapping of text to speech (TTS) is non-deterministic, letters may be pronounced differently based on context, or phonemes can vary depending on various physiological and stylistic factors like gender, age, accent, emotions, etc. Neural speaker embeddings, trained to identify or verify speakers are typically used to represent and transfer such characteristics from reference speech to synthesized speech. Speech separation on the other hand is the challenging task of separating individual speakers from an overlapping mixed signal of various speakers. Speaker attractors are high-dimensional embedding vectors that pull the time-frequency bins of each speaker's speech towards themselves while repelling those belonging to other speakers. In this work, we explore the possibility of using these powerful speaker attractors for zero-shot speaker adaptation in multi-speaker TTS synthesis and propose speaker attractor text to speech (SATTS). Through various experiments, we show that SATTS can synthesize natural speech from text from an unseen target speaker's reference signal which might have less than ideal recording conditions, i.e. reverberations or mixed with other speakers."
   ],
   "doi": "10.21437/Interspeech.2022-133"
  },
  "vaessen22_interspeech": {
   "authors": [
    [
     "Nik",
     "Vaessen"
    ],
    [
     "David",
     "van Leeuwen"
    ]
   ],
   "title": "Training speaker recognition systems with limited data",
   "original": "135",
   "page_count": 5,
   "order": 964,
   "p1": 4760,
   "pn": 4764,
   "abstract": [
    "This work considers training neural networks for speaker recognition with a much smaller dataset size compared to contemporary work. We artificially restrict the amount of data by proposing three subsets of the popular VoxCeleb2 dataset. These subsets are restricted to 50\\,k audio files (versus over 1\\,M files available), and vary on the axis of number of speakers and session variability. We train three speaker recognition systems on these subsets; the X-vector, ECAPA-TDNN, and wav2vec2 network architectures. We show that the self-supervised, pre-trained weights of wav2vec2 substantially improve performance when training data is limited. Code and data subsets are available at \\url{https://github.com/nikvaessen/w2v2-speaker-few-samples}."
   ],
   "doi": "10.21437/Interspeech.2022-135"
  },
  "rajan22_interspeech": {
   "authors": [
    [
     "Rajeev",
     "Rajan"
    ],
    [
     "Ananya",
     "Ayasi"
    ]
   ],
   "title": "Oktoechos Classification in Liturgical Music Using SBU-LSTM/GRU",
   "original": "136",
   "page_count": 5,
   "order": 489,
   "p1": 2403,
   "pn": 2407,
   "abstract": [
    "A distinguishing feature of the music repertoire of the Syrian tradition is the system of classifying melodies into eight tunes, called 'oktoe\\={c}hos'. It inspired many traditions, such as Greek and Indian liturgical music. In oktoe\\={c}hos tradition, liturgical hymns are sung in eight modes or eight colours (known as eight 'niram', regionally). In this paper, the automatic oktoe\\={c}hos genre classification is addressed using musical texture features (MTF), i-vectors and Mel-spectrograms through stacked bidirectional and unidirectional long-short term memory (SBU-LSTM) and GRU (SB-GRU) architectures. The performance of the proposed approaches is evaluated using a newly created corpus of liturgical music in Malayalam. SBU-LSTM and SB-GRU frameworks report average classification accuracy of 88.19\\% and 87.50\\%, with a significant margin over other frameworks. The experiments demonstrate the potential of stacked architectures in learning temporal information from MTF for the proposed task."
   ],
   "doi": "10.21437/Interspeech.2022-136"
  },
  "kang22_interspeech": {
   "authors": [
    [
     "Woohyun",
     "Kang"
    ],
    [
     "Md Jahangir",
     "Alam"
    ],
    [
     "Abderrahim",
     "Fathan"
    ]
   ],
   "title": "End-to-end framework for spoof-aware speaker verification",
   "original": "139",
   "page_count": 5,
   "order": 884,
   "p1": 4362,
   "pn": 4366,
   "abstract": [
    "In this paper, we propose a novel end-to-end framework for training a spoof-aware speaker verification (SASV) system. To match the SASV scenario, where the test samples may be either spoof or genuine, we propose a novel contrastive objective and a modified mixup regularization strategy. The proposed end-to-end system and other SASV systems were evaluated on the ASVSpoof2019 LA evaluation set according to the SASV 2022 challenge rules. Our results showed that the proposed framework can learn complementary information to the conventional embedding fusion-based SASV systems. Using the proposed system in conjunction with the conventional embedding fusion systems has achieved a relative improvement of 61.07% in terms of SASV-EER compared to the best performing baseline result provided by the challenge organizers."
   ],
   "doi": "10.21437/Interspeech.2022-139"
  },
  "kang22b_interspeech": {
   "authors": [
    [
     "Woohyun",
     "Kang"
    ],
    [
     "Md Jahangir",
     "Alam"
    ],
    [
     "Abderrahim",
     "Fathan"
    ]
   ],
   "title": "Mixup regularization strategies for spoofing countermeasure system",
   "original": "140",
   "page_count": 5,
   "order": 757,
   "p1": 3734,
   "pn": 3738,
   "abstract": [
    "The main objective of the spoof detection system for speaker verification applications is to capture the artifacts from the given speech sample. Therefore, it is important to ensure that the countermeasure system generalizes well to spoof artifact patterns unobserved during the training process. To achieve this, we explore various possible adaptations of the mixup augmentation technique to the spoof detection system training, which involves mixing within-bonafide, within-spoof, and between bonafide and spoof samples. Moreover, we propose novel two-stage mixup strategies which are designed to increase the generalization power of the end-to-end spoof detection system to unseen attack types. The systems trained with different mixup configurations were experimented on the logical access (LA) task of the ASVSpoof2019 challenge dataset, and the proposed framework showed the best performance."
   ],
   "doi": "10.21437/Interspeech.2022-140"
  },
  "feng22_interspeech": {
   "authors": [
    [
     "Tiantian",
     "Feng"
    ],
    [
     "Shrikanth",
     "Narayanan"
    ]
   ],
   "title": "Semi-FedSER: Semi-supervised Learning for Speech Emotion Recognition On Federated Learning using Multiview Pseudo-Labeling",
   "original": "141",
   "page_count": 5,
   "order": 1022,
   "p1": 5050,
   "pn": 5054,
   "abstract": [
    "Speech Emotion Recognition (SER) application is frequently associated with privacy concerns as it often acquires and transmits speech data at the client-side to remote cloud platforms for further processing. These speech data can reveal not only speech content and affective information but the speaker's identity, demographic traits, and health status. Federated learning (FL) is a distributed machine learning algorithm that coordinates clients to train a model collaboratively without sharing local data. This algorithm shows enormous potential for SER applications as sharing raw speech or speech features from a user's device is vulnerable to privacy attacks. However, a major challenge in FL is limited availability of high-quality labeled data samples. In this work, we propose a semi-supervised federated learning framework, Semi-FedSER, that utilizes both labeled and unlabeled data samples to address the challenge of limited labeled data samples in FL. We show that our Semi-FedSER can generate desired SER performance even when the local label rate $l=20\\%$ using two SER benchmark datasets: IEMOCAP and MSP-Improv. The implementation of this work is at \\href{https://github.com/usc-sail/fed-ser-semi}{https://github.com/usc-sail/fed-ser-semi}."
   ],
   "doi": "10.21437/Interspeech.2022-141"
  },
  "kang22c_interspeech": {
   "authors": [
    [
     "Woohyun",
     "Kang"
    ],
    [
     "Md Jahangir",
     "Alam"
    ],
    [
     "Abderrahim",
     "Fathan"
    ]
   ],
   "title": "MIM-DG: Mutual information minimization-based domain generalization for speaker verification",
   "original": "142",
   "page_count": 5,
   "order": 745,
   "p1": 3674,
   "pn": 3678,
   "abstract": [
    "In the field of speaker verification, the current trend is to train a neural network-based speaker discriminative system and use the hidden representation as a speaker embedding vector. This framework have showed impressive performance in various speaker verification tasks, their performance is limited when it comes to mismatched conditions due to the variability within them unrelated to the speaker identity. In order to overcome this problem, we propose a novel training strategy that regularizes the embedding network to have minimum information about the nuisance attributes. More specifically, our proposed method aims to minimize the mutual information between the speaker embedding and the nuisance labels during the training process, where the mutual information is estimated using the statistics obtained via an auxiliary normalizing flow model. The proposed method is evaluated on cross-lingual and multi-genre speaker verification datasets, and the results show that the proposed strategy can effectively minimize the within-speaker variability on the embedding space."
   ],
   "doi": "10.21437/Interspeech.2022-142"
  },
  "babu22_interspeech": {
   "authors": [
    [
     "Arun",
     "Babu"
    ],
    [
     "Changhan",
     "Wang"
    ],
    [
     "Andros",
     "Tjandra"
    ],
    [
     "Kushal",
     "Lakhotia"
    ],
    [
     "Qiantong",
     "Xu"
    ],
    [
     "Naman",
     "Goyal"
    ],
    [
     "Kritika",
     "Singh"
    ],
    [
     "Patrick",
     "von Platen"
    ],
    [
     "Yatharth",
     "Saraf"
    ],
    [
     "Juan",
     "Pino"
    ],
    [
     "Alexei",
     "Baevski"
    ],
    [
     "Alexis",
     "Conneau"
    ],
    [
     "Michael",
     "Auli"
    ]
   ],
   "title": "XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale",
   "original": "143",
   "page_count": 5,
   "order": 464,
   "p1": 2278,
   "pn": 2282,
   "abstract": [
    "This paper presents XLS-R, a large-scale model for cross-lingual speech representation learning based on wav2vec 2.0. We train models with up to 2B parameters on nearly half a million hours of publicly available speech audio in 128 languages, an order of magnitude more public data than the largest known prior work. Our evaluation covers a wide range of tasks, domains, data regimes and languages, both high and low-resource. On the CoVoST-2 speech translation benchmark, we improve the previous state of the art by an average of 7.4 BLEU over 21 translation directions into English. For speech recognition, XLS-R improves over the best known prior work on BABEL and CommonVoice. XLS-R also sets a new state of the art on VoxLingua107 language identification. Moreover, we show that with sufficient model size, cross-lingual pretraining can perform as well as English-only pretraining when translating English speech into other languages, a setting which favors monolingual pretraining. We hope XLS-R can help to improve speech processing tasks for many more languages of the world."
   ],
   "doi": "10.21437/Interspeech.2022-143"
  },
  "wu22_interspeech": {
   "authors": [
    [
     "Ting-Wei",
     "Wu"
    ],
    [
     "I-Fan",
     "Chen"
    ],
    [
     "Ankur",
     "Gandhe"
    ]
   ],
   "title": "Learning to rank with BERT-based confidence models in ASR rescoring",
   "original": "145",
   "page_count": 5,
   "order": 335,
   "p1": 1651,
   "pn": 1655,
   "abstract": [
    "We propose a learning-to-rank (LTR) approach to the ASR rescoring problem. The proposed LTR framework has the flexibility of embracing wide varieties of linguistic, semantic, and implicit user feedback signals in rescoring process. BERT-based confidence models (CM) taking account of both acoustic and text information are also proposed to provide features better representing hypothesis quality to the LTR models. We show the knowledge of the entire N-best list is crucial for the confidence and LTR models to achieve best rescoring results. Experimental results on de-identified Alexa data show the proposed LTR framework provides an additional 5.16% relative word error rate reduction (WERR) on top of a neural language model rescored ASR system. On LibriSpeech, a 9.38 % WERR and a 13.63 % WERR are observed on the test-clean and test-other sets, respectively."
   ],
   "doi": "10.21437/Interspeech.2022-145"
  },
  "gessinger22_interspeech": {
   "authors": [
    [
     "Iona",
     "Gessinger"
    ],
    [
     "Michelle",
     "Cohn"
    ],
    [
     "Georgia",
     "Zellou"
    ],
    [
     "Bernd",
     "Möbius"
    ]
   ],
   "title": "Cross-Cultural Comparison of Gradient Emotion Perception: Human vs. Alexa TTS Voices",
   "original": "146",
   "page_count": 5,
   "order": 1006,
   "p1": 4970,
   "pn": 4974,
   "abstract": [
    "This study compares how American (US) and German (DE) listeners perceive emotional expressiveness from Amazon Alexa text-to-speech (TTS) and human voices. Participants heard identical stimuli, manipulated from an emotionally ‘neutral' production to three levels of increased happiness generated by resynthesis. Results show that, for both groups, ‘happiness' manipulations lead to higher ratings of emotional valence (i.e., more positive) for the human voice. Moreover, there was a difference across the groups in their perception of arousal (i.e., excitement): US listeners show higher ratings for human voices with manipulations, while DE listeners perceive the Alexa voice as sounding less ‘excited' overall. We discuss these findings in terms of theories of cross-cultural emotion perception and human-computer interaction."
   ],
   "doi": "10.21437/Interspeech.2022-146"
  },
  "lutati22_interspeech": {
   "authors": [
    [
     "Shahar",
     "Lutati"
    ],
    [
     "Eliya",
     "Nachmani"
    ],
    [
     "Lior",
     "Wolf"
    ]
   ],
   "title": "SepIt: Approaching a Single Channel Speech Separation Bound",
   "original": "149",
   "page_count": 5,
   "order": 1079,
   "p1": 5323,
   "pn": 5327,
   "abstract": [
    "We present an upper bound for the Single Channel Speech Separation task, which is based on an assumption regarding the nature of short segments of speech. Using the bound, we are able to show that while the recent methods have made great progress for a few speakers, there is room for improvement for five and ten speakers. We then introduce a Deep neural network, SepIt, that iteratively improves the different speakers' estimation. At test time, SpeIt has a varying number of iterations per test sample, based on a mutual information criterion that arises from our analysis. In an extensive set of experiments, SepIt outperforms the state of the art neural networks for 2, 3, 5, and 10 speakers."
   ],
   "doi": "10.21437/Interspeech.2022-149"
  },
  "ogayo22_interspeech": {
   "authors": [
    [
     "Perez",
     "Ogayo"
    ],
    [
     "Graham",
     "Neubig"
    ],
    [
     "Alan",
     "W Black"
    ]
   ],
   "title": "Building African Voices",
   "original": "152",
   "page_count": 5,
   "order": 257,
   "p1": 1263,
   "pn": 1267,
   "abstract": [
    "Modern speech synthesis techniques can produce natural-sounding speech given sufficient high-quality data and compute resources. However, such data is not readily available for many languages. This paper focuses on speech synthesis for low-resourced African languages, from corpus creation to sharing and deploying the Text-to-Speech (TTS) systems. We first create a set of general-purpose instructions on building speech synthesis systems with minimum technological resources and subject-matter expertise. Next, we create new datasets and curate datasets from \"found\" data (existing recordings) through a participatory approach while considering accessibility, quality, and breadth. We demonstrate that we can develop synthesizers that generate intelligible speech with 25 minutes of created speech, even when recorded in suboptimal environments. Finally, we release the speech data, code, and trained voices for 12 African languages to support researchers and developers."
   ],
   "doi": "10.21437/Interspeech.2022-152"
  },
  "chen22c_interspeech": {
   "authors": [
    [
     "Zhuangqi",
     "Chen"
    ],
    [
     "Pingjian",
     "Zhang"
    ]
   ],
   "title": "Lightweight Full-band and Sub-band Fusion Network for Real Time Speech Enhancement",
   "original": "153",
   "page_count": 5,
   "order": 188,
   "p1": 921,
   "pn": 925,
   "abstract": [
    "Recent studies in deep learning based real-time speech enhancement have proven the advantage of sub-band processing in parameter reduction. However, most sub-band based methods utilize the same model for all sub-bands, which limits the upper bound of performance, giving the fact that the spectral patterns in each sub-band are different. In this paper, we take into account this fact and propose a lightweight full-band and sub-band fusion network, where dual-branch based architecture is employed for modeling local and global spectral pattern simultaneously. A simple yet effective sub-band module, the weighted progressive convolutional module, is designed with a small number of parameters, which captures clean features progressively from local perspective. Each sub-band is handled by one module. A novel asymmetric convolutional recurrent network is also proposed to focus on full-band context and extract more robust global features, which is complementary to the sub-band module. We have conducted extensive experiments on both the VoiceBank+Demand and the DNS Challenge datasets, and the experimental results show that our proposed method has achieved superior performance to other state-of-the-art approaches with smaller model size and lower latency."
   ],
   "doi": "10.21437/Interspeech.2022-153"
  },
  "li22b_interspeech": {
   "authors": [
    [
     "Nan",
     "LI"
    ],
    [
     "Meng",
     "Ge"
    ],
    [
     "Longbiao",
     "Wang"
    ],
    [
     "Masashi",
     "Unoki"
    ],
    [
     "Sheng",
     "Li"
    ],
    [
     "Jianwu",
     "Dang"
    ]
   ],
   "title": "Global Signal-to-noise Ratio Estimation Based on Multi-subband Processing Using Convolutional Neural Network",
   "original": "154",
   "page_count": 5,
   "order": 73,
   "p1": 361,
   "pn": 365,
   "abstract": [
    "The global signal-to-noise ratio (gSNR) is defined as the ratio of speech energy to noise energy in whole noisy audio. However, due to the increase in noise interference, the generalization ability declines when the traditional features (e.g., raw waveforms and MFCCs) are fed directly to the statistical model to estimate a single fullband gSNR. In this paper, we propose a multi-subband-based gSNR estimation network (MSGNet). Specifically, we split the noisy speech waveforms into Bark-scale subbands to obtain higher resolution signals to the middle and low frequencies. Then, convolutional neural networks (CNNs) are used to learn a non-linear function to estimate the speech and noise energy ratio of each subband from the input muti-subband features. Finally, by integrating subbands with different speech and noise energies, gSNR in the fullband is calculated. Extensive experimental results on the AURORA-2J dataset demonstrate that the proposed MSGNet significantly reduces the mean absolute error compared to other baseline gSNR estimation methods."
   ],
   "doi": "10.21437/Interspeech.2022-154"
  },
  "kim22b_interspeech": {
   "authors": [
    [
     "Ju-Ho",
     "Kim"
    ],
    [
     "Jungwoo",
     "Heo"
    ],
    [
     "Hye-jin",
     "Shim"
    ],
    [
     "Ha-Jin",
     "Yu"
    ]
   ],
   "title": "Extended U-Net for Speaker Verification in Noisy Environments",
   "original": "155",
   "page_count": 5,
   "order": 119,
   "p1": 590,
   "pn": 594,
   "abstract": [
    "Background noise is a well-known factor that deteriorates the accuracy and reliability of speaker verification (SV) systems by blurring speech intelligibility. Various studies have used separate pretrained enhancement models as the front-end module of the SV system in noisy environments, and these methods effectively remove noises. However, the denoising process of independent enhancement models not tailored to the SV task can also distort the speaker information included in utterances. We argue that the enhancement network and speaker embedding extractor should be fully jointly trained for SV tasks under noisy conditions to alleviate this issue. Therefore, we proposed a U-Net-based integrated framework that simultaneously optimizes speaker identification and feature enhancement losses. Moreover, we analyzed the structural limitations of using U-Net directly for noise SV tasks and further proposed Extended U-Net to reduce these drawbacks. We evaluated the models on the noise-synthesized VoxCeleb1 test set and VOiCES development set recorded in various noisy scenarios. The experimental results demonstrate that the U-Net-based fully joint training framework is more effective than the baseline, and the extended U-Net exhibited state-of-the-art performance versus the recently proposed compensation systems."
   ],
   "doi": "10.21437/Interspeech.2022-155"
  },
  "pan22b_interspeech": {
   "authors": [
    [
     "Zexu",
     "Pan"
    ],
    [
     "Meng",
     "Ge"
    ],
    [
     "Haizhou",
     "Li"
    ]
   ],
   "title": "A Hybrid Continuity Loss to Reduce Over-Suppression for Time-domain Target Speaker Extraction",
   "original": "157",
   "page_count": 5,
   "order": 362,
   "p1": 1786,
   "pn": 1790,
   "abstract": [
    "The speaker extraction algorithm extracts the target speech from a mixture speech containing interference speech and background noise. The extraction process sometimes over-suppresses the extracted target speech, which not only creates artifacts during listening but also harms the performance of downstream automatic speech recognition algorithms. We propose a hybrid continuity loss function for time-domain speaker extraction algorithms to settle the over-suppression problem. On top of the waveform-level loss used for superior signal quality, i.e., SI-SDR, we introduce a multi-resolution delta spectrum loss in the frequency-domain, to ensure the continuity of an extracted speech signal, thus alleviating the over-suppression. We examine the hybrid continuity loss function using a time-domain audio-visual speaker extraction algorithm on the YouTube LRS2-BBC dataset. Experimental results show that the proposed loss function reduces the over-suppression and improves the word error rate of speech recognition on both clean and noisy two-speakers mixtures, without harming the reconstructed speech quality."
   ],
   "doi": "10.21437/Interspeech.2022-157"
  },
  "li22c_interspeech": {
   "authors": [
    [
     "Andong",
     "Li"
    ],
    [
     "Guochen",
     "Yu"
    ],
    [
     "Chengshi",
     "Zheng"
    ],
    [
     "Xiaodong",
     "Li"
    ]
   ],
   "title": "TaylorBeamformer: Learning All-Neural Beamformer for Multi-Channel Speech Enhancement from Taylor’s Approximation Theory",
   "original": "159",
   "page_count": 5,
   "order": 1097,
   "p1": 5413,
   "pn": 5417,
   "abstract": [
    "While existing end-to-end beamformers achieve impressive performance in various front-end speech processing tasks, they usually encapsulate the whole process into a black box and thus lack adequate interpretability. As an attempt to fill the blank, we propose a novel neural beamformer inspired by Taylor's approximation theory called TaylorBeamformer for multi-channel speech enhancement. The core idea is that the recovery process can be formulated as the spatial filtering in the neighborhood of the input mixture. Based on that, we decompose it into the superimposition of the 0th-order non-derivative and high-order derivative terms, where the former serves as the spatial filter and the latter is viewed as the residual noise canceller to further improve the speech quality. To enable end-to-end training, we replace the derivative operations with trainable networks and thus can learn from training data. Extensive experiments are conducted on the synthesized dataset based on LibriSpeech and results show that the proposed approach performs favorably against the previous advanced baselines."
   ],
   "doi": "10.21437/Interspeech.2022-159"
  },
  "yang22b_interspeech": {
   "authors": [
    [
     "Da-Hee",
     "Yang"
    ],
    [
     "Joon-Hyuk",
     "Chang"
    ]
   ],
   "title": "FiLM Conditioning with Enhanced Feature to the Transformer-based End-to-End Noisy Speech Recognition",
   "original": "161",
   "page_count": 5,
   "order": 831,
   "p1": 4098,
   "pn": 4102,
   "abstract": [
    "Ensuring robustness against environmental noise is an important concern in the design of automatic speech recognition (ASR) systems. This is typically achieved by utilizing a speech enhancement (SE) network in an ASR system to boost noise robustness. The performance of ASR systems can be improved using SE networks as a front-end or by retraining the ASR system on enhanced speech. Although the SE network is effective, it does not always result in improved performance in the ASR system owing to artifacts. To address this problem, we propose the use of enhanced speech from an SE network as a conditioning feature instead of a direct input feature of the ASR system. This is achieved by stacking a feature-wise linear modulation (FiLM) layer on each transformer layer of the end-to-end ASR encoder and combining the input and conditioning features. The results indicate that the proposed FiLM training method exhibits greater robustness against noise owing to the use of enhanced speech as conditioning information rather than as direct ASR input."
   ],
   "doi": "10.21437/Interspeech.2022-161"
  },
  "tesch22_interspeech": {
   "authors": [
    [
     "Kristina",
     "Tesch"
    ],
    [
     "Nils-Hendrik",
     "Mohrmann"
    ],
    [
     "Timo",
     "Gerkmann"
    ]
   ],
   "title": "On the Role of Spatial, Spectral, and Temporal Processing for DNN-based Non-linear Multi-channel Speech Enhancement",
   "original": "162",
   "page_count": 5,
   "order": 590,
   "p1": 2908,
   "pn": 2912,
   "abstract": [
    "Employing deep neural networks (DNNs) to directly learn filters for multi-channel speech enhancement has potentially two key advantages over a traditional approach combining a linear spatial filter with an independent tempo-spectral post-filter: 1) non-linear spatial filtering allows to overcome potential restrictions originating from a linear processing model and 2) joint processing of spatial and tempo-spectral information allows to exploit interdependencies between different sources of information. A variety of DNN-based non-linear filters have been proposed recently, for which good enhancement performance is reported. However, little is known about the internal mechanisms which turns network architecture design into a game of chance. Therefore, in this paper, we perform experiments to better understand the internal processing of spatial, spectral and temporal information by DNN-based non-linear filters. On the one hand, our experiments in a difficult speech extraction scenario confirm the importance of non-linear spatial filtering, which outperforms an oracle linear spatial filter by 0.24 POLQA score. On the other hand, we demonstrate that joint processing results in a large performance gap of 0.4 POLQA score between network architectures exploiting spectral versus temporal information besides spatial information."
   ],
   "doi": "10.21437/Interspeech.2022-162"
  },
  "yue22_interspeech": {
   "authors": [
    [
     "Zhengjun",
     "Yue"
    ],
    [
     "Erfan",
     "Loweimi"
    ],
    [
     "Heidi",
     "Christensen"
    ],
    [
     "Jon",
     "Barker"
    ],
    [
     "Zoran",
     "Cvetkovic"
    ]
   ],
   "title": "Dysarthric Speech Recognition From Raw Waveform with Parametric CNNs",
   "original": "163",
   "page_count": 5,
   "order": 7,
   "p1": 31,
   "pn": 35,
   "abstract": [
    "Raw waveform acoustic modelling has recently received increasing attention. Compared with the task-blind hand-crafted features which may discard useful information, representations directly learned from the raw waveform are task-specific and potentially include all task-relevant information. In the context of automatic dysarthric speech recognition (ADSR), raw waveform acoustic modelling is under-explored owing to data scarcity. Parametric convolutional neural networks (CNNs) can compensate for this problem due to having notably fewer parameters and requiring less training data in comparison with conventional non-parametric CNNs. In this paper, we explore the usefulness of raw waveform acoustic modelling using various parametric CNNs for ADSR. We investigate the properties of the learned filters and monitor the training dynamics of various models. Furthermore, we study the effectiveness of data augmentation and multi-stream acoustic modelling through combining the non-parametric and parametric CNNs fed by hand-crafted and raw waveform features. Experimental results on the TORGO dysarthric database show that the parametric CNNs significantly outperform the non-parametric CNNs, reaching up to 36.2% and 12.6% WERs (up to 3.4% and 1.1% absolute error reduction) for dysarthric and typical speech, respectively. Multi-stream acoustic modelling further improves the performance resulting in up to 33.2% and 10.3% WERs for dysarthric and typical speech, respectively."
   ],
   "doi": "10.21437/Interspeech.2022-163"
  },
  "zuo22_interspeech": {
   "authors": [
    [
     "Ronglai",
     "Zuo"
    ],
    [
     "Brian",
     "Mak"
    ]
   ],
   "title": "Local Context-aware Self-attention for Continuous Sign Language Recognition",
   "original": "164",
   "page_count": 5,
   "order": 974,
   "p1": 4810,
   "pn": 4814,
   "abstract": [
    "Transformer-based architectures are adopted in many continuous sign language recognition (CSLR) works for sequence modeling due to their strong capability of extracting global contexts. However, since vanilla self-attention (SA), the core module of Transformer, computes a weighted average over all time steps, the local temporal semantics of sign videos may not be fully exploited. In this work, we propose local context-aware self-attention (LCSA) to enhance the vanilla SA to leverage both local and global contexts. We introduce the local contexts at two different levels of model computation: score and query levels. At the score level, we modulate the attention scores explicitly with an additional Gaussian bias. At the query level, local contexts are modeled implicitly using depth-wise temporal convolutional networks (DTCNs). However, the vanilla Gaussian bias has two major shortcomings: first, its window size is fixed and needs to be fine-tuned laboriously; second, the fixed window size is common among all time steps. In this work, a dynamic Gaussian bias is further proposed to address the above issues. Experimental results on two benchmarks, PHOENIX-2014 and CSL, validate the effectiveness and superiority of our method."
   ],
   "doi": "10.21437/Interspeech.2022-164"
  },
  "fukuda22_interspeech": {
   "authors": [
    [
     "Takashi",
     "Fukuda"
    ],
    [
     "Samuel",
     "Thomas"
    ],
    [
     "Masayuki",
     "Suzuki"
    ],
    [
     "Gakuto",
     "Kurata"
    ],
    [
     "George",
     "Saon"
    ],
    [
     "Brian",
     "Kingsbury"
    ]
   ],
   "title": "Global RNN Transducer Models For Multi-dialect Speech Recognition",
   "original": "165",
   "page_count": 5,
   "order": 636,
   "p1": 3138,
   "pn": 3142,
   "abstract": [
    "Constructing single, unified automatic speech recognition (ASR) models that work effectively across various dialects of a language is a challenging problem. Although many recently proposed approaches are effective, they are computationally more expensive compared to the conventional approach of using ASR models designed separately for each dialect. In this paper, we propose a novel modeling technique for constructing accurate, multi-dialect, speech recognition systems with a single unified model, based on recurrent neural network transducers (RNN-T), which does not incur any extra computational costs at decoding time. Once a model has been created, the same decoding settings can also be used across all dialects. In our proposed approach an RNN-T model with a shared encoder, common joint network and multi-branch prediction networks is first constructed. After training each prediction network on an ASR task corresponding to various dialects, an effective interpolation step combines the multi-branch prediction networks back into a computationally-efficient single branch. The effectiveness of the proposed technique is shown on ASR tasks on major English dialects. The proposed method approaches oracle performance and improves by 15-30% relative over dialect-specific models in dialect agnostic conditions."
   ],
   "doi": "10.21437/Interspeech.2022-165"
  },
  "li22d_interspeech": {
   "authors": [
    [
     "Kai",
     "Li"
    ],
    [
     "Xiaolin",
     "Hu"
    ],
    [
     "Yi",
     "Luo"
    ]
   ],
   "title": "On the Use of Deep Mask Estimation Module for Neural Source Separation Systems",
   "original": "174",
   "page_count": 5,
   "order": 1080,
   "p1": 5328,
   "pn": 5332,
   "abstract": [
    "Most of the recent neural source separation systems rely on a masking-based pipeline where a set of multiplicative masks are estimated from and applied to a signal representation of the input mixture. The estimation of such masks, in almost all network architectures, is done by a single layer followed by an optional nonlinear activation function. However, recent literatures have investigated the use of a deep mask estimation module and observed performance improvement compared to a shallow mask estimation module. In this paper, we analyze the role of such deeper mask estimation module by connecting it to a recently proposed unsupervised source separation method, and empirically show that the deep mask estimation module is an efficient approximation of the so-called overseparation-grouping paradigm with the conventional shallow mask estimation layers."
   ],
   "doi": "10.21437/Interspeech.2022-174"
  },
  "zhao22b_interspeech": {
   "authors": [
    [
     "Zifeng",
     "Zhao"
    ],
    [
     "Dongchao",
     "Yang"
    ],
    [
     "Rongzhi",
     "Gu"
    ],
    [
     "Haoran",
     "Zhang"
    ],
    [
     "Yuexian",
     "Zou"
    ]
   ],
   "title": "Target Confusion in End-to-end Speaker Extraction: Analysis and Approaches",
   "original": "176",
   "page_count": 5,
   "order": 1081,
   "p1": 5333,
   "pn": 5337,
   "abstract": [
    "Recently, end-to-end speaker extraction has attracted increasing attention and shown promising results. However, its performance is often inferior to that of a blind speech separation (BSS) counterpart with a similar network architecture, due to the auxiliary speaker encoder may sometimes generate ambiguous speaker embeddings. Such ambiguous guidance information may confuse the separation network and hence lead to wrong extraction results, which deteriorates the overall performance. We refer to this as the target confusion problem. In this paper, we conduct an analysis of such an issue and solve it in two stages. In the training phase, we propose to integrate metric learning methods to improve the distinguishability of embeddings produced by the speaker encoder. While for inference, a novel post-filtering strategy is designed to revise wrong results. Specifically, we first identify these confusion samples by measuring the similarities between output estimates and enrollment utterances, after which the true target sources are recovered by a subtraction operation. Experiments show that performance improvement of more than 1 dB SI-SDRi can be brought, which validates the effectiveness of our methods and emphasizes the impact of the target confusion problem."
   ],
   "doi": "10.21437/Interspeech.2022-176"
  },
  "quintas22_interspeech": {
   "authors": [
    [
     "Sebastião",
     "Quintas"
    ],
    [
     "Julie",
     "Mauclair"
    ],
    [
     "Virginie",
     "Woisard"
    ],
    [
     "Julien",
     "Pinquier"
    ]
   ],
   "title": "Automatic Assessment of Speech Intelligibility using Consonant Similarity for Head and Neck Cancer",
   "original": "182",
   "page_count": 5,
   "order": 730,
   "p1": 3608,
   "pn": 3612,
   "abstract": [
    "The automatic prediction of speech intelligibility is a widely known problem in the context of pathological speech. It has been seen as a growing and viable alternative to perceptual evaluation, which is typically time-consuming, highly subjective and strongly biased. Due to this, the development of automatic systems that are able to output not only unbiased predictions,but also interpretable scores become relevant. In this paper we investigate a method to predict speech intelligibility based on consonant phonetic similarity. The proposed methodology re-lies on a siamese network to compute similarity scores between healthy and pathological phonemes, and based on the combination of those scores, regresses the intelligibility values. Our experimental evaluation suggests a high baseline correlation value of p= 0.82, when applied to our corpus of head and neck cancer. Moreover, further conditioning of the system on specific phonemes in key contexts increased the correlation up to p= 0.89. The given methodology also aims to promote interpretability of the predicted intelligibility score, which is highly relevant in a clinical setting."
   ],
   "doi": "10.21437/Interspeech.2022-182"
  },
  "agaskar22_interspeech": {
   "authors": [
    [
     "Ameya",
     "Agaskar"
    ]
   ],
   "title": "Practical Over-the-air Perceptual AcousticWatermarking",
   "original": "183",
   "page_count": 5,
   "order": 144,
   "p1": 714,
   "pn": 718,
   "abstract": [
    "In this work, we demonstrate a novel technique for automatically scaling over-the-air acoustic watermarks to maximize amplitude while remaining imperceptible to human listeners. These watermarks have been demonstrated in prior work to be robust to the indoor acoustic channel. However, they require careful calibration to ensure that they are (a) detectable by the device and (b) imperceptible to humans. While previously this was done using listening tests, we show that psychoacoustic masking curves can be used to automatically scale each watermark frame's amplitude to be as high as possible while remaining below the masking level. This maximizes watermark detectability by the self-correlation decoder described in earlier work, while ensuring that the watermark is not heard."
   ],
   "doi": "10.21437/Interspeech.2022-183"
  },
  "kaland22_interspeech": {
   "authors": [
    [
     "Constantijn",
     "Kaland"
    ]
   ],
   "title": "Bending the string: intonation contour length as a correlate of macro-rhythm",
   "original": "185",
   "page_count": 5,
   "order": 1061,
   "p1": 5233,
   "pn": 5237,
   "abstract": [
    "Macro-rhythm was introduced as a means to distinguish typologically different languages according to their overall tonal rhythmicity at the phrase level. The concept of macro-rhythm has been established theoretically, although little work has attempted to quantify it acoustically and led to mixed results. More research is needed to obtain an acoustic approximation of macro-rhythm using more diverse types of speech from different languages. This is done in the current paper by investigating phrases taken from TED talks in Greek, German and Portuguese, languages that are assumed to differ in their degree of macro-rhythm. The current approach takes the length of a stylized and speaker-corrected intonation contour as a core measure of macro-rhythm. Results show that this method indeed ranks the three languages in the expected way, although more work capturing tonal regularity is needed to improve the outcomes."
   ],
   "doi": "10.21437/Interspeech.2022-185"
  },
  "prananta22_interspeech": {
   "authors": [
    [
     "Luke",
     "Prananta"
    ],
    [
     "Bence",
     "Halpern"
    ],
    [
     "Siyuan",
     "Feng"
    ],
    [
     "Odette",
     "Scharenborg"
    ]
   ],
   "title": "The Effectiveness of Time Stretching for Enhancing Dysarthric Speech for Improved Dysarthric Speech Recognition",
   "original": "190",
   "page_count": 5,
   "order": 8,
   "p1": 36,
   "pn": 40,
   "abstract": [
    "In this paper, we investigate several existing and a new state-of-the-art generative adversarial network-based (GAN) voice conversion method for enhancing dysarthric speech for improved dysarthric speech recognition. We compare key components of existing methods as part of a rigorous ablation study to find the most effective solution to improve dysarthric speech recognition. We find that straightforward signal processing methods such as stationary noise removal and vocoder-based time stretching lead to dysarthric speech recognition results comparable to those obtained when using state-of-the-art GAN-based voice conversion methods as measured using a phoneme recognition task. Additionally, our proposed solution of a combination of MaskCycleGAN-VC and time stretching is able to improve the phoneme recognition results for certain dysarthric speakers compared to our time stretched baseline."
   ],
   "doi": "10.21437/Interspeech.2022-190"
  },
  "vangysel22_interspeech": {
   "authors": [
    [
     "Christophe",
     "Van Gysel"
    ],
    [
     "Mirko",
     "Hannemann"
    ],
    [
     "Ernest",
     "Pusateri"
    ],
    [
     "Youssef",
     "Oualil"
    ],
    [
     "Ilya",
     "Oparin"
    ]
   ],
   "title": "Space-Efficient Representation of Entity-centric Query Language Models",
   "original": "193",
   "page_count": 5,
   "order": 137,
   "p1": 679,
   "pn": 683,
   "abstract": [
    "Virtual assistants make use of automatic speech recognition (ASR) to help users answer entity-centric queries. However, spoken entity recognition is a difficult problem, due to the large number of frequently-changing named entities. In addition, resources available for recognition are constrained when ASR is performed on-device. In this work, we investigate the use of probabilistic grammars as language models within the finite-state transducer (FST) framework. We introduce a deterministic approximation to probabilistic grammars that avoids the explicit expansion of non-terminals at model creation time, integrates directly with the FST framework, and is complementary to n-gram models. We obtain a 10% relative word error rate improvement on long tail entity queries compared to when a similarly-sized n-gram model is used without our method."
   ],
   "doi": "10.21437/Interspeech.2022-193"
  },
  "borsos22_interspeech": {
   "authors": [
    [
     "Zalan",
     "Borsos"
    ],
    [
     "Matthew",
     "Sharifi"
    ],
    [
     "Marco",
     "Tagliasacchi"
    ]
   ],
   "title": "SpeechPainter: Text-conditioned Speech Inpainting",
   "original": "194",
   "page_count": 5,
   "order": 87,
   "p1": 431,
   "pn": 435,
   "abstract": [
    "We propose SpeechPainter, a model for filling in gaps of up to one second in speech samples by leveraging an auxiliary textual input. We demonstrate that the model performs speech inpainting with the appropriate content, while maintaining speaker identity, prosody and recording environment conditions, and generalizing to unseen speakers. Our approach significantly outperforms baselines constructed using adaptive TTS, as judged by human raters in side-by-side preference and MOS tests."
   ],
   "doi": "10.21437/Interspeech.2022-194"
  },
  "hou22_interspeech": {
   "authors": [
    [
     "Yuanbo",
     "Hou"
    ],
    [
     "Zhaoyi",
     "Liu"
    ],
    [
     "Bo",
     "Kang"
    ],
    [
     "Yun",
     "Wang"
    ],
    [
     "Dick",
     "Botteldooren"
    ]
   ],
   "title": "CT-SAT: Contextual Transformer for Sequential Audio Tagging",
   "original": "196",
   "page_count": 5,
   "order": 841,
   "p1": 4147,
   "pn": 4151,
   "abstract": [
    "Sequential audio event tagging can provide not only the type information of audio events, but also the order information between events and the number of events that occur in an audio clip. Most previous works on audio event sequence analysis rely on connectionist temporal classification (CTC). However, CTC's conditional independence assumption prevents it from effectively learning correlations between diverse audio events. This paper first introduces the Transformer into sequential audio tagging, since Transformers perform well in sequence-related tasks. To better utilize contextual information of audio event sequences, we draw on the idea of bidirectional recurrent neural networks, and propose a contextual Transformer (cTransformer) with a bidirectional decoder that could exploit the forward and backward information of event sequences. Experiments on the real-life polyphonic audio dataset show that, compared to CTC-based methods, the cTransformer can effectively combine the fine-grained acoustic representations from the encoder and coarse-grained audio event cues to exploit contextual information to successfully recognize and predict the audio event sequence in polyphonic audio clips."
   ],
   "doi": "10.21437/Interspeech.2022-196"
  },
  "bernhard22_interspeech": {
   "authors": [
    [
     "Vera",
     "Bernhard"
    ],
    [
     "Sandra",
     "Schwab"
    ],
    [
     "Jean-Philippe",
     "Goldman"
    ]
   ],
   "title": "Acoustic Stress Detection in Isolated English Words for Computer-Assisted Pronunciation Training",
   "original": "197",
   "page_count": 5,
   "order": 637,
   "p1": 3143,
   "pn": 3147,
   "abstract": [
    "We propose a system for automatic lexical stress detection in isolated English words. It is designed to be part of the computer-assisted pronunciation training application MIAPARLE (miaparle.unige.ch) that specifically focuses on stress contrasts acquisition. Training lexical stress cannot be disregarded in language education as the accuracy in production highly affects the intelligibility and perceived fluency of an L2 speaker. The pipeline automatically segments audio input into syllables over which duration, intensity, pitch, and spectral information is calculated. Since the stress of a syllable is defined relative to its neighboring syllables, the values obtained over the syllables are complemented with differential values to the preceding and following syllables. The resulting feature vectors, retrieved from 1011 recordings of single words spoken by English natives, are used to train a Voting Classifier composed of four supervised classifiers, namely a Support Vector Machine, a Neural Net, a K Nearest Neighbor, and a Random Forest classifier. The approach determines syllables of a single word as stressed or unstressed with an F1 score of 94% and an accuracy of 96%."
   ],
   "doi": "10.21437/Interspeech.2022-197"
  },
  "uchida22_interspeech": {
   "authors": [
    [
     "Naokazu",
     "Uchida"
    ],
    [
     "Takeshi",
     "Homma"
    ],
    [
     "Makoto",
     "Iwayama"
    ],
    [
     "Yasuhiro",
     "Sogawa"
    ]
   ],
   "title": "Reducing Offensive Replies in Open Domain Dialogue Systems",
   "original": "200",
   "page_count": 5,
   "order": 219,
   "p1": 1076,
   "pn": 1080,
   "abstract": [
    "In recent years, a series of open-domain dialogue systems using large-scale language models have been proposed. These dialogue systems are attracting business attention because these do significantly natural and diverse dialogues with humans. However, it has been noted that these dialogue systems reflect gender, race, and other biases inherent in the data and may generate offensive replies or replies that agree with offensive utterances. This study examined a dialogue system that outputs appropriate replies to offensive utterances. Specifically, our system incorporates multiple dialogue models, each of which is specialized to suppress offensive replies in a specific category, then selects the most non-offensive reply from the outputs of the models. We evaluated the utility of our system when suppressing offensive replies of DialoGPT. We confirmed ours reduces the offensive replies to less than 1%, whereas one of the state-of-the-art suppressing methods reduces to 9.8%."
   ],
   "doi": "10.21437/Interspeech.2022-200"
  },
  "harvill22_interspeech": {
   "authors": [
    [
     "John",
     "Harvill"
    ],
    [
     "Mark",
     "Hasegawa-Johnson"
    ],
    [
     "Chang D.",
     "Yoo"
    ]
   ],
   "title": "Frame-Level Stutter Detection",
   "original": "204",
   "page_count": 5,
   "order": 577,
   "p1": 2843,
   "pn": 2847,
   "abstract": [
    "Previous studies on the detection of stuttered speech have focused on classification at the utterance level (e.g., for speech therapy applications), and on the correct insertion of stutter events in sequence into an orthographic transcript. In this paper, we propose the task of frame-level stutter detection which seeks to identify the time alignment of stutter events in a speech utterance, and we evaluate our approach on the stutter correction task. Limited previous work on stutter correction has relied on simple signal processing techniques and only been evaluated on small datasets. Our approach is the first large scale data-driven technique proposed to identify stuttering probabilistically at the frame level, and we make use of the largest available stuttering dataset to date during training. Predicted frame-level probabilities of different stuttering events can be used in downstream applications for Automatic Speech Recognition (ASR) as either additional features or part of a speech preprocessing pipeline to clean speech before analysis by an ASR system."
   ],
   "doi": "10.21437/Interspeech.2022-204"
  },
  "zhan22_interspeech": {
   "authors": [
    [
     "Haoyue",
     "Zhan"
    ],
    [
     "Xinyuan",
     "YU"
    ],
    [
     "Haitong",
     "Zhang"
    ],
    [
     "Yang",
     "Zhang"
    ],
    [
     "Yue",
     "Lin"
    ]
   ],
   "title": "Exploring Timbre Disentanglement in Non-Autoregressive Cross-Lingual Text-to-Speech",
   "original": "205",
   "page_count": 5,
   "order": 861,
   "p1": 4247,
   "pn": 4251,
   "abstract": [
    "In this paper, we study the disentanglement of speaker and language representations in non-autoregressive cross-lingual TTS models from various aspects. We propose a phoneme length regulator that solves the length mismatch problem between IPA input sequence and monolingual alignment results. Using the phoneme length regulator, we present a FastPitch-based cross-lingual model with IPA symbols as input representations. Our experiments show that language-independent input representations (e.g. IPA symbols), an increasing number of training speakers, and explicit modeling of speech variance information all encourage non-autoregressive cross-lingual TTS model to disentangle speaker and language representations. The subjective evaluation shows that our proposed model can achieve decent naturalness and speaker similarity in cross-language voice cloning."
   ],
   "doi": "10.21437/Interspeech.2022-205"
  },
  "yang22c_interspeech": {
   "authors": [
    [
     "Longfei",
     "Yang"
    ],
    [
     "Jinsong",
     "Zhang"
    ],
    [
     "Takahiro",
     "Shinozaki"
    ]
   ],
   "title": "Self-Supervised Learning with Multi-Target Contrastive Coding for Non-Native Acoustic Modeling of Mispronunciation Verification",
   "original": "207",
   "page_count": 5,
   "order": 874,
   "p1": 4312,
   "pn": 4316,
   "abstract": [
    "Non-native mispronunciation verification is an important component in computer-aided language learning (CALL) systems. However, the data sparsity problem makes it difficult to establish an accurate acoustic model directly on non-native data with supervised approaches since it is impractical to collect and manually label a large amount of non-native speech data. In this paper, we propose a pre-training approach based on self-supervised learning with multi-target contrastive coding utilizing plenty of raw resources of two native languages for non-native acoustic modeling of mispronunciation verification. In our work, the model is designed to learn the representations of discrepancy with respect to phonetic structures in and across different languages, and speakers by making predictions that are contrastive to different targets. In addition, an additional term is incorporated as a regularization term by reconstructing the original speech from the shared components. Through the experiments on the Japanese part of the BLCU inter-Chinese speech corpus, results show that our proposed approaches are effective to improve the performance for the non-native acoustic modeling of phone recognition and mispronunciation verification."
   ],
   "doi": "10.21437/Interspeech.2022-207"
  },
  "zhang22_interspeech": {
   "authors": [
    [
     "Daniel",
     "Zhang"
    ],
    [
     "Ashwinkumar",
     "Ganesan"
    ],
    [
     "Sarah",
     "Campbell"
    ],
    [
     "Daniel",
     "Korzekwa"
    ]
   ],
   "title": "L2-GEN: A Neural Phoneme Paraphrasing Approach to L2 Speech Synthesis for Mispronunciation Diagnosis",
   "original": "209",
   "page_count": 5,
   "order": 875,
   "p1": 4317,
   "pn": 4321,
   "abstract": [
    "In this paper, we study the problem of generating mispronounced speech mimicking non-native (L2) speakers learning English as a Second Language (ESL) for the mispronunciation detection and diagnosis (MDD) task. The paper is motivated by the widely observed yet not well addressed data sparsity issue in MDD research where both L2 speech audio and its fine-grained phonetic annotations are difficult to obtain, leading to unsatisfactory mispronunciation feedback accuracy. We propose L2-GEN, a new data augmentation framework to generate L2 phoneme sequences that capture realistic mispronunciation patterns by devising an unique machine translation-based sequence paraphrasing model. A novel diversified and preference-aware decoding algorithm is proposed to generalize L2-GEN to handle both unseen words and new learner population with very limited L2 training data. A contrastive augmentation technique is further designed to optimize MDD performance improvements with the generated synthetic L2 data. We evaluate L2-GEN on public L2-ARCTIC and SpeechOcean762 datasets. The results have shown that L2-GEN leads to up to 3.9%, and 5.0% MDD F1-score improvements in in-domain and out-of-domain scenarios respectively."
   ],
   "doi": "10.21437/Interspeech.2022-209"
  },
  "choi22b_interspeech": {
   "authors": [
    [
     "Jeong-Hwan",
     "Choi"
    ],
    [
     "Joon-Young",
     "Yang"
    ],
    [
     "Ye-Rin",
     "Jeoung"
    ],
    [
     "Joon-Hyuk",
     "Chang"
    ]
   ],
   "title": "HYU Submission for the SASV Challenge 2022: Reforming Speaker Embeddings with Spoofing-Aware Conditioning",
   "original": "210",
   "page_count": 5,
   "order": 583,
   "p1": 2873,
   "pn": 2877,
   "abstract": [
    "In this paper, we introduce the spoofing-aware speaker verification (SASV) system submitted by the Hanyang University team for SASV Challenge 2022. Our strategy is to learn spoofing-aware speaker embeddings (SASEs) that can effectively produce SASV scores by using a simple cosine similarity scoring backend. To achieve this, we develop a neural-network-based SASE model that uses a spoofing countermeasure (CM) embedding and speaker embedding to produce an SASE. The baseline anti-spoofing model is used to extract CM embeddings, and ResNet-34- and Res2Net-based models are employed to extract speaker embeddings. When evaluated on the ASVspoof2019 logical access dataset, our best proposed SASV system achieved SASV equal error rates of 0.1817% and 0.2793% on the development and evaluation set partitions, respectively, placing 3rd in the SASV Challenge 2022."
   ],
   "doi": "10.21437/Interspeech.2022-210"
  },
  "irino22_interspeech": {
   "authors": [
    [
     "Toshio",
     "Irino"
    ],
    [
     "Honoka",
     "Tamaru"
    ],
    [
     "Ayako",
     "Yamamoto"
    ]
   ],
   "title": "Speech intelligibility of simulated hearing loss sounds and its prediction using the Gammachirp Envelope Similarity Index (GESI)",
   "original": "211",
   "page_count": 5,
   "order": 796,
   "p1": 3929,
   "pn": 3933,
   "abstract": [
    "In the present study, speech intelligibility (SI) experiments were performed using simulated hearing loss (HL) sounds in laboratory and remote environments to clarify the effects of peripheral dysfunction. Noisy speech sounds were processed to simulate the average HL of 70- and 80-year-olds using Wadai Hearing Impairment Simulator (WHIS). These sounds were presented to normal hearing (NH) listeners whose cognitive function could be assumed to be normal. The results showed that the divergence was larger in the remote experiments than in the laboratory ones. However, the remote results could be equalized to the laboratory ones, mostly through data screening using the results of tone pip tests prepared on the experimental web page. In addition, a newly proposed objective intelligibility measure (OIM) called the Gammachirp Envelope Similarity Index (GESI) explained the psychometric functions in the laboratory and remote experiments fairly well. GESI has the potential to explain the SI of HI listeners by properly setting HL parameters."
   ],
   "doi": "10.21437/Interspeech.2022-211"
  },
  "chen22d_interspeech": {
   "authors": [
    [
     "Yi-Chang",
     "Chen"
    ],
    [
     "Yu-Chuan",
     "Steven"
    ],
    [
     "Yen-Cheng",
     "Chang"
    ],
    [
     "Yi-Ren",
     "Yeh"
    ]
   ],
   "title": "g2pW: A Conditional Weighted Softmax BERT for Polyphone Disambiguation in Mandarin",
   "original": "216",
   "page_count": 5,
   "order": 390,
   "p1": 1926,
   "pn": 1930,
   "abstract": [
    "Polyphone disambiguation is the most crucial task in Mandarin grapheme-to-phoneme (g2p) conversion. Previous studies have approached this problem using pre-trained language models, restricted output, and extra information from Part-Of-Speech (POS) tagging. Inspired by these strategies, we propose a novel approach, called g2pW, which adapts learnable softmax-weights to condition the outputs of BERT with the polyphonic character of interest and its POS tagging. Rather than using the hard mask as in previous works, our experiments show that learning a soft-weighting function for the candidate phonemes benefits performance. In addition, our proposed g2pW does not require extra pre-trained POS tagging models while using POS tags as auxiliary features since we train the POS tagging model simultaneously with the unified encoder. Experimental results show that our g2pW outperforms existing methods on the public CPP dataset. All codes, model weights, and a user-friendly package are publicly available."
   ],
   "doi": "10.21437/Interspeech.2022-216"
  },
  "nguyen22_interspeech": {
   "authors": [
    [
     "Linh The",
     "Nguyen"
    ],
    [
     "Nguyen Luong",
     "Tran"
    ],
    [
     "Long",
     "Doan"
    ],
    [
     "Manh",
     "Luong"
    ],
    [
     "Dat Quoc",
     "Nguyen"
    ]
   ],
   "title": "A High-Quality and Large-Scale Dataset for English-Vietnamese Speech Translation",
   "original": "218",
   "page_count": 5,
   "order": 350,
   "p1": 1726,
   "pn": 1730,
   "abstract": [
    "In this paper, we introduce a high-quality and large-scale benchmark dataset for English-Vietnamese speech translation with 508 audio hours, consisting of 331K triplets of (sentence-lengthed audio, English source transcript sentence, Vietnamese target subtitle sentence). We also conduct empirical experiments using strong baselines and find that the traditional \"Cascaded\" approach still outperforms the modern \"End-to-End\" approach. To the best of our knowledge, this is the first large-scale English-Vietnamese speech translation study. We hope both our publicly available dataset and study can serve as a starting point for future research and applications on English-Vietnamese speech translation."
   ],
   "doi": "10.21437/Interspeech.2022-218"
  },
  "liang22b_interspeech": {
   "authors": [
    [
     "Chengdong",
     "Liang"
    ],
    [
     "Yijiang",
     "Chen"
    ],
    [
     "Jiadi",
     "Yao"
    ],
    [
     "Xiao-Lei",
     "Zhang"
    ]
   ],
   "title": "Multi-Channel Far-Field Speaker Verification with Large-Scale Ad-hoc Microphone Arrays",
   "original": "219",
   "page_count": 5,
   "order": 746,
   "p1": 3679,
   "pn": 3683,
   "abstract": [
    "Speaker verification based on ad-hoc microphone arrays has the potential of reducing the error significantly in adverse acoustic environments. However, existing approaches extract utterance-level speaker embeddings from each channel of an ad-hoc microphone array, which does not consider fully the spatial-temporal information across the devices. In this paper, we propose to aggregate the multichannel signals of the ad-hoc microphone array at the frame-level by exploring the cross-channel information deeply with two attention mechanisms. The first one is a self-attention method. It consists of a cross-frame self-attention layer and a cross-channel self-attention layer successively, both working at the frame level. The second one learns the cross-frame and cross-channel information via two graph attention layers. Experimental results demonstrate that the proposed methods reach the state-of-the-art performance. Moreover, the graph-attention method is better than the self-attention method in most cases."
   ],
   "doi": "10.21437/Interspeech.2022-219"
  },
  "yang22d_interspeech": {
   "authors": [
    [
     "Xue",
     "Yang"
    ],
    [
     "Changchun",
     "Bao"
    ]
   ],
   "title": "Embedding Recurrent Layers with Dual-Path Strategy in a Variant of Convolutional Network for Speaker-Independent Speech Separation",
   "original": "220",
   "page_count": 5,
   "order": 1082,
   "p1": 5338,
   "pn": 5342,
   "abstract": [
    "Speaker-independent speech separation has achieved remarkable performance in recent years with the development of deep neural network (DNN). Various network architectures, from traditional convolutional neural network (CNN) and recurrent neural network (RNN) to advanced transformer, have been designed sophistically to improve separation performance. However, the state-of-the-art models usually suffer from several flaws related to the computation, such as large model size, huge memory consumption and computational complexity. To find the balance between the performance and computational efficiency and to further explore the modeling ability of traditional network structure, we combine RNN and a newly proposed variant of convolutional network to cope with speech separation problem. By embedding two RNNs into basic block of this variant with the help of dual-path strategy, the proposed network can effectively learn the local information and global dependency. Besides, a four-staged structure enables the separation procedure to be performed gradually at finer and finer scales as the feature dimension increases. The experimental results on various datasets have proven the effectiveness of the proposed method and shown that a trade-off between the separation performance and computational efficiency is well achieved."
   ],
   "doi": "10.21437/Interspeech.2022-220"
  },
  "shen22_interspeech": {
   "authors": [
    [
     "Zhijie",
     "Shen"
    ],
    [
     "Wu",
     "Guo"
    ]
   ],
   "title": "An Improved Deliberation Network with Text Pre-training for Code-Switching Automatic Speech Recognition",
   "original": "221",
   "page_count": 5,
   "order": 781,
   "p1": 3854,
   "pn": 3858,
   "abstract": [
    "This paper proposes an improved deliberation network (DN) for end-to-end code-switching (CS) automatic speech recognition (ASR). In a conventional DN, acoustic encoding and first-pass hypothesis encoding are utilized separately and are simply combined by summation, which cannot take full advantage of their potential complementarity. Hence, the proposed improved DN model exploits the relationship between the two encodings through a two-staged process. First, by integrating the two encodings into a unified semantic space through a shared encoder, and second, by capturing the relevant information from the acoustic encoding through an attention mechanism before the final decoding process. Moreover, the lack of paired training data restricts the generalization ability of the model in CS ASR. To address this problem, the developed DN is pre-trained based on a denoising sequence-to-sequence (seq2seq) objective using unpaired text data. Experiments on a Chinese-English CS dataset demonstrate the effectiveness of the proposed method. Compared with the conventional DN, a 13.5% relative error rate reduction is observed."
   ],
   "doi": "10.21437/Interspeech.2022-221"
  },
  "kim22c_interspeech": {
   "authors": [
    [
     "Minchan",
     "Kim"
    ],
    [
     "Myeonghun",
     "Jeong"
    ],
    [
     "Byoung Jin",
     "Choi"
    ],
    [
     "Sunghwan",
     "Ahn"
    ],
    [
     "Joun Yeop",
     "Lee"
    ],
    [
     "Nam Soo",
     "Kim"
    ]
   ],
   "title": "Transfer Learning Framework for Low-Resource Text-to-Speech using a Large-Scale Unlabeled Speech Corpus",
   "original": "225",
   "page_count": 5,
   "order": 159,
   "p1": 788,
   "pn": 792,
   "abstract": [
    "Training a text-to-speech (TTS) model requires a large-scale text labeled speech corpus, which is troublesome to collect. In this paper, we propose a transfer learning framework for TTS that utilizes a large amount of unlabeled speech dataset for pre-training. By leveraging wav2vec2.0 representation, unlabeled speech can highly improve performance, especially in the lack of labeled speech. We also extend the proposed method to zero-shot multi-speaker TTS (ZS-TTS). The experimental results verify the effectiveness of the proposed method in terms of naturalness, intelligibility, and speaker generalization. We highlight that the single speaker TTS model fine-tuned on only 10 minutes of labeled dataset outperforms the other baselines, and the ZS-TTS model fine-tuned on only 30 minutes of single speaker dataset can generate the voice of the arbitrary speaker, by pre-training on an unlabeled multi-speaker speech corpus."
   ],
   "doi": "10.21437/Interspeech.2022-225"
  },
  "algayres22_interspeech": {
   "authors": [
    [
     "Robin",
     "Algayres"
    ],
    [
     "Adel",
     "Nabli"
    ],
    [
     "Benoît",
     "Sagot"
    ],
    [
     "Emmanuel",
     "Dupoux"
    ]
   ],
   "title": "Speech Sequence Embeddings using Nearest Neighbors Contrastive Learning",
   "original": "226",
   "page_count": 5,
   "order": 433,
   "p1": 2123,
   "pn": 2127,
   "abstract": [
    "We introduce a simple neural encoder architecture that can be trained using an unsupervised contrastive learning objective which gets its positive samples from data-augmented k-Nearest Neighbors search. We show that when built on top of recent self-supervised audio representations, this method can be applied iteratively and yield competitive SSE as evaluated on two tasks: query-by-example of random sequences of speech, and spoken term discovery. On both tasks our method pushes the state-of-the-art by a significant margin across 5 different languages. Finally, we establish a benchmark on a query-by-example task on the LibriSpeech dataset to monitor future improvements in the field."
   ],
   "doi": "10.21437/Interspeech.2022-226"
  },
  "koutini22_interspeech": {
   "authors": [
    [
     "Khaled",
     "Koutini"
    ],
    [
     "Jan",
     "Schlüter"
    ],
    [
     "Hamid",
     "Eghbal-zadeh"
    ],
    [
     "Gerhard",
     "Widmer"
    ]
   ],
   "title": "Efficient Training of Audio Transformers with Patchout",
   "original": "227",
   "page_count": 5,
   "order": 559,
   "p1": 2753,
   "pn": 2757,
   "abstract": [
    "The great success of transformer-based models in natural language processing (NLP) has led to various attempts at adapting these architectures to other domains such as vision and audio. Recent work has shown that transformers can outperform Convolutional Neural Networks (CNNs) on vision and audio tasks. However, one of the main shortcomings of transformer models, compared to the well-established CNNs, is the computational complexity. In transformers, the compute and memory complexity is known to grow quadratically with the input length. Therefore, there has been extensive work on optimizing transformers, but often at the cost of degrading predictive performance. In this work, we propose a novel method to optimize and regularize transformers on audio spectrograms. Our proposed models achieve a new state-of-the-art performance on Audioset and can be trained on a single consumer-grade GPU. Furthermore, we propose a transformer model that outperforms CNNs in terms of both performance and training speed."
   ],
   "doi": "10.21437/Interspeech.2022-227"
  },
  "zhang22b_interspeech": {
   "authors": [
    [
     "Song",
     "Zhang"
    ],
    [
     "Ken",
     "Zheng"
    ],
    [
     "Xiaoxu",
     "Zhu"
    ],
    [
     "Baoxiang",
     "Li"
    ]
   ],
   "title": "A polyphone BERT for Polyphone Disambiguation in Mandarin Chinese",
   "original": "229",
   "page_count": 5,
   "order": 88,
   "p1": 436,
   "pn": 440,
   "abstract": [
    "Grapheme-to-phoneme (G2P) conversion is an indispensable part of the Chinese Mandarin text-to-speech (TTS) system, and the core of G2P conversion is to solve the problem of polyphone disambiguation, which is to pick up the correct pronunciation for several candidates for a Chinese polyphonic character. In this paper, we propose a Chinese polyphone BERT model to predict the pronunciations of Chinese polyphonic characters. Firstly, we create 741 new Chinese monophonic characters from 354 source Chinese polyphonic characters by pronunciation. Then we get a Chinese polyphone BERT by extending a pre-trained Chinese BERT with 741 new Chinese monophonic characters and adding a corresponding embedding layer for new tokens, which is initialized by the embeddings of source Chinese polyphonic characters. In this way, we can turn the polyphone disambiguation task into a pre-training task of the Chinese polyphone BERT. Experimental results demonstrate the effectiveness of the proposed model, and the polyphone BERT model obtain 2% (from 92.1% to 94.1%) improvement of average accuracy compared with the BERT-based classifier model, which is the prior state-of-the-art in polyphone disambiguation."
   ],
   "doi": "10.21437/Interspeech.2022-229"
  },
  "chen22e_interspeech": {
   "authors": [
    [
     "Hangting",
     "Chen"
    ],
    [
     "Yi",
     "Yang"
    ],
    [
     "Feng",
     "Dang"
    ],
    [
     "Pengyuan",
     "Zhang"
    ]
   ],
   "title": "Beam-Guided TasNet: An Iterative Speech Separation Framework with Multi-Channel Output",
   "original": "230",
   "page_count": 5,
   "order": 177,
   "p1": 866,
   "pn": 870,
   "abstract": [
    "Time-domain audio separation network (TasNet) has achieved remarkable performance in blind source separation (BSS). Classic multi-channel speech processing framework employs signal estimation and beamforming. For example, Beam-TasNet links multi-channel convolutional TasNet (MC-Conv-TasNet) with minimum variance distortionless response (MVDR) beamforming, which leverages the strong modeling ability of data-driven network and boosts the performance of beamforming with an accurate estimation of speech statistics. Such integration can be viewed as a directed acyclic graph by accepting multi-channel input and generating multi-source output. In this paper, we design a \"multi-channel input, multi-channel multi-source output'' (MIMMO) speech separation system entitled \"Beam-Guided TasNet'', where MC-Conv-TasNet and MVDR can interact and promote each other more compactly under a directed cyclic flow. Specifically, the first stage uses Beam-TasNet to generate estimated single-speaker signals, which favors the separation in the second stage. The proposed framework facilitates iterative signal refinement with the guide of beamforming and seeks to reach the upper bound of the MVDR-based methods. Experimental results on the spatialized WSJ0-2MIX demonstrate that the Beam-Guided TasNet has achieved an SDR of 21.5 dB, exceeding the baseline Beam-TasNet by 4.1 dB under the same model size and narrowing the gap with the oracle signal-based MVDR to 2 dB."
   ],
   "doi": "10.21437/Interspeech.2022-230"
  },
  "aguirre22_interspeech": {
   "authors": [
    [
     "Diego",
     "Aguirre"
    ],
    [
     "Nigel",
     "Ward"
    ],
    [
     "Jonathan E.",
     "Avila"
    ],
    [
     "Heike",
     "Lehnert-LeHouillier"
    ]
   ],
   "title": "Comparison of Models for Detecting Off-Putting Speaking Styles",
   "original": "232",
   "page_count": 5,
   "order": 469,
   "p1": 2303,
   "pn": 2307,
   "abstract": [
    "In human-human interaction, speaking styles variation is pervasive. Modeling such variation has seen increasing interest, but there has been relatively little work on how best to discriminate among styles, and apparently none on how to exploit pretrained models for this. Moreover, little computational work has addressed questions of how styles are perceived, although this is often the most important aspect in terms of social and interpersonal relevance. Here we develop models of whether an utterance is likely to be perceived as off-putting. We explore different ways to leverage state-of-the-art pretrained representations, namely those for TRILL, COLA, and TRILLsson. We obtain reasonably good performance in detecting off-putting styles, and find that architectures and learned representations designed to capture multi-second temporal information perform better."
   ],
   "doi": "10.21437/Interspeech.2022-232"
  },
  "sanchez22_interspeech": {
   "authors": [
    [
     "Ariadna",
     "Sanchez"
    ],
    [
     "Alessio",
     "Falai"
    ],
    [
     "Ziyao",
     "Zhang"
    ],
    [
     "Orazio",
     "Angelini"
    ],
    [
     "Kayoko",
     "Yanagisawa"
    ]
   ],
   "title": "Unify and Conquer: How Phonetic Feature Representation Affects Polyglot Text-To-Speech (TTS)",
   "original": "233",
   "page_count": 5,
   "order": 601,
   "p1": 2963,
   "pn": 2967,
   "abstract": [
    "An essential design decision for multilingual Neural Text-To-Speech (NTTS) systems is how to represent input linguistic features within the model. Looking at the wide variety of approaches in the literature, two main paradigms emerge, unified and separate representations. The former uses a shared set of phonetic tokens across languages, whereas the latter uses unique phonetic tokens for each language. In this paper, we conduct a comprehensive study comparing multilingual NTTS systems models trained with both representations. Our results reveal that the unified approach consistently achieves better crosslingual synthesis with respect to both naturalness and accent. Separate representations tend to have an order of magnitude more tokens than unified ones, which may affect model capacity. For this reason, we carry out an ablation study to understand the interaction of the representation type with the size of the token embedding. We find that the difference between the two paradigms only emerges above a certain threshold embedding size. This study provides strong evidence that unified representations should be the preferred paradigm when building multilingual NTTS systems."
   ],
   "doi": "10.21437/Interspeech.2022-233"
  },
  "sadeghi22_interspeech": {
   "authors": [
    [
     "Mostafa",
     "Sadeghi"
    ],
    [
     "Paul",
     "Magron"
    ]
   ],
   "title": "A Sparsity-promoting Dictionary Model for Variational Autoencoders",
   "original": "237",
   "page_count": 5,
   "order": 74,
   "p1": 366,
   "pn": 370,
   "abstract": [
    "Structuring the latent space in probabilistic deep generative models, e.g., variational autoencoders (VAEs), is important to yield more expressive models and interpretable representations, and to avoid overfitting. One way to achieve this objective is to impose a sparsity constraint on the latent variables, e.g., via a Laplace prior. However, such approaches usually complicate the training phase, and they sacrifice the reconstruction quality to promote sparsity. In this paper, we propose a simple yet effective methodology to structure the latent space via a sparsity-promoting dictionary model, which assumes that each latent code can be written as a sparse linear combination of a dictionary's columns. In particular, we leverage a computationally efficient and tuning-free method, which relies on a zero-mean Gaussian latent prior with learnable variances. We derive a variational inference scheme to train the model. Experiments on speech generative modeling demonstrate the advantage of the proposed approach over competing techniques, since it promotes sparsity while not deteriorating the output speech quality."
   ],
   "doi": "10.21437/Interspeech.2022-237"
  },
  "sunder22_interspeech": {
   "authors": [
    [
     "Vishal",
     "Sunder"
    ],
    [
     "Eric",
     "Fosler-Lussier"
    ],
    [
     "Samuel",
     "Thomas"
    ],
    [
     "Hong-Kwang",
     "Kuo"
    ],
    [
     "Brian",
     "Kingsbury"
    ]
   ],
   "title": "Tokenwise Contrastive Pretraining for Finer Speech-to-BERT Alignment in End-to-End Speech-to-Intent Systems",
   "original": "239",
   "page_count": 5,
   "order": 545,
   "p1": 2683,
   "pn": 2687,
   "abstract": [
    "Recent advances in End-to-End (E2E) Spoken Language Understanding (SLU) have been primarily due to effective pretraining of speech representations. One such pretraining paradigm is the distillation of semantic knowledge from state-of-the-art text-based models like BERT to speech encoder neural networks. This work is a step towards doing the same in a much more efficient and fine-grained manner where we align speech embeddings and BERT embeddings on a token-by-token basis. We introduce a simple yet novel technique of using a cross-modal attention mechanism to extract token-level contextual embeddings from a speech encoder such that these can be directly compared and aligned with BERT based contextual embeddings. This alignment is performed using a novel tokenwise contrastive loss. Fine-tuning such a pretrained model to perform intent recognition using speech directly yields state-of-the-art performance on two widely used SLU datasets. Our model improves further when fine-tuned with additional regularization using SpecAugment especially when speech is noisy, giving an absolute improvement as high as 8% over previous results."
   ],
   "doi": "10.21437/Interspeech.2022-239"
  },
  "wu22b_interspeech": {
   "authors": [
    [
     "Ting-Wei",
     "Wu"
    ],
    [
     "Biing",
     "Juang"
    ]
   ],
   "title": "Induce Spoken Dialog Intents via Deep Unsupervised Context Contrastive Clustering",
   "original": "240",
   "page_count": 5,
   "order": 220,
   "p1": 1081,
   "pn": 1085,
   "abstract": [
    "Intent detection is one of most critical tasks in spoken language understanding. However, most systems could only identify a predefined set of intents, without covering a ubiquitous space of real-world semantics. Discovering new dialog intents with clustering to explore additional requests is crucial particularly in complex domains like customer support services. Leveraging the strong coherence between the user query utterance and their following contexts in the dialog, we present an effective intent induction approach with fine-tuning and clustering with contrastive learning. In particular, we first transform pretrained LMs into conversational encoders with in-domain dialogs. Then we conduct context-aware contrastive learning to reveal latent intent semantics via the coherence from dialog contexts. After obtaining the initial representations on both views of the query and their contexts, we propose a novel clustering method to iteratively refine the representation by minimizing semantic distances between pairs of utterances or contexts, under the same cluster assignment on the opposite view. The experimental results validate the robustness and versatility of our framework, which also achieves superior performances over competitive baselines without the label supervision."
   ],
   "doi": "10.21437/Interspeech.2022-240"
  },
  "zhang22c_interspeech": {
   "authors": [
    [
     "Ziyao",
     "Zhang"
    ],
    [
     "Alessio",
     "Falai"
    ],
    [
     "Ariadna",
     "Sanchez"
    ],
    [
     "Orazio",
     "Angelini"
    ],
    [
     "Kayoko",
     "Yanagisawa"
    ]
   ],
   "title": "Mix and Match: An Empirical Study on Training Corpus Composition for Polyglot Text-To-Speech (TTS)",
   "original": "242",
   "page_count": 5,
   "order": 479,
   "p1": 2353,
   "pn": 2357,
   "abstract": [
    "Training multilingual Neural Text-To-Speech (NTTS) models using only monolingual corpora has emerged as a popular way for building voice cloning based Polyglot NTTS systems. In order to train these models, it is essential to understand how the composition of the training corpora affects the quality of multilingual speech synthesis. In this context, it is common to hear questions such as \"Would including more Spanish data help my Italian synthesis, given the closeness of both languages?”. Unfortunately, we found existing literature on the topic lacking in completeness in this regard. In the present work, we conduct an extensive ablation study aimed at understanding how various factors of the training corpora, such as language family affiliation, gender composition, and the number of speakers, contribute to the quality of Polyglot synthesis. Our findings include the observation that female speaker data are preferred in most scenarios, and that it is not always beneficial to have more speakers from the target language variant in the training corpus. The findings herein are informative for the process of data procurement and corpora building."
   ],
   "doi": "10.21437/Interspeech.2022-242"
  },
  "hu22_interspeech": {
   "authors": [
    [
     "Ke",
     "Hu"
    ],
    [
     "Tara",
     "Sainath"
    ],
    [
     "Yanzhang",
     "He"
    ],
    [
     "Rohit",
     "Prabhavalkar"
    ],
    [
     "Trevor",
     "Strohman"
    ],
    [
     "Sepand",
     "Mavandadi"
    ],
    [
     "Weiran",
     "Wang"
    ]
   ],
   "title": "Improving Deliberation by Text-Only and Semi-Supervised Training",
   "original": "243",
   "page_count": 5,
   "order": 1000,
   "p1": 4940,
   "pn": 4944,
   "abstract": [
    "Text-only and semi-supervised training based on audio-only data has gained popularity recently due to the wide availability of unlabeled text and speech data. In this work, we propose incorporating text-only and semi-supervised training into an attention-based deliberation model. By incorporating text-only data in training a bidirectional encoder representation from transformer (BERT) for the deliberation text encoder, and large-scale text-to-speech and audio-only utterances using joint acoustic and text decoder (JATD) and semi-supervised training, we achieved 4%-12% WER reduction for various tasks compared to the baseline deliberation. Compared to a state-of-the-art language model (LM) rescoring method, the deliberation model reduces the Google Voice Search WER by 11% relative. We show that the deliberation model also achieves a positive human side-by-side evaluation compared to the state-of-the-art LM rescorer with reasonable endpointer latencies."
   ],
   "doi": "10.21437/Interspeech.2022-243"
  },
  "yin22_interspeech": {
   "authors": [
    [
     "Dacheng",
     "Yin"
    ],
    [
     "Chuanxin",
     "Tang"
    ],
    [
     "Yanqing",
     "Liu"
    ],
    [
     "Xiaoqiang",
     "Wang"
    ],
    [
     "Zhiyuan",
     "Zhao"
    ],
    [
     "Yucheng",
     "Zhao"
    ],
    [
     "Zhiwei",
     "Xiong"
    ],
    [
     "Sheng",
     "Zhao"
    ],
    [
     "Chong",
     "Luo"
    ]
   ],
   "title": "RetrieverTTS: Modeling Decomposed Factors for Text-Based Speech Insertion",
   "original": "245",
   "page_count": 5,
   "order": 319,
   "p1": 1571,
   "pn": 1575,
   "abstract": [
    "This paper proposes a new \"decompose-and-edit\" paradigm for the text-based speech insertion task that facilitates arbitrary-length speech insertion and even full sentence generation. In the proposed paradigm, global and local factors in speech are explicitly decomposed and separately manipulated to achieve high speaker similarity and continuous prosody. Specifically, we proposed to represent the global factors by multiple tokens, which are extracted by cross-attention operation and then injected back by link-attention operation. Due to the rich representation of global factors, we manage to achieve high speaker similarity in a zero-shot manner. In addition, we introduce a prosody smoothing task to make the local prosody factor context-aware and therefore achieve satisfactory prosody continuity. We further achieve high voice quality with an adversarial training stage. In the subjective test, our method achieves state-of-the-art performance in both naturalness and similarity. Audio samples can be found at https://ydcustc.github.io/retrieverTTS-demo/."
   ],
   "doi": "10.21437/Interspeech.2022-245"
  },
  "ristea22_interspeech": {
   "authors": [
    [
     "Nicolaea Catalin",
     "Ristea"
    ],
    [
     "Radu Tudor",
     "Ionescu"
    ],
    [
     "Fahad Shahbaz",
     "Khan"
    ]
   ],
   "title": "SepTr: Separable Transformer for Audio Spectrogram Processing",
   "original": "249",
   "page_count": 5,
   "order": 832,
   "p1": 4103,
   "pn": 4107,
   "abstract": [
    "Following the successful application of vision transformers in multiple computer vision tasks, these models have drawn the attention of the signal processing community. This is because signals are often represented as spectrograms (e.g. through Discrete Fourier Transform) which can be directly provided as input to vision transformers. However, naively applying transformers to spectrograms is suboptimal. Since the axes represent distinct dimensions, i.e. frequency and time, we argue that a better approach is to separate the attention dedicated to each axis. To this end, we propose the Separable Transformer (SepTr), an architecture that employs two transformer blocks in a sequential manner, the first attending to tokens within the same time interval, and the second attending to tokens within the same frequency bin. We conduct experiments on three benchmark data sets, showing that our separable architecture outperforms conventional vision transformers and other state-of-the-art methods. Unlike standard transformers, SepTr linearly scales the number of trainable parameters with the input size, thus having a lower memory footprint. Our code is available as open source at https://github.com/ristea/septr."
   ],
   "doi": "10.21437/Interspeech.2022-249"
  },
  "mehmood22_interspeech": {
   "authors": [
    [
     "Haaris",
     "Mehmood"
    ],
    [
     "Agnieszka",
     "Dobrowolska"
    ],
    [
     "Karthikeyan",
     "Saravanan"
    ],
    [
     "Mete",
     "Ozay"
    ]
   ],
   "title": "FedNST: Federated Noisy Student Training for Automatic Speech Recognition",
   "original": "252",
   "page_count": 5,
   "order": 204,
   "p1": 1001,
   "pn": 1005,
   "abstract": [
    "Federated Learning (FL) enables training state-of-the-art Automatic Speech Recognition (ASR) models on user devices (clients) in distributed systems, hence preventing transmission of raw user data to a central server. A key challenge facing practical adoption of FL for ASR is obtaining ground-truth labels on the clients. Existing approaches rely on clients to manually transcribe their speech, which is impractical for obtaining large training corpora. A promising alternative is using semi-/self-supervised learning approaches to leverage unlabelled user data. To this end, we propose FedNST, a novel method for training distributed ASR models using private and unlabelled user data. We explore various facets of FedNST, such as training models with different proportions of labelled and unlabelled data, and evaluate the proposed approach on 1173 simulated clients. Evaluating FedNST on LibriSpeech, where 960 hours of speech data is split equally into server (labelled) and client (unlabelled) data, showed a 22.5\\% relative word error rate reduction (WERR) over a supervised baseline trained only on server data."
   ],
   "doi": "10.21437/Interspeech.2022-252"
  },
  "kanda22b_interspeech": {
   "authors": [
    [
     "Naoyuki",
     "Kanda"
    ],
    [
     "Jian",
     "Wu"
    ],
    [
     "Yu",
     "Wu"
    ],
    [
     "Xiong",
     "Xiao"
    ],
    [
     "Zhong",
     "Meng"
    ],
    [
     "Xiaofei",
     "Wang"
    ],
    [
     "Yashesh",
     "Gaur"
    ],
    [
     "Zhuo",
     "Chen"
    ],
    [
     "Jinyu",
     "Li"
    ],
    [
     "Takuya",
     "Yoshioka"
    ]
   ],
   "title": "Streaming Speaker-Attributed ASR with Token-Level Speaker Embeddings",
   "original": "253",
   "page_count": 5,
   "order": 105,
   "p1": 521,
   "pn": 525,
   "abstract": [
    "This paper presents a streaming speaker-attributed automatic speech recognition (SA-ASR) model that can recognize ``who spoke what'' with low latency even when multiple people are speaking simultaneously. Our model is based on token-level serialized output training (t-SOT) which was recently proposed to transcribe multi-talker speech in a streaming fashion. To further recognize speaker identities, we propose an encoder-decoder based speaker embedding extractor that can estimate a speaker representation for each recognized token not only from non-overlapping speech but also from overlapping speech. The proposed speaker embedding, named t-vector, is extracted synchronously with the t-SOT ASR model, enabling joint execution of speaker identification (SID) or speaker diarization (SD) with the multi-talker transcription with low latency. We evaluate the proposed model for a joint task of ASR and SID/SD by using LibriSpeechMix and LibriCSS corpora. The proposed model achieves substantially better accuracy than a prior streaming model and shows comparable or sometimes even superior results to the state-of-the-art offline SA-ASR model."
   ],
   "doi": "10.21437/Interspeech.2022-253"
  },
  "udagawa22_interspeech": {
   "authors": [
    [
     "Kenta",
     "Udagawa"
    ],
    [
     "Yuki",
     "Saito"
    ],
    [
     "Hiroshi",
     "Saruwatari"
    ]
   ],
   "title": "Human-in-the-loop Speaker Adaptation for DNN-based Multi-speaker TTS",
   "original": "257",
   "page_count": 5,
   "order": 602,
   "p1": 2968,
   "pn": 2972,
   "abstract": [
    "This paper proposes a human-in-the-loop speaker-adaptation method for multi-speaker text-to-speech. With a conventional speaker-adaptation method, a target speaker's embedding vector is extracted from his/her reference speech using a speaker encoder trained on a speaker-discriminative task. However, this method cannot obtain an embedding vector for the target speaker when the reference speech is unavailable. Our method is based on a human-in-the-loop optimization framework, which incorporates a user to explore the speaker-embedding space to find the target speaker's embedding. The proposed method uses a sequential line search algorithm that repeatedly asks a user to select a point on a line segment in the embedding space. To efficiently choose the best speech sample from multiple stimuli, we also developed a system in which a user can switch between multiple speakers' voices for each phoneme. Experimental results indicate that the proposed method can achieve comparable performance to the conventional one in objective and subjective evaluations even if reference speech is not used as the input of a speaker encoder directly."
   ],
   "doi": "10.21437/Interspeech.2022-257"
  },
  "mitsui22_interspeech": {
   "authors": [
    [
     "Kentaro",
     "Mitsui"
    ],
    [
     "Tianyu",
     "Zhao"
    ],
    [
     "Kei",
     "Sawada"
    ],
    [
     "Yukiya",
     "Hono"
    ],
    [
     "Yoshihiko",
     "Nankaku"
    ],
    [
     "Keiichi",
     "Tokuda"
    ]
   ],
   "title": "End-to-End Text-to-Speech Based on Latent Representation of Speaking Styles Using Spontaneous Dialogue",
   "original": "259",
   "page_count": 5,
   "order": 474,
   "p1": 2328,
   "pn": 2332,
   "abstract": [
    "The recent text-to-speech (TTS) has achieved quality comparable to that of humans; however, its application in spoken dialogue has not been widely studied. This study aims to realize a TTS that closely resembles human dialogue. First, we record and transcribe actual spontaneous dialogues. Then, the proposed dialogue TTS is trained in two stages: first stage, variational autoencoder (VAE)-VITS or Gaussian mixture variational autoencoder (GMVAE)-VITS is trained, which introduces an utterance-level latent variable into variational inference with adversarial learning for end-to-end text-to-speech (VITS), a recently proposed end-to-end TTS model. A style encoder that extracts a latent speaking style representation from speech is trained jointly with TTS. In the second stage, a style predictor is trained to predict the speaking style to be synthesized from dialogue history. During inference, by passing the speaking style representation predicted by the style predictor to VAE/GMVAE-VITS, speech can be synthesized in a style appropriate to the context of the dialogue. Subjective evaluation results demonstrate that the proposed method outperforms the original VITS in terms of dialogue-level naturalness."
   ],
   "doi": "10.21437/Interspeech.2022-259"
  },
  "novitasari22_interspeech": {
   "authors": [
    [
     "Sashi",
     "Novitasari"
    ],
    [
     "Takashi",
     "Fukuda"
    ],
    [
     "Gakuto",
     "Kurata"
    ]
   ],
   "title": "Improving ASR Robustness in Noisy Condition Through VAD Integration",
   "original": "260",
   "page_count": 5,
   "order": 767,
   "p1": 3784,
   "pn": 3788,
   "abstract": [
    "Automatic speech recognition (ASR) systems are often deployed together with a VAD system to run ASR only on the voiced acoustic signals. Although it can maintain ASR performance by removing unnecessary non-speech parts from input audio signals during inference, an error propagates when VAD fails to split speech and non-speech segments correctly. Specifically, because ASR systems are commonly constructed using segmented speech utterances only, unexpected insertion errors can occur when VAD-segmented utterances contain a long non-speech part or only consist of non-speech. Note VAD is more prone to fail in noisy environments or in unknown acoustic domains, which triggers insertion errors in ASR more prominently. In this paper, we focus on explicitly incorporating VAD information into training of an RNN-T based ASR to make the model more robust to noisy conditions through feature integration and a multi-task learning strategy. A technique is also explored that utilizes audio-only untranscribed data by distilling VAD-related knowledge to the ASR part of the model. By combining the multi-task learning approach with the feature integration architecture, our system yields up to 10% relative improvements in very low SNR conditions compared with the system simply trained on mixed data consisting of speech and long non-speech segments."
   ],
   "doi": "10.21437/Interspeech.2022-260"
  },
  "abeysinghe22_interspeech": {
   "authors": [
    [
     "Binu Nisal",
     "Abeysinghe"
    ],
    [
     "Jesin",
     "James"
    ],
    [
     "Catherine",
     "Watson"
    ],
    [
     "Felix",
     "Marattukalam"
    ]
   ],
   "title": "Visualising Model Training via Vowel Space for Text-To-Speech Systems",
   "original": "264",
   "page_count": 5,
   "order": 103,
   "p1": 511,
   "pn": 515,
   "abstract": [
    "With the recent developments in speech synthesis via machine learning, this study explores incorporating linguistics knowledge to visualise and evaluate synthetic speech model training. If changes to the first and second formant (in turn, the vowel space) can be seen and heard in synthetic speech, this knowledge can inform speech synthesis technology developers. A speech synthesis model trained on a large General American English database was fine-tuned into a New Zealand English voice to identify if the changes in the vowel space of synthetic speech could be seen and heard. The vowel spaces at different intervals during the fine-tuning were analysed to determine if the model learned the New Zealand English vowel space. Our findings based on vowel space analysis show that we can visualise how a speech synthesis model learns the vowel space of the database it is trained on. Perception tests confirmed that humans could perceive when a speech synthesis model has learned characteristics of the speech database it is training on. Using the vowel space as an intermediary evaluation helps understand what sounds are to be added to the training database and build speech synthesis models based on linguistics knowledge."
   ],
   "doi": "10.21437/Interspeech.2022-264"
  },
  "kishiyama22_interspeech": {
   "authors": [
    [
     "Takeshi",
     "Kishiyama"
    ],
    [
     "Chuyu",
     "Huang"
    ],
    [
     "Yuki",
     "Hirose"
    ]
   ],
   "title": "One-step models in pitch perception: Experimental evidence from Japanese",
   "original": "265",
   "page_count": 5,
   "order": 373,
   "p1": 1841,
   "pn": 1845,
   "abstract": [
    "Several psycholinguistic and computational models have examined the perception of illusory vowels, where listeners of a language insert an epenthetic vowel to repair illegal consonant clusters, perceiving VCCV as VCVCV. This study investigated whether these top-down effects can be extended to pitch patterns and induce illusory pitches, where a pitch was perceived on the epenthetic vowel. Tokyo and Kinki Japanese are two dialects in Japan with the same phonotactics, but Tokyo and Kinki Japanese regard LLH (low low high) and LHH as illegal tonal patterns, respectively. We controlled an index representing linguistic exposure to the Tokyo pitch pattern and used an AXB discrimination task to investigate whether the pitch patterns influence the perception. We found that Tokyo dialect listeners with the high index, who have long exposure to the Tokyo pitch pattern, perceived \"H” pitch between L and H, whereas the subjects with the low index did not show any preference. These results indicated that pitch patterns were also used in the perception of illusory pitches and were reproduced in a simulation study."
   ],
   "doi": "10.21437/Interspeech.2022-265"
  },
  "zhao22c_interspeech": {
   "authors": [
    [
     "Jing",
     "Zhao"
    ],
    [
     "Haoyu",
     "Wang"
    ],
    [
     "Jinpeng",
     "Li"
    ],
    [
     "Shuzhou",
     "Chai"
    ],
    [
     "Guanbo",
     "Wang"
    ],
    [
     "Guoguo",
     "Chen"
    ],
    [
     "Wei-Qiang",
     "Zhang"
    ]
   ],
   "title": "The THUEE System Description for the IARPA OpenASR21 Challenge",
   "original": "269",
   "page_count": 5,
   "order": 983,
   "p1": 4855,
   "pn": 4859,
   "abstract": [
    "This paper describes the THUEE team's speech recognition system for the IARPA Open Automatic Speech Recognition Challenge (OpenASR21), with further experiment explorations. We achieve outstanding results under both the Constrained and Constrained-plus training conditions. For the Constrained training condition, we construct our basic ASR system based on the standard hybrid architecture. To alleviate the Out-Of-Vocabulary (OOV) problem, we extend the pronunciation lexicon using Grapheme-to-Phoneme (G2P) techniques for both OOV and potential new words. Standard acoustic model structures such as CNN-TDNN-F and CNN-TDNN-F-A are adopted. In addition, multiple data augmentation techniques are applied. For the Constrained-plus training condition, we use the self-supervised learning framework wav2vec2.0. We experiment with various fine-tuning techniques with the Connectionist Temporal Classification (CTC) criterion on top of the publicly available pre-trained model XLSR-53. We find that the frontend feature extractor plays an important role when applying the wav2vec2.0 pre-trained model to the encoder-decoder based CTC/Attention ASR architecture. Extra improvements can be achieved by using the CTC model finetuned in the target language as the frontend feature extractor."
   ],
   "doi": "10.21437/Interspeech.2022-269"
  },
  "saijo22_interspeech": {
   "authors": [
    [
     "Kohei",
     "Saijo"
    ],
    [
     "Robin",
     "Scheibler"
    ]
   ],
   "title": "Independence-based Joint Dereverberation and Separation with Neural Source Model",
   "original": "271",
   "page_count": 5,
   "order": 48,
   "p1": 236,
   "pn": 240,
   "abstract": [
    "We propose an independence-based joint dereverberation and separation method with a neural source model. We introduce a neural network in the framework of time-decorrelation iterative source steering, which is an extension of independent vector analysis to joint dereverberation and separation. The network is trained in an end-to-end manner with a permutation invariant loss on the time-domain separation output signals. Our proposed method can be applied in any situation with at least as many microphones as sources, regardless of their number. In experiments, we demonstrate that our method results in high performance in terms of both speech quality metrics and word error rate (WER), even for mixtures with a different number of speakers than training. Furthermore, the model, trained on synthetic mixtures, without any modifications, greatly reduces the WER on the recorded dataset LibriCSS."
   ],
   "doi": "10.21437/Interspeech.2022-271"
  },
  "luong22_interspeech": {
   "authors": [
    [
     "Manh",
     "Luong"
    ],
    [
     "Viet Anh",
     "Tran"
    ]
   ],
   "title": "FlowVocoder: A small Footprint Neural Vocoder based Normalizing Flow for Speech Synthesis",
   "original": "272",
   "page_count": 5,
   "order": 320,
   "p1": 1576,
   "pn": 1580,
   "abstract": [
    "Recently, autoregressive neural vocoders have provided remarkable performance in generating high-fidelity speech and have been able to produce synthetic speech in real-time. However, autoregressive neural vocoders such as WaveFlow are capable of modeling waveform signals from mel-spectrogram, the number of parameters of WaveFlow is significant to deploy on edge devices. Though NanoFlow, which has a small number of parameters, is a state-of-the-art autoregressive neural vocoder, the performance of NanoFlow is marginally lower than WaveFlow. Therefore, we propose a new type of autoregressive neural vocoder called FlowVocoder, which has a small memory footprint and is capable of generating high-fidelity audio in real-time. Our proposed model improves the density estimation of flow blocks by utilizing a mixture of Cumulative Distribution Functions (CDF) for bipartite transformation. Hence, the proposed model is capable of modeling waveform signals, while its memory footprint is much smaller than WaveFlow. As shown in experiments, FlowVocoder achieves competitive results with baseline methods in terms of both subjective and objective evaluation, also, it is more suitable for real-time text-to-speech applications."
   ],
   "doi": "10.21437/Interspeech.2022-272"
  },
  "saijo22b_interspeech": {
   "authors": [
    [
     "Kohei",
     "Saijo"
    ],
    [
     "Robin",
     "Scheibler"
    ]
   ],
   "title": "Spatial Loss for Unsupervised Multi-channel Source Separation",
   "original": "274",
   "page_count": 5,
   "order": 49,
   "p1": 241,
   "pn": 245,
   "abstract": [
    "We propose a spatial loss for unsupervised multi-channel source separation. The proposed loss exploits the duality of direction of arrival (DOA) and beamforming: the steering and beamforming vectors should be aligned for the target source, but orthogonal for interfering ones. The spatial loss encourages consistency between the mixing and demixing systems from a classic DOA estimator and a neural separator, respectively. With the proposed loss, we train the neural separators based on minimum variance distortionless response (MVDR) beamforming and independent vector analysis (IVA). We also investigate the effectiveness of combining our spatial loss and a signal loss, which uses the outputs of blind source separation as the references. We evaluate our proposed method on synthetic and recorded (LibriCSS) mixtures. We find that the spatial loss is most effective to train IVA-based separators. For the neural MVDR beamformer, it performs best when combined with a signal loss. On synthetic mixtures, the proposed unsupervised loss leads to the same performance as a supervised loss in terms of word error rate. On LibriCSS, we obtain close to state-of-the-art performance without any labeled training data."
   ],
   "doi": "10.21437/Interspeech.2022-274"
  },
  "liu22c_interspeech": {
   "authors": [
    [
     "Yanqing",
     "Liu"
    ],
    [
     "Ruiqing",
     "Xue"
    ],
    [
     "Lei",
     "He"
    ],
    [
     "Xu",
     "Tan"
    ],
    [
     "Sheng",
     "Zhao"
    ]
   ],
   "title": "DelightfulTTS 2: End-to-End Speech Synthesis with Adversarial Vector-Quantized Auto-Encoders",
   "original": "277",
   "page_count": 5,
   "order": 321,
   "p1": 1581,
   "pn": 1585,
   "abstract": [
    "This paper describes DelightfulTTS 2, a new end-to-end architecture for speech synthesis jointly optimizing acoustic model and vocoder modules with phoneme/text and audio data pairs. Current TTS systems usually leverage a cascaded acoustic model and vocoder pipeline with mel-spectrograms as the intermediate representations, which suffer from two limitations: first, the acoustic model and vocoder are separately trained instead of jointly optimized, which incurs cascaded errors; second, the intermediate speech representations (e.g., mel-spectrogram) are predesigned and lose phase information, which are sub-optimal. To solve these problems, in this paper, we develop DelightfulTTS 2, a new end-to-end speech synthesis system with automatically learned speech representations and jointly optimized acoustic model and vocoder. Specifically, 1) We propose a new codec network based on vector-quantized auto-encoders with adversarial training (VQ-GAN) to extract intermediate frame-level speech representations (instead of traditional representations like mel-spectrograms) and reconstruct speech waveform. 2) We jointly optimize the acoustic model (based on DelightfulTTS) and the vocoder (the decoder of VQ-GAN), with an auxiliary loss on the acoustic model to predict intermediate speech representations. Experiments show that DelightfulTTS 2 achieves a CMOS gain +0.14 over DelightfulTTS, and more method analyses further verify the effectiveness of the developed system."
   ],
   "doi": "10.21437/Interspeech.2022-277"
  },
  "horii22_interspeech": {
   "authors": [
    [
     "Koharu",
     "Horii"
    ],
    [
     "Meiko",
     "Fukuda"
    ],
    [
     "Kengo",
     "Ohta"
    ],
    [
     "Ryota",
     "Nishimura"
    ],
    [
     "Atsunori",
     "Ogawa"
    ],
    [
     "Norihide",
     "Kitaoka"
    ]
   ],
   "title": "End-to-End Spontaneous Speech Recognition Using Disfluency Labeling",
   "original": "281",
   "page_count": 5,
   "order": 833,
   "p1": 4108,
   "pn": 4112,
   "abstract": [
    "Spontaneous speech often contains disfluent acoustic features such as fillers and hesitations, which are major causes of errors during automatic speech recognition (ASR). In this paper, we propose a method of \"disfluency labeling” to address this problem. Our proposed method replaces disfluent phenomena in the transcription of speech data used for training with two types of labels, filler (＃) and hesitation (＠), and trains an end-to-end ASR model using this data, which makes it possible to recognize disfluent acoustic phenomena as recognition targets, like characters. In addition, by removing the disfluency labels that are included in the recognition results, the words that the speaker actually intended to say can be extracted from the disfluent speech. The results of our evaluation experiments show that both the character and sentence error rates were reduced for all of the ASR test sets when disfluency labeling was applied, compared to the baseline method. The proposed method also outperformed other methods intended to reduce disfluency-related errors, even when more disfluent, spontaneous dialog speech was used. This study shows that explicit learning of two disfluent features, fillers and hesitations, is effective in spontaneous speech recognition."
   ],
   "doi": "10.21437/Interspeech.2022-281"
  },
  "zhang22d_interspeech": {
   "authors": [
    [
     "Zhuohuang",
     "Zhang"
    ],
    [
     "Donald",
     "Williamson"
    ],
    [
     "Yi",
     "Shen"
    ]
   ],
   "title": "Investigation on the Band Importance of Phase-aware Speech Enhancement",
   "original": "284",
   "page_count": 5,
   "order": 942,
   "p1": 4651,
   "pn": 4655,
   "abstract": [
    "Many existing phase-aware speech enhancement algorithms consider the phase at all spectral frequencies to be equally important to perceptual quality and intelligibility. Although improvements are observed according to both objective and subjective measures, as compared to phase-insensitive approaches, it is not clear whether phase information is equally important across the frequency spectrum. In this paper, we investigate the importance of estimating phase across spectral regions, by conducting a pairwise listening study to determine if phase enhancement can be limited to certain frequency bands. Our experimental results suggest that estimating phase at lower-frequency bands is mostly important for speech quality in normal-hearing (NH) listeners. We further propose a hybrid deep-learning framework that adopts two sub-networks for handling phase differently across the spectrum. The proposed hybrid-net significantly improves the model compatibility with low-resource platforms while achieving superior performance to the original phase-aware speech enhancement approaches."
   ],
   "doi": "10.21437/Interspeech.2022-284"
  },
  "kasun22_interspeech": {
   "authors": [
    [
     "Chamara",
     "Kasun"
    ],
    [
     "Chung Soo",
     "Ahn"
    ],
    [
     "Jagath",
     "Rajapakse"
    ],
    [
     "Zhiping",
     "Lin"
    ],
    [
     "Guang-Bin",
     "Huang"
    ]
   ],
   "title": "Discriminative Adversarial Learning for Speaker Independent Emotion Recognition",
   "original": "285",
   "page_count": 5,
   "order": 1007,
   "p1": 4975,
   "pn": 4979,
   "abstract": [
    "Traditional adversarial learning (AL) algorithms learns a speaker independent embedding from low level audio features. This paper introduces discriminative adversarial learning (DAL) which learn a discriminative speaker independent embedding from low level audio features such as mel frequency cepstral coefficients (MFCC) and high level audio features such as Interspeech Para-linguistics Challenge 2010. To this end, DAL jointly minimize triplet and cross-entropy losses with gradient reversal strategy for speaker independent emotion recognition (SIER). Triplet loss reduce intra-class and increase the inter-class embedding distance to improve the discriminativeness of the embedding while the cross-entropy loss determine the emotion or speaker class of the embedding and gradient reversal learn speaker independent embedding for SIER. Experiments on Emo-DB and RAVDESS datasets show that DAL outperform other traditional adversarial learning (AL) algorithms."
   ],
   "doi": "10.21437/Interspeech.2022-285"
  },
  "yuan22_interspeech": {
   "authors": [
    [
     "Xin",
     "Yuan"
    ],
    [
     "Robin",
     "Feng"
    ],
    [
     "Mingming",
     "Ye"
    ],
    [
     "Cheng",
     "Tuo"
    ],
    [
     "Minhang",
     "Zhang"
    ]
   ],
   "title": "AdaVocoder: Adaptive Vocoder for Custom Voice",
   "original": "288",
   "page_count": 5,
   "order": 322,
   "p1": 1586,
   "pn": 1590,
   "abstract": [
    "Custom voice is to construct a personal speech synthesis system by adapting the source speech synthesis model to the target model through the target few recordings. The solution to constructing a custom voice is to combine an adaptive acoustic model with a robust vocoder. However, training a robust vocoder usually requires a multi-speaker dataset, which should include various age groups and various timbres, so that the trained vocoder can be used for unseen speakers. Collecting such a multi-speaker dataset is difficult, and the dataset distribution always has a mismatch with the distribution of the target speaker dataset. This paper proposes an adaptive vocoder for custom voice from another novel perspective to solve the above problems. The adaptive vocoder mainly uses a cross-domain consistency loss to solve the overfitting problem encountered by the GAN-based neural vocoder in the transfer learning of few-shot scenes. We construct two adaptive vocoders, AdaMelGAN and AdaHiFi-GAN. First, We pre-train the source vocoder model on AISHELL3 and CSMSC datasets, respectively. Then, fine-tune it on the internal dataset VXI-children with few adaptation data. The empirical results show that a high-quality custom voice system can be built by combining a adaptive acoustic model with a adaptive vocoder."
   ],
   "doi": "10.21437/Interspeech.2022-288"
  },
  "zheng22_interspeech": {
   "authors": [
    [
     "Siqi",
     "Zheng"
    ],
    [
     "Hongbin",
     "Suo"
    ],
    [
     "Qian",
     "Chen"
    ]
   ],
   "title": "PRISM: Pre-trained Indeterminate Speaker Representation Model for Speaker Diarization and Speaker Verification",
   "original": "289",
   "page_count": 5,
   "order": 291,
   "p1": 1431,
   "pn": 1435,
   "abstract": [
    "Speaker embedding has been a fundamental feature for speaker-related tasks such as verification, clustering, and diarization. Traditionally, speaker embeddings are represented as fixed vectors in high-dimensional space. This could lead to biased estimations, especially when handling shorter utterances. In this paper we propose to represent a speaker utterance as \"floating\" vector whose state is indeterminate without knowing the context. The state of a speaker representation is jointly determined by itself, other speech from the same speaker, as well as other speakers it is being compared to. The content of the speech also contributes to determining the final state of a speaker representation. We pre-train an indeterminate speaker representation model that estimates the state of an utterance based on the context. The pre-trained model can be fine-tuned for downstream tasks such as speaker verification, speaker clustering, and speaker diarization. Substantial improvements are observed across all downstream tasks."
   ],
   "doi": "10.21437/Interspeech.2022-289"
  },
  "du22_interspeech": {
   "authors": [
    [
     "Yeqian",
     "Du"
    ],
    [
     "Jie",
     "Zhang"
    ],
    [
     "Qiu-shi",
     "Zhu"
    ],
    [
     "Lirong",
     "Dai"
    ],
    [
     "MingHui",
     "Wu"
    ],
    [
     "Xin",
     "Fang"
    ],
    [
     "ZhouWang",
     "Yang"
    ]
   ],
   "title": "A Complementary Joint Training Approach Using Unpaired Speech and Text A Complementary Joint Training Approach Using Unpaired Speech and Text",
   "original": "291",
   "page_count": 5,
   "order": 531,
   "p1": 2613,
   "pn": 2617,
   "abstract": [
    "Unpaired data has shown to be beneficial for low-resource automatic speech recognition (ASR), which can be involved in the design of hybrid models with multi-task training or language model dependent pre-training. In this work, we leverage unpaired data to train a general sequence-to-sequence model. Unpaired speech and text are used in the form of data pairs by generating the corresponding missing parts in prior to model training. Inspired by the complementarity of speech-PseudoLabel pair and SynthesizedAudio-text pair in both acoustic features and linguistic features, we propose a complementary joint training (CJT) method that trains a model alternatively with two data pairs. Furthermore, label masking for pseudo-labels and gradient restriction for synthesized audio are proposed to further cope with the deviations from real data, termed as CJT++. Experimental results show that compared to speech-only training, the proposed basic CJT achieves great performance improvements on clean/other test sets, and the CJT++ re-training yields further performance enhancements. It is also apparent that the proposed method outperforms the wav2vec2.0 model with the same model size and beam size, particularly in extreme low-resource cases."
   ],
   "doi": "10.21437/Interspeech.2022-291"
  },
  "saeki22_interspeech": {
   "authors": [
    [
     "Takaaki",
     "Saeki"
    ],
    [
     "Kentaro",
     "Tachibana"
    ],
    [
     "Ryuichi",
     "Yamamoto"
    ]
   ],
   "title": "DRSpeech: Degradation-Robust Text-to-Speech Synthesis with Frame-Level and Utterance-Level Acoustic Representation Learning",
   "original": "294",
   "page_count": 5,
   "order": 160,
   "p1": 793,
   "pn": 797,
   "abstract": [
    "Most text-to-speech (TTS) methods use high-quality speech corpora recorded in a well-designed environment, incurring a high cost for data collection. To solve this problem, existing noise-robust TTS methods are intended to use noisy speech corpora as training data. However, they only address either time-invariant or time-variant noises. We propose a degradation-robust TTS method, which can be trained on speech corpora that contain both additive noises and environmental distortions. It jointly represents the time-variant additive noises with a frame-level encoder and the time-invariant environmental distortions with an utterance-level encoder. We also propose a regularization method to attain clean environmental embedding that is disentangled from the utterance-dependent information such as linguistic contents and speaker characteristics. Evaluation results show that our method achieved significantly higher-quality synthetic speech than previous methods in the condition including both additive noise and reverberation."
   ],
   "doi": "10.21437/Interspeech.2022-294"
  },
  "mitsui22b_interspeech": {
   "authors": [
    [
     "Kentaro",
     "Mitsui"
    ],
    [
     "Kei",
     "Sawada"
    ]
   ],
   "title": "MSR-NV: Neural Vocoder Using Multiple Sampling Rates",
   "original": "295",
   "page_count": 5,
   "order": 161,
   "p1": 798,
   "pn": 802,
   "abstract": [
    "The development of neural vocoders (NVs) has resulted in the high-quality and fast generation of waveforms. However, conventional NVs target a single sampling rate and require re-training when applied to different sampling rates. A suitable sampling rate varies from application to application due to the trade-off between speech quality and generation speed. In this study, we propose a method to handle multiple sampling rates in a single NV, called the MSR-NV. By generating waveforms step-by-step starting from a low sampling rate, MSR-NV can efficiently learn the characteristics of each frequency band and synthesize high-quality speech at multiple sampling rates. It can be regarded as an extension of the previously proposed NVs, and in this study, we extend the structure of Parallel WaveGAN (PWG). Experimental evaluation results demonstrate that the proposed method achieves remarkably higher subjective quality than the original PWG trained separately at 16, 24, and 48 kHz, without increasing the inference time. We also show that MSR-NV can leverage speech with lower sampling rates to further improve the quality of the synthetic speech."
   ],
   "doi": "10.21437/Interspeech.2022-295"
  },
  "zhao22d_interspeech": {
   "authors": [
    [
     "Zhiyuan",
     "Zhao"
    ],
    [
     "Chuanxin",
     "Tang"
    ],
    [
     "Chengdong",
     "Yao"
    ],
    [
     "Chong",
     "Luo"
    ]
   ],
   "title": "An Anchor-Free Detector for Continuous Speech Keyword Spotting",
   "original": "296",
   "page_count": 5,
   "order": 654,
   "p1": 3228,
   "pn": 3232,
   "abstract": [
    "Continuous Speech Keyword Spotting (CSKWS) is a task to detect predefined keywords in a continuous speech. In this paper, we regard CSKWS as a one-dimensional object detection task and propose a novel anchor-free detector, named AF-KWS, to solve the problem. AF-KWS directly regresses the center locations and lengths of the keywords through a single-stage deep neural network. In particular, AF-KWS is tailored for this speech task as we introduce an auxiliary unknown class to exclude other words from non-speech or silent background. We have built two benchmark datasets named LibriTop-20 and continuous meeting analysis keywords (CMAK) dataset for CSKWS. Evaluations on these two datasets show that our proposed AF-KWS outperforms reference schemes by a large margin, and therefore provides a decent baseline for future research."
   ],
   "doi": "10.21437/Interspeech.2022-296"
  },
  "saeki22b_interspeech": {
   "authors": [
    [
     "Takaaki",
     "Saeki"
    ],
    [
     "Shinnosuke",
     "Takamichi"
    ],
    [
     "Tomohiko",
     "Nakamura"
    ],
    [
     "Naoko",
     "Tanji"
    ],
    [
     "Hiroshi",
     "Saruwatari"
    ]
   ],
   "title": "SelfRemaster: Self-Supervised Speech Restoration with Analysis-by-Synthesis Approach Using Channel Modeling",
   "original": "298",
   "page_count": 5,
   "order": 893,
   "p1": 4406,
   "pn": 4410,
   "abstract": [
    "We present a self-supervised speech restoration method without paired speech corpora. Because the previous general speech restoration method uses artificial paired data created by applying various distortions to high-quality speech corpora, it cannot sufficiently represent acoustic distortions of real data, limiting the applicability. Our model consists of analysis, synthesis, and channel modules that simulate the recording process of degraded speech and is trained with real degraded speech data in a self-supervised manner. The analysis module extracts distortionless speech features and distortion features from degraded speech, while the synthesis module synthesizes the restored speech waveform, and the channel module adds distortions to the speech waveform. Our model also enables audio effect transfer, in which only acoustic distortions are extracted from degraded speech and added to arbitrary high-quality audio. Experimental evaluations with both simulated and real data show that our method achieves significantly higher-quality speech restoration than the previous supervised method, suggesting its applicability to real degraded speech materials."
   ],
   "doi": "10.21437/Interspeech.2022-298"
  },
  "kim22d_interspeech": {
   "authors": [
    [
     "Junghun",
     "Kim"
    ],
    [
     "Yoojin",
     "An"
    ],
    [
     "Jihie",
     "Kim"
    ]
   ],
   "title": "Improving Speech Emotion Recognition Through Focus and Calibration Attention Mechanisms",
   "original": "299",
   "page_count": 5,
   "order": 28,
   "p1": 136,
   "pn": 140,
   "abstract": [
    "Attention has become one of the most commonly used mechanisms in deep learning approaches. The attention mechanism can help the system focus more on the feature space's critical regions. For example, high amplitude regions can play an important role for Speech Emotion Recognition (SER). In this paper, we identify misalignments between the attention and the signal amplitude in the existing multi-head self-attention. To improve the attention area, we propose to use a Focus-Attention (FA) mechanism and a novel Calibration-Attention (CA) mechanism in combination with the multi-head self-attention. Through the FA mechanism, the network can detect the largest amplitude part in the segment. By employing the CA mechanism, the network can modulate the information flow by assigning different weights to each attention head and improve the utilization of surrounding contexts. To evaluate the proposed method, experiments are performed with the IEMOCAP and RAVDESS datasets. Experimental results show that the proposed framework significantly outperforms the state-of-the-art approaches on both datasets."
   ],
   "doi": "10.21437/Interspeech.2022-299"
  },
  "saito22_interspeech": {
   "authors": [
    [
     "Yuki",
     "Saito"
    ],
    [
     "Yuto",
     "Nishimura"
    ],
    [
     "Shinnosuke",
     "Takamichi"
    ],
    [
     "Kentaro",
     "Tachibana"
    ],
    [
     "Hiroshi",
     "Saruwatari"
    ]
   ],
   "title": "STUDIES: Corpus of Japanese Empathetic Dialogue Speech Towards Friendly Voice Agent",
   "original": "300",
   "page_count": 5,
   "order": 1043,
   "p1": 5155,
   "pn": 5159,
   "abstract": [
    "We present STUDIES, a new speech corpus for developing a voice agent that can speak in a friendly manner. Humans naturally control their speech prosody to empathize with each other. By incorporating this ``empathetic dialogue'' behavior into a spoken dialogue system, we can develop a voice agent that can respond to a user more naturally. We designed the STUDIES corpus to include a speaker who speaks with empathy for the interlocutor's emotion explicitly. We describe our methodology to construct an empathetic dialogue speech corpus and report the analysis results of the STUDIES corpus. We conducted a text-to-speech experiment to initially investigate how we can develop more natural voice agent that can tune its speaking style corresponding to the interlocutor's emotion. The results show that the use of interlocutor's emotion label and conversational context embedding can produce speech with the same degree of naturalness as that synthesized by using the agent's emotion label. Our project page of the STUDIES corpus is \\url{http://sython.org/Corpus/STUDIES}."
   ],
   "doi": "10.21437/Interspeech.2022-300"
  },
  "koizumi22_interspeech": {
   "authors": [
    [
     "Yuma",
     "Koizumi"
    ],
    [
     "Heiga",
     "Zen"
    ],
    [
     "Kohei",
     "Yatabe"
    ],
    [
     "Nanxin",
     "Chen"
    ],
    [
     "Michiel",
     "Bacchiani"
    ]
   ],
   "title": "SpecGrad: Diffusion Probabilistic Model based Neural Vocoder with Adaptive Noise Spectral Shaping",
   "original": "301",
   "page_count": 5,
   "order": 162,
   "p1": 803,
   "pn": 807,
   "abstract": [
    "Neural vocoder using denoising diffusion probabilistic model (DDPM) has been improved by adaptation of the diffusion noise distribution to given acoustic features. In this study, we propose SpecGrad that adapts the diffusion noise so that its time-varying spectral envelope becomes close to the conditioning log-mel spectrogram. This adaptation by time-varying filtering improves the sound quality especially in the high-frequency bands. It is processed in the time-frequency domain to keep the computational cost almost the same as the conventional DDPM-based neural vocoders. Experimental results showed that SpecGrad generates higher-fidelity speech waveform than conventional DDPM-based neural vocoders in both analysis-synthesis and speech enhancement scenarios. Audio demos are available at wavegrad.github.io/specgrad/."
   ],
   "doi": "10.21437/Interspeech.2022-301"
  },
  "koizumi22b_interspeech": {
   "authors": [
    [
     "Yuma",
     "Koizumi"
    ],
    [
     "Shigeki",
     "Karita"
    ],
    [
     "Arun",
     "Narayanan"
    ],
    [
     "Sankaran",
     "Panchapagesan"
    ],
    [
     "Michiel",
     "Bacchiani"
    ]
   ],
   "title": "SNRi Target Training for Joint Speech Enhancement and Recognition",
   "original": "302",
   "page_count": 5,
   "order": 239,
   "p1": 1173,
   "pn": 1177,
   "abstract": [
    "Speech enhancement (SE) is used as a frontend in speech applications including automatic speech recognition (ASR) and telecommunication. A difficulty in using the SE frontend is that the appropriate noise reduction level differs depending on applications and/or noise characteristics. In this study, we propose \"signal-to-noise ratio improvement (SNRi) target training\"; the SE frontend is trained to output a signal whose SNRi is controlled by an auxiliary scalar input. In joint training with a backend, the target SNRi value is estimated by an auxiliary network. By training all networks to minimize the backend task loss, we can estimate the appropriate noise reduction level for each noisy input in a data-driven scheme. Our experiments showed that the SNRi target training enables control of the output SNRi. In addition, the proposed joint training relatively reduces word error rate by 4.0\\% and 5.7\\% compared to a Conformer-based standard ASR model and conventional SE-ASR joint training model, respectively. Furthermore, by analyzing the predicted target SNRi, we observed the jointly trained network automatically controls the target SNRi according to noise characteristics. Audio demos are available in our demo page (google.github.io/df-conformer/snri_target/)."
   ],
   "doi": "10.21437/Interspeech.2022-302"
  },
  "yoshinaga22_interspeech": {
   "authors": [
    [
     "Tsukasa",
     "Yoshinaga"
    ],
    [
     "Kikuo",
     "Maekawa"
    ],
    [
     "Akiyoshi",
     "Iida"
    ]
   ],
   "title": "Variability in Production of Non-Sibilant Fricative [ç] in /hi/",
   "original": "303",
   "page_count": 5,
   "order": 125,
   "p1": 620,
   "pn": 624,
   "abstract": [
    "The alveolo-palatal sibilant fricative [ɕ] in /si/ and palatal non-sibilant fricative [ç] in /hi/ are known to be distinguished by the geometrical change of the vocal tract only in the coronal direction and acoustically unstable in some Japanese speakers. In this study, we reconstructed the vocal tract geometry with three repetitive magnetic resonance imaging (MRI) on the same subject and investigated the effects of coronal vocal tract shapes on the airflow and sound characteristics in [ç]. The computational grids were constructed on each vocal tract and a numerical airflow simulation was conducted to predict the turbulent airflow and aeroacoustic sound generation. The predicted sound properties were in good agreement with the measurement of Japanese subjects. The comparison of the airflow and acoustic characteristics among three vocal tracts showed that the slight changes of the constriction area and flow channel downstream from the constriction influenced the sound source generation and peak amplitudes at around 3 kHz, indicating that the characteristic peak of [ç] was variable due to the constriction shape at the middle part of the hard palate."
   ],
   "doi": "10.21437/Interspeech.2022-303"
  },
  "makishima22_interspeech": {
   "authors": [
    [
     "Naoki",
     "Makishima"
    ],
    [
     "Satoshi",
     "Suzuki"
    ],
    [
     "Atsushi",
     "Ando"
    ],
    [
     "Ryo",
     "Masumura"
    ]
   ],
   "title": "Speaker consistency loss and step-wise optimization for semi-supervised joint training of TTS and ASR using unpaired text data",
   "original": "304",
   "page_count": 5,
   "order": 106,
   "p1": 526,
   "pn": 530,
   "abstract": [
    "In this paper, we investigate the semi-supervised joint training of text to speech (TTS) and automatic speech recognition (ASR), where a small amount of paired data and a large amount of unpaired text data are available. Conventional studies form a cycle called the TTS-ASR pipeline, where the multi-speaker TTS model synthesizes speech from text with a reference speech and the ASR model reconstructs the text from the synthesized speech, after which both models are trained with a cycle-consistency loss. However, the synthesized speech does not reflect the speaker characteristics of the reference speech and the synthesized speech becomes overly easy for the ASR model to recognize after training. This not only decreases the TTS model quality but also limits the ASR model improvement. To solve this problem, we propose improving the cycle-consistency-based training with a speaker consistency loss and step-wise optimization. The speaker consistency loss brings the speaker characteristics of the synthesized speech closer to that of the reference speech. In the step-wise optimization, we first freeze the parameter of the TTS model before both models are trained to avoid over-adaptation of the TTS model to the ASR model. Experimental results demonstrate the efficacy of the proposed method."
   ],
   "doi": "10.21437/Interspeech.2022-304"
  },
  "sanada22_interspeech": {
   "authors": [
    [
     "Yutaro",
     "Sanada"
    ],
    [
     "Takumi",
     "Nakagawa"
    ],
    [
     "Yuichiro",
     "Wada"
    ],
    [
     "Kosaku",
     "Takanashi"
    ],
    [
     "Yuhui",
     "Zhang"
    ],
    [
     "Kiichi",
     "Tokuyama"
    ],
    [
     "Takafumi",
     "Kanamori"
    ],
    [
     "Tomonori",
     "Yamada"
    ]
   ],
   "title": "Deep Self-Supervised Learning of Speech Denoising from Noisy Speeches",
   "original": "306",
   "page_count": 5,
   "order": 240,
   "p1": 1178,
   "pn": 1182,
   "abstract": [
    "In the last few years, unsupervised learning methods have been proposed in speech denoising by taking advantage of Deep Neural Networks (DNNs). The reason is that such unsupervised methods are more practical than the supervised counterparts. In our scenario, we are given a set of noisy speech data, where any two data do not share the same clean data. Our goal is to obtain the denoiser by training a DNN based model. Using the set, we train the model via the following two steps: 1) From the noisy speech data, construct another noisy speech data via our proposed masking technique. 2) Minimize our proposed loss defined from the DNN and the two noisy speech data. We evaluate our method using Gaussian and real-world noises in our numerical experiments. As a result, our method outperforms the state-of-the-art method on average for both noises. In addition, we provide the theoretical explanation of why our method can be efficient if the noise has Gaussian distribution."
   ],
   "doi": "10.21437/Interspeech.2022-306"
  },
  "liu22d_interspeech": {
   "authors": [
    [
     "Peng",
     "Liu"
    ],
    [
     "Songbin",
     "Li"
    ],
    [
     "Jigang",
     "Tang"
    ]
   ],
   "title": "An End-to-End Macaque Voiceprint Verification Method Based on Channel Fusion Mechanism",
   "original": "307",
   "page_count": 5,
   "order": 305,
   "p1": 1501,
   "pn": 1505,
   "abstract": [
    "Primates are facing a serious survival crisis. Tracking the range of animal activities and population changes is of great significance for efficient animal protection. Primates are highly alert and inaccessible to humans so that it is difficult to track animals through direct observation, DNA fingerprinting, or marking methods. Primate recognition based on animal calls has the advantages of wide monitoring range, low equipment cost, and good concealment. In this work, we propose an effective macaque speech feature extraction structure, and innovatively propose a feature fusion mechanism to effectively obtain the feature representation of each call. Furthermore, we construct a public open source macaque voiceprint verification dataset. The experimental results show that the proposed method is superior to the existing state-of-the-art human voiceprint verification algorithms with different call durations. The equal error rate (EER) of our macaque voiceprint verification algorithm reaches 6.19%."
   ],
   "doi": "10.21437/Interspeech.2022-307"
  },
  "park22_interspeech": {
   "authors": [
    [
     "Sangjun",
     "Park"
    ],
    [
     "Kihyun",
     "Choo"
    ],
    [
     "Joohyung",
     "Lee"
    ],
    [
     "Anton V.",
     "Porov"
    ],
    [
     "Konstantin",
     "Osipov"
    ],
    [
     "June Sig",
     "Sung"
    ]
   ],
   "title": "Bunched LPCNet2: Efficient Neural Vocoders Covering Devices from Cloud to Edge",
   "original": "310",
   "page_count": 5,
   "order": 163,
   "p1": 808,
   "pn": 812,
   "abstract": [
    "Text-to-Speech (TTS) services that run on edge devices have many advantages compared to cloud TTS, e.g., latency and privacy issues. However, neural vocoders with a low complexity and small model footprint inevitably generate annoying sounds. This study proposes a Bunched LPCNet2, an improved LPCNet architecture that provides highly efficient performance in high-quality for cloud servers and in a low-complexity for low-resource edge devices. Single logistic distribution achieves computational efficiency, and insightful tricks reduce the model footprint while maintaining speech quality. A DualRate architecture, which generates a lower sampling rate from a prosody model, is also proposed to reduce maintenance costs. The experiments demonstrate that Bunched LPCNet2 generates satisfactory speech quality with a model footprint of 1.1MB while operating faster than real-time on a RPi 3B. Our audio samples are available at https://srtts.github.io/bunchedLPCNet2."
   ],
   "doi": "10.21437/Interspeech.2022-310"
  },
  "segal22_interspeech": {
   "authors": [
    [
     "Yael",
     "Segal"
    ],
    [
     "Kasia",
     "Hitczenko"
    ],
    [
     "Matt",
     "Goldrick"
    ],
    [
     "Adam",
     "Buchwald"
    ],
    [
     "Angela",
     "Roberts"
    ],
    [
     "Joseph",
     "Keshet"
    ]
   ],
   "title": "DDKtor: Automatic Diadochokinetic Speech Analysis",
   "original": "311",
   "page_count": 5,
   "order": 934,
   "p1": 4611,
   "pn": 4615,
   "abstract": [
    "Diadochokinetic speech tasks (DDK), in which participants repeatedly produce syllables, are commonly used as part of the assessment of speech motor impairments. These studies rely on manual analyses that are time-intensive, subjective, and provide only a coarse-grained picture of speech. This paper presents two deep neural network models that automatically segment consonants and vowels from unannotated, untranscribed speech. Both models work on the raw waveform and use convolutional layers for feature extraction. The first model is based on an LSTM classifier followed by fully connected layers, while the second model adds more convolutional layers followed by fully connected layers. These segmentations predicted by the models are used to obtain measures of speech rate and sound duration. Results on a young healthy individuals dataset show that our LSTM model outperforms the current state-of-the-art systems and performs comparably to trained human annotators. Moreover, the LSTM model also presents comparable results to trained human annotators when evaluated on unseen older individuals with Parkinson's Disease dataset."
   ],
   "doi": "10.21437/Interspeech.2022-311"
  },
  "puffay22_interspeech": {
   "authors": [
    [
     "Corentin",
     "Puffay"
    ],
    [
     "Jana",
     "Van Canneyt"
    ],
    [
     "Jonas",
     "Vanthornhout"
    ],
    [
     "Hugo",
     "Van hamme"
    ],
    [
     "Tom",
     "Francart"
    ]
   ],
   "title": "Relating the fundamental frequency of speech with EEG using a dilated convolutional network",
   "original": "315",
   "page_count": 5,
   "order": 819,
   "p1": 4038,
   "pn": 4042,
   "abstract": [
    "To investigate how speech is processed in the brain, we can model the relation between features of a natural speech signal and the corresponding recorded electroencephalogram (EEG). Usually, linear models are used in regression tasks. Either EEG is predicted, or speech is reconstructed, and the correlation between predicted and actual signal is used to measure the brain's decoding ability. However, given the nonlinear nature of the brain, the modeling ability of linear models is limited. Recent studies introduced nonlinear models to relate the speech envelope to EEG. We set out to include other features of speech that are not coded in the envelope, notably the fundamental frequency of the voice (f0). F0 is a higher-frequency feature primarily coded at the brainstem to midbrain level. We present a dilated-convolutional model to provide evidence of neural tracking of the f0. We show that a combination of f0 and the speech envelope improves the performance of a state-of-the-art envelope-based model. This suggests the dilated-convolutional model can extract non-redundant information from both f0 and the envelope. We also show the ability of the dilated- convolutional model to generalize to subjects not included during training. This latter finding will accelerate f0-based hearing diagnosis."
   ],
   "doi": "10.21437/Interspeech.2022-315"
  },
  "iwamoto22_interspeech": {
   "authors": [
    [
     "Kazuma",
     "Iwamoto"
    ],
    [
     "Tsubasa",
     "Ochiai"
    ],
    [
     "Marc",
     "Delcroix"
    ],
    [
     "Rintaro",
     "Ikeshita"
    ],
    [
     "Hiroshi",
     "Sato"
    ],
    [
     "Shoko",
     "Araki"
    ],
    [
     "Shigeru",
     "Katagiri"
    ]
   ],
   "title": "How bad are artifacts?: Analyzing the impact of speech enhancement errors on ASR",
   "original": "318",
   "page_count": 5,
   "order": 1098,
   "p1": 5418,
   "pn": 5422,
   "abstract": [
    "It is challenging to improve automatic speech recognition (ASR) performance in noisy conditions with single-channel speech enhancement (SE). In this paper, we investigate the causes of ASR performance degradation by decomposing the SE errors using orthogonal projection-based decomposition (OPD). OPD decomposes the SE errors into noise and artifact components, which are obtained by projecting the SE errors onto (1) a speech-noise subspace spanned by the speech and noise signals and (2) a subspace orthogonal to the speech-noise subspace. We propose manually scaling the error components to analyze their impact on ASR. We experimentally identify the artifact component as the main cause of performance degradation, and we find that mitigating the artifact can greatly improve ASR performance. Furthermore, we demonstrate that the simple observation adding (OA) technique (i.e., adding a scaled version of the observed signal to the enhanced signal) can monotonically increase the signal-to-artifact ratio under a mild condition. Accordingly, we experimentally confirm that OA improves ASR performance for both simulated and real recordings. The findings of this paper provide a better understanding of the influence of SE errors on ASR and open the door to future research on novel approaches for designing effective single-channel SE front-ends for ASR."
   ],
   "doi": "10.21437/Interspeech.2022-318"
  },
  "lin22_interspeech": {
   "authors": [
    [
     "Jucai",
     "Lin"
    ],
    [
     "Tingwei",
     "Chen"
    ],
    [
     "Jingbiao",
     "Huang"
    ],
    [
     "Ruidong",
     "Fang"
    ],
    [
     "Jun",
     "Yin"
    ],
    [
     "Yuanping",
     "Yin"
    ],
    [
     "Wei",
     "Shi"
    ],
    [
     "Weizhen",
     "Huang"
    ],
    [
     "Yapeng",
     "Mao"
    ]
   ],
   "title": "The CLIPS System for 2022 Spoofing-Aware Speaker Verification Challenge",
   "original": "320",
   "page_count": 4,
   "order": 885,
   "p1": 4367,
   "pn": 4370,
   "abstract": [
    "In this paper, a spoofing-aware speaker verification (SASV) system that integrates the automatic speaker verification (ASV) system and countermeasure (CM) system is developed. Firstly, a modified re-parameterized VGG (ARepVGG) module is utilized to extract high-level representation from the multi-scale feature that learns from the raw waveform though sinc-filters, and then a spectra-temporal graph attention network is used to learn the final decision information whether the audio is spoofed or not. Secondly, a new network that is inspired from the MaxFeature-Map (MFM) layers is constructed to fine-tune the CM system while keeping the ASV system fixed. Our proposed SASV system significantly improves the SASV equal error rate (SASV-EER) from 6.73% to 1.36% on the evaluation dataset and 4.85% to 0.98% on the development dataset in the 2022 Spoofing-Aware Speaker Verification Challenge(2022 SASV)."
   ],
   "doi": "10.21437/Interspeech.2022-320"
  },
  "proszewska22_interspeech": {
   "authors": [
    [
     "Magdalena",
     "Proszewska"
    ],
    [
     "Grzegorz",
     "Beringer"
    ],
    [
     "Daniel",
     "Sáez-Trigueros"
    ],
    [
     "Thomas",
     "Merritt"
    ],
    [
     "Abdelhamid",
     "Ezzerg"
    ],
    [
     "Roberto",
     "Barra-Chicote"
    ]
   ],
   "title": "GlowVC: Mel-spectrogram space disentangling model for language-independent text-free voice conversion",
   "original": "322",
   "page_count": 5,
   "order": 603,
   "p1": 2973,
   "pn": 2977,
   "abstract": [
    "In this paper, we propose GlowVC: a multilingual multi-speaker flow-based model for language-independent text-free voice conversion. We build on Glow-TTS, which provides an architecture that enables use of linguistic features during training without the necessity of using them for VC inference. We consider two versions of our model: GlowVC-conditional and GlowVC-explicit. GlowVC-conditional models the distribution of mel-spectrograms with speaker-conditioned flow and disentangles the mel-spectrogram space into content- and pitch-relevant dimensions, while GlowVC-explicit models the explicit distribution with unconditioned flow and disentangles said space into content-, pitch- and speaker-relevant dimensions. We evaluate our models in terms of intelligibility, speaker similarity and naturalness for intra- and cross-lingual conversion in seen and unseen languages. GlowVC models greatly outperform AutoVC baseline in terms of intelligibility, while achieving just as high speaker similarity in intra-lingual VC, and slightly worse in the cross-lingual setting. Moreover, we demonstrate that GlowVC-explicit surpasses both GlowVC-conditional and AutoVC in terms of naturalness."
   ],
   "doi": "10.21437/Interspeech.2022-322"
  },
  "nihei22_interspeech": {
   "authors": [
    [
     "Fumio",
     "Nihei"
    ],
    [
     "Ryo",
     "Ishii"
    ],
    [
     "Yukiko",
     "Nakano"
    ],
    [
     "Kyosuke",
     "Nishida"
    ],
    [
     "Ryo",
     "Masumura"
    ],
    [
     "Atsushi",
     "Fukayama"
    ],
    [
     "Takao",
     "Nakamura"
    ]
   ],
   "title": "Dialogue Acts Aided Important Utterance Detection Based on Multiparty and Multimodal Information",
   "original": "324",
   "page_count": 5,
   "order": 221,
   "p1": 1086,
   "pn": 1090,
   "abstract": [
    "It has been reported that visualization of important utterances in a meeting enables efficient understanding of the meeting. Therefore, creating a model to estimate important utterances and improving its performance is an important issue. Several studies have reported that introducing auxiliary tasks as estimation targets improves the estimation performance of the main task. In this study, we develop estimation models of important utterances using dialogue acts (DAs) as an auxiliary task. The MATRICS corpus of four-party face-to-face meetings was used as the analysis data. A transformer with historical information was used for the model to estimate important utterances, and three types of modal information (text, audio, and video) were used as input data. In addition, audio and video data were separated into information about the speaker and others. As a result, the best model for important utterances was the one that used the speaker's text and audio, as well as others' audio and video data, with the assistance of DAs, with an estimation performance of 0.809 in f-measure. The results also showed that the model performed better than the one that only estimates important utterances, indicating that the assistance of DAs is effective in the estimation of important utterances."
   ],
   "doi": "10.21437/Interspeech.2022-324"
  },
  "ohsugi22_interspeech": {
   "authors": [
    [
     "Yasuhito",
     "Ohsugi"
    ],
    [
     "Itsumi",
     "Saito"
    ],
    [
     "Kyosuke",
     "Nishida"
    ],
    [
     "Sen",
     "Yoshida"
    ]
   ],
   "title": "Japanese ASR-Robust Pre-trained Language Model with Pseudo-Error Sentences Generated by Grapheme-Phoneme Conversion",
   "original": "327",
   "page_count": 5,
   "order": 546,
   "p1": 2688,
   "pn": 2692,
   "abstract": [
    "Spoken language understanding systems typically consist of a pipeline of automatic speech recognition (ASR) and natural language processing (NLP) modules. Although pre-trained language models (PLMs) have been successful in NLP by training on large corpora of written texts; spoken language with serious ASR errors that change its meaning is difficult to understand. We propose a method for pre-training Japanese LMs robust against ASR errors without using ASR. With the proposed method using written texts, sentences containing pseudo-ASR errors are generated using a pseudo-error dictionary constructed using grapheme-to-phoneme and phoneme-to-grapheme models based on neural networks. Experiments on spoken dialogue summarization showed that the ASR-robust LM pre-trained with the proposed method outperformed the LM pre-trained with standard masked language modeling by 3.17 points on ROUGE-L when fine-tuning with dialogues including ASR errors."
   ],
   "doi": "10.21437/Interspeech.2022-327"
  },
  "vaaras22_interspeech": {
   "authors": [
    [
     "Einari",
     "Vaaras"
    ],
    [
     "Manu",
     "Airaksinen"
    ],
    [
     "Okko",
     "Räsänen"
    ]
   ],
   "title": "Analysis of Self-Supervised Learning and Dimensionality Reduction Methods in Clustering-Based Active Learning for Speech Emotion Recognition",
   "original": "329",
   "page_count": 5,
   "order": 233,
   "p1": 1143,
   "pn": 1147,
   "abstract": [
    "When domain experts are needed to perform data annotation for complex machine-learning tasks, reducing annotation effort is crucial in order to cut down time and expenses. For cases when there are no annotations available, one approach is to utilize the structure of the feature space for clustering-based active learning (AL) methods. However, these methods are heavily dependent on how the samples are organized in the feature space and what distance metric is used. Unsupervised methods such as contrastive predictive coding (CPC) can potentially be used to learn organized feature spaces, but these methods typically create high-dimensional features which might be challenging for estimating data density. In this paper, we combine CPC and multiple dimensionality reduction methods in search of functioning practices for clustering-based AL. Our experiments for simulating speech emotion recognition system deployment show that both the local and global topology of the feature space can be successfully used for AL, and that CPC can be used to improve clustering-based AL performance over traditional signal features. Additionally, we observe that compressing data dimensionality does not harm AL performance substantially, and that 2-D feature representations achieved similar AL performance as higher-dimensional representations when the number of annotations is not very low."
   ],
   "doi": "10.21437/Interspeech.2022-329"
  },
  "rumberg22_interspeech": {
   "authors": [
    [
     "Lars",
     "Rumberg"
    ],
    [
     "Christopher",
     "Gebauer"
    ],
    [
     "Hanna",
     "Ehlert"
    ],
    [
     "Maren",
     "Wallbaum"
    ],
    [
     "Lena",
     "Bornholt"
    ],
    [
     "Jörn",
     "Ostermann"
    ],
    [
     "Ulrike",
     "Lüdtke"
    ]
   ],
   "title": "kidsTALC: A Corpus of 3- to 11-year-old German Children’s Connected Natural Speech",
   "original": "330",
   "page_count": 5,
   "order": 1044,
   "p1": 5160,
   "pn": 5164,
   "abstract": [
    "In this paper we present kidsTALC an audio dataset with orthographic and phonetic transcriptions of German children's speech collected to facilitate the development of speech based technological solutions. The dataset is part of a larger project aiming to develop machine-learning applications to support automation in child speech and language assessment for research and clinical purposes. At the same time, the interdisciplinary project was established to increase the accessibility of corpora of continuous child speech in Germany and globally to train accurate automated speech recognition tools for children. In the first stage we collected and transcribed 25 hours of continuous speech from typically developing children aged 3 ½–11 years. Here, we discuss the key features of the dataset, data collection, transcription protocol and future datasets in the project. We also present important statistics of our dataset and will demonstrate the speech recognition performance of one baseline model on the dataset."
   ],
   "doi": "10.21437/Interspeech.2022-330"
  },
  "rumberg22b_interspeech": {
   "authors": [
    [
     "Lars",
     "Rumberg"
    ],
    [
     "Christopher",
     "Gebauer"
    ],
    [
     "Hanna",
     "Ehlert"
    ],
    [
     "Ulrike",
     "Lüdtke"
    ],
    [
     "Jörn",
     "Ostermann"
    ]
   ],
   "title": "Improving Phonetic Transcriptions of Children’s Speech by Pronunciation Modelling with Constrained CTC-Decoding",
   "original": "332",
   "page_count": 5,
   "order": 276,
   "p1": 1357,
   "pn": 1361,
   "abstract": [
    "Language sample analysis (LSA) is a powerful tool for both therapeutic applications and research of child speech and language development. Nevertheless, it is not routinely used, due to the high cost of manual transcription and analysis. Assistance by automatic speech recognition for children has the potential to enable a wide-spread use of LSA. However, the development of modern speech recognition systems heavily relies on large scale datasets. Therefore, it faces the same obstacle of high cost for transcription as LSA itself. In this paper, we study how cheaply transcribed child speech, i. e., limited to an orthographic transcription, can be improved on a phonetic level by leveraging a CTC based automatic speech recognition model, trained on a small phonetically transcribed dataset. We constrain the CTC decoding by modeling variation of the pronunciation given the orthographic transcription using weighted finite state automata. Our experiments show that the transcription is improved in terms of phone error rate by relative 14% when applying our method. Additionally, we show how the improved transcript can in turn be leveraged to improve the training of a new model."
   ],
   "doi": "10.21437/Interspeech.2022-332"
  },
  "park22b_interspeech": {
   "authors": [
    [
     "Byeongseon",
     "Park"
    ],
    [
     "Ryuichi",
     "Yamamoto"
    ],
    [
     "Kentaro",
     "Tachibana"
    ]
   ],
   "title": "A Unified Accent Estimation Method Based on Multi-Task Learning for Japanese Text-to-Speech",
   "original": "334",
   "page_count": 5,
   "order": 391,
   "p1": 1931,
   "pn": 1935,
   "abstract": [
    "We propose a unified accent estimation method for Japanese text-to-speech (TTS). Unlike the conventional two-stage methods, which separately train two models for predicting accent phrase boundaries and accent nucleus positions, our method merges the two models and jointly optimizes the entire model in a multi-task learning framework. Furthermore, considering the hierarchical linguistic structure of intonation phrases (IPs), accent phrases, and accent nuclei, we generalize the proposed approach to simultaneously model the IP boundaries with accent information. Objective evaluation results reveal that the proposed method achieves an accent estimation accuracy of 80.4%, which is 6.67% higher than the conventional two-stage method. When the proposed method is incorporated into a neural TTS framework, the system achieves a 4.29 mean opinion score with respect to prosody naturalness."
   ],
   "doi": "10.21437/Interspeech.2022-334"
  },
  "soky22_interspeech": {
   "authors": [
    [
     "Kak",
     "Soky"
    ],
    [
     "Sheng",
     "Li"
    ],
    [
     "Masato",
     "Mimura"
    ],
    [
     "Chenhui",
     "Chu"
    ],
    [
     "Tatsuya",
     "Kawahara"
    ]
   ],
   "title": "Leveraging Simultaneous Translation for Enhancing Transcription of Low-resource Language via Cross Attention Mechanism",
   "original": "343",
   "page_count": 5,
   "order": 277,
   "p1": 1362,
   "pn": 1366,
   "abstract": [
    "This work addresses automatic speech recognition (ASR) of a low-resource language using a translation corpus, which includes the simultaneous translation of the low-resource language. In multi-lingual events such as international meetings and court proceedings, simultaneous interpretation by a human is often available for speeches of low-resource languages. In this setting, we can assume that the content of its back-translation is the same as the transcription of the original speech. Thus, the former is expected to enhance the later process. We formulate this framework as a joint process of ASR and machine translation (MT) and implement it with a combination of cross attention mechanisms of the ASR encoder and the MT encoder. We evaluate the proposed method using the spoken language translation corpus of the Extraordinary Chambers in the Courts of Cambodia (ECCC), achieving a significant improvement in the ASR word error rate (WER) of Khmer by 8.9% relative. The effectiveness is also confirmed in the Fisher-CallHome-Spanish corpus with the reduction of WER in Spanish by 1.7% relative."
   ],
   "doi": "10.21437/Interspeech.2022-343"
  },
  "martnek22_interspeech": {
   "authors": [
    [
     "Jiřı́",
     "Martı́nek"
    ],
    [
     "Christophe",
     "Cerisara"
    ],
    [
     "Pavel",
     "Kral"
    ],
    [
     "Ladislav",
     "Lenc"
    ],
    [
     "Josef",
     "Baloun"
    ]
   ],
   "title": "Weak supervision for Question Type Detection with large language models",
   "original": "345",
   "page_count": 5,
   "order": 665,
   "p1": 3283,
   "pn": 3287,
   "abstract": [
    "Large pre-trained language models (LLM) have shown remarkable Zero-Shot Learning performances in many Natural Language Processing tasks. However, designing effective prompts is still very difficult for some tasks, in particular for dialogue act recognition. We propose an alternative way to leverage pretrained LLM for such tasks that replace manual prompts with simple rules, which are more intuitive and easier to design for some tasks. We demonstrate this approach on the question type recognition task, and show that our zero-shot model obtains competitive performances both with a supervised LSTM trained on the full training corpus, and another supervised model from previously published works on the MRDA corpus. We further analyze the limits of the proposed approach, which can not be applied on any task, but may advantageously complement prompt programming for specific classes."
   ],
   "doi": "10.21437/Interspeech.2022-345"
  },
  "matsui22_interspeech": {
   "authors": [
    [
     "Sanae",
     "Matsui"
    ],
    [
     "Kyoji",
     "Iwamoto"
    ],
    [
     "Reiko",
     "Mazuka"
    ]
   ],
   "title": "Development of allophonic realization until adolescence: A production study of the affricate-fricative variation of /z/ among Japanese children",
   "original": "346",
   "page_count": 5,
   "order": 149,
   "p1": 739,
   "pn": 743,
   "abstract": [
    "The development of allophonic variants of phonemes is poorly understood. This study aimed to examine when the allophonic variants of a phoneme are realized like adults. Japanese children aged 5–13 years and adults participated in an elicited production task. We analyzed developmental changes in allophonic variation of the phoneme /z/, which is realized variably either as an affricate or a fricative. The results revealed that children aged nine years or younger realized /z/ as affricate significantly more than 13-year-old and adult speakers. Once the children reached 11 years of age, the difference compared to adults was not statistically significant, which denotes a similar developmental pattern as that of speech motor control (e.g., lip and jaw) and cognitive-linguistic skill. Moreover, we examined whether the developmental changes of allophonic realization of /z/ are due to speech rate and the time to articulate /z/. The results showed that the allophonic realization of /z/ is not affected by those factors, different from that of adults. We also found that the effects of speech rate and the time to articulate /z/ on the allophonic realization become adult-like at around 11 years of age."
   ],
   "doi": "10.21437/Interspeech.2022-346"
  },
  "bayerl22_interspeech": {
   "authors": [
    [
     "Sebastian Peter",
     "Bayerl"
    ],
    [
     "Gabriel",
     "Roccabruna"
    ],
    [
     "Shammur Absar",
     "Chowdhury"
    ],
    [
     "Tommaso",
     "Ciulli"
    ],
    [
     "Morena",
     "Danieli"
    ],
    [
     "Korbinian",
     "Riedhammer"
    ],
    [
     "Giuseppe",
     "Riccardi"
    ]
   ],
   "title": "What can Speech and Language Tell us About the Working Alliance in Psychotherapy",
   "original": "347",
   "page_count": 5,
   "order": 497,
   "p1": 2443,
   "pn": 2447,
   "abstract": [
    "We are interested in the problem of conversational analysis and its application to the health domain. Cognitive Behavioral Therapy is a structured approach in psychotherapy, allowing the therapist to help the patient to identify and modify the malicious thoughts, behavior, or actions. This cooperative effort can be evaluated using the Working Alliance Inventory Observer-rated Shortened - a 12 items inventory covering task, goal, and relationship - which has a relevant influence on therapeutic outcomes. In this work, we investigate the relation between this alliance inventory and the spoken conversations (sessions) between the patient and the psychotherapist. We have delivered eight weeks of e-therapy, collected their audio and video call sessions and manually transcribed them. The spoken conversations have been annotated and evaluated with WAI ratings by professional therapists. We have investigated speech and language features and their association with WAI items. The feature types include turn dynamics, lexical entrainment and conversational descriptors extracted from the speech and language signals. Our findings provide strong evidence that a subset of these features are strong indicators of working alliance. To the best of our knowledge, this is the first and a novel study to exploit speech and language for characterising working alliance."
   ],
   "doi": "10.21437/Interspeech.2022-347"
  },
  "xu22c_interspeech": {
   "authors": [
    [
     "Liang",
     "Xu"
    ],
    [
     "Jing",
     "Wang"
    ],
    [
     "Lizhong",
     "Wang"
    ],
    [
     "Sijun",
     "Bi"
    ],
    [
     "Jianqian",
     "Zhang"
    ],
    [
     "Qiuyue",
     "Ma"
    ]
   ],
   "title": "Human Sound Classification based on Feature Fusion Method with Air and Bone Conducted Signal",
   "original": "348",
   "page_count": 5,
   "order": 306,
   "p1": 1506,
   "pn": 1510,
   "abstract": [
    "The human sound classification task aims at distinguishing different sounds made by human, which can be widely used in medical and health detection area. Different from other sounds in acoustic scene classification task, human sounds can be transmitted either through air or bone conduction. The bone conducted (BC) signal generated by a speaker has strong anti-noise properties and can assist the air conducted (AC) signal to extract additional acoustic features. In this paper, we explore the effect of the BC signal on human sound classification task. Two stream audios combing BC and AC signals are input to a CNN-based model. An attentional feature fusion method suitable for BC and AC signal features is proposed to improve the performance according to the complementarity between the two signal features. Further improvement can be obtained by using a BC signal feature enhancement method. Experiments on an open access and a self-built dataset show that fusing bone conducted signal can achieve 6.2%/17.4% performance improvement over the baseline with only AC signal as input. The results demonstrate the application value of bone conducted signals and the superior performance of the proposed methods."
   ],
   "doi": "10.21437/Interspeech.2022-348"
  },
  "xu22d_interspeech": {
   "authors": [
    [
     "Shengyuan",
     "Xu"
    ],
    [
     "Wenxiao",
     "Zhao"
    ],
    [
     "Jing",
     "Guo"
    ]
   ],
   "title": "RefineGAN: Universally Generating Waveform Better than Ground Truth with Highly Accurate Pitch and Intensity Responses",
   "original": "349",
   "page_count": 5,
   "order": 323,
   "p1": 1591,
   "pn": 1595,
   "abstract": [
    "Most GAN(Generative Adversarial Network)-based approaches towards high-fidelity waveform generation heavily rely on discriminators to improve their performance. However, GAN methods introduce much uncertainty into the generation process and often result in mismatches of pitch and intensity, which is fatal when it comes to sensitive use cases such as singing voice synthesis(SVS). To address this problem, we propose RefineGAN, a high-fidelity neural vocoder focused on the robustness, pitch and intensity accuracy, and high-speed full-band audio generation. We applyed a pitch-guided refine architecture with a multi-scale spectrogram-based loss function to help stabilize the training process and maintain the robustness of the neural vocoder while using the GAN-based training method. Audio generated using this method shows a better performance in subjective tests when compared with the ground-truth audio. This result shows that the fidelity is even improved during the waveform reconstruction by eliminating defects produced by recording procedures. Moreover, it shows that models trained on a specified type of data can perform on totally unseen language and unseen speaker identically well. Generated sample pairs are provided on https://timedomain-tech.github.io/refinegan/."
   ],
   "doi": "10.21437/Interspeech.2022-349"
  },
  "pelloin22_interspeech": {
   "authors": [
    [
     "Valentin",
     "Pelloin"
    ],
    [
     "Franck",
     "Dary"
    ],
    [
     "Nicolas",
     "Hervé"
    ],
    [
     "Benoit",
     "Favre"
    ],
    [
     "Nathalie",
     "Camelin"
    ],
    [
     "Antoine",
     "LAURENT"
    ],
    [
     "Laurent",
     "Besacier"
    ]
   ],
   "title": "ASR-Generated Text for Language Model Pre-training Applied to Speech Tasks",
   "original": "352",
   "page_count": 5,
   "order": 699,
   "p1": 3453,
   "pn": 3457,
   "abstract": [
    "We aim at improving spoken language modeling (LM) using very large amount of automatically transcribed speech. We leverage the INA (French National Audiovisual Institute) collection and obtain 19GB of text after applying ASR on 350,000 hours of di- verse TV shows. From this, spoken language models are trained either by fine-tuning an existing LM (FlauBERT) or through training a LM from scratch. New models (FlauBERT-Oral) are shared with the community and evaluated for 3 downstream tasks: spoken language understanding, classification of TV shows and speech syntactic parsing. Results show that FlauBERT-Oral can be beneficial compared to its initial FlauBERT version demonstrating that, despite its inherent noisy nature, ASR-generated text can be used to build spoken language models."
   ],
   "doi": "10.21437/Interspeech.2022-352"
  },
  "zanonboito22_interspeech": {
   "authors": [
    [
     "Marcely",
     "Zanon Boito"
    ],
    [
     "Laurent",
     "Besacier"
    ],
    [
     "Natalia",
     "Tomashenko"
    ],
    [
     "Yannick",
     "Estève"
    ]
   ],
   "title": "A Study of Gender Impact in Self-supervised Models for Speech-to-Text Systems",
   "original": "353",
   "page_count": 5,
   "order": 260,
   "p1": 1278,
   "pn": 1282,
   "abstract": [
    "Self-supervised models for speech processing emerged recently as popular foundation blocks in speech processing pipelines. These models are pre-trained on unlabeled audio data and then used in speech processing downstream tasks such as automatic speech recognition (ASR) or speech translation (ST). Since these models are now used in research and industrial systems alike, it becomes necessary to understand the impact caused by some features such as gender distribution within pre-training data. Using French as our investigation language, we train and compare gender-specific wav2vec 2.0 models against models containing different degrees of gender balance in their pre-training data. The comparison is performed by applying these models to two speech-to-text downstream tasks: ASR and ST. Results show the type of downstream integration matters. We observe lower overall performance using gender-specific pre-training before fine-tuning an end-to-end ASR system. However, when self-supervised models are used as feature extractors, the overall ASR and ST results follow more complex patterns in which the balanced pre-trained model does not necessarily lead to the best results. Lastly, our crude 'fairness' metric, the relative performance difference measured between female and male test sets, does not display a strong variation from balanced to gender-specific pre-trained wav2vec 2.0 models."
   ],
   "doi": "10.21437/Interspeech.2022-353"
  },
  "liu22e_interspeech": {
   "authors": [
    [
     "Hexin",
     "Liu"
    ],
    [
     "Leibny Paola",
     "Garcia Perera"
    ],
    [
     "Andy",
     "Khong"
    ],
    [
     "Suzy",
     "Styles"
    ],
    [
     "Sanjeev",
     "Khudanpur"
    ]
   ],
   "title": "PHO-LID: A Unified Model Incorporating Acoustic-Phonetic and Phonotactic Information for Language Identification",
   "original": "354",
   "page_count": 5,
   "order": 455,
   "p1": 2233,
   "pn": 2237,
   "abstract": [
    "We propose a novel model to hierarchically incorporate phoneme and phonotactic information for language identification (LID) without requiring phoneme annotations for training. In this model, named PHO-LID, a self-supervised phoneme segmentation task and a LID task share a convolutional neural network (CNN) module, which encodes both language identity and sequential phonemic information in the input speech to generate an intermediate sequence of \"phonotactic” embeddings. These embeddings are then fed into transformer encoder layers for utterance-level LID. We call this architecture CNN-Trans. We evaluate it on AP17-OLR data and the MLS14 set of NIST LRE 2017, and show that the PHO-LID model with multi-task optimization exhibits the highest LID performance among all models, achieving over 40% relative improvement in terms of average cost on AP17-OLR data compared to a CNN-Trans model optimized only for LID. The visualized confusion matrices imply that our proposed method achieves higher performance on languages of the same cluster in NIST LRE 2017 data than the CNN-Trans model. A comparison between predicted phoneme boundaries and corresponding audio spectrograms illustrates the leveraging of phoneme information for LID."
   ],
   "doi": "10.21437/Interspeech.2022-354"
  },
  "lyu22_interspeech": {
   "authors": [
    [
     "Anqi",
     "Lyu"
    ],
    [
     "Zhiming",
     "Wang"
    ],
    [
     "Huijia",
     "Zhu"
    ]
   ],
   "title": "Ant Multilingual Recognition System for OLR 2021 Challenge",
   "original": "355",
   "page_count": 5,
   "order": 747,
   "p1": 3684,
   "pn": 3688,
   "abstract": [
    "This paper presents a comprehensive description of the Ant multilingual recognition system for the 6th Oriental Language Recognition(OLR 2021) Challenge. Inspired by the transfer learning scheme, the encoder components of language identification(LID) model is initialized from pretrained automatic speech recognition(ASR) networks for integrating the lexical phonetic information into language identification. The ASR model is encoder-decoder networks based on U2++ architecture; then inheriting the shared conformer encoder from pretrained ASR model which is effective at global information capturing and local invariance modeling, the LID model, with an attentive statistical pooling layer and a following linear projection layer added on the encoder, is further finetuned until its optimum. Furthermore, data augmentation, score normalization and model ensemble are good strategies to improve performance indicators, which are investigated and analysed in detail within our paper. In the OLR 2021 Challenge, our submitted systems ranked the top in both tasks 1 and 2 with primary metrics of 0.0025 and 0.0039 respectively, less than 1/3 of the second place, which fully illustrates that our methodologies for multilingual identification are effectual and competitive in real-life scenarios."
   ],
   "doi": "10.21437/Interspeech.2022-355"
  },
  "weise22_interspeech": {
   "authors": [
    [
     "Tobias",
     "Weise"
    ],
    [
     "Philipp",
     "Klumpp"
    ],
    [
     "Andreas",
     "Maier"
    ],
    [
     "Elmar",
     "Nöth"
    ],
    [
     "Björn",
     "Heismann"
    ],
    [
     "Maria",
     "Schuster"
    ],
    [
     "Seung Hee",
     "Yang"
    ]
   ],
   "title": "Disentangled Latent Speech Representation for Automatic Pathological Intelligibility Assessment",
   "original": "356",
   "page_count": 5,
   "order": 975,
   "p1": 4815,
   "pn": 4819,
   "abstract": [
    "Speech intelligibility assessment plays an important role in the therapy of patients suffering from pathological speech disorders. Automatic and objective measures are desirable to assist therapists in their traditionally subjective and labor-intensive assessments. In this work, we investigate a novel approach for obtaining such a measure using the divergence in disentangled latent speech representations of a parallel utterance pair, obtained from a healthy reference and a pathological speaker. Experiments on an English database of Cerebral Palsy patients, using all available utterances per speaker, show high and significant correlation values (R=-0.9) with subjective intelligibility measures, while having only minimal deviation (+- 0.01) across four different reference speaker pairs. We also demonstrate the robustness of the proposed method (R=-0.89 deviating +- 0.02 over 1000 iterations) by considering a significantly smaller amount of utterances per speaker. Our results are among the first to show that disentangled speech representations can be used for automatic pathological speech intelligibility assessment, resulting in a reference speaker pair invariant method, applicable in scenarios with only few utterances available."
   ],
   "doi": "10.21437/Interspeech.2022-356"
  },
  "lee22b_interspeech": {
   "authors": [
    [
     "Munhak",
     "Lee"
    ],
    [
     "Joon-Hyuk",
     "Chang"
    ],
    [
     "Sang-Eon",
     "Lee"
    ],
    [
     "Ju-Seok",
     "Seong"
    ],
    [
     "Chanhee",
     "Park"
    ],
    [
     "Haeyoung",
     "Kwon"
    ]
   ],
   "title": "Regularizing Transformer-based Acoustic Models by Penalizing Attention Weights",
   "original": "362",
   "page_count": 5,
   "order": 12,
   "p1": 56,
   "pn": 60,
   "abstract": [
    "The application of deep learning has significantly advanced the performance of automatic speech recognition (ASR) systems. Various components make up an ASR system, such as the acoustic model (AM), language model, and lexicon. Generally, the AM has benefited the most from deep learning. Numerous types of neural network-based AMs have been studied, but the structure that has received the most attention in recent years is the Transformer. In this study, we demonstrate that the Transformer model is more vulnerable to input sparsity compared to the convolutional neural network (CNN) and analyze the cause of performance degradation through structural characteristics of the Transformer. Moreover, we also propose a novel regularization method that makes the transformer model robust against input sparsity. The proposed sparsity regularization method directly regulates attention weights using silence label information in forced-alignment and has the advantage of not requiring additional module training and excessive computation. We tested the proposed method on five benchmarks and observed an average relative error rate reduction (RERR) of 4.7%."
   ],
   "doi": "10.21437/Interspeech.2022-362"
  },
  "karlapati22_interspeech": {
   "authors": [
    [
     "Sri",
     "Karlapati"
    ],
    [
     "Penny",
     "Karanasou"
    ],
    [
     "Mateusz",
     "Łajszczak"
    ],
    [
     "Syed",
     "Ammar Abbas"
    ],
    [
     "Alexis",
     "Moinet"
    ],
    [
     "Peter",
     "Makarov"
    ],
    [
     "Ray",
     "Li"
    ],
    [
     "Arent",
     "van Korlaar"
    ],
    [
     "Simon",
     "Slangen"
    ],
    [
     "Thomas",
     "Drugman"
    ]
   ],
   "title": "CopyCat2: A Single Model for Multi-Speaker TTS and Many-to-Many Fine-Grained Prosody Transfer",
   "original": "367",
   "page_count": 5,
   "order": 681,
   "p1": 3363,
   "pn": 3367,
   "abstract": [
    "In this paper, we present CopyCat2 (CC2), a novel model capable of: a) synthesizing speech with different speaker identities, b) generating speech with expressive and contextually appropriate prosody, and c) transferring prosody at fine-grained level between any pair of seen speakers. We do this by activating distinct parts of the network for different tasks. We train our model using a novel approach to two-stage training. In Stage I, the model learns speaker-independent word-level prosody representations from speech which it uses for many-to-many fine-grained prosody transfer. In Stage II, we learn to predict these prosody representations using the contextual information available in text, thereby, enabling multi-speaker TTS with contextually appropriate prosody. We compare CC2 to two strong baselines, one in TTS with contextually appropriate prosody, and one in fine-grained prosody transfer. CC2 reduces the gap in naturalness between our baseline and copy-synthesised speech by 22.79%. In fine-grained prosody transfer evaluations, it obtains a relative improvement of 33.15% in target speaker similarity."
   ],
   "doi": "10.21437/Interspeech.2022-367"
  },
  "neijman22_interspeech": {
   "authors": [
    [
     "Marise",
     "Neijman"
    ],
    [
     "Femke",
     "Hof"
    ],
    [
     "Noelle",
     "Oosterom"
    ],
    [
     "Roland",
     "Pfau"
    ],
    [
     "Bertus",
     "van Rooy"
    ],
    [
     "Rob J.J.H.",
     "van Son"
    ],
    [
     "Michiel M.W.M.",
     "van den Brekel"
    ]
   ],
   "title": "Compensation in Verbal and Nonverbal Communication after Total Laryngectomy",
   "original": "369",
   "page_count": 5,
   "order": 731,
   "p1": 3613,
   "pn": 3617,
   "abstract": [
    "Total laryngectomy is a major surgical procedure with life-changing consequences. As a result of the surgery, the upper and lower airways are disconnected, the natural voice is lost, and patients breathe through a tracheostoma in the neck. Tracheoesophageal speech is the most common speech rehabilitation technique. Due to the lack of air volume, and the amount of muscle tension in the esophagus, some patients may suffer from a hyper- or hypo-tonic voice, resulting in less intelligible speech. To communicate as intelligibly as possible, patients likely adapt their verbal and nonverbal communication to their physical disabilities. The current study aimed to explore the compensation techniques in verbal and nonverbal communication after total laryngectomy focusing on the complexity of grammar and the use of co-speech gestures. We analyzed previously obtained interviews of eight laryngectomized women on the syntactic complexity in speech and the use and type of co-speech gestures. Results were compared with analyses of productions by healthy controls. We found that laryngectomized women reduce the syntactic complexity of their speech, and use nonverbal gestures in their communication. Further research is needed with systematically obtained data and more suitable match-groups. Index Terms: total laryngectomy, communication, speech, co-speech gestures, grammar, compensation"
   ],
   "doi": "10.21437/Interspeech.2022-369"
  },
  "deseyssel22_interspeech": {
   "authors": [
    [
     "Maureen",
     "de Seyssel"
    ],
    [
     "Marvin",
     "Lavechin"
    ],
    [
     "Yossi",
     "Adi"
    ],
    [
     "Emmanuel",
     "Dupoux"
    ],
    [
     "Guillaume",
     "Wisniewski"
    ]
   ],
   "title": "Probing phoneme, language and speaker information in unsupervised speech representations",
   "original": "373",
   "page_count": 5,
   "order": 285,
   "p1": 1402,
   "pn": 1406,
   "abstract": [
    "Unsupervised models of representations based on Contrastive Predictive Coding (CPC)[1] are primarily used in spoken language modelling in that they encode phonetic information. In this study, we ask what other types of information are present in CPC speech representations. We focus on three categories: phone class, gender and language, and compare monolingual and bilingual models. Using qualitative and quantitative tools, we find that both gender and phone class information are present in both types of models. Language information, however, is very salient in the bilingual model only, suggesting CPC models learn to discriminate languages when trained on multiple languages. Some language information can also be retrieved from monolingual models, but it is more diffused across all features. These patterns hold when analyses are carried on the discrete units from a downstream clustering model. However, although there is no effect of the number of target clusters on phone class and language information, more gender information is encoded with more clusters. Finally, we find that there is some cost to being exposed to two languages on a downstream phoneme discrimination task."
   ],
   "doi": "10.21437/Interspeech.2022-373"
  },
  "salim22_interspeech": {
   "authors": [
    [
     "Shinimol",
     "Salim"
    ],
    [
     "Syed",
     "Shahnawazuddin"
    ],
    [
     "Waquar",
     "Ahmad"
    ]
   ],
   "title": "Automatic Speaker Verification System for Dysarthria Patients",
   "original": "375",
   "page_count": 5,
   "order": 1026,
   "p1": 5070,
   "pn": 5074,
   "abstract": [
    "Dysarthria is one of the most common speech communication disorder associate with a neurological damage that weakens the muscles necessary for speech. In this paper, we present our efforts towards developing an automatic speaker verification (ASV) system based on x-vectors for dysarthric speakers with varying speech intelligibility (low, medium and high). For that purpose, a baseline ASV system was trained on speech data from healthy speakers since there is severe scarcity of data from dysarthric speakers. To improve the performance with respect to dysarthric speakers, data augmentation based on duration modification is proposed in this study. Duration modification with several scaling factors was applied to healthy training speech. An ASV was then trained on healthy speech augmented with its duration modified versions. It compensates for the substantial disparities in phone duration between normal and dysarthric speakers of varying speech intelligibilty. Experiment evaluations presented in this study show that proposed duration modification-based data augmentation resulted in a relative improvement of 22% over the baseline. Further to that, a relative improvement of 26% was obtained in the case of speakers with high severity level of dysarthria."
   ],
   "doi": "10.21437/Interspeech.2022-375"
  },
  "fernau22_interspeech": {
   "authors": [
    [
     "Daniel",
     "Fernau"
    ],
    [
     "Stefan",
     "Hillmann"
    ],
    [
     "Nils",
     "Feldhus"
    ],
    [
     "Tim",
     "Polzehl"
    ]
   ],
   "title": "Towards Automated Dialog Personalization using MBTI Personality Indicators",
   "original": "376",
   "page_count": 5,
   "order": 402,
   "p1": 1968,
   "pn": 1972,
   "abstract": [
    "As conversational interfaces mature in both capacity and usage, the need to personalize towards specific user characteristics becomes apparent, in order to improve users' acceptance, satisfaction and trust in the conversations. We utilize the concept of Myers-Briggs personality type indicators in order to adapt chatbot behavior. In a user study, we investigate the impact and realization of the so-called ``law of attraction'' by providing users with a chatbot that mirrors their own personality. This entails predicting the personality from the user behavior, in this work chat messages, by utilizing a pre-trained language model rather than composing many resources like lexicons. We conduct a user study with aligned and misaligned personality and analyze the effect on usability. Results show that alignment significantly improves major usability factors such as satisfaction, perceived naturalness, recommendation likelihood, appropriateness and trustworthiness of our interaction. Further, comparing different language models, contrastive learning approaches outperform previous methods. Predicting the thinking vs. feeling and introversion vs. extroversion indicator dichotomies, we achieve 76.14% f1 and 69.11 f1, respectively, with setting a new state-of-the-art performance in the literature for the former. Finally, our work adds transparency to the design of linguistic personality cues, hitherto rarely reported in the literature."
   ],
   "doi": "10.21437/Interspeech.2022-376"
  },
  "he22_interspeech": {
   "authors": [
    [
     "YUHANG",
     "HE"
    ],
    [
     "Andrew",
     "Markham"
    ]
   ],
   "title": "SoundDoA: Learn Sound Source Direction of Arrival and Semantics from Sound Raw Waveforms",
   "original": "378",
   "page_count": 5,
   "order": 490,
   "p1": 2408,
   "pn": 2412,
   "abstract": [
    "A fundamental task for an agent to understand an environment acoustically is to detect sound source locationand semantic label. It is a challenging task: firstly, sound sources overlap in time, frequency and space; secondly, while semantics are largely conveyed through time-frequency energy~(amplitude) contours, DoA is encoded in inter-channel phase difference; lastly, although the number of microphone sensors are sparse, recorded sound waveform is temporally dense due to the high sampling rates. Existing methods for predicting DoA mostly depend on pre-extracted 2D acoustic feature such as GCC-PHAT and Mel spectrograms so as to benefit from the success of mature 2D image based deep neural networks. We instead propose a novel end-to-end trainable framework, named \\textSoundDoA}, that is capable of learning sound source DoA directly from sound raw waveforms. We first use a learnable front-end to dynamically encode sound source semantics and DoA relevant features into a compact representation. A backbone network consisting of two identical sub-networks with layerwise communication is then proposed to further learn semantic label and DoA both separately and jointly. Extensive experimental results on DCASE 2020 sound event detection and localization dataset demonstrate the superiority of SoundDoA, when comparing with other existing methods"
   ],
   "doi": "10.21437/Interspeech.2022-378"
  },
  "makarov22_interspeech": {
   "authors": [
    [
     "Peter",
     "Makarov"
    ],
    [
     "Syed",
     "Ammar Abbas"
    ],
    [
     "Mateusz",
     "Łajszczak"
    ],
    [
     "Arnaud",
     "Joly"
    ],
    [
     "Sri",
     "Karlapati"
    ],
    [
     "Alexis",
     "Moinet"
    ],
    [
     "Thomas",
     "Drugman"
    ],
    [
     "Penny",
     "Karanasou"
    ]
   ],
   "title": "Simple and Effective Multi-sentence TTS with Expressive and Coherent Prosody",
   "original": "379",
   "page_count": 5,
   "order": 682,
   "p1": 3368,
   "pn": 3372,
   "abstract": [
    "Generating expressive and contextually appropriate prosody remains a challenge for modern text-to-speech (TTS) systems. This is particularly evident for long, multi-sentence inputs. In this paper, we examine simple extensions to a Transformer-based FastSpeech-like system, with the goal of improving prosody for multi-sentence TTS. We find that long context, powerful text features, and training on multi-speaker data all improve prosody. More interestingly, they result in synergies. Long context disambiguates prosody, improves coherence, and plays to the strengths of Transformers. Fine-tuning word-level features from a powerful language model, such as BERT, appears to profit from more training data, readily available in a multi-speaker setting. We look into objective metrics on pausing and pacing and perform thorough subjective evaluations for speech naturalness. Our main system, which incorporates all the extensions, achieves consistently strong results, including statistically significant improvements in speech naturalness over all its competitors."
   ],
   "doi": "10.21437/Interspeech.2022-379"
  },
  "lee22c_interspeech": {
   "authors": [
    [
     "Jihyun",
     "Lee"
    ],
    [
     "Gary Geunbae",
     "Lee"
    ]
   ],
   "title": "SF-DST: Few-Shot Self-Feeding Reading Comprehension Dialogue State Tracking with Auxiliary Task",
   "original": "380",
   "page_count": 5,
   "order": 251,
   "p1": 1233,
   "pn": 1237,
   "abstract": [
    "Few-shot dialogue state tracking (DST) model tracks user requests in dialogue with reliable accuracy even with a small amount of data. In this paper, we introduce an ontology-free few-shot DST with self-feeding belief state input. The self-feeding belief state input increases the accuracy in multi-turn dialogue by summarizing previous dialogue. Also, we newly developed a slot-gate auxiliary task. This new auxiliary task helps classify whether a slot is mentioned in the dialogue. Our model achieved the best score in a few-shot setting for four domains on multiWOZ 2.0."
   ],
   "doi": "10.21437/Interspeech.2022-380"
  },
  "pupier22_interspeech": {
   "authors": [
    [
     "Adrien",
     "Pupier"
    ],
    [
     "Maximin",
     "Coavoux"
    ],
    [
     "Benjamin",
     "Lecouteux"
    ],
    [
     "Jerome",
     "Goulian"
    ]
   ],
   "title": "End-to-End Dependency Parsing of Spoken French",
   "original": "381",
   "page_count": 5,
   "order": 368,
   "p1": 1816,
   "pn": 1820,
   "abstract": [
    "Research efforts in syntactic parsing have focused on written texts. As a result, speech parsing is usually performed on transcriptions, either in unrealistic settings (gold transcriptions) or on predicted transcriptions. Parsing speech from transcriptions, though straightforward to implement using out-of-the-box tools for Automatic Speech Recognition (ASR) and dependency parsing has two important limitations. First, relying on transcriptions will lead to error propagation due to recognition mistakes. Secondly, many acoustic cues that are important for parsing (prosody, pauses, ...) are no longer available in transcriptions. To address these limitations, we introduce wav2tree, an end-to-end dependency parsing model whose only input is the raw signal. Our model builds on a pretrained wav2vec2 encoder with a CTC loss to perform ASR. We extract token segmentation from the CTC layer to construct vector representations for each predicted token. Then, we use these token representations as input to a generic parsing algorithm. The whole model is trained end-to-end with a multitask objective (ASR, parsing) to reduce error propagation. Our experiments on the Orfeo treebank of spoken French show that direct parsing from speech is feasible: wav2tree outperforms a pipeline approach based on wav2vec (for ASR) and flauBERT (for parsing)."
   ],
   "doi": "10.21437/Interspeech.2022-381"
  },
  "frost22_interspeech": {
   "authors": [
    [
     "Geoffrey T.",
     "Frost"
    ],
    [
     "Grant",
     "Theron"
    ],
    [
     "Thomas",
     "Niesler"
    ]
   ],
   "title": "TB or not TB? Acoustic cough analysis for tuberculosis classification",
   "original": "383",
   "page_count": 5,
   "order": 498,
   "p1": 2448,
   "pn": 2452,
   "abstract": [
    "In this work, we explore recurrent neural network architectures for tuberculosis (TB) cough classification. In contrast to previous unsuccessful attempts to implement deep architectures in this domain, we show that a basic bidirectional long short-term memory network (BiLSTM) can achieve improved performance. In addition, we show that by performing greedy feature selection in conjunction with a newly-proposed attention-based architecture that learns patient invariant features, substantially better generalisation can be achieved compared to a baseline and other considered architectures. Furthermore, this attention mechanism allows an inspection of the temporal regions of the audio signal considered to be important for classification to be performed. Finally, we develop a neural style transfer technique to infer idealised inputs which can subsequently be analysed. We find distinct differences between the idealised power spectra of TB and non-TB coughs, which provide clues about the origin of the features in the audio signal."
   ],
   "doi": "10.21437/Interspeech.2022-383"
  },
  "ammarabbas22_interspeech": {
   "authors": [
    [
     "Syed",
     "Ammar Abbas"
    ],
    [
     "Thomas",
     "Merritt"
    ],
    [
     "Alexis",
     "Moinet"
    ],
    [
     "Sri",
     "Karlapati"
    ],
    [
     "Ewa",
     "Muszynska"
    ],
    [
     "Simon",
     "Slangen"
    ],
    [
     "Elia",
     "Gatti"
    ],
    [
     "Thomas",
     "Drugman"
    ]
   ],
   "title": "Expressive, Variable, and Controllable Duration Modelling in TTS",
   "original": "384",
   "page_count": 5,
   "order": 921,
   "p1": 4546,
   "pn": 4550,
   "abstract": [
    "Duration modelling has become an important research problem once more with the rise of non-attention neural text-to-speech systems. The current approaches largely fall back to relying on previous statistical parametric speech synthesis technology for duration prediction, which poorly models the expressiveness and variability in speech. In this paper, we propose two alternate approaches to improve duration modelling. First, we propose a duration model conditioned on phrasing that improves the predicted durations and provides better modelling of pauses. We show that the duration model conditioned on phrasing improves the naturalness of speech over our baseline duration model. Second, we also propose a multi-speaker duration model called Cauliflow, that uses normalising flows to predict durations that better match the complex target duration distribution. Cauliflow performs on par with our other proposed duration model in terms of naturalness, whilst providing variable durations for the same prompt and variable levels of expressiveness. Lastly, we propose to condition Cauliflow on parameters that provide an intuitive control of the pacing and pausing in the synthesised speech in a novel way."
   ],
   "doi": "10.21437/Interspeech.2022-384"
  },
  "cattan22_interspeech": {
   "authors": [
    [
     "Oralie",
     "Cattan"
    ],
    [
     "Sahar",
     "Ghannay"
    ],
    [
     "Christophe",
     "Servan"
    ],
    [
     "Sophie",
     "Rosset"
    ]
   ],
   "title": "Benchmarking Transformers-based models on French Spoken Language Understanding tasks",
   "original": "385",
   "page_count": 5,
   "order": 252,
   "p1": 1238,
   "pn": 1242,
   "abstract": [
    "In the last five years, the rise of the self-attentional Transformer-based architectures led to state-of-the-art performances over many natural language tasks. Although these approaches are increasingly popular, they require large amounts of data and computational resources. There is still a substantial need for benchmarking methodologies ever upwards on under-resourced languages in data-scarce application conditions. Most pre-trained language models were massively studied using the English language and only a few of them were evaluated on French. In this paper, we propose a unified benchmark, focused on evaluating models quality and their ecological impact on two well-known French spoken language understanding tasks. Especially we benchmark thirteen well-established Transformer-based models on the two available spoken language understanding tasks for French: MEDIA and ATIS-FR. Within this framework, we show that compact models can reach comparable results to bigger ones while their ecological impact is considerably lower. However, this assumption is nuanced and depends on the considered compression method."
   ],
   "doi": "10.21437/Interspeech.2022-385"
  },
  "birladeanu22_interspeech": {
   "authors": [
    [
     "Andrei",
     "Bîrlădeanu"
    ],
    [
     "Helen",
     "Minnis"
    ],
    [
     "Alessandro",
     "Vinciarelli"
    ]
   ],
   "title": "Automatic Detection of Reactive Attachment Disorder Through Turn-Taking Analysis in Clinical Child-Caregiver Sessions",
   "original": "387",
   "page_count": 4,
   "order": 286,
   "p1": 1407,
   "pn": 1410,
   "abstract": [
    "To the best of our knowledge, this is the first work aimed at automatic detection of Reactive Attachment Disorder, a psychiatric issue typically affecting children that experienced abuse and neglect. The proposed approach is based on the analysis of turn-taking during clinical sessions and the exper- iments involved 61 children and their caregivers. The results show that it is possible to detect the pathology with accuracy up to 69.2% (F1 Score 68.8%). In addition, the experiments show that the pathology tends to leave different behavioral traces in different activities. This might explain why Reactive Attachment Disorder is difficult to diagnose and tends to re- main undetected. In such a context, methodologies like those proposed in this work can be a valuable support in clinical practice."
   ],
   "doi": "10.21437/Interspeech.2022-387"
  },
  "fernandez22_interspeech": {
   "authors": [
    [
     "Raul",
     "Fernandez"
    ],
    [
     "David",
     "Haws"
    ],
    [
     "Guy",
     "Lorberbom"
    ],
    [
     "Slava",
     "Shechtman"
    ],
    [
     "Alexander",
     "Sorin"
    ]
   ],
   "title": "Transplantation of Conversational Speaking Style with Interjections in Sequence-to-Sequence Speech Synthesis",
   "original": "388",
   "page_count": 5,
   "order": 1112,
   "p1": 5488,
   "pn": 5492,
   "abstract": [
    "Sequence-to-Sequence Text-to-Speech architectures that directly generate low level acoustic features from phonetic sequences are known to produce natural and expressive speech when provided with adequate amounts of training data. Such systems can learn and transfer desired speaking styles from one seen speaker to another (in multi-style multi-speaker settings), which is highly desirable for creating scalable and customizable Human-Computer Interaction systems. In this work we explore one-to-many style transfer from a dedicated single-speaker conversational corpus with style nuances and interjections. We elaborate on the corpus design and explore the feasibility of such style transfer when assisted with Voice-Conversion-based data augmentation. In a set of subjective listening experiments, this approach resulted in high-fidelity style transfer with no quality degradation. However, a certain voice persona shift was observed, requiring further improvements in voice conversion."
   ],
   "doi": "10.21437/Interspeech.2022-388"
  },
  "pundak22_interspeech": {
   "authors": [
    [
     "Golan",
     "Pundak"
    ],
    [
     "Tsendsuren",
     "Munkhdalai"
    ],
    [
     "Khe Chai",
     "Sim"
    ]
   ],
   "title": "On-the-fly ASR Corrections with Audio Exemplars",
   "original": "389",
   "page_count": 5,
   "order": 638,
   "p1": 3148,
   "pn": 3152,
   "abstract": [
    "On-device end-to-end (E2E) models are required to handle long-tail vocabulary and a large number of acoustic conditions. With finite amount of training data some of these conditions and vocabulary words are unseen during training, which often leads to recognition errors. Text-based contextual biasing is intended to mitigate this problem, yet it works well only when sufficient textual context is provided, and when the speech signal is well modeled by the ASR system. In this work, we propose to extend biasing to operate directly in the audio domain. We address a scenario where audio samples and the associated transcriptions are available, as is the case of manually corrected voice typing. We propose to directly compare incoming audio embeddings against a list of Audio Exemplars (AE), each associated with a text correction. We demonstrate the effectiveness of our approach by correcting the outputs of a production-quality RNNT model, which results in relative-WER reduction of 21.7% (one-shot) and 33.7% (multi-shot) on the Wiki-Names data set."
   ],
   "doi": "10.21437/Interspeech.2022-389"
  },
  "chan22_interspeech": {
   "authors": [
    [
     "David",
     "Chan"
    ],
    [
     "Shalini",
     "Ghosh"
    ]
   ],
   "title": "Content-Context Factorized Representations for Automated Speech Recognition",
   "original": "390",
   "page_count": 5,
   "order": 13,
   "p1": 61,
   "pn": 65,
   "abstract": [
    "Deep neural networks have largely demonstrated their ability to perform automated speech recognition (ASR) by extracting meaningful features from input audio frames. Such features, however, may consist not only of information about the spoken language content, but also may contain information about unnecessary contexts such as background noise and sounds or speaker identity, accent, or protected attributes. Such information can directly harm generalization performance, by introducing spurious correlations between the spoken words and the context in which such words were spoken. In this work, we introduce an unsupervised, encoder-agnostic method for factoring speech-encoder representations into explicit content-encoding representations and spurious context-encoding representations. By doing so, we demonstrate improved performance on standard ASR benchmarks, as well as improved performance in both real-world and artificially noisy ASR scenarios."
   ],
   "doi": "10.21437/Interspeech.2022-390"
  },
  "soltau22_interspeech": {
   "authors": [
    [
     "Hagen",
     "Soltau"
    ],
    [
     "Izhak",
     "Shafran"
    ],
    [
     "Mingqiu",
     "Wang"
    ],
    [
     "Laurent El",
     "Shafey"
    ]
   ],
   "title": "RNN Transducers for Named Entity Recognition with constraints on alignment for understanding medical conversations",
   "original": "391",
   "page_count": 5,
   "order": 385,
   "p1": 1901,
   "pn": 1905,
   "abstract": [
    "Understanding medical conversations requires detecting entities such as Medications, Symptoms, Treatment, Conditions and Diagnosis, which leads to large ontologies with overlapping spans.Popular solutions to Named Entity Recognition (NER) such as conditional random fields, sequence-to-sequence models, or the question-answering framework are not suitable for this task. We address this problem by proposing a new model for NER task -- an RNN transducer, which has hitherto been used only in speech recognition. These models are trained using paired input and output sequences without explicitly specifying the alignment between them, similar to other seq-to-seq models. In NER tasks, however, the alignment between words and labels are available from the human annotations. We propose a fixed alignment model that utilizes the given alignment, while preserving the benefits of RNN-Ts such as modeling output dependencies. We also propose a constrained alignment model where users can specify a relaxation and the model will learn an alignment within the given constraints. In other words, we propose a family of seq-to-seq models which can leverage alignments between input and target sequences when available. Through empirical experiments on a challenging real-world medical NER task with multiple ontologies, we demonstrate that our fixed alignment model outperforms the standard RNN-T model."
   ],
   "doi": "10.21437/Interspeech.2022-391"
  },
  "meneses22_interspeech": {
   "authors": [
    [
     "Michel Cardoso",
     "Meneses"
    ],
    [
     "Rafael Bérgamo",
     "Holanda"
    ],
    [
     "Luis Vasconcelos",
     "Peres"
    ],
    [
     "Gabriela Dantas",
     "Rocha"
    ]
   ],
   "title": "SiDi KWS: A Large-Scale Multilingual Dataset for Keyword Spotting",
   "original": "394",
   "page_count": 5,
   "order": 935,
   "p1": 4616,
   "pn": 4620,
   "abstract": [
    "Keyword spotting (KWS) has become a hot topic in speech processing due to the rise of commercial applications based on voice command detection, such as voice assistants. Like tasks in computer vision, natural language processing, and even speech processing, most current successful approaches for KWS rely on deep learning. However, differently from all those tasks, there is a lack of large-scale datasets designed for training and evaluating deep learning models for KWS. The current work presents SiDi KWS, a public large-scale multilingual dataset currently composed of 24.3 million audio recordings of labeled single-spoken keywords. It intends to boost the development of new KWS systems, especially those based on deep learning. That dataset has been created by applying automatic forced alignment on public datasets of transcribed speech. This work introduces SiDi KWS and KeywordMiner, an open-source framework used to generate that dataset, to benefit the speech processing research community."
   ],
   "doi": "10.21437/Interspeech.2022-394"
  },
  "tsukada22_interspeech": {
   "authors": [
    [
     "Kimiko",
     "Tsukada"
    ],
    [
     "Yurong",
     "Yurong"
    ]
   ],
   "title": "Non-native Perception of Japanese Singleton/Geminate Contrasts: Comparison of Mandarin and Mongolian Speakers Differing in Japanese Experience",
   "original": "397",
   "page_count": 5,
   "order": 622,
   "p1": 3068,
   "pn": 3072,
   "abstract": [
    "Japanese is a quantity language that uses durational variation contrastively for both vowels and consonants. Therefore, correct use of contrastive length is important for efficient communication. However, it is widely acknowledged that length contrast is difficult for non-native speakers from diverse first language (L1) backgrounds. To examine the effects of L1 and Japanese learning experience on consonant length (i.e., short/singleton vs long/geminate) processing, we compared the perception of Japanese singleton/geminate by native speakers of Mandarin and Mongolian at different levels of Japanese experience. A group of 10 native Japanese speakers participated as controls. Neither Mandarin nor Mongolian uses consonant length contrastively, but Mongolian uses vowel length contrastively. Further, unlike Japanese and Mandarin, Mongolian frequently uses closed syllables and permits a wide range of consonants in coda position. The participants responded to 200 trials via the AXB discrimination task. All Mandarin groups, including the most advanced one, were significantly less accurate than the Japanese group. On the other hand, none of the Mongolian groups, including the one without Japanese experience, differed from the Japanese group. The most advanced Mandarin group was the closest to (and did not significantly differ from) the Mongolian groups, showing a positive effect of Japanese learning."
   ],
   "doi": "10.21437/Interspeech.2022-397"
  },
  "lu22_interspeech": {
   "authors": [
    [
     "Zhiyun",
     "Lu"
    ],
    [
     "Yongqiang",
     "Wang"
    ],
    [
     "Yu",
     "Zhang"
    ],
    [
     "Wei",
     "Han"
    ],
    [
     "Zhehuai",
     "Chen"
    ],
    [
     "Parisa",
     "Haghani"
    ]
   ],
   "title": "Unsupervised Data Selection via Discrete Speech Representation for ASR",
   "original": "399",
   "page_count": 5,
   "order": 687,
   "p1": 3393,
   "pn": 3397,
   "abstract": [
    "Self-supervised learning of speech representations has achieved impressive results in improving automatic speech recognition (ASR). In this paper, we show that data selection is important for self-supervised learning. We propose a simple and effective unsupervised data selection method which selects acoustically similar speech to a target domain. It takes the discrete speech representation available in common self-supervised learning frameworks as input, and applies a contrastive data selection method on the discrete tokens. Through extensive empirical studies we show that our proposed method reduces the amount of required pre-training data and improves the downstream ASR performance. Pre-training on a selected subset of 6% of the general data pool results in 11.8% relative improvements in LibriSpeech test-other compared to pre-training on the full set. On Multilingual LibriSpeech French, German, and Spanish test sets, selecting 6% data for pre-training reduces word error rate by more than 15% relatively compared to the full set, and achieves competitive results compared to current state-of-the-art performances."
   ],
   "doi": "10.21437/Interspeech.2022-399"
  },
  "olivier22_interspeech": {
   "authors": [
    [
     "Raphael",
     "Olivier"
    ],
    [
     "Bhiksha",
     "Raj"
    ]
   ],
   "title": "Recent improvements of ASR models in the face of adversarial attacks",
   "original": "400",
   "page_count": 5,
   "order": 834,
   "p1": 4113,
   "pn": 4117,
   "abstract": [
    "Like many other tasks involving neural networks, Speech Recognition models are vulnerable to adversarial attacks. However recent research has pointed out differences between attacks and defenses on ASR models compared to image models. Improving the robustness of ASR models requires a paradigm shift from evaluating attacks on one or a few models to a systemic approach in evaluation. We lay the ground for such research by evaluating on various architectures a representative set of adversarial attacks: targeted and untargeted, optimization and speech processing-based, white-box, black-box and targeted attacks. Our results show that the relative strengths of different attack algorithms vary considerably when changing the model architecture, and that the results of some attacks are not to be blindly trusted. They also indicate that training choices such as self-supervised pretraining can significantly impact robustness by enabling transferable perturbations. We release our source code as a package that should help future research in evaluating their attacks and defenses."
   ],
   "doi": "10.21437/Interspeech.2022-400"
  },
  "janbakhshi22_interspeech": {
   "authors": [
    [
     "Parvaneh",
     "Janbakhshi"
    ],
    [
     "Ina",
     "Kodrasi"
    ]
   ],
   "title": "Adversarial-Free Speaker Identity-Invariant Representation Learning for Automatic Dysarthric Speech Classification",
   "original": "402",
   "page_count": 5,
   "order": 436,
   "p1": 2138,
   "pn": 2142,
   "abstract": [
    "Speech representations which are robust to pathology-unrelated cues such as speaker identity information have been shown to be advantageous for automatic dysarthric speech classification. A recently proposed technique to learn speaker identity-invariant representations for dysarthric speech classification is based on adversarial training. However, adversarial training can be challenging, unstable, and sensitive to training parameters. To avoid adversarial training, in this paper we propose to learn speaker-identity invariant representations exploiting a feature separation framework relying on mutual information minimization. Experimental results on a database of neurotypical and dysarthric speech show that the proposed adversarial-free framework successfully learns speaker identity-invariant representations. Further, it is shown that such representations result in a similar dysarthric speech classification performance as the representations obtained using adversarial training, while the training procedure is more stable and less sensitive to training parameters."
   ],
   "doi": "10.21437/Interspeech.2022-402"
  },
  "nishimura22_interspeech": {
   "authors": [
    [
     "Yuto",
     "Nishimura"
    ],
    [
     "Yuki",
     "Saito"
    ],
    [
     "Shinnosuke",
     "Takamichi"
    ],
    [
     "Kentaro",
     "Tachibana"
    ],
    [
     "Hiroshi",
     "Saruwatari"
    ]
   ],
   "title": "Acoustic Modeling for End-to-End Empathetic Dialogue Speech Synthesis Using Linguistic and Prosodic Contexts of Dialogue History",
   "original": "403",
   "page_count": 5,
   "order": 683,
   "p1": 3373,
   "pn": 3377,
   "abstract": [
    "We propose an end-to-end empathetic dialogue speech synthesis (DSS) model that considers both the linguistic and prosodic contexts of dialogue history. Empathy is the active attempt by humans to get inside the interlocutor in dialogue, and empathetic DSS is a technology to implement this act in spoken dialogue systems. Our model is conditioned by the history of linguistic and prosody features for predicting appropriate dialogue context. As such, it can be regarded as an extension of the conventional linguistic-feature-based dialogue history modeling. To train the empathetic DSS model effectively, we investigate 1) a self-supervised learning model pretrained with large speech corpora, 2) a style-guided training using a prosody embedding of the current utterance to be predicted by the dialogue context embedding, 3) a cross-modal attention to combine text and speech modalities, and 4) a sentence-wise embedding to achieve fine-grained prosody modeling rather than utterance-wise modeling. The evaluation results demonstrate that 1) simply considering prosodic contexts of the dialogue history does not improve the quality of speech in empathetic DSS and 2) introducing style-guided training and sentence-wise embedding modeling achieves higher speech quality than that by the conventional method."
   ],
   "doi": "10.21437/Interspeech.2022-403"
  },
  "manocha22_interspeech": {
   "authors": [
    [
     "Pranay",
     "Manocha"
    ],
    [
     "Zeyu",
     "Jin"
    ],
    [
     "Adam",
     "Finkelstein"
    ]
   ],
   "title": "Audio Similarity is Unreliable as a Proxy for Audio Quality",
   "original": "405",
   "page_count": 5,
   "order": 719,
   "p1": 3553,
   "pn": 3557,
   "abstract": [
    "Many audio processing tasks require perceptual assessment. However, the time and expense of obtaining \"gold standard” human judgments limit the availability of such data. Most applications incorporate full reference or other similarity-based metrics (e.g., PESQ) that depend on a clean reference. Researchers have relied on such metrics to evaluate and compare various proposed methods, often concluding that small, measured differences imply one is more effective than another. This paper demonstrates several practical scenarios where similarity metrics fail to agree with human perception, because they: (1) vary with clean references; (2) rely on attributes that humans factor out when considering quality, and (3) are sensitive to imperceptible signal level differences. In those scenarios, we show that no-reference metrics do not suffer from such shortcomings and correlate better with human perception. We conclude therefore that similarity serves as an unreliable proxy for audio quality."
   ],
   "doi": "10.21437/Interspeech.2022-405"
  },
  "manocha22b_interspeech": {
   "authors": [
    [
     "Pranay",
     "Manocha"
    ],
    [
     "Anurag",
     "Kumar"
    ],
    [
     "Buye",
     "Xu"
    ],
    [
     "Anjali",
     "Menon"
    ],
    [
     "Israel Degene",
     "Gebru"
    ],
    [
     "Vamsi Krishna",
     "Ithapu"
    ],
    [
     "Paul",
     "Calamia"
    ]
   ],
   "title": "SAQAM: Spatial Audio Quality Assessment Metric",
   "original": "406",
   "page_count": 5,
   "order": 131,
   "p1": 649,
   "pn": 653,
   "abstract": [
    "Audio quality assessment is critical for assessing the perceptual realism of sounds. However, the time and expense of obtaining \"gold standard” human judgments limit the availability of such data. For AR&VR, good perceived sound quality and localizability of sources are among the key elements to ensure complete immersion of the user. Our work introduces SAQAM which uses a multi-task learning framework to assess listening quality (LQ) and spatialization quality (SQ) between any given pair of binaural signals without using any subjective data. We model LQ by training on a simulated dataset of triplet human judgments, and SQ by utilizing activation-level distances from networks trained for direction of arrival (DOA) estimation. We show that SAQAM correlates well with human responses across four diverse datasets. Since it is a deep network, the metric is differentiable, making it suitable as a loss function for other tasks. For example, simply replacing an existing loss with our metric yields improvement in a speech-enhancement network."
   ],
   "doi": "10.21437/Interspeech.2022-406"
  },
  "manocha22c_interspeech": {
   "authors": [
    [
     "Pranay",
     "Manocha"
    ],
    [
     "Anurag",
     "Kumar"
    ]
   ],
   "title": "Speech Quality Assessment through MOS using Non-Matching References",
   "original": "407",
   "page_count": 5,
   "order": 132,
   "p1": 654,
   "pn": 658,
   "abstract": [
    "Human judgments obtained through Mean Opinion Scores (MOS) are the most reliable way to assess the quality of speech signals. However, several recent attempts to automatically estimate MOS using deep learning approaches lack robustness and generalization capabilities, limiting their use in real-world applications. In this work, we present a novel framework, NORESQA-MOS, for estimating the MOS of a speech signal. Unlike prior works, our approach uses non-matching references as a form of conditioning to ground the MOS estimation by neural networks. We show that NORESQA-MOS provides better generalization and more robust MOS estimation than previous state-of-the-art methods such as DNSMOS and NISQA, even though we use a smaller training set. Moreover, we also show that our generic framework can be combined with other learning methods such as self-supervised learning and can further supplement the benefits from these methods."
   ],
   "doi": "10.21437/Interspeech.2022-407"
  },
  "bekal22_interspeech": {
   "authors": [
    [
     "Dhanush",
     "Bekal"
    ],
    [
     "Sundararajan",
     "Srinivasan"
    ],
    [
     "Srikanth",
     "Ronanki"
    ],
    [
     "Sravan",
     "Bodapati"
    ],
    [
     "Katrin",
     "Kirchhoff"
    ]
   ],
   "title": "Contextual Acoustic Barge-In Classification for Spoken Dialog Systems",
   "original": "408",
   "page_count": 5,
   "order": 222,
   "p1": 1091,
   "pn": 1095,
   "abstract": [
    "In this work, we define barge-in verification as a supervised learning task where audio-only information is used to classify user spoken dialogue into true and false barge-ins. Following the success of pre-trained models, we use low-level speech representations from a self-supervised representation learning model for our downstream classification task. Further, we propose a novel technique to infuse lexical information directly into speech representations to improve the domain-specific language information implicitly learned during pre-training. Experiments conducted on spoken dialog data show that our proposed model trained to validate barge-in entirely from speech representations is faster by 38% relative and achieves 4.5% relative F1 score improvement over a baseline LSTM model that uses both audio and Automatic Speech Recognition (ASR) 1-best hypotheses. On top of this, our best proposed model with lexically infused representations along with contextual features provides a fur- ther relative improvement of 5.7% in the F1 score but only 22% faster than the baseline."
   ],
   "doi": "10.21437/Interspeech.2022-408"
  },
  "seshadri22_interspeech": {
   "authors": [
    [
     "Shreyas",
     "Seshadri"
    ],
    [
     "Tuomo",
     "Raitio"
    ],
    [
     "Dan",
     "Castellani"
    ],
    [
     "Jiangchuan",
     "Li"
    ]
   ],
   "title": "Emphasis Control for Parallel Neural TTS",
   "original": "411",
   "page_count": 5,
   "order": 684,
   "p1": 3378,
   "pn": 3382,
   "abstract": [
    "Recent parallel neural text-to-speech (TTS) synthesis methods are able to generate speech with high fidelity while maintaining high performance. However, these systems often lack control over the output prosody, thus restricting the semantic information conveyable for a given text. This paper proposes a hierarchical parallel neural TTS system for prosodic emphasis control by learning a latent space that directly corresponds to a change in emphasis. Three candidate features for the latent space are compared: 1) Variance of pitch and duration within words in a sentence, 2) Wavelet-based feature computed from pitch, energy, and duration, and 3) Learned combination of the two aforementioned approaches. At inference time, word-level prosodic emphasis is achieved by increasing the feature values of the latent space for the given words. Experiments show that all the proposed methods are able to achieve the perception of increased emphasis with little loss in overall quality. Moreover, emphasized utterances were preferred in a pairwise comparison test over the non-emphasized utterances, indicating promise for real-world applications."
   ],
   "doi": "10.21437/Interspeech.2022-411"
  },
  "fu22_interspeech": {
   "authors": [
    [
     "Li",
     "Fu"
    ],
    [
     "Xiaoxiao",
     "Li"
    ],
    [
     "Runyu",
     "Wang"
    ],
    [
     "Lu",
     "Fan"
    ],
    [
     "Zhengchen",
     "Zhang"
    ],
    [
     "Meng",
     "Chen"
    ],
    [
     "Youzheng",
     "Wu"
    ],
    [
     "Xiaodong",
     "He"
    ]
   ],
   "title": "SCaLa: Supervised Contrastive Learning for End-to-End Speech Recognition",
   "original": "412",
   "page_count": 5,
   "order": 205,
   "p1": 1006,
   "pn": 1010,
   "abstract": [
    "End-to-end Automatic Speech Recognition (ASR) models are usually trained to optimize the loss of the whole token sequence, while neglecting explicit phonemic-granularity supervision. This could result in recognition errors due to similar-phoneme confusion or phoneme reduction. To alleviate this problem, we propose a novel framework based on Supervised Contrastive Learning (SCaLa) to enhance phonemic representation learning for end-to-end ASR systems. Specifically, we extend the self-supervised Masked Contrastive Predictive Coding (MCPC) to a fully-supervised setting, where the supervision is applied in the following way. First, SCaLa masks variable-length encoder features according to phoneme boundaries given phoneme forced-alignment extracted from a pre-trained acoustic model; it then predicts the masked features via contrastive learning. The forced-alignment can provide phoneme labels to mitigate the noise introduced by positive-negative pairs in self-supervised MCPC. Experiments on reading and spontaneous speech datasets show that our proposed approach achieves 2.8 and 1.4 points Character Error Rate (CER) absolute reductions compared to the baseline, respectively."
   ],
   "doi": "10.21437/Interspeech.2022-412"
  },
  "fasoli22_interspeech": {
   "authors": [
    [
     "Andrea",
     "Fasoli"
    ],
    [
     "Chia-Yu",
     "Chen"
    ],
    [
     "Mauricio",
     "Serrano"
    ],
    [
     "Swagath",
     "Venkataramani"
    ],
    [
     "George",
     "Saon"
    ],
    [
     "Xiaodong",
     "Cui"
    ],
    [
     "Brian",
     "Kingsbury"
    ],
    [
     "Kailash",
     "Gopalakrishnan"
    ]
   ],
   "title": "Accelerating Inference and Language Model Fusion of Recurrent Neural Network Transducers via End-to-End 4-bit Quantization",
   "original": "413",
   "page_count": 5,
   "order": 416,
   "p1": 2038,
   "pn": 2042,
   "abstract": [
    "We report on aggressive quantization strategies that greatly accelerate inference of Recurrent Neural Network Transducers (RNN-T). We use a 4 bit integer representation for both weights and activations and apply Quantization Aware Training (QAT) to retrain the full model (acoustic encoder and language model) and achieve near-iso-accuracy. We show that customized quantization schemes that are tailored to the local properties of the network are essential to achieve good performance while limiting the computational overhead of QAT. Density ratio Language Model fusion has shown remarkable accuracy gains on RNN-T workloads but it severely increases the computational cost of inference. We show that our quantization strategies enable using large beam widths for hypothesis search while achieving streaming-compatible runtimes and a full model compression ratio of 7.6x compared to the full precision model. Via hardware simulations, we estimate a 3.4x acceleration from FP16 to INT4 for the end-to-end quantized RNN-T inclusive of LM fusion, resulting in a Real Time Factor (RTF) of 0.06. On the NIST Hub5 2000, Hub5 2001, and RT-03 test sets, we retain most of the gains associated with LM fusion, improving the average WER by >1.5%."
   ],
   "doi": "10.21437/Interspeech.2022-413"
  },
  "shi22b_interspeech": {
   "authors": [
    [
     "Jiatong",
     "Shi"
    ],
    [
     "George",
     "Saon"
    ],
    [
     "David",
     "Haws"
    ],
    [
     "Shinji",
     "Watanabe"
    ],
    [
     "Brian",
     "Kingsbury"
    ]
   ],
   "title": "VQ-T: RNN Transducers using Vector-Quantized Prediction Network States",
   "original": "414",
   "page_count": 5,
   "order": 336,
   "p1": 1656,
   "pn": 1660,
   "abstract": [
    "Beam search, which is the dominant ASR decoding algorithm for end-to-end models, generates tree-structured hypotheses. However, recent studies have shown that decoding with hypothesis merging can achieve a more efficient search with comparable or better performance. But, the full context in recurrent networks is not compatible with hypothesis merging. We propose to use vector-quantized long short-term memory units (VQ-LSTM) in the prediction network of RNN transducers. By training the discrete representation jointly with the ASR network, hypotheses can be actively merged for lattice generation. Our experiments on the Switchboard corpus show that the proposed VQ RNN transducers improve ASR performance over transducers with regular prediction networks while also producing denser lattices with a very low oracle word error rate (WER) for the same beam size. Additional language model rescoring experiments also demonstrate the effectiveness of the proposed lattice generation scheme."
   ],
   "doi": "10.21437/Interspeech.2022-414"
  },
  "he22b_interspeech": {
   "authors": [
    [
     "Mutian",
     "He"
    ],
    [
     "Jingzhou",
     "Yang"
    ],
    [
     "Lei",
     "He"
    ],
    [
     "Frank",
     "Soong"
    ]
   ],
   "title": "Neural Lexicon Reader: Reduce Pronunciation Errors in End-to-end TTS by Leveraging External Textual Knowledge",
   "original": "420",
   "page_count": 5,
   "order": 89,
   "p1": 441,
   "pn": 445,
   "abstract": [
    "End-to-end TTS requires a large amount of speech/text paired data to cover all necessary knowledge, particularly how to pronounce different words in diverse contexts, so that a neural model may learn such knowledge accordingly. But in real applications, such high demand of training data is hard to be satisfied and additional knowledge often needs to be injected manually. For example, to capture pronunciation knowledge on languages without regular orthography, a complicated grapheme-to-phoneme pipeline needs to be built based on a large structured pronunciation lexicon, leading to extra, sometimes high, costs to extend neural TTS to such languages. In this paper, we propose a framework to learn to automatically extract knowledge from unstructured external resources using a novel Token2Knowledge attention module. The framework is applied to build a TTS model named Neural Lexicon Reader that extracts pronunciations from raw lexicon texts in an end-to-end manner. Experiments show the proposed model significantly reduces pronunciation errors in low-resource, end-to-end Chinese TTS, and the lexicon-reading capability can be transferred to other languages with a smaller amount of data."
   ],
   "doi": "10.21437/Interspeech.2022-420"
  },
  "mussakhojayeva22_interspeech": {
   "authors": [
    [
     "Saida",
     "Mussakhojayeva"
    ],
    [
     "Yerbolat",
     "Khassanov"
    ],
    [
     "Huseyin",
     "Atakan Varol"
    ]
   ],
   "title": "KSC2: An Industrial-Scale Open-Source Kazakh Speech Corpus",
   "original": "421",
   "page_count": 5,
   "order": 278,
   "p1": 1367,
   "pn": 1371,
   "abstract": [
    "We present the first industrial-scale open-source Kazakh speech corpus for automatic speech recognition research and development. Our corpus subsumes two previously presented corpora: 1) Kazakh speech corpus (KSC) and 2) Kazakh text-to-speech 2 (KazakhTTS2). We also provide additional data from other sources, including television news, television and radio programs, parliament speeches, and podcasts. Our corpus, which we have named KSC2, contains over a thousand hours of high-quality transcribed data, which is triple the size of KSC. KSC2 was manually transcribed with the help of native Kazakh speakers and validated via preliminary speech recognition experiments on various evaluation sets. Moreover, it contains utterances with Kazakh-Russian code-switching, a conversational practice common among Kazakh speakers. We believe that our corpus will facilitate speech processing research for Kazakh, which is widely considered an under-resourced language. To ensure the reproducibility of experiments, we share the KSC2 corpus, training recipes, and pretrained models."
   ],
   "doi": "10.21437/Interspeech.2022-421"
  },
  "shim22_interspeech": {
   "authors": [
    [
     "Kyuhong",
     "Shim"
    ],
    [
     "Wonyong",
     "Sung"
    ]
   ],
   "title": "Similarity and Content-based Phonetic Self Attention for Speech Recognition",
   "original": "422",
   "page_count": 5,
   "order": 835,
   "p1": 4118,
   "pn": 4122,
   "abstract": [
    "Transformer-based speech recognition models have achieved great success due to the self-attention (SA) mechanism that utilizes every frame in the feature extraction process. Especially, SA heads in lower layers capture various phonetic characteristics by the query-key dot product, which is designed to compute the pairwise relationship between frames. In this paper, we propose a variant of SA to extract more representative phonetic features. The proposed phonetic self-attention (phSA) is composed of two different types of phonetic attention; one is similarity-based and the other is content-based. In short, similarity-based attention captures the correlation between frames while content-based attention only considers each frame without being affected by other frames. We identify which parts of the original dot product equation are related to two different attention patterns and improve each part with simple modifications. Our experiments on phoneme classification and speech recognition show that replacing SA with phSA for lower layers improves the recognition performance without increasing the latency and the parameter size."
   ],
   "doi": "10.21437/Interspeech.2022-422"
  },
  "lou22_interspeech": {
   "authors": [
    [
     "Yijie",
     "Lou"
    ],
    [
     "Shiliang",
     "Pu"
    ],
    [
     "Jianfeng",
     "Zhou"
    ],
    [
     "Xin",
     "Qi"
    ],
    [
     "Qinbo",
     "Dong"
    ],
    [
     "Hongwei",
     "Zhou"
    ]
   ],
   "title": "A Deep One-Class Learning Method for Replay Attack Detection",
   "original": "427",
   "page_count": 5,
   "order": 965,
   "p1": 4765,
   "pn": 4769,
   "abstract": [
    "Replay-attack is a serious issue for automatic speaker verification (ASV) and recently lots of countermeasures have been proposed to protect ASV from spoofing attacks. Traditional countermeasures are a binary-classification system which was observed to have limited generalization on unseen attacks. Oneclass learning methods which have been widely used in anomaly detection is a promising method to enhance the robustness of replay detection system. In this paper, we propose a deep one-class learning scheme to model the genuine speeches in a compact embedding space. To reduce the variance of genuine embedding space, we design an architecture unit, called residual variability block, which can be flexibly integrated into usual convolutional neural networks. Comprehensive experiments show that the proposed deep one-class learning scheme is effective for replay attack detection under cross-database scenarios. Besides, in our internal collected dataset, such a scheme shows better robustness under mismatched conditions between the enrollment and test phase."
   ],
   "doi": "10.21437/Interspeech.2022-427"
  },
  "cheng22_interspeech": {
   "authors": [
    [
     "Jiaming",
     "Cheng"
    ],
    [
     "Ruiyu",
     "Liang"
    ],
    [
     "Yue",
     "Xie"
    ],
    [
     "Li",
     "Zhao"
    ],
    [
     "Björn",
     "Schuller"
    ],
    [
     "Jie",
     "Jia"
    ],
    [
     "Yiyuan",
     "Peng"
    ]
   ],
   "title": "Cross-Layer Similarity Knowledge Distillation for Speech Enhancement",
   "original": "429",
   "page_count": 5,
   "order": 189,
   "p1": 926,
   "pn": 930,
   "abstract": [
    "Speech enhancement (SE) algorithms based on deep neural networks (DNNs) often encounter challenges of limited hardware resources or strict latency requirements when deployed in real-world scenarios. However, a strong enhancement effect typically requires a large DNN. In this paper, a knowledge distillation framework for SE is proposed to compress the DNN model. We study the strategy of cross-layer connection paths, which fuses multi-level information from the teacher and transfers it to the student. To adapt to the SE task, we propose a frame-level similarity distillation loss. We apply this method to the deep complex convolution recurrent network (DCCRN) and make targeted adjustments. Experimental results show that the proposed method considerably improves the enhancement effect of the compressed DNN and outperforms other distillation methods."
   ],
   "doi": "10.21437/Interspeech.2022-429"
  },
  "pia22_interspeech": {
   "authors": [
    [
     "Nicola",
     "Pia"
    ],
    [
     "Kishan",
     "Gupta"
    ],
    [
     "Srikanth",
     "Korse"
    ],
    [
     "Markus",
     "Multrus"
    ],
    [
     "Guillaume",
     "Fuchs"
    ]
   ],
   "title": "NESC: Robust Neural End-2-End Speech Coding with GANs",
   "original": "430",
   "page_count": 5,
   "order": 854,
   "p1": 4212,
   "pn": 4216,
   "abstract": [
    "Neural networks have proven to be a formidable tool to tackle the problem of speech coding at very low bit rates. However, the design of a neural coder that can be operated robustly under real-world conditions remains a major challenge. Therefore, we present Neural End-2-End Speech Codec (NESC) a robust, scalable end-to-end neural speech codec for high-quality wideband speech coding at 3 kbps. The encoder uses a new architecture configuration, which relies on our proposed DualPathConvRNN layer, while the decoder architecture is based on our previous work SSMGAN. Our subjective listening tests on clean and noisy speech show that NESC is particularly robust to unseen conditions and signal perturbations."
   ],
   "doi": "10.21437/Interspeech.2022-430"
  },
  "yang22e_interspeech": {
   "authors": [
    [
     "Dongchao",
     "Yang"
    ],
    [
     "Helin",
     "Wang"
    ],
    [
     "Zhongjie",
     "Ye"
    ],
    [
     "Yuexian",
     "Zou"
    ],
    [
     "WenWu",
     "Wang"
    ]
   ],
   "title": "RaDur: A Reference-aware and Duration-robust Network for Target Sound Detection",
   "original": "433",
   "page_count": 5,
   "order": 307,
   "p1": 1511,
   "pn": 1515,
   "abstract": [
    "Target sound detection (TSD) aims to detect the target sound from a mixture audio given the reference information. Previous methods use a conditional network to extract a sound-discriminative embedding from the reference audio, and then use it to detect the target sound from the mixture audio. However, the network performs much differently when using different reference audios (\\text{e.g.} performs poorly for noisy and short-duration reference audios), and tends to make wrong decisions for transient events (\\text{i.e.} shorter than $1$ second). To overcome these problems, in this paper, we present a reference-aware and duration-robust network (RaDur) for TSD. More specifically, in order to make the network more aware of the reference information, we propose an embedding enhancement module to take into account the mixture audio while generating the embedding, and apply the attention pooling to enhance the features of target sound-related frames and weaken the features of noisy frames. In addition, a duration-robust focal loss is proposed to help model different-duration events. To evaluate our method, we build two TSD datasets based on UrbanSound and Audioset. Extensive experiments show the effectiveness of our methods."
   ],
   "doi": "10.21437/Interspeech.2022-433"
  },
  "song22_interspeech": {
   "authors": [
    [
     "Kaitao",
     "Song"
    ],
    [
     "Teng",
     "Wan"
    ],
    [
     "Bixia",
     "Wang"
    ],
    [
     "Huiqiang",
     "Jiang"
    ],
    [
     "Luna",
     "Qiu"
    ],
    [
     "Jiahang",
     "Xu"
    ],
    [
     "Liping",
     "Jiang"
    ],
    [
     "Qun",
     "Lou"
    ],
    [
     "Yuqing",
     "Yang"
    ],
    [
     "Dongsheng",
     "Li"
    ],
    [
     "Xudong",
     "Wang"
    ],
    [
     "Lili",
     "Qiu"
    ]
   ],
   "title": "Improving Hypernasality Estimation with Automatic Speech Recognition in Cleft Palate Speech",
   "original": "438",
   "page_count": 5,
   "order": 976,
   "p1": 4820,
   "pn": 4824,
   "abstract": [
    "Hypernasality is an abnormal resonance in human speech production, especially in patients with craniofacial anomalies such as cleft palate. In clinical application, hypernasality estimation is crucial in cleft palate diagnosis, as its results determine the subsequent surgery and additional speech therapy. Therefore, designing an automatic hypernasality assessment method will facilitate speech-language pathologists to make precise diagnoses. Existing methods for hypernasality estimation only conduct acoustic analysis based on low-resource cleft palate dataset, by using statistical or neural network-based features. In this paper, we propose a novel approach that uses automatic speech recognition model to improve hypernasality estimation. Specifically, we first pre-train an encoder-decoder framework in an automatic speech recognition (ASR) objective by using speech-to-text dataset, and then fine-tune ASR encoder on the cleft palate dataset for hypernasality estimation. Benefiting from such design, our model for hypernasality estimation can enjoy the advantages of ASR model: 1) compared with low-resource cleft palate dataset, the ASR task usually includes large-scale speech data in the general domain, which enables better model generalization; 2) the text annotations in ASR dataset guide model to extract better acoustic features. Experimental results on two cleft palate datasets demonstrate that our method achieves superior performance compared with previous approaches."
   ],
   "doi": "10.21437/Interspeech.2022-438"
  },
  "saeki22c_interspeech": {
   "authors": [
    [
     "Takaaki",
     "Saeki"
    ],
    [
     "Detai",
     "Xin"
    ],
    [
     "Wataru",
     "Nakata"
    ],
    [
     "Tomoki",
     "Koriyama"
    ],
    [
     "Shinnosuke",
     "Takamichi"
    ],
    [
     "Hiroshi",
     "Saruwatari"
    ]
   ],
   "title": "UTMOS: UTokyo-SaruLab System for VoiceMOS Challenge 2022",
   "original": "439",
   "page_count": 5,
   "order": 916,
   "p1": 4521,
   "pn": 4525,
   "abstract": [
    "We present the UTokyo-SaruLab mean opinion score (MOS) prediction system submitted to VoiceMOS Challenge 2022. The challenge is to predict the MOS values of speech samples collected from previous Blizzard Challenges and Voice Conversion Challenges for two tracks: a main track for in-domain prediction and an out-of-domain (OOD) track for which there is less labeled data from different listening tests. Our system is based on ensemble learning of strong and weak learners. Strong learners incorporate several improvements to the previous fine-tuning models of self-supervised learning (SSL) models, while weak learners use basic machine-learning methods to predict scores from SSL features. In the Challenge, our system had the highest score on several metrics for both the main and OOD tracks. In addition, we conducted ablation studies to investigate the effectiveness of our proposed methods."
   ],
   "doi": "10.21437/Interspeech.2022-439"
  },
  "kunihara22b_interspeech": {
   "authors": [
    [
     "Takuya",
     "Kunihara"
    ],
    [
     "Chuanbo",
     "Zhu"
    ],
    [
     "Daisuke",
     "Saito"
    ],
    [
     "Nobuaki",
     "Minematsu"
    ],
    [
     "Noriko",
     "Nakanishi"
    ]
   ],
   "title": "Detection of Learners' Listening Breakdown with Oral Dictation and Its Use to Model Listening Skill Improvement Exclusively Through Shadowing",
   "original": "440",
   "page_count": 5,
   "order": 904,
   "p1": 4461,
   "pn": 4465,
   "abstract": [
    "In language learners' speech, mispronounced words, word fragments, repairs, filled pauses, etc are often found, and they can be detected with ASR-based CALL systems. When learners are listening, some segments in a given utterance are often difficult to identify or misidentified due to lack of listening skill. In this study, we aim at detecting learners' listening breakdown to measure their listening skill. Listening skill is often quantified by imposing manual dictation on learners, but it has inevitable problems because manual dictation is generally an offline task. To solve the problems, oral dictation is imposed instead, and speaking breakdown is detected in the dictation utterances. Here, we assume that learners' speaking breakdown is attributed to their listening breakdown. This method is applied to measure their listening skill and to model its improvement exclusively through shadowing, which is oral dictation with a short delay and was introduced to language education originally as listening training. 35 Japanese university students attended a 42-day intensive shadowing training, and their shadowing utterances were analyzed to detect listening breakdown. Our model exhibits very monotonous improvement of listening skill as a function of how many days learners attended shadowing."
   ],
   "doi": "10.21437/Interspeech.2022-440"
  },
  "li22e_interspeech": {
   "authors": [
    [
     "Haoyu",
     "Li"
    ],
    [
     "Junichi",
     "Yamagishi"
    ]
   ],
   "title": "DDS: A new device-degraded speech dataset for speech enhancement",
   "original": "441",
   "page_count": 5,
   "order": 591,
   "p1": 2913,
   "pn": 2917,
   "abstract": [
    "A large and growing amount of speech content in real-life scenarios is being recorded on consumer-grade devices in uncontrolled environments, resulting in degraded speech quality. Transforming such low-quality device-degraded speech into high-quality speech is a goal of speech enhancement (SE). This paper introduces a new speech dataset, DDS, to facilitate the research on SE. DDS provides aligned parallel recordings of high-quality speech (recorded in professional studios) and a number of versions of low-quality speech, producing approximately 2,000 hours speech data. The DDS dataset covers 27 realistic recording conditions by combining diverse acoustic environments and microphone devices, and each version of a condition consists of multiple recordings from six microphone positions to simulate different noise and reverberation levels. We also test several SE baseline systems on the DDS dataset and show the impact of recording diversity on performance."
   ],
   "doi": "10.21437/Interspeech.2022-441"
  },
  "ho22_interspeech": {
   "authors": [
    [
     "Tuan Vu",
     "Ho"
    ],
    [
     "Quoc Huy",
     "Nguyen"
    ],
    [
     "Masato",
     "Akagi"
    ],
    [
     "Masashi",
     "Unoki"
    ]
   ],
   "title": "Vector-quantized Variational Autoencoder for Phase-aware Speech Enhancement",
   "original": "443",
   "page_count": 5,
   "order": 36,
   "p1": 176,
   "pn": 180,
   "abstract": [
    "Recent speech enhancement methods based on the complex ideal ratio mask (cIRM) have achieved promising results. These methods often deploy a deep neural network to jointly estimate the real and imaginary components of the cIRM defined in the complex domain. However, the unbounded property of cIRM poses difficulties when it comes to effectively training a neural network. To alleviate this problem, this paper proposes a phase-aware speech enhancement method by estimating the magnitude and phase of a complex adaptive Wiener filter. In this method, a noise-robust vector-quantized variational autoencoder is utilized for estimating the magnitude Wiener filter by using the Itakura-Saito divergence on time-frequency domain, while the phase of the Wiener filter is estimated by a convolutional recurrent network using the scale-invariant signal-to-noise ratio constraint in the time domain. The proposed method was evaluated on the open Voice Bank+DEMAND dataset to provide a direct comparison with other speech enhancement studies and achieved the PESQ score of 2.85 and STOI score of 0.94, which is better than the state-of-art method based on cIRM estimation in the 2020 Deep Noise Challenge."
   ],
   "doi": "10.21437/Interspeech.2022-443"
  },
  "takamichi22_interspeech": {
   "authors": [
    [
     "Shinnosuke",
     "Takamichi"
    ],
    [
     "Wataru",
     "Nakata"
    ],
    [
     "Naoko",
     "Tanji"
    ],
    [
     "Hiroshi",
     "Saruwatari"
    ]
   ],
   "title": "J-MAC: Japanese multi-speaker audiobook corpus for speech synthesis",
   "original": "444",
   "page_count": 5,
   "order": 480,
   "p1": 2358,
   "pn": 2362,
   "abstract": [
    "In this paper, we construct a Japanese audiobook speech corpus called ``J-MAC'' for speech synthesis research. With the success of reading-style speech synthesis, the research target is shifting to tasks that use complicated contexts. Audiobook speech synthesis is a good example that requires cross-sentence, expressiveness, etc. Unlike reading-style speech, speaker-specific expressiveness in audiobook speech also becomes the context. To enhance this research, we propose a method of constructing a corpus from audiobooks read by professional speakers. From many audiobooks and their texts, our method can automatically extract and refine the data without any language dependency. Specifically, we use vocal-instrumental separation to extract clean data, connectionist temporal classification to roughly align text and audio, and voice activity detection to refine the alignment. J-MAC is open-sourced in our project page. We also conduct audiobook speech synthesis evaluations, and the results give insights into audiobook speech synthesis."
   ],
   "doi": "10.21437/Interspeech.2022-444"
  },
  "zhou22_interspeech": {
   "authors": [
    [
     "Jing",
     "Zhou"
    ],
    [
     "Changchun",
     "Bao"
    ]
   ],
   "title": "Multi-source wideband DOA estimation method by frequency focusing and error weighting",
   "original": "445",
   "page_count": 5,
   "order": 1099,
   "p1": 5423,
   "pn": 5427,
   "abstract": [
    "In this paper, a new multi-source wideband direction of arrival (MSW-DOA) estimation method is proposed for the signals with non-uniform distribution using the sub-array of uniform linear array. Different from conventional methods, based on the free far-field model, the proposed method mainly makes two contributions. One is that the sub-array decomposition is adopted to improve the accuracy of MSW-DOA estimation by minimizing the weighted error, and the other one is that the frequency focusing procedure is optimized according to the presence probability of sound sources for reducing the influence of the sub-bands with low signal to noise ratio (SNR). Simulation results show that the proposed method can effectively improve the performance of wideband DOA estimation in the case of multiple sound sources."
   ],
   "doi": "10.21437/Interspeech.2022-445"
  },
  "zhao22e_interspeech": {
   "authors": [
    [
     "Haodong",
     "Zhao"
    ],
    [
     "Wei",
     "Du"
    ],
    [
     "Junjie",
     "Guo"
    ],
    [
     "Gongshen",
     "Liu"
    ]
   ],
   "title": "A Universal Identity Backdoor Attack against Speaker Verification based on Siamese Network",
   "original": "446",
   "page_count": 5,
   "order": 966,
   "p1": 4770,
   "pn": 4774,
   "abstract": [
    "Speaker verification has been widely used in many authentication scenarios. However, training models for speaker verification requires large amounts of data and computing power, so users often use untrustworthy third-party data or deploy third-party models directly, which may create security risks. In this paper, we propose a backdoor attack for the above scenario. Specifically, for the Siamese network in the speaker verification system, we try to implant a universal identity in the model that can simulate any enrolled speaker and pass the verification. So the attacker does not need to know the victim, which makes the attack more flexible and stealthy. In addition, we design and compare three ways of selecting attacker utterances and two ways of poisoned training for the GE2E loss function in different scenarios. The results on the TIMIT and Voxceleb1 datasets show that our approach can achieve a high attack success rate while guaranteeing the normal verification accuracy. Our work reveals the vulnerability of the speaker verification system and provides a new perspective to further improve the robustness of the system."
   ],
   "doi": "10.21437/Interspeech.2022-446"
  },
  "taniguchi22_interspeech": {
   "authors": [
    [
     "Shuta",
     "Taniguchi"
    ],
    [
     "Tsuneo",
     "Kato"
    ],
    [
     "Akihiro",
     "Tamura"
    ],
    [
     "Keiji",
     "Yasuda"
    ]
   ],
   "title": "Transformer-Based Automatic Speech Recognition with Auxiliary Input of Source Language Text Toward Transcribing Simultaneous Interpretation",
   "original": "448",
   "page_count": 5,
   "order": 571,
   "p1": 2813,
   "pn": 2817,
   "abstract": [
    "In the training programs of human simultaneous interpreters, trainee speech is transcribed into text for quality assessment. Though interpreter speech contains irregular speech events such as hesitations, filled pauses, and self-repairs, automatic speech recognition (ASR) is expected to be introduced to save labor of transcription. In the training programs, source language text can be used for ASR because the training materials are prepared in advance. Thus, we propose a Transformer-based end-to-end ASR with an auxiliary input of a source language text toward transcribing simultaneous interpretation. Because a sufficient amount of human interpreter speech with source language text is not available for training the model, we conducted the initial evaluation of the model by simulating speech with source language text by changing the inputs and outputs of large-scale corpora for developing end-to-end speech translation (ST). Our proposed model significantly reduced word error rates (WERs) for four ST corpora: MuST-C English speech - Netherlandic text, English speech - German text, CoVoST 2 English speech - Japanese text, and our original TED-based English speech - Japanese text corpus."
   ],
   "doi": "10.21437/Interspeech.2022-448"
  },
  "zhang22e_interspeech": {
   "authors": [
    [
     "Zewang",
     "Zhang"
    ],
    [
     "Yibin",
     "Zheng"
    ],
    [
     "Xinhui",
     "Li"
    ],
    [
     "Li",
     "Lu"
    ]
   ],
   "title": "WeSinger: Data-augmented Singing Voice Synthesis with Auxiliary Losses",
   "original": "454",
   "page_count": 5,
   "order": 862,
   "p1": 4252,
   "pn": 4256,
   "abstract": [
    "In this paper, we develop a new multi-singer Chinese neural singing voice synthesis (SVS) system named WeSinger. To improve the accuracy and naturalness of synthesized singing voice, we design several specifical modules and techniques: 1) A deep bi-directional LSTM-based duration model with multi-scale rhythm loss and post-processing step; 2) A Transformer-alike acoustic model with progressive pitch-weighted decoder loss; 3) a 24 kHz pitch-aware LPCNet neural vocoder to produce high-quality singing waveforms; 4) A novel data augmentation method with multi-singer pre-training for stronger robustness and naturalness. To our knowledge, WeSinger is the first SVS system to adopt 24 kHz LPCNet and multi-singer pre-training simultaneously. Both quantitative and qualitative evaluation results demonstrate the effectiveness of WeSinger in terms of accuracy and naturalness, and WeSinger achieves state-of-the-art performance on the recent public Chinese singing corpus Opencpop\\footnote{https://wenet.org.cn/opencpop/}. Some synthesized singing samples are available online\\footnote{https://zzw922cn.github.io/wesinger/}."
   ],
   "doi": "10.21437/Interspeech.2022-454"
  },
  "sun22_interspeech": {
   "authors": [
    [
     "Guangzhi",
     "Sun"
    ],
    [
     "Chao",
     "Zhang"
    ],
    [
     "Phil",
     "Woodland"
    ]
   ],
   "title": "Tree-constrained Pointer Generator with Graph Neural Network Encodings for Contextual Speech Recognition",
   "original": "461",
   "page_count": 5,
   "order": 417,
   "p1": 2043,
   "pn": 2047,
   "abstract": [
    "Incorporating biasing words obtained as contextual knowledge is critical for many automatic speech recognition (ASR) applications. This paper proposes the use of graph neural network (GNN) encodings in a tree-constrained pointer generator (TCPGen) component for end-to-end contextual ASR. By encoding the biasing words in the prefix-tree with a tree-based GNN, lookahead for future wordpieces in end-to-end ASR decoding is achieved at each tree node by incorporating information about all wordpieces on the tree branches rooted from it, which allows a more accurate prediction of the generation probability of the biasing words. Systems were evaluated on the Librispeech corpus using simulated biasing tasks, and on the AMI corpus by proposing a novel visual-grounded contextual ASR pipeline that extracts biasing words from slides alongside each meeting. Results showed that TCPGen with GNN encodings achieved about a further 15\\% relative WER reduction on the biasing words compared to the original TCPGen, with a negligible increase in the computation cost for decoding."
   ],
   "doi": "10.21437/Interspeech.2022-461"
  },
  "lesnichaia22_interspeech": {
   "authors": [
    [
     "Mariia",
     "Lesnichaia"
    ],
    [
     "Veranika",
     "Mikhailava"
    ],
    [
     "Natalia",
     "Bogach"
    ],
    [
     "Iurii",
     "Lezhenin"
    ],
    [
     "John",
     "Blake"
    ],
    [
     "Evgeny",
     "Pyshkin"
    ]
   ],
   "title": "Classification of Accented English Using CNN Model Trained on Amplitude Mel-Spectrograms",
   "original": "462",
   "page_count": 5,
   "order": 744,
   "p1": 3669,
   "pn": 3673,
   "abstract": [
    "Automatic speech recognition is hindered by the linguistic differences occurring in accented speech. This paper advances a classification method for accented speech using a CNN-based model trained and tested on English with Germanic, Romance and Slavic accents. The input feature set was examined to find the optimal combination of time-frequency and energy characteristics of speech fed into the machine learning model. We also tuned model hyperparameters and the dimensionality of input features. We argue that mel-scale amplitude spectrograms on a liner scale appear more powerful in accent classification tasks compared to conventional feature sets based on MFCCs and raw spectrograms. Our models used only sparse data from the Speech Accent Archive, yet produced state-of-the-art classification results for English with Germanic, Romance and Slavic accents. The accuracy of our models trained on linear scale amplitude mel-spectrograms ranged from 0.964 to 0.987, outperforming existing models classifying accents using the same dataset."
   ],
   "doi": "10.21437/Interspeech.2022-462"
  },
  "obrien22_interspeech": {
   "authors": [
    [
     "Benjamin",
     "O'Brien"
    ],
    [
     "Christine",
     "Meunier"
    ],
    [
     "Alain",
     "Ghio"
    ]
   ],
   "title": "Evaluating the effects of modified speech on perceptual speaker identification performance",
   "original": "463",
   "page_count": 5,
   "order": 623,
   "p1": 3073,
   "pn": 3077,
   "abstract": [
    "This paper details a study to evaluate the effects of modified speech on perceptual speaker identification (SID) performance by naive listeners. Speech recordings made by eight male, native-French speakers were selected from the PTSVox database. The pitch and speech tempo of the recordings were modified at the word-level. The first 75% of words spoken were modified, such that the percentage of modification began at 100% and gradually decayed to 0%. The direction of the modifications was also examined, such that pitch modifications began at +/-600 cents and speech tempo modifications began at a ratio of either 1:2 or 3:2 (modified to normal speech tempo). Following a familiarization period, participants completed two rounds of 48 \"go/no-go\" task trials (balanced), where each round corresponded to a different speech modification type. The main results showed perceptual SID performance was significantly affected when participants were presented speech recordings that contained pitch modifications in comparison to speech tempo modifications. The findings revealed participants were able to overcome higher percentages of speech tempo modifications to make correct distinctions between speakers. Although modified pitch influenced in voice perception performance, high variability between participant responses were observed, which suggests listeners model speakers differently."
   ],
   "doi": "10.21437/Interspeech.2022-463"
  },
  "xiong22_interspeech": {
   "authors": [
    [
     "Feifei",
     "Xiong"
    ],
    [
     "Weiguang",
     "Chen"
    ],
    [
     "Pengyu",
     "Wang"
    ],
    [
     "Xiaofei",
     "Li"
    ],
    [
     "Jinwei",
     "Feng"
    ]
   ],
   "title": "Spectro-Temporal SubNet for Real-Time Monaural Speech Denoising and Dereverberation",
   "original": "468",
   "page_count": 5,
   "order": 190,
   "p1": 931,
   "pn": 935,
   "abstract": [
    "This paper presents an improved subband neural network applied to joint speech denoising and dereverberation for online single-channel scenarios. Preserving the advantages of subband model (SubNet) that processes each frequency band independently and requires small amount of resources for good generalization, the proposed framework named STSubNet exploits sufficient spectro-temporal receptive fields (STRFs) from speech spectrum via a two-dimensional convolution network cooperating with a bi-directional long short-term memory network across frequency bands, to further improve the neural network discrimination between desired speech component and undesired interference including noise and reverberation. The importance of this STRF extractor is analyzed by evaluating the contribution of individual module to the STSubNet performance for simultaneously denoising and dereverberation. Experimental results show that STSubNet outperforms other subband variants and achieves competitive performance compared to state-of-the-art models on two publicly benchmark test sets."
   ],
   "doi": "10.21437/Interspeech.2022-468"
  },
  "zhang22f_interspeech": {
   "authors": [
    [
     "Peng",
     "Zhang"
    ],
    [
     "Peng",
     "Hu"
    ],
    [
     "Xueliang",
     "Zhang"
    ]
   ],
   "title": "Norm-constrained Score-level Ensemble for Spoofing Aware Speaker Verification",
   "original": "470",
   "page_count": 5,
   "order": 886,
   "p1": 4371,
   "pn": 4375,
   "abstract": [
    "In this paper, we present our system submitted to the Spoofing Aware Speaker Verification Challenge (SASVC) 2022. Our submission focuses on bridging the gap between automatic speaker verification (ASV) and countermeasure (CM) systems. We introduce a general norm-constrained score-level ensemble method that can improve robustness to zero-effort impostors and spoofing attacks by jointly processing the scores extracted from the ASV and CM subsystems. Furthermore, we explore that the ensemble system can provide better performance when both ASV and CM subsystems are optimized. Experimental results show that our primary system yields 0.45% SV-EER, 0.26% SPF-EER, and 0.37% SASV-EER on the SASVC 2022 evaluation set. The relative improvements are 96.08%, 66.67%, and 94.19% over the best official baseline, respectively. All of our code and pre-trained model weights are publicly available and reproducible."
   ],
   "doi": "10.21437/Interspeech.2022-470"
  },
  "sun22b_interspeech": {
   "authors": [
    [
     "Yifan",
     "Sun"
    ],
    [
     "Qinlong",
     "Huang"
    ],
    [
     "Xihong",
     "Wu"
    ]
   ],
   "title": "Unsupervised Acoustic-to-Articulatory Inversion with Variable Vocal Tract Anatomy",
   "original": "477",
   "page_count": 5,
   "order": 943,
   "p1": 4656,
   "pn": 4660,
   "abstract": [
    "Acoustic and articulatory variability across speakers has always limited the generalization performance of acoustic-to-articulatory inversion (AAI) methods. Speaker-independent AAI (SI-AAI) methods generally focus on the transformation of acoustic features, but rarely consider the direct matching in the articulatory space. Unsupervised AAI methods have the potential of better generalization ability but typically use a fixed morphological setting of a physical articulatory synthesizer even for different speakers, which may cause nonnegligible articulatory compensation. In this paper, we propose to jointly estimate articulatory movements and vocal tract anatomy during the inversion of speech. An unsupervised AAI framework is employed, where estimated vocal tract anatomy is used to set the configuration of a physical articulatory synthesizer, which in turn is driven by estimated articulation movements to imitate a given speech. Experiments show that the estimation of vocal tract anatomy can bring both acoustic and articulatory benefits. Acoustically, the reconstruction quality is higher; articulatorily, the estimated articulatory movement trajectories better match the measured ones. Moreover, the estimated anatomy parameters show clear clusterings by speakers, indicating successful decoupling of speaker characteristics and linguistic content."
   ],
   "doi": "10.21437/Interspeech.2022-477"
  },
  "liu22f_interspeech": {
   "authors": [
    [
     "Bei",
     "Liu"
    ],
    [
     "Zhengyang",
     "Chen"
    ],
    [
     "Yanmin",
     "Qian"
    ]
   ],
   "title": "Attentive Feature Fusion for Robust Speaker Verification",
   "original": "478",
   "page_count": 5,
   "order": 58,
   "p1": 286,
   "pn": 290,
   "abstract": [
    "As the most widely used technique, deep speaker embedding learning has become predominant in speaker verification task recently. This approach utilizes deep neural networks to extract fixed dimension embedding vectors which represent different speaker identities. Two network architectures such as ResNet and ECAPA-TDNN have been commonly adopted in prior studies and achieved the state-of-the-art performance. One omnipresent part, feature fusion, plays an important role in both of them. For example, shortcut connections are designed to fuse the identity mapping of inputs and outputs of residual blocks in ResNet. ECAPA-TDNN employs the multi-layer feature aggregation to integrate shallow feature maps with deep ones. Traditional feature fusion is often implemented via simple operations, such as element-wise addition or concatenation. In this paper, we propose a more effective feature fusion scheme, namely Attentive Feature Fusion (AFF), to render dynamic weighted fusion of different features. It utilizes attention modules to learn fusion weights based on the feature contents. Additionally, two fusion strategies are designed: sequential fusion and parallel fusion. Experiments on Voxceleb dataset show that our proposed attentive feature fusion scheme can result in up to 40% relative improvement over the baseline systems."
   ],
   "doi": "10.21437/Interspeech.2022-478"
  },
  "liu22g_interspeech": {
   "authors": [
    [
     "Bei",
     "Liu"
    ],
    [
     "Zhengyang",
     "Chen"
    ],
    [
     "Yanmin",
     "Qian"
    ]
   ],
   "title": "Dual Path Embedding Learning for Speaker Verification with Triplet Attention",
   "original": "481",
   "page_count": 5,
   "order": 59,
   "p1": 291,
   "pn": 295,
   "abstract": [
    "Currently, many different network architectures have been explored in speaker verification, including time-delay neural network (TDNN), convolutional neural network (CNN), transformer and multi-layer perceptrons (MLP). However, hybrid networks with diverse structures are rarely investigated. In this paper, we present a novel and effective dual path embedding learning framework, named Dual Path Network (DPNet), for speaker verification with triplet attention. A new topology of integrating CNN with a separate recurrent layer connection path internally is designed, which introduces the sequential structure along depth into CNN. This new architecture inherits both advantages of residual and recurrent networks, enabling better feature re-usage and re-exploitation. Additionally, an efficient triplet attention module is utilized to capture cross-dimension interactions between features. The experimental results conducted on Voxceleb dataset show that our proposed hybrid network with triplet attention can outperform the corresponding ResNet by a significant margin."
   ],
   "doi": "10.21437/Interspeech.2022-481"
  },
  "zhang22g_interspeech": {
   "authors": [
    [
     "Binbin",
     "Zhang"
    ],
    [
     "Di",
     "Wu"
    ],
    [
     "Zhendong",
     "Peng"
    ],
    [
     "Xingchen",
     "Song"
    ],
    [
     "Zhuoyuan",
     "Yao"
    ],
    [
     "Hang",
     "Lv"
    ],
    [
     "Lei",
     "Xie"
    ],
    [
     "Chao",
     "Yang"
    ],
    [
     "Fuping",
     "Pan"
    ],
    [
     "Jianwei",
     "Niu"
    ]
   ],
   "title": "WeNet 2.0: More Productive End-to-End Speech Recognition Toolkit",
   "original": "483",
   "page_count": 5,
   "order": 337,
   "p1": 1661,
   "pn": 1665,
   "abstract": [
    "Recently, we made available WeNet [1], a production-oriented end-to-end speech recognition toolkit, which introduces a unified two-pass (U2) framework and a built-in runtime to address the streaming and non-streaming decoding modes in a single model. To further improve ASR performance and facilitate various production requirements, in this paper, we present WeNet 2.0 with four important updates. (1) We propose U2++, a unified two-pass framework with bidirectional attention decoders, which includes the future contextual information by a right-to-left attention decoder to improve the representative ability of the shared encoder and the performance during the rescoring stage. (2) We introduce an n-gram based language model and a WFST-based decoder into WeNet 2.0, promoting the use of rich text data in production scenarios. (3) We design a unified contextual biasing framework, which leverages user-specific context (e.g., contact lists) to provide rapid adaptation ability for production and improves ASR accuracy in both with-LM and without-LM scenarios. (4) We design a unified IO to support large-scale data for effective model training. In summary, the brand-new WeNet 2.0 achieves up to 10% relative recognition performance improvement over the original WeNet on various corpora and makes available several important production-oriented features."
   ],
   "doi": "10.21437/Interspeech.2022-483"
  },
  "liu22h_interspeech": {
   "authors": [
    [
     "Bei",
     "Liu"
    ],
    [
     "Zhengyang",
     "Chen"
    ],
    [
     "Shuai",
     "Wang"
    ],
    [
     "Haoyu",
     "Wang"
    ],
    [
     "Bing",
     "Han"
    ],
    [
     "Yanmin",
     "Qian"
    ]
   ],
   "title": "DF-ResNet: Boosting Speaker Verification Performance with Depth-First Design",
   "original": "484",
   "page_count": 5,
   "order": 60,
   "p1": 296,
   "pn": 300,
   "abstract": [
    "Embeddings extracted by deep neural networks have become the state-of-the-art utterance representation in speaker verification (SV). Despite the various network architectures that have been investigated in previous works, how to design and scale up networks to achieve a better trade-off on performance and complexity in a principled manner has been rarely discussed in the SV field. In this paper, we first systematically study model scaling from the perspective of the depth and width of networks and empirically discover that depth is more important than the width of networks for speaker verification task. Based on this observation, we design a new backbone constructed entirely from standard convolutional network modules by significantly increasing the number of layers while maintaining the network complexity following the depth-first rule and scale it up to obtain a family of much deeper models dubbed DF-ResNets. Comprehensive comparisons with other state-of-the-art systems on the Voxceleb dataset demonstrate that DF-ResNets achieve a much better trade-off than previous SV systems in terms of performance and complexity."
   ],
   "doi": "10.21437/Interspeech.2022-484"
  },
  "ruida22_interspeech": {
   "authors": [
    [
     "Li",
     "Ruida"
    ],
    [
     "Fang",
     "Shuo"
    ],
    [
     "Ma",
     "Chenguang"
    ],
    [
     "Li",
     "Liang"
    ]
   ],
   "title": "Adaptive Rectangle Loss for Speaker Verification",
   "original": "486",
   "page_count": 5,
   "order": 61,
   "p1": 301,
   "pn": 305,
   "abstract": [
    "From the perspective of pair similarity optimization, speaker verification is expected to satisfy the criterion that each intraclass similarity is higher than the maximal inter-class similarity. However, we find that most softmax-based losses are suboptimal which encourages each sample to have a higher target similarity score only than its corresponding non-target similarity scores but not all the non-target ones. To this end, we propose a batch-wise maximum softmax loss, in which the non-target logits are replaced by the ones derived from the whole batch. To further emphasize the minority hard non-target pairs, an adaptive margin mechanism is introduced at the same time. The proposed loss is named Adaptive Rectangle loss due to its rectangle decision boundary. In addition, an annealing strategy is introduced to improve the stability of the training process and boost the convergence. Experimentally, we demonstrate the superiority of adaptive rectangle loss on speaker verification tasks. Results on VoxCeleb show that our proposed loss outperforms state-of-the-art by 10.11% in EER."
   ],
   "doi": "10.21437/Interspeech.2022-486"
  },
  "tripathi22_interspeech": {
   "authors": [
    [
     "Achyut",
     "Tripathi"
    ],
    [
     "Konark",
     "Paul"
    ]
   ],
   "title": "Temporal Self Attention-Based Residual Network for Environmental Sound Classification",
   "original": "488",
   "page_count": 5,
   "order": 308,
   "p1": 1516,
   "pn": 1520,
   "abstract": [
    "Recent years have witnessed a remarkable performance of attention mechanisms for learning representative and prototypical features for tasks such as the classification of distinct sounds and images. Classification of environmental sounds is also an equally challenging task to the classification of speech and music. The presence of semantically irrelevant and silent frames are two major issues that persist in environmental sound classification (ESC). This paper presents a linear self-attention (LSA) mechanism with a learnable memory unit that encodes temporal and spectral characteristics of the spectrogram used while training the deep ESC model. The memory unit can be easily designed using two linear layers followed by a normalization layer. Unlike traditional self-attention mechanisms, the proposed LA mechanism has a linear computational cost. The efficacy of the proposed method is evaluated on two benchmark ESC datasets, viz. ESC-10 and DCASE-2019 Task-1A datasets. The experiments and results show that the model trained with the proposed attention mechanism efficiently learns temporal and spectral information from spectrogram of a signal. The performance of the proposed deep ESC model is comparable or superior to state-of-the-art attention-based deep ESC models."
   ],
   "doi": "10.21437/Interspeech.2022-488"
  },
  "du22b_interspeech": {
   "authors": [
    [
     "Chenpeng",
     "Du"
    ],
    [
     "Yiwei",
     "Guo"
    ],
    [
     "Xie",
     "Chen"
    ],
    [
     "Kai",
     "Yu"
    ]
   ],
   "title": "VQTTS: High-Fidelity Text-to-Speech Synthesis with Self-Supervised VQ Acoustic Feature",
   "original": "489",
   "page_count": 5,
   "order": 324,
   "p1": 1596,
   "pn": 1600,
   "abstract": [
    "The mainstream neural text-to-speech(TTS) pipeline is a cascade system, including an acoustic model(AM) that predicts acoustic feature from the input transcript and a vocoder that generates waveform according to the given acoustic feature. However, the acoustic feature in current TTS systems is typically mel-spectrogram, which is highly correlated along both time and frequency axes in a complicated way, leading to a great difficulty for the AM to predict. Although high-fidelity audio can be generated by recent neural vocoders from ground-truth(GT) mel-spectrogram, the gap between the GT and the predicted mel-spectrogram from AM degrades the performance of the entire TTS system. In this work, we propose VQTTS, consisting of an AM txt2vec and a vocoder vec2wav, which uses self-supervised vector-quantized(VQ) acoustic feature rather than mel-spectrogram. We redesign both the AM and the vocoder accordingly. In particular, txt2vec basically becomes a classification model instead of a traditional regression model while vec2wav uses an additional feature encoder before HifiGAN generator for smoothing the discontinuous quantized feature. Our experiments show that vec2wav achieves better reconstruction performance than HifiGAN when using self-supervised VQ acoustic feature. Moreover, our entire TTS system VQTTS achieves state-of-the-art performance in terms of naturalness among all current publicly available TTS systems."
   ],
   "doi": "10.21437/Interspeech.2022-489"
  },
  "hughes22_interspeech": {
   "authors": [
    [
     "Vincent",
     "Hughes"
    ],
    [
     "Carmen",
     "Llamas"
    ],
    [
     "Thomas",
     "Kettig"
    ]
   ],
   "title": "Eliciting and evaluating likelihood ratios for speaker recognition by human listeners under forensically realistic channel-mismatched conditions",
   "original": "490",
   "page_count": 5,
   "order": 1062,
   "p1": 5238,
   "pn": 5242,
   "abstract": [
    "This paper describes an experiment which elicits and then evaluates LR-like scores from non-expert, human listeners in a speaker recognition task under conditions reflective of forensic casework. In doing so, it provides a framework for comparing and combining listener judgements with the output of ASR systems (or other data-driven speaker recognition approaches). Stimuli consisted of 45 same-speaker and 45 different-speaker pairs of voices from young, male speakers of Standard Southern British English, using 10 second, channel-mismatched samples. 81 listeners provided ratings of the similarity between voices and their typicality within the wider accent population, which in turn were used to calculated LR-like scores. These scores were converted to log LRs via cross-validated logistic regression calibration. Overall, the human listeners produced an EER of 26.67% and a Cllr of 0.773. However, considerable variation was found across individual listeners (13.3-66.7% EER). Fusion of the listener judgements with an x-vector ASR system provided very marginal improvement in performance compared with the ASR system in isolation. Importantly, the magnitude of the four errors made by the ASR system were reduced because of the listener judgements. The implications of this work for forensics will be discussed."
   ],
   "doi": "10.21437/Interspeech.2022-490"
  },
  "xiong22b_interspeech": {
   "authors": [
    [
     "Feifei",
     "Xiong"
    ],
    [
     "Pengyu",
     "Wang"
    ],
    [
     "Zhongfu",
     "Ye"
    ],
    [
     "Jinwei",
     "Feng"
    ]
   ],
   "title": "Joint Estimation of Direction-of-Arrival and Distance for Arrays with Directional Sensors based on Sparse Bayesian Learning",
   "original": "497",
   "page_count": 5,
   "order": 178,
   "p1": 871,
   "pn": 875,
   "abstract": [
    "Source localization with sensor arrays is an active research topic in many areas, such as speaker localization and communication. The existing estimators usually focus on arrays with omnidirectional sensors, but struggle on arrays with directional sensors. In this work, a new method is proposed for locating the near-field sources based on sparse Bayesian learning (SBL), which is capable of integrating the near-field signal model to jointly estimate direction-of-arrival (DOA) and distance. By further considering the directionality of sensors in the signal model which takes full advantage of the magnitude information, the proposed method can handle arrays with both omnidirectional and directional sensors. Simulation results show that the proposed method yields a sharp spatial spectrum, and performs more accurately than traditional near-field Multiple Signal Classification (MUSIC) and Steered-Response Power Phase Transform (SRP-PHAT) for arrays covering heterogeneous directional sensors."
   ],
   "doi": "10.21437/Interspeech.2022-497"
  },
  "zhao22f_interspeech": {
   "authors": [
    [
     "Kaiqi",
     "Zhao"
    ],
    [
     "Hieu",
     "Nguyen"
    ],
    [
     "Animesh",
     "Jain"
    ],
    [
     "Nathan",
     "Susanj"
    ],
    [
     "Athanasios",
     "Mouchtaris"
    ],
    [
     "Lokesh",
     "Gupta"
    ],
    [
     "Ming",
     "Zhao"
    ]
   ],
   "title": "Knowledge Distillation via Module Replacing for Automatic Speech Recognition with Recurrent Neural Network Transducer",
   "original": "500",
   "page_count": 5,
   "order": 899,
   "p1": 4436,
   "pn": 4440,
   "abstract": [
    "Automatic Speech Recognition (ASR) is increasingly used by edge applications such as intelligent virtual assistants. However, state-of-the-art ASR models such as Recurrent Neural Network - Transducer (RNN-T) are computationally intensive on resource-constrained edge devices. Knowledge Distillation (KD) is a promising approach to compress large models by using a large model (”teacher”) to train a small model (”student”). This paper proposes a novel KD method called Log-Curriculum based Module Replacing (LCMR) for RNN-T. LCMR compresses RNN-T and addresses its unique characteristics by replacing teacher modules including multiple LSTM/Dense layers with substitutional student modules that contain less Long Short Term Memory (LSTM)/Dense layers. LCMR employs a novel nonlinear Curriculum Learning driven replacement strategy to further improve the performance by updating replacing rates with a dynamic, smoothing mechanism. Under LCMR, the student and teacher are able to interact at gradient level, and tranfser knowledge more effectively than conventional KD. Evaluation shows that LCMR reduces word-error-rate (WER) by 14.47%-33.24% relative compared to conventional KD."
   ],
   "doi": "10.21437/Interspeech.2022-500"
  },
  "wu22c_interspeech": {
   "authors": [
    [
     "Zixiu",
     "Wu"
    ],
    [
     "Rim",
     "Helaoui"
    ],
    [
     "Diego",
     "Reforgiato Recupero"
    ],
    [
     "Daniele",
     "Riboni"
    ]
   ],
   "title": "Towards Automated Counselling Decision-Making: Remarks on Therapist Action Forecasting on the AnnoMI Dataset",
   "original": "506",
   "page_count": 5,
   "order": 386,
   "p1": 1906,
   "pn": 1910,
   "abstract": [
    "Substantial progress has been made in recent years on natural language processing approaches to counselling conversation analysis. However, few studies have investigated therapist action forecasting, which aims to suggest dialogue actions that the therapist can take in the next turn, partly due to generally limited access to counselling dialogue data resulting from privacy-related constraints. In this work, we leverage a recently released public dataset of therapy conversations and experiment with a range of natural language processing techniques to approach the task of therapist action forecasting with language models. We probe various factors that could impact model performance, including data augmentation, dialogue context length, incorporating therapist/client utterance labels in the input, and contrasting high- and low-quality counselling dialogues. With our findings, we hope to provide insights on this task and inspire future efforts in counselling dialogue analysis."
   ],
   "doi": "10.21437/Interspeech.2022-506"
  },
  "wang22c_interspeech": {
   "authors": [
    [
     "Fan-Lin",
     "Wang"
    ],
    [
     "Hung-Shin",
     "Lee"
    ],
    [
     "Yu",
     "Tsao"
    ],
    [
     "Hsin-Min",
     "Wang"
    ]
   ],
   "title": "Disentangling the Impacts of Language and Channel Variability on Speech Separation Networks",
   "original": "509",
   "page_count": 5,
   "order": 1083,
   "p1": 5343,
   "pn": 5347,
   "abstract": [
    "Because the performance of speech separation is excellent for speech in which two speakers completely overlap, research attention has been shifted to dealing with more realistic scenarios. However, domain mismatch between training/test situations due to factors, such as speaker, content, channel, and environment, remains a severe problem for speech separation. Speaker and environment mismatches have been studied in the existing literature. Nevertheless, there are few studies on speech content and channel mismatches. Moreover, the impacts of language and channel in these studies are mostly tangled. In this study, we create several datasets for various experiments. The results show that the impacts of different languages are small enough to be ignored compared to the impacts of different channels. In our experiments, training on data recorded by Android phones leads to the best generalizability. Moreover, we provide a new solution for channel mismatch by evaluating projection, where the channel similarity can be measured and used to effectively select additional training data to improve the performance of in-the-wild test data."
   ],
   "doi": "10.21437/Interspeech.2022-509"
  },
  "azeemi22_interspeech": {
   "authors": [
    [
     "Abdul Hameed",
     "Azeemi"
    ],
    [
     "Ihsan Ayyub",
     "Qazi"
    ],
    [
     "Agha Ali",
     "Raza"
    ]
   ],
   "title": "Dataset Pruning for Resource-constrained Spoofed Audio Detection",
   "original": "514",
   "page_count": 5,
   "order": 84,
   "p1": 416,
   "pn": 420,
   "abstract": [
    "The performance of neural anti-spoofing models has rapidly improved in recent years due to larger network architectures and better training methodologies. However, these systems require considerable training data for achieving high performance, which makes it challenging to train them in compute-restricted environments. To make these systems accessible in resource-constrained environments, we consider the task of training neural anti-spoofing models with limited training data. We apply multiple dataset pruning techniques to the ASVspoof 2019 dataset for selecting the most informative training examples and pruning a significant chunk of the data with minimal decrease in performance. We find that the existing pruning metrics are not simultaneously granular and stable. To address this problem and further improve the performance of anti-spoofing models on pruned data, we propose a new metric, Forgetting Norm, to score individual training examples with higher granularity. Extensive experiments on two anti-spoofing models, AASIST-L and RawNet2, and several pruning settings demonstrate up to 23% relative improvement with forgetting norm over other baseline pruning heuristics. We also demonstrate the desirable properties of the proposed metric by analyzing the training landscape of the neural anti-spoofing models."
   ],
   "doi": "10.21437/Interspeech.2022-514"
  },
  "li22f_interspeech": {
   "authors": [
    [
     "Juncheng",
     "Li"
    ],
    [
     "Shuhui",
     "Qu"
    ],
    [
     "Po-Yao",
     "Huang"
    ],
    [
     "Florian",
     "Metze"
    ]
   ],
   "title": "AudioTagging Done Right: 2nd comparison of deep learning methods for environmental sound classification",
   "original": "515",
   "page_count": 5,
   "order": 309,
   "p1": 1521,
   "pn": 1525,
   "abstract": [
    "After its sweeping success in vision and language tasks, pure attention-based neural architectures (e.g. DeiT) are emerging to the top of audio tagging (AT) leaderboards, which seemingly obsoletes traditional convolutional neural networks (CNNs), feed-forward networks or recurrent networks. However, taking a closer look, there is great variability in published research, for instance, performances of models initialized with pretrained weights differ drastically from without pretraining training time for a model varies from hours to weeks, and often, essences are hidden in seemingly trivial details. This urgently calls for a comprehensive study since our 1st comparsion is half-decade old. In this work, we perform extensive experiments on AudioSet~\\cite{gemmeke2017audio} which is the largest weakly-labeled sound event dataset available, we also did analysis based on the data quality and efficiency. We compare a few state-of-the-art baselines on the AT task, and study the performance and efficiency of 2 major categories of neural architectures: CNN variants and attention-based variants. We also closely examine their optimization procedures. Our opensourced experimental results{https://github.com/lijuncheng16/AllaboutAudioSet} provide insights to trade off between performance, efficiency, optimization process, for both practitioners and researchers."
   ],
   "doi": "10.21437/Interspeech.2022-515"
  },
  "cao22_interspeech": {
   "authors": [
    [
     "Ruizhe",
     "Cao"
    ],
    [
     "Sherif",
     "Abdulatif"
    ],
    [
     "Bin",
     "Yang"
    ]
   ],
   "title": "CMGAN: Conformer-based Metric GAN for Speech Enhancement",
   "original": "517",
   "page_count": 5,
   "order": 191,
   "p1": 936,
   "pn": 940,
   "abstract": [
    "Recently, convolution-augmented transformer (Conformer) has achieved promising performance in automatic speech recognition (ASR) and time-domain speech enhancement (SE), as it can capture both local and global dependencies in the speech signal. In this paper, we propose a conformer-based metric generative adversarial network (CMGAN) for SE in the time-frequency (TF) domain. In the generator, we utilize two-stage conformer blocks to aggregate all magnitude and complex spectrogram information by modeling both time and frequency dependencies. The estimation of magnitude and complex spectrogram is decoupled in the decoder stage and then jointly incorporated to reconstruct the enhanced speech. In addition, a metric discriminator is employed to further improve the quality of the enhanced estimated speech by optimizing the generator with respect to a corresponding evaluation score. Quantitative analysis on Voice Bank+DEMAND dataset indicates the capability of CMGAN in outperforming various previous models with a margin, i.e., PESQ of 3.41 and SSNR of 11.10 dB."
   ],
   "doi": "10.21437/Interspeech.2022-517"
  },
  "wang22d_interspeech": {
   "authors": [
    [
     "Bruce Xiao",
     "Wang"
    ],
    [
     "Vincent",
     "Hughes"
    ]
   ],
   "title": "Reducing uncertainty at the score-to-LR stage in likelihood ratio-based forensic voice comparison using automatic speaker recognition systems",
   "original": "518",
   "page_count": 5,
   "order": 1063,
   "p1": 5243,
   "pn": 5247,
   "abstract": [
    "In data-driven forensic voice comparison (FVC), empirical testing of a system is an essential step to demonstrate validity and reliability. Numerous studies have focused on improving system validity, while studies of reliability are comparatively limited. In the present study, simulated scores were generated from i-vector and GMM-UBM automatic speaker recognition systems using real speech data to demonstrate the variability in system reliability as a function of score skewness, sample size, and calibration methods (logistic regression or a Bayesian model). Using logistic regression with small samples of skewed scores, Cllr range is 1.3 for the i-vector system and 0.69 for the GMM-UBM system. When scores follow a normal distribution, Cllr ranges reduce to 0.49 (i-vector) and 0.69 (GMM-UBM). Using the Bayesian model, the Cllr ranges are 0.31 and 0.60 for i-vector and GMM-UBM systems respectively when scores are skewed, and the Cllr range remains stable when scores follow a normal distribution irrespective of sample size. The results suggests that score skewness has a substantial effect on system reliability. With this in mind, in FVC it may be preferable to use an older generation of system which produces less variable results, but slightly weaker discrimination, especially when sample size is small."
   ],
   "doi": "10.21437/Interspeech.2022-518"
  },
  "huang22b_interspeech": {
   "authors": [
    [
     "Kuan Po",
     "Huang"
    ],
    [
     "Yu-Kuan",
     "Fu"
    ],
    [
     "Yu",
     "Zhang"
    ],
    [
     "Hung-yi",
     "Lee"
    ]
   ],
   "title": "Improving Distortion Robustness of Self-supervised Speech Processing Tasks with Domain Adaptation",
   "original": "519",
   "page_count": 5,
   "order": 447,
   "p1": 2193,
   "pn": 2197,
   "abstract": [
    "Speech distortions are a long-standing problem that degrades the performance of supervisely trained speech processing models. It is high time that we enhance the robustness of speech processing models to obtain good performance when encountering speech distortions while not hurting the original performance on clean speech. In this work, we propose to improve the robustness of speech processing models by domain adversarial training (DAT). We conducted experiments based on the SUPERB framework on five different speech processing tasks. In case we do not always have knowledge of the distortion types for speech data, we analyzed the binary-domain and multi-domain settings, where the former treats all distorted speech as one domain, and the latter views different distortions as different domains. In contrast to supervised training methods, we obtained promising results in target domains where speech data is distorted with different distortions including new unseen distortions introduced during testing."
   ],
   "doi": "10.21437/Interspeech.2022-519"
  },
  "wu22d_interspeech": {
   "authors": [
    [
     "Ho-Hsiang",
     "Wu"
    ],
    [
     "Magdalena",
     "Fuentes"
    ],
    [
     "Prem",
     "Seetharaman"
    ],
    [
     "Juan Pablo",
     "Bello"
    ]
   ],
   "title": "How to Listen? Rethinking Visual Sound Localization",
   "original": "520",
   "page_count": 5,
   "order": 179,
   "p1": 876,
   "pn": 880,
   "abstract": [
    "Localizing visual sounds consists of locating the position of objects that emit sound within an image. It is a growing research area with potential applications in monitoring natural and urban environments, such as wildlife migration and urban traffic. Previous works were usually evaluated with datasets having mostly a single dominant visible object, and their proposed models usually require the introduction of localization modules during training or dedicated sampling strategies, but it remains unclear how these design choices play a role in the adaptability of these methods in more challenging scenarios. In this work, we analyze various model choices for visual sound localization and discuss how their different components affect the model's performance, namely the encoders' architecture, the loss function and the localization strategy. Furthermore, we study the interaction between these decisions, the model performance, and the data, by digging into different evaluation datasets spanning different difficulties and characteristics, and discuss the implications of such decisions in the context of real-world applications. Our code and model weights are open-sourced and made available for further applications."
   ],
   "doi": "10.21437/Interspeech.2022-520"
  },
  "nossier22_interspeech": {
   "authors": [
    [
     "Soha",
     "Nossier"
    ],
    [
     "Julie",
     "Wall"
    ],
    [
     "Mansour",
     "Moniri"
    ],
    [
     "Cornelius",
     "Glackin"
    ],
    [
     "Nigel",
     "Cannings"
    ]
   ],
   "title": "Convolutional Recurrent Smart Speech Enhancement Architecture for Hearing Aids",
   "original": "522",
   "page_count": 5,
   "order": 1100,
   "p1": 5428,
   "pn": 5432,
   "abstract": [
    "Speech enhancement is the process of removing noise to improve speech quality and intelligibility for applications including hearing aids. Many deep neural networks for speech enhancement have shown great ability in eliminating noise, regardless of its type. In hearing aids, this process may result in removing important noise used in emergency situations, such as fire alarms and car horns. In order to prevent this, a smart speech enhancement architecture is presented in this paper, where a convolution based noise classifier is used to detect emergency noise and activates the speech enhancement model to run in an audio enhancement mode, in which both the emergency noise and the speech are the target system output. The developed speech enhancement model is a deep convolutional recurrent network with several dilated layers to improve feature extraction without increasing network complexity. The results show that the speech enhancement model outperforms state of the art architectures by a 0.22 increase in the PESQ score. Moreover, the smart speech enhancement architecture improves speech and emergency noise quality when evaluated using objective metrics for both normal and hearing-impaired listeners."
   ],
   "doi": "10.21437/Interspeech.2022-522"
  },
  "berg22_interspeech": {
   "authors": [
    [
     "Axel",
     "Berg"
    ],
    [
     "Mark",
     "O'Connor"
    ],
    [
     "Kalle",
     "Åström"
    ],
    [
     "Magnus",
     "Oskarsson"
    ]
   ],
   "title": "Extending GCC-PHAT using Shift Equivariant Neural Networks",
   "original": "524",
   "page_count": 5,
   "order": 363,
   "p1": 1791,
   "pn": 1795,
   "abstract": [
    "Speaker localization using microphone arrays depends on accurate time delay estimation techniques. For decades, methods based on the generalized cross correlation with phase transform (GCC-PHAT) have been widely adopted for this purpose. Recently, the GCC-PHAT has also been used to provide input features to neural networks in order to remove the effects of noise and reverberation, but at the cost of losing theoretical guarantees in noise-free conditions. We propose a novel approach to extending the GCC-PHAT, where the received signals are filtered using a shift equivariant neural network that preserves the timing information contained in the signals. By extensive experiments we show that our model consistently reduces the error of the GCC-PHAT in adverse environments, with guarantees of exact time delay recovery in ideal conditions."
   ],
   "doi": "10.21437/Interspeech.2022-524"
  },
  "wang22e_interspeech": {
   "authors": [
    [
     "Zhihan",
     "Wang"
    ],
    [
     "Feng",
     "Hou"
    ],
    [
     "Yuanhang",
     "Qiu"
    ],
    [
     "Zhizhong",
     "Ma"
    ],
    [
     "Satwinder",
     "Singh"
    ],
    [
     "Ruili",
     "Wang"
    ]
   ],
   "title": "CyclicAugment: Speech Data Random Augmentation with Cosine Annealing Scheduler for Automatic Speech Recognition",
   "original": "526",
   "page_count": 5,
   "order": 782,
   "p1": 3859,
   "pn": 3863,
   "abstract": [
    "Recent speech data augmentation approaches use static augmentationoperations or policies with consistency magnitude scaling. However, few work is done to explore the influence of the dynamic magnitude of augmentation policies. In this paper, we propose a novel speech data augmentation approach, CyclicAugment, to generate more diversified augmentation policies by dynamically configuring the magnitude of augmentation policies with a cosine annealing scheduler. We also propose additional augmentation operations to enlarge the diversity of augmentation policies. Motivated by learning rate warm restart and cyclical learning rates, we hypothesize that using dynamically configured magnitude for augmentation policies can also help escape local optima more efficiently than static augmentation policies with consistency magnitude scaling. Experimental results demonstrate that our approach is effective for escaping local optima. Our approach achieves 12%-35% relative improvement in word error rate (WER) over SpecAugment and RandAugment on the LibriSpeech 960h dataset, and achieves state-of-the-art result 7.1% in phoneme error rate (PER) on the TIMIT 5h dataset."
   ],
   "doi": "10.21437/Interspeech.2022-526"
  },
  "lee22d_interspeech": {
   "authors": [
    [
     "Chi-Chang",
     "Lee"
    ],
    [
     "Cheng-Hung",
     "Hu"
    ],
    [
     "Yu-Chen",
     "Lin"
    ],
    [
     "Chu-Song",
     "Chen"
    ],
    [
     "Hsin-Min",
     "Wang"
    ],
    [
     "Yu",
     "Tsao"
    ]
   ],
   "title": "NASTAR: Noise Adaptive Speech Enhancement with Target-Conditional Resampling",
   "original": "527",
   "page_count": 5,
   "order": 241,
   "p1": 1183,
   "pn": 1187,
   "abstract": [
    "For deep learning-based speech enhancement (SE) systems, the training-test acoustic mismatch can cause notable performance degradation. To address the mismatch issue, numerous noise adaptation strategies have been derived. In this paper, we propose a novel method, called noise adaptive speech enhancement with target-conditional resampling (NASTAR), which reduces mismatches with only one sample (one-shot) of noisy speech in the target environment. NASTAR uses a feedback mechanism to simulate adaptive training data via a noise extractor and a retrieval model. The noise extractor estimates the target noise from the noisy speech, called pseudo-noise. The noise retrieval model retrieves relevant noise samples from a pool of noise signals according to the noisy speech, called relevant-cohort. The pseudo-noise and the relevant-cohort set are jointly sampled and mixed with the source speech corpus to prepare simulated training data for noise adaptation. Experimental results show that NASTAR can effectively use one noisy speech sample to adapt an SE model to a target condition. Moreover, both the noise extractor and the noise retrieval model contribute to model adaptation. To our best knowledge, NASTAR is the first work to perform one-shot noise adaptation through noise extraction and retrieval."
   ],
   "doi": "10.21437/Interspeech.2022-527"
  },
  "nguyen22b_interspeech": {
   "authors": [
    [
     "Huy",
     "Nguyen"
    ],
    [
     "Kai",
     "Li"
    ],
    [
     "Masashi",
     "Unoki"
    ]
   ],
   "title": "Automatic Mean Opinion Score Estimation with Temporal Modulation Features on Gammatone Filterbank for Speech Assessment",
   "original": "528",
   "page_count": 5,
   "order": 917,
   "p1": 4526,
   "pn": 4530,
   "abstract": [
    "The mean opinion score (MOS) obtained by listening tests is a key component of speech quality evaluation. However, as subjective tests are too costly to conduct on a large scale, it is necessary to estimate the MOS objectively. Thus far, the features used in existing methods for automatic MOS prediction are not based on human perception of speech. In this paper, we propose an automatic MOS estimation method using temporal modulation features on the gammatone filterbank to improve the correlation of the predicted MOS with human perception. We evaluated our method using utterance-level and system-level mean squared errors (MSEs) and Spearman rank correlation coefficients (SRCCs). Compared with the baseline method of the VoiceMOS challenge, the proposed method had a better performance in both utterance-level metrics and system-level SRCC. It also exhibited a significant improvement for utterances with low MOS values."
   ],
   "doi": "10.21437/Interspeech.2022-528"
  },
  "ghosh22_interspeech": {
   "authors": [
    [
     "Arindam",
     "Ghosh"
    ],
    [
     "Mark",
     "Fuhs"
    ],
    [
     "Deblin",
     "Bagchi"
    ],
    [
     "Bahman",
     "Farahani"
    ],
    [
     "Monika",
     "Woszczyna"
    ]
   ],
   "title": "Low-resource Low-footprint Wake-word Detection using Knowledge Distillation",
   "original": "529",
   "page_count": 5,
   "order": 758,
   "p1": 3739,
   "pn": 3743,
   "abstract": [
    "As virtual assistants have become more diverse and specialized, so has the demand for application or brand-specific wake words. However, the wake-word-specific datasets typically used to train wake-word detectors are costly to create. In this paper, we explore two techniques to leverage acoustic modeling data for large-vocabulary speech recognition to improve a purpose-built wake-word detector: transfer learning and knowledge distillation. We also explore how these techniques interact with time-synchronous training targets to improve detection latency. Experiments are presented on the open-source \"Hey Snips” dataset and a more challenging in-house far-field dataset. Using phone-synchronous targets and knowledge distillation from a large acoustic model, we are able to improve accuracy across dataset sizes for both datasets while reducing latency."
   ],
   "doi": "10.21437/Interspeech.2022-529"
  },
  "yeh22_interspeech": {
   "authors": [
    [
     "Sung-Lin",
     "Yeh"
    ],
    [
     "Hao",
     "Tang"
    ]
   ],
   "title": "Autoregressive Co-Training for Learning Discrete Speech Representation",
   "original": "530",
   "page_count": 5,
   "order": 1012,
   "p1": 5000,
   "pn": 5004,
   "abstract": [
    "While several self-supervised approaches for learning discrete speech representation have been proposed, it is unclear how these seemingly similar approaches relate to each other. In this paper, we consider a generative model with discrete latent variables that learns a discrete representation for speech. The objective of learning the generative model is formulated as information-theoretic co-training. Besides the wide generality, the objective can be optimized with several approaches, subsuming HuBERT-like training and vector quantization for learning discrete representation. Empirically, we find that the proposed approach learns discrete representation that is highly correlated with phonetic units, more correlated than HuBERT-like training and vector quantization."
   ],
   "doi": "10.21437/Interspeech.2022-530"
  },
  "qian22_interspeech": {
   "authors": [
    [
     "Fan",
     "Qian"
    ],
    [
     "Hongwei",
     "Song"
    ],
    [
     "Jiqing",
     "Han"
    ]
   ],
   "title": "Word-wise Sparse Attention for Multimodal Sentiment Analysis",
   "original": "532",
   "page_count": 5,
   "order": 403,
   "p1": 1973,
   "pn": 1977,
   "abstract": [
    "For multimodal sentiment analysis (MSA), the text-centric approach has been shown to be superior in performance, which adopts powerful text models (e.g., BERT) as backbone and studies how to effectively incorporate non-verbal modalities (i.e., audio and visual) to obtain more refined and expressive word representations. In previous methods, the non-verbal information injected into a word representation only comes from a non-verbal segment corresponding to the time span of the word, ignoring the long-range dependencies across modalities. Meanwhile, these methods utilize the Softmax normalization function-based attention mechanism, which makes it difficult to highlight the important information in non-verbal sequences. To this end, this paper proposes a non-verbal information injection method called Word-wise Sparse Attention (WSA) to capture the cross-modal long-range dependencies. When injecting the non-verbal information into a word, the word is used as the semantic anchor to search for the most relevant non-verbal information from holistic non-verbal sequences. Furthermore, an advanced Multimodal Adaptive Gating (MAG) mechanism is introduced to determine the amount of information injected from non-verbal modalities. We evaluate our method on the two publicly available multimodal sentiment datasets. Experimental results show that the proposed approach improves the baseline model consistently on all metrics."
   ],
   "doi": "10.21437/Interspeech.2022-532"
  },
  "liu22i_interspeech": {
   "authors": [
    [
     "Rui",
     "Liu"
    ],
    [
     "Berrak",
     "Sisman"
    ],
    [
     "Björn",
     "Schuller"
    ],
    [
     "Guanglai",
     "Gao"
    ],
    [
     "Haizhou",
     "Li"
    ]
   ],
   "title": "Accurate Emotion Strength Assessment for Seen and Unseen Speech Based on Data-Driven Deep Learning",
   "original": "534",
   "page_count": 5,
   "order": 1113,
   "p1": 5493,
   "pn": 5497,
   "abstract": [
    "Emotion classification of speech and assessment of the emotion strength are required in applications such as emotional text-to-speech and voice conversion. The emotion attribute ranking function based on Support Vector Machine (SVM) was proposed to predict emotion strength for emotional speech corpus. However, the trained ranking function doesn't generalize to new domains, which limits the scope of applications, especially for out-of-domain or unseen speech. In this paper, we propose a data-driven deep learning model, i.e. StrengthNet, to improve the generalization of emotion strength assessment for seen and unseen speech. This is achieved by the fusion of emotional data from various domains. We follow a multi-task learning network architecture that includes an acoustic encoder, a strength predictor, and an auxiliary emotion predictor. Experiments show that the predicted emotion strength of the proposed StrengthNet is highly correlated with ground truth scores for both seen and unseen speech. We release the source codes at: https://github.com/ttslr/StrengthNet."
   ],
   "doi": "10.21437/Interspeech.2022-534"
  },
  "nie22_interspeech": {
   "authors": [
    [
     "Mengxi",
     "Nie"
    ],
    [
     "Ming",
     "Yan"
    ],
    [
     "Caixia",
     "Gong"
    ]
   ],
   "title": "Prompt-based Re-ranking Language Model for ASR",
   "original": "536",
   "page_count": 5,
   "order": 783,
   "p1": 3864,
   "pn": 3868,
   "abstract": [
    "In Automatic Speech Recognition(ASR), the language model re-ranking based on unlabeled text can improve the performance and realize flexibly scene adaptation. The scheme of ASR re-ranking is usually to build a language model and then use it to reorder the speech recognition N-best hypotheses. Recently, BERT-based re-ranking has achieved impressive results, benefiting from the powerful modeling capability of contextual semantic. In the view of that BERT's non-autoregressive structure limits the calculation speed of the language model scores(perplexity, ppl), we use a classification method in prompt paradigm instead of the re-ranking method based on ppl. The prompt-based re-ranking scheme simplifies the pipeline of re-ranking as well as ensures the performance. Experiments on AISHELL-1 dataset show the effective of our proposed method. On the test set, the inference speed is accelerated by 49 times and compared to baseline the Character Error Rate(CER) is relatively decreased by 13.51%~14.43%."
   ],
   "doi": "10.21437/Interspeech.2022-536"
  },
  "zhu22_interspeech": {
   "authors": [
    [
     "Jian",
     "Zhu"
    ],
    [
     "Cong",
     "Zhang"
    ],
    [
     "David",
     "Jurgens"
    ]
   ],
   "title": "ByT5 model for massively multilingual grapheme-to-phoneme conversion",
   "original": "538",
   "page_count": 5,
   "order": 90,
   "p1": 446,
   "pn": 450,
   "abstract": [
    "In this study, we tackle massively multilingual grapheme-to-phoneme conversion through implementing G2P models based on ByT5. We have curated a G2P dataset from various sources that covers around 100 languages and trained large-scale multilingual G2P models based on ByT5. We found that ByT5 operating on byte-level inputs significantly outperformed the token-based mT5 model in terms of multilingual G2P. Pairwise comparison with monolingual models in these languages suggests that multilingual ByT5 models generally lower the phone error rate by jointly learning from a variety of languages. The pretrained model can further benefit low resource G2P through zero-shot prediction on unseen languages or provides pretrained weights for finetuning, which helps the model converge to a lower phone error rate than randomly initialized weights. To facilitate future research on multilingual G2P, we make available our code and pretrained multilingual G2P models at: https://github.com/lingjzhu/CharsiuG2P."
   ],
   "doi": "10.21437/Interspeech.2022-538"
  },
  "kim22e_interspeech": {
   "authors": [
    [
     "Jounghee",
     "Kim"
    ],
    [
     "Pilsung",
     "Kang"
    ]
   ],
   "title": "K-Wav2vec 2.0: Automatic Speech Recognition based on Joint Decoding of Graphemes and Syllables",
   "original": "547",
   "page_count": 5,
   "order": 1001,
   "p1": 4945,
   "pn": 4949,
   "abstract": [
    "Wav2vec 2.0 is an end-to-end framework of self-supervised learning for speech representation that is successful in automatic speech recognition (ASR), but most of the work has been developed with a single language: English. Therefore, it is unclear whether the self-supervised framework is effective in recognizing other languages with different writing systems, such as Korean. In this paper, we present K-Wav2Vec 2.0, which is a modified version of Wav2vec 2.0 designed for Korean ASR by exploring and optimizing various factors of the original Wav2vec 2.0. In fine-tuning, we propose a multi-task hierarchical architecture to reflect the Korean writing structure. Moreover, a joint decoder is applied to alleviate the out-of-vocabulary problem. In pre-training, we attempted the cross-lingual transfer of the pre-trained model by further pre-training the English Wav2vec 2.0 on a Korean dataset, considering limited resources. Our experimental results demonstrate that the proposed method efficiently yields robust and better performance on both Korean ASR datasets."
   ],
   "doi": "10.21437/Interspeech.2022-547"
  },
  "lee22e_interspeech": {
   "authors": [
    [
     "Joosung",
     "Lee"
    ]
   ],
   "title": "The Emotion is Not One-hot Encoding: Learning with Grayscale Label for Emotion Recognition in Conversation",
   "original": "551",
   "page_count": 5,
   "order": 29,
   "p1": 141,
   "pn": 145,
   "abstract": [
    "In emotion recognition in conversation (ERC), the emotion of the current utterance is predicted by considering the previous context, which can be utilized in many natural language processing tasks. Although multiple emotions can coexist in a given sentence, most previous approaches take the perspective of a classification task to predict only a given label. However, it is expensive and difficult to label the emotion of a sentence with confidence or multi-label. In this paper, we automatically construct a grayscale label considering the correlation between emotions and use it for learning. That is, instead of using a given label as a one-hot encoding, we construct a grayscale label by measuring scores for different emotions. We introduce several methods for constructing grayscale labels and confirm that each method improves the emotion recognition performance. Our method is simple, effective, and universally applicable to previous systems. The experiments show a significant improvement in the performance of baselines."
   ],
   "doi": "10.21437/Interspeech.2022-551"
  },
  "wang22f_interspeech": {
   "authors": [
    [
     "Qian",
     "Wang"
    ],
    [
     "Chen",
     "Wang"
    ],
    [
     "Jiajun",
     "Zhang"
    ]
   ],
   "title": "Investigating Parameter Sharing in Multilingual Speech Translation",
   "original": "552",
   "page_count": 5,
   "order": 351,
   "p1": 1731,
   "pn": 1735,
   "abstract": [
    "End-to-end multilingual speech translation (ST) directly models the mapping from the speech in source languages to the text of multiple target languages. While multilingual neural machine translation has been proved effective in modeling the general knowledge with shared parameters and handling inter-task interference with language-specific parameters, it still lacks exploration of when and where parameter sharing matters in multilingual ST. This work offers such a study by proposing a comprehensive analysis on the influence of various heuristically designed sharing strategies. We further investigate the inter-task interference through gradient similarity between different tasks, and improve the parameter sharing strategy in multilingual ST under the guidance of inter-task gradient similarity. Experimental results on the one-to-many MuST-C dataset have shown that the gradient-guided sharing method can significantly improve the translation quality with a comparable or even lower cost in terms of parameter scale."
   ],
   "doi": "10.21437/Interspeech.2022-552"
  },
  "bellows22_interspeech": {
   "authors": [
    [
     "Samuel",
     "Bellows"
    ],
    [
     "Timothy W.",
     "Leishman"
    ]
   ],
   "title": "Effect of Head Orientation on Speech Directivity",
   "original": "553",
   "page_count": 5,
   "order": 50,
   "p1": 246,
   "pn": 250,
   "abstract": [
    "The directional characteristics of human speech have many applications in speech acoustics, audio, telecommunications, room acoustical design, and other areas. However, professionals in these fields require carefully conducted, high-resolution, spherical speech directivity measurements taken under distinct circumstances to gain additional insights for their work. Because head orientation and human-body diffraction influence speech radiation, this work explores such effects under various controlled conditions through the changing directivity patterns of a head and torso simulator. The results show that head orientation and body diffraction at low frequencies impact directivities only slightly. However, the effects are more substantial at higher frequencies, particularly above 1 kHz."
   ],
   "doi": "10.21437/Interspeech.2022-553"
  },
  "dutta22_interspeech": {
   "authors": [
    [
     "Satwik",
     "Dutta"
    ],
    [
     "Sarah Anne",
     "Tao"
    ],
    [
     "Jacob C.",
     "Reyna"
    ],
    [
     "Rebecca Elizabeth",
     "Hacker"
    ],
    [
     "Dwight W.",
     "Irvin"
    ],
    [
     "Jay F.",
     "Buzhardt"
    ],
    [
     "John H.L.",
     "Hansen"
    ]
   ],
   "title": "Challenges remain in Building ASR for Spontaneous Preschool Children Speech in Naturalistic Educational Environments",
   "original": "555",
   "page_count": 5,
   "order": 876,
   "p1": 4322,
   "pn": 4326,
   "abstract": [
    "Monitoring child development in terms of speech/language skills has a long-term impact on their overall growth. As student diversity continues to expand in US classrooms, there is a growing need to benchmark social-communication engagement, both from a teacher-student perspective, as well as student-student content. Given various challenges with direct observation, deploying speech technology will assist in extracting meaningful information for teachers. These will help teachers to identify and respond to students in need, immediately impacting their early learning and interest. This study takes a deep dive into exploring various hybrid ASR solutions for low-resource spontaneous preschool (3-5yrs) children (with & without developmental delays) speech, being involved in various activities, and interacting with teachers and peers in naturalistic classrooms. Various out-of-domain corpora over a wide and limited age range, both scripted and spontaneous were considered. Acoustic models based on factorized TDNNs infused with Attention, and both N-gram and RNN language models were considered. Results indicate that young children have significantly different/developing articulation skills as compared to older children. Out-of-domain transcripts of interactions between young children and adults however enhance language model performance. Overall transcription of such data, including various non-linguistic markers, poses additional challenges."
   ],
   "doi": "10.21437/Interspeech.2022-555"
  },
  "li22g_interspeech": {
   "authors": [
    [
     "Xin-Chun",
     "Li"
    ],
    [
     "Jin-Lin",
     "Tang"
    ],
    [
     "Shaoming",
     "Song"
    ],
    [
     "Bingshuai",
     "Li"
    ],
    [
     "Yinchuan",
     "Li"
    ],
    [
     "Yunfeng",
     "Shao"
    ],
    [
     "Le",
     "Gan"
    ],
    [
     "De-Chuan",
     "Zhan"
    ]
   ],
   "title": "Avoid Overfitting User Specific Information in Federated Keyword Spotting",
   "original": "558",
   "page_count": 5,
   "order": 784,
   "p1": 3869,
   "pn": 3873,
   "abstract": [
    "Keyword spotting (KWS) aims to discriminate a specific wake-up word from other signals precisely and efficiently for different users. Recent works utilize various deep networks to train KWS models with all users' speech data centralized without considering data privacy. Federated KWS (FedKWS) could serve as a solution without directly sharing users' data. However, the small amount of data, different user habits, and various accents could lead to fatal problems, e.g., overfitting or weight divergence. Hence, we propose several strategies to encourage the model not to overfit user-specific information in FedKWS. Specifically, we first propose an adversarial learning strategy, which updates the downloaded global model against an overfitted local model and explicitly encourages the global model to capture user-invariant information. Furthermore, we propose an adaptive local training strategy, letting clients with more training data and more uniform class distributions undertake more local update steps. Equivalently, this strategy could weaken the negative impacts of those users whose data is less qualified. Our proposed FedKWS-UI could explicitly and implicitly learn user-invariant information in FedKWS. Abundant experimental results on federated Google Speech Commands verify the effectiveness of FedKWS-UI."
   ],
   "doi": "10.21437/Interspeech.2022-558"
  },
  "song22b_interspeech": {
   "authors": [
    [
     "Jieun",
     "Song"
    ],
    [
     "Hae-Sung",
     "Jeon"
    ],
    [
     "Jieun",
     "Kiaer"
    ]
   ],
   "title": "Use of prosodic and lexical cues for disambiguating wh-words in Korean",
   "original": "561",
   "page_count": 5,
   "order": 17,
   "p1": 81,
   "pn": 85,
   "abstract": [
    "Previous research has shown that the ambiguity of wh-words in Korean can be resolved by prosody. The present study investigated the interplay between prosody and lexical cues in disambiguation. Our written survey results showed that the use of certain adverbs (e.g., a little, once) with a wh-word increases the likelihood of a yes-no question interpretation. The results of our speech production experiment found an interaction of lexical and prosodic cues in the disambiguation. In particular, the presence of a lexical cue affected speakers' phrasing choice, but not the type of Intonational Phrase (IP) boundary tones or acoustic prominence. The finding supports the proposal that speech production is affected by the amount of linguistic information available for speakers. We further suggest how the phrasing structure could affect speakers' choice of the IP boundary tone in Korean."
   ],
   "doi": "10.21437/Interspeech.2022-561"
  },
  "zhang22h_interspeech": {
   "authors": [
    [
     "Yang",
     "Zhang"
    ],
    [
     "Zhiqiang",
     "Lv"
    ],
    [
     "Haibin",
     "Wu"
    ],
    [
     "Shanshan",
     "Zhang"
    ],
    [
     "Pengfei",
     "Hu"
    ],
    [
     "Zhiyong",
     "Wu"
    ],
    [
     "Hung-yi",
     "Lee"
    ],
    [
     "Helen",
     "Meng"
    ]
   ],
   "title": "MFA-Conformer: Multi-scale Feature Aggregation Conformer for Automatic Speaker Verification",
   "original": "563",
   "page_count": 5,
   "order": 62,
   "p1": 306,
   "pn": 310,
   "abstract": [
    "In this paper, we present Multi-scale Feature Aggregation Conformer (MFA-Conformer), an easy-to-implement, simple but effective backbone for automatic speaker verification based on the Convolution-augmented Transformer (Conformer). The architecture of the MFA-Conformer is inspired by recent state-of-the-art models in speech recognition and speaker verification. Firstly, we introduce a convolution subsampling layer to decrease the computational cost of the model. Secondly, we adopt Conformer blocks which combine Transformers and convolution neural networks (CNNs) to capture global and local features effectively. Finally, the output feature maps from all Conformer blocks are concatenated to aggregate multi-scale representations before final pooling. We evaluate the MFA-Conformer on the widely used benchmarks. The best system obtains 0.64%, 1.29% and 1.63% EER on VoxCeleb1-O, SITW.Dev, and SITW.Eval set, respectively. MFA-Conformer significantly outperforms the popular ECAPA-TDNN systems in both recognition performance and inference speed. Last but not the least, the ablation studies clearly demonstrate that the combination of global and local feature learning can lead to robust and accurate speaker embedding extraction. We have also released the code for future comparison."
   ],
   "doi": "10.21437/Interspeech.2022-563"
  },
  "kawano22_interspeech": {
   "authors": [
    [
     "Seiya",
     "Kawano"
    ],
    [
     "Muteki",
     "Arioka"
    ],
    [
     "Akishige",
     "Yuguchi"
    ],
    [
     "Kenta",
     "Yamamoto"
    ],
    [
     "Koji",
     "Inoue"
    ],
    [
     "Tatsuya",
     "Kawahara"
    ],
    [
     "Satoshi",
     "Nakamura"
    ],
    [
     "Koichiro",
     "Yoshino"
    ]
   ],
   "title": "Multimodal Persuasive Dialogue Corpus using Teleoperated Android",
   "original": "565",
   "page_count": 5,
   "order": 470,
   "p1": 2308,
   "pn": 2312,
   "abstract": [
    "We collected a corpus of persuasive dialogues containing multimodal information for building a persuasive dialogue system that encourages users to change their behaviors using multimodal information. The corpus is constructed with an android robot that was remotely controlled by the WoZ method during user interactions with the system. We transcribed the collected speech and annotated dialogue act labels. We also extracted the facial features of the dialogue participants. Pre- and post-questionnaires identified the subjects' personality, their awareness of the target domain of persuasion, the changes in their awareness before/after the persuasion, and whether they agreed to the persuasion during the dialogues. In addition, we conducted a follow-up survey with each subject to investigate whether the persuasion actually affected their behavioral change. Moreover, we built linear classifiers that predict persuasion success to investigate effective features."
   ],
   "doi": "10.21437/Interspeech.2022-565"
  },
  "chang22_interspeech": {
   "authors": [
    [
     "Shuo-Yiin",
     "Chang"
    ],
    [
     "Bo",
     "Li"
    ],
    [
     "Tara",
     "Sainath"
    ],
    [
     "Chao",
     "Zhang"
    ],
    [
     "Trevor",
     "Strohman"
    ],
    [
     "Qiao",
     "Liang"
    ],
    [
     "Yanzhang",
     "He"
    ]
   ],
   "title": "Turn-Taking Prediction for Natural Conversational Speech",
   "original": "566",
   "page_count": 5,
   "order": 369,
   "p1": 1821,
   "pn": 1825,
   "abstract": [
    "While a streaming voice assistant system has been used in many applications, this system typically focuses on unnatural, one-shot interactions assuming input from a single voice query without hesitation or disfluency. However, a common conversational utterance often involves multiple queries with turn-taking, in addition to disfluencies. These disfluencies include pausing to think, hesitations, word lengthening, filled pauses and repeated phrases. This makes doing speech recognition with conversational speech, including one with multiple queries, a challenging task. To better model the conversational interaction, it is critical to discriminate disfluencies and end of query in order to allow the user to hold the floor for disfluencies while having the system respond as quickly as possible when the user has finished speaking. In this paper, we present a turn-taking predictor built on top of the end-to-end (E2E) speech recognizer. Our best system is obtained by jointly optimizing for ASR task and detecting when the user is paused to think or finished speaking."
   ],
   "doi": "10.21437/Interspeech.2022-566"
  },
  "gupta22_interspeech": {
   "authors": [
    [
     "Tarun",
     "Gupta"
    ],
    [
     "Tuan Duc",
     "Truong"
    ],
    [
     "Tran The",
     "Anh"
    ],
    [
     "Eng Siong",
     "Chng"
    ]
   ],
   "title": "Estimation of speaker age and height from speech signal using bi-encoder transformer mixture model",
   "original": "567",
   "page_count": 5,
   "order": 404,
   "p1": 1978,
   "pn": 1982,
   "abstract": [
    "The estimation of speaker characteristics such as age and height is a challenging task, having numerous applications in voice forensic analysis. In this work, we propose a bi-encoder transformer mixture model for speaker age and height estimation. Considering the wide differences in male and female voice characteristics such as differences in formant and fundamental frequencies, we propose the use of two separate transformer encoders for the extraction of specific voice features in the male and female gender, using wav2vec 2.0 as a common-level feature extractor. This architecture reduces the interference effects during backpropagation and improves the generalizability of the model. We perform our experiments on the TIMIT dataset and significantly outperform the current state-of-the-art results on age estimation. Specifically, we achieve root mean squared error (RMSE) of 5.54 years and 6.49 years for male and female age estimation, respectively. Further experiment to evaluate the relative importance of different phonetic types for our task demonstrate that vowel sounds are the most distinguishing for age estimation."
   ],
   "doi": "10.21437/Interspeech.2022-567"
  },
  "chang22b_interspeech": {
   "authors": [
    [
     "Shuo-Yiin",
     "Chang"
    ],
    [
     "Guru",
     "Prakash"
    ],
    [
     "Zelin",
     "Wu"
    ],
    [
     "Tara",
     "Sainath"
    ],
    [
     "Bo",
     "Li"
    ],
    [
     "Qiao",
     "Liang"
    ],
    [
     "Adam",
     "Stambler"
    ],
    [
     "Shyam",
     "Upadhyay"
    ],
    [
     "Manaal",
     "Faruqui"
    ],
    [
     "Trevor",
     "Strohman"
    ]
   ],
   "title": "Streaming Intended Query Detection using E2E Modeling for Continued Conversation",
   "original": "569",
   "page_count": 5,
   "order": 370,
   "p1": 1826,
   "pn": 1830,
   "abstract": [
    "In voice-enabled applications, a predetermined hotword is usually used to activate a device in order to attend to the query. However, speaking queries followed by a hotword each time introduces a cognitive burden in continued conversations. To avoid repeating a hotword, we propose a streaming end-to-end (E2E) intended query detector that identifies the utterances directed towards the device and filters out other utterances not directed towards device. The proposed approach incorporates the intended query detector into the E2E model that already folds different components of the speech recognition pipeline into one neural network. The E2E modeling on speech decoding and intended query detection also allows us to declare a quick intended query detection based on early partial recognition result, which is important to decrease latency and make the system responsive. We demonstrate that the proposed E2E approach yields a 22% relative improvement on equal error rate (EER) for the detection accuracy and 600 ms latency improvement compared with an independent intended query detector. In our experiment, the proposed model detects whether the user is talking to the device with a 8.7% EER within 1.4 seconds of median latency after user starts speaking."
   ],
   "doi": "10.21437/Interspeech.2022-569"
  },
  "xue22_interspeech": {
   "authors": [
    [
     "Liumeng",
     "Xue"
    ],
    [
     "Shan",
     "Yang"
    ],
    [
     "Na",
     "Hu"
    ],
    [
     "Dan",
     "Su"
    ],
    [
     "Lei",
     "Xie"
    ]
   ],
   "title": "Learning Noise-independent Speech Representation for High-quality Voice Conversion for Noisy Target Speakers",
   "original": "570",
   "page_count": 5,
   "order": 518,
   "p1": 2548,
   "pn": 2552,
   "abstract": [
    "Building a voice conversion system for noisy target speakers, such as users providing noisy samples or Internet found data, is a challenging task since the use of contaminated speech in model training will apparently degrade the conversion performance. In this paper, we leverage the advances of our recently proposed Glow-WaveGAN and propose a noise-independent speech representation learning approach for high-quality voice conversion for noisy target speakers. Specifically, we learn a latent feature space where we ensure that the target distribution modeled by the conversion model is exactly from the modeled distribution of the waveform generator. With this premise, we further manage to make the latent feature to be noise-invariant. Specifically, we introduce a noise-controllable WaveGAN, which directly learns the noise-independent acoustic representation from waveform by the encoder and conducts noise control in the hidden space through a FiLM module in the decoder. As for the conversion model, importantly, we use a flow-based model to learn the distribution of noise-independent but speaker-related latent features from phoneme posteriorgrams. Experimental results demonstrate that the proposed model achieves high speech quality and speaker similarity in the voice conversion for noisy target speakers."
   ],
   "doi": "10.21437/Interspeech.2022-570"
  },
  "yang22f_interspeech": {
   "authors": [
    [
     "SiCheng",
     "Yang"
    ],
    [
     "Methawee",
     "Tantrawenith"
    ],
    [
     "Haolin",
     "Zhuang"
    ],
    [
     "Zhiyong",
     "Wu"
    ],
    [
     "Aolan",
     "Sun"
    ],
    [
     "Jianzong",
     "Wang"
    ],
    [
     "Ning",
     "Cheng"
    ],
    [
     "Huaizhen",
     "Tang"
    ],
    [
     "Xintao",
     "Zhao"
    ],
    [
     "Jie",
     "Wang"
    ],
    [
     "Helen",
     "Meng"
    ]
   ],
   "title": "Speech Representation Disentanglement with Adversarial Mutual Information Learning for One-shot Voice Conversion",
   "original": "571",
   "page_count": 5,
   "order": 519,
   "p1": 2553,
   "pn": 2557,
   "abstract": [
    "One-shot voice conversion (VC) with only a single target-speaker speech for reference has become a new research direction. Existing works generally disentangle timbre, while information about pitch, rhythm and content is still mixed together. To perform one-shot VC effectively with further disentangling these speech components, we employ random resampling for pitch and content encoder and use the variational contrastive log-ratio upper bound of mutual information and gradient reversal layer based adversarial mutual information learning to ensure the different parts of the latent space containing only the desired disentanglement during training. Experiments on the VCTK dataset show the model is a state-of-the-art one-shot VC framework in terms of naturalness and intellgibility of converted speech. In addition, we can transfer style of one-shot VC on timbre, pitch and rhythm separately by speech representation disentanglement. Our code, pre-trained models and demo are available at https://im1eon.github.io/IS2022-SRDVC/."
   ],
   "doi": "10.21437/Interspeech.2022-571"
  },
  "jain22_interspeech": {
   "authors": [
    [
     "Arjit",
     "Jain"
    ],
    [
     "Pranay Reddy",
     "Samala"
    ],
    [
     "Deepak",
     "Mittal"
    ],
    [
     "Preethi",
     "Jyothi"
    ],
    [
     "Maneesh",
     "Singh"
    ]
   ],
   "title": "SPLICEOUT: A Simple and Efficient Audio Augmentation Method",
   "original": "572",
   "page_count": 5,
   "order": 544,
   "p1": 2678,
   "pn": 2682,
   "abstract": [
    "Time masking has become a de facto augmentation technique for speech and audio tasks, including automatic speech recognition (ASR) and audio classification, most notably as a part of SpecAugment. In this work, we propose SPLICEOUT, a simple modification to time masking which makes it computationally more efficient. SPLICEOUT performs comparably to (and sometimes outperforms) SpecAugment on a wide variety of speech and audio tasks, including ASR for seven different languages using varying amounts of training data, as well as on speech translation, sound and music classification, thus establishing itself as a broadly applicable audio augmentation method. SPLICEOUT also provides additional gains when used in conjunction with other augmentation techniques. Apart from the fully-supervised setting, we also demonstrate that SPLICEOUT can complement unsupervised representation learning with performance gains in the semi-supervised and self-supervised settings."
   ],
   "doi": "10.21437/Interspeech.2022-572"
  },
  "wang22g_interspeech": {
   "authors": [
    [
     "Xiao",
     "Wang"
    ],
    [
     "Song",
     "Cheng"
    ],
    [
     "Jun",
     "Li"
    ],
    [
     "Shushan",
     "Qiao"
    ],
    [
     "Yumei",
     "Zhou"
    ],
    [
     "Yi",
     "Zhan"
    ]
   ],
   "title": "Low-complex and Highly-performed Binary Residual Neural Network for Small-footprint Keyword Spotting",
   "original": "573",
   "page_count": 5,
   "order": 655,
   "p1": 3233,
   "pn": 3237,
   "abstract": [
    "The hardware power-aware Keyword Spotting (KWS) implementation requires small memory footprint, low-complex computation, and high accuracy performances. In this article, three aspects are introduced to satisfy these three stringent requirements. Firstly, a lightweight Binary Residual Neural Network (B-ResNet) is proposed and applied to the small-footprint KWS. The parameters and calculations inside the net-work are greatly downscaled during the binary quantization. Secondly, during the forward propagation, distribution of the binary activation is optimized by our proposed learnable activation function with fix-valued shift initialization. Thirdly, our variable periodic window (PW) for the backward gradient correction (BGC) is also put forward to avoid gradient mismatch and vanishing problems during the back-propagation. These two improvements effectively increase the accuracy performance during the binarization. Our studies in this article are very helpful and promising for the future hardware KWS implementations."
   ],
   "doi": "10.21437/Interspeech.2022-573"
  },
  "mathur22_interspeech": {
   "authors": [
    [
     "Puneet",
     "Mathur"
    ],
    [
     "Franck",
     "Dernoncourt"
    ],
    [
     "Quan Hung",
     "Tran"
    ],
    [
     "Jiuxiang",
     "Gu"
    ],
    [
     "Ani",
     "Nenkova"
    ],
    [
     "Vlad",
     "Morariu"
    ],
    [
     "Rajiv",
     "Jain"
    ],
    [
     "Dinesh",
     "Manocha"
    ]
   ],
   "title": "DocLayoutTTS: Dataset and Baselines for Layout-informed Document-level Neural Speech Synthesis",
   "original": "574",
   "page_count": 5,
   "order": 91,
   "p1": 451,
   "pn": 455,
   "abstract": [
    "We propose a new task of synthesizing speech directly from semi-structured documents where the extracted text tokens from OCR systems may not be in the correct reading order due to the complex document layout. We refer to this task as layout-informed document-level TTS and present the DocSpeech dataset which consists of 10K audio clips of a single-speaker reading layout-enriched Word document. For each document, we provide the natural reading order of text tokens, their corresponding bounding boxes, and the audio clips synthesized in the correct reading order. We also introduce DocLayoutTTS, a Transformer encoder-decoder architecture that generates speech in an end-to-end manner given a document image with OCR extracted text. Our architecture simultaneously learns text reordering and mel-spectrogram prediction in a multi-task setup. Moreover, we take advantage of curriculum learning to progressively learn longer, more challenging document-level text utilizing both \\texttt{DocSpeech} and LJSpeech datasets. Our empirical results show that the underlying task is challenging. Our proposed architecture performs slightly better than competitive baseline TTS models with a pre-trained model providing reading order priors. We release samples of the DocSpeech dataset."
   ],
   "doi": "10.21437/Interspeech.2022-574"
  },
  "takeda22_interspeech": {
   "authors": [
    [
     "Ryu",
     "Takeda"
    ],
    [
     "Yui",
     "Sudo"
    ],
    [
     "Kazuhiro",
     "Nakadai"
    ],
    [
     "Kazunori",
     "Komatani"
    ]
   ],
   "title": "Empirical Sampling from Latent Utterance-wise Evidence Model for Missing Data ASR based on Neural Encoder-Decoder Model",
   "original": "576",
   "page_count": 5,
   "order": 768,
   "p1": 3789,
   "pn": 3793,
   "abstract": [
    "Missing data automatic speech recognition (MD-ASR) can utilize the uncertainty of speech enhancement (SE) results without re-training of model parameters. Such uncertainty is represented by a probabilistic evidence model, and the design and the expectation calculation of it are important. Two problems arise in applying the MD approach to utterance-wise ASR based on neural encoder-decoder model: the high-dimensionality of an utterance-wise evidence model and the discontinuity among frames of generated samples in approximating the expectation with Monte-Carlo method. We propose new utterance-wise evidence models using a latent variable and an empirical method for sampling from them. The space of our latent model is restricted by simpler conditional probability density functions (pdfs) given the latent variable, which enables us to generate samples from the low-dimensional space in deterministic or stochastic way. Because the variable also works as a common smoothing parameter among simple pdfs, the generated samples are continuous among frames, which improves the ASR performance unlike frame-wise models. The uncertainty from a neural SE is also used as a component in our mixture pdf models. Experiments showed that the character error rate of the enhanced speech was further improved by 2.5 points on average with our MD-ASR using transformer model."
   ],
   "doi": "10.21437/Interspeech.2022-576"
  },
  "huang22c_interspeech": {
   "authors": [
    [
     "Jiahong",
     "Huang"
    ],
    [
     "Wen",
     "Xu"
    ],
    [
     "Yule",
     "Li"
    ],
    [
     "Junshi",
     "Liu"
    ],
    [
     "Dongpeng",
     "Ma"
    ],
    [
     "Wei",
     "Xiang"
    ]
   ],
   "title": "FlowCPCVC: A Contrastive Predictive Coding Supervised Flow Framework for Any-to-Any Voice Conversion",
   "original": "577",
   "page_count": 5,
   "order": 520,
   "p1": 2558,
   "pn": 2562,
   "abstract": [
    "Recently, the research of any-to-any voice conversion(VC) has been developed rapidly. However, they often suffer from unsatisfactory quality and require two stages for training, in which a spectrum generation process is indispensable. In this paper, we propose the FlowCPCVC system, which results in higher speech naturalness and timbre similarity. FlowCPCVC is the first one-stage training system for any-to-any task in our knowledge by taking advantage of VAE and contrastive learning. We employ a speaker encoder to extract timbre information, and a contrastive predictive coding(CPC) based content extractor to guide the flow module to discard the timbre and keeping the linguistic information. Our method directly incorporates the vocoder into the training, thus avoiding the loss of spectral information as in two-stage training. With a fancy method in training any-to-any task, we can also get robust results when using it in any-to-many conversion. Experiments show that FlowCPCVC achieves obvious improvement when compared to VQMIVC which is current state-of-the-art any-to-any voice conversion system. Our demo is available online."
   ],
   "doi": "10.21437/Interspeech.2022-577"
  },
  "shin22_interspeech": {
   "authors": [
    [
     "Hyeon-Kyeong",
     "Shin"
    ],
    [
     "Hyewon",
     "Han"
    ],
    [
     "Doyeon",
     "Kim"
    ],
    [
     "Soo-Whan",
     "Chung"
    ],
    [
     "Hong-Goo",
     "Kang"
    ]
   ],
   "title": "Learning Audio-Text Agreement for Open-vocabulary Keyword Spotting",
   "original": "580",
   "page_count": 5,
   "order": 379,
   "p1": 1871,
   "pn": 1875,
   "abstract": [
    "In this paper, we propose a novel end-to-end user-defined keyword spotting method that utilizes linguistically corresponding patterns between speech and text sequences. Unlike previous approaches requiring speech keyword enrollment, our method compares input queries with an enrolled text keyword sequence. To place the audio and text representations within a common latent space, we adopt an attention-based cross-modal matching approach that is trained in an end-to-end manner with monotonic matching loss and keyword classification loss. We also utilize a de-noising loss for the acoustic embedding network to improve robustness in noisy environments. Additionally, we introduce the LibriPhrase dataset, a new short-phrase dataset based on LibriSpeech for efficiently training keyword spotting models. Our proposed method achieves competitive results on various evaluation sets compared to other single-modal and cross-modal baselines."
   ],
   "doi": "10.21437/Interspeech.2022-580"
  },
  "kim22f_interspeech": {
   "authors": [
    [
     "Juntae",
     "Kim"
    ],
    [
     "Jeehye",
     "Lee"
    ]
   ],
   "title": "Generalizing RNN-Transducer to Out-Domain Audio via Sparse Self-Attention Layers",
   "original": "581",
   "page_count": 5,
   "order": 836,
   "p1": 4123,
   "pn": 4127,
   "abstract": [
    "Recurrent neural network transducer (RNN-T) is an end-to-end speech recognition framework converting input acoustic frames into a character sequence. The state-of-the-art encoder network for RNN-T is the Conformer, which can effectively model the local-global context information via its convolution and self-attention layers. Although Conformer RNN-T has shown outstanding performance, most studies have been verified in the setting where the train and test data are drawn from the same domain. The domain mismatch problem for Conformer RNN-T has not been intensively investigated yet, which is an important issue for the product-level speech recognition system. In this study, we identified that fully connected self-attention layers in the Conformer caused high deletion errors, specifically in the long-form out-domain utterances. To address this problem, we introduce sparse self-attention layers for Conformer-based encoder networks, which can exploit local and generalized global information by pruning most of the in-domain fitted global connections. Also, we propose a state reset method for the generalization of the prediction network to cope with long-form utterances. Applying proposed methods to an out-domain test, we obtained 27.6% relative character error rate (CER) reduction compared to the fully connected self-attention layer-based Conformers."
   ],
   "doi": "10.21437/Interspeech.2022-581"
  },
  "zellers22_interspeech": {
   "authors": [
    [
     "Margaret",
     "Zellers"
    ]
   ],
   "title": "An overview of discourse clicks in Central Swedish",
   "original": "583",
   "page_count": 5,
   "order": 693,
   "p1": 3423,
   "pn": 3427,
   "abstract": [
    "Clicks, ingressive stop sounds produced on a velaric airstream, have been shown to be used for discourse-pragmatic purposes in a number of languages where they are not phonemic. The current work investigates clicks in spontaneous Central Swedish conversation, with a particular focus on their phonetic context and their functionality in discourse. Clicks used to take up a conversational turn or extend a previous one are frequent in Swedish, as are clicks used during a word search or in backchanneling. Unlike reports for some other languages, clicks as markers of stance arise only rarely. While different click functions tend to have different phonetic features surrounding the clicks, the clicks themselves do not appear to be phonetically modified for different functions. This study contributes to the characterization of speech features which do not belong to the linguistic phonological system as such, but which still convey pragmatic meanings in conversational settings."
   ],
   "doi": "10.21437/Interspeech.2022-583"
  },
  "santoso22_interspeech": {
   "authors": [
    [
     "Jennifer",
     "Santoso"
    ],
    [
     "Takeshi",
     "Yamada"
    ],
    [
     "Kenkichi",
     "Ishizuka"
    ],
    [
     "Taiichi",
     "Hashimoto"
    ],
    [
     "Shoji",
     "Makino"
    ]
   ],
   "title": "Performance Improvement of Speech Emotion Recognition by Neutral Speech Detection Using Autoencoder and Intermediate Representation",
   "original": "584",
   "page_count": 5,
   "order": 952,
   "p1": 4700,
   "pn": 4704,
   "abstract": [
    "In recent years, classification-based speech emotion recognition (SER) methods have achieved high overall performance. However, these methods tend to have lower performance for neutral speeches, which account for a large proportion in most practical situations. To solve the problem and improve the SER performance, we propose a neutral speech detector (NSD) based on the anomaly detection approach, which uses an autoencoder, the intermediate layer output of a pretrained SER classifier and only neutral data for training. The intermediate layer output of a pretrained SER classifier enables the reconstruction of both acoustic and text features, which are optimized for SER tasks. We then propose the combination of the SER classifier and the NSD used as a screening mechanism for correcting the class probability of the incorrectly recognized neutral speeches. Results of our experiment using the IEMOCAP dataset indicate that the NSD can reconstruct both the acoustic and textual features, achieving a satisfactory performance for use as a reliable screening method. Furthermore, we evaluated the performance of our proposed screening mechanism, and our experiments show significant improvement of 12.9% in the F-score of the neutral class to 80.3%, and 8.4% in the class-average weighted accuracy to 84.5% compared with state-of-the-art SER classifiers."
   ],
   "doi": "10.21437/Interspeech.2022-584"
  },
  "noguchi22_interspeech": {
   "authors": [
    [
     "Hiroto",
     "Noguchi"
    ],
    [
     "Sanae",
     "Matsui"
    ],
    [
     "Naoya",
     "Watabe"
    ],
    [
     "Chuyu",
     "Huang"
    ],
    [
     "Ayako",
     "Hashimoto"
    ],
    [
     "Ai",
     "Mizoguchi"
    ],
    [
     "Mafuyu",
     "Kitahara"
    ]
   ],
   "title": "VOT and F0 perturbations for the realization of voicing contrast in Tohoku Japanese",
   "original": "587",
   "page_count": 5,
   "order": 694,
   "p1": 3428,
   "pn": 3432,
   "abstract": [
    "Intervocalic voicing neutralization in Tohoku Japanese was studied based on auditory impressions until our previous research revealed this phenomenon by acoustically and quantitatively measuring voice onset time (VOT). While the research excluded fully pre-voiced tokens due to measurement difficulties, the current study included such tokens by using the method proposed in an earlier study. We also measured the onset F0 of the following vowel and preceding vowel duration to discuss the possibility of secondary cues for the voicing contrast. Our result confirmed the overlap in VOT values in the positive region, which agrees with our previous result, and revealed another overlap in the negative region from the fully pre-voiced tokens newly added in the current study. Moreover, a positive linear correlation between VOT and F0, known as consonant-intrinsic F0, was observed in Tohoku Japanese. Although our results did not support F0 of the following vowel and preceding vowel duration as a secondary cue, investigating acoustic cues may contribute to further increasing our understanding of Tohoku Japanese."
   ],
   "doi": "10.21437/Interspeech.2022-587"
  },
  "hu22b_interspeech": {
   "authors": [
    [
     "Hang-Rui",
     "Hu"
    ],
    [
     "Yan",
     "Song"
    ],
    [
     "Li-Rong",
     "Dai"
    ],
    [
     "Ian",
     "McLoughlin"
    ],
    [
     "Lin",
     "Liu"
    ]
   ],
   "title": "Class-Aware Distribution Alignment based Unsupervised Domain Adaptation for Speaker Verification",
   "original": "591",
   "page_count": 5,
   "order": 748,
   "p1": 3689,
   "pn": 3693,
   "abstract": [
    "Existing speaker verification (SV) systems usually suffer from significant performance degradation when applied to a new domain that lies outside the training distribution. Given the unlabeled target-domain dataset, most Unsupervised Domain Adaptation (UDA) methods aim to minimize the distribution divergence between different domains. However, global distribution alignment strategies fail to consider the latent speaker label information and can hardly guarantee the feature discriminative capability in target domain. In this paper, we propose a novel UDA approach called WBDA (Within-class and Between-class Distribution Alignment), which aims to transfer the class-aware information (i.e., within- and between-class distributions) learned from the well-labeled source-domain to unlabeled target-domain. Motivated by the recent progress of self-supervised contrastive learning, the positive and negative pairs are constructed separately for source and target domains, from which the within- and between-class distribution can be estimated. And the SV system can then be learned by jointly optimizing the cross-domain class-aware distribution discrepancy loss and source-domain classification loss in an end-to-end manner. Evaluations on NIST SRE16 and SRE18 achieve a relative performance improvement of about 43.7% and 26.2% over the baseline in terms of Equal Error Rate (EER) separately, significantly outperforming the previous adaption methods based on global distribution alignment."
   ],
   "doi": "10.21437/Interspeech.2022-591"
  },
  "zhao22g_interspeech": {
   "authors": [
    [
     "Jinming",
     "Zhao"
    ],
    [
     "Hao",
     "Yang"
    ],
    [
     "Gholamreza",
     "Haffari"
    ],
    [
     "Ehsan",
     "Shareghi"
    ]
   ],
   "title": "M-Adapter: Modality Adaptation for End-to-End Speech-to-Text Translation",
   "original": "592",
   "page_count": 5,
   "order": 23,
   "p1": 111,
   "pn": 115,
   "abstract": [
    "End-to-end speech-to-text translation models are often initialized with pre-trained speech encoder and pre-trained text decoder. This leads to a significant training gap between pre-training and fine-tuning, largely due to the modality differences between speech outputs from the encoder and text inputs to the decoder. In this work, we aim to bridge the modality gap between speech and text to improve translation quality. We propose M-Adapter, a novel Transformer-based module, to adapt speech representations to text. While shrinking the speech sequence, M-Adapter produces features desired for speech-to-text translation via modelling global and local dependencies of a speech sequence. Our experimental results show that our model outperforms a strong baseline by up to 1 BLEU score on the Must-C En$\\rightarrow$DE dataset.\\footnote{Our code is available at https://github.com/mingzi151/w2v2-st.}"
   ],
   "doi": "10.21437/Interspeech.2022-592"
  },
  "lin22b_interspeech": {
   "authors": [
    [
     "Guan-Ting",
     "Lin"
    ],
    [
     "Shang-Wen",
     "Li"
    ],
    [
     "Hung-yi",
     "Lee"
    ]
   ],
   "title": "Listen, Adapt, Better WER: Source-free Single-utterance Test-time Adaptation for Automatic Speech Recognition",
   "original": "600",
   "page_count": 5,
   "order": 448,
   "p1": 2198,
   "pn": 2202,
   "abstract": [
    "Although deep learning-based end-to-end Automatic Speech Recognition (ASR) has shown remarkable performance in recent years, it suffers severe performance regression on test samples drawn from different data distributions. Test-time Adaptation (TTA), previously explored in the computer vision area, aims to adapt the model trained on source domains to yield better predictions for test samples, often out-of-domain, without accessing the source data. Here, we propose the Single-Utterance Test-time Adaptation (SUTA) framework for ASR, which is the first TTA study in speech area to our best knowledge. The single-utterance TTA is a more realistic setting that does not assume test data are sampled from identical distribution and does not delay on-demand inference due to pre-collection for the batch of adaptation data. SUTA consists of unsupervised objectives with an efficient adaptation strategy. The empirical results demonstrate that SUTA effectively improves the performance of the source ASR model evaluated on multiple out-of-domain target corpora and in-domain test samples."
   ],
   "doi": "10.21437/Interspeech.2022-600"
  },
  "heo22_interspeech": {
   "authors": [
    [
     "Jungwoo",
     "Heo"
    ],
    [
     "Ju-Ho",
     "Kim"
    ],
    [
     "Hyun-seo",
     "Shin"
    ]
   ],
   "title": "Two Methods for Spoofing-Aware Speaker Verification: Multi-Layer Perceptron Score Fusion Model and Integrated Embedding Projector",
   "original": "602",
   "page_count": 5,
   "order": 584,
   "p1": 2878,
   "pn": 2882,
   "abstract": [
    "The use of deep neural networks (DNN) has dramatically elevated the performance of automatic speaker verification (ASV) over the last decade. However, ASV systems can be easily neutralized by spoofing attacks. Therefore, the Spoofing-Aware Speaker Verification (SASV) challenge is designed and held to promote development of systems that can perform ASV considering spoofing attacks by integrating ASV and spoofing countermeasure (CM) systems. In this paper, we propose two back-end systems: multi-layer perceptron score fusion model (MSFM) and integrated embedding projector (IEP) to incorporate ASV and CM systems. The MSFM, score fusion back-end system, derived SASV score utilizing ASV and CM scores and embeddings. On the other hand, IEP combines ASV and CM embeddings into SASV embedding and calculates final SASV score based on the cosine similarity. We effectively integrated ASV and CM systems through proposed MSFM and IEP and achieved the SASV equal error rates 0.56%, 1.32% on the official evaluation trials of the SASV 2022 challenge."
   ],
   "doi": "10.21437/Interspeech.2022-602"
  },
  "shchekotov22_interspeech": {
   "authors": [
    [
     "Ivan",
     "Shchekotov"
    ],
    [
     "Pavel K.",
     "Andreev"
    ],
    [
     "Oleg",
     "Ivanov"
    ],
    [
     "Aibek",
     "Alanov"
    ],
    [
     "Dmitry",
     "Vetrov"
    ]
   ],
   "title": "FFC-SE: Fast Fourier Convolution for Speech Enhancement",
   "original": "603",
   "page_count": 5,
   "order": 242,
   "p1": 1188,
   "pn": 1192,
   "abstract": [
    "Fast Fourier convolution (FFC) is the recently proposed neural operator showing promising performance in several computer vision problems. The FFC operator allows employing large receptive field operations within early layers of the neural network. It was shown to be especially helpful for inpainting of periodic structures which are common in audio processing. In this work, we design neural network architectures which adapt FFC for speech enhancement. We hypothesize that a large receptive field allows these networks to produce more coherent phases than vanilla convolutional models, and validate this hypothesis experimentally. We found that neural networks based on Fast Fourier convolution outperform analogous convolutional models and show better or comparable results with other speech enhancement baselines."
   ],
   "doi": "10.21437/Interspeech.2022-603"
  },
  "liu22j_interspeech": {
   "authors": [
    [
     "Yufei",
     "Liu"
    ],
    [
     "Rao",
     "Ma"
    ],
    [
     "Haihua",
     "Xu"
    ],
    [
     "Yi",
     "He"
    ],
    [
     "Zejun",
     "Ma"
    ],
    [
     "Weibin",
     "Zhang"
    ]
   ],
   "title": "Internal Language Model Estimation Through Explicit Context Vector Learning for Attention-based Encoder-decoder ASR",
   "original": "606",
   "page_count": 5,
   "order": 338,
   "p1": 1666,
   "pn": 1670,
   "abstract": [
    "An end-to-end (E2E) ASR model implicitly learns a prior Internal Language Model (ILM) from the training transcripts. To fuse an external LM using Bayes posterior theory, the log-likelihood produced by the ILM has to be accurately estimated and subtracted. In this paper we propose two novel approaches to estimate the ILM based on Listen-Attend-Spell (LAS) framework. The first method is to replace the context vector of the LAS decoder at every time step with a vector that is learned with training transcripts. Furthermore, we propose another method that uses a lightweight feed-forward network to directly map query vector to context vector in a dynamic sense. Since the context vectors are learned by minimizing the perplexities on training transcripts, and their estimation is independent of encoder output, hence the ILMs are accurately learned for both methods. Experiments show that the ILMs achieve the lowest perplexity, indicating the efficacy of the proposed methods. In addition, they also significantly outperform the shallow fusion method, as well as two previously proposed ILM Estimation (ILME) approaches on several datasets."
   ],
   "doi": "10.21437/Interspeech.2022-606"
  },
  "dinkel22_interspeech": {
   "authors": [
    [
     "Heinrich",
     "Dinkel"
    ],
    [
     "Yongqing",
     "Wang"
    ],
    [
     "Zhiyong",
     "Yan"
    ],
    [
     "Junbo",
     "Zhang"
    ],
    [
     "Yujun",
     "Wang"
    ]
   ],
   "title": "UniKW-AT: Unified Keyword Spotting and Audio Tagging",
   "original": "607",
   "page_count": 5,
   "order": 656,
   "p1": 3238,
   "pn": 3242,
   "abstract": [
    "Within the audio research community and the industry, keyword spotting (KWS) and audio tagging (AT) are seen as two distinct tasks and research fields. However, from a technical point of view, both of these tasks are identical: they predict a label (keyword in KWS, sound event in AT) for some fixed-sized input audio segment. This work proposes UniKW-AT: An initial approach for jointly training both KWS and AT. UniKW-AT enhances the noise-robustness for KWS, while also being able to predict specific sound events and enabling conditional wake-ups on sound events. Our approach extends the AT pipeline with additional labels describing the presence of a keyword. Experiments are conducted on the Google Speech Commands V1 (GSCV1) and the balanced Audioset (AS) datasets. The proposed MobileNetV2 model achieves an accuracy of 97.53% on the GSCV1 dataset and an mAP of 33.4 on the AS evaluation set. Further, we show that significant noise-robustness gains can be observed on a real-world KWS dataset, greatly outperforming standard KWS approaches. Our study shows that KWS and AT can be merged into a single framework without significant performance degradation."
   ],
   "doi": "10.21437/Interspeech.2022-607"
  },
  "li22h_interspeech": {
   "authors": [
    [
     "Tao",
     "Li"
    ],
    [
     "Xinsheng",
     "Wang"
    ],
    [
     "Qicong",
     "Xie"
    ],
    [
     "Zhichao",
     "Wang"
    ],
    [
     "Mingqi",
     "Jiang"
    ],
    [
     "Lei",
     "Xie"
    ]
   ],
   "title": "Cross-speaker Emotion Transfer Based On Prosody Compensation for End-to-End Speech Synthesis",
   "original": "610",
   "page_count": 5,
   "order": 1114,
   "p1": 5498,
   "pn": 5502,
   "abstract": [
    "Cross-speaker emotion transfer speech synthesis aims to synthesize emotional speech for a target speaker by transferring the emotion from reference speech recorded by another (source) speaker. In this task, extracting speaker-independent emotion embedding from reference speech plays an important role. However, the emotional information conveyed by such emotion embedding tends to be weakened in the process to squeeze out the source speaker's timbre information. In response to this problem, a prosody compensation module (PCM) is proposed in this paper to compensate for the emotional information loss. Specifically, the PCM tries to obtain speaker-independent emotional information from the intermediate feature of a pre-trained ASR model. To this end, a prosody compensation encoder with global context (GC) blocks is introduced to obtain global emotional information from the ASR model's intermediate feature. Experiments demonstrate that the proposed PCM can effectively compensate the emotion embedding for the emotional information loss, and meanwhile maintain the timbre of the target speaker. Comparisons with state-of-the-art models show that our proposed method presents obvious superiority on the cross-speaker emotion transfer task."
   ],
   "doi": "10.21437/Interspeech.2022-610"
  },
  "lin22c_interspeech": {
   "authors": [
    [
     "Guan-Ting",
     "Lin"
    ],
    [
     "Yung-Sung",
     "Chuang"
    ],
    [
     "Ho-Lam",
     "Chung"
    ],
    [
     "Shu-wen",
     "Yang"
    ],
    [
     "Hsuan-Jui",
     "Chen"
    ],
    [
     "Shuyan Annie",
     "Dong"
    ],
    [
     "Shang-Wen",
     "Li"
    ],
    [
     "Abdelrahman",
     "Mohamed"
    ],
    [
     "Hung-yi",
     "Lee"
    ],
    [
     "Lin-shan",
     "Lee"
    ]
   ],
   "title": "DUAL: Discrete Spoken Unit Adaptive Learning for Textless Spoken Question Answering",
   "original": "612",
   "page_count": 5,
   "order": 1045,
   "p1": 5165,
   "pn": 5169,
   "abstract": [
    "Spoken Question Answering (SQA) is to find the answer from a spoken document given a question, which is crucial for personal assistants when replying to the queries from the users. Existing SQA methods all rely on Automatic Speech Recognition (ASR) transcripts. Not only does ASR need to be trained with massive annotated data that are time and cost-prohibitive to collect for low-resourced languages, but more importantly, very often the answers to the questions include name entities or out-of-vocabulary words that cannot be recognized correctly. Also, ASR aims to minimize recognition errors equally over all words, including many function words irrelevant to the SQA task. Therefore, SQA without ASR transcripts (textless) is always highly desired, although known to be very difficult. This work proposes Discrete Spoken Unit Adaptive Learning (DUAL), leveraging unlabeled data for pre-training and fine-tuned by the SQA downstream task. The time intervals of spoken answers can be directly predicted from spoken documents. We also release a new SQA benchmark corpus, NMSQA, for data with more realistic scenarios. We empirically showed that DUAL yields results comparable to those obtained by cascading ASR and text QA model and robust to real-world data. Our code and model will be open-sourced."
   ],
   "doi": "10.21437/Interspeech.2022-612"
  },
  "bensimon22_interspeech": {
   "authors": [
    [
     "Talia",
     "Ben Simon"
    ],
    [
     "Felix",
     "Kreuk"
    ],
    [
     "Faten",
     "Awwad"
    ],
    [
     "Jacob T.",
     "Cohen"
    ],
    [
     "Joseph",
     "Keshet"
    ]
   ],
   "title": "Correcting Mispronunciations in Speech using Spectrogram Inpainting",
   "original": "615",
   "page_count": 5,
   "order": 246,
   "p1": 1208,
   "pn": 1212,
   "abstract": [
    "Learning a new language involves constantly comparing speech productions with reference productions from the environment. Early in speech acquisition, children make articulatory adjustments to match their caregivers' speech. Grownup learners of a language tweak their speech to match the tutor reference. This paper proposes a method to synthetically generate correct pronunciation feedback given incorrect production. Furthermore, our aim is to generate the corrected production while maintaining the speaker's original voice. The system prompts the user to pronounce a phrase. The speech is recorded, and the samples associated with the inaccurate phoneme are masked with zeros. This waveform serves as an input to a speech generator, implemented as a deep learning inpainting system with a U-net architecture, and trained to output a reconstructed speech. The training set is composed of unimpaired proper speech examples, and the generator is trained to reconstruct the original proper speech. We evaluated the performance of our system on phoneme replacement of minimal pair words of English as well as on children with pronunciation disorders. Results suggest that human listeners slightly prefer our generated speech over a smoothed replacement of the inaccurate phoneme with a production of a different speaker."
   ],
   "doi": "10.21437/Interspeech.2022-615"
  },
  "wang22h_interspeech": {
   "authors": [
    [
     "Xin",
     "Wang"
    ],
    [
     "Chuan",
     "Xie"
    ],
    [
     "Qiang",
     "Wu"
    ],
    [
     "Huayi",
     "Zhan"
    ],
    [
     "Ying",
     "Wu"
    ]
   ],
   "title": "A Novel Phoneme-based Modeling for Text-independent Speaker Identification",
   "original": "617",
   "page_count": 5,
   "order": 967,
   "p1": 4775,
   "pn": 4779,
   "abstract": [
    "Text-independent speaker identification attracted growing attention while it remains challenging to extract speaker-specific features from a speech with arbitrary content. End-to-end systems trained with utterance-level features suffer from performance degradation caused by speech content variation. To address this issue, this paper proposes a novel phoneme-based approach with the following key features: first, it restricts the variety of speech content by splitting each utterance into a set of phoneme segments and develops the phoneme-constrained models to extract segment-level embeddings of speakers; second, it leverages a soft-voting mechanism with mono-phonemic thresholds and weights to combine the results of different phonemes. Experimental results on AISHELL and ASRU2019 datasets show that the proposed approach is effective and robust, which outperforms the state-of-the-art methods in both EER and accuracy, especially with a larger phonemic mismatch between the enrollment and test utterances. In addition, the proposed system is efficient that can be trained well on a small-scale dataset."
   ],
   "doi": "10.21437/Interspeech.2022-617"
  },
  "wei22_interspeech": {
   "authors": [
    [
     "Zeyuan",
     "Wei"
    ],
    [
     "Li",
     "Hao"
    ],
    [
     "Xueliang",
     "Zhang"
    ]
   ],
   "title": "Model Compression by Iterative Pruning with Knowledge Distillation and Its Application to Speech Enhancement",
   "original": "619",
   "page_count": 5,
   "order": 192,
   "p1": 941,
   "pn": 945,
   "abstract": [
    "Over the past decade, deep learning has demonstrated its effectiveness and keeps setting new records in a wide variety of tasks. However, good model performance usually leads to a huge amount of parameters and extremely high computational complexity which greatly limit the use cases of deep learning models, particularly in embedded systems. Therefore, model compression is getting more and more attention. In this paper, we propose a compression strategy based on iterative pruning and knowledge distillation. Specifically, in each iteration, we first utilize a pruning criterion to drop the weights which have less impact on performance. Then, the model before pruning is used as a teacher to fine-tune the student which is the model after pruning. After several iterations, we get the final compressed model. The proposed method is verified on gated convolutional recurrent network (GCRN) and long short-term memory (LSTM) for single-channel speech enhancement task. Experimental results show that the proposed compression strategy can dramatically reduce the model size by 40x without significant performance degradation for GCRN."
   ],
   "doi": "10.21437/Interspeech.2022-619"
  },
  "zhang22i_interspeech": {
   "authors": [
    [
     "Guangyan",
     "Zhang"
    ],
    [
     "Kaitao",
     "Song"
    ],
    [
     "Xu",
     "Tan"
    ],
    [
     "Daxin",
     "Tan"
    ],
    [
     "Yuzi",
     "Yan"
    ],
    [
     "Yanqing",
     "Liu"
    ],
    [
     "Gang",
     "Wang"
    ],
    [
     "Wei",
     "Zhou"
    ],
    [
     "Tao",
     "Qin"
    ],
    [
     "Tan",
     "Lee"
    ],
    [
     "Sheng",
     "Zhao"
    ]
   ],
   "title": "Mixed-Phoneme BERT: Improving BERT with Mixed Phoneme and Sup-Phoneme Representations for Text to Speech",
   "original": "621",
   "page_count": 5,
   "order": 92,
   "p1": 456,
   "pn": 460,
   "abstract": [
    "Recently, leveraging BERT pre-training to improve the phoneme encoder in text to speech (TTS) has drawn increasing attention. However, the works apply pre-training with character-based units to enhance the TTS phoneme encoder, which is inconsistent with the TTS fine-tuning that takes phonemes as input. Pre-training only with phonemes as input can alleviate the input mismatch but lack the ability to model rich representations and sematic information due to limited phoneme vocabulary. In this paper, we propose Mixed-Phoneme BERT, a novel variant of the BERT model that uses mixed phoneme and sup-phoneme representations to enhance the learning capability. Specifically, we merge the adjacent phonemes into sup-phonemes and combine the phoneme sequence and the merged sup-phoneme sequence as the model input, which can enhance the model capacity to learn rich contextual representations. Experiment results demonstrate that our proposed Mixed-Phoneme BERT significantly improves the TTS performance with 0.30 CMOS gain compared with the FastSpeech 2 baseline. The Mixed-Phoneme BERT achieves $3\\times$ inference speedup and similar voice quality to the previous TTS pre-trained model PnG BERT."
   ],
   "doi": "10.21437/Interspeech.2022-621"
  },
  "meng22b_interspeech": {
   "authors": [
    [
     "Weixin",
     "Meng"
    ],
    [
     "Chengshi",
     "Zheng"
    ],
    [
     "Xiaodong",
     "Li"
    ]
   ],
   "title": "Fully Automatic Balance between Directivity Factor and White Noise Gain for Large-scale Microphone Arrays in Diffuse Noise Fields",
   "original": "625",
   "page_count": 5,
   "order": 1101,
   "p1": 5433,
   "pn": 5437,
   "abstract": [
    "In diffuse noise fields, a superdirective beamformer maximizes the directivity factor at the expense of amplifying spatially white noise in low-frequency bands. It is extremely important to balance directivity factor and white noise gain automatically for practical applications, especially when using large-scale microphone arrays. Considering both environmental noise and spatially white noise, this paper proposes two robust adaptive superdirective beamformers that aim to automatically maximize the amount of noise reduction without introducing a constraint on the white noise gain manually. Specifically, one is an adaptive combination of the delay-and-sum and superdirective beamformers, and the other can be regarded as an adaptively regularized superdirective beamformer. We derive that both the optimal combination parameter and the optimal regularization parameter in each time-frequency bin are related to the power spectral density of the spatially white noise and that of the diffuse noise. Simulations and experimental results show that the proposed two robust beamformers are superior to many state-of-the-art beamformers in different environmental noise and spatially white noise levels."
   ],
   "doi": "10.21437/Interspeech.2022-625"
  },
  "abdullah22_interspeech": {
   "authors": [
    [
     "Badr M.",
     "Abdullah"
    ],
    [
     "Bernd",
     "Möbius"
    ],
    [
     "Dietrich",
     "Klakow"
    ]
   ],
   "title": "Integrating Form and Meaning: A Multi-Task Learning Model for Acoustic Word Embeddings",
   "original": "626",
   "page_count": 5,
   "order": 380,
   "p1": 1876,
   "pn": 1880,
   "abstract": [
    "Models of acoustic word embeddings (AWEs) learn to map variable-length spoken word segments onto fixed-dimensionality vector representations such that different acoustic exemplars of the same word are projected nearby in the embedding space. In addition to their speech technology applications, AWE models have been shown to predict human performance on a variety of auditory lexical processing tasks. Current AWE models are based on neural networks and trained in a bottom-up approach that integrates acoustic cues to build up a word representation given an acoustic or symbolic supervision signal. Therefore, these models do not leverage or capture high-level lexical knowledge during the learning process. In this paper, we propose a multi-task learning model that incorporates top-down lexical knowledge into the training procedure of AWEs. Our model learns a mapping between the acoustic input and a lexical representation that encodes high-level information such as word semantics in addition to bottom-up form-based supervision. We experiment with three languages and demonstrate that incorporating lexical knowledge improves the embedding space discriminability and encourages the model to better separate lexical categories."
   ],
   "doi": "10.21437/Interspeech.2022-626"
  },
  "gong22_interspeech": {
   "authors": [
    [
     "Xun",
     "Gong"
    ],
    [
     "Zhikai",
     "Zhou"
    ],
    [
     "Yanmin",
     "Qian"
    ]
   ],
   "title": "Knowledge Transfer and Distillation from Autoregressive to Non-Autoregessive Speech Recognition",
   "original": "632",
   "page_count": 5,
   "order": 532,
   "p1": 2618,
   "pn": 2622,
   "abstract": [
    "Modern non-autoregressive (NAR) speech recognition systems aim to accelerate the inference speed; however, they suffer from performance degradation compared with autoregressive (AR) models as well as the huge model size issue. We propose a novel knowledge transfer and distillation architecture that leverages knowledge from AR models to improve the NAR performance while reducing the model's size. Frame- and sequencelevel objectives are well-designed for transfer learning. To further boost the performance of NAR, a beam search method on Mask-CTC is developed to enlarge the search space during the inference stage. Experiments show that the proposed NAR beam search relatively reduces CER by over 5% on AISHELL1 benchmark with a tolerable real-time-factor (RTF) increment. By knowledge transfer, the NAR student who has the same size as the AR teacher obtains relative CER reductions of 8/16% on AISHELL-1 dev/test sets, and over 25% relative WER reductions on Librispeech test-clean/other sets. Moreover, the ∼9x smaller NAR models achieve ∼25% relative CER/WER reductions on both AISHELL-1 and Librispeech benchmarks with the proposed knowledge transfer and distillation."
   ],
   "doi": "10.21437/Interspeech.2022-632"
  },
  "song22c_interspeech": {
   "authors": [
    [
     "Zeyang",
     "Song"
    ],
    [
     "Qi",
     "Liu"
    ],
    [
     "Qu",
     "Yang"
    ],
    [
     "Haizhou",
     "Li"
    ]
   ],
   "title": "Knowledge distillation for In-memory keyword spotting model",
   "original": "633",
   "page_count": 5,
   "order": 837,
   "p1": 4128,
   "pn": 4132,
   "abstract": [
    "We study a light-weight implementation of keyword spotting (KWS) for voice command and control, that can be implemented on an in-memory computing (IMC) unit with same accuracy at a lower computational cost than the state-of-the-art methods. KWS is expected to be always-on for mobile devices with limited resources. IMC represents one of the solutions. However, it only supports multiplication-accumulation and Boolean operations. We note that common feature extraction methods, such as MFCC and SincConv, are not supported by IMC as they depend on expensive logarithm computing. On the other hand, some neural network solutions to KWS involve a large number of parameters that are not feasible for mobile devices. In this work, we propose a knowledge distillation technique to replace the complex speech frontend like MFCC or SincConv with a light-weight encoder without performance loss. Experiments show that the proposed model outperforms the KWS model with MFCC and SincConv front-end in terms of accuracy and computational cost."
   ],
   "doi": "10.21437/Interspeech.2022-633"
  },
  "hu22c_interspeech": {
   "authors": [
    [
     "Ying",
     "Hu"
    ],
    [
     "Yuwu",
     "Tang"
    ],
    [
     "Hao",
     "Huang"
    ],
    [
     "Liang",
     "He"
    ]
   ],
   "title": "A Graph Isomorphism Network with Weighted Multiple Aggregators for Speech Emotion Recognition",
   "original": "637",
   "page_count": 5,
   "order": 953,
   "p1": 4705,
   "pn": 4709,
   "abstract": [
    "Speech emotion recognition (SER) is an essential part of human-computer interaction. In this paper, we propose an SER network based on a Graph Isomorphism Network with Weighted Multiple Aggregators (WMA-GIN), which can effectively handle the problem of information confusion when neighbour nodes' features are aggregated together in GIN structure. Moreover, a Full-Adjacent (FA) layer is adopted for alleviating the over-squashing problem, which is existed in all Graph Neural Network (GNN) structures, including GIN. Furthermore, a multi-phase attention mechanism and multi-loss training strategy are employed to avoid missing the useful emotional information in the stacked WMA-GIN layers. We evaluated the performance of our proposed WMA-GIN on the popular IEMOCAP dataset. The experimental results show that WMA-GIN outperforms other GNN-based methods and is comparable to some advanced non-graph-based methods by achieving 72.48% of weighted accuracy (WA) and 67.72% of unweighted accuracy (UA)."
   ],
   "doi": "10.21437/Interspeech.2022-637"
  },
  "nakata22_interspeech": {
   "authors": [
    [
     "Wataru",
     "Nakata"
    ],
    [
     "Tomoki",
     "Koriyama"
    ],
    [
     "Shinnosuke",
     "Takamichi"
    ],
    [
     "Yuki",
     "Saito"
    ],
    [
     "Yusuke",
     "Ijima"
    ],
    [
     "Ryo",
     "Masumura"
    ],
    [
     "Hiroshi",
     "Saruwatari"
    ]
   ],
   "title": "Predicting VQVAE-based Character Acting Style from Quotation-Annotated Text for Audiobook Speech Synthesis",
   "original": "638",
   "page_count": 5,
   "order": 922,
   "p1": 4551,
   "pn": 4555,
   "abstract": [
    "We propose a speech-synthesis model for predicting appropriate voice styles on the basis of the character-annotated text for audiobook speech synthesis. An audiobook is more engaging when the narrator makes distinctive voices depending on the story characters. Our goal is to produce such distinctive voices in the speech-synthesis framework. However, such distinction has not been extensively investigated in audiobook speech synthesis. To enable the speech-synthesis model to achieve distinctive voices depending on characters with minimum extra annotation, we propose a speech synthesis model to predict character appropriate voices from quotation-annotated text. Our proposed model involves character-acting-style extraction based on a vector quantized variational autoencoder, and style prediction from quotation-annotated texts which enables us to automate audiobook creation with character-distinctive voices from quotation-annotated texts. To the best of our knowledge, this is the first attempt to model intra-speaker voice style depending on character acting for audiobook speech synthesis. We conducted subjective evaluations of our model, and the results indicate that the proposed model generated more distinctive character voices compared to models that do not use the explicit character-acting-style while maintaining the naturalness of synthetic speech."
   ],
   "doi": "10.21437/Interspeech.2022-638"
  },
  "priyasad22_interspeech": {
   "authors": [
    [
     "Darshana",
     "Priyasad"
    ],
    [
     "Andi",
     "Partovi"
    ],
    [
     "Sridha",
     "Sridharan"
    ],
    [
     "Maryam",
     "Kashefpoor"
    ],
    [
     "Tharindu",
     "Fernando"
    ],
    [
     "Simon",
     "Denman"
    ],
    [
     "Clinton",
     "Fookes"
    ],
    [
     "Jia",
     "Tang"
    ],
    [
     "David",
     "Kaye"
    ]
   ],
   "title": "Detecting Heart Failure Through Voice Analysis using Self-Supervised Mode-Based Memory Fusion",
   "original": "643",
   "page_count": 5,
   "order": 578,
   "p1": 2848,
   "pn": 2852,
   "abstract": [
    "Congestive Heart Failure (CHF) is a progressive disease that affects millions of people worldwide, severely impacting their quality of life. Missed detection of CHF and its progression affects life expectancy, thus it is critical to develop applications to continuously monitor CHF symptoms and disease progression in a patient-centric and cost-effective manner. This paper focuses on a novel non-invasive technique to identify CHF using patients' speech traits. Pulmonary congestion and breathlessness is the most common symptom of heart failure and one of the major contributors to hospitalisation. Since pulmonary congestion results in impairment of a patient's voice, we propose a novel, non invasive method for monitoring CHF through analysis of the patient's speech. We also introduce a new balanced dataset, containing voice recordings from both healthy participants and participants diagnosed with CHF, which contains voice alterations reflective of CHF status. We propose a novel deep machine learning architecture based on mode driven memory fusion for CHF recognition from audio recordings of subject's speech. We have achieved 90% accuracy under a subject-independent evaluation setting, highlighting the applicability of such methods for tele-health and home monitoring applications."
   ],
   "doi": "10.21437/Interspeech.2022-643"
  },
  "guan22_interspeech": {
   "authors": [
    [
     "Yuansheng",
     "Guan"
    ],
    [
     "Guochen",
     "Yu"
    ],
    [
     "Andong",
     "Li"
    ],
    [
     "Chengshi",
     "Zheng"
    ],
    [
     "Jie",
     "Wang"
    ]
   ],
   "title": "TMGAN-PLC: Audio Packet Loss Concealment using Temporal Memory Generative Adversarial Network",
   "original": "644",
   "page_count": 5,
   "order": 114,
   "p1": 565,
   "pn": 569,
   "abstract": [
    "Real-time communications in packet-switched networks have become widely used in daily communication, while they inevitably suffer from network delays and data losses in constrained real-time conditions. To solve these problems, audio packet loss concealment (PLC) algorithms have been developed to mitigate voice transmission failures by reconstructing the lost information. Limited by the transmission latency and device memory, it is still intractable for PLC to accomplish high-quality voice reconstruction using a relatively small packet buffer. In this paper, we propose a temporal memory generative adversarial network for audio PLC, dubbed TMGAN-PLC, which is comprised of a novel nested-UNet generator and the time-domain/frequency-domain discriminators. Specifically, a combination of the nested-UNet and temporal feature-wise linear modulation is elaborately devised in the generator to finely adjust the intra-frame information and establish inter-frame temporal dependencies. To complement the missing speech content caused by longer loss bursts, we employ multi-stage gated vector quantizers to capture the correct content and reconstruct the near-real smooth audio. Extensive experiments on the PLC Challenge dataset demonstrate that the proposed method yields promising performance in terms of speech quality, intelligibility, and PLCMOS."
   ],
   "doi": "10.21437/Interspeech.2022-644"
  },
  "zhang22j_interspeech": {
   "authors": [
    [
     "Leying",
     "Zhang"
    ],
    [
     "Zhengyang",
     "Chen"
    ],
    [
     "Yanmin",
     "Qian"
    ]
   ],
   "title": "Enroll-Aware Attentive Statistics Pooling for Target Speaker Verification",
   "original": "645",
   "page_count": 5,
   "order": 63,
   "p1": 311,
   "pn": 315,
   "abstract": [
    "The well-developed robust speaker verification system can remove the environment noise and retain speaker information automatically. However, when the uttering voice is disturbed by another interfering speaker's voice, the speaker verification system usually cannot selectively extract only the target speaker's information. Some works have been done by introducing a speech separation network to separate the target speaker's speech in advance. However, adding a speech separation network for speaker verification task could be redundant. Here, we proposed enroll-aware attentive statistic pooling (EA-ASP) layer to help the speaker verification system extract specific speaker's information. To evaluate the system, we simulate the multi-speaker evaluation data based on Voxceleb1 data. The results show that our proposed EA-ASP can outperform the baseline system by a large margin and achieved 50% relative Equal Error Rate (EER) reduction."
   ],
   "doi": "10.21437/Interspeech.2022-645"
  },
  "qin22_interspeech": {
   "authors": [
    [
     "Xiaoyi",
     "Qin"
    ],
    [
     "Na",
     "Li"
    ],
    [
     "Weng",
     "Chao"
    ],
    [
     "Dan",
     "Su"
    ],
    [
     "Ming",
     "Li"
    ]
   ],
   "title": "Cross-Age Speaker Verification: Learning Age-Invariant Speaker Embeddings",
   "original": "648",
   "page_count": 5,
   "order": 292,
   "p1": 1436,
   "pn": 1440,
   "abstract": [
    "Automatic speaker verification has achieved remarkable progress in recent years. However, there is little research on cross-age speaker verification due to insufficient data. In this paper, we mine cross-age test sets based on the VoxCeleb and propose our age-invariant speaker representation learning method. Since the VoxCeleb is collected from the YouTube platform, the dataset consists of cross-age data inherently. However, the meta-data does not contain the speaker age label. Therefore, we adopt the face age estimation method to predict the speaker age value from the associated visual data, then label the audio recording with the estimated age. We construct multiple Cross-Age test sets on VoxCeleb (Vox-CA), which deliberately select the positive trials with large age-gap. Also, the effect of nationality and gender is considered in selecting negative pairs to align with Vox-H cases. The baseline system performance drops from 1.939% EER on the Vox-H test set to 10.419\\% on the Vox-CA20 test set, which indicates how difficult the cross-age scenario is. Consequently, we propose an age-decoupling adversarial learning (ADAL) method to alleviate the negative effect of the age gap and reduce intra-class variance. Our method outperforms the baseline system by over 10% related EER reduction on the Vox-CA20 test set."
   ],
   "doi": "10.21437/Interspeech.2022-648"
  },
  "zhong22_interspeech": {
   "authors": [
    [
     "Guolong",
     "Zhong"
    ],
    [
     "Hongyu",
     "Song"
    ],
    [
     "Ruoyu",
     "Wang"
    ],
    [
     "Lei",
     "Sun"
    ],
    [
     "Diyuan",
     "Liu"
    ],
    [
     "Jia",
     "Pan"
    ],
    [
     "Xin",
     "Fang"
    ],
    [
     "Jun",
     "Du"
    ],
    [
     "Jie",
     "Zhang"
    ],
    [
     "Lirong",
     "Dai"
    ]
   ],
   "title": "External Text Based Data Augmentation for Low-Resource Speech Recognition in the Constrained Condition of OpenASR21 Challenge",
   "original": "649",
   "page_count": 5,
   "order": 984,
   "p1": 4860,
   "pn": 4864,
   "abstract": [
    "This paper describes our USTC_NELSLIP system submitted to the Open Automatic Speech Recognition (OpenASR21) Challenge for the Constrained condition, where only a 10-hour speech dataset is allowed for training while additional text data is unlimited. To improve the low-resource speech recognition performance, we collect external text data for language modeling and train a text-to-speech (TTS) model to generate speech-text paired data. Our system is then built based on the conventional hybrid structure, where various subsystems are developed using different acoustic neural network architectures and different data augmentation methods. Finally, system fusion is employed to obtain the final result. Experiments on the OpenASR21 challenge show that the proposed system achieves the best performance for all testing languages."
   ],
   "doi": "10.21437/Interspeech.2022-649"
  },
  "zhang22k_interspeech": {
   "authors": [
    [
     "Yi-Kai",
     "Zhang"
    ],
    [
     "Da-Wei",
     "Zhou"
    ],
    [
     "Han-Jia",
     "Ye"
    ],
    [
     "De-Chuan",
     "Zhan"
    ]
   ],
   "title": "Audio-Visual Generalized Few-Shot Learning with Prototype-Based Co-Adaptation",
   "original": "652",
   "page_count": 5,
   "order": 107,
   "p1": 531,
   "pn": 535,
   "abstract": [
    "Although deep learning-based audio-visual speech recognition (AVSR) systems recognize base closed-set categories well, extending their discerning ability to additional novel categories with limited labeled training data is challenging since the model easily over-fits. In this paper, we propose Prototype-based Co-Adaptation with Transformer (Proto-CAT), a multi-modal generalized few-shot learning (GFSL) method for AVSR systems. In other words, Proto-CAT learns to recognize a novel class multi-modal object with few-shot training data, while maintaining its ability on those base closed-set categories. The main idea is to transform the prototypes (i.e., class centers) by incorporating cross-modality complementary information and calibrating cross-category semantic differences. In particular, Proto-CAT co-adapts the embeddings from audio-visual and category levels, so that it generalizes its predictions on all categories dynamically. Proto-CAT achieves state-of-the-art performance on various AVSR-GFSL benchmarks. The code is available at https://github.com/ZhangYikaii/Proto-CAT."
   ],
   "doi": "10.21437/Interspeech.2022-652"
  },
  "quan22_interspeech": {
   "authors": [
    [
     "Zongfeng",
     "Quan"
    ],
    [
     "Nick J.C.",
     "Wang"
    ],
    [
     "Wei",
     "Chu"
    ],
    [
     "Tao",
     "Wei"
    ],
    [
     "Shaojun",
     "Wang"
    ],
    [
     "Jing",
     "Xiao"
    ]
   ],
   "title": "FFM: A Frame Filtering Mechanism To Accelerate Inference Speed For Conformer In Speech Recognition",
   "original": "656",
   "page_count": 5,
   "order": 639,
   "p1": 3153,
   "pn": 3157,
   "abstract": [
    "This paper proposes a frame filtering mechanism (FFM) to accelerate inference speed for speech recognition. The FFM consists of three parts: one frame invalid indicator distinguishing whether the frame is invalid or not, one filtering strategy removing invalid frames, and one extractor attention block recalling useful information from filtered frames. The feature sequence will become shorter after FFM block. As a result, the inference is accelerated. Compared to other downsampling approaches on LibriSpeech, our method can achieve best WER with lowest RTF. Experiments on Aishell-1 show that our approach reduces the sequence length by up to 73% and achieves 21.1%--34.5% relative RTF reduction with relative WER increasing no more than 5.8\\%."
   ],
   "doi": "10.21437/Interspeech.2022-656"
  },
  "choi22c_interspeech": {
   "authors": [
    [
     "Sunmook",
     "Choi"
    ],
    [
     "Il-Youp",
     "Kwak"
    ],
    [
     "Seungsang",
     "Oh"
    ]
   ],
   "title": "Overlapped Frequency-Distributed Network: Frequency-Aware Voice Spoofing Countermeasure",
   "original": "657",
   "page_count": 5,
   "order": 720,
   "p1": 3558,
   "pn": 3562,
   "abstract": [
    "Numerous IT companies around the world are developing and deploying artificial voice assistants via their products, but they are still vulnerable to spoofing attacks. Since 2015, the competition \"Automatic Speaker Verification Spoofing and Countermeasures Challenge (ASVspoof)\" has been held every two years to encourage people to design systems that can detect spoofing attacks. In this paper, we focused on developing spoofing countermeasure systems mainly based on Convolutional Neural Networks (CNNs). However, CNNs have translation invariant property, which may cause loss of frequency information when a spectrogram is used as input. Hence, we propose models which split inputs along the frequency axis: 1) Overlapped Frequency-Distributed (OFD) model and 2) Non-overlapped Frequency-Distributed (Non-OFD) model. Using ASVspoof 2019 dataset, we measured their performances with two different activations; ReLU and Max feature map (MFM). The best performing model on LA dataset is the Non-OFD model with ReLU which achieved an equal error rate (EER) of 1.35%, and the best performing model on PA dataset is the OFD model with MFM which achieved an EER of 0.35%."
   ],
   "doi": "10.21437/Interspeech.2022-657"
  },
  "dong22_interspeech": {
   "authors": [
    [
     "Jingjing",
     "Dong"
    ],
    [
     "Jiayi",
     "Fu"
    ],
    [
     "Peng",
     "Zhou"
    ],
    [
     "Hao",
     "Li"
    ],
    [
     "Xiaorui",
     "Wang"
    ]
   ],
   "title": "Improving Spoken Language Understanding with Cross-Modal Contrastive Learning",
   "original": "658",
   "page_count": 5,
   "order": 547,
   "p1": 2693,
   "pn": 2697,
   "abstract": [
    "Spoken language understanding(SLU) is conventionally based on pipeline architecture with error propagation issues. To mitigate this problem, end-to-end(E2E) models are proposed to directly map speech input to desired semantic outputs. Meanwhile, others try to leverage linguistic information in addition to acoustic information by adopting a multi-modal architecture. In this work, we propose a novel multi-modal SLU method, named CMCL, which utilizes cross-modal contrastive learning to learn better multi-modal representation. In particular, a two-stream multi-modal framework is designed, and a contrastive learning task is performed across speech and text representations. Moreover, CMCL employs a multi-modal shared classification task combined with a contrastive learning task to guide the learned representation to improve the performance on the intent classification task. We also investigate the efficacy of employing crossmodal contrastive learning during pretraining. CMCL achieves 99.69% and 92.50% accuracy on FSC and Smartlights datasets, respectively, outperforming state-of-the-art comparative methods. Also, performances only decrease by 0.32% and 2.8%, respectively, when trained on 10% and 1% of the FSC dataset, indicating its advancement under few-shot seniors."
   ],
   "doi": "10.21437/Interspeech.2022-658"
  },
  "sun22c_interspeech": {
   "authors": [
    [
     "Yifan",
     "Sun"
    ],
    [
     "Qinlong",
     "Huang"
    ],
    [
     "Xihong",
     "Wu"
    ]
   ],
   "title": "Unsupervised Inference of Physiologically Meaningful Articulatory Trajectories with VocalTractLab",
   "original": "659",
   "page_count": 5,
   "order": 944,
   "p1": 4661,
   "pn": 4665,
   "abstract": [
    "Recently, the introduction of reinforcement learning methods and the Embodied Joint Embedding (EmJEm) approach has made it feasible to unsupervisedly infer articulatory movements from arbitrary utterances. However, the quality of re-synthesized utterances is still unsatisfactory and there is a lack of direct evaluation of the inferred articulatory movements to see if they are physiologically meaningful. In this work, we extend the EmJEm approach to tackle these problems of unsupervised acoustic-to-articulatory inversion (AAI). The VocalTractLab is adopted as the articulatory synthesizer and a novel architecture of the articulatory inference network is proposed. To obtain physiologically meaningful articulatory trajectories, a smoothness constraint is introduced as an articulatory prior during training. Experiments show that the proposed approach is able to re-synthesize utterances with state-of-the-art quality while effectively smooth the articulatory trajectories. We directly compare the unsupervisedly obtained articulatory trajectories with the recorded articulatory data from the HPRC database and it turns out that the inferred articulatory trajectories have a relatively high correlation with the recorded trajectories. This encouraging result shows the practical potential of unsupervised AAI methods."
   ],
   "doi": "10.21437/Interspeech.2022-659"
  },
  "yang22g_interspeech": {
   "authors": [
    [
     "Jingyuan",
     "Yang"
    ],
    [
     "Rongjun",
     "Li"
    ],
    [
     "Wei",
     "Peng"
    ]
   ],
   "title": "ASR Error Correction with Constrained Decoding on Operation Prediction",
   "original": "660",
   "page_count": 5,
   "order": 785,
   "p1": 3874,
   "pn": 3878,
   "abstract": [
    "Error correction techniques remain effective to refine outputs from automatic speech recognition (ASR) models. Existing end-to-end error correction methods based on an encoder-decoder architecture process all tokens in the decoding phase, creating undesirable latency. In this paper, we propose an ASR error correction method utilizing the predictions of correction operations. More specifically, we construct a predictor between the encoder and the decoder to learn if a token should be kept (\"K\"), deleted (\"D\"), or changed (\"C\") to restrict decoding to only part of the input sequence embeddings (the \"C\" tokens) for fast inference. Experiments on three public datasets demonstrate the effectiveness of the proposed approach in reducing the latency of the decoding process in ASR correction. It enhances the inference speed by at least three times (3.4 and 5.7 times) while maintaining the same level of accuracy (with WER reductions of 0.53% and 1.69% respectively) for our two proposed models compared to a solid encoder-decoder baseline. In the meantime, we produce and release a benchmark dataset contributing to the ASR error correction community to foster research along this line."
   ],
   "doi": "10.21437/Interspeech.2022-660"
  },
  "kirchhubel22_interspeech": {
   "authors": [
    [
     "Christin",
     "Kirchhübel"
    ],
    [
     "Georgina",
     "Brown"
    ]
   ],
   "title": "Spoofed speech from the perspective of a forensic phonetician",
   "original": "661",
   "page_count": 5,
   "order": 266,
   "p1": 1308,
   "pn": 1312,
   "abstract": [
    "While previous work has uncovered the performances of automatic systems when presented with spoofed speech samples, this work looks deeper into these samples from the perspective of an experienced forensic phonetician. From an evaluation of 300 samples, this paper reveals how similar some of the spoofed speech samples are to genuine human speech samples. One speech synthesis method ‘sticks out' in this respect by producing speech samples that bear a collection of natural speech characteristics. On the other hand, spoofing methods that have been shown to present problems to automatic systems in past work do not present problems to the forensic phonetician. The main contribution of this paper is a descriptive account of different spoofed speech samples, based on the auditory-acoustic evaluation of a forensic phonetician. This is to spark an awareness of the current possibilities of spoofing methods that perhaps have so far been ‘off the radar' for many forensic phoneticians. Such an account can bring great value to the forensic phonetics community as spoofed speech samples could plausibly find their way into forensic casework scenarios."
   ],
   "doi": "10.21437/Interspeech.2022-661"
  },
  "sriram22_interspeech": {
   "authors": [
    [
     "Anuroop",
     "Sriram"
    ],
    [
     "Michael",
     "Auli"
    ],
    [
     "Alexei",
     "Baevski"
    ]
   ],
   "title": "Wav2Vec-Aug: Improved self-supervised training with limited data",
   "original": "667",
   "page_count": 5,
   "order": 1002,
   "p1": 4950,
   "pn": 4954,
   "abstract": [
    "Self-supervised learning (SSL) of speech representations has received much attention over the last few years but most work has focused on languages and domains with an abundance of unlabeled data. However, for many languages there is a short- age even in the unlabeled data which limits the effectiveness of SSL. In this work, we focus on the problem of applying SSL to domains with limited available data by leveraging data augmentation for Wav2Vec 2.0 pretraining. Further, we propose improvements to each component of the model which result in a combined relative word error rate (WER) improvement of up to 13% compared to Wav2Vec 2.0 on Librispeech test-clean / other."
   ],
   "doi": "10.21437/Interspeech.2022-667"
  },
  "luo22c_interspeech": {
   "authors": [
    [
     "Xiaoxue",
     "Luo"
    ],
    [
     "Chengshi",
     "Zheng"
    ],
    [
     "Andong",
     "Li"
    ],
    [
     "Yuxuan",
     "Ke"
    ],
    [
     "Xiaodong",
     "Li"
    ]
   ],
   "title": "Bifurcation and Reunion: A Loss-Guided Two-Stage Approach for Monaural Speech Dereverberation",
   "original": "668",
   "page_count": 5,
   "order": 509,
   "p1": 2503,
   "pn": 2507,
   "abstract": [
    "Speech dereverberation is challenging for various speech processing systems. Recently, phase recovery is proved to be significant for improving speech quality and intelligibility, and numerous supervised speech dereverberation algorithms begin focusing on complex spectrum estimation. However, these methods recover clean speech phase at the expense of severe magnitude distortion due to the magnitude-phase compensation effect. To address this problem, we propose a novel loss-guided two-stage framework to progressively guide the process of complex spectrum recovery. In the first stage, a bifurcated network is proposed to separately optimize the magnitude and phase of the complex spectrum coarsely by two distinct loss functions. After that, a reunited network is devised to exploit the complementary characteristics of previous estimations and further refine the complex spectrum. A mathematical derivation is presented to reveal the magnitude-phase compromise phenomenon and validate the rationality of the proposed objective optimization strategy. Experimental results demonstrate that the proposed method improves both speech quality and intelligibility in the dereverberation task, and outperforms other baseline methods."
   ],
   "doi": "10.21437/Interspeech.2022-668"
  },
  "cheng22b_interspeech": {
   "authors": [
    [
     "Linjuan",
     "Cheng"
    ],
    [
     "Chengshi",
     "Zheng"
    ],
    [
     "Andong",
     "Li"
    ],
    [
     "Yuquan",
     "Wu"
    ],
    [
     "Renhua",
     "Peng"
    ],
    [
     "Xiaodong",
     "Li"
    ]
   ],
   "title": "A deep complex multi-frame filtering network for stereophonic acoustic echo cancellation",
   "original": "669",
   "page_count": 5,
   "order": 510,
   "p1": 2508,
   "pn": 2512,
   "abstract": [
    "In hands-free communication system, the coupling between loudspeaker and microphone generates echo signal, which can severely influence the quality of communication. Meanwhile, various types of noise in communication environments further reduce speech quality and intelligibility. It is difficult to extract the near-end signal from the microphone signal within one step, especially in low signal-to-noise ratio scenarios. In this paper, we propose a deep complex network approach to address this issue. Specially, we decompose the stereophonic acoustic echo cancellation into two stages, including linear stereophonic acoustic echo cancellation module and residual echo suppression module, where both modules are based on deep learning architectures. A multi-frame filtering strategy is introduced to benefit the estimation of linear echo by capturing more inter-frame information. Moreover, we decouple the complex spectral mapping into magnitude estimation and complex spectrum refinement. Experimental results demonstrate that our proposed approach achieves stage-of-the-art performance over previous advanced algorithms under various conditions."
   ],
   "doi": "10.21437/Interspeech.2022-669"
  },
  "ivry22_interspeech": {
   "authors": [
    [
     "Amir",
     "Ivry"
    ],
    [
     "Israel",
     "Cohen"
    ],
    [
     "Baruch",
     "Berdugo"
    ]
   ],
   "title": "Objective Metrics to Evaluate Residual-Echo Suppression During Double-Talk in the Stereophonic Case",
   "original": "673",
   "page_count": 5,
   "order": 1084,
   "p1": 5348,
   "pn": 5352,
   "abstract": [
    "Speech quality, as evaluated by humans, is most accurately assessed by subjective human ratings. The objective acoustic echo cancellation mean opinion score (AECMOS) metric was recently introduced and achieved high accuracy in predicting human perception during double-talk. Residual-echo suppression (RES) systems, however, employ the signal-to-distortion ratio (SDR) metric to quantify speech-quality in double-talk. In this study, we focus on stereophonic acoustic echo cancellation, and show that the stereo SDR (SSDR) poorly correlates with subjective human ratings according to the AECMOS, since the SSDR is influenced by both distortion of desired speech and presence of residual-echo. We introduce a pair of objective metrics that distinctly assess the stereo desired-speech maintained level (SDSML) and stereo residual-echo suppression level (SRESL) during double-talk. By employing a tunable RES system based on deep learning and using 100 hours of real and simulated recordings, the SDSML and SRESL metrics show high correlation with the AECMOS across various setups. We also investigate into how the design parameter governs the SDSML-SRESL tradeoff, and harness this relation to allow optimal performance for frequently-changing user demands in practical cases."
   ],
   "doi": "10.21437/Interspeech.2022-673"
  },
  "wang22i_interspeech": {
   "authors": [
    [
     "Helin",
     "Wang"
    ],
    [
     "Dongchao",
     "Yang"
    ],
    [
     "Chao",
     "Weng"
    ],
    [
     "Jianwei",
     "Yu"
    ],
    [
     "Yuexian",
     "Zou"
    ]
   ],
   "title": "Improving Target Sound Extraction with Timestamp Information",
   "original": "676",
   "page_count": 5,
   "order": 310,
   "p1": 1526,
   "pn": 1530,
   "abstract": [
    "Target sound extraction (TSE) aims to extract the sound part of a target sound event class from a mixture audio with multiple sound events. The previous works mainly focus on the problems of weakly-labelled data, jointly learning and new classes, however, no one cares about the onset and offset times of the target sound event, which has been emphasized in the auditory scene analysis. In this paper, we study to utilize such timestamp information to help extract the target sound via a target sound detection network and a target-weighted time-frequency loss function. More specifically, we use the detection result of a target sound detection (TSD) network as the additional information to guide the learning of target sound extraction network. We also find that the result of TSE can further improve the performance of the TSD network, so that a mutual learning framework of the target sound detection and extraction is proposed. In addition, a target-weighted time-frequency loss function is designed to pay more attention to the temporal regions of the target sound during training. Experimental results on the synthesized data generated from the Freesound Datasets show that our proposed method can significantly improve the performance of TSE."
   ],
   "doi": "10.21437/Interspeech.2022-676"
  },
  "wang22j_interspeech": {
   "authors": [
    [
     "Weiqing",
     "Wang"
    ],
    [
     "Ming",
     "Li"
    ],
    [
     "Qingjian",
     "Lin"
    ]
   ],
   "title": "Online Target Speaker Voice Activity Detection for Speaker Diarization",
   "original": "677",
   "page_count": 5,
   "order": 293,
   "p1": 1441,
   "pn": 1445,
   "abstract": [
    "This paper proposes an online target speaker voice activity detection system for speaker diarization tasks, which does not require a priori knowledge from the clustering-based diarization system to obtain the target speaker embeddings. First, we employ a ResNet-based front-end model to extract the frame-level speaker embeddings for each coming block of a signal. Next, we predict the detection state of each speaker based on these frame-level speaker embeddings and the previously estimated target speaker embedding. Then, the target speaker embeddings are updated by aggregating these frame-level speaker embeddings according to the predictions in the current block. We iteratively extract the results for each block and update the target speaker embedding until reaching the end of the signal. Experimental results show that the proposed method is better than the offline clustering-based diarization system on the Alimeeting dataset."
   ],
   "doi": "10.21437/Interspeech.2022-677"
  },
  "xu22e_interspeech": {
   "authors": [
    [
     "Junhao",
     "Xu"
    ],
    [
     "Shoukang",
     "Hu"
    ],
    [
     "Xunying",
     "Liu"
    ],
    [
     "Helen",
     "Meng"
    ]
   ],
   "title": "Towards Green ASR: Lossless 4-bit Quantization of a Hybrid TDNN System on the 300-hr Swithboard Corpus",
   "original": "678",
   "page_count": 5,
   "order": 434,
   "p1": 2128,
   "pn": 2132,
   "abstract": [
    "State-of-the-art time automatic speech recognition (ASR) systems are becoming increasingly complex and expensive for practical applications. This paper presents the development of a high performance and low-footprint 4-bit quantized LF-MMI trained factored time delay neural networks (TDNNs) based ASR system on the 300-hr Switchboard corpus. A key feature of the overall system design is to account for the fine-grained, varying performance sensitivity at different model components to quantization errors. To this end, a set of neural architectural compression and mixed precision quantization approaches were used to facilitate hidden layer level auto-configuration of optimal factored TDNN weight matrix subspace dimensionality and quantization bit-widths. The proposed techniques were also used to produce 2-bit mixed precision quantized Transformer language models. Experiments conducted on the Switchboard data suggest that the proposed neural architectural compression and mixed precision quantization techniques consistently outperform the uniform precision quantised baseline systems of comparable bit-widths in terms of word error rate (WER). An overall \"lossless” compression ratio of 13.6 was obtained over the baseline full precision system including both the TDNN and Transformer components while incurring no statistically significant WER increase."
   ],
   "doi": "10.21437/Interspeech.2022-678"
  },
  "zhao22h_interspeech": {
   "authors": [
    [
     "Yan",
     "Zhao"
    ],
    [
     "Jincen",
     "Wang"
    ],
    [
     "Ru",
     "Ye"
    ],
    [
     "Yuan",
     "Zong"
    ],
    [
     "Wenming",
     "Zheng"
    ],
    [
     "Li",
     "Zhao"
    ]
   ],
   "title": "Deep Transductive Transfer Regression Network for Cross-Corpus Speech Emotion Recognition",
   "original": "679",
   "page_count": 5,
   "order": 75,
   "p1": 371,
   "pn": 375,
   "abstract": [
    "In this paper, we focus on the research of cross-corpus speech emotion recognition (SER), in which the training (source) and testing (target) speech samples come from different corpora leading to a feature distribution gap between them. To solve this problem, we propose a simple yet effective method called deep transductive transfer regression network (DTTRN). The basic idea of DTTRN is to learn a corpus invariant deep neural network to bridge the source and target speech samples and their label information. Following this idea, we make use of a transductive learning way to enforce a deep regressor to build the relationship between the features and emotional labels jointly in both speech corpora. Meanwhile, we also design an emotion guided regularization term for learning DTTRN by aligning source and target speech samples feature distributions from three different scales. Thus, the DTTRN only absorbing the label information provided by source speech samples is able to correctly predict the emotions of the target ones. To evaluate DTTRN, we conduct extensive cross-corpus SER experiments on EmoDB, CASIA, and eNTERFACE corpora. Experimental results show the superior performance of our DTTRN over recent state-of-the-art deep transfer learning methods in dealing with the cross-corpus SER tasks."
   ],
   "doi": "10.21437/Interspeech.2022-679"
  },
  "deng22_interspeech": {
   "authors": [
    [
     "Jiajun",
     "DENG"
    ],
    [
     "Xurong",
     "Xie"
    ],
    [
     "Tianzi",
     "Wang"
    ],
    [
     "Mingyu",
     "Cui"
    ],
    [
     "Boyang",
     "Xue"
    ],
    [
     "Zengrui",
     "Jin"
    ],
    [
     "Mengzhe",
     "Geng"
    ],
    [
     "Guinan",
     "Li"
    ],
    [
     "Xunying",
     "Liu"
    ],
    [
     "Helen",
     "Meng"
    ]
   ],
   "title": "Confidence Score Based Conformer Speaker Adaptation for Speech Recognition",
   "original": "680",
   "page_count": 5,
   "order": 533,
   "p1": 2623,
   "pn": 2627,
   "abstract": [
    "A key challenge for automatic speech recognition (ASR) systems is to model the speaker level variability. In this paper, compact speaker dependent learning hidden unit contributions (LHUC) are used to facilitate both speaker adaptive training (SAT) and test time unsupervised speaker adaptation for state-of-the-art Conformer based end-to-end ASR systems. The sensitivity during adaptation to supervision error rate is reduced using confidence score based selection of the more \"trustworthy” subset of speaker specific data. A confidence estimation module is used to smooth the over-confident Conformer decoder output probabilities before serving as confidence scores. The increased data sparsity due to speaker level data selection is addressed using Bayesian estimation of LHUC parameters. Experiments on the 300-hour Switchboard corpus suggest that the proposed LHUC-SAT Conformer with confidence score based test time unsupervised adaptation outperformed the baseline speaker independent and i-vector adapted Conformer systems by up to 1.0%, 1.0%, and 1.2% absolute (9.0%, 7.9%, and 8.9% relative) word error rate (WER) reductions on the NIST Hub5'00, RT02, and RT03 evaluation sets respectively. Consistent performance improvements were retained after external Transformer and LSTM language models were used for rescoring."
   ],
   "doi": "10.21437/Interspeech.2022-680"
  },
  "zheng22b_interspeech": {
   "authors": [
    [
     "Weiqiao",
     "Zheng"
    ],
    [
     "Ping",
     "Yang"
    ],
    [
     "Rongfeng",
     "Lai"
    ],
    [
     "Kongyang",
     "Zhu"
    ],
    [
     "Tao",
     "Zhang"
    ],
    [
     "Junpeng",
     "Zhang"
    ],
    [
     "Hongcheng",
     "Fu"
    ]
   ],
   "title": "Exploring Multi-task Learning Based Gender Recognition and Age Estimation for Class-imbalanced Data",
   "original": "682",
   "page_count": 5,
   "order": 405,
   "p1": 1983,
   "pn": 1987,
   "abstract": [
    "Automatic gender recognition and age estimation from speaker's audio is desired by applications in music recommendation, speaker profiling etc. However, its performance degrades greatly with the class-imbalanced data distribution. This paper explores a novel multi-task learning based gender recognition and age estimation system using speaker embedding. We apply the label distribution smoothing referred as LDS and investigate a weight mean squared error focal loss named as w-MSE-FL to reshape the weight assigned to the centralized-distribution samples during training. For a limited dataset, we pretrain a deep convolution neural network stacked with an attentive statistic pooling layer for speaker recognition task on a speaker speech dataset to extract robust speaker embedding feature. Then, we further fine-tune the multi-task learning network for gender recognition and age estimation simultaneously using classifier and regressor on a specific gender and age dataset, respectively. Experimental results verify our proposed system achieves better results on the TIMIT dataset with RMSE of 7.17 and 7.25 years on age estimation for male and female speakers, respectively, while performs an overall gender recognition accuracy of 99.30%."
   ],
   "doi": "10.21437/Interspeech.2022-682"
  },
  "lei22_interspeech": {
   "authors": [
    [
     "Yi",
     "Lei"
    ],
    [
     "Shan",
     "Yang"
    ],
    [
     "Jian",
     "Cong"
    ],
    [
     "Lei",
     "Xie"
    ],
    [
     "Dan",
     "Su"
    ]
   ],
   "title": "Glow-WaveGAN 2: High-quality Zero-shot Text-to-speech Synthesis and Any-to-any Voice Conversion",
   "original": "684",
   "page_count": 5,
   "order": 521,
   "p1": 2563,
   "pn": 2567,
   "abstract": [
    "The zero-shot scenario for speech generation aims at synthesizing a novel unseen voice with only one utterance of the target speaker. Although the challenges of adapting new voices in zero-shot scenario exist in both stages -- acoustic modeling and vocoder, previous works usually consider the problem from only one stage. In this paper, we extend our previous Glow-WaveGAN to Glow-WaveGAN 2, aiming to solve the problem from both stages for high-quality zero-shot text-to-speech and any-to-any voice conversion. We first build a universal WaveGAN model for extracting latent distribution $p(z)$ of speech and reconstructing waveform from it. Then a flow-based acoustic model only needs to learn the same $p(z)$ from texts, which naturally avoids the mismatch between the acoustic model and the vocoder, resulting in high-quality generated speech without model fine-tuning. Based on a continuous speaker space and the reversible property of flows, the conditional distribution can be obtained for any speaker, and thus we can further conduct high-quality zero-shot speech generation for new speakers. We particularly investigate two methods to construct the speaker space, namely pre-trained and jointly-trained speaker encoder. The superiority of Glow-WaveGAN 2 has been proved through TTS and VC experiments conducted on LibriTTS corpus and VTCK corpus."
   ],
   "doi": "10.21437/Interspeech.2022-684"
  },
  "wu22e_interspeech": {
   "authors": [
    [
     "Yihan",
     "Wu"
    ],
    [
     "Xi",
     "Wang"
    ],
    [
     "Shaofei",
     "Zhang"
    ],
    [
     "Lei",
     "He"
    ],
    [
     "Ruihua",
     "Song"
    ],
    [
     "Jian-Yun",
     "Nie"
    ]
   ],
   "title": "Self-supervised Context-aware Style Representation for Expressive Speech Synthesis",
   "original": "686",
   "page_count": 5,
   "order": 1115,
   "p1": 5503,
   "pn": 5507,
   "abstract": [
    "Expressive speech synthesis, like audiobook synthesis, is still challenging for style representation learning and prediction. Deriving from reference audio or predicting style tags from text requires a huge amount of labeled data, which is costly to acquire and difficult to define and annotate accurately. In this paper, we propose a novel framework for learning style representation from abundant plain text in a self-supervised manner. It leverages an emotion lexicon and uses contrastive learning and deep clustering. We further integrate the style representation as a conditioned embedding in a multi-style Transformer TTS. Comparing with multi-style TTS by predicting style tags trained on the same dataset but with human annotations, our method achieves improved results according to subjective evaluations on both in-domain and out-domain test sets in audiobook speech. Moreover, with implicit context-aware style representation, the emotion transition of synthesized audio in a long paragraph appears more natural. The audio samples are available on the demo website."
   ],
   "doi": "10.21437/Interspeech.2022-686"
  },
  "berisha22_interspeech": {
   "authors": [
    [
     "Visar",
     "Berisha"
    ],
    [
     "Chelsea",
     "Krantsevich"
    ],
    [
     "Gabriela",
     "Stegmann"
    ],
    [
     "Shira",
     "Hahn"
    ],
    [
     "Julie",
     "Liss"
    ]
   ],
   "title": "Are reported accuracies in the clinical speech machine learning literature overoptimistic?",
   "original": "691",
   "page_count": 5,
   "order": 499,
   "p1": 2453,
   "pn": 2457,
   "abstract": [
    "Building clinical speech analytics models that will reliably translate in-clinic requires a realistic characterization of their performance. So, how well do we estimate the accuracy of published models in the literature? We evaluate the relationship between sample size and reported accuracy across 77 journal publications that use speech to classify between healthy controls and patients with dementia. The studies are combined across three meta-analyses that use the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) protocol. The results show that reported accuracy declines as a function of increasing sample size, with small sample size studies yielding an overoptimistic estimate of the accuracy. For correctly trained models, this is unexpected as the ability of a machine learning model to predict group membership ought to remain the same or improve with additional training data. We posit that the overoptimism is the result of a combination of publication bias and overfitting and suggest mitigation strategies."
   ],
   "doi": "10.21437/Interspeech.2022-691"
  },
  "tal22_interspeech": {
   "authors": [
    [
     "Or",
     "Tal"
    ],
    [
     "Moshe",
     "Mandel"
    ],
    [
     "Felix",
     "Kreuk"
    ],
    [
     "Yossi",
     "Adi"
    ]
   ],
   "title": "A Systematic Comparison of Phonetic Aware Techniques for Speech Enhancement",
   "original": "695",
   "page_count": 5,
   "order": 243,
   "p1": 1193,
   "pn": 1197,
   "abstract": [
    "Speech enhancement has seen great improvement in recent years using end-to-end neural networks. However, most models are agnostic to the spoken phonetic content. Recently, several studies suggested phonetic-aware speech enhancement, mostly using perceptual supervision. Yet, injecting phonetic features during model optimization can take additional forms (e.g., model conditioning). In this paper, we conduct a systematic comparison between different methods of incorporating phonetic information in a speech enhancement model. By conducting a series of controlled experiments, we observe the influence of different phonetic content models as well as various feature-injection techniques on enhancement performance, considering both causal and non-causal models. Specifically, we evaluate three settings for injecting phonetic information, namely: i) feature conditioning; ii) perceptual supervision; and iii) regularization. Phonetic features are obtained using an intermediate layer of either a supervised pre-trained Automatic Speech Recognition (ASR) model or by using a pre-trained Self-Supervised Learning (SSL) model. We further observe the effect of choosing different embedding layers on performance, considering both manual and learned configurations. Results suggest that using a SSL model as phonetic features outperforms the ASR one in most cases. Interestingly, the conditioning setting performs best among the evaluated configurations."
   ],
   "doi": "10.21437/Interspeech.2022-695"
  },
  "cui22_interspeech": {
   "authors": [
    [
     "Mingyu",
     "Cui"
    ],
    [
     "Jiajun",
     "Deng"
    ],
    [
     "Shoukang",
     "Hu"
    ],
    [
     "Xurong",
     "Xie"
    ],
    [
     "Tianzi",
     "Wang"
    ],
    [
     "Shujie",
     "Hu"
    ],
    [
     "Mengzhe",
     "Geng"
    ],
    [
     "Boyang",
     "Xue"
    ],
    [
     "Xunying",
     "Liu"
    ],
    [
     "Helen",
     "Meng"
    ]
   ],
   "title": "Two-pass Decoding and Cross-adaptation Based System Combination of End-to-end Conformer and Hybrid TDNN ASR Systems",
   "original": "696",
   "page_count": 5,
   "order": 640,
   "p1": 3158,
   "pn": 3162,
   "abstract": [
    "Fundamental modelling differences between hybrid and end-to-end (E2E) automatic speech recognition (ASR) systems create large diversity and complementarity among them. This paper investigates multi-pass rescoring and cross adaptation based system combination approaches for hybrid TDNN and Conformer E2E ASR systems. In multi-pass rescoring, state-of-the-art hybrid LF-MMI trained CNN-TDNN system featuring speed perturbation, SpecAugment and Bayesian learning hidden unit contributions (LHUC) speaker adaptation was used to produce initial N-best outputs before being rescored by the speaker adapted Conformer system using a 2-way cross system score interpolation. In cross adaptation, the hybrid CNN-TDNN system was adapted to the 1-best output of the Conformer system or vice versa. Experiments on the 300-hour Switchboard corpus suggest that the combined systems derived using either of the two system combination approaches outperformed the individual systems. The best combined system obtained using multi-pass rescoring produced statistically significant word error rate (WER) reductions of 2.5% to 3.9% absolute (22.5% to 28.9% relative) over the stand alone Conformer system on the NIST Hub5'00, Rt03 and Rt02 evaluation data."
   ],
   "doi": "10.21437/Interspeech.2022-696"
  },
  "hou22b_interspeech": {
   "authors": [
    [
     "junfeng",
     "Hou"
    ],
    [
     "Jinkun",
     "Chen"
    ],
    [
     "Wanyu",
     "Li"
    ],
    [
     "Yufeng",
     "Tang"
    ],
    [
     "Jun",
     "Zhang"
    ],
    [
     "Zejun",
     "Ma"
    ]
   ],
   "title": "Bring dialogue-context into RNN-T for streaming ASR",
   "original": "697",
   "page_count": 5,
   "order": 418,
   "p1": 2048,
   "pn": 2052,
   "abstract": [
    "Recently the conversational end-to-end (E2E) automatic speech recognition (ASR) models, which directly integrate dialogue-context such as historical utterances into E2E models, have shown superior performance than single-utterance E2E models. However, few works investigate how to inject the dialogue-context into the recurrent neural network transducer (RNN-T) model. In this work, we bring dialogue-context into a streaming RNN-T model and explore various structures of contextual RNN-T model as well as training strategies to better utilize the dialogue-context. Firstly, we propose a deep fusion architecture which efficiently integrates the dialogue-context within the encoder and predictor of RNN-T. Secondly, we propose contextual & non-contextual model joint training as regularization, and propose context perturbation to relieve the context mismatch between training and inference. Moreover, we adopt a context-aware language model (CLM) for contextual RNN-T decoding to take full advantage of the dialogue-context for conversational ASR. We conduct experiments on the Switchboard-2000h task and observe performance gains from the proposed techniques. Compared with non-contextual RNN-T, our contextual RNN-T model yields 4.8% / 6.0% relative improvement on Switchboard and Callhome Hub5'00 testsets. By additionally integrating a CLM, the gain is further increased to 10.6% / 7.8%."
   ],
   "doi": "10.21437/Interspeech.2022-697"
  },
  "zhuang22_interspeech": {
   "authors": [
    [
     "Xuyi",
     "Zhuang"
    ],
    [
     "Lu",
     "Zhang"
    ],
    [
     "Zehua",
     "Zhang"
    ],
    [
     "Yukun",
     "Qian"
    ],
    [
     "Mingjiang",
     "Wang"
    ]
   ],
   "title": "Coarse-Grained Attention Fusion With Joint Training Framework for Complex Speech Enhancement and End-to-End Speech Recognition",
   "original": "698",
   "page_count": 5,
   "order": 769,
   "p1": 3794,
   "pn": 3798,
   "abstract": [
    "Joint training of speech enhancement and ASR can make the model work robustly in noisy environments. However, most of these models work directly in series, and the information of the noisy is not reused before the ASR input, leading to a large amount of distortion in the features of the input ASR. In order to solve the problem of distortion from the root, we propose a CSE network which is used to denoise the noisy by combining mask and mapping in the complex domain. Secondly, we also propose CAF, which re-extracts the original speech features of from the noisy by the coarse-grained attention mechanism and deeply fuses them with the enhanced speech features. In addition, to make the output space of CAF closer to the input space expected by ASR, we also propose to compute loss for CAF with multi-layer output of pretrained model. Our experiments are trained and tested on the dataset generated by AISHELL-1 and DNS3. Experimental results show that the CER of our model is 13.425 under the condition of SNR of 0dB and the CER of 20.671 under the condition of SNR of -5dB. And the robustness is 93.869% on dataset generated by AISHELL-2 and MUSAN."
   ],
   "doi": "10.21437/Interspeech.2022-698"
  },
  "rixen22_interspeech": {
   "authors": [
    [
     "Joel",
     "Rixen"
    ],
    [
     "Matthias",
     "Renz"
    ]
   ],
   "title": "QDPN - Quasi-dual-path Network for single-channel Speech Separation",
   "original": "700",
   "page_count": 5,
   "order": 1085,
   "p1": 5353,
   "pn": 5357,
   "abstract": [
    "The goal of single-channel source separation is to produce an accurate estimation of multiple sources given a mixture where these sources overlap. In this paper, we propose a new architecture to do this called the quasi-dual-path network (QDPN). The main difference to previous methods which mostly use the dual-path approach is to have the advantages of the dual-path approach like the short-term and long-term pattern recognition without the disadvantages of the dual-path approach like the doubling of the input size. Since the input size of the QDPN is considerably smaller than the input size of the dual-path approach, it enables us to turn up the hyperparameters for more accuracy. On the WSJ02-Mix benchmark, the QDPN achieves a scale-invariant signal-to-noise ratio of 23.6 dB and on the WHAMR! benchmark, the QDPN reaches a scale-invariant signal-to-noise ratio of 14.4 dB."
   ],
   "doi": "10.21437/Interspeech.2022-700"
  },
  "wei22b_interspeech": {
   "authors": [
    [
     "Jie",
     "Wei"
    ],
    [
     "Guanyu",
     "Hu"
    ],
    [
     "Xinyu",
     "Yang"
    ],
    [
     "Anh Tuan",
     "Luu"
    ],
    [
     "Yizhuo",
     "Dong"
    ]
   ],
   "title": "Audio-Visual Domain Adaptation Feature Fusion for Speech Emotion Recognition",
   "original": "703",
   "page_count": 5,
   "order": 406,
   "p1": 1988,
   "pn": 1992,
   "abstract": [
    "Speech emotion recognition has made significant progress in recent years, in which feature representation learning has been paid more attention, but discriminative emotional features extraction has remained unresolved. In this paper, we propose MDSCM - a Multi-attention based Depthwise Separable Convolutional Model for speech emotional feature extraction that can reduce the feature redundancy through separating spatial-wise convolution and channel-wise convolution. MDSCM also enhances the feature discriminability by the multi-attention module that focuses on learning features with more emotional information. In addition, we propose an Audio-Visual Domain Adaptation Learning paradigm (AVDAL) to learn an audio-visual emotion-identity space. A shared audio-visual representation encoder is built to transfer the emotional knowledge learned from the visual domain to complement and enhance the emotional features that only extracted from speech. Domain classifier and emotion classifier are used for encoder training to reduce the mismatching of domain features, and enhance the discriminability of features for emotion recognition. The experimental results on the IEMOCAP dataset demonstrate that our proposed method outperforms other state-of-the-art speech emotion recognition systems, achieving 72.43% on weighted accuracy and 73.22% on unweighted accuracy. The code is available at https://github.com/Janie1996/AV4SER."
   ],
   "doi": "10.21437/Interspeech.2022-703"
  },
  "li22i_interspeech": {
   "authors": [
    [
     "Zehan",
     "Li"
    ],
    [
     "Haoran",
     "Miao"
    ],
    [
     "Keqi",
     "Deng"
    ],
    [
     "Gaofeng",
     "Cheng"
    ],
    [
     "Sanli",
     "Tian"
    ],
    [
     "Ta",
     "Li"
    ],
    [
     "Yonghong",
     "Yan"
    ]
   ],
   "title": "Improving Streaming End-to-End ASR on Transformer-based Causal Models with Encoder States Revision Strategies",
   "original": "707",
   "page_count": 5,
   "order": 339,
   "p1": 1671,
   "pn": 1675,
   "abstract": [
    "There is often a trade-off between performance and latency in streaming automatic speech recognition (ASR). Traditional methods such as look-ahead and chunk-based methods, usually require information from future frames to advance recognition accuracy, which incurs inevitable latency even if the computation is fast enough. A causal model that computes without any future frames can avoid this latency, but its performance is significantly worse than traditional methods. In this paper, we propose corresponding revision strategies to improve the causal model. Firstly, we introduce a real-time encoder states revision strategy to modify previous states. Encoder forward computation starts once the data is received and revises the previous encoder states after several frames, which is no need to wait for any right context. Furthermore, a CTC spike position alignment decoding algorithm is designed to reduce time costs brought by the proposed revision strategy. Experiments are all conducted on Librispeech datasets. Fine-tuning on the CTC-based wav2vec2.0 model, our best method can achieve 3.7/9.2 WERs on test-clean/other sets and brings 45% relative improvement for causal models, which is also competitive with the chunk-based methods and the knowledge distillation methods."
   ],
   "doi": "10.21437/Interspeech.2022-707"
  },
  "bai22_interspeech": {
   "authors": [
    [
     "Ye",
     "Bai"
    ],
    [
     "Jie",
     "Li"
    ],
    [
     "Wenjing",
     "Han"
    ],
    [
     "Hao",
     "Ni"
    ],
    [
     "Kaituo",
     "Xu"
    ],
    [
     "Zhuo",
     "Zhang"
    ],
    [
     "Cheng",
     "Yi"
    ],
    [
     "Xiaorui",
     "Wang"
    ]
   ],
   "title": "Parameter-Efficient Conformers via Sharing Sparsely-Gated Experts for End-to-End Speech Recognition",
   "original": "709",
   "page_count": 5,
   "order": 340,
   "p1": 1676,
   "pn": 1680,
   "abstract": [
    "While transformers and their variant conformers show promising performance in speech recognition, the over-parameterized property leads to the much memory cost during training and inference. Some works use cross-layer weight-sharing to reduce the parameters of the model. However, the inevitable loss of capacity affects the model performance. To address this issue, this paper proposes a parameter-efficient conformer via sharing sparsely-gated experts. Specifically, we use sparsely-gated mixture-of-experts (MoE) to extend the capacity of a conformer block without increasing computation. Then, the parameters of the grouped conformer blocks are shared so that the number of parameters is reduced. Next, to ensure the shared blocks with the flexibility of adapting representations at different levels, we design the MoE routers and normalization individually. Moreover, we use knowledge distillation to further improve the performance. Experimental results show that the proposed model achieves competitive performance with 1/3 of the parameters of the encoder, compared with the full-parameter model."
   ],
   "doi": "10.21437/Interspeech.2022-709"
  },
  "wang22k_interspeech": {
   "authors": [
    [
     "Tianzi",
     "Wang"
    ],
    [
     "Jiajun",
     "Deng"
    ],
    [
     "Mengzhe",
     "Geng"
    ],
    [
     "Zi",
     "Ye"
    ],
    [
     "Shoukang",
     "Hu"
    ],
    [
     "Yi",
     "Wang"
    ],
    [
     "Mingyu",
     "Cui"
    ],
    [
     "Zengrui",
     "Jin"
    ],
    [
     "Xunying",
     "Liu"
    ],
    [
     "Helen",
     "Meng"
    ]
   ],
   "title": "Conformer Based Elderly Speech Recognition System for Alzheimer’s Disease Detection",
   "original": "712",
   "page_count": 5,
   "order": 977,
   "p1": 4825,
   "pn": 4829,
   "abstract": [
    "Early diagnosis of Alzheimer's disease (AD) is crucial in facilitating preventive care to delay further progression. This paper presents the development of a state-of-the-art Conformer-based speech recognition system built on the DementiaBank Pitt corpus for automatic AD detection. The baseline Conformer system trained with speed perturbation and SpecAugment based data augmentation is significantly improved by incorporating a set of purposefully designed modeling features including neural architecture search based auto-configuration of domain-specific Conformer hyper-parameters in addition to parameter fine-tuning; fine-grained elderly speaker adaptation using learning hidden unit contributions (LHUC); and two-pass cross-system rescoring based combination with hybrid TDNN systems. An overall word error rate (WER) reduction of 13.6% absolute (34.8% relative) was obtained on the evaluation data of 48 elderly speakers. Using the final systems' recognition outputs to extract textual features, the best-published speech recognition based AD detection accuracy of 91.7% was obtained."
   ],
   "doi": "10.21437/Interspeech.2022-712"
  },
  "choi22d_interspeech": {
   "authors": [
    [
     "Kwanghee",
     "Choi"
    ],
    [
     "Hyung-Min",
     "Park"
    ]
   ],
   "title": "Distilling a Pretrained Language Model to a Multilingual ASR Model",
   "original": "716",
   "page_count": 5,
   "order": 449,
   "p1": 2203,
   "pn": 2207,
   "abstract": [
    "Multilingual speech data often suffer from long-tailed language distribution, resulting in performance degradation. However, multilingual text data is much easier to obtain, yielding a more useful general language model. Hence, we are motivated to distill the rich knowledge embedded inside a well-trained teacher text model to the student speech model. We propose a novel method called the Distilling a Language model to a Speech model (Distill-L2S), which aligns the latent representations of two different modalities. The subtle differences are handled by the shrinking mechanism, nearest-neighbor interpolation, and a learnable linear projection layer. We demonstrate the effectiveness of our distillation method by applying it to the multilingual automatic speech recognition (ASR) task. We distill the transformer-based cross-lingual language model (InfoXLM) while fine-tuning the large-scale multilingual ASR model (XLSR-wav2vec 2.0) for each language. We show the superiority of our method on 20 low-resource languages of the CommonVoice dataset with less than 100 hours of speech data."
   ],
   "doi": "10.21437/Interspeech.2022-716"
  },
  "ye22_interspeech": {
   "authors": [
    [
     "Lingxuan",
     "Ye"
    ],
    [
     "Gaofeng",
     "Cheng"
    ],
    [
     "Runyan",
     "Yang"
    ],
    [
     "Zehui",
     "Yang"
    ],
    [
     "Sanli",
     "Tian"
    ],
    [
     "Pengyuan",
     "Zhang"
    ],
    [
     "Yonghong",
     "Yan"
    ]
   ],
   "title": "Improving Recognition of Out-of-vocabulary Words in E2E Code-switching ASR by Fusing Speech Generation Methods",
   "original": "719",
   "page_count": 5,
   "order": 641,
   "p1": 3163,
   "pn": 3167,
   "abstract": [
    "Out-of-vocabulary (OOV) is a common problem for end-to-end (E2E) ASR. For code-switching (CS), the OOV problem on the embedded language is further aggravated and becomes a primary obstacle in deploying E2E code-switching speech recognition (CSSR) systems. Existing recipes for monolingual scenarios typically take advantage of text-to-speech (TTS) synthesis or utilize fine-grained modeling units. However, the sparsity of CS greatly decreases the probability of words to be covered (mainly the embedded language), which hinders the collecting of corresponding CS text for TTS. Using fine-grained units brings limited improvement to the OOV words while increasing the risk of misspelling. In this paper, we propose two distinct CS speech generation methods to improve the recognition of CSSR systems on OOV words. First, we utilize monolingual corpora to generate spliced CS speech containing OOV words. Second, we propose an algorithm to generate CS text containing OOV words, thus enabling using TTS to synthesize CS speech. Both methods are carefully designed to ensure acoustic and semantic smoothness of generated speech. In addition, we provide restrictive methods to suppress the side-effects of using artificially generated data and help avoid misspelling. Finally, we reduced WER on OOV words by 56.3% absolutely on the test set."
   ],
   "doi": "10.21437/Interspeech.2022-719"
  },
  "zhu22b_interspeech": {
   "authors": [
    [
     "Han",
     "Zhu"
    ],
    [
     "Jindong",
     "Wang"
    ],
    [
     "Gaofeng",
     "Cheng"
    ],
    [
     "Pengyuan",
     "Zhang"
    ],
    [
     "Yonghong",
     "Yan"
    ]
   ],
   "title": "Decoupled Federated Learning for ASR with Non-IID Data",
   "original": "720",
   "page_count": 5,
   "order": 534,
   "p1": 2628,
   "pn": 2632,
   "abstract": [
    "Automatic speech recognition (ASR) with federated learning (FL) makes it possible to leverage data from multiple clients without compromising privacy. The quality of FL-based ASR could be measured by recognition performance, communication and computation costs. When data among different clients are not independently and identically distributed (non-IID), the performance could degrade significantly. In this work, we tackle the non-IID issue in FL-based ASR with personalized FL, which learns personalized models for each client. Concretely, we propose two types of personalized FL approaches for ASR. Firstly, we adapt the personalization layer based FL for ASR, which keeps some layers locally to learn personalization models. Secondly, to reduce the communication and computation costs, we propose decoupled federated learning (DecoupleFL). On one hand, DecoupleFL moves the computation burden to the server, thus decreasing the computation on clients. On the other hand, DecoupleFL communicates secure high-level features instead of model parameters, thus reducing communication cost when models are large. Experiments demonstrate two proposed personalized FL-based ASR approaches could reduce WER by 2.3% - 3.4% compared with FedAvg. Among them, DecoupleFL has only 11.4% communication and 75% computation cost compared with FedAvg, which is also significantly less than the personalization layer based FL."
   ],
   "doi": "10.21437/Interspeech.2022-720"
  },
  "lee22f_interspeech": {
   "authors": [
    [
     "Jonathan Him Nok",
     "Lee"
    ],
    [
     "Dehua",
     "Tao"
    ],
    [
     "Harold",
     "Chui"
    ],
    [
     "Tan",
     "Lee"
    ],
    [
     "Sarah",
     "Luk"
    ],
    [
     "Nicolette Wing Tung",
     "Lee"
    ],
    [
     "Koonkan",
     "Fung"
    ]
   ],
   "title": "Durational Patterning at Discourse Boundaries in Relation to Therapist Empathy in Psychotherapy",
   "original": "722",
   "page_count": 5,
   "order": 1064,
   "p1": 5248,
   "pn": 5252,
   "abstract": [
    "This study investigates how durational patterning at discourse boundaries (duration of utterance-final syllables, silent pause, and speech rate) is related to therapist empathy in psychotherapy. Four psychotherapy sessions each from 39 therapist-client dyads conducted in Cantonese were videotaped, transcribed, and analyzed. Clients rated therapist empathy using the Barrett-Lennard Relationship Inventory (BLRI). Mixed-effects regression showed significant effects of silent pause and speech rate on BLRI. The shorter the silent pause or the faster the speech rate, the higher the therapist empathy. Additionally, there were significant interaction effects of the duration of utterance-final syllables with silent pause and speech rate respectively. For the same unit of increment in the duration of utterance-final syllables, shorter silent pause or faster speech rate was predicted to have a greater magnitude of improvement in therapist empathy than longer silent pause or slower speech rate. Also, the interaction between silent pause and speech rate was significant. For utterances with fast (slow) speech rates, the longer the silent pause, the lower (higher) the therapist empathy. Our results have shown that clients integrated low-order durational patterning at the discourse boundaries in their higher-order perception of therapist empathy, which have clinical/educational implications for the use of prosody in psychotherapy."
   ],
   "doi": "10.21437/Interspeech.2022-722"
  },
  "wang22l_interspeech": {
   "authors": [
    [
     "Yi",
     "Wang"
    ],
    [
     "Tianzi",
     "Wang"
    ],
    [
     "Zi",
     "Ye"
    ],
    [
     "Lingwei",
     "Meng"
    ],
    [
     "Shoukang",
     "Hu"
    ],
    [
     "Xixin",
     "Wu"
    ],
    [
     "Xunying",
     "Liu"
    ],
    [
     "Helen",
     "Meng"
    ]
   ],
   "title": "Exploring linguistic feature and model combination for speech recognition based automatic AD detection",
   "original": "723",
   "page_count": 5,
   "order": 674,
   "p1": 3328,
   "pn": 3332,
   "abstract": [
    "Early diagnosis of Alzheimer's disease (AD) is crucial in facilitating preventive care and delay progression. Speech based automatic AD screening systems provide a non-intrusive and more scalable alternative to other clinical screening techniques. Scarcity of such specialist data leads to uncertainty in both model selection and feature learning when developing such systems. To this end, this paper investigates the use of feature and model combination approaches to improve the robustness of domain fine-tuning of BERT and Roberta pre-trained text encoders on limited data, before the resulting embedding features being fed into an ensemble of backend classifiers to produce the final AD detection decision via majority voting. Experiments conducted on the ADReSS20 Challenge dataset suggest consistent performance improvements were obtained using model and feature combination in system development. State-of-the-art AD detection accuracies of 91.67% and 93.75% were obtained using manual and ASR speech transcripts respectively on the ADReSS20 test set consisting of 48 elderly speakers."
   ],
   "doi": "10.21437/Interspeech.2022-723"
  },
  "huang22d_interspeech": {
   "authors": [
    [
     "Zi",
     "Huang"
    ],
    [
     "Shulei",
     "Ji"
    ],
    [
     "Zhilan",
     "Hu"
    ],
    [
     "Chuangjian",
     "Cai"
    ],
    [
     "Jing",
     "Luo"
    ],
    [
     "Xinyu",
     "Yang"
    ]
   ],
   "title": "ADFF: Attention Based Deep Feature Fusion Approach for Music Emotion Recognition",
   "original": "726",
   "page_count": 5,
   "order": 842,
   "p1": 4152,
   "pn": 4156,
   "abstract": [
    "Music emotion recognition (MER), a sub-task of music information retrieval (MIR), has developed rapidly in recent years. However, the learning of affect-salient features remains a challenge. In this paper, we propose an end-to-end attention-based deep feature fusion (ADFF) approach for MER. Only taking log Mel-spectrogram as input, this method uses adapted VGGNet as spatial feature learning module (SFLM) to obtain spatial features across different levels. Then, these features are fed into squeeze-and-excitation (SE) attention-based temporal feature learning module (TFLM) to get multi-level emotion-related spatial-temporal features (ESTFs), which can discriminate emotions well in the final emotion space. In addition, a novel data processing is devised to cut the single-channel input into multi-channel to improve calculative efficiency while ensuring the quality of MER. Experiments show that our proposed method achieves 10.43% and 4.82% relative improvement of valence and arousal respectively on the R2 score compared to the state-of-the-art model, meanwhile, performs better on datasets with distinct scales and in multi-task learning."
   ],
   "doi": "10.21437/Interspeech.2022-726"
  },
  "yang22h_interspeech": {
   "authors": [
    [
     "Zehui",
     "Yang"
    ],
    [
     "Yifan",
     "Chen"
    ],
    [
     "Lei",
     "Luo"
    ],
    [
     "Runyan",
     "Yang"
    ],
    [
     "Lingxuan",
     "Ye"
    ],
    [
     "Gaofeng",
     "Cheng"
    ],
    [
     "Ji",
     "Xu"
    ],
    [
     "Yaohui",
     "Jin"
    ],
    [
     "Qingqing",
     "Zhang"
    ],
    [
     "Pengyuan",
     "Zhang"
    ],
    [
     "Lei",
     "Xie"
    ],
    [
     "Yonghong",
     "Yan"
    ]
   ],
   "title": "Open Source MagicData-RAMC: A Rich Annotated Mandarin Conversational(RAMC) Speech Dataset",
   "original": "729",
   "page_count": 5,
   "order": 352,
   "p1": 1736,
   "pn": 1740,
   "abstract": [
    "This paper introduces a high-quality rich annotated Mandarin conversational (RAMC) speech dataset called MagicData-RAMC. The MagicData-RAMC corpus contains 180 hours of conversational speech data recorded from native speakers of Mandarin Chinese over mobile phones with a sampling rate of 16 kHz. The dialogs in MagicData-RAMC are classified into 15 diversified domains and tagged with topic labels, ranging from science and technology to ordinary life. Accurate transcription and precise speaker voice activity timestamps are manually labeled for each sample. Speakers' detailed information is also provided. As a Mandarin speech dataset designed for dialog scenarios with high quality and rich annotations, MagicData-RAMC enriches the data diversity in the Mandarin speech community and allows extensive research on a series of speech-related tasks, including automatic speech recognition, speaker diarization, topic detection, keyword search, text-to-speech, etc. We also conduct several relevant tasks and provide experimental results to help evaluate the dataset."
   ],
   "doi": "10.21437/Interspeech.2022-729"
  },
  "mengnan22_interspeech": {
   "authors": [
    [
     "He",
     "Mengnan"
    ],
    [
     "Tingwei",
     "Guo"
    ],
    [
     "Zhenxing",
     "Lu"
    ],
    [
     "Zhang",
     "Ruixiong"
    ],
    [
     "Gong",
     "Caixia"
    ]
   ],
   "title": "Improving GAN-based vocoder for fast and high-quality speech synthesis",
   "original": "730",
   "page_count": 5,
   "order": 325,
   "p1": 1601,
   "pn": 1605,
   "abstract": [
    "Following tremendous success in the Generative Adversarial Network(GAN), the GAN-based vocoders have recently shown much faster speed in waveform generation. However, the quality of generated speech is slightly inferior, and the real-time factor (RTF) still can’t be satisfied in many devices with limited resources. To address the issues, we propose a new GAN-based vocoder model.Firstly, we introduce the Shuffle-Residual Block into the generator to get a lower RTF. Secondly, we propose a Frequency Transformation Block in the discriminator to capture the correlation between different frequency bins in every frame. To the best of our knowledge, our model achieves the lowest RTF of the GAN-based vocoders under the premise of ensuring the speech quality. In our experiments, our model shows a lower RTF with more than 40% improvement and higher speech quality than MB-MelGAN and HiFi-GAN V2."
   ],
   "doi": "10.21437/Interspeech.2022-730"
  },
  "brummer22_interspeech": {
   "authors": [
    [
     "Niko",
     "Brummer"
    ],
    [
     "Albert",
     "Swart"
    ],
    [
     "Ladislav",
     "Mosner"
    ],
    [
     "Anna",
     "Silnova"
    ],
    [
     "Oldrich",
     "Plchot"
    ],
    [
     "Themos",
     "Stafylakis"
    ],
    [
     "Lukas",
     "Burget"
    ]
   ],
   "title": "Probabilistic Spherical Discriminant Analysis: An Alternative to PLDA for length-normalized embeddings",
   "original": "731",
   "page_count": 5,
   "order": 294,
   "p1": 1446,
   "pn": 1450,
   "abstract": [
    "In speaker recognition, where speech segments are mapped to embeddings on the unit hypersphere, two scoring backends are commonly used, namely cosine scoring or PLDA. Both have advantages and disadvantages, depending on the context. Cosine scoring follows naturally from the spherical geometry, but for PLDA the blessing is mixed—length normalization Gaussianizes the between-speaker distribution, but violates the assumption of a speaker-independent within-speaker distribution. We propose PSDA, an analogue to PLDA that uses Von Mises- Fisher distributions on the hypersphere for both within and between-class distributions. We show how the self-conjugacy of this distribution gives closed-form likelihood-ratio scores, making it a drop-in replacement for PLDA at scoring time. All kinds of trials can be scored, including single-enroll and multienroll verification, as well as more complex likelihood-ratios that could be used in clustering and diarization. Learning is done via an EM-algorithm with closed-form updates. We explain the model and present some first experiments."
   ],
   "doi": "10.21437/Interspeech.2022-731"
  },
  "weninger22_interspeech": {
   "authors": [
    [
     "Felix",
     "Weninger"
    ],
    [
     "Marco",
     "Gaudesi"
    ],
    [
     "Md Akmal",
     "Haidar"
    ],
    [
     "Nicola",
     "Ferri"
    ],
    [
     "Jesús",
     "Andrés-Ferrer"
    ],
    [
     "Puming",
     "Zhan"
    ]
   ],
   "title": "Conformer with dual-mode chunked attention for joint online and offline ASR",
   "original": "733",
   "page_count": 5,
   "order": 419,
   "p1": 2053,
   "pn": 2057,
   "abstract": [
    "In this paper, we present an in-depth study on online attention mechanisms and distillation techniques for dual-mode (i.e., joint online and offline) ASR using the Conformer Transducer. In the dual-mode Conformer Transducer model, layers can function in online or offline mode while sharing parameters, and in-place knowledge distillation from offline to online mode is applied in training to improve online accuracy. In our study, we first demonstrate accuracy improvements from using chunked attention in the Conformer encoder compared to autoregressive attention with and without lookahead. Furthermore, we explore the efficient KLD and 1-best KLD losses with different shifts between online and offline outputs in the knowledge distillation. Finally, we show that a simplified dual-mode Conformer that only has mode-specific self-attention performs equally well as the one also having mode-specific convolutions and normalization. Our experiments are based on two very different datasets: the Librispeech task and an internal corpus of medical conversations. Results show that the proposed dual-mode system using chunked attention yields 5% and 4% relative WER improvement on the Librispeech and medical tasks, compared to the dual-mode system using autoregressive attention with similar average lookahead."
   ],
   "doi": "10.21437/Interspeech.2022-733"
  },
  "baskar22_interspeech": {
   "authors": [
    [
     "Murali Karthick",
     "Baskar"
    ],
    [
     "Andrew",
     "Rosenberg"
    ],
    [
     "Bhuvana",
     "Ramabhadran"
    ],
    [
     "Yu",
     "Zhang"
    ],
    [
     "Nicolás",
     "Serrano"
    ]
   ],
   "title": "Reducing Domain mismatch in Self-supervised speech pre-training",
   "original": "736",
   "page_count": 5,
   "order": 614,
   "p1": 3028,
   "pn": 3032,
   "abstract": [
    "Masked speech modeling (MSM) methods such as wav2vec2 or w2v-BERT learn representations over speech frames which are randomly masked within an utterance. While these methods improve performance of Automatic Speech Recognition (ASR) systems, they have one major limitation. They treat all unsupervised speech samples with equal weight, which hinders learning as not all samples have relevant information to learn meaningful representations. In this work, we address this limitation. We propose ask2mask (ATM), a novel approach to focus on specific samples during MSM pre-training. ATM employs an external ASR model or scorer to weight unsupervised input samples by performing a fine-grained data selection. ATM performs masking over the highly confident input frames as chosen by the scorer. This allows the model to learn meaningful representations. We conduct fine-tuning experiments on two well-benchmarked corpora: LibriSpeech (matching the pre-training data) and, AMI and CHiME-6 (not matching the pre-training data). The results substantiate the efficacy of ATM on significantly improving the recognition performance under mismatched conditions while still yielding modest improvements under matched conditions."
   ],
   "doi": "10.21437/Interspeech.2022-736"
  },
  "bae22b_interspeech": {
   "authors": [
    [
     "Jaesung",
     "Bae"
    ],
    [
     "Jinhyeok",
     "Yang"
    ],
    [
     "Taejun",
     "Bak"
    ],
    [
     "Young-Sun",
     "Joo"
    ]
   ],
   "title": "Hierarchical and Multi-Scale Variational Autoencoder for Diverse and Natural Non-Autoregressive Text-to-Speech",
   "original": "737",
   "page_count": 5,
   "order": 164,
   "p1": 813,
   "pn": 817,
   "abstract": [
    "This paper proposes a hierarchical and multi-scale variational autoencoder-based non-autoregressive text-to-speech model (HiMuV-TTS) to generate natural speech with diverse speaking styles. Recent advances in non-autoregressive TTS (NAR-TTS) models have significantly improved the inference speed and robustness of synthesized speech. However, the diversity of speaking styles and naturalness are needed to be improved. To solve this problem, we propose the HiMuV-TTS model that first determines the global-scale prosody and then determines the local-scale prosody via conditioning on the global-scale prosody and the learned text representation. In addition, we improve the quality of speech by adopting the adversarial training technique. Experimental results verify that the proposed HiMuV-TTS model can generate more diverse and natural speech as compared to TTS models with single-scale variational autoencoders, and can represent different prosody information in each scale."
   ],
   "doi": "10.21437/Interspeech.2022-737"
  },
  "zhao22i_interspeech": {
   "authors": [
    [
     "Running",
     "Zhao"
    ],
    [
     "Jiangtao",
     "Yu"
    ],
    [
     "Tingle",
     "Li"
    ],
    [
     "Hang",
     "Zhao"
    ],
    [
     "Edith C. H.",
     "Ngai"
    ]
   ],
   "title": "Radio2Speech: High Quality Speech Recovery from Radio Frequency Signals",
   "original": "738",
   "page_count": 5,
   "order": 945,
   "p1": 4666,
   "pn": 4670,
   "abstract": [
    "Considering the microphone is easily affected by noise and soundproof materials, the radio frequency (RF) signal is a promising candidate to recover audio as it is immune to noise and can traverse many soundproof objects. In this paper, we introduce Radio2Speech, a system that uses RF signals to recover high quality speech from the loudspeaker. Radio2Speech can recover speech comparable to the quality of the microphone, advancing from recovering only single tone music or incomprehensible speech in existing approaches. We use Radio UNet to accurately recover speech in time-frequency domain from RF signals with limited frequency band. Also, we incorporate the neural vocoder to synthesize the speech waveform from the estimated time-frequency representation without using the contaminated phase. Quantitative and qualitative evaluations show that in quiet, noisy and soundproof scenarios, Radio2Speech achieves state-of-the-art performance and is on par with the microphone that works in quiet scenarios."
   ],
   "doi": "10.21437/Interspeech.2022-738"
  },
  "zhang22l_interspeech": {
   "authors": [
    [
     "Chenhui",
     "Zhang"
    ],
    [
     "Xiang",
     "Pan"
    ]
   ],
   "title": "Single-channel speech enhancement using Graph Fourier Transform",
   "original": "740",
   "page_count": 5,
   "order": 193,
   "p1": 946,
   "pn": 950,
   "abstract": [
    "This paper presents combination of Graph Fourier Trans- form (GFT) and U-net, proposes a deep neural network (DNN) named G-Unet for single channel speech enhancement. GFT is carried out over speech data for creating inputs of U-net. The GFT outputs are combined with the mask estimated by U- net in time-graph (T-G) domain to reconstruct enhanced speech in time domain by Inverse GFT. The G-Unet outperforms the combination of Short time Fourier Transform (STFT) and mag- nitude estimation U-net in improving speech quality and de- reverberation, and outperforms the combination of STFT and complex U-net in improving speech quality in some cases, which is validated by testing on LibriSpeech and NOISEX92 dataset."
   ],
   "doi": "10.21437/Interspeech.2022-740"
  },
  "lei22b_interspeech": {
   "authors": [
    [
     "Han",
     "Lei"
    ],
    [
     "Ning",
     "Chen"
    ]
   ],
   "title": "Audio-Visual Scene Classification Based on Multi-modal Graph Fusion",
   "original": "741",
   "page_count": 5,
   "order": 843,
   "p1": 4157,
   "pn": 4161,
   "abstract": [
    "Audio-Visual Scene Classification (AVSC) task tries to achieve scene classification through joint analysis of the audio and video modalities. Most of the existing AVSC models are based on feature-level or decision-level fusion. The possible problems are: i) Due to the distribution difference of the corresponding features in different modalities is large, the direct concatenation of them in the feature-level fusion may not result in good performance. ii) The decision-level fusion cannot take full advantage of the common as well as complementary properties between the features and corresponding similarities of different modalities. To solve these problems, Graph Convolutional Network (GCN)-based multi-modal fusion algorithm is proposed for AVSC task. First, the Deep Neural Network (DNN) is trained to extract essential feature from each modality. Then, the Sample-to-Sample Cross Similarity Graph (SSCSG) is constructed based on each modality features. Finally, the DynaMic GCN (DM-GCN) and the ATtention GCN (AT-GCN) are introduced respectively to realize both feature-level and similarity-level fusion to ensure the classification accuracy. Experimental results on TAU Audio-Visual Urban Scenes 2021 development dataset demonstrate that the proposed scheme, called AVSC-MGCN achieves higher classification accuracy and lower computational complexity than state-of-the-art schemes."
   ],
   "doi": "10.21437/Interspeech.2022-741"
  },
  "han22b_interspeech": {
   "authors": [
    [
     "Bing",
     "Han"
    ],
    [
     "Zhengyang",
     "Chen"
    ],
    [
     "Yanmin",
     "Qian"
    ]
   ],
   "title": "Self-Supervised Speaker Verification Using Dynamic Loss-Gate and Label Correction",
   "original": "742",
   "page_count": 5,
   "order": 968,
   "p1": 4780,
   "pn": 4784,
   "abstract": [
    "For self-supervised speaker verification, the quality of pseudo labels decides the upper bound of the system due to the massive unreliable labels. In this work, we propose dynamic loss-gate and label correction (DLG-LC) to alleviate the performance degradation caused by unreliable estimated labels. In DLG, we adopt Gaussian Mixture Model (GMM) to dynamically model the loss distribution and use the estimated GMM to distinguish the reliable and unreliable labels automatically. Besides, to better utilize the unreliable data instead of dropping them directly, we correct the unreliable label with model predictions. Moreover, we apply the negative-pairs-free DINO framework in our experiments for further improvement. Compared to the best-known speaker verification system with self-supervised learning, our proposed DLG-LC converges faster and achieves 11.45%, $18.35% and 15.16% relative improvement on Vox-O, Vox-E and Vox-H trials of Voxceleb1 evaluation dataset."
   ],
   "doi": "10.21437/Interspeech.2022-742"
  },
  "sakuma22_interspeech": {
   "authors": [
    [
     "Jin",
     "Sakuma"
    ],
    [
     "Shinya",
     "Fujie"
    ],
    [
     "Tetsunori",
     "Kobayashi"
    ]
   ],
   "title": "Response Timing Estimation for Spoken Dialog System using Dialog Act Estimation",
   "original": "746",
   "page_count": 5,
   "order": 909,
   "p1": 4486,
   "pn": 4490,
   "abstract": [
    "We propose neural networks for predicting response timing of spoken dialog systems. Response timing varies depending on the dialog context. This context-dependent response timing is conventionally estimated directly from acoustic event sequences and word sequences extracted from past utterances. Since there are so wide varieties in these sequences, large amounts of training data are required to build reliable models. While, there is no large dialog databases with response timings annotated. The proposed method estimates dialog act for each utterance as an auxiliary task, and uses its intermediate states for response timing estimation in addition to acoustic and linguistic features. Since dialog act has significantly less variation than word sequences and is closely related to response timing, we expect to be able to construct a highly reliable model even with small training data. We evaluate our approach on the HarperValleyBank corpus. The experimental results show that the proposed approach is more effective than the conventional approach that does not use dialog act information for each utterance such as dialog act."
   ],
   "doi": "10.21437/Interspeech.2022-746"
  },
  "liu22k_interspeech": {
   "authors": [
    [
     "Yukun",
     "Liu"
    ],
    [
     "Ta",
     "Li"
    ],
    [
     "Pengyuan",
     "Zhang"
    ],
    [
     "Yonghong",
     "Yan"
    ]
   ],
   "title": "NAS-SCAE: Searching Compact Attention-based Encoders For End-to-end Automatic Speech Recognition",
   "original": "748",
   "page_count": 5,
   "order": 206,
   "p1": 1011,
   "pn": 1015,
   "abstract": [
    "Recently plenty of attention-based encoders have been proposed for end-to-end (E2E) automatic speech recognition (ASR). Despite the impressive performance, these encoders usually have a large model size and suffer from expensive memory and computation costs. To obtain more compact encoders for E2E ASR, we propose searching compact attention-based encoders using neural architecture search (NAS) in this paper, named NAS-SCAE. NAS-SCAE consists of one search space that contains a set of candidate encoders and one search algorithm responsible for searching the optimal encoder from the search space. On one hand, NAS-SCAE designs a topology-fused search space to integrate different architecture topologies of existing encoders (e.g. Transformer, Conformer) and explore more brand-new architectures. On the other hand, combined with the training pipeline of E2E ASR, NAS-SCAE develops a resource-aware differentiable search algorithm to search compact encoders efficiently and proposes an adjustable search scheme to alleviate the joint optimization problem of the differentiable search algorithm. On four Mandarin and English datasets, NAS-SCAE can effectively reduce the encoder resource consumption with negligible performance drop and achieve at least 2.13x/2.09x parameters/FLOPs reduction than the human-designed baselines."
   ],
   "doi": "10.21437/Interspeech.2022-748"
  },
  "shrem22_interspeech": {
   "authors": [
    [
     "Yosi",
     "Shrem"
    ],
    [
     "Felix",
     "Kreuk"
    ],
    [
     "Joseph",
     "Keshet"
    ]
   ],
   "title": "Formant Estimation and Tracking using Probabilistic Heat-Maps",
   "original": "749",
   "page_count": 5,
   "order": 721,
   "p1": 3563,
   "pn": 3567,
   "abstract": [
    "Formants are the spectral maxima that result from acoustic resonances of the human vocal tract, and their accurate estimation is among the most fundamental speech processing problems. Recent work has been shown that those frequencies can accurately be estimated using deep learning techniques. However, when presented with a speech from a different domain than that in which they have been trained on, these methods exhibit a decline in performance, limiting their usage as generic tools. The contribution of this paper is to propose a new network architecture that performs well on a variety of different speaker and speech domains. Our proposed model is composed of a shared encoder that gets as input a spectrogram and outputs a domain-invariant representation. Then, multiple decoders further process this representation, each responsible for predicting a different formant while considering the lower formant predictions. An advantage of our model is that it is based on heatmaps that generate a probability distribution over formant predictions. Results suggest that our proposed model better represents the signal over various domains and leads to better formant frequency tracking and estimation."
   ],
   "doi": "10.21437/Interspeech.2022-749"
  },
  "maouche22_interspeech": {
   "authors": [
    [
     "Mohamed",
     "Maouche"
    ],
    [
     "Brij Mohan Lal",
     "Srivastava"
    ],
    [
     "Nathalie",
     "Vauquier"
    ],
    [
     "Aurélien",
     "Bellet"
    ],
    [
     "Marc",
     "Tommasi"
    ],
    [
     "Emmanuel",
     "Vincent"
    ]
   ],
   "title": "Enhancing Speech Privacy with Slicing",
   "original": "752",
   "page_count": 5,
   "order": 1017,
   "p1": 5025,
   "pn": 5029,
   "abstract": [
    "Privacy preservation calls for anonymization methods which hide the speaker's identity in speech signals while minimizing the impact on downstream tasks such as automatic speech recognition (ASR) training or decoding. In the VoicePrivacy 2020 Challenge, voice anonymization methods have been proposed to transform speech utterances in a way that preserves their verbal and prosodic contents while reducing the accuracy of a speaker verification system. In this paper, we propose to further increase the privacy achieved by such methods by segmenting the utterances into shorter slices. We show that our approach has two major impacts on privacy. First, it reduces the accuracy of speaker verification with respect to unsegmented utterances. Second, it also reduces the amount of personal information that can be extracted from the verbal content, in a way that cannot easily be reversed by an attacker. We also show that it is possible to train an ASR system from anonymized speech slices with negligible impact on the word error rate."
   ],
   "doi": "10.21437/Interspeech.2022-752"
  },
  "baruah22_interspeech": {
   "authors": [
    [
     "Murchana",
     "Baruah"
    ],
    [
     "Bonny",
     "Banerjee"
    ]
   ],
   "title": "Speech Emotion Recognition via Generation using an Attention-based Variational Recurrent Neural Network",
   "original": "753",
   "page_count": 5,
   "order": 954,
   "p1": 4710,
   "pn": 4714,
   "abstract": [
    "The last decade has seen an exponential rise in the number of attention-based models for speech emotion recognition (SER). Most of these models use a spectrogram as the input speech representation and the CNN or RNN or convolutional RNN as the key machine learning (ML) component, and learn feature weights to implement attention. We propose an attention-based model for SER that uses MFCC as the input speech representation and a variational RNN (VRNN) as the key ML component. Since the MFCC is of lower dimension than a spectrogram, the model is size- and data-efficient. The VRNN has been used for problems in vision but rarely for SER. Our model is predictive in nature. At each instant, it infers the emotion class and generates the next observation, computes the generation error, and selectively samples (attends to) the locations of high error. Thus, attention emerges in our model, and does not require learning feature weights. This simple model provides interesting insights when evaluated for SER on benchmark datasets. The model can operate on variable length and infinite duration audio files. This work is the first to explore simultaneous generation and recognition for SER, where the generation capability is necessary for efficient recognition."
   ],
   "doi": "10.21437/Interspeech.2022-753"
  },
  "lu22b_interspeech": {
   "authors": [
    [
     "Shun",
     "Lu"
    ],
    [
     "Yang",
     "Wang"
    ],
    [
     "Peng",
     "Yao"
    ],
    [
     "Chenxing",
     "Li"
    ],
    [
     "Jianchao",
     "Tan"
    ],
    [
     "Feng",
     "Deng"
    ],
    [
     "Xiaorui",
     "Wang"
    ],
    [
     "Chengru",
     "Song"
    ]
   ],
   "title": "Conformer Space Neural Architecture Search for Multi-Task Audio Separation",
   "original": "755",
   "page_count": 5,
   "order": 1086,
   "p1": 5358,
   "pn": 5362,
   "abstract": [
    "Multi-task audio source separation aims to separate the audios collected from the complex environment into three fixed types of signal sources. Existing methods like EAD-Conformer usually take a manually designed model to process the separation. These networks may be sub-optimal since it is hard for humans to train and test all possible architectures. Especially, it is natural to adopt different optimal sub-structures for decoding different types of signals, which, however, is very hard for humans to enumerate. In this paper, we quantitatively analyze the redundancy of the EAD-Conformer network and customize an effective and efficient search space. We propose an efficient K-path search method to search for the optimal architectures from the Conformer-based search space. We conduct a comprehensive search in terms of block numbers, head numbers, and channel numbers. Extensive experiments demonstrate that our searched architectures outperform existing methods in terms of efficiency and effectiveness."
   ],
   "doi": "10.21437/Interspeech.2022-755"
  },
  "lenglet22_interspeech": {
   "authors": [
    [
     "Martin",
     "Lenglet"
    ],
    [
     "Olivier",
     "Perrotin"
    ],
    [
     "Gérard",
     "Bailly"
    ]
   ],
   "title": "Speaking Rate Control of end-to-end TTS Models by Direct Manipulation of the Encoder's Output Embeddings",
   "original": "759",
   "page_count": 5,
   "order": 3,
   "p1": 11,
   "pn": 15,
   "abstract": [
    "Since neural Text-To-Speech models have achieved such high standards in terms of naturalness, the main focus of the field has gradually shifted to gaining more control over the expressiveness of the synthetic voices. One of these leverages is the control of the speaking rate that has become harder for a human operator to control since the introduction of neural attention networks to model speech dynamics. While numerous models have reintroduced an explicit duration control (ex: FastSpeech2), these models generally rely on additional tasks to complete during their training. In this paper, we show how an acoustic analysis of the internal embeddings delivered by the encoder of an unsupervised end-to-end TTS Tacotron2 model is enough to identify and control some acoustic parameters of interest. Specifically, we compare this speaking rate control with the duration control offered by a supervised FastSpeech2 model. Experimental results show that the control provided by embeddings reproduces a behaviour closer to natural speech data."
   ],
   "doi": "10.21437/Interspeech.2022-759"
  },
  "avila22_interspeech": {
   "authors": [
    [
     "Anderson R.",
     "Avila"
    ],
    [
     "Khalil",
     "Bibi"
    ],
    [
     "Rui Heng",
     "Yang"
    ],
    [
     "Xinlin",
     "Li"
    ],
    [
     "Chao",
     "Xing"
    ],
    [
     "Xiao",
     "Chen"
    ]
   ],
   "title": "Low-bit Shift Network for End-to-End Spoken Language Understanding",
   "original": "760",
   "page_count": 5,
   "order": 548,
   "p1": 2698,
   "pn": 2702,
   "abstract": [
    "Deep neural networks (DNN) have achieved impressive success in multiple domains. Over the years, the accuracy of these models has increased with the proliferation of deeper and more complex architectures. Thus, state-of-the-art solutions are often computationally expensive, which makes them unfit to be deployed on edge computing platforms. In order to mitigate the high computation, memory, and power requirements of inferring convolutional neural networks (CNNs), we propose the use of power-of-two quantization, which quantizes continuous parameters into low-bit power-of-two values. This reduces computational complexity by removing expensive multiplication operations and with the use of low-bit weights. ResNet is adopted as the building block of our solution and the proposed model is evaluated on a spoken language understanding (SLU) task. Experimental results show improved performance for shift neural network architectures, with our low-bit quantization achieving 98.76 % on the test set which is comparable performance to its full-precision counterpart and state-of-the-art solutions."
   ],
   "doi": "10.21437/Interspeech.2022-760"
  },
  "guo22b_interspeech": {
   "authors": [
    [
     "Zixun",
     "Guo"
    ],
    [
     "Chen",
     "Chen"
    ],
    [
     "Eng Siong",
     "Chng"
    ]
   ],
   "title": "DENT-DDSP: Data-efficient noisy speech generator using differentiable digital signal processors for explicit distortion modelling and noise-robust speech recognition",
   "original": "763",
   "page_count": 5,
   "order": 770,
   "p1": 3799,
   "pn": 3803,
   "abstract": [
    "The performances of automatic speech recognition (ASR) systems degrade drastically under noisy conditions. Explicit distortion modelling (EDM), as a feature compensation step, is able to enhance ASR systems under such conditions by simulating the in-domain noisy speeches from the clean counterparts. Yet, existing distortion models are either non-trainable or unexplainable and often lack controllability and generalization ability. In this paper, we propose a fully explainable and controllable model: DENT-DDSP to achieve EDM. DENT-DDSP utilizes novel differentiable digital signal processing (DDSP) components and requires only 10 seconds of training data to achieve high fidelity. The experiment shows that the simulated noisy data from DENT-DDSP achieves the highest simulation fidelity compared to other baseline models in terms of multi-scale spectral loss (MSSL). Moreover, to validate whether the data simulated by DENT-DDSP are able to replace the scarce in-domain noisy data in the noise-robust ASR tasks, several downstream ASR models with the same architecture are trained using the simulated data and the real data. The experiment shows that the model trained with the simulated noisy data from DENT-DDSP achieves similar performances to the benchmark with a 2.7% difference in terms of word error rate (WER). The code of the model is released online."
   ],
   "doi": "10.21437/Interspeech.2022-763"
  },
  "hu22d_interspeech": {
   "authors": [
    [
     "Ying",
     "Hu"
    ],
    [
     "Xiujuan",
     "Zhu"
    ],
    [
     "Yunlong",
     "Li"
    ],
    [
     "Hao",
     "Huang"
    ],
    [
     "Liang",
     "He"
    ]
   ],
   "title": "A Multi-grained based Attention Network for Semi-supervised Sound Event Detection",
   "original": "767",
   "page_count": 5,
   "order": 311,
   "p1": 1531,
   "pn": 1535,
   "abstract": [
    "Sound event detection (SED) is an interesting but challenging task due to the scarcity of data and diverse sound events in real life. This paper presents a multi-grained based attention network (MGA-Net) for semi-supervised sound event detection. To obtain the feature representations related to sound events, a residual hybrid convolution (RH-Conv) block is designed to boost the vanilla convolution's ability to extract the time-frequency features. Moreover, a multi-grained attention (MGA) module is designed to learn temporal resolution features from coarse-level to fine-level. With the MGA module, the network could capture the characteristics of target events with short- or long-duration, resulting in more accurately determining the onset and offset of sound events. Furthermore, to effectively boost the performance of the Mean Teacher (MT) method, a spatial shift (SS) module as a data perturbation mechanism is introduced to increase the diversity of data. Experimental results show that the MGA-Net outperforms the published state-of-the-art competitors, achieving 53.27% and 56.96% event-based macro F1 (EB-F1) score, 0.709 and 0.739 polyphonic sound detection score (PSDS) on the validation and public set respectively."
   ],
   "doi": "10.21437/Interspeech.2022-767"
  },
  "zevallos22_interspeech": {
   "authors": [
    [
     "Rodolfo",
     "Zevallos"
    ],
    [
     "Núria",
     "Bel"
    ],
    [
     "Guillermo",
     "Cámbara"
    ],
    [
     "Mireia",
     "Farrús"
    ],
    [
     "Jordi",
     "Luque"
    ]
   ],
   "title": "Data Augmentation for Low-Resource Quechua ASR Improvement",
   "original": "770",
   "page_count": 5,
   "order": 712,
   "p1": 3518,
   "pn": 3522,
   "abstract": [
    "Automatic Speech Recognition (ASR) is a key element in new services that helps users to interact with an automated system. Deep learning methods have made it possible to deploy systems with a word error rate close to only 5% for ASR of English. However, the use of these methods is only available for languages with hundreds or thousands of hours of audio and their corresponding transcriptions. For the so-called low- resource languages to speed up the availability of resources that can improve the performance of their ASR systems, methods of creating new resources on the basis of existing ones are being investigated. In this paper we describe our DA approach to improve the results of ASR models for low-resource and agglutinative languages. We carry out experiments developing an ASR for Quechua using the Wav2letter++ model. We reduced WER by 8.73% through our approach to the base model. The resulting ASR model obtained 22.75% WER and was trained with 99 hours of original resources and 99 hours of synthetic data obtained with a combination of text augmentation and synthetic speech generation."
   ],
   "doi": "10.21437/Interspeech.2022-770"
  },
  "tian22_interspeech": {
   "authors": [
    [
     "Sanli",
     "Tian"
    ],
    [
     "Keqi",
     "Deng"
    ],
    [
     "Zehan",
     "Li"
    ],
    [
     "Lingxuan",
     "Ye"
    ],
    [
     "Gaofeng",
     "Cheng"
    ],
    [
     "Ta",
     "Li"
    ],
    [
     "Yonghong",
     "Yan"
    ]
   ],
   "title": "Knowledge Distillation For CTC-based Speech Recognition Via Consistent Acoustic Representation Learning",
   "original": "775",
   "page_count": 5,
   "order": 535,
   "p1": 2633,
   "pn": 2637,
   "abstract": [
    "Recently, end-to-end ASR models based on connectionist temporal classification (CTC) have achieved impressive results, but their performance is limited in lightweight models. Knowledge distillation (KD) is a popular model compression method for improving the performance of lightweight models. However, CTC-models emit spiky posterior distribution making KL-divergence hard to converge, thus hindering the application of KD. To address this issue, we propose a new frame-level KD method that significantly improves the performance of lightweight CTC-based ASR models. First, we design a blank-frame-elimination mechanism that addresses the difficulty of applying KL-divergence on CTC posterior distribution. Second, we propose a consistent-acoustic-representation-learning (CARL) method to improve the representation ability of student model. Unlike matching the student model's feature to the teacher model's feature directly, CARL passes the teacher and student encoder's output features through the teacher's pre-trained classifier to produce similar outputs by blank-frame-elimination, making teacher and student represent acoustic features in a consistent way. Third, we introduce a two-stage process to further improve the accuracy of ASR, which performs feature-level KD via cosine-similarity in stage1 and softmax-level KD by CARL in stage2. Compared to the vanilla CTC-baseline model, our method relatively reduces CER by 16.1% and WER by 26.0% on Aishell-1 and Ted-lium2."
   ],
   "doi": "10.21437/Interspeech.2022-775"
  },
  "gabeur22_interspeech": {
   "authors": [
    [
     "Valentin",
     "Gabeur"
    ],
    [
     "Paul Hongsuck",
     "Seo"
    ],
    [
     "Arsha",
     "Nagrani"
    ],
    [
     "Chen",
     "Sun"
    ],
    [
     "Karteek",
     "Alahari"
    ],
    [
     "Cordelia",
     "Schmid"
    ]
   ],
   "title": "AVATAR: Unconstrained Audiovisual Speech Recognition",
   "original": "776",
   "page_count": 5,
   "order": 572,
   "p1": 2818,
   "pn": 2822,
   "abstract": [
    "Audio-visual automatic speech recognition (AV-ASR) is an extension of ASR that incorporates visual cues, often from the movements of a speaker's mouth. Unlike works that simply focus on the lip motion, we investigate the contribution of entire visual frames (visual actions, objects, background etc.). This is particularly useful for unconstrained videos, where the speaker is not necessarily visible. To solve this task, we propose a new sequence-to-sequence AudioVisual ASR TrAnsformeR (AVATAR) which is trained end-to-end from spectrograms and full-frame RGB. To prevent the audio stream from dominating training, we propose different word-masking strategies, thereby encouraging our model to pay attention to the visual stream. We demonstrate the contribution of the visual modality on the How2 AV-ASR benchmark, especially in the presence of simulated noise, and show that our model outperforms all other prior work by a large margin. Finally, we also create a new, real-world test bed for AV-ASR called VisSpeech, which demonstrates the contribution of the visual modality under challenging audio conditions."
   ],
   "doi": "10.21437/Interspeech.2022-776"
  },
  "dissen22_interspeech": {
   "authors": [
    [
     "Yehoshua",
     "Dissen"
    ],
    [
     "Felix",
     "Kreuk"
    ],
    [
     "Joseph",
     "Keshet"
    ]
   ],
   "title": "Self-supervised Speaker Diarization",
   "original": "777",
   "page_count": 5,
   "order": 814,
   "p1": 4013,
   "pn": 4017,
   "abstract": [
    "Over the last few years, deep learning has grown in popularity for speaker verification, identification, and diarization. Inarguably, a significant part of this success is due to the demonstrated effectiveness of their speaker representations. These, however, are heavily dependent on large amounts of annotated data and can be sensitive to new domains. This study proposes an entirely unsupervised deep-learning model for speaker diarization. Specifically, the study focuses on generating high-quality neural speaker representations without any annotated data, as well as on estimating secondary hyperparameters of the model without annotations. The speaker embeddings are represented by an encoder trained in a self-supervised fashion using pairs of adjacent segments assumed to be of the same speaker. The trained encoder model is then used to self-generate pseudo-labels to subsequently train a similarity score between different segments of the same call using probabilistic linear discriminant analysis (PLDA) and further to learn a clustering stopping threshold. We compared our model to state-of-the-art unsupervised as well as supervised baselines on the CallHome benchmarks. According to empirical results, our approach outperforms unsupervised methods when only two speakers are present in the call, and is only slightly worse than recent supervised models."
   ],
   "doi": "10.21437/Interspeech.2022-777"
  },
  "guo22c_interspeech": {
   "authors": [
    [
     "Zilu",
     "Guo"
    ],
    [
     "Xu",
     "Xu"
    ],
    [
     "Zhongfu",
     "Ye"
    ]
   ],
   "title": "Joint Optimization of the Module and Sign of the Spectral Real Part Based on CRN for Speech Denoising.",
   "original": "778",
   "page_count": 5,
   "order": 194,
   "p1": 951,
   "pn": 955,
   "abstract": [
    "Recently some novel techniques have utilized sophisticated algorithms to correct phase or use phase information by processing real- and image-part respectively or simultaneously in the STFT domain. However, neural networks can not process a complex-valued feature, i.e., the STFT of a noisy speech. Therefore, these methods estimating the STFT of a clean signal can only obtain sub-optimal solutions. To avoid tackling complex-value operations, we formulate that only the real part of 2K-point STFT is utilized as the feature that holds all signal information. Therefore, speech enhancement in the STFT domain turns into a real-valued task. However, it is hard for the network to estimate the correct sign. Consequently, we develop an estimator to predict the real part sign and a decoder to estimate the targeted real part's mask. Then, we devise some experiments to evaluate our model over kinds of metrics. The results indicate that our model outperforms several state-of-the-art (SOTA) models."
   ],
   "doi": "10.21437/Interspeech.2022-778"
  },
  "xue22b_interspeech": {
   "authors": [
    [
     "Huaying",
     "Xue"
    ],
    [
     "Xiulian",
     "Peng"
    ],
    [
     "Xue",
     "Jiang"
    ],
    [
     "Yan",
     "Lu"
    ]
   ],
   "title": "Towards Error-Resilient Neural Speech Coding",
   "original": "779",
   "page_count": 5,
   "order": 855,
   "p1": 4217,
   "pn": 4221,
   "abstract": [
    "Neural audio coding has shown very promising results recently in the literature to largely outperform traditional codecs but limited attention has been paid on its error resilience. Neural codecs trained considering only source coding tend to be extremely sensitive to channel noises, especially in wireless channels with high error rate. In this paper, we investigate how to elevate the error resilience of neural audio codecs for packet losses that often occur during real-time communications. We propose a feature-domain packet loss concealment algorithm (FD-PLC) for real-time neural speech coding. Specifically, we introduce a self-attention-based module on the received latent features to recover lost frames in the feature domain before the decoder. A hybrid segment-level and frame-level frequency-domain discriminator is employed to guide the network to focus on both the generative quality of lost frames and the continuity with neighbouring frames. Experimental results on several error patterns show that the proposed scheme can achieve better robustness compared with the corresponding error-free and error-resilient baselines. We also show that feature-domain concealment is superior to waveform-domain counterpart as post-processing."
   ],
   "doi": "10.21437/Interspeech.2022-779"
  },
  "chang22c_interspeech": {
   "authors": [
    [
     "Ya-Hsin",
     "Chang"
    ],
    [
     "Yun-Nung",
     "Chen"
    ]
   ],
   "title": "Contrastive Learning for Improving ASR Robustness in Spoken Language Understanding",
   "original": "781",
   "page_count": 5,
   "order": 700,
   "p1": 3458,
   "pn": 3462,
   "abstract": [
    "Spoken language understanding (SLU) is an essential task for machines to understand human speech for better interactions. However, errors from the automatic speech recognizer (ASR) usually hurt the understanding performance. In reality, ASR systems may not be easy to adjust for the target scenarios. Therefore, this paper focuses on learning utterance representations that are robust to ASR errors using a contrastive objective, and further strengthens the generalization ability by combining supervised contrastive learning and self-distillation. Experiments on three benchmark datasets demonstrate the effectiveness of our proposed approach."
   ],
   "doi": "10.21437/Interspeech.2022-781"
  },
  "zhou22b_interspeech": {
   "authors": [
    [
     "Peilin",
     "Zhou"
    ],
    [
     "Dading",
     "Chong"
    ],
    [
     "Helin",
     "Wang"
    ],
    [
     "Qingcheng",
     "Zeng"
    ]
   ],
   "title": "Calibrate and Refine! A Novel and Agile Framework for ASR Error Robust Intent Detection",
   "original": "786",
   "page_count": 5,
   "order": 223,
   "p1": 1096,
   "pn": 1100,
   "abstract": [
    "The past ten years have witnessed the rapid development of text-based intent detection, whose benchmark performances have already been taken to a remarkable level by deep learning techniques. However, automatic speech recognition (ASR) errors are inevitable in real-world applications due to the environment noise, unique speech patterns and etc, leading to sharp performance drop in state-of-the-art text-based intent detection models. Essentially, this phenomenon is caused by the semantic drift brought by ASR errors and most existing works tend to focus on designing new model structures to reduce its impact, which is at the expense of versatility and flexibility. Different from previous one-piece model, in this paper, we propose a novel and agile framework called CR-ID for ASR error robust intent detection with two plug-and-play modules, namely semantic drift calibration module (SDCM) and phonemic refinement module (PRM), which are both model-agnostic and thus could be easily integrated to any existing intent detection models without mod-ifying their structures. Experimental results on SNIPS dataset show that, our proposed CR-ID framework achieves competi-tive performance and outperform all the baseline methods on ASR outputs, which verifies that CR-ID can effectively allevi-ate the semantic drift caused by ASR errors."
   ],
   "doi": "10.21437/Interspeech.2022-786"
  },
  "lee22g_interspeech": {
   "authors": [
    [
     "Jaesong",
     "Lee"
    ],
    [
     "Lukas",
     "Lee"
    ],
    [
     "Shinji",
     "Watanabe"
    ]
   ],
   "title": "Memory-Efficient Training of RNN-Transducer with Sampled Softmax",
   "original": "787",
   "page_count": 5,
   "order": 900,
   "p1": 4441,
   "pn": 4445,
   "abstract": [
    "RNN-Transducer has been one of promising architectures for end-to-end automatic speech recognition. Although RNN-Transducer has many advantages including its strong accuracy and streaming-friendly property, its high memory consumption during training has been a critical problem for development. In this work, we propose to apply sampled softmax to RNN-Transducer, which requires only a small subset of vocabulary during training thus saves its memory consumption. We further extend sampled softmax to optimize memory consumption for minibatch, and employ distributions of auxiliary CTC losses for sampling vocabulary to improve model accuracy. We present experimental results on LibriSpeech, AISHELL-1, and CSJ-APS, where sampled softmax greatly reduces memory consumption and still maintains the accuracy of the baseline model."
   ],
   "doi": "10.21437/Interspeech.2022-787"
  },
  "webber22_interspeech": {
   "authors": [
    [
     "Jacob",
     "Webber"
    ],
    [
     "Samuel K.",
     "Lo"
    ],
    [
     "Isaac L.",
     "Bleaman"
    ]
   ],
   "title": "REYD – The First Yiddish Text-to-Speech Dataset and System",
   "original": "789",
   "page_count": 5,
   "order": 481,
   "p1": 2363,
   "pn": 2367,
   "abstract": [
    "Modern text-to-speech (TTS) systems generate high-quality natural-sounding speech, but they only support a limited number of languages. Building data-hungry systems that require large amounts of accurately paired speech and text is challenging for languages with limited resources. Yiddish is a minority language that lacks many of the computational resources available in more widely-spoken languages. No modern TTS system exists for Yiddish. We introduce the Reading Electronic Yiddish Documents or REYD (Yiddish for 'speech') project. Found data is used to create a high-quality, hand-corrected TTS dataset. This dataset is used to train FastSpeech2, a state-of-the-art TTS system. A formal evaluation by expert and non-expert listeners found that the system produced speech that was both intelligible and natural-sounding. The results of this evaluation were used to further improve the dataset. The final hand-corrected dataset, code for creating a TTS system, trained models and other Yiddish text processing tools used in our work are publicly released. We hope the availability of these resources will enable new speech technology projects that better serve the needs of Yiddish-speaking communities."
   ],
   "doi": "10.21437/Interspeech.2022-789"
  },
  "jeon22_interspeech": {
   "authors": [
    [
     "Hae-Sung",
     "Jeon"
    ],
    [
     "Stephen",
     "Nichols"
    ]
   ],
   "title": "Investigating Prosodic Variation in British English Varieties using ProPer",
   "original": "792",
   "page_count": 5,
   "order": 267,
   "p1": 1313,
   "pn": 1317,
   "abstract": [
    "This study used ProPer (PROsodic analysis with PERiodic energy), a tool for automatic prosodic analysis, to investigate prosodic variation in four British English varieties. The central question was how to efficiently describe systematic prosodic variation without extensive manual annotations. We analysed ProPer parameters, the magnitude of F0 change between two successive syllables, synchrony (i.e. the general trend of the F0 movement within a syllable), and periodic energy mass. We then carried out audiovisual analysis of the F0 movement. Our overall assessment is that ProPer allows holistic prosodic analysis, significantly reducing researchers' workload in prosodic annotations. The ProPer analysis is optimised for syllable-sized intervals. It requires the examination of a vast amount of acoustic information and possibly the careful design of experimental materials for collecting laboratory speech data."
   ],
   "doi": "10.21437/Interspeech.2022-792"
  },
  "johnson22_interspeech": {
   "authors": [
    [
     "Alexander",
     "Johnson"
    ],
    [
     "Kevin",
     "Everson"
    ],
    [
     "Vijay",
     "Ravi"
    ],
    [
     "Anissa",
     "Gladney"
    ],
    [
     "Mari",
     "Ostendorf"
    ],
    [
     "Abeer",
     "Alwan"
    ]
   ],
   "title": "Automatic Dialect Density Estimation for African American English",
   "original": "796",
   "page_count": 5,
   "order": 261,
   "p1": 1283,
   "pn": 1287,
   "abstract": [
    "In this paper, we explore automatic prediction of dialect density of the African American English (AAE) dialect, where dialect density is defined as the percentage of words in an utterance that contain characteristics of the non-standard dialect. We investigate several acoustic and language modeling features, including the commonly used X-vector representation and ComParE feature set, in addition to information extracted from ASR transcripts of the audio files and prosodic information. To address issues of limited labeled data, we use a weakly supervised model to compress prosodic and X-vector features into more task-relevant representations. An XGBoost model is then used to predict the speaker's dialect density from these features and show which are most significant during inference. We evaluate the utility of these features both alone and in combination for the given task. This work, which does not rely on hand-labeled transcripts, is performed on audio segments from the CORAAL database. We show a significant correlation between our predicted and ground truth dialect density measures for AAE speech in this database and propose this work as a way to automatically explain and mitigate bias in speech technology."
   ],
   "doi": "10.21437/Interspeech.2022-796"
  },
  "chinen22_interspeech": {
   "authors": [
    [
     "Michael",
     "Chinen"
    ],
    [
     "Jan",
     "Skoglund"
    ],
    [
     "Chandan K. A.",
     "Reddy"
    ],
    [
     "Alessandro",
     "Ragano"
    ],
    [
     "Andrew",
     "Hines"
    ]
   ],
   "title": "Using Rater and System Metadata to Explain Variance in the VoiceMOS Challenge 2022 Dataset",
   "original": "799",
   "page_count": 5,
   "order": 918,
   "p1": 4531,
   "pn": 4535,
   "abstract": [
    "Non-reference speech quality models are important for a growing number of applications. The VoiceMOS 2022 challenge provided a dataset of synthetic voice conversion and text-to-speech samples with subjective labels. This study looks at the amount of variance that can be explained in subjective ratings of speech quality from metadata and the distribution imbalances of the dataset. Speech quality models were constructed using wav2vec 2.0 with additional metadata features that included rater groups and system identifiers and obtained competitive metrics including a Spearman rank correlation coefficient (SRCC) of 0.934 and MSE of 0.088 at the system-level, and 0.877 and 0.198 at the utterance-level. Using data and metadata that the test restricted or blinded further improved the metrics. A metadata analysis showed that the system-level metrics do not represent the model's system-level prediction as a result of the wide variation in the number of utterances used for each system on the validation and test datasets. We conclude that, in general, conditions should have enough utterances in the test set to bound the sample mean error, and be relatively balanced in utterance count between systems, otherwise the utterance-level metrics may be more reliable and interpretable."
   ],
   "doi": "10.21437/Interspeech.2022-799"
  },
  "kawahara22_interspeech": {
   "authors": [
    [
     "Hideki",
     "Kawahara"
    ],
    [
     "Kohei",
     "Yatabe"
    ],
    [
     "Ken-Ichi",
     "Sakakibara"
    ],
    [
     "Tatsuya",
     "Kitamura"
    ],
    [
     "Hideki",
     "Banno"
    ],
    [
     "Masanori",
     "Morise"
    ]
   ],
   "title": "An objective test tool for pitch extractors' response attributes",
   "original": "800",
   "page_count": 5,
   "order": 133,
   "p1": 659,
   "pn": 663,
   "abstract": [
    "We propose an objective measurement method for pitch extractors' responses to frequency-modulated signals. It enables us to evaluate different pitch extractors with unified criteria. The method uses extended time-stretched pulses combined by binary orthogonal sequences. It provides simultaneous measurement results consisting of the linear and the non-linear time-invariant responses and random and time-varying responses. We tested representative pitch extractors using fundamental frequencies spanning 80~Hz to 800~Hz with 1/48 octave steps and produced more than 2000 modulation frequency response plots. We found that making scientific visualization by animating these plots enables us to understand different pitch extractors' behavior at once. Such efficient and effortless inspection is impossible by inspecting all individual plots. The proposed measurement method with visualization leads to further improvement of the performance of one of the extractors mentioned above. In other words, our procedure turns the specific pitch extractor into the best reliable measuring equipment that is crucial for scientific research. We open-sourced MATLAB codes of the proposed objective measurement method and visualization procedure."
   ],
   "doi": "10.21437/Interspeech.2022-800"
  },
  "lepage22_interspeech": {
   "authors": [
    [
     "Theo",
     "Lepage"
    ],
    [
     "Reda",
     "Dehak"
    ]
   ],
   "title": "Label-Efficient Self-Supervised Speaker Verification With Information Maximization and Contrastive Learning",
   "original": "802",
   "page_count": 5,
   "order": 815,
   "p1": 4018,
   "pn": 4022,
   "abstract": [
    "State-of-the-art speaker verification systems are inherently dependent on some kind of human supervision as they are trained on massive amounts of labeled data. However, manually annotating utterances is slow, expensive and not scalable to the amount of data available today. In this study, we explore self-supervised learning for speaker verification by learning representations directly from raw audio. The objective is to produce robust speaker embeddings that have small intra-speaker and large inter-speaker variance. Our approach is based on recent information maximization learning frameworks and an intensive data augmentation pre-processing step. We evaluate the ability of these methods to work without contrastive samples before showing that they achieve better performance when combined with a contrastive loss. Furthermore, we conduct experiments to show that our method reaches competitive results compared to existing techniques and can get better performances compared to a supervised baseline when fine-tuned with a small portion of labeled data."
   ],
   "doi": "10.21437/Interspeech.2022-802"
  },
  "jia22_interspeech": {
   "authors": [
    [
     "Junteng",
     "Jia"
    ],
    [
     "Jay",
     "Mahadeokar"
    ],
    [
     "Weiyi",
     "Zheng"
    ],
    [
     "Yuan",
     "Shangguan"
    ],
    [
     "Ozlem",
     "Kalinli"
    ],
    [
     "Frank",
     "Seide"
    ]
   ],
   "title": "Federated Domain Adaptation for ASR with Full Self-Supervision",
   "original": "803",
   "page_count": 5,
   "order": 108,
   "p1": 536,
   "pn": 540,
   "abstract": [
    "Cross-device federated learning (FL) protects user privacy by collaboratively training a model on user devices, therefore eliminating the need for collecting, storing, and manually labeling user data. Previous works have considered cross-device FL for automatic speech recognition (ASR), however, there are a few important challenges that havenot been fully addressed. These include the lack of ground-truth ASR transcriptions, and the scarcity of compute resource and network bandwidth on edge devices. In this paper, we address these two challenges. First, we propose a federated learning system to support on-device ASR adaptation with full self-supervision, which uses self-labeling together with data augmentation and filtering techniques. The proposed system can improve a strong Emformer-Transducer based ASR model pretrained on out-of-domain data, using in-domain audios without any ground-truth transcriptions. Second, to reduce the training cost, we propose a self-restricted RNN Transducer (SR-RNN-T) loss, a new variant of alignment-restricted RNN-T that uses Viterbi forced-alignment from self-supervision. To further reduce the compute and network cost, we systematically explore adapting only a subset of weights in the Emformer-Transducer. Our best training recipe achieves a 12.9% relative WER reduction over the strong out-of-domain baseline, which equals 70% of the reduction achievable with full human supervision and centralized training."
   ],
   "doi": "10.21437/Interspeech.2022-803"
  },
  "jabeen22_interspeech": {
   "authors": [
    [
     "Farhat",
     "Jabeen"
    ],
    [
     "Simon",
     "Betz"
    ]
   ],
   "title": "Hesitations in Urdu/Hindi: Distribution and Properties of Fillers & Silences",
   "original": "805",
   "page_count": 5,
   "order": 910,
   "p1": 4491,
   "pn": 4495,
   "abstract": [
    "This research presents an analysis of hesitations in Urdu/Hindi semi-spontaneous dialogues. We annotated and analyzed twenty-five minutes of speech to investigate the frequency of hesitations and the properties of fillers as well as the formants in fillers' vocalic intervals to determine their vowel quality. We found that our participants used fillers, silences, and prolongations with varying frequency. Moreover, Urdu/Hindi speakers used the fillers with only vocalic intervals (uh) more frequently than the ones with vocalic intervals followed by nasals (um). The regression analysis showed that the um_type fillers were significantly longer and followed by longer silences as compared with the uh_type fillers. Furthermore, the um_types were placed more frequently at the turn medial position, whereas the uh_type fillers occurred at turn initial or medial position with similar frequency. The analysis of their formants showed that the vocalic intervals used in the fillers differed from other vowels in the inventory of Urdu/Hindi. Our data confirms the existing claim that uh and um are two distinct types of fillers. Our results are relevant for developing speech synthesis systems for Urdu/Hindi as well as improving the existing models seeking to incorporate hesitations and fillers in a realistic manner."
   ],
   "doi": "10.21437/Interspeech.2022-805"
  },
  "zhang22m_interspeech": {
   "authors": [
    [
     "Hao",
     "Zhang"
    ],
    [
     "Ashutosh",
     "Pandey"
    ],
    [
     "DeLiang",
     "Wang"
    ]
   ],
   "title": "Attentive Recurrent Network for Low-Latency Active Noise Control",
   "original": "811",
   "page_count": 5,
   "order": 195,
   "p1": 956,
   "pn": 960,
   "abstract": [
    "Processing latency is a critical issue for active noise control (ANC) due to the causality constraint of ANC systems. This paper addresses low-latency ANC in the deep learning framework (i.e. deep ANC). A time-domain method using an attentive recurrent network is employed to perform deep ANC with smaller frame sizes, thus reducing algorithmic latency of deep ANC. In addition, a delay-compensated training strategy is introduced to perform ANC using predicted noise for several milliseconds. Moreover, we utilize a revised overlap-add method during signal resynthesis to avoid the latency introduced due to overlaps between neighboring time frames. Experimental results show that the proposed strategies are effective for achieving low-latency deep ANC. Combining the proposed strategies is capable of yielding zero, even negative, algorithmic latency without significantly affecting ANC performance."
   ],
   "doi": "10.21437/Interspeech.2022-811"
  },
  "flemotomos22_interspeech": {
   "authors": [
    [
     "Nikolaos",
     "Flemotomos"
    ],
    [
     "Shrikanth",
     "Narayanan"
    ]
   ],
   "title": "Multimodal Clustering with Role Induced Constraints for Speaker Diarization",
   "original": "814",
   "page_count": 5,
   "order": 1027,
   "p1": 5075,
   "pn": 5079,
   "abstract": [
    "Speaker clustering is an essential step in conventional speaker diarization systems and is typically addressed as an audio-only speech processing task. The language used by the participants in a conversation, however, carries additional information that can help improve the clustering performance. This is especially true in conversational interactions, such as business meetings, interviews, and lectures, where specific roles assumed by interlocutors (manager, client, teacher, etc.) are often associated with distinguishable linguistic patterns. In this paper we propose to employ a supervised text-based model to extract speaker roles and then use this information to guide an audio-based spectral clustering step by imposing must-link and cannot-link constraints between segments. The proposed method is applied on two different domains, namely on medical interactions and on podcast episodes, and is shown to yield improved results when compared to the audio-only approach."
   ],
   "doi": "10.21437/Interspeech.2022-814"
  },
  "ni22_interspeech": {
   "authors": [
    [
     "Junrui",
     "Ni"
    ],
    [
     "Liming",
     "Wang"
    ],
    [
     "Heting",
     "Gao"
    ],
    [
     "Kaizhi",
     "Qian"
    ],
    [
     "Yang",
     "Zhang"
    ],
    [
     "Shiyu",
     "Chang"
    ],
    [
     "Mark",
     "Hasegawa-Johnson"
    ]
   ],
   "title": "Unsupervised Text-to-Speech Synthesis by Unsupervised Automatic Speech Recognition",
   "original": "816",
   "page_count": 5,
   "order": 93,
   "p1": 461,
   "pn": 465,
   "abstract": [
    "An unsupervised text-to-speech synthesis (TTS) system learns to generate speech waveforms corresponding to any written sentence in a language by observing: 1) a collection of untranscribed speech waveforms in that language; 2) a collection of texts written in that language without access to any transcribed speech. Developing such a system can significantly improve the availability of speech technology to languages without a large amount of parallel speech and text data. This paper proposes an unsupervised TTS system based on an alignment module that outputs pseudo-text and another synthesis module that uses pseudo-text for training and real text for inference. Our unsupervised system can achieve comparable performance to the supervised system in seven languages with about 10-20 hours of speech each. A careful study on the effect of text units and vocoders has also been conducted to better understand what factors may affect unsupervised TTS performance. The samples generated by our models can be found at https://cactuswiththoughts.github.io/UnsupTTS-Demo, and our code can be found at https://github.com/lwang114/UnsupTTS."
   ],
   "doi": "10.21437/Interspeech.2022-816"
  },
  "rugayan22_interspeech": {
   "authors": [
    [
     "Janine",
     "Rugayan"
    ],
    [
     "Torbjørn",
     "Svendsen"
    ],
    [
     "Giampiero",
     "Salvi"
    ]
   ],
   "title": "Semantically Meaningful Metrics for Norwegian ASR Systems",
   "original": "817",
   "page_count": 5,
   "order": 465,
   "p1": 2283,
   "pn": 2287,
   "abstract": [
    "Evaluation metrics are important for quanitfying the performance of Automatic Speech Recognition (ASR) systems. However, the widely used word error rate (WER) captures errors at the word-level only and weighs each error equally, which makes it insufficient to discern ASR system performance for downstream tasks such as Natural Language Understanding (NLU) or information retrieval. We explore in this paper a more robust and discriminative evaluation metric for Norwegian ASR systems through the use of semantic information modeled by a transformer-based language model. We propose Aligned Semantic Distance (ASD) which employs dynamic programming to quantify the similarity between the reference and hypothesis text. First, embedding vectors are generated using the NorBERT model. Afterwards, the minimum global distance of the optimal alignment between these vectors is obtained and normalized by the sequence length of the reference embedding vector. In addition, we present results using Semantic Distance (SemDist), and compare them with ASD. Results show that for the same WER, ASD and SemDist values can vary significantly, thus, exemplifying that not all recognition errors can be considered equally important. We investigate the resulting data, and present examples which demonstrate the nuances of both metrics in evaluating various transcription errors."
   ],
   "doi": "10.21437/Interspeech.2022-817"
  },
  "kopuklu22_interspeech": {
   "authors": [
    [
     "Okan",
     "Köpüklü"
    ],
    [
     "Maja",
     "Taseska"
    ]
   ],
   "title": "ResectNet: An Efficient Architecture for Voice Activity Detection on Mobile Devices",
   "original": "820",
   "page_count": 5,
   "order": 1087,
   "p1": 5363,
   "pn": 5367,
   "abstract": [
    "We present ResectNet, a RESource Efficient and CompacT Convolutional Recurrent Neural Network architecture for Voice Activity Detection (VAD) on mobile devices, which achieves state-of-the-art performance with less than 12k parameters. ResectNet operates on raw audio signals and consists of sinc convolutions, depthwise convolutions, grouped pointwise convolutions, frequency shift module and a gated recurrent unit. We propose a simple width-multiplier hyperparameter, which allows scaling ResectNet for the desired trade-off between efficiency and performance. We present a detailed ablation study on resource and performance trade-offs on the VAD task."
   ],
   "doi": "10.21437/Interspeech.2022-820"
  },
  "cui22b_interspeech": {
   "authors": [
    [
     "Xiaodong",
     "Cui"
    ],
    [
     "George",
     "Saon"
    ],
    [
     "Tohru",
     "Nagano"
    ],
    [
     "Masayuki",
     "Suzuki"
    ],
    [
     "Takashi",
     "Fukuda"
    ],
    [
     "Brian",
     "Kingsbury"
    ],
    [
     "Gakuto",
     "Kurata"
    ]
   ],
   "title": "Improving Generalization of Deep Neural Network Acoustic Models with Length Perturbation and N-best Based Label Smoothing",
   "original": "821",
   "page_count": 5,
   "order": 536,
   "p1": 2638,
   "pn": 2642,
   "abstract": [
    "We introduce two techniques, length perturbation and n-best based label smoothing, to improve generalization of deep neural network (DNN) acoustic models for automatic speech recognition (ASR). Length perturbation is a data augmentation algorithm that randomly drops and inserts frames of an utterance to alter the length of the speech feature sequence. N-best based label smoothing randomly injects noise to ground truth labels during training in order to avoid overfitting, where the noisy labels are generated from n-best hypotheses. We evaluate these two techniques extensively on the 300-hour Switchboard (SWB300) dataset and an in-house 500-hour Japanese (JPN500) dataset using recurrent neural network transducer (RNNT) acoustic models for ASR. We show that both techniques improve the generalization of RNNT models individually and they can also be complementary. In particular, they yield good improvements over a strong SWB300 baseline and give state-of-art performance on SWB300 using RNNT models."
   ],
   "doi": "10.21437/Interspeech.2022-821"
  },
  "dingliwal22_interspeech": {
   "authors": [
    [
     "Saket",
     "Dingliwal"
    ],
    [
     "Ashish",
     "Shenoy"
    ],
    [
     "Sravan",
     "Bodapati"
    ],
    [
     "Ankur",
     "Gandhe"
    ],
    [
     "Ravi Teja",
     "Gadde"
    ],
    [
     "Katrin",
     "Kirchhoff"
    ]
   ],
   "title": "Domain Prompts: Towards memory and compute efficient domain adaptation of ASR systems",
   "original": "824",
   "page_count": 5,
   "order": 138,
   "p1": 684,
   "pn": 688,
   "abstract": [
    "Automatic Speech Recognition (ASR) systems have found their use in numerous industrial applications in very diverse domains creating a need to adapt to new domains with small memory and deployment overhead. In this work, we introduce domain-prompts, a methodology that involves training a small number of domain embedding parameters to prime a Transformer-based Language Model (LM) to a particular domain. Using this domain-adapted LM for rescoring ASR hypotheses can achieve 7-13% WER reduction for a new domain with just 1000 unlabeled textual domain-specific sentences. This improvement is comparable or even better than fully fine-tuned models even though just 0.02% of the parameters of the base LM are updated. Additionally, our method is deployment-friendly as the learnt domain embeddings are prefixed to the input to the model rather than changing the base model architecture. Therefore, our method is an ideal choice for on-the-fly adaptation of LMs used in ASR systems to progressively scale it to new domains."
   ],
   "doi": "10.21437/Interspeech.2022-824"
  },
  "raitio22_interspeech": {
   "authors": [
    [
     "Tuomo",
     "Raitio"
    ],
    [
     "Petko",
     "Petkov"
    ],
    [
     "Jiangchuan",
     "Li"
    ],
    [
     "Muhammed",
     "Shifas"
    ],
    [
     "Andrea",
     "Davis"
    ],
    [
     "Yannis",
     "Stylianou"
    ]
   ],
   "title": "Vocal effort modeling in neural TTS for improving the intelligibility of synthetic speech in noise",
   "original": "825",
   "page_count": 5,
   "order": 392,
   "p1": 1936,
   "pn": 1940,
   "abstract": [
    "We present a neural text-to-speech (TTS) method that models natural vocal effort variation to improve the intelligibility of synthetic speech in the presence of noise. The method consists of first measuring the spectral tilt of unlabeled conventional speech data, and then conditioning a neural TTS model with normalized spectral tilt among other prosodic factors. Changing the spectral tilt parameter and keeping other prosodic factors unchanged enables effective vocal effort control at synthesis time independent of other prosodic factors. By extrapolation of the spectral tilt values beyond what has been seen in the original data, we can generate speech with high vocal effort levels, thus improving the intelligibility of speech in the presence of masking noise. We evaluate the intelligibility and quality of normal speech and speech with increased vocal effort in the presence of various masking noise conditions, and compare these to well-known speech intelligibility-enhancing algorithms. The evaluations show that the proposed method can improve the intelligibility of synthetic speech with little loss in speech quality."
   ],
   "doi": "10.21437/Interspeech.2022-825"
  },
  "zhou22c_interspeech": {
   "authors": [
    [
     "Wei",
     "Zhou"
    ],
    [
     "Wilfried",
     "Michel"
    ],
    [
     "Ralf",
     "Schlüter"
    ],
    [
     "Hermann",
     "Ney"
    ]
   ],
   "title": "Efficient Training of Neural Transducer for Speech Recognition",
   "original": "829",
   "page_count": 5,
   "order": 420,
   "p1": 2058,
   "pn": 2062,
   "abstract": [
    "As one of the most popular sequence-to-sequence modeling approaches for speech recognition, the RNN-Transducer has achieved evolving performance with more and more sophisticated neural network models of growing size and increasing training epochs. While strong computation resources seem to be the prerequisite of training superior models, we try to overcome it by carefully designing a more efficient training pipeline. In this work, we propose an efficient 3-stage progressive training pipeline to build highly-performing neural transducer models from scratch with very limited computation resources in a reasonable short time period. The effectiveness of each stage is experimentally verified on both Librispeech and Switchboard corpora. The proposed pipeline is able to train transducer models approaching state-of-the-art performance with a single GPU in just 2-3 weeks. Our best conformer transducer achieves 4.1% WER on Librispeech test-other with only 35 epochs of training."
   ],
   "doi": "10.21437/Interspeech.2022-829"
  },
  "peng22_interspeech": {
   "authors": [
    [
     "Yukun",
     "Peng"
    ],
    [
     "Zhenhua",
     "Ling"
    ]
   ],
   "title": "Decoupled Pronunciation and Prosody Modeling in Meta-Learning-based Multilingual Speech Synthesis",
   "original": "831",
   "page_count": 5,
   "order": 863,
   "p1": 4257,
   "pn": 4261,
   "abstract": [
    "This paper presents a method of decoupled pronunciation and prosody modeling to improve the performance of meta-learning-based multilingual speech synthesis. The baseline meta-learning synthesis method adopts a single text encoder with a parameter generator conditioned on language embeddings and a single decoder to predict mel-spectrograms for all languages. In contrast, our proposed method designs a two-stream model structure that contains two encoders and two decoders for pronunciation and prosody modeling, respectively, considering that the pronunciation knowledge and the prosody knowledge should be shared in different ways among languages. In our experiments, our proposed method effectively improved the intelligibility and naturalness of multilingual speech synthesis comparing with the baseline meta-learning synthesis method."
   ],
   "doi": "10.21437/Interspeech.2022-831"
  },
  "zhang22n_interspeech": {
   "authors": [
    [
     "Yuanyuan",
     "Zhang"
    ],
    [
     "Yixuan",
     "Zhang"
    ],
    [
     "Bence",
     "Halpern"
    ],
    [
     "Tanvina",
     "Patel"
    ],
    [
     "Odette",
     "Scharenborg"
    ]
   ],
   "title": "Mitigating bias against non-native accents",
   "original": "836",
   "page_count": 5,
   "order": 642,
   "p1": 3168,
   "pn": 3172,
   "abstract": [
    "Automatic Speech Recognition (ASR) systems have seen substantial improvements in the past decade; however, not for all speaker groups. Recent research shows that bias exists against different types of speech, including non-native accents, in state-of-the-art (SOTA) ASR systems. To attain inclusive speech recognition, i.e., ASR for everyone irrespective of how one speaks or the accent one has, bias mitigation is necessary. Here we focus on bias mitigation against non-native accents using two different approaches: data augmentation and by using more effective training methods. We used an autoencoder-based cross-lingual voice conversion (VC) model to increase the amount of non-native accented speech training data in addition to data augmentation through speed perturbation. Moreover, we investigate two training methods, i.e., fine-tuning and Domain Adversarial Training (DAT), to see whether they can use the limited non-native accented speech data more effectively than a standard training approach. Experimental results show that VC-based data augmentation successfully mitigates the bias against non-native accents for the SOTA end-to-end (E2E) Dutch ASR system. Combining VC and speed perturbed data gave the lowest Word Error Rate and the smallest bias against non-native accents. Fine-tuning and DAT reduced the bias against non-native accents but at the cost of native performance."
   ],
   "doi": "10.21437/Interspeech.2022-836"
  },
  "lonergan22_interspeech": {
   "authors": [
    [
     "Liam",
     "Lonergan"
    ],
    [
     "Mengjie",
     "Qian"
    ],
    [
     "Neasa Ní",
     "Chiaráin"
    ],
    [
     "Christer",
     "Gobl"
    ],
    [
     "Ailbhe Ní",
     "Chasaide"
    ]
   ],
   "title": "Cross-dialect lexicon optimisation for an endangered language ASR system: the case of Irish",
   "original": "838",
   "page_count": 5,
   "order": 985,
   "p1": 4865,
   "pn": 4869,
   "abstract": [
    "Lexicon optimisation strategies, addressing the problem of dialect divergence, are tested in an ASR system for Irish. As in many endangered languages, Irish has no spoken standard, but rather, three very different dialects of Ulster (Ul), Connaught (Co) and Munster (Mu). Furthermore, the complex sound system and ancient, opaque writing system result in sound-to-grapheme mappings that differ considerably across dialects. A hybrid ASR system was trained on (predominantly) native speaker speech data, balanced across the dialects. Experiment 1 tested whether a Global lexicon, which captures dialect variant forms with relatively abstract representations, can perform as well as a Multi-dialect lexicon containing all dialect variants. Three dialect-specific lexicons were also included in the tests. The Global lexicon did yield the best performance and experiment 2 tested whether further reductions to its phoneset might further enhance its performance. These included (i) merging a Tense-Lax contrast among coronal sonorants, not common to all dialects, and (ii) merging the contrast of voiceless-voiced sonorants, as the voiceless member is relatively infrequent. Results showed but a slight enhancement and only for Mu dialect, which is the one most aligned to the phoneset reduction."
   ],
   "doi": "10.21437/Interspeech.2022-838"
  },
  "heo22b_interspeech": {
   "authors": [
    [
     "Seong-Hwan",
     "Heo"
    ],
    [
     "WonKee",
     "Lee"
    ],
    [
     "Jong-Hyeok",
     "Lee"
    ]
   ],
   "title": "mcBERT: Momentum Contrastive Learning with BERT for Zero-Shot Slot Filling",
   "original": "839",
   "page_count": 5,
   "order": 253,
   "p1": 1243,
   "pn": 1247,
   "abstract": [
    "Zero-shot slot filling has received considerable attention to cope with the problem of limited available data for the target domain. One of the important factors in zero-shot learning is to make the model learn generalized and reliable representations. For this purpose, we present mcBERT, which stands for 'm'omentum 'c'ontrastive learning with BERT, to develop a robust zero-shot slot filling model. mcBERT uses BERT to initialize the two encoders, the query encoder and key encoder, and is trained by applying momentum contrastive learning. Our experimental results on the SNIPS benchmark show that mcBERT substantially outperforms the previous models, recording a new state-of-the-art. Besides, we also show that each component composing mcBERT contributes to the performance improvement."
   ],
   "doi": "10.21437/Interspeech.2022-839"
  },
  "koppelmann22_interspeech": {
   "authors": [
    [
     "Timm",
     "Koppelmann"
    ],
    [
     "Luca",
     "Becker"
    ],
    [
     "Alexandru",
     "Nelus"
    ],
    [
     "Rene",
     "Glitza"
    ],
    [
     "Lea",
     "Schönherr"
    ],
    [
     "Rainer",
     "Martin"
    ]
   ],
   "title": "Clustering-based Wake Word Detection in Privacy-aware Acoustic Sensor Networks",
   "original": "842",
   "page_count": 5,
   "order": 145,
   "p1": 719,
   "pn": 723,
   "abstract": [
    "This work investigates privacy-aware collaborative wake word detection (WWD) in acoustic sensor networks. To meet state-of-the-art privacy constraints, the proposed WWD scheme is based on privacy-aware unsupervised clustered federated learning that groups microphone nodes w.r.t. active sound sources and on a privacy-preserving high-level feature representation. Using the partition of microphone nodes into clusters, we apply intra- and inter-cluster feature enhancement strategies directly in the privacy-preserving feature domain and thus circumvent the need for communicating privacy-sensitive information between nodes. The approach is demonstrated for an acoustic sensor network deployed in a smart-home environment. We show that the proposed collaborative WWD system clearly outperforms independent decisions of individual microphone nodes. Index Terms: privacy, wake word detection, clustering, federated learning, unsupervised clustered federated learning"
   ],
   "doi": "10.21437/Interspeech.2022-842"
  },
  "daoudi22_interspeech": {
   "authors": [
    [
     "Khalid",
     "Daoudi"
    ],
    [
     "Biswajit",
     "Das"
    ],
    [
     "Solange",
     "Milhé de Saint Victor"
    ],
    [
     "Alexandra",
     "Foubert-Samier"
    ],
    [
     "Margherita",
     "Fabbri"
    ],
    [
     "Anne",
     "Pavy-Le Traon"
    ],
    [
     "Olivier",
     "Rascol"
    ],
    [
     "Virginie",
     "Woisard"
    ],
    [
     "Wassilios G.",
     "Meissner"
    ]
   ],
   "title": "A comparative study on vowel articulation in Parkinson's disease and multiple system atrophy",
   "original": "845",
   "page_count": 5,
   "order": 458,
   "p1": 2248,
   "pn": 2252,
   "abstract": [
    "Acoustic realisation of the working vowel space has been widely studied in Parkinson's disease (PD). However, it has never been studied in atypical parkinsonian disorders (APD). The latter are neurodegenerative diseases which share similar clinical features with PD, rendering the differential diagnosis very challenging in early disease stages. This paper presents the first contribution in vowel space analysis in APD, by comparing corner vowel realisation in PD and the parkinsonian variant of Multiple System Atrophy (MSA-P). Our study has the particularity of focusing exclusively on early stage PD and MSA-P patients, as our main purpose was early differential diagnosis between these two diseases. We analysed the corner vowels, extracted from a spoken sentence, using traditional vowel space metrics. We found no statistical difference between the PD group and healthy controls (HC) while MSA-P exhibited significant differences with the PD and HC groups. We also found that some metrics conveyed complementary discriminative information. Consequently, we argue that restriction in the acoustic realisation of corner vowels cannot be a viable early marker of PD, as hypothesised by some studies, but it might be a candidate as an early hypokinetic marker of MSA-P (when the clinical target is discrimination between PD and MSA-P)."
   ],
   "doi": "10.21437/Interspeech.2022-845"
  },
  "bergler22_interspeech": {
   "authors": [
    [
     "Christian",
     "Bergler"
    ],
    [
     "Alexander",
     "Barnhill"
    ],
    [
     "Dominik",
     "Perrin"
    ],
    [
     "Manuel",
     "Schmitt"
    ],
    [
     "Andreas",
     "Maier"
    ],
    [
     "Elmar",
     "Nöth"
    ]
   ],
   "title": "ORCA-WHISPER: An Automatic Killer Whale Sound Type Generation Toolkit Using Deep Learning",
   "original": "846",
   "page_count": 5,
   "order": 491,
   "p1": 2413,
   "pn": 2417,
   "abstract": [
    "Even today, the current understanding and interpretation of animal-specific vocalization paradigms is largely based on historical and manual data analysis considering comparatively small data corpora, primarily because of time- and human-resource limitations, next to the scarcity of available species-related machine-learning techniques. Partial human-based data inspections neither represent the overall real-world vocal repertoire, nor the variations within intra- and inter animal-specific call type portfolios, typically resulting only in small collections of category-specific ground truth data. Modern machine (deep) learning concepts are an essential requirement to identify statistically significant animal-related vocalization patterns within massive bioacoustic data archives. However, the applicability of pure supervised training approaches is challenging, due to limited call-specific ground truth data, combined with strong class-imbalances between individual call type events. The current study is the first presenting a deep bioacoustic signal generation framework, entitled ORCA-WHISPER, a Generative Adversarial Network (GAN), trained on low-resource killer whale (Orcinus Orca) call type data. Besides audiovisual inspection, supervised call type classification, and model transferability, the auspicious quality of generated fake vocalizations was further demonstrated by visualizing, representing, and enhancing the real-world orca signal data manifold. Moreover, previous orca/noise segmentation results were outperformed by integrating fake signals to the original data partition."
   ],
   "doi": "10.21437/Interspeech.2022-846"
  },
  "perezramon22_interspeech": {
   "authors": [
    [
     "Rubén",
     "Pérez Ramón"
    ],
    [
     "Martin",
     "Cooke"
    ],
    [
     "Maria Luisa",
     "Garcia Lecumberri"
    ]
   ],
   "title": "Generating iso-accented stimuli for second language research: methodology and a dataset for Spanish-accented English",
   "original": "850",
   "page_count": 5,
   "order": 374,
   "p1": 1846,
   "pn": 1850,
   "abstract": [
    "A non-native accent can be conveyed at both the segmental and suprasegmental level. Previous studies have developed techniques to isolate the effect of segmental foreign accent by splicing accented segments from a bilingual speaker into non-accented words produced by the same speaker. The current work addresses the issue of between-segment variability by developing a technique to convert from acoustically-equal accent gradations to perceptually-equal steps. The procedure is used to derive the first corpus of Spanish-accented English composed of lexical tokens each generated with one of five degrees of non-native accent. As an example application, corpus tokens are used to elicit accentedness judgements from four listener cohorts with first languages which differ as to whether they share the native language, the non-native (accented) language of the corpus or have a closer phonological inventory to one or the other. Findings highlight the importance of the relationship between listeners' phonological systems and those of the native and non-native languages of the corpus, especially for vowels, with respect to sensitivity to foreign accent."
   ],
   "doi": "10.21437/Interspeech.2022-850"
  },
  "yang22i_interspeech": {
   "authors": [
    [
     "Yuhong",
     "Yang"
    ],
    [
     "Xufeng",
     "Chen"
    ],
    [
     "Qingmu",
     "Liu"
    ],
    [
     "Weiping",
     "Tu"
    ],
    [
     "Hongyang",
     "Chen"
    ],
    [
     "Linjun",
     "Cai"
    ]
   ],
   "title": "Mandarin Lombard Grid: a Lombard-grid-like corpus of Standard Chinese",
   "original": "854",
   "page_count": 5,
   "order": 624,
   "p1": 3078,
   "pn": 3082,
   "abstract": [
    "The Lombard effect is natural, whereby speakers automatically adjust the vocal effort to facilitate speech understanding in noise. Since real-world applications are generally involved in noisy environments, the Lombard effect of highly variable speech features due to changing background noise is one of those challenges to match these real scenarios. Existing Lombard corpora show variations in the background noise level, ranging from 35 to 96 dB sound pressure level (SPL). However, it remains unclear if we need to collect all SPLs to build a comprehensive Lombard corpus. And most existing Lombard corpora are built for English; however, Mandarin and English are different in pronunciation. This paper describes our effort to build the first open-source Lombard corpus of standard Chinese, the Mandarin Lombard Grid. The effort involves three steps: (1) Classify Mandarin Lombard styles according to different background noise levels. (2) Create the corpus containing each style. (3) Analyze Mandarin Lombard effects showing their differences from English. We found three critical Lombard styles ranging from 30 dB to 85 dB-SPL and built the corpus containing the three Lombard styles and one reference plain style. Lombard effect analyses on this corpus showed consistency and some differences from the English Lombard Grid corpus."
   ],
   "doi": "10.21437/Interspeech.2022-854"
  },
  "ding22_interspeech": {
   "authors": [
    [
     "Shaojin",
     "Ding"
    ],
    [
     "Rajeev",
     "Rikhye"
    ],
    [
     "Qiao",
     "Liang"
    ],
    [
     "Yanzhang",
     "He"
    ],
    [
     "Quan",
     "Wang"
    ],
    [
     "Arun",
     "Narayanan"
    ],
    [
     "Tom",
     "O’Malley"
    ],
    [
     "Ian",
     "McGraw"
    ]
   ],
   "title": "Personal VAD 2.0: Optimizing Personal Voice Activity Detection for On-Device Speech Recognition",
   "original": "856",
   "page_count": 5,
   "order": 759,
   "p1": 3744,
   "pn": 3748,
   "abstract": [
    "Personalization of on-device speech recognition (ASR) has seen explosive growth in recent years, largely due to the increasing popularity of personal assistant features on mobile devices and smart home speakers. In this work, we present Personal VAD 2.0, a personalized voice activity detector that detects the voice activity of a target speaker, as part of a streaming on-device ASR system. Although previous proof-of-concept studies have validated the effectiveness of Personal VAD, there are still several critical challenges to address before this model can be used in production: first, the quality must be satisfactory in both enrollment and enrollment-less scenarios; second, it should operate in a streaming fashion; and finally, the model size should be small enough to fit a limited latency and CPU/Memory budget. To meet the multi-faceted requirements, we propose a series of novel designs: 1) advanced speaker embedding modulation methods; 2) a new training paradigm to generalize to enrollment-less conditions; 3) architecture and runtime optimizations for latency and resource restrictions. Extensive experiments on a realistic speech recognition system demonstrated the state-of-the-art performance of our proposed method."
   ],
   "doi": "10.21437/Interspeech.2022-856"
  },
  "zhang22o_interspeech": {
   "authors": [
    [
     "Zhenglin",
     "Zhang"
    ],
    [
     "Li-Zhuang",
     "Yang"
    ],
    [
     "Xun",
     "Wang"
    ],
    [
     "Hai",
     "Li"
    ]
   ],
   "title": "Automated Detection of Wilson’s Disease Based on Improved Mel-frequency Cepstral Coefficients with Signal Decomposition",
   "original": "859",
   "page_count": 5,
   "order": 437,
   "p1": 2143,
   "pn": 2147,
   "abstract": [
    "Wilson's disease (WD), a rare genetic movement disorder, is characterized by early-onset dysarthria. Automated speech assessment is thus valuable in early diagnosis and intervention. Time-frequency features, such as Mel-frequency cepstral coefficients (MFCC), have been frequently used. However, human speech signals are nonlinear and nonstationary, which cannot be captured by traditional features based on the Fourier transform. Moreover, the dysarthria type of WD patients is complex and different from other movement disorders such as Parkinson's disease. Thus, developing sensitive time-frequency measures for WD patients is needed. The present study proposes DMFCC, the improved MFCC using signal decomposition. We validate the usefulness of DMFCC in WD detection with a sample of 60 WD patients and 60 matched healthy controls. Results show that the DMFCC achieves the best classification accuracy (86.1%), improving by 13.9%-44.4% compared to baseline features such as MFCC and the state-of-art Hilbert cepstral coefficients (HCCs). The present study is a first attempt to demonstrate the validity of automated acoustic measures in WD detection, and the proposed DMFCC provides a novel tool for speech assessment."
   ],
   "doi": "10.21437/Interspeech.2022-859"
  },
  "reddy22_interspeech": {
   "authors": [
    [
     "Chandan",
     "Reddy"
    ],
    [
     "Vishak",
     "Gopal"
    ],
    [
     "Harishchandra",
     "Dubey"
    ],
    [
     "Ross",
     "Cutler"
    ],
    [
     "Sergiy",
     "Matusevych"
    ],
    [
     "Robert",
     "Aichner"
    ]
   ],
   "title": "MusicNet: Compact Convolutional Neural Network for Real-time Background Music Detection",
   "original": "864",
   "page_count": 5,
   "order": 844,
   "p1": 4162,
   "pn": 4166,
   "abstract": [
    "With the recent growth of remote work, online meetings often encounter challenging audio contexts such as background noise, music, and echo. Accurate real-time detection of music events can help to improve the user experience. In this paper, we present MusicNet, a compact neural model for detecting background music in the real-time communications pipeline. In video meetings, music frequently co-occurs with speech and background noises, making the accurate classification quite challenging. We propose a compact convolutional neural network core preceded by an in-model featurization layer. Music- Net takes 9 seconds of raw audio as input and does not require any model-specific featurization in the product stack. We train our model on the balanced subset of the Audio Set [1] data and validate it on 1000 crowd-sourced real test clips. Finally, we compare MusicNet performance with 20 state-of-the-art models. MusicNet has a true positive rate (TPR) of 81.3% at a 0.1% false-positive rate (FPR), which is significantly better than state-of-the-art models included in our study. MusicNet is also 10x smaller and has 4x faster inference than the best-performing models we benchmarked."
   ],
   "doi": "10.21437/Interspeech.2022-864"
  },
  "kim22g_interspeech": {
   "authors": [
    [
     "Min-Kyung",
     "Kim"
    ],
    [
     "Joon-Hyuk",
     "Chang"
    ]
   ],
   "title": "Adversarial and Sequential Training for Cross-lingual Prosody Transfer TTS",
   "original": "865",
   "page_count": 5,
   "order": 923,
   "p1": 4556,
   "pn": 4560,
   "abstract": [
    "This study presents a method for improving the performance of the text-to-speech (TTS) model by using three global speech-style representations: language, speaker, and prosody. Synthesizing different languages and prosody in the speaker's voice regardless of their own language and prosody is possible. To construct the embedding of each representation conditioned in the TTS model such that it is independent of the other representations, we propose an adversarial training method for the general architecture of TTS models. Furthermore, we introduce a sequential training method that includes rehearsal-based continual learning to train complex and small amounts of data without forgetting previously learned information. The experimental results show that the proposed method can generate good-quality speech and yield high similarity for speakers and prosody, even for representations that the speaker in the dataset does not contain."
   ],
   "doi": "10.21437/Interspeech.2022-865"
  },
  "gu22_interspeech": {
   "authors": [
    [
     "Bin",
     "Gu"
    ]
   ],
   "title": "Deep speaker embedding with frame-constrained training strategy for speaker verification",
   "original": "867",
   "page_count": 5,
   "order": 295,
   "p1": 1451,
   "pn": 1455,
   "abstract": [
    "Speech signals contain a lot of side information (content,stress, etc.), besides the voiceprint statistics. These sessionvariablilites pose a huge challenge for modeling speaker characteristics. To alleviate this problem, we propose a novel frame-constrained training (FCT) strategy in this paper. It enhances the speaker information in frame-level layers for better embedding extraction. More precisely, a similarity matrix is calculated based on the frame-level features among each batch of the training samples, and a FCT loss is obtained through this similarity matrix. Finally, the speaker embedding network is trained by the combination of the FCT loss and the speaker classification loss. Experiments are performed on the VoxCeleb1 and VOiCES databases. The results demonstrate that the proposed training strategy boosts the system performance."
   ],
   "doi": "10.21437/Interspeech.2022-867"
  },
  "huang22e_interspeech": {
   "authors": [
    [
     "Jen-Hung",
     "Huang"
    ],
    [
     "Chung-Hsien",
     "Wu"
    ]
   ],
   "title": "Memory-Efficient Multi-Step Speech Enhancement with Neural ODE",
   "original": "868",
   "page_count": 5,
   "order": 196,
   "p1": 961,
   "pn": 965,
   "abstract": [
    "Although deep learning-based models proposed in the past years have achieved remarkable results on the speech enhancement tasks, the existing multi-step denoising methods require a memory size proportional to the number of steps during training, which makes it difficult to apply to large models. In this paper, we propose a memory-efficient multi-step speech enhancement method that requires only constant amount of memory for model training. This End-to-End method combines Neural Ordinary Differential Equations (Neural ODEs) with the Memory-efficient Asynchronous Leapfrog Integrator (MALI) for multi-step training. Experiments on the Voice Bank and DEMAND datasets showed that the multi-step method using MALI had better performance than the single-step method, with maximum improvements of 0.16 on PESQ and 0.5% on STOI. In addition to reducing the memory required for model training, this method is also quite competitive with the current state-of-the-art methods."
   ],
   "doi": "10.21437/Interspeech.2022-868"
  },
  "zhang22p_interspeech": {
   "authors": [
    [
     "Zhan",
     "Zhang"
    ],
    [
     "Yuehai",
     "Wang"
    ],
    [
     "Jianyi",
     "Yang"
    ]
   ],
   "title": "End-to-end Mispronunciation Detection with Simulated Error Distance",
   "original": "870",
   "page_count": 5,
   "order": 877,
   "p1": 4327,
   "pn": 4331,
   "abstract": [
    "With the development of deep learning, the performance of the mispronunciation detection model has improved greatly. However, the annotation for mispronunciation is quite expensive as it requires the experts to carefully judge the error for each pronounced phoneme. As a result, the supervised end-to-end mispronunciation detection model faces the problem of data shortage. Although the text-based data augmentation can partially alleviate this problem, we analyze that it only simulates the categorical phoneme error. Such a simulation is inefficient for the real situation. In this paper, we propose a novel unit-based data augmentation method. Our method converts the continuous audio signal into the robust audio vector and then into the discrete unit sequence. By modifying this unit sequence, we generate a more reasonable mispronunciation and can get the vector distance as the error indicator. By training on such simulated data, the experiments on L2Arctic show that our method can improve the performance of the mispronunciation detection task compared with the text-based method."
   ],
   "doi": "10.21437/Interspeech.2022-870"
  },
  "pham22_interspeech": {
   "authors": [
    [
     "Ngoc-Quan",
     "Pham"
    ],
    [
     "Alexander",
     "Waibel"
    ],
    [
     "Jan",
     "Niehues"
    ]
   ],
   "title": "Adaptive multilingual speech recognition with pretrained models",
   "original": "872",
   "page_count": 5,
   "order": 786,
   "p1": 3879,
   "pn": 3883,
   "abstract": [
    "Multilingual speech recognition with supervised learning has achieved great results as reflected in recent research. With the development of pretraining methods on audio and text data, it is imperative to transfer the knowledge from unsupervised multilingual models to facilitate recognition, especially in many languages with limited data. Our work investigated the effectiveness of using two pretrained models for two modalities: wav2vec 2.0 for audio and MBART50 for text, together with the adaptive weight techniques to massively improve the recognition quality on the public datasets containing CommonVoice and Europarl. Overall, we noticed an 44% improvement over purely supervised learning, and more importantly, each technique provides a different reinforcement in different languages. We also explore other possibilities to potentially obtain the best model by slightly adding either depth or relative attention to the architecture."
   ],
   "doi": "10.21437/Interspeech.2022-872"
  },
  "zhen22_interspeech": {
   "authors": [
    [
     "Kai",
     "Zhen"
    ],
    [
     "Hieu Duy",
     "Nguyen"
    ],
    [
     "Raviteja",
     "Chinta"
    ],
    [
     "Nathan",
     "Susanj"
    ],
    [
     "Athanasios",
     "Mouchtaris"
    ],
    [
     "Tariq",
     "Afzal"
    ],
    [
     "Ariya",
     "Rastrow"
    ]
   ],
   "title": "Sub-8-Bit Quantization Aware Training for 8-Bit Neural Network Accelerator with On-Device Speech Recognition",
   "original": "874",
   "page_count": 5,
   "order": 615,
   "p1": 3033,
   "pn": 3037,
   "abstract": [
    "We present a novel sub-8-bit quantization-aware training (S8BQAT) scheme for 8-bit neural network accelerators. Our method is inspired from Lloyd-Max compression theory with practical adaptations for a feasible computational overhead during training. With the quantization centroids derived from a 32-bit baseline, we augment training loss with a Multi-Regional Absolute Cosine (MRACos) regularizer that aggregates weights towards their nearest centroid, effectively acting as a pseudo compressor. Additionally, a periodically invoked hard compressor is introduced to improve the convergence rate by emulating runtime model weight quantization. We apply S8BQAT on speech recognition tasks using Recurrent Neural Network-Transducer (RNN-T) architecture. With S8BQAT, we are able to increase the model parameter size to reduce the word error rate by 4-16% relatively, while still improving latency by 5%."
   ],
   "doi": "10.21437/Interspeech.2022-874"
  },
  "li22j_interspeech": {
   "authors": [
    [
     "Chengfei",
     "Li"
    ],
    [
     "Shuhao",
     "Deng"
    ],
    [
     "Yaoping",
     "Wang"
    ],
    [
     "Guangjing",
     "Wang"
    ],
    [
     "Yaguang",
     "Gong"
    ],
    [
     "Changbin",
     "Chen"
    ],
    [
     "Jinfeng",
     "Bai"
    ]
   ],
   "title": "TALCS: An open-source Mandarin-English code-switching corpus and a speech recognition baseline",
   "original": "877",
   "page_count": 5,
   "order": 353,
   "p1": 1741,
   "pn": 1745,
   "abstract": [
    "This paper introduces a new corpus of Mandarin-English code-switching speech recognition—TALCS corpus, suitable for training and evaluating code-switching speech recognition systems. TALCS corpus is derived from real online one-to-one English teaching scenes in TAL education group, which contains roughly 587 hours of speech sampled at 16 kHz. To our best knowledge, TALCS corpus is the largest well labeled Mandarin-English code-switching open source automatic speech recognition (ASR) dataset in the world. In this paper, we will introduce the recording procedure in detail, including audio capturing devices and corpus environments. And the TALCS corpus is freely available for download under the permissive license[ https://ai.100tal.com/dataset]. Using TALCS corpus, we conduct ASR experiments in two popular speech recognition toolkits to make a baseline system, including ESPnet and Wenet. The Mixture Error Rate (MER) performance in the two speech recognition toolkits is compared in TALCS corpus. The experimental results implies that the quality of audio recordings and transcriptions are promising and the baseline system is workable."
   ],
   "doi": "10.21437/Interspeech.2022-877"
  },
  "zhang22q_interspeech": {
   "authors": [
    [
     "Zhan",
     "Zhang"
    ],
    [
     "Yuehai",
     "Wang"
    ],
    [
     "Jianyi",
     "Yang"
    ]
   ],
   "title": "BiCAPT: Bidirectional Computer-Assisted Pronunciation Training with Normalizing Flows",
   "original": "878",
   "page_count": 5,
   "order": 878,
   "p1": 4332,
   "pn": 4336,
   "abstract": [
    "Computer-Assisted Pronunciation Training (CAPT) plays an important role in language learning. So far, most existing CAPT methods are discriminative and focus on detecting where the mispronunciation is. Although learners receive feedback about their current pronunciation, they may still not be able to learn the correct pronunciation. Nevertheless, there has been little discussion about speech-based teaching in CAPT. To fill this gap, we propose a novel bidirectional CAPT method to detect mispronunciations and generate the corrected pronunciations simultaneously. This correction-based feedback can better preserve the speaking style to make the learning process more personalized. In addition, we propose to adopt normalizing flows to share the latent for these two mirrored discriminative-generative tasks, making the whole model more compact. Experiments show that our method is efficient for mispronunciation detection and can naturally correct the speech under different CAPT granularity requirements."
   ],
   "doi": "10.21437/Interspeech.2022-878"
  },
  "kumar22_interspeech": {
   "authors": [
    [
     "Anoop",
     "Kumar"
    ],
    [
     "Pankaj Kumar",
     "Sharma"
    ],
    [
     "Aravind",
     "Illa"
    ],
    [
     "Sriram",
     "Venkatapathy"
    ],
    [
     "Subhrangshu",
     "Nandi"
    ],
    [
     "Pritam",
     "Varma"
    ],
    [
     "Anurag",
     "Dwarakanath"
    ],
    [
     "Aram",
     "Galstyan"
    ]
   ],
   "title": "Learning Under Label Noise for Robust Spoken Language Understanding systems",
   "original": "880",
   "page_count": 5,
   "order": 701,
   "p1": 3463,
   "pn": 3467,
   "abstract": [
    "Most real-world datasets contain inherent label noise, which typically leads to memorization and overfitting when training over-parameterized deep neural networks (DNNs) on such data. While memorization in DNNs has been studied extensively in computer vision literature, the impact of noisy labels and var- ious mitigation strategies in spoken language understanding tasks is largely under-explored. In this paper, we perform a systematic study on the effectiveness of five noise-mitigation methods in spoken language text classification tasks. First, we experiment on three publicly available datasets by synthetically injecting noise into the labels, and evaluate the effectiveness of various methods at different noise intensity. We then evaluate those methods on a real-word data coming from the large-scale industrial Spoken Language Understanding system. Our results show that most methods are effective mitigating the impact of the noise, with two of those methods showing consistently bet- ter results. For the industrial Spoken Language Understand- ing systems, the best performing methods recover 65% (1.99% accuracy recovery out of 3.07%) of the underfitting caused by noise overall. In one class it recovers 97% underfitting (3.74% out of 3.86%)."
   ],
   "doi": "10.21437/Interspeech.2022-880"
  },
  "afshan22_interspeech": {
   "authors": [
    [
     "Amber",
     "Afshan"
    ],
    [
     "Abeer",
     "Alwan"
    ]
   ],
   "title": "Attention-based conditioning methods using variable frame rate for style-robust speaker verification",
   "original": "882",
   "page_count": 5,
   "order": 475,
   "p1": 2333,
   "pn": 2337,
   "abstract": [
    "We propose an approach to extract speaker embeddings that are robust to speaking style variations in text-independent speaker verification. Typically, speaker embedding extraction includes training a DNN for speaker classification and using the bottleneck features as speaker representations. Such a network has a pooling layer to transform frame-level to utterance-level features by calculating statistics over all utterance frames, with equal weighting. However, self-attentive embeddings perform weighted pooling such that the weights correspond to the importance of the frames in a speaker classification task. Entropy can capture acoustic variability due to speaking style variations. Hence, an entropy-based variable frame rate vector is proposed as an external conditioning vector for the self-attention layer to provide the network with information that can address style effects. This work explores five different approaches to conditioning. The best conditioning approach, concatenation with gating, provided statistically significant improvements over the x-vector baseline in 12/23 tasks and was the same as the baseline in 11/23 tasks when using the UCLA speaker variability database. It also significantly outperformed self-attention without conditioning in 9/23 tasks and was worse in 1/23. The method also showed significant improvements in multi-speaker scenarios of SITW."
   ],
   "doi": "10.21437/Interspeech.2022-882"
  },
  "afshan22b_interspeech": {
   "authors": [
    [
     "Amber",
     "Afshan"
    ],
    [
     "Abeer",
     "Alwan"
    ]
   ],
   "title": "Learning from human perception to improve automatic speaker verification in style-mismatched conditions",
   "original": "883",
   "page_count": 5,
   "order": 476,
   "p1": 2338,
   "pn": 2342,
   "abstract": [
    "Our prior experiments show that humans and machines seem to employ different approaches to speaker discrimination, especially in the presence of speaking style variability. The experiments examined read versus conversational speech. Listeners focused on speaker-specific idiosyncrasies while \"telling speakers together\", and on relative distances in a shared acoustic space when \"telling speakers apart\". However, automatic speaker verification (ASV) systems use the same loss function irrespective of target or non-target trials. To improve ASV performance in the presence of style variability, insights learnt from human perception are used to design a new training loss function that we refer to as \"CllrCE loss\". CllrCE loss uses both speaker-specific idiosyncrasies and relative acoustic distances between speakers to train the ASV system. When using the UCLA speaker variability database, in the x-vector and conditioning setups, CllrCE loss. results in significant relative improvements in EER by 1-66%, and minDCF by 1-31% and 1-56%, respectively, when compared to the x-vector baseline. Using the SITW evaluation tasks, which involve different conversational speech tasks, the proposed loss combined with self-attention conditioning results in significant relative improvements in EER by 2-5% and minDCF by 6-12% over baseline. In the SITW case, performance improvements were consistent only with conditioning."
   ],
   "doi": "10.21437/Interspeech.2022-883"
  },
  "shi22c_interspeech": {
   "authors": [
    [
     "Bowen",
     "Shi"
    ],
    [
     "Abdelrahman",
     "Mohamed"
    ],
    [
     "Wei-Ning",
     "Hsu"
    ]
   ],
   "title": "Learning Lip-Based Audio-Visual Speaker Embeddings with AV-HuBERT",
   "original": "885",
   "page_count": 5,
   "order": 969,
   "p1": 4785,
   "pn": 4789,
   "abstract": [
    "This paper investigates self-supervised pre-training for audio-visual speaker representation learning where a visual stream showing the speaker's mouth area is used alongside speech as inputs. Our study focuses on the Audio-Visual Hidden Unit BERT (AV-HuBERT) approach, a recently developed general-purpose audio-visual speech pre-training framework. We conducted extensive experiments probing the effectiveness of pre-training and visual modality. Experimental results suggest that AV-HuBERT generalizes decently to speaker-related downstream tasks, improving label efficiency by roughly ten fold for both audio-only and audio-visual speaker verification. We also show that incorporating visual information, even just the lip area, greatly improves the performance and noise robustness, reducing EER by 38% in the clean condition and 75% in noisy conditions."
   ],
   "doi": "10.21437/Interspeech.2022-885"
  },
  "tian22b_interspeech": {
   "authors": [
    [
     "Yusheng",
     "Tian"
    ],
    [
     "Jingyu",
     "Li"
    ],
    [
     "Tan",
     "Lee"
    ]
   ],
   "title": "Transport-Oriented Feature Aggregation for Speaker Embedding Learning",
   "original": "886",
   "page_count": 5,
   "order": 64,
   "p1": 316,
   "pn": 320,
   "abstract": [
    "Pooling is needed to aggregate frame-level features into utterance-level representations for speaker modeling. Given the success of statistics-based pooling methods, we hypothesize that speaker characteristics are well represented in the statistical distribution over the pre-aggregation layer's output, and propose to use transport-oriented feature aggregation for deriving speaker embeddings. The aggregated representation encodes the geometric structure of the underlying feature distribution, which is expected to contain valuable speaker-specific information that may not be represented by the commonly used statistical measures like mean and variance. The original transport-oriented feature aggregation is also extended to a weighted-frame version to incorporate the attention mechanism. Experiments on speaker verification with the Voxceleb dataset show improvement over statistics pooling and its attentive variant."
   ],
   "doi": "10.21437/Interspeech.2022-886"
  },
  "yi22_interspeech": {
   "authors": [
    [
     "Yuan-Hao",
     "Yi"
    ],
    [
     "Lei",
     "He"
    ],
    [
     "Shifeng",
     "Pan"
    ],
    [
     "Xi",
     "Wang"
    ],
    [
     "Yuchao",
     "Zhang"
    ]
   ],
   "title": "SoftSpeech: Unsupervised Duration Model in FastSpeech 2",
   "original": "887",
   "page_count": 5,
   "order": 326,
   "p1": 1606,
   "pn": 1610,
   "abstract": [
    "In this paper, we propose a neural Text-To-Speech (TTS) system SoftSpeech, which employs a novel soft length regulated duration attention based decoder. It learns the encoder output mapping to decoder output simultaneously from an unsupervised duration model (Soft-LengthRegulator) without the requirement of external duration information. The Soft-LengthRegulator consists of a Feed-Forward Transformer (FFT) block with Conditional Layer Normalization (CLN), following a learned upsampling layer with multi-head attention and guided multi-head attention constraint, and it is integrated in each decoder layer and achieves accelerated training convergence and better naturalness within FastSpeech 2 framework. Soft Dynamic Time Warping (Soft-DTW) is adopted to align the mismatch spectrogram loss. Moreover, a Fine-Grained style Variational AutoEncoder (VAE) is designed to further improve the naturalness of synthesized speech. The experiments show SoftSpeech outperforms FastSpeech 2 in subjective tests, and can be successfully applied to minority languages with low resources."
   ],
   "doi": "10.21437/Interspeech.2022-887"
  },
  "ahn22b_interspeech": {
   "authors": [
    [
     "Chung-Soo",
     "Ahn"
    ],
    [
     "Chamara",
     "Kasun"
    ],
    [
     "Sunil",
     "Sivadas"
    ],
    [
     "Jagath",
     "Rajapakse"
    ]
   ],
   "title": "Recurrent multi-head attention fusion network for combining audio and text for speech emotion recognition",
   "original": "888",
   "page_count": 5,
   "order": 150,
   "p1": 744,
   "pn": 748,
   "abstract": [
    "To infer emotions accurately from speech, fusion of audio and text is essential as words carry most information about semantics and emotions. Attention mechanism is essential component in multimodal fusion architecture as it dynamically pairs different regions within multimodal sequences. However, existing architecture lacks explicit structure to model dynamics between fused representations. Thus we propose recurrent multi-head attention in a fusion architecture, which selects salient fused representations and learns dynamics between them. Multiple 2-D attention layers select salient pairs among all possible pairs of audio and text representations, which are combined with fusion operation. Lastly, multiple fused representations are fed into recurrent unit to learn dynamics between fused representations. Our method outperforms existing approaches for fusion of audio and text for speech emotion recognition and achieves state-of-the-art accuracies on benchmark IEMOCAP dataset."
   ],
   "doi": "10.21437/Interspeech.2022-888"
  },
  "sang22_interspeech": {
   "authors": [
    [
     "Mufan",
     "Sang"
    ],
    [
     "John H.L.",
     "Hansen"
    ]
   ],
   "title": "Multi-Frequency Information Enhanced Channel Attention Module for Speaker Representation Learning",
   "original": "892",
   "page_count": 5,
   "order": 65,
   "p1": 321,
   "pn": 325,
   "abstract": [
    "Recently, attention mechanisms have been applied successfully in neural network-based speaker verification systems. Incorporating the Squeeze-and-Excitation block into convolutional neural networks has achieved remarkable performance. However, it uses global average pooling (GAP) to simply average the features along time and frequency dimensions, which is incapable of preserving sufficient speaker information in the feature maps. In this study, we show that GAP is a special case of a discrete cosine transform (DCT) on time-frequency domain mathematically using only the lowest frequency component in frequency decomposition. To strengthen the speaker information extraction ability, we propose to utilize multi-frequency information and design two novel and effective attention modules, called Single-Frequency Single-Channel (SFSC) attention module and Multi-Frequency Single-Channel (MFSC) attention module. The proposed attention modules can effectively capture more speaker information from multiple frequency components on the basis of DCT. We conduct comprehensive experiments on the VoxCeleb datasets and a probe evaluation on the 1st 48-UTD forensic corpus. Experimental results demonstrate that our proposed SFSC and MFSC attention modules can efficiently generate more discriminative speaker representations and outperform ResNet34-SE and ECAPA-TDNN systems with relative 20.9% and 20.2% reduction in EER, without adding extra network parameters."
   ],
   "doi": "10.21437/Interspeech.2022-892"
  },
  "fu22b_interspeech": {
   "authors": [
    [
     "Kaiqi",
     "Fu"
    ],
    [
     "Shaojun",
     "Gao"
    ],
    [
     "Xiaohai",
     "Tian"
    ],
    [
     "Wei",
     "Li"
    ],
    [
     "MA",
     "Zejun"
    ]
   ],
   "title": "Using Fluency Representation Learned from Sequential Raw Features for Improving Non-native Fluency Scoring",
   "original": "896",
   "page_count": 5,
   "order": 879,
   "p1": 4337,
   "pn": 4341,
   "abstract": [
    "Automatic non-native fluency scoring is a challenging task which relies heavily on the effectiveness of the handcrafted fluency features used for predicting fluency scores. In this paper, we investigate the use of sequence model to automatically learn utterance-level fluency representation from phone-level raw sequential features. Specifically, the raw counterpart of traditional handcrafted features (e.g., GOP, speech rate, and speech break) are first collected at the phone-level, pre-processing net, BLSTM, and average pooling are then applied to a sequence of those features to get utterance-level fluency representation for final fluency scoring. Experimental results conducted on the non-native database suggest that the proposed framework outperforms the handcrafted feature based systems in terms of Pearson correlation coefficients (PCC). In addition, an ablation study is performed to better understand the improvements brought by different raw features and representation strategies used in our proposed fluency scorer."
   ],
   "doi": "10.21437/Interspeech.2022-896"
  },
  "wu22f_interspeech": {
   "authors": [
    [
     "Yihan",
     "Wu"
    ],
    [
     "Xu",
     "Tan"
    ],
    [
     "Bohan",
     "Li"
    ],
    [
     "Lei",
     "He"
    ],
    [
     "Sheng",
     "Zhao"
    ],
    [
     "Ruihua",
     "Song"
    ],
    [
     "Tao",
     "Qin"
    ],
    [
     "Tie-Yan",
     "Liu"
    ]
   ],
   "title": "AdaSpeech 4: Adaptive Text to Speech in Zero-Shot Scenarios",
   "original": "901",
   "page_count": 5,
   "order": 522,
   "p1": 2568,
   "pn": 2572,
   "abstract": [
    "Adaptive text to speech (TTS) can synthesize new voices in zero-shot scenarios efficiently, by using a well-trained source TTS model without adapting it on the speech data of new speakers. Considering seen and unseen speakers have diverse characteristics, zero-shot adaptive TTS requires strong generalization ability on speaker characteristics, which brings modeling challenges. In this paper, we develop AdaSpeech 4, a zero-shot adaptive TTS system for high-quality speech synthesis. We model the speaker characteristics systematically to improve the generalization on new speakers. Generally, the modeling of speaker characteristics can be categorized into three steps: extracting speaker representation, taking this speaker representation as condition, and synthesizing speech/mel-spectrogram given this speaker representation. Accordingly, we improve the modeling in three steps: 1) To extract speaker representation with better generalization, we factorize the speaker characteristics into basis vectors and extract speaker representation by weighted combining of these basis vectors through attention. 2) We leverage conditional layer normalization to integrate the extracted speaker representation to TTS model. 3) We propose a novel supervision loss based on the distribution of basis vectors to maintain the corresponding speaker characteristics in generated mel-spectrograms. Without any fine-tuning, AdaSpeech 4 achieves better voice quality and similarity than baselines in multiple datasets."
   ],
   "doi": "10.21437/Interspeech.2022-901"
  },
  "valin22_interspeech": {
   "authors": [
    [
     "Jean-Marc",
     "Valin"
    ],
    [
     "Ahmed",
     "Mustafa"
    ],
    [
     "Christopher",
     "Montgomery"
    ],
    [
     "Timothy B.",
     "Terriberry"
    ],
    [
     "Michael",
     "Klingbeil"
    ],
    [
     "Paris",
     "Smaragdis"
    ],
    [
     "Arvindh",
     "Krishnaswamy"
    ]
   ],
   "title": "Real-Time Packet Loss Concealment With Mixed Generative and Predictive Model",
   "original": "903",
   "page_count": 5,
   "order": 115,
   "p1": 570,
   "pn": 574,
   "abstract": [
    "As deep speech enhancement algorithms have recently demonstrated capabilities greatly surpassing their traditional counterparts for suppressing noise, reverberation and echo, attention is turning to the problem of packet loss concealment (PLC). PLC is a challenging task because it not only involves real-time speech synthesis, but also frequent transitions between the received audio and the synthesized concealment. We propose a hybrid neural PLC architecture where the missing speech is synthesized using a generative model conditioned using a predictive model. The resulting algorithm achieves natural concealment that surpasses the quality of existing conventional PLC algorithms and ranked second in the Interspeech 2022 PLC Challenge. We show that our solution not only works for uncompressed audio, but is also applicable to a modern speech codec."
   ],
   "doi": "10.21437/Interspeech.2022-903"
  },
  "hansen22_interspeech": {
   "authors": [
    [
     "John H.L.",
     "Hansen"
    ],
    [
     "ZHENYU",
     "WANG"
    ]
   ],
   "title": "Audio Anti-spoofing Using Simple Attention Module and Joint Optimization Based on Additive Angular Margin Loss and Meta-learning",
   "original": "904",
   "page_count": 5,
   "order": 76,
   "p1": 376,
   "pn": 380,
   "abstract": [
    "Automatic speaker verification systems are vulnerable to a variety of access threats, prompting research into the formulation of effective spoofing detection systems to act as a gate to filter out such spoofing attacks. This study introduces a simple attention module to infer 3-dim attention weights for the feature map in a convolutional layer, which then optimizes an energy function to determine each neuron's importance. With the advancement of both voice conversion and speech synthesis technologies, un-seen spoofing attacks are constantly emerging to limit spoofing detection system performance. Here, we propose a joint optimization approach based on the weighted additive angular margin loss for binary classification, with a meta-learning training framework to develop an efficient system that is robust to a wide range of spoofing attacks for model generalization en- enhancement. As a result, when compared to current state-of-the-art systems, our proposed approach delivers a competitive result with a pooled EER of 0.99% and min t-DCF of 0.0289."
   ],
   "doi": "10.21437/Interspeech.2022-904"
  },
  "hwang22_interspeech": {
   "authors": [
    [
     "Hyun Kyung",
     "Hwang"
    ],
    [
     "Manami",
     "Hirayama"
    ],
    [
     "Takaomi",
     "Kato"
    ]
   ],
   "title": "Perceived prominence and downstep in Japanese",
   "original": "908",
   "page_count": 4,
   "order": 268,
   "p1": 1318,
   "pn": 1321,
   "abstract": [
    "Perceived prominence as a function of fundamental frequency (f0) is examined with special attention given to downstep in Standard Japanese, a lowered f0 of the material following an accented word compared with that following an unaccented word. This lowering does not signal a reduction in prominence, whereas f0 compression triggered by focus is associated with prominence. The purpose of this study is to investigate how much f0 lowering is perceived as equally prominent to understand a perceptually acceptable pitch range of Japanese downstep. A prominence perception test was conducted with varying peak f0s of two successive phrases. Results of the test reveal that there is a particular f0 level of the following phrase to be perceived as equally prominent as the preceding phrase, regardless of the peak f0 of the preceding phrase. This result indicates that f0 differences between the preceding and the following phrases are greater as the f0 of the preceding phrase increases, corroborating the effect of f0 on prominence perception found in the literature. Furthermore, an interesting asymmetry is suggested between production and perception of downstep in Japanese."
   ],
   "doi": "10.21437/Interspeech.2022-908"
  },
  "zhu22c_interspeech": {
   "authors": [
    [
     "Han",
     "Zhu"
    ],
    [
     "Li",
     "Wang"
    ],
    [
     "Gaofeng",
     "Cheng"
    ],
    [
     "Jindong",
     "Wang"
    ],
    [
     "Pengyuan",
     "Zhang"
    ],
    [
     "Yonghong",
     "Yan"
    ]
   ],
   "title": "Wav2vec-S: Semi-Supervised Pre-Training for Low-Resource ASR",
   "original": "909",
   "page_count": 5,
   "order": 986,
   "p1": 4870,
   "pn": 4874,
   "abstract": [
    "Self-supervised pre-training could effectively improve the performance of low-resource automatic speech recognition (ASR). However, existing self-supervised pre-training are taskagnostic, i.e., could be applied to various downstream tasks. Although it enlarges the scope of its application, the capacity of the pre-trained model is not fully utilized for the ASR task, and the learned representations may not be optimal for ASR. In this work, in order to build a better pre-trained model for low-resource ASR, we propose a pre-training approach called wav2vec-S, where we use task-specific semi-supervised pretraining to refine the self-supervised pre-trained model for the ASR task thus more effectively utilize the capacity of the pretrained model to generate task-specific representations for ASR. Experiments show that compared to wav2vec 2.0, wav2vecS only requires a marginal increment of pre-training time but could significantly improve ASR performance on in-domain, cross-domain and cross-lingual datasets. Average relative WER reductions are 24.5% and 6.6% for 1h and 10h fine-tuning, respectively. Furthermore, we show that semi-supervised pretraining could close the representation gap between the selfsupervised pre-trained model and the corresponding fine-tuned model through canonical correlation analysis."
   ],
   "doi": "10.21437/Interspeech.2022-909"
  },
  "subramani22_interspeech": {
   "authors": [
    [
     "Krishna",
     "Subramani"
    ],
    [
     "Jean-Marc",
     "Valin"
    ],
    [
     "Umut",
     "Isik"
    ],
    [
     "Paris",
     "Smaragdis"
    ],
    [
     "Arvindh",
     "Krishnaswamy"
    ]
   ],
   "title": "End-to-end LPCNet: A Neural Vocoder With Fully-Differentiable LPC Estimation",
   "original": "912",
   "page_count": 5,
   "order": 165,
   "p1": 818,
   "pn": 822,
   "abstract": [
    "Neural vocoders have recently demonstrated high quality speech synthesis, but typically require a high computational complexity. LPCNet was proposed as a way to reduce the complexity of neural synthesis by using linear prediction (LP) to assist an autoregressive model. At inference time, LPCNet relies on the LP coefficients being explicitly computed from the input acoustic features. That makes the design of LPCNet-based systems more complicated, while adding the constraint that the input features must represent a clean speech spectrum. We propose an end-to-end version of LPCNet that lifts these limitations by learning to infer the LP coefficients from the input features in the frame rate network. Results show that the proposed end-to-end approach equals or exceeds the quality of the original LPCNet model, but without explicit LP analysis. Our open-source end-to-end model still benefits from LPCNet's low complexity, while allowing for any type of conditioning features."
   ],
   "doi": "10.21437/Interspeech.2022-912"
  },
  "cai22_interspeech": {
   "authors": [
    [
     "Linjun",
     "Cai"
    ],
    [
     "Yuhong",
     "Yang"
    ],
    [
     "Xufeng",
     "Chen"
    ],
    [
     "Weiping",
     "Tu"
    ],
    [
     "Hongyang",
     "Chen"
    ]
   ],
   "title": "CS-CTCSCONV1D: Small footprint speaker verification with channel split time-channel-time separable 1-dimensional convolution",
   "original": "913",
   "page_count": 5,
   "order": 66,
   "p1": 326,
   "pn": 330,
   "abstract": [
    "We present an efficient small-footprint network for speaker verification. We start by introducing the bottleneck to the QuartzNet model. Then we proposed a Channel Split Time Channel-Time Separable 1-dimensional Convolution (CS-CTCSConv1d) module, yielding stronger performance over the State-Of-The-Art small footprint speaker verification system. We apply knowledge distillation to further improve performance to learn better speaker embedding from the large model. We evaluate the proposed approach on Voxceleb dataset, obtaining better performances concerning the baseline method. The proposed model takes only 238.9K parameters to outperform the baseline system by 10% relatively in equal error rate (EER)."
   ],
   "doi": "10.21437/Interspeech.2022-913"
  },
  "fan22_interspeech": {
   "authors": [
    [
     "Zhiyun",
     "Fan"
    ],
    [
     "Zhenlin",
     "Liang"
    ],
    [
     "Linhao",
     "Dong"
    ],
    [
     "Yi",
     "Liu"
    ],
    [
     "Shiyu",
     "Zhou"
    ],
    [
     "Meng",
     "Cai"
    ],
    [
     "Jun",
     "Zhang"
    ],
    [
     "Zejun",
     "Ma"
    ],
    [
     "Bo",
     "Xu"
    ]
   ],
   "title": "Token-level Speaker Change Detection Using Speaker Difference and Speech Content via Continuous Integrate-and-fire",
   "original": "914",
   "page_count": 5,
   "order": 760,
   "p1": 3749,
   "pn": 3753,
   "abstract": [
    "In multi-talker scenarios such as meetings and conversations, speech processing systems are usually required to segment the audio and then transcribe each segmentation. These two stages are addressed separately by speaker change detection (SCD) and automatic speech recognition (ASR). Most previous SCD systems rely solely on speaker information and ignore the importance of speech content. In this paper, we propose a novel SCD system that considers both cues of speaker difference and speech content. These two cues are converted into token-level representations by the continuous integrate-and-fire (CIF) mechanism and then combined for detecting speaker changes on the token acoustic boundaries. We evaluate the performance of our approach on a public real-recorded meeting dataset, AISHELL-4. The experiment results show that our method outperforms a competitive frame-level baseline system by 2.45% equal coverage-purity (ECP). In addition, we demonstrate the importance of speech content and speaker difference to the SCD task, and the advantages of conducting SCD on the token acoustic boundaries compared with conducting SCD frame by frame."
   ],
   "doi": "10.21437/Interspeech.2022-914"
  },
  "li22k_interspeech": {
   "authors": [
    [
     "Jin",
     "Li"
    ],
    [
     "Rongfeng",
     "Su"
    ],
    [
     "Xurong",
     "Xie"
    ],
    [
     "Lan",
     "Wang"
    ],
    [
     "Nan",
     "Yan"
    ]
   ],
   "title": "A Multi-level Acoustic Feature Extraction Framework for Transformer Based End-to-End Speech Recognition",
   "original": "915",
   "page_count": 5,
   "order": 643,
   "p1": 3173,
   "pn": 3177,
   "abstract": [
    "Transformer based end-to-end modeling approaches with multiple stream inputs have been achieved great success in various automatic speech recognition (ASR) tasks. An important issue associated with such approaches is that the intermediate features derived from each stream might have similar representations and thus it is lacking of feature diversity, such as the descriptions related to speaker characteristics. To address this issue, this paper proposed a novel multi-level acoustic feature extraction framework that can be easily combined with Transformer based ASR models. The framework consists of two input streams: a shallow stream with high-resolution spectrograms and a deep stream with low-resolution spectrograms. The shallow stream is used to acquire traditional shallow features that are beneficial for the classification of phones or words while the deep stream is used to obtain utterance-level speaker-invariant deep features for improving the feature diversity. A feature correlation based fusion strategy is used to aggregate both features across the frequency and time domains and then fed into the Transformer encoder-decoder module. By using the proposed multi-level acoustic feature extraction framework, state-of-the-art word error rate of 21.7% and 2.5% were obtained on the HKUST Mandarin telephone and Librispeech speech recognition tasks respectively."
   ],
   "doi": "10.21437/Interspeech.2022-915"
  },
  "gao22_interspeech": {
   "authors": [
    [
     "Yingying",
     "Gao"
    ],
    [
     "Junlan",
     "Feng"
    ],
    [
     "Chao",
     "Deng"
    ],
    [
     "Shilei",
     "Zhang"
    ]
   ],
   "title": "Meta Auxiliary Learning for Low-resource Spoken Language Understanding",
   "original": "916",
   "page_count": 5,
   "order": 549,
   "p1": 2703,
   "pn": 2707,
   "abstract": [
    "Spoken language understanding (SLU) treats automatic speech recognition (ASR) and natural language understanding (NLU) as a unified task and usually suffers from data scarcity. We exploit an ASR and NLU joint training method based on meta auxiliary learning to improve the performance of low-resource SLU task by only taking advantage of abundant manual transcriptions of speech data. One obvious advantage of such method is that it provides a flexible framework to implement a low-resource SLU training task without requiring access to any further semantic annotations. In particular, a NLU model is taken as label generation network to predict intent and slot tags from texts; a multi-task network trains ASR task and SLU task synchronously from speech; and the predictions of label generation network are delivered to the multi-task network as semantic targets. The efficiency of the proposed algorithm is demonstrated with experiments on the public CATSLU dataset, which produces more suitable ASR hypotheses for the downstream NLU task."
   ],
   "doi": "10.21437/Interspeech.2022-916"
  },
  "wu22g_interspeech": {
   "authors": [
    [
     "Haibin",
     "Wu"
    ],
    [
     "Lingwei",
     "Meng"
    ],
    [
     "Jiawen",
     "Kang"
    ],
    [
     "Jinchao",
     "Li"
    ],
    [
     "Xu",
     "Li"
    ],
    [
     "Xixin",
     "Wu"
    ],
    [
     "Hung-yi",
     "Lee"
    ],
    [
     "Helen",
     "Meng"
    ]
   ],
   "title": "Spoofing-Aware Speaker Verification by Multi-Level Fusion",
   "original": "920",
   "page_count": 5,
   "order": 883,
   "p1": 4357,
   "pn": 4361,
   "abstract": [
    "Recently, many novel techniques have been introduced to deal with spoofing attacks, and achieve promising countermeasure (CM) performances. However, these works only take the stand-alone CM models into account. Nowadays, a spoofing aware speaker verification (SASV) challenge which aims to facilitate the research of integrated CM and ASV models, arguing that jointly optimizing CM and ASV models will lead to better performance, is taking place. In this paper, we propose a novel multi-model and multi-level fusion strategy to tackle the SASV task. Compared with purely scoring fusion and embedding fusion methods, this framework first utilizes embeddings from CM models, propagating CM embeddings into a CM block to obtain a CM score. In the second-level fusion, the CM score and ASV scores directly from ASV systems will be concatenated into a prediction block for the final decision. As a result, the best single fusion system has achieved the SASV-EER of 0.97% on the evaluation set. Then by ensembling the top-5 fusion systems, the final SASV-EER reached 0.89%."
   ],
   "doi": "10.21437/Interspeech.2022-920"
  },
  "kim22h_interspeech": {
   "authors": [
    [
     "Byeonggeun",
     "Kim"
    ],
    [
     "Seunghan",
     "Yang"
    ],
    [
     "Inseop",
     "Chung"
    ],
    [
     "Simyung",
     "Chang"
    ]
   ],
   "title": "Dummy Prototypical Networks for Few-Shot Open-Set Keyword Spotting",
   "original": "921",
   "page_count": 5,
   "order": 936,
   "p1": 4621,
   "pn": 4625,
   "abstract": [
    "Keyword spotting is the task of detecting a keyword in streaming audio. Conventional keyword spotting targets predefined keywords classification, but there is growing attention in few-shot (query-by-example) keyword spotting, e.g., N-way classification given M-shot support samples. Moreover, in real-world scenarios, there can be utterances from unexpected categories (open-set) which need to be rejected rather than classified as one of the N classes. Combining the two needs, we tackle few-shot open-set keyword spotting with a new benchmark setting, named splitGSC. We propose episode-known dummy prototypes based on metric learning to detect an open-set better and introduce a simple and powerful approach, Dummy Prototypical Networks (D-ProtoNets). Our D-ProtoNets shows clear margins compared to recent few-shot open-set recognition (FSOSR) approaches in the suggested splitGSC. We also verify our method on a standard benchmark, miniImageNet, and D-ProtoNets shows the state-of-the-art open-set detection rate in FSOSR."
   ],
   "doi": "10.21437/Interspeech.2022-921"
  },
  "tran22_interspeech": {
   "authors": [
    [
     "Tho Nguyen Duc",
     "Tran"
    ],
    [
     "The Chuong",
     "Chu"
    ],
    [
     "Vu",
     "Hoang"
    ],
    [
     "Trung Huu",
     "Bui"
    ],
    [
     "Hung Quoc",
     "Truong"
    ]
   ],
   "title": "An Efficient and High Fidelity Vietnamese Streaming End-to-End Speech Synthesis",
   "original": "922",
   "page_count": 5,
   "order": 94,
   "p1": 466,
   "pn": 470,
   "abstract": [
    "In recent years, parallel end-to-end speech synthesis systems have outperformed the 2-stage TTS approaches in audio quality and latency. A parallel end-to-end speech like VITS can generate the audio with high MOS comparable to ground truth and achieve low latency on GPU. However, the VITS still has high latency when synthesizing long utterances on CPUs. Therefore, in this paper, we propose a streaming method for the parallel speech synthesis model like VITS to synthesize with the long texts effectively on CPU. Our system has achieved human-like speech quality in both the non-streaming and streaming mode on the in-house Vietnamese evaluation set, while the synthesis speed of our system is approximately four times faster than that of the VITS in the non-streaming mode. Furthermore, the customer perceived latency of our system in streaming mode is 25 times faster than the VITS on computer CPU. Our system in non-streaming mode achieves a MOS of 4.43 compared to ground-truth with MOS 4.56; it also has high-quality speech with a MOS of 4.35 in streaming mode. Finally, we release a Vietnamese single accent dataset used in our experiments."
   ],
   "doi": "10.21437/Interspeech.2022-922"
  },
  "tian22c_interspeech": {
   "authors": [
    [
     "Jinchuan",
     "Tian"
    ],
    [
     "Jianwei",
     "Yu"
    ],
    [
     "Chunlei",
     "Zhang"
    ],
    [
     "Yuexian",
     "Zou"
    ],
    [
     "Dong",
     "Yu"
    ]
   ],
   "title": "LAE: Language-Aware Encoder for Monolingual and Multilingual ASR",
   "original": "923",
   "page_count": 5,
   "order": 644,
   "p1": 3178,
   "pn": 3182,
   "abstract": [
    "Despite the rapid progress in automatic speech recognition (ASR) research, recognizing multilingual speech using a unified ASR system remains highly challenging. Previous works on multilingual speech recognition mainly focus on two directions: recognizing multiple monolingual speech or recognizing code-switched speech that uses different languages interchangeably within a single utterance. However, a pragmatic multilingual recognizer is expected to be compatible with both directions. In this work, a novel language-aware encoder (LAE) architecture is proposed to handle both situations by disentangling language-specific information and generating frame-level language-aware representations during encoding. In the LAE, the primary encoding is implemented by the shared block while the language-specific blocks are used to extract specific representations for each language. To learn language-specific information discriminatively, a language-aware training method is proposed to optimize the language-specific blocks in LAE. Experiments conducted on Mandarin-English code-switched speech suggest that the proposed LAE is capable of discriminating different languages in frame-level and shows superior performance on both monolingual and multilingual ASR tasks. With either a real-recorded or simulated code-switched dataset, the proposed LAE achieves statistically significant improvements on both CTC and neural transducer systems. Code is released1."
   ],
   "doi": "10.21437/Interspeech.2022-923"
  },
  "ju22_interspeech": {
   "authors": [
    [
     "Yooncheol",
     "Ju"
    ],
    [
     "Ilhwan",
     "Kim"
    ],
    [
     "Hongsun",
     "Yang"
    ],
    [
     "Ji-Hoon",
     "Kim"
    ],
    [
     "Byeongyeol",
     "Kim"
    ],
    [
     "Soumi",
     "Maiti"
    ],
    [
     "Shinji",
     "Watanabe"
    ]
   ],
   "title": "TriniTTS: Pitch-controllable End-to-end TTS without External Aligner",
   "original": "925",
   "page_count": 5,
   "order": 4,
   "p1": 16,
   "pn": 20,
   "abstract": [
    "Three research directions that have recently advanced the text-to-speech (TTS) field are end-to-end architecture, prosody control modeling, and on-the-fly duration alignment of non-auto-regressive models. However, these three agendas have yet to be tackled at once in a single solution. Current studies are limited either by a lack of control over prosody modeling or by the inefficient training inherent in building a two-stage TTS pipeline. We propose TriniTTS, a pitch-controllable end-to-end TTS without an external aligner that generates natural speech by addressing the issues mentioned above at once. It eliminates the training inefficiency in the two-stage TTS pipeline by the end-to-end architecture. Moreover, it manages to learn the latent vector representing the data distribution of the speeches through performing tasks (alignment search, pitch estimation, waveform generation) simultaneously. Experimental results demonstrate that TriniTTS enables prosody modeling with user input parameters to generate deterministic speech, while synthesizing comparable speech to the state-of-the-art VITS. Furthermore, eliminating normalizing flow modules used in VITS increases the inference speed by 28.84% in CPU environment and by 29.16% in GPU environment."
   ],
   "doi": "10.21437/Interspeech.2022-925"
  },
  "li22l_interspeech": {
   "authors": [
    [
     "Pengqi",
     "Li"
    ],
    [
     "Lantian",
     "Li"
    ],
    [
     "Askar",
     "Hamdulla"
    ],
    [
     "Dong",
     "Wang"
    ]
   ],
   "title": "Reliable Visualization for Deep Speaker Recognition",
   "original": "926",
   "page_count": 5,
   "order": 67,
   "p1": 331,
   "pn": 335,
   "abstract": [
    "In spite of the impressive success of convolutional neural networks (CNNs) in speaker recognition, our understanding to CNNs' internal functions is still limited. A major obstacle is that some popular visualization tools are difficult to apply, for example those producing saliency maps. The reason is that speaker information does not show clear spatial patterns in the temporal-frequency space, which makes it hard to interpret the visualization results, and hence hard to confirm the reliability of a visualization tool. In this paper, we conduct an extensive analysis on three popular visualization methods based on class activation map(CAM): Grad-CAM, Score-CAM and Layer-CAM, to investigate their reliability for speaker recognition tasks. Experiments conducted on a state-of-the-art ResNet34SE model show that the Layer-CAM algorithm can produce reliable visualization, and thus can be used as a promising tool to explain CNN-based speaker models. The source code and examples are available in our project page: http://project.cslt.org/."
   ],
   "doi": "10.21437/Interspeech.2022-926"
  },
  "thithuuyen22_interspeech": {
   "authors": [
    [
     "Hoang",
     "Thi Thu Uyen"
    ],
    [
     "Nguyen Anh",
     "Tu"
    ],
    [
     "Ta",
     "Duc Huy"
    ]
   ],
   "title": "Vietnamese Capitalization and Punctuation Recovery Models",
   "original": "931",
   "page_count": 5,
   "order": 787,
   "p1": 3884,
   "pn": 3888,
   "abstract": [
    "Despite the rise of recent performant methods in Automatic Speech Recognition (ASR), such methods do not ensure proper casing and punctuation for their outputs. This problem has a significant impact on the comprehension of both Natural Language Processing (NLP) algorithms and human to process. Capitalization and punctuation restoration is imperative in pre-processing pipelines for raw textual inputs. For low resource languages like Vietnamese, public datasets for this task are scarce. In this paper, we contribute a public dataset for capitalization and punctuation recovery for Vietnamese; and propose a joint model for both tasks named JointCapPunc. Experimental results on the Vietnamese dataset show the effectiveness of our joint model compare to single model and previous joint learning model. We publicly release our dataset and the implementation of our model at https://github.com/anhtunguyen98/JointCapPunc"
   ],
   "doi": "10.21437/Interspeech.2022-931"
  },
  "deng22b_interspeech": {
   "authors": [
    [
     "Keqi",
     "Deng"
    ],
    [
     "Shinji",
     "Watanabe"
    ],
    [
     "Jiatong",
     "Shi"
    ],
    [
     "Siddhant",
     "Arora"
    ]
   ],
   "title": "Blockwise Streaming Transformer for Spoken Language Understanding and Simultaneous Speech Translation",
   "original": "933",
   "page_count": 5,
   "order": 354,
   "p1": 1746,
   "pn": 1750,
   "abstract": [
    "Although Transformers have gained success in several speech processing tasks like spoken language understanding (SLU) and speech translation (ST), achieving online processing while keeping competitive performance is still essential for real-world interaction. In this paper, we take the first step on streaming SLU and simultaneous ST using a blockwise streaming Transformer, which is based on contextual block processing and blockwise synchronous beam search. Furthermore, we design an automatic speech recognition (ASR)-based intermediate loss regularization for the streaming SLU task to improve the classification performance further. As for the simultaneous ST task, we propose a cross-lingual encoding method, which employs a CTC branch optimized with target language translations. In addition, the CTC translation output is also used to refine the search space with CTC prefix score, achieving joint CTC/attention simultaneous translation for the first time. Experiments for SLU are conducted on FSC and SLURP corpora, while the ST task is evaluated on Fisher-CallHome Spanish and MuST-C En-De corpora. Experimental results show that the blockwise streaming Transformer achieves competitive results compared to offline models, especially with our proposed methods that further yield a 2.4% accuracy gain on the SLU task and a 4.3 BLEU gain on the ST task over streaming baselines."
   ],
   "doi": "10.21437/Interspeech.2022-933"
  },
  "lee22h_interspeech": {
   "authors": [
    [
     "Jaeuk",
     "Lee"
    ],
    [
     "Joon-Hyuk",
     "Chang"
    ]
   ],
   "title": "One-Shot Speaker Adaptation Based on Initialization by Generative Adversarial Networks for TTS",
   "original": "934",
   "page_count": 5,
   "order": 604,
   "p1": 2978,
   "pn": 2982,
   "abstract": [
    "Speaker adaptation for personalizing text-to-speech (TTS) has become increasingly important. Herein, we propose a novel adaptation using a few seconds of data obtained from an unseen speaker. We first use a speaker embedding lookup table to train a multi-speaker TTS model, wherein each speaker embedding in the lookup table contains information representing a speaker's timbre. We propose an initial embedding predictor that extracts initial embedding suitable for the adaptation of unseen speakers. We use trained speaker embeddings to train the initial embedding predictor. Further, adversarial training is applied to improve the performance. After adversarial training, the initial embedding predictor infers the unseen speaker's initial embedding, and it is fine-tuned. As the initial embedding contains timbre information of the unseen speaker, adaptation is achieved faster and with less data than with conventional methods. We validate the performance with a mean opinion score (MOS) and demonstrate that adaptation is feasible with only 5 s of data."
   ],
   "doi": "10.21437/Interspeech.2022-934"
  },
  "ng22_interspeech": {
   "authors": [
    [
     "Si-Ioi",
     "Ng"
    ],
    [
     "Cymie Wing-Yee",
     "Ng"
    ],
    [
     "Jiarui",
     "Wang"
    ],
    [
     "Tan",
     "Lee"
    ]
   ],
   "title": "Automatic Detection of Speech Sound Disorder in Child Speech Using Posterior-based Speaker Representations",
   "original": "935",
   "page_count": 5,
   "order": 579,
   "p1": 2853,
   "pn": 2857,
   "abstract": [
    "This paper presents a macroscopic approach to automatic detection of speech sound disorder (SSD) in child speech. Typically, SSD is manifested by persistent articulation and phonological errors on specific phonemes in the language. The disorder can be detected by focally analyzing the phonemes or the words elicited by the child subject. In the present study, instead of attempting to detect individual phone- and word-level errors, we propose to extract a subject-level representation from a long utterance that is constructed by concatenating multiple test words. The speaker verification approach, and posterior features generated by deep neural network models, are applied to derive various types of holistic representations. A linear classifier is trained to differentiate disordered speech in normal one. On the task of detecting SSD in Cantonese-speaking children, experimental results show that the proposed approach achieves improved detection performance over previous method that requires fusing phone-level detection results. Using articulatory posterior features to derive i-vectors from multiple-word utterances achieves an unweighted average recall of 78.2% and a macro F1 score of 78.0%."
   ],
   "doi": "10.21437/Interspeech.2022-935"
  },
  "wang22m_interspeech": {
   "authors": [
    [
     "Chengyi",
     "Wang"
    ],
    [
     "Yiming",
     "Wang"
    ],
    [
     "Yu",
     "Wu"
    ],
    [
     "Sanyuan",
     "Chen"
    ],
    [
     "Jinyu",
     "Li"
    ],
    [
     "Shujie",
     "Liu"
    ],
    [
     "Furu",
     "Wei"
    ]
   ],
   "title": "Supervision-Guided Codebooks for Masked Prediction in Speech Pre-training",
   "original": "936",
   "page_count": 5,
   "order": 537,
   "p1": 2643,
   "pn": 2647,
   "abstract": [
    "Recently, masked prediction pre-training has seen remarkable progress in self-supervised learning (SSL) for speech recognition. It usually requires a codebook obtained in an unsupervised way, making it less accurate and difficult to interpret. We propose two supervision-guided codebook generation approaches to improve automatic speech recognition (ASR) performance and also the pre-training efficiency, either through decoding with a hybrid ASR system to generate phoneme-level alignments (named PBERT), or performing clustering on the supervised speech features extracted from an end-to-end CTC model (named CTC clustering). Both the hybrid and CTC models are trained on the same small amount of labeled speech as used in fine-tuning. Experiments demonstrate significant superiority of our methods to various SSL and self-training baselines, with up to 17.0\\% relative WER reduction. Our pre-trained models also show good transferability in a non-ASR speech task."
   ],
   "doi": "10.21437/Interspeech.2022-936"
  },
  "yoon22_interspeech": {
   "authors": [
    [
     "Jeewoo",
     "Yoon"
    ],
    [
     "Jinyoung",
     "Han"
    ],
    [
     "Erik",
     "Bucy"
    ],
    [
     "Jungseock",
     "Joo"
    ]
   ],
   "title": "Predicting Emotional Intensity in Political Debates via Non-verbal Signals",
   "original": "938",
   "page_count": 5,
   "order": 808,
   "p1": 3983,
   "pn": 3987,
   "abstract": [
    "Non-verbal expressions of politicians are important in election. In particular, the emotional intensity of politician revealed in a debate can be strongly linked to voters' evaluation. This paper proposes a multimodal deep-learning model for predicting the perceived emotional intensity of a candidate, which utilizes voice, face, and gesture to capture the comprehensive information of one's emotional intensity revealed in a debate. We collect a dataset of political debate videos from the 2020 Democratic presidential primaries in the USA, and train the proposed model with randomly sampled clips from the debate videos. By applying the proposed model to 23 candidates in 11 debate videos, we show that the standard deviation of the perceived emotional intensity is positively correlated with the changes in candidates' favorability in public polls."
   ],
   "doi": "10.21437/Interspeech.2022-938"
  },
  "zhuang22b_interspeech": {
   "authors": [
    [
     "Xiaobin",
     "Zhuang"
    ],
    [
     "Huiran",
     "Yu"
    ],
    [
     "Weifeng",
     "Zhao"
    ],
    [
     "Tao",
     "Jiang"
    ],
    [
     "Peng",
     "Hu"
    ]
   ],
   "title": "KaraTuner: Towards End-to-End Natural Pitch Correction for Singing Voice in Karaoke",
   "original": "939",
   "page_count": 5,
   "order": 864,
   "p1": 4262,
   "pn": 4266,
   "abstract": [
    "An automatic pitch correction system typically includes several stages, such as pitch extraction, deviation estimation, pitch shift processing, and cross-fade smoothing. However, designing these components with strategies often requires domain expertise and they are likely to fail on corner cases. In this paper, we present KaraTuner, an end-to-end neural architecture that predicts pitch curve and resynthesizes the singing voice directly from the tuned pitch and vocal spectrum extracted from the original recordings. Several vital technical points have been introduced in KaraTuner to ensure pitch accuracy, pitch naturalness, timbre consistency, and sound quality. A feed-forward Transformer is employed in the pitch predictor to capture long-term dependencies in the vocal spectrum and musical note. We also develop a pitch-controllable vocoder based on a novel source-filter block and the Fre-GAN architecture. KaraTuner obtains a higher preference than the rule-based pitch correction approach through A/B tests, and perceptual experiments show that the proposed vocoder achieves significant advantages in timbre consistency and sound quality compared with the parametric WORLD vocoder, phase vocoder and CLPC vocoder."
   ],
   "doi": "10.21437/Interspeech.2022-939"
  },
  "yang22j_interspeech": {
   "authors": [
    [
     "Seunghan",
     "Yang"
    ],
    [
     "Debasmit",
     "Das"
    ],
    [
     "Janghoon",
     "Cho"
    ],
    [
     "Hyoungwoo",
     "Park"
    ],
    [
     "Sungrack",
     "Yun"
    ]
   ],
   "title": "Domain Agnostic Few-shot Learning for Speaker Verification",
   "original": "940",
   "page_count": 5,
   "order": 120,
   "p1": 595,
   "pn": 599,
   "abstract": [
    "Deep learning models for verification systems often fail to generalize to new users and new environments, even though they learn highly discriminative features. To address this problem, we propose a few-shot domain generalization framework that learns to tackle distribution shift for new users and new domains. Our framework consists of domain-specific and domain-aggregation networks, which are the experts on specific and combined domains, respectively. By using these networks, we generate episodes that mimic the presence of both novel users and novel domains in the training phase to eventually produce better generalization. To save memory, we reduce the number of domain-specific networks by clustering similar domains together. Upon extensive evaluation on artificially generated noise domains, we can explicitly show generalization ability of our framework. In addition, we apply our proposed methods to the existing competitive architecture on the standard benchmark, which shows further performance improvements."
   ],
   "doi": "10.21437/Interspeech.2022-940"
  },
  "yang22k_interspeech": {
   "authors": [
    [
     "Longfei",
     "Yang"
    ],
    [
     "Wenqing",
     "Wei"
    ],
    [
     "Sheng",
     "Li"
    ],
    [
     "Jiyi",
     "Li"
    ],
    [
     "Takahiro",
     "Shinozaki"
    ]
   ],
   "title": "Augmented Adversarial Self-Supervised Learning for Early-Stage Alzheimer's Speech Detection",
   "original": "943",
   "page_count": 5,
   "order": 109,
   "p1": 541,
   "pn": 545,
   "abstract": [
    "The early-stage detection of Alzheimer's disease has been considered an important field of medical studies. While speech-based automatic detection methods have raised attention in the community, traditional machine learning methods suffer from data shortage because Alzheimer's record data is very difficult to get from medical institutions. To address this problem, this study proposes an augmented adversarial self-supervised learning method for Alzheimer's disease detection using limited speech data. In our approach, Alzheimer-like patterns are captured through an augmented adversarial self-supervised framework, which is trained in an adversarial manner using limited Alzheimer's data with a large scale of easily-collected normal speech data and an augmented set of Alzheimer's data. Experimental results show that our model can effectively handle the data sparsity problems and outperform the several baselines by a large margin. The performance for the ``AD\" class has been improved significantly, which is very important to actual AD detection applications."
   ],
   "doi": "10.21437/Interspeech.2022-943"
  },
  "chen22f_interspeech": {
   "authors": [
    [
     "Yifan",
     "Chen"
    ],
    [
     "Yifan",
     "Guo"
    ],
    [
     "Qingxuan",
     "Li"
    ],
    [
     "Gaofeng",
     "Cheng"
    ],
    [
     "Pengyuan",
     "Zhang"
    ],
    [
     "Yonghong",
     "Yan"
    ]
   ],
   "title": "Interrelate Training and Searching: A Unified Online Clustering Framework for Speaker Diarization",
   "original": "944",
   "page_count": 5,
   "order": 296,
   "p1": 1456,
   "pn": 1460,
   "abstract": [
    "For online speaker diarization, samples arrive incremen tally, and the overall distribution of the samples is invisible. Moreover, in most existing clustering-based methods, the train ing objective of the embedding extractor is not designed spe cially for clustering. To improve online speaker diarization per formance, we propose a unified online clustering framework, which provides an interactive manner between embedding ex tractors and clustering algorithms. Specifically, the framework consists of two highly coupled parts: clustering-guided recur rent training (CGRT) and paths truncated beam search (PTBS). The CGRT introduces the clustering algorithm into the training process of embedding extractors, which could provide not only cluster-aware information for the embedding extractor, but also crucial parameters for the clustering process afterward. And with these parameters, which contain preliminary information of the metric space, the PTBS penalizes the probability score of each cluster, in order to output more accurate clustering re sults in online fashion with low latency. With the above innova tions, our proposed online clustering system achieves 14.48% DER with collar 0.25 at 2.5s latency on the AISHELL-4, while the DER of the offline agglomerative hierarchical clustering is 14.57%."
   ],
   "doi": "10.21437/Interspeech.2022-944"
  },
  "wang22n_interspeech": {
   "authors": [
    [
     "Jun",
     "Wang"
    ]
   ],
   "title": "ESSumm: Extractive Speech Summarization from Untranscribed Meeting",
   "original": "945",
   "page_count": 5,
   "order": 657,
   "p1": 3243,
   "pn": 3247,
   "abstract": [
    "In this paper, we propose a novel architecture for direct extractive speech-to-speech summarization, ESSumm, which is an unsupervised model without dependence on intermediate transcribed text. Different from previous methods with text presentation, we are aimed at generating a summary directly from speech without transcription. First, a set of smaller speech segments are extracted based on speech signal's acoustic features. For each candidate speech segment, a distance-based summarization confidence score is designed for latent speech representation measure. Specifically, we leverage the off-the-shelf self-supervised convolutional neural network to extract the deep speech features from raw audio. Our approach automatically predicts the optimal sequence of speech segments that capture the key information with a target summary length. Extensive results on two well-known meeting datasets (AMI and ICSI corpora) show the effectiveness of our direct speech-based method to improve the summarization quality with untranscribed data. We also observe that our unsupervised speech-based method even performs on par with recent transcript-based summarization approaches, where extra speech recognition is required."
   ],
   "doi": "10.21437/Interspeech.2022-945"
  },
  "yang22l_interspeech": {
   "authors": [
    [
     "Seunghan",
     "Yang"
    ],
    [
     "Byeonggeun",
     "Kim"
    ],
    [
     "Inseop",
     "Chung"
    ],
    [
     "Simyung",
     "Chang"
    ]
   ],
   "title": "Personalized Keyword Spotting through Multi-task Learning",
   "original": "947",
   "page_count": 5,
   "order": 381,
   "p1": 1881,
   "pn": 1885,
   "abstract": [
    "Keyword spotting (KWS) plays an essential role in enabling speech-based user interaction on smart devices, and conventional KWS (C-KWS) approaches have concentrated on detecting user-agnostic pre-defined keywords. However, in practice, most user interactions come from target users enrolled in the device which motivates to construct personalized keyword spotting. We design two personalized KWS tasks; (1) Target user Biased KWS (TB-KWS) and (2) Target user Only KWS (TO-KWS). To solve the tasks, we propose personalized keyword spotting through multi-task learning (PK-MTL) that consists of multi-task learning and task-adaptation. First, we introduce applying multi-task learning on keyword spotting and speaker verification to leverage user information to the keyword spotting system. Next, we design task-specific scoring functions to adapt to the personalized KWS tasks thoroughly. We evaluate our framework on conventional and personalized scenarios, and the results show that PK-MTL can dramatically reduce the false alarm rate, especially in various practical scenarios."
   ],
   "doi": "10.21437/Interspeech.2022-947"
  },
  "park22c_interspeech": {
   "authors": [
    [
     "Sangwook",
     "Park"
    ],
    [
     "Sandeep Reddy",
     "Kothinti"
    ],
    [
     "Mounya",
     "Elhilali"
    ]
   ],
   "title": "Temporal coding with magnitude-phase regularization for sound event detection",
   "original": "950",
   "page_count": 5,
   "order": 312,
   "p1": 1536,
   "pn": 1540,
   "abstract": [
    "Sound Event Detection (SED) is the challenge of identifying sound events into their temporal boundaries as well as sound category. With recent advances in deep learning, more effective SED techniques are investigated through the annual challenge of Detection and Classification of Acoustic Scenes and Events (DCASE). Most SED systems rely on data-driven learning where a deep neural network is trained to minimize the error between model prediction and the truth. While this framework is generally effective at identifying sound classes present in an audio recording, it results in unreliable estimates of temporal information for identifying sound boundaries. In order to heighten the temporal precision, this paper proposes a novel temporal coding of magnitude and phase for embedding vectors in an intermediate layer. This coding is reflected as a regularization term in the objective function for training the model. The regularization allows magnitude of embedding vectors to increase near event boundaries, which represent the onset and offset points. Simultaneously, each of the boundaries are distinguishable from others using phase difference between two neighboring vectors. This approach results in notable improvement in timing sensitivity compared to a baseline system tested on SED task in the context of DCASE2021 challenge."
   ],
   "doi": "10.21437/Interspeech.2022-950"
  },
  "guo22d_interspeech": {
   "authors": [
    [
     "Haohan",
     "Guo"
    ],
    [
     "Feng-Long",
     "Xie"
    ],
    [
     "Frank",
     "Soong"
    ],
    [
     "Xixin",
     "Wu"
    ],
    [
     "Helen",
     "Meng"
    ]
   ],
   "title": "A Multi-Stage Multi-Codebook VQ-VAE Approach to High-Performance Neural TTS",
   "original": "952",
   "page_count": 5,
   "order": 327,
   "p1": 1611,
   "pn": 1615,
   "abstract": [
    "We propose a Multi-Stage, Multi-Codebook (MSMC) approach to high performance neural TTS synthesis. A vector-quantized, variational autoencoder (VQ-VAE) based feature analyzer is used to encode Mel spectrograms of speech training data by down-sampling progressively in multiple stages into MSMC Representations (MSMCRs) with different time resolutions, and quantizing them with multiple VQ codebooks, respectively. Multi-stage predictors are trained to map the input text sequence to MSMCRs progressively by minimizing a combined loss of the reconstruction Mean Square Error (MSE) and \"triplet loss”. In synthesis, the neural vocoder converts the predicted MSMCRs into final speech waveforms. The proposed approach is trained and tested with an English TTS database of 16 hours by a female speaker. The proposed TTS achieves an MOS score of 4.41, which outperforms the baseline with an MOS of 3.62. Compact versions of the proposed TTS with much less parameters can still preserve high MOS scores. Ablation studies show that both multiple stages and multiple codebooks are effective for achieving high TTS performance."
   ],
   "doi": "10.21437/Interspeech.2022-952"
  },
  "byun22_interspeech": {
   "authors": [
    [
     "Joon",
     "Byun"
    ],
    [
     "Seungmin",
     "Shin"
    ],
    [
     "Jongmo",
     "Sung"
    ],
    [
     "Seungkwon",
     "Beack"
    ],
    [
     "Youngcheol",
     "Park"
    ]
   ],
   "title": "Optimization of Deep Neural Network (DNN) Speech Coder Using a Multi Time Scale Perceptual Loss Function",
   "original": "955",
   "page_count": 5,
   "order": 894,
   "p1": 4411,
   "pn": 4415,
   "abstract": [
    "In this paper, we propose a method of perceptually optimizing the deep neural network (DNN)-based speech coder using multi-time-scale perceptual loss functions. We utilize a psychoacoustic model (PAM) to measure a perceptual distortion. Perceptual optimization is performed using losses based on a frame-wise global distortion and subframe-wise local distortions. To this end, the input frame is divided into seven subframes, and quantization noise spectra and global masking thresholds (GMTs) are estimated both frame-wise and subframe-wise and combined. The proposed optimization method was tested on a baseline DNN speech coder comprising stacks of Resnet-type gated linear units (ResGLUs). We employed a uniform noise model for the quantizer at the bottleneck. Test results showed that the proposed coder could control the quantization noise globally and locally so that it achieved higher perceptual quality than AMR-WB and OPUS, especially at a low bitrate."
   ],
   "doi": "10.21437/Interspeech.2022-955"
  },
  "mitra22_interspeech": {
   "authors": [
    [
     "Vikramjit",
     "Mitra"
    ],
    [
     "Hsiang-Yun Sherry",
     "Chien"
    ],
    [
     "Vasudha",
     "Kowtha"
    ],
    [
     "Joseph Yitan",
     "Cheng"
    ],
    [
     "Erdrin",
     "Azemi"
    ]
   ],
   "title": "Speech Emotion: Investigating Model Representations, Multi-Task Learning and Knowledge Distillation",
   "original": "957",
   "page_count": 5,
   "order": 955,
   "p1": 4715,
   "pn": 4719,
   "abstract": [
    "Estimating dimensional emotions, such as activation, valence and dominance, from acoustic speech signals has been widely explored over the past few years. While accurate estimation of activation and dominance from speech seem to be possible, the same for valence remains challenging. Previous research has shown that the use of lexical information can improve valence estimation performance. Lexical information can be obtained from pre-trained acoustic models, where the learned representations can improve valence estimation from speech. We investigate the use of pre-trained model representations to improve valence estimation from acoustic speech signal. We also explore fusion of representations to improve emotion estimation across all three emotion dimensions: activation, valence and dominance. Additionally, we investigate if representations from pre-trained models can be distilled into models trained with low-level features, resulting in models with a less number of parameters. We show that fusion of pre-trained model em- beddings result in a 79% relative improvement in concordance correlation coefficient on valence estimation compared to standard acoustic feature baseline, while distillation from pre-trained model embeddings to lower- dimensional representations yielded a relative 12% improve- ment. Such performance gains were observed over two evaluation sets, indicating that our proposed architecture generalizes across those evaluation sets."
   ],
   "doi": "10.21437/Interspeech.2022-957"
  },
  "wang22o_interspeech": {
   "authors": [
    [
     "Ye",
     "Wang"
    ],
    [
     "Baishun",
     "Ling"
    ],
    [
     "Yanmeng",
     "Wang"
    ],
    [
     "Junhao",
     "Xue"
    ],
    [
     "Shaojun",
     "Wang"
    ],
    [
     "Jing",
     "Xiao"
    ]
   ],
   "title": "Adversarial Knowledge Distillation For Robust Spoken Language Understanding",
   "original": "958",
   "page_count": 5,
   "order": 550,
   "p1": 2708,
   "pn": 2712,
   "abstract": [
    "In spoken dialog systems, Spoken Language Understanding (SLU) usually consists of two parts, Automatic Speech Recog\u0002nition (ASR) and Natural Language Understanding (NLU). In practice, such decoupled ASR/NLU design is beneficial for fast model iteration on both components. However, it also leads to the problem that NLU model suffers from the errors intro\u0002duced by ASR, which degrades the overall performance. Im\u0002proving the NLU model through Knowledge Distillation (KD) from large Pre-trained Language Models (PLMs) is proved to be effective and has drawn a lot of attention recently. In this work, we propose a novel Robust Adversarial Knowledge Dis\u0002tillation (RAKD) framework by introducing adversarial training into knowledge distillation to improve the robustness of NLU model to ASR-error. We conduct experiments on our own built classification dataset from a real-world spoken dialog system as well as existing datasets, where our proposed framework is proved to yield significant improvement over competitive base\u0002lines."
   ],
   "doi": "10.21437/Interspeech.2022-958"
  },
  "chang22d_interspeech": {
   "authors": [
    [
     "Joon-Hyuk",
     "Chang"
    ],
    [
     "Won-Gook",
     "Choi"
    ]
   ],
   "title": "Convolutional Recurrent Neural Network with Auxiliary Stream for Robust Variable-Length Acoustic Scene Classification",
   "original": "959",
   "page_count": 5,
   "order": 492,
   "p1": 2418,
   "pn": 2422,
   "abstract": [
    "Deep learning has proven to be suitable for acoustic scene classification (ASC). Therefore, it exhibits significant improvement in performance while using neural networks. However, several studies have been performed using convolutional neural network (CNN) rather than recurrent neural network (RNN) or convolutional recurrent neural network (CRNN), even though acoustic scene data is treated as a temporal signal. In practice, CRNNs are rarely adopted and are ranked lower in recent detection and classification of acoustic scenes and events (DCASE) challenges for fixed-length (i.e., 10 s) ASC. In this paper, an auxiliary stream technique is proposed that can improve the performance of CRNNs compared with that of CNNs by controlling the inductive bias of RNN. The auxiliary stream trains CNN by effectively extracting embeddings and is only connected on training steps. Therefore, it does not affect the model complexity on the inference steps. The experimental results demonstrate the superiority of the proposed method, regardless of the CNN model used for CRNN. Additionally, the proposed method yields robustness on variable-length ASC by performing streaming inferences and demonstrates the importance of CRNN."
   ],
   "doi": "10.21437/Interspeech.2022-959"
  },
  "xue22c_interspeech": {
   "authors": [
    [
     "Heyang",
     "Xue"
    ],
    [
     "Xinsheng",
     "Wang"
    ],
    [
     "Yongmao",
     "Zhang"
    ],
    [
     "Lei",
     "Xie"
    ],
    [
     "Pengcheng",
     "Zhu"
    ],
    [
     "Mengxiao",
     "Bi"
    ]
   ],
   "title": "Learn2Sing 2.0: Diffusion and Mutual Information-Based Target Speaker SVS by Learning from Singing Teacher",
   "original": "960",
   "page_count": 5,
   "order": 865,
   "p1": 4267,
   "pn": 4271,
   "abstract": [
    "Building a high-quality singing corpus for a person who is not good at singing is non-trivial, thus making it challenging to create a singing voice synthesizer for this person. Learn2Sing is dedicated to synthesizing the singing voice of a speaker without his or her singing data by learning from data recorded by others, i.e., the singing teacher. Inspired by the fact that pitch is the key style factor to distinguish singing from speaking voice, the proposed Learn2Sing 2.0 first generates the preliminary acoustic feature with averaged pitch value in the phone level, which allows the training of this process for different styles, i.e., speaking or singing, share same conditions except for the speaker information. Then, conditioned on the specific style, a diffusion decoder, which is accelerated by a fast sampling algorithm during the inference stage, is adopted to gradually restore the final acoustic feature. During the training, to avoid the information confusion of the speaker embedding and the style embedding, mutual information is employed to restrain the learning of speaker embedding and style embedding. Experiments show that the proposed approach is capable of synthesizing high-quality singing voice for the target speaker without singing data with 10 decoding steps."
   ],
   "doi": "10.21437/Interspeech.2022-960"
  },
  "liu22l_interspeech": {
   "authors": [
    [
     "Wenjing",
     "Liu"
    ],
    [
     "Chuan",
     "Xie"
    ]
   ],
   "title": "Gated Convolutional Fusion for Time-Domain Target Speaker Extraction Network",
   "original": "961",
   "page_count": 5,
   "order": 1088,
   "p1": 5368,
   "pn": 5372,
   "abstract": [
    "Target speaker extraction aims to extract the target speaker's voice from mixed utterances based on auxillary reference speech of the target speaker. A speaker embedding is usually extracted from the reference speech and fused with the learned acoustic representation. The majority of existing works perform simple operation-based fusion of concatenation. However, potential cross-modal correlation may not be effectively explored by this naive approach that directly fuse the speaker embedding into the acoustic representation. In this work, we propose a gated convolutional fusion approach by exploring global conditional modeling and trainable gating mechanism for learning sophisticated interaction between speaker embedding and acoustic representation. Experiments on WSJ0-2mix-extr dataset proves the efficacy of the proposed fusion approach, which performs favorably against other fusion methods with considerable improvement in terms of SDRi and SI-SDRi. Moreover, our method can be flexibly incorporated into similar time-domain speaker extraction networks to attain better performance."
   ],
   "doi": "10.21437/Interspeech.2022-961"
  },
  "kim22i_interspeech": {
   "authors": [
    [
     "Minseung",
     "Kim"
    ],
    [
     "Hyungchan",
     "Song"
    ],
    [
     "Sein",
     "Cheong"
    ],
    [
     "Jong Won",
     "Shin"
    ]
   ],
   "title": "iDeepMMSE: An improved deep learning approach to MMSE speech and noise power spectrum estimation for speech enhancement",
   "original": "964",
   "page_count": 5,
   "order": 37,
   "p1": 181,
   "pn": 185,
   "abstract": [
    "Deep learning approaches have been successfully applied to single channel speech enhancement exhibiting significant performance improvement. Recently, approaches unifying deep learning techniques into a statistical speech enhancement framework were proposed, including Deep Xi and DeepMMSE in which a priori signal-to-noise ratios (SNRs) were estimated by deep neural networks (DNNs) and noise power spectral density (PSD) and spectral gain functions were computed with estimated parameters. In this paper, we propose an improved DeepMMSE (iDeepMMSE) which estimates the speech PSD and speech presence probability as well as the a priori SNR using a DNN for MMSE estimation of the speech and noise PSDs. The a priori and a posteriori SNRs are refined with the estimated PSDs, which in turn are used to compute spectral gain function. We also replaced the DNN architecture with the Conformer which efficiently captures the local and global sequential information. Experimental results on the Voice Bank-DEMAND dataset and Deep Xi dataset showed the proposed iDeepMMSE outperformed the DeepMMSE in terms of the perceptual evaluation of speech quality (PESQ) scores and composite objective measures."
   ],
   "doi": "10.21437/Interspeech.2022-964"
  },
  "li22m_interspeech": {
   "authors": [
    [
     "Jingyu",
     "Li"
    ],
    [
     "Wei",
     "Liu"
    ],
    [
     "Tan",
     "Lee"
    ]
   ],
   "title": "EDITnet: A Lightweight Network for Unsupervised Domain Adaptation in Speaker Verification",
   "original": "967",
   "page_count": 5,
   "order": 749,
   "p1": 3694,
   "pn": 3698,
   "abstract": [
    "Performance degradation caused by language mismatch is a common problem when applying a speaker verification system on speech data in different languages. This paper proposes a domain transfer network, named EDITnet, to alleviate the language-mismatch problem on speaker embeddings without requiring speaker labels. The network leverages a conditional variational auto-encoder to transfer embeddings from the target domain into the source domain. A self-supervised learning strategy is imposed on the transferred embeddings so as to increase the cosine distance between embeddings from different speakers. In the training process of the EDITnet, the embedding extraction model is fixed without fine-tuning, which renders the training efficient and low-cost. Experiments on Voxceleb and CN-Celeb show that the embeddings transferred by EDITnet outperform the un-transferred ones by around 30% with the ECAPA-TDNN512. Performance improvement can also be achieved with other embedding extraction models, e.g., TDNN, SE-ResNet34."
   ],
   "doi": "10.21437/Interspeech.2022-967"
  },
  "huang22f_interspeech": {
   "authors": [
    [
     "Wen Chin",
     "Huang"
    ],
    [
     "Erica",
     "Cooper"
    ],
    [
     "Yu",
     "Tsao"
    ],
    [
     "Hsin-Min",
     "Wang"
    ],
    [
     "Tomoki",
     "Toda"
    ],
    [
     "Junichi",
     "Yamagishi"
    ]
   ],
   "title": "The VoiceMOS Challenge 2022",
   "original": "970",
   "page_count": 5,
   "order": 919,
   "p1": 4536,
   "pn": 4540,
   "abstract": [
    "We present the first edition of the VoiceMOS Challenge, a scientific event that aims to promote the study of automatic prediction of the mean opinion score (MOS) of synthetic speech. This challenge drew 22 participating teams from academia and industry who tried a variety of approaches to tackle the problem of predicting human ratings of synthesized speech. The listening test data for the main track of the challenge consisted of samples from 187 different text-to-speech and voice conversion systems spanning over a decade of research, and the out-of-domain track consisted of data from more recent systems rated in a separate listening test. Results of the challenge show the effectiveness of fine-tuning self-supervised speech models for the MOS prediction task, as well as the difficulty of predicting MOS ratings for unseen speakers and listeners, and for unseen systems in the out-of-domain setting."
   ],
   "doi": "10.21437/Interspeech.2022-970"
  },
  "alicehajic22_interspeech": {
   "authors": [
    [
     "Andrea",
     "Alicehajic"
    ],
    [
     "Silke",
     "Hamann"
    ]
   ],
   "title": "The discrimination of [zi]-[dʑi] by Japanese listeners and the prospective phonologization of /zi/",
   "original": "973",
   "page_count": 5,
   "order": 269,
   "p1": 1322,
   "pn": 1326,
   "abstract": [
    "The sequence /zi/ does not occur in the Japanese language, and in loanwords from English, it is adapted as /dʑi/, e.g., English busy - Japanese /bi.dʑi/. While similar sequences of alveolar obstruent plus high vowel, such as /ti/ or /tɯ/, also used to be avoided in Japanese, they are occurring in recent loanwords. Such formerly impossible sound sequences are likely to have emerged because current Japanese speakers have more contact with English than previous generations: extended exposure to English with its less restrictive phonotactics leads younger speakers to acquire less strong restrictions against the sequences in question in their Japanese. In the present study, we tested whether Japanese listeners are sensitive to the difference between [zi]-[dʑi] in an online AX discrimination task. Our participants ranged in age from 18 to 65. We hypothesized that younger listeners would be better at discriminating [zi]-[dʑi] in non-words as they had more exposure to English, which allows /zi/. Our study could not confirm this hypothesis. Instead, we found large individual variation in performance, and a good discriminability in general, which leads us to expect that the occurrence of /zi/ in Japanese loanwords is imminent."
   ],
   "doi": "10.21437/Interspeech.2022-973"
  },
  "rho22_interspeech": {
   "authors": [
    [
     "Daniel",
     "Rho"
    ],
    [
     "Jinhyeok",
     "Park"
    ],
    [
     "Jong Hwan",
     "Ko"
    ]
   ],
   "title": "NAS-VAD: Neural Architecture Search for Voice Activity Detection",
   "original": "975",
   "page_count": 5,
   "order": 761,
   "p1": 3754,
   "pn": 3758,
   "abstract": [
    "Various neural network-based approaches have been proposed for more robust and accurate voice activity detection (VAD). Manual design of such neural architectures is an error-prone and time-consuming process, which prompted the development of neural architecture search (NAS) that automatically design and optimize network architectures. While NAS has been successfully applied to improve performance in a variety of tasks, it has not yet been exploited in the VAD domain. In this paper, we present the first work that utilizes NAS approaches on the VAD task. To effectively search architectures for the VAD task, we propose a modified macro structure and a new search space with a much broader range of operations that includes attention operations. The results show that the network structures found by the propose NAS framework outperform previous manually designed state-of-the-art VAD models in various noise-added and real-world-recorded datasets. We also show that the architectures searched on a particular dataset achieve improved generalization performance on unseen audio datasets. Our code and models are available at https://github.com/daniel03c1/NAS_VAD."
   ],
   "doi": "10.21437/Interspeech.2022-975"
  },
  "saijo22c_interspeech": {
   "authors": [
    [
     "Kohei",
     "Saijo"
    ],
    [
     "Tetsuji",
     "Ogawa"
    ]
   ],
   "title": "Unsupervised Training of Sequential Neural Beamformer Using Coarsely-separated and Non-separated Signals",
   "original": "976",
   "page_count": 5,
   "order": 51,
   "p1": 251,
   "pn": 255,
   "abstract": [
    "We present an unsupervised training method of the sequential neural beamformer (Seq-BF) using coarsely-separated and non-separated supervisory signals. The signal coarsely separated by blind source separation (BSS) has been used for training neural separators in an unsupervised manner. However, the performance is limited due to distortions in the supervision. In contrast, remix-cycle-consistent learning (RCCL) enables a separator to be trained on distortion-free observed mixtures by making the remixed mixtures obtained by repeatedly separating and remixing the two different mixtures closer to the original mixtures. Still, training with RCCL from scratch often falls into a trivial solution, i.e., not separating signals. The present study provides a novel unsupervised learning algorithm for the Seq-BF with two stacked neural separators, in which the separators are pre-trained using the BSS outputs and then fine-tuned with RCCL. Such configuration compensates for the shortcomings of both approaches: the guiding mechanism in Seq-BF accelerates separation to exceed BSS performance, thereby stabilizing RCCL. Experimental comparisons demonstrated that the proposed unsupervised learning achieved performance comparable to supervised learning (0.4 point difference in word error rate)."
   ],
   "doi": "10.21437/Interspeech.2022-976"
  },
  "guo22e_interspeech": {
   "authors": [
    [
     "Shuai",
     "Guo"
    ],
    [
     "Jiatong",
     "Shi"
    ],
    [
     "Tao",
     "Qian"
    ],
    [
     "Shinji",
     "Watanabe"
    ],
    [
     "Qin",
     "Jin"
    ]
   ],
   "title": "SingAug: Data Augmentation for Singing Voice Synthesis with Cycle-consistent Training Strategy",
   "original": "978",
   "page_count": 5,
   "order": 866,
   "p1": 4272,
   "pn": 4276,
   "abstract": [
    "Deep learning based singing voice synthesis (SVS) systems have been demonstrated to flexibly generate singing with better qualities, compared to conventional statistical parametric based methods. However, neural systems are generally data-hungry and have difficulty to reach reasonable singing quality with limited public available training data. In this work, we explore different data augmentation methods to boost the training of SVS systems, including several strategies customized to SVS based on pitch augmentation and mix-up augmentation. To further stabilize the training, we introduce the cycle-consistent training strategy. Extensive experiments on two public singing databases demonstrate that our proposed augmentation methods and the stabilizing training strategy can significantly improve the performance on both objective and subjective evaluations."
   ],
   "doi": "10.21437/Interspeech.2022-978"
  },
  "ouyang22_interspeech": {
   "authors": [
    [
     "Zhiheng",
     "Ouyang"
    ],
    [
     "Miao",
     "Wang"
    ],
    [
     "Wei-Ping",
     "Zhu"
    ]
   ],
   "title": "Small Footprint Neural Networks for Acoustic Direction of Arrival Estimation",
   "original": "979",
   "page_count": 5,
   "order": 180,
   "p1": 881,
   "pn": 885,
   "abstract": [
    "In this paper we propose acoustic direction of arrival (DOA) estimation with neural networks. Conventional signal processing tasks such as DOA estimation have benefited from recent advancements in deep learning, which leads to a data-driven approach that allows neural networks to be employed in a black-box manner. From traditional aspects, modern network models often lack interpretability when directly employed in signal processing realm. As an alternative, we introduce a learnable network from spatial acoustical DOA estimation. Convolutional variants on feature projection can be derived while maintaining the explainability in both acoustical and neural network aspects. We introduce factorized spatial-temporal-spectral filtering which can significantly reduce computational cost and memory footprint. Experiments show the proposed networks perform well in harsh acoustic conditions with reduced requirement for hardware resources."
   ],
   "doi": "10.21437/Interspeech.2022-979"
  },
  "pattanayak22_interspeech": {
   "authors": [
    [
     "Biswaranjan",
     "Pattanayak"
    ],
    [
     "Gayadhar",
     "Pradhan"
    ]
   ],
   "title": "Significance of single frequency filter for the development of children’s KWS system",
   "original": "980",
   "page_count": 5,
   "order": 645,
   "p1": 3183,
   "pn": 3187,
   "abstract": [
    "Spotting a defined set of keywords from a running speech is known as keyword spotting (KWS). When keywords are detected using speech data from child speakers with the acoustic model built using speech data from adult speakers, it is named as children's KWS system. Owing to the differences in pitch and speaking rate between the two kind of speakers, the performance of children's KWS system deteriorates severely. In this paper, a pitch independent feature extraction method is proposed exploiting single frequency filtering (SFF) approach to address this issue. The method aims at finding the amplitude envelopes at Mel spaced frequencies. These amplitude envelopes are then averaged per analysis frame. Logarithm of the means are computed followed by Discrete Cosine Transform (DCT) to determine the required pitch robust feature, here denoted as Mel spaced single frequency filtering cepstral coefficient (MS-SFF-CC). The proposed feature outperforms several explored features with acoustic model trained on deep neural network-hidden Markov model (DNN-HMM) under pitch matched and mismatched test scenarios without and with data-augmented training."
   ],
   "doi": "10.21437/Interspeech.2022-980"
  },
  "ren22_interspeech": {
   "authors": [
    [
     "Shuo",
     "Ren"
    ],
    [
     "Shujie",
     "Liu"
    ],
    [
     "Yu",
     "Wu"
    ],
    [
     "Long",
     "Zhou"
    ],
    [
     "Furu",
     "Wei"
    ]
   ],
   "title": "Speech Pre-training with Acoustic Piece",
   "original": "981",
   "page_count": 5,
   "order": 538,
   "p1": 2648,
   "pn": 2652,
   "abstract": [
    "Previous speech pre-training methods, such as wav2vec2.0 and HuBERT, pre-train a Transformer encoder to learn deep representations from audio data, with objectives predicting either elements from latent vector quantized space or pre-generated labels (known as target codes) with offline clustering. However, those training signals (quantized elements or codes) are independent across different tokens without considering their relations. According to our observation and analysis, the target codes share obvious patterns aligned with phonemized text data. Based on that, we propose to leverage those patterns to better pre-train the model considering the relations among the codes. The patterns we extracted, called \"acoustic pieces\", are from the sentence piece result of HuBERT codes. With the acoustic piece as the training signal, we can implicitly bridge the input audio and natural language, which benefits audio-to-text tasks, such as automatic speech recognition (ASR). Simple but effective, our method \"HuBERT-AP'' significantly outperforms strong baselines on the LibriSpeech ASR task."
   ],
   "doi": "10.21437/Interspeech.2022-981"
  },
  "liu22m_interspeech": {
   "authors": [
    [
     "Zhaoci",
     "Liu"
    ],
    [
     "Ningqian",
     "Wu"
    ],
    [
     "Yajie",
     "Zhang"
    ],
    [
     "Zhenhua",
     "Ling"
    ]
   ],
   "title": "Integrating Discrete Word-Level Style Variations into Non-Autoregressive Acoustic Models for Speech Synthesis",
   "original": "984",
   "page_count": 5,
   "order": 1116,
   "p1": 5508,
   "pn": 5512,
   "abstract": [
    "This paper presents a method of integrating word-level style variations (WSVs) into non-autoregressive acoustic models for speech synthesis. WSVs are discrete latent representations extracted from the acoustic features of words, which have been proposed in our previous work to improve the naturalness of the Tacotron2 model. In this paper, we integrate WSVs into FastSpeech2, a non-autoregressive acoustic model. In the WSV extractor, a Gumbel-Sigmoid activation function is introduced for WSV representation and is compared with the original Gumbel-Softmax activation by experiments. The WSV predictor utilizes the word embeddings provided by BERT and has a non-autoregressive structure to be compatible with FastSpeech2. Experimental results show that our proposed method with the Gumbel-Sigmoid activation achieved better objective performance on F0 prediction than the FastSpeech2 baseline and the method using the Gumbel-Softmax activation. The subjective performance of our proposed models was also significantly better than the FastSpeech2 baseline."
   ],
   "doi": "10.21437/Interspeech.2022-984"
  },
  "hu22e_interspeech": {
   "authors": [
    [
     "Desheng",
     "Hu"
    ],
    [
     "Xinhui",
     "Hu"
    ],
    [
     "Xinkang",
     "Xu"
    ]
   ],
   "title": "Multiple Enhancements to LSTM for Learning Emotion-Salient Features in Speech Emotion Recognition",
   "original": "985",
   "page_count": 5,
   "order": 956,
   "p1": 4720,
   "pn": 4724,
   "abstract": [
    "Emotion-relevant feature extraction is key to the speech emotion recognition (SER) task. Although neural network for extracting features has achieved excellent results, in particular long short-term memory (LSTM) based models, there is still ample space for improvement. In this paper, from the perspective of utilizing advantages of multiple models, we propose an approach of multiple enhancements for learning emotion-salient features in SER, which is based on the combination of LSTM, one-dimensional convolution and transformer networks. Firstly, we introduce residual-BLSTM (Bidirectional LSTM) module to make the network deeper and to increase the learning ability of the model by adding feed-forward network (FFN) to the output of BLSTM and building residual connections at the same time. Secondly, time pooling employed in residual-BLSTM module is proposed to reduce features redundancy and overcome training overfitting. Finally, we propose an E-transformer module by combining transformer and convolution neural network. This approach enables it to learn local information while capturing global dependencies. We conduct evaluations on the IEMOCAP dataset using the proposed methods, and it shows the state-of-the-art performances."
   ],
   "doi": "10.21437/Interspeech.2022-985"
  },
  "kim22j_interspeech": {
   "authors": [
    [
     "Changhwan",
     "Kim"
    ],
    [
     "Seyun",
     "Um"
    ],
    [
     "Hyungchan",
     "Yoon"
    ],
    [
     "Hong-Goo",
     "Kang"
    ]
   ],
   "title": "FluentTTS: Text-dependent Fine-grained Style Control for Multi-style TTS",
   "original": "988",
   "page_count": 5,
   "order": 924,
   "p1": 4561,
   "pn": 4565,
   "abstract": [
    "In this paper, we propose a method to flexibly control the local prosodic variation of a neural text-to-speech (TTS) model. To provide expressiveness for synthesized speech, conventional TTS models utilize utterance-wise global style embeddings that are obtained by compressing frame-level embeddings along the time axis. However, since utterance-wise global features do not contain sufficient information to represent the characteristics of word-level local features, they are not appropriate for direct use on controlling prosody at a fine scale. In multi-style TTS models, it is very important to have the capability to control local prosody because it plays a key role in finding the most appropriate text-to-speech pair among many one-to-many mapping candidates. To explicitly present local prosodic characteristics to the contextual information of the corresponding input text, we propose a module to predict the fundamental frequency (F0) of each text by conditioning on the utterance-wise global style embedding. We also estimate multi-style embeddings using a multi-style encoder, which takes as inputs both a global utterance-wise embedding and a local F0 embedding. Our multi-style embedding enhances the naturalness and expressiveness of synthesized speech and is able to control prosody styles at the word-level or phoneme-level."
   ],
   "doi": "10.21437/Interspeech.2022-988"
  },
  "park22d_interspeech": {
   "authors": [
    [
     "Tae Jin",
     "Park"
    ],
    [
     "Nithin Rao",
     "Koluguri"
    ],
    [
     "Jagadeesh",
     "Balam"
    ],
    [
     "Boris",
     "Ginsburg"
    ]
   ],
   "title": "Multi-scale Speaker Diarization with Dynamic Scale Weighting",
   "original": "991",
   "page_count": 5,
   "order": 1028,
   "p1": 5080,
   "pn": 5084,
   "abstract": [
    "Speaker diarization systems are challenged by a trade-off between the temporal resolution and the fidelity of the speaker representation. By obtaining a superior temporal resolution with an enhanced accuracy, a multi-scale approach is a way to cope with such a trade-off. In this paper, we propose a more advanced multi-scale diarization system based on a multi-scale diarization decoder. There are two main contributions in this study that significantly improve the diarization performance. First, we use multi-scale clustering as an initialization to estimate the number of speakers and obtain the average speaker representation vector for each speaker and each scale. Next, we propose the use of 1-D convolutional neural networks that dynamically determine the importance of each scale at each time step. To handle a variable number of speakers and overlapping speech, the proposed system can estimate the number of existing speakers. Our proposed system achieves a state-of-art performance on the CALLHOME and AMI MixHeadset datasets, with 3.92% and 1.05% diarization error rates, respectively."
   ],
   "doi": "10.21437/Interspeech.2022-991"
  },
  "huang22g_interspeech": {
   "authors": [
    [
     "Wei-Ping",
     "Huang"
    ],
    [
     "Po-Chun",
     "Chen"
    ],
    [
     "Sung-Feng",
     "Huang"
    ],
    [
     "Hung-yi",
     "Lee"
    ]
   ],
   "title": "Few Shot Cross-Lingual TTS Using Transferable Phoneme Embedding",
   "original": "994",
   "page_count": 5,
   "order": 925,
   "p1": 4566,
   "pn": 4570,
   "abstract": [
    "This paper studies a transferable phoneme embedding framework that aims to deal with the cross-lingual text-to-speech (TTS) problem under the few-shot setting. Transfer learning is a common approach when it comes to few-shot learning since training from scratch on few-shot training data is bound to overfit. Still, we find that the naive transfer learning approach fails to adapt to unseen languages under extremely few-shot settings, where less than 8 minutes of data is provided. We deal with the problem by proposing a framework that consists of a phoneme-based TTS model and a codebook module to project phonemes from different languages into a learned latent space. Furthermore, by utilizing phoneme-level averaged self-supervised learned features, we effectively improve the quality of synthesized speeches. Experiments show that using 4 utterances, which is about 30 seconds of data, is enough to synthesize intelligible speech when adapting to an unseen language using our framework."
   ],
   "doi": "10.21437/Interspeech.2022-994"
  },
  "wang22p_interspeech": {
   "authors": [
    [
     "Yang",
     "Wang"
    ],
    [
     "Chenxing",
     "Li"
    ],
    [
     "Feng",
     "Deng"
    ],
    [
     "Shun",
     "Lu"
    ],
    [
     "Peng",
     "Yao"
    ],
    [
     "Jianchao",
     "Tan"
    ],
    [
     "Chengru",
     "Song"
    ],
    [
     "Xiaorui",
     "Wang"
    ]
   ],
   "title": "WA-Transformer: Window Attention-based Transformer with Two-stage Strategy for Multi-task Audio Source Separation",
   "original": "995",
   "page_count": 5,
   "order": 1089,
   "p1": 5373,
   "pn": 5377,
   "abstract": [
    "The standard Conformer adopts convolution layers to exploit local features. However, the one-dimensional convolution ignores the correlation of adjacent time-frequency features. In this paper, we design a two-dimensional window attention block with dilation, and then we propose a window attention-based Transformer network (named WA-Transformer) for multi-task audio source separation. The proposed WA-Transformer adopts self-attention and window attention blocks to model global dependencies and local correlation in a parameter-efficient way. Besides, it follows a two-stage pipeline, in which the first stage separates the mixture and outputs the three types of audio signals, and the second stage performs signal compensation. Experiments demonstrate the effectiveness of WA-Transformer. WA-Transformer achieves 13.86 dB, 12.22 dB, 11.21 dB signal-to-distortion ratio improvement on speech, music, noise track, respectively, and advantages over several well-known models."
   ],
   "doi": "10.21437/Interspeech.2022-995"
  },
  "dekorte22_interspeech": {
   "authors": [
    [
     "Marcel",
     "de Korte"
    ],
    [
     "Jaebok",
     "Kim"
    ],
    [
     "Aki",
     "Kunikoshi"
    ],
    [
     "Adaeze",
     "Adigwe"
    ],
    [
     "Esther",
     "Klabbers"
    ]
   ],
   "title": "Data-augmented cross-lingual synthesis in a teacher-student framework",
   "original": "9995",
   "page_count": 5,
   "order": 482,
   "p1": 2368,
   "pn": 2372,
   "abstract": [
    "Cross-lingual synthesis can be defined as the task of letting a speaker generate fluent synthetic speech in another language. This is a challenging task, and resulting speech can suffer from reduced naturalness, accented speech, and/or loss of essential voice characteristics. Previous research shows that many models appear to have insufficient generalization capabilities to perform well on every of these cross-lingual aspects. To overcome these generalization problems, we propose to apply the teacher-student paradigm to cross-lingual synthesis. While a teacher model is commonly used to produce teacher forced data, we propose to also use it to produce augmented data of unseen speaker-language pairs, where the aim is to retain essential speaker characteristics. Both sets of data are then used for student model training, which is trained to retain the naturalness and prosodic variation present in the teacher forced data, while learning the speaker identity from the augmented data. Some modifications to the student model are proposed to make the separation of teacher forced and augmented data more straightforward. Results show that the proposed approach improves the retention of speaker characteristics in the speech, while managing to retain high levels of naturalness and prosodic variation."
   ],
   "doi": "10.21437/Interspeech.2022-9995"
  },
  "gao22b_interspeech": {
   "authors": [
    [
     "Zhifu",
     "Gao"
    ],
    [
     "ShiLiang",
     "Zhang"
    ],
    [
     "Ian",
     "McLoughlin"
    ],
    [
     "Zhijie",
     "Yan"
    ]
   ],
   "title": "Paraformer: Fast and Accurate Parallel Transformer for Non-autoregressive End-to-End Speech Recognition",
   "original": "9996",
   "page_count": 5,
   "order": 421,
   "p1": 2063,
   "pn": 2067,
   "abstract": [
    "Transformers have recently dominated the ASR field. Although able to yield good performance, they involve an autoregressive (AR) decoder to generate tokens one by one, which is computationally inefficient. To speed up inference, non-autoregressive (NAR) methods, e.g. single-step NAR, were designed, to enable parallel generation. However, due to an independence assumption within the output tokens, performance of single-step NAR is inferior to that of AR models, especially with a large-scale corpus. There are two challenges to improving single-step NAR: Firstly to accurately predict the number of output tokens and extract hidden variables; secondly, to enhance modeling of interdependence between output tokens. To tackle both challenges, we propose a fast and accurate parallel transformer, termed Paraformer. This utilizes a continuous integrate-and-fire based predictor to predict the number of tokens and generate hidden variables. A glancing language model sampler then generates semantic embeddings to enhance the NAR decoder's ability to model context interdependence. Finally, we design a strategy to generate negative samples for minimum word error rate training to further improve performance. Experiments using the AISHELL-1, AISHELL-2 benchmark, and an industrial-level 20,000 hour task demonstrate that the proposed Paraformer can attain comparable performance to the state-of-the-art AR transformer, with over 10x speedup."
   ],
   "doi": "10.21437/Interspeech.2022-9996"
  },
  "hung22_interspeech": {
   "authors": [
    [
     "Kuo-Hsuan",
     "Hung"
    ],
    [
     "Szu-wei",
     "Fu"
    ],
    [
     "Huan-Hsin",
     "Tseng"
    ],
    [
     "Hsin-Tien",
     "Chiang"
    ],
    [
     "Yu",
     "Tsao"
    ],
    [
     "Chii-Wann",
     "Lin"
    ]
   ],
   "title": "Boosting Self-Supervised Embeddings for Speech Enhancement",
   "original": "10002",
   "page_count": 5,
   "order": 38,
   "p1": 186,
   "pn": 190,
   "abstract": [
    "Self-supervised learning (SSL) representation for speech has achieved state-of-the-art (SOTA) performance on several downstream tasks. However, there remains room for improvement in speech enhancement (SE) tasks. In this study, we used a cross-domain feature to solve the problem that SSL embeddings may lack fine-grained information to regenerate speech signals. By integrating the SSL representation and spectrogram, the result can be significantly boosted. We further study the relationship between the noise robustness of SSL representation via clean-noisy distance (CN distance) and the layer importance for SE. Consequently, we found that SSL representations with lower noise robustness are more important. Furthermore, our experiments on the VCTK-DEMAND dataset demonstrated that fine-tuning an SSL representation with an SE model can outperform the SOTA SSL-based SE methods in PESQ, CSIG and COVL without invoking complicated network architectures. In later experiments, the CN distance in SSL embeddings was observed to increase after fine-tuning. These results verify our expectations and may help design SE-related SSL training in the future."
   ],
   "doi": "10.21437/Interspeech.2022-10002"
  },
  "fan22b_interspeech": {
   "authors": [
    [
     "Zixia",
     "Fan"
    ],
    [
     "Jing",
     "Shao"
    ],
    [
     "Weigong",
     "Pan"
    ],
    [
     "Min",
     "Xu"
    ],
    [
     "Lan",
     "Wang"
    ]
   ],
   "title": "The effect of backward noise on lexical tone discrimination in Mandarin-speaking amusics",
   "original": "10004",
   "page_count": 5,
   "order": 438,
   "p1": 2148,
   "pn": 2152,
   "abstract": [
    "Congenital amusia is a neurogenetic disorder, affecting music pitch processing. It also transfers to the language domain and negatively influences the perception of linguistic components relying on pitch, such as lexical tones. It has been well established that unfavorable listening conditions impact lexical tone perception in amusics. For instance, both Mandarin- and Cantonese-speaking amusics were impaired in tone processing under simultaneously noisy conditions. Backward noise is one of the adverse listening conditions, but its interference mechanism is distinct from the simultaneous noise. Therefore, it warrants more studies to explore whether and how backward masking noise affects tone processing in amusics. In the current study, eighteen Mandarin-speaking amusics and 18 controls were tested on discrimination of Mandarin tones under two conditions: a quiet condition involving relatively low-level processing and a backward masking condition involving high-level processing (e.g., tone categorization) where a native multi-talker babble noise was added to target tones. The results revealed that amusics performed similarly to controls in quiet conditions, whereas poorer performance in backward noise conditions. These findings shed light on how adverse listening environments influence amusics' lexical tone processing and provided further empirical evidence that amusics may be impaired in the high-level phonological processing of lexical tone."
   ],
   "doi": "10.21437/Interspeech.2022-10004"
  },
  "dai22_interspeech": {
   "authors": [
    [
     "Ziqian",
     "Dai"
    ],
    [
     "Jianwei",
     "Yu"
    ],
    [
     "Yan",
     "Wang"
    ],
    [
     "Nuo",
     "Chen"
    ],
    [
     "Yanyao",
     "Bian"
    ],
    [
     "GuangZhi",
     "Li"
    ],
    [
     "Deng",
     "Cai"
    ],
    [
     "Dong",
     "Yu"
    ]
   ],
   "title": "Automatic Prosody Annotation with Pre-Trained Text-Speech Model",
   "original": "10005",
   "page_count": 5,
   "order": 1117,
   "p1": 5513,
   "pn": 5517,
   "abstract": [
    "Prosodic boundary plays an important role in text-to-speech synthesis (TTS) in terms of naturalness and readability. However, the acquisition of prosodic boundary labels relies on manual annotation, which is costly and time-consuming. In this paper, we propose to automatically extract prosodic boundary labels from text-audio data via a neural text-speech model with pre-trained audio encoders. This model is pre-trained on text and speech data separately and jointly fine-tuned on TTS data in a triplet format: \\{speech, text, prosody\\}. The experimental results on both automatic evaluation and human evaluation demonstrate that: 1) the proposed text-speech prosody annotation framework significantly outperforms text-only baselines; 2) the quality of automatic prosodic boundary annotations is comparable to human annotations; 3) TTS systems trained with model-annotated boundaries are slightly better than systems that use manual ones."
   ],
   "doi": "10.21437/Interspeech.2022-10005"
  },
  "li22n_interspeech": {
   "authors": [
    [
     "Bo",
     "Li"
    ],
    [
     "Tara",
     "Sainath"
    ],
    [
     "Ruoming",
     "Pang"
    ],
    [
     "Shuo-Yiin",
     "Chang"
    ],
    [
     "Qiumin",
     "Xu"
    ],
    [
     "Trevor",
     "Strohman"
    ],
    [
     "Vince",
     "Chen"
    ],
    [
     "Qiao",
     "Liang"
    ],
    [
     "Heguang",
     "Liu"
    ],
    [
     "Yanzhang",
     "He"
    ],
    [
     "Parisa",
     "Haghani"
    ],
    [
     "Sameer",
     "Bidichandani"
    ]
   ],
   "title": "A Language Agnostic Multilingual Streaming On-Device ASR System",
   "original": "10006",
   "page_count": 5,
   "order": 646,
   "p1": 3188,
   "pn": 3192,
   "abstract": [
    "On-device end-to-end (E2E) models have shown improvements over a conventional model on English Voice Search tasks in both quality and latency. E2E models have also shown promising results for multilingual automatic speech recognition (ASR). In this paper, we extend our previous capacity solution to streaming applications and present a streaming multilingual E2E ASR system that runs fully on device with comparable quality and latency to individual monolingual models. To achieve that, we propose an Encoder Endpointer model and an End-of-Utterance (EOU) Joint Layer for a better quality and latency trade-off. Our system is built in a language agnostic manner allowing it to natively support intersentential code switching in real time. To address the feasibility concerns on large models, we conducted on-device profiling and replaced the time consuming LSTM decoder with the recently developed Embedding decoder. With these changes, we managed to run such a system on a mobile device in less than real time."
   ],
   "doi": "10.21437/Interspeech.2022-10006"
  },
  "conneau22_interspeech": {
   "authors": [
    [
     "Alexis",
     "Conneau"
    ],
    [
     "Ankur",
     "Bapna"
    ],
    [
     "Yu",
     "Zhang"
    ],
    [
     "Min",
     "Ma"
    ],
    [
     "Patrick",
     "von Platen"
    ],
    [
     "Anton",
     "Lozhkov"
    ],
    [
     "Colin",
     "Cherry"
    ],
    [
     "Ye",
     "Jia"
    ],
    [
     "Clara",
     "Rivera"
    ],
    [
     "Mihir",
     "Kale"
    ],
    [
     "Daan",
     "van Esch"
    ],
    [
     "Vera",
     "Axelrod"
    ],
    [
     "Simran",
     "Khanuja"
    ],
    [
     "Jonathan",
     "Clark"
    ],
    [
     "Orhan",
     "Firat"
    ],
    [
     "Michael",
     "Auli"
    ],
    [
     "Sebastian",
     "Ruder"
    ],
    [
     "Jason",
     "Riesa"
    ],
    [
     "Melvin",
     "Johnson"
    ]
   ],
   "title": "XTREME-S: Evaluating Cross-lingual Speech Representations",
   "original": "10007",
   "page_count": 5,
   "order": 658,
   "p1": 3248,
   "pn": 3252,
   "abstract": [
    "We introduce XTREME-S, a new benchmark to evaluate universal cross-lingual speech representations in many languages. XTREME-S covers four task families: speech recognition, classification, speech-to-text translation and retrieval. Covering 102 languages from 10+ language families, 3 different domains and 4 task families, XTREME-S aims to simplify multilingual speech representation evaluation, as well as catalyze research in \"universal\" speech representation learning. This paper describes the new benchmark and establishes the first speech-only and speech-text baselines using XLS-R and mSLAM on all downstream tasks. We motivate the design choices and detail how to use the benchmark. Datasets and fine-tuning scripts are made easily accessible through the HuggingFace platform (https://hf.co/datasets/google/xtreme_s)."
   ],
   "doi": "10.21437/Interspeech.2022-10007"
  },
  "liu22n_interspeech": {
   "authors": [
    [
     "Miao",
     "Liu"
    ],
    [
     "Jing",
     "Wang"
    ],
    [
     "Liang",
     "Xu"
    ],
    [
     "Jianqian",
     "Zhang"
    ],
    [
     "Shicong",
     "Li"
    ],
    [
     "Fei",
     "Xiang"
    ]
   ],
   "title": "BIT-MI Deep Learning-based Model to Non-intrusive Speech Quality Assessment Challenge in Online Conferencing Applications",
   "original": "10010",
   "page_count": 5,
   "order": 666,
   "p1": 3288,
   "pn": 3292,
   "abstract": [
    "This paper presents the details of the BIT-MI deep learning-based model submitted to the ConferencingSpeech challenge 2022. Due to the large time and labor costs of subjective tests, the challenge aims to promote the non-intrusive objective quality assessment research for speech communication and targets for effective evaluation on the speech quality of online conferencing applications. We propose a novel deep learning-based model involving a new convolution neural network (CNN) architecture, a bidirectional long short term memory (BLSTM), an average pooling and a range clipping method. Meanwhile, we construct a two-parts target function combining the mean square error (MSE) and pearson correlation coefficient (PCC) between predictions and labels in order to jointly optimize the performance of the assessment model from both aspects. Experiment results show that the proposed model significantly outperforms the official baseline system both on the validation and test set."
   ],
   "doi": "10.21437/Interspeech.2022-10010"
  },
  "dong22b_interspeech": {
   "authors": [
    [
     "Qianqian",
     "Dong"
    ],
    [
     "Fengpeng",
     "Yue"
    ],
    [
     "Tom",
     "Ko"
    ],
    [
     "Mingxuan",
     "Wang"
    ],
    [
     "Qibing",
     "Bai"
    ],
    [
     "Yu",
     "Zhang"
    ]
   ],
   "title": "Leveraging Pseudo-labeled Data to Improve Direct Speech-to-Speech Translation",
   "original": "10011",
   "page_count": 5,
   "order": 361,
   "p1": 1781,
   "pn": 1785,
   "abstract": [
    "Direct Speech-to-speech translation (S2ST) has drawn more and more attention recently. The task is very challenging due to data scarcity and complex speech-to-speech mapping. In this paper, we report our recent achievements in S2ST. Firstly, we build a S2ST Transformer baseline which outperforms the original Translatotron. Secondly, we utilize the external data by pseudo-labeling and obtain a new state-of-the-art result on the Fisher English-to-Spanish test set. Indeed, we exploit the pseudo data with a combination of popular techniques which are not trivial when applied to S2ST. Moreover, we evaluate our approach on both syntactically similar (Spanish-English) and distant (English-Chinese) language pairs. Our implementation is available at \\url{https://github.com/fengpeng-yue/speech-to-speech-translation}."
   ],
   "doi": "10.21437/Interspeech.2022-10011"
  },
  "jung22b_interspeech": {
   "authors": [
    [
     "Myunghun",
     "Jung"
    ],
    [
     "Hoi Rin",
     "Kim"
    ]
   ],
   "title": "Asymmetric Proxy Loss for Multi-View Acoustic Word Embeddings",
   "original": "10013",
   "page_count": 5,
   "order": 1046,
   "p1": 5170,
   "pn": 5174,
   "abstract": [
    "Acoustic word embeddings (AWEs) are discriminative representations of speech segments, and learned embedding space reflects the phonetic similarity between words. With multi-view learning, where text labels are considered as supplementary input, AWEs are jointly trained with acoustically grounded word embeddings (AGWEs). In this paper, we expand the multi-view approach into a proxy-based framework for deep metric learning by equating AGWEs with proxies. A simple modification in computing the similarity matrix allows the general pair weighting to formulate the data-to-proxy relationship. Under the systematized framework, we propose an asymmetric-proxy loss that combines different parts of loss functions asymmetrically while keeping their merits. It follows the assumptions that the optimal function for anchor-positive pairs may differ from one for anchor-negative pairs, and a proxy may have a different impact when it substitutes for different positions in the triplet. We present comparative experiments with various proxy-based losses including our asymmetric-proxy loss, and evaluate AWEs and AGWEs for word discrimination tasks on WSJ corpus. The results demonstrate the effectiveness of the proposed method."
   ],
   "doi": "10.21437/Interspeech.2022-10013"
  },
  "fan22c_interspeech": {
   "authors": [
    [
     "Zixia",
     "Fan"
    ],
    [
     "Jing",
     "Shao"
    ],
    [
     "Weigong",
     "Pan"
    ],
    [
     "Lan",
     "Wang"
    ]
   ],
   "title": "Revisiting visuo-spatial processing in individuals with congenital amusia",
   "original": "10014",
   "page_count": 5,
   "order": 978,
   "p1": 4830,
   "pn": 4834,
   "abstract": [
    "Congenital amusia is a lifelong developmental disorder of pitch, not only specific to music. Several studies have explored whether amusia impacts spatial processing, as pitch perception is associated with spatial representations in nature. However, to date, the results were still inconclusive, with some researchers claiming amusics have general spatial processing deficits while others not. To better understand this question, the present study examined some basic capabilities of spatial processing via Corsi Blocks task and the mental rotation task. Additionally, it has been documented that the processing of spatial representations normally shares cognitive mechanisms (e.g., the perception of magnitude precision) with melody memory, a pitch short-memory (span) task was also conducted. Eighteen amusics and 18 controls participated in the experiments. The results showed that in Corsi Block task, amusics' performances were comparable to the musically intact controls, suggesting that amusics possess intact visual short-term memory. However, poorer performance on the mental rotation task indicates that amusics have poor spatial awareness. Furthermore, the results also showed a strong association between pitch memory capacity and mental rotation accuracy, confirming that they shared certain underlying cognitive mechanisms like the perception of magnitude precision, probably contributing to their deficits in pitch memory and spatial processing."
   ],
   "doi": "10.21437/Interspeech.2022-10014"
  },
  "qin22b_interspeech": {
   "authors": [
    [
     "Siqing",
     "Qin"
    ],
    [
     "Longbiao",
     "Wang"
    ],
    [
     "Sheng",
     "Li"
    ],
    [
     "Yuqin",
     "Lin"
    ],
    [
     "Jianwu",
     "Dang"
    ]
   ],
   "title": "Finer-grained Modeling units-based Meta-Learning for Low-resource Tibetan Speech Recognition",
   "original": "10015",
   "page_count": 5,
   "order": 435,
   "p1": 2133,
   "pn": 2137,
   "abstract": [
    "Tibetan is a typical under-resourced language due to its relatively smaller population. Although a character-based end-to-end (E2E) automatic speech recognition (ASR) model with transfer learning and multilingual training strategies has mitigated the problem of low resources, it often meets overfitting problem. Recently meta-learning performs great in solving overfitting problem. However, the widely-used coarse-grained modeling units are not significantly correlated to their pronunciation, which limits the performance improvement of the low-resource ASR system. Furthermore, meta-learning consists of a meta-training period and fast self-adaption on the target language, and the past meta-training period is lack target language-specific information. Therefore, we propose a novel E2E low-resource Lhasa dialect ASR model based on the finer-grained modeling units and transfer learning with reference to the properties of Chinese Pinyin. Chinese Pinyin and Tibetan decomposed radicals are more related to pronunciation than characters are, which can compensate for more acoustic information in low-resource situations. Furthermore, Tibetan modeling units are utilized in both meta-training and fast self-adaption processes to offer language-specific information to solve the low-resource problem. Experiments show that our proposed method achieves a 54.9% relative character error reduction rate than the baseline system."
   ],
   "doi": "10.21437/Interspeech.2022-10015"
  },
  "quan22b_interspeech": {
   "authors": [
    [
     "Changsheng",
     "Quan"
    ],
    [
     "Xiaofei",
     "Li"
    ]
   ],
   "title": "Multichannel Speech Separation with Narrow-band Conformer",
   "original": "10018",
   "page_count": 5,
   "order": 1090,
   "p1": 5378,
   "pn": 5382,
   "abstract": [
    "This work proposes a multichannel speech separation method with narrow-band Conformer (named NBC). The network is trained to learn to automatically exploit narrow-band speech separation information, such as spatial vector clustering of multiple speakers. Specifically, in the short-time Fourier transform (STFT) domain, the network processes each frequency independently, and is shared by all frequencies. For one frequency, the network inputs the STFT coefficients of multichannel mixture signals, and predicts the STFT coefficients of separated speech signals. Clustering of spatial vectors shares a similar principle with the self-attention mechanism in the sense of computing the similarity of vectors and then aggregating similar vectors. Therefore, Conformer would be especially suitable for the present problem. Experiments show that the proposed narrow-band Conformer achieves better speech separation performance than other state-of-the-art methods by a large margin."
   ],
   "doi": "10.21437/Interspeech.2022-10018"
  },
  "chen22g_interspeech": {
   "authors": [
    [
     "Sanyuan",
     "Chen"
    ],
    [
     "Yu",
     "Wu"
    ],
    [
     "Chengyi",
     "Wang"
    ],
    [
     "Shujie",
     "Liu"
    ],
    [
     "Zhuo",
     "Chen"
    ],
    [
     "Peidong",
     "Wang"
    ],
    [
     "Gang",
     "Liu"
    ],
    [
     "Jinyu",
     "Li"
    ],
    [
     "Jian",
     "Wu"
    ],
    [
     "Xiangzhan",
     "Yu"
    ],
    [
     "Furu",
     "Wei"
    ]
   ],
   "title": "Why does Self-Supervised Learning for Speech Recognition Benefit Speaker Recognition?",
   "original": "10019",
   "page_count": 5,
   "order": 750,
   "p1": 3699,
   "pn": 3703,
   "abstract": [
    "Recently, self-supervised learning (SSL) has demonstrated strong performance in speaker recognition, even if the pre-training objective is designed for speech recognition. In this paper, we study which factor leads to the success of self-supervised learning on speaker-related tasks, e.g. speaker verification (SV), through a series of carefully designed experiments. Our empirical results on the Voxceleb-1 dataset suggest that the benefit of SSL to SV task is from a combination of mask speech prediction loss, data scale, and model size, while the SSL quantizer has a minor impact. We further employ the integrated gradients attribution method and loss landscape visualization to understand the effectiveness of self-supervised learning for speaker recognition performance."
   ],
   "doi": "10.21437/Interspeech.2022-10019"
  },
  "jayesh22_interspeech": {
   "authors": [
    [
     "M K",
     "Jayesh"
    ],
    [
     "Mukesh",
     "Sharma"
    ],
    [
     "Praneeth",
     "Vonteddu"
    ],
    [
     "Mahaboob Ali Basha",
     "Shaik"
    ],
    [
     "Sriram",
     "Ganapathy"
    ]
   ],
   "title": "Transformer Networks for Non-Intrusive Speech Quality Prediction",
   "original": "10020",
   "page_count": 5,
   "order": 827,
   "p1": 4078,
   "pn": 4082,
   "abstract": [
    "This paper presents the details of our speech quality prediction system submitted to the Conferencing Speech-2022 challenge. The challenge involved the task of non-intrusive speech quality assessment intended for online conferencing applications. We propose two approaches for speech quality prediction in this work. The first approach uses a combination of deep convolutional neural network (CNN) and LSTM neural network with Kullback-Leibler (KL) loss function and cross entropy (CE) loss function for estimating the mean opinion scores (MOS). Our second approach uses transformer based encoder network before applying attention pooling. We observe that our proposed second method gives significant improvements compared to our first method as well as on the baselines provided by the challenge organizers with respect to Pearson Correlation Coefficient (PCC) and Spearman Rank Correlation Coefficient (SRCC) along with reductions in root mean square error (RMSE). The model is also seen to generalize for unseen data resources on the evaluation dataset."
   ],
   "doi": "10.21437/Interspeech.2022-10020"
  },
  "peng22b_interspeech": {
   "authors": [
    [
     "Zhiyuan",
     "Peng"
    ],
    [
     "Xuanji",
     "He"
    ],
    [
     "Ke",
     "Ding"
    ],
    [
     "Tan",
     "Lee"
    ],
    [
     "Guanglu",
     "Wan"
    ]
   ],
   "title": "Unifying Cosine and PLDA Back-ends for Speaker Verification",
   "original": "10021",
   "page_count": 5,
   "order": 68,
   "p1": 336,
   "pn": 340,
   "abstract": [
    "State-of-art speaker verification (SV) systems use a back-end model to score the similarity of speaker embeddings extracted from a neural network. The commonly used back-ends are the cosine scoring and the probabilistic linear discriminant analysis (PLDA) scoring. With the recently developed neural embeddings, the theoretically more appealing PLDA approach is found to have no advantage against or even be inferior to the simple cosine scoring in terms of verification performance. This paper presents an investigation on the relation between the two back-ends, aiming to explain the above counter-intuitive observation. It is shown that the cosine scoring is essentially a special case of PLDA scoring. In other words, by properly setting the parameters of PLDA, the two back-ends become equivalent. As a consequence, the cosine scoring not only inherits the basic assumptions for the PLDA but also introduces additional assumptions on speaker embeddings. Experiments show that the dimensional independence assumption required by the cosine scoring contributes most to the performance gap between the two methods under the domain-matched condition. When there is severe domain mismatch, the dimensional independence assumption does not hold and the PLDA would perform better than the cosine for domain adaptation."
   ],
   "doi": "10.21437/Interspeech.2022-10021"
  },
  "tian22d_interspeech": {
   "authors": [
    [
     "Xiaohai",
     "Tian"
    ],
    [
     "Kaiqi",
     "Fu"
    ],
    [
     "Shaojun",
     "Gao"
    ],
    [
     "Yiwei",
     "Gu"
    ],
    [
     "Kai",
     "Wang"
    ],
    [
     "Wei",
     "Li"
    ],
    [
     "Zejun",
     "Ma"
    ]
   ],
   "title": "A Transfer and Multi-Task Learning based Approach for MOS Prediction",
   "original": "10022",
   "page_count": 5,
   "order": 1102,
   "p1": 5438,
   "pn": 5442,
   "abstract": [
    "Automatic speech quality assessment aims to train a model capable of automatically measuring the performance of synthesis systems. This is a challenging task, especially when the domain of the evaluation data is different to that of the training data. In this paper, we present a multi-task and transfer learning framework for predicting the mean opinion score (MOS) of synthetic speech from different domains. Specifically, the proposed framework consists of a common encoder shared by data from different domains and two domain-specific decoders for in-domain and out-of-domain data, respectively. A wav2vec2 fine-tuned for phone recognition task is utilized as an initialization of the shared encoder to make full use of its learned knowledge from large number of unlabeled data and task-related labeled data. The experiments are conducted on the VoiceMOS Challenge dataset. The results show that the proposed system outperforms the baseline solutions for both in-domain and out-of-domain MOS prediction scenarios. Further, we show that the wav2vec2 encoder fine-tuned for phone recognition can be transferred to boost the performance of the MOS prediction."
   ],
   "doi": "10.21437/Interspeech.2022-10022"
  },
  "hwang22b_interspeech": {
   "authors": [
    [
     "Seorim",
     "Hwang"
    ],
    [
     "Sung Wook",
     "Park"
    ],
    [
     "Youngcheol",
     "Park"
    ]
   ],
   "title": "Monoaural Speech Enhancement Using a Nested U-Net with Two-Level Skip Connections",
   "original": "10025",
   "page_count": 5,
   "order": 39,
   "p1": 191,
   "pn": 195,
   "abstract": [
    "Capturing the contextual information in multi-scale is known to be beneficial for improving the performance of DNN-based speech enhancement (SE) models. This paper proposes a new SE model, called NUNet-TLS, having two-level skip connections between the residual U-Blocks nested in each layer of a large U-Net structure. The proposed model also has a causal time-frequency attention (CFTA) at the output of the residual U-Block to boost dynamic representation of the speech context in multi-scale. Even having the two-level skip connections, the proposed model slightly increases the network parameters, but the performance improvement is significant. Experimental results show that the proposed NUNet-TLS has superior performance in various objective evaluation metrics to other state-of-the-art models. The code of our model is available at https://github.com/seorim0/NUNet-TLS"
   ],
   "doi": "10.21437/Interspeech.2022-10025"
  },
  "larsen22_interspeech": {
   "authors": [
    [
     "Claus",
     "Larsen"
    ],
    [
     "Peter",
     "Koch"
    ],
    [
     "Zheng-Hua",
     "Tan"
    ]
   ],
   "title": "Adversarial Multi-Task Deep Learning for Noise-Robust Voice Activity Detection with Low Algorithmic Delay",
   "original": "10033",
   "page_count": 5,
   "order": 762,
   "p1": 3759,
   "pn": 3763,
   "abstract": [
    "Voice Activity Detection (VAD) is an important pre-processing step in a wide variety of speech processing systems. VAD should in a practical application be able to detect speech in both noisy and noise-free environments, while not introducing significant latency. In this work we propose to introduce an adversarial multi-task learning method when training a supervised VAD. The method has been applied to the state-of-the-art VAD Waveform-based Voice Activity Detection. Additionally the performance of the VAD is investigated under different algorithmic delays, which is an important factor in latency. Introducing adversarial multi-task learning to the model is observed to increase performance in terms of Area Under Curve (AUC), particularly in noisy environments, while the performance is not degraded at higher SNR levels. The adversarial multi-task learning is only applied in the training phase and thus introduces no additional cost in testing. Furthermore the correlation between performance and algorithmic delays is investigated, and it is observed that the VAD performance degradation is only moderate when lowering the algorithmic delay from 398 ms to 23 ms."
   ],
   "doi": "10.21437/Interspeech.2022-10033"
  },
  "xu22f_interspeech": {
   "authors": [
    [
     "Xinmeng",
     "Xu"
    ],
    [
     "Yang",
     "Wang"
    ],
    [
     "Jie",
     "Jia"
    ],
    [
     "Binbin",
     "Chen"
    ],
    [
     "Jianjun",
     "Hao"
    ]
   ],
   "title": "GLD-Net: Improving Monaural Speech Enhancement by Learning Global and Local Dependency Features with GLD Block",
   "original": "10034",
   "page_count": 5,
   "order": 197,
   "p1": 966,
   "pn": 970,
   "abstract": [
    "For monaural speech enhancement, contextual information is important for accurate speech estimation. However, commonly used convolution neural networks (CNNs) are weak in capturing temporal contexts since they only build blocks that process one local neighborhood at a time. To address this problem, we learn from human auditory perception to introduce a two-stage trainable reasoning mechanism, referred as Global-Local Dependency (GLD) block. GLD blocks capture long-term dependency of time-frequency bins both in global level and local level from the noisy spectrogram to help detecting correlations among speech part, noise part, and whole noisy input. What is more, we conduct a monaural speech enhancement network called GLD-Net, which adopts encoder-decoder architecture and consists of speech object branch, interference branch, and global noisy branch. The extracted speech feature at global-level and local-level are efficiently reasoned and aggregated in each of the branches. We have compared the proposed GLD-Net with existing state-of-art methods on WSJ0 and DEMAND dataset. The results have shown that GLD-Net outperforms the state-of-the-art methods in terms of PESQ and STOI."
   ],
   "doi": "10.21437/Interspeech.2022-10034"
  },
  "shao22_interspeech": {
   "authors": [
    [
     "Nian",
     "Shao"
    ],
    [
     "Erfan",
     "Loweimi"
    ],
    [
     "Xiaofei",
     "Li"
    ]
   ],
   "title": "RCT: Random consistency training for semi-supervised sound event detection",
   "original": "10037",
   "page_count": 5,
   "order": 313,
   "p1": 1541,
   "pn": 1545,
   "abstract": [
    "Sound event detection (SED), as a core module of acoustic environmental analysis, suffers from the problem of data deficiency. The integration of semi-supervised learning (SSL) largely mitigates such problem. This paper researches on several core modules of SSL, and introduces a random consistency training (RCT) strategy. First, a hard mixup data augmentation is proposed to account for the additive property of sounds. Second, a random augmentation scheme is applied to stochastically combine different types of data augmentation methods with high flexibility. Third, a self-consistency loss is proposed to be fused with the teacher-student model, aiming at stabilizing the training. Performance-wise, the proposed modules outperform their respective competitors, and as a whole the proposed SED strategies achieve 44.0% and 67.1% in terms of the PSDS_1 and PSDS_2 metrics proposed by the DCASE challenge, which notably outperforms other widely-used alternatives."
   ],
   "doi": "10.21437/Interspeech.2022-10037"
  },
  "shi22d_interspeech": {
   "authors": [
    [
     "Jiatong",
     "Shi"
    ],
    [
     "Shuai",
     "Guo"
    ],
    [
     "Tao",
     "Qian"
    ],
    [
     "Tomoki",
     "Hayashi"
    ],
    [
     "Yuning",
     "Wu"
    ],
    [
     "Fangzheng",
     "Xu"
    ],
    [
     "Xuankai",
     "Chang"
    ],
    [
     "Huazhe",
     "Li"
    ],
    [
     "Peter",
     "Wu"
    ],
    [
     "Shinji",
     "Watanabe"
    ],
    [
     "Qin",
     "Jin"
    ]
   ],
   "title": "Muskits: an End-to-end Music Processing Toolkit for Singing Voice Synthesis",
   "original": "10039",
   "page_count": 5,
   "order": 867,
   "p1": 4277,
   "pn": 4281,
   "abstract": [
    "This paper introduces a new open-source platform named Muskits, for end-to-end music processing, which mainly focuses on end-to-end singing voice synthesis (E2E-SVS). Muskits supports state-of-the-art SVS models, including RNN SVS, transformer SVS, and XiaoiceSing. The design of Muskits follows the style of widely-used speech processing toolkits, ESPnet and Kaldi, for data prepossessing, training, and recipe pipelines. To the best of our knowledge, this toolkit is the first platform that allows a fair and highly-reproducible comparison between several published works in SVS. In addition, we also demonstrate several advanced usages based on the toolkit functionalities, including multilingual training and transfer learning. This paper describes the major framework of Muskits, its functionalities, and experimental results in single-singer, multi-singer, multilingual, and transfer learning scenarios. The toolkit is publicly available at https://github.com/SJTMusicTeam/Muskits."
   ],
   "doi": "10.21437/Interspeech.2022-10039"
  },
  "xu22g_interspeech": {
   "authors": [
    [
     "Xinmeng",
     "Xu"
    ],
    [
     "Yang",
     "Wang"
    ],
    [
     "Jie",
     "Jia"
    ],
    [
     "Binbin",
     "Chen"
    ],
    [
     "Dejun",
     "Li"
    ]
   ],
   "title": "Improving Visual Speech Enhancement Network by Learning Audio-visual Affinity with Multi-head Attention",
   "original": "10041",
   "page_count": 5,
   "order": 198,
   "p1": 971,
   "pn": 975,
   "abstract": [
    "Audio-visual speech enhancement system is regarded as one of the promising solutions for isolating and enhancing the speech of the desired speaker. Typical methods focus on predicting clean speech spectrum via a naive convolution neural network-based encoder-decoder architecture, and these methods a) are not adequate to use data fully, b) are unable to effectively balance audio-visual features. The proposed model alleviates these drawbacks by a) applying a model that fuses audio and visual features layer by layer in the encoding phase, that feeds fused audio-visual features to each corresponding decoder layer, and more importantly, b) introducing 2-stage multi-head cross attention (MHCA) mechanism to infer audio-visual speech enhancement for balancing the fused audio-visual features and eliminating irrelevant features. This paper proposes an attentional audio-visual multi-layer feature fusion model, in which MHCA units are applied to feature mapping at every layer of the decoder. The proposed model demonstrates the superior performance of the network against the state-of-the-art models. Speech samples are available at: https://XinmengXu.github.io/AVSE/AVCRN.html"
   ],
   "doi": "10.21437/Interspeech.2022-10041"
  },
  "violeta22_interspeech": {
   "authors": [
    [
     "Lester Phillip",
     "Violeta"
    ],
    [
     "Wen Chin",
     "Huang"
    ],
    [
     "Tomoki",
     "Toda"
    ]
   ],
   "title": "Investigating Self-supervised Pretraining Frameworks for Pathological Speech Recognition",
   "original": "10043",
   "page_count": 5,
   "order": 9,
   "p1": 41,
   "pn": 45,
   "abstract": [
    "We investigate the performance of self-supervised pretraining frameworks on pathological speech datasets used for automatic speech recognition (ASR). Modern end-to-end models require thousands of hours of data to train well, but only a small number of pathological speech datasets are publicly available. A proven solution to this problem is by first pretraining the model on a huge number of healthy speech datasets and then fine-tuning it on the pathological speech datasets. One new pretraining framework called self-supervised learning (SSL) trains a network using only speech data, providing more flexibility in training data requirements and allowing more speech data to be used in pretraining. We investigate SSL frameworks such as the wav2vec 2.0 and WavLM models using different setups and compare their performance with different supervised pretraining setups, using two types of pathological speech, namely, Japanese electrolaryngeal and English dysarthric. Our results show that although SSL has shown success with minimally resourced healthy speech, we do not find this to be the case with pathological speech. The best supervised setup outperforms the best SSL setup by 13.9% character error rate in electrolaryngeal speech and 16.8% word error rate in dysarthric speech."
   ],
   "doi": "10.21437/Interspeech.2022-10043"
  },
  "levkovitch22_interspeech": {
   "authors": [
    [
     "Alon",
     "Levkovitch"
    ],
    [
     "Eliya",
     "Nachmani"
    ],
    [
     "Lior",
     "Wolf"
    ]
   ],
   "title": "Zero-Shot Voice Conditioning for Denoising Diffusion TTS Models",
   "original": "10045",
   "page_count": 5,
   "order": 605,
   "p1": 2983,
   "pn": 2987,
   "abstract": [
    "We present a novel way of conditioning a pretrained denoising diffusion speech model to produce speech in the voice of a novel person unseen during training. The method requires a short (~3 seconds) sample from the target person, and generation is steered at inference time, without any training steps. At the heart of the method lies a sampling process that combines the estimation of the denoising model with a low-pass version of the new speaker's sample. The objective and subjective evaluations show that our sampling method can generate a voice similar to that of the target speaker in terms of frequency, with an accuracy comparable to state-of-the-art methods, and without training."
   ],
   "doi": "10.21437/Interspeech.2022-10045"
  },
  "karakasidis22_interspeech": {
   "authors": [
    [
     "Georgios",
     "Karakasidis"
    ],
    [
     "Tamás",
     "Grósz"
    ],
    [
     "Mikko",
     "Kurimo"
    ]
   ],
   "title": "Comparison and Analysis of New Curriculum Criteria for End-to-End ASR",
   "original": "10046",
   "page_count": 5,
   "order": 14,
   "p1": 66,
   "pn": 70,
   "abstract": [
    "It is common knowledge that the quantity and quality of the training data play a significant role in the creation of a good machine learning model. In this paper, we take it one step further and demonstrate that the way the training examples are arranged is also of crucial importance. Curriculum Learning is built on the observation that organized and structured assimilation of knowledge has the ability to enable faster training and better comprehension. When humans learn to speak, they first try to utter basic phones and then gradually move towards more complex structures such as words and sentences. This methodology is known as Curriculum Learning, and we employ it in the context of Automatic Speech Recognition. We hypothesize that end-to-end models can achieve better performance when provided with an organized training set consisting of examples that exhibit an increasing level of difficulty (i.e. a curriculum). To impose structure on the training set and to define the notion of an easy example, we explored multiple scoring functions that either use feedback from an external neural network or incorporate feedback from the model itself. Empirical results show that with different curriculums we can balance the training times and the network's performance."
   ],
   "doi": "10.21437/Interspeech.2022-10046"
  },
  "leemann22_interspeech": {
   "authors": [
    [
     "Adrian",
     "Leemann"
    ],
    [
     "Péter",
     "Jeszenszky"
    ],
    [
     "Carina",
     "Steiner"
    ],
    [
     "Corinne",
     "Lanthemann"
    ]
   ],
   "title": "Factors affecting the percept of Yanny v. Laurel (or mixed): Insights from a large-scale study on Swiss German listeners",
   "original": "10048",
   "page_count": 5,
   "order": 375,
   "p1": 1851,
   "pn": 1855,
   "abstract": [
    "In May 2018, Yanny v. Laurel went viral: when listening to the same audio clip, some people claimed to hear only Yanny, others insisted it must be Laurel, and some had a mixed percept. Phoneticians have identified the acoustic features which caused this perceptual ambiguity, but we still know little about the factors affecting individuals' perception of the illusion. We conducted a controlled study with 974 Swiss German listeners, balanced for age, gender, and regional origin. Overall, nearly two thirds heard Yanny, one quarter Laurel, and about 12% had a mixed percept. We found age, gender, and electronic device to play a significant role: younger, female, and laptop-using participants demonstrated higher proportions of Yanny responses. These findings contribute to the growing body of research on polyperceivable words."
   ],
   "doi": "10.21437/Interspeech.2022-10048"
  },
  "wang22q_interspeech": {
   "authors": [
    [
     "Dong",
     "Wang"
    ],
    [
     "Yanhui",
     "Ding"
    ],
    [
     "Qing",
     "Zhao"
    ],
    [
     "Peilin",
     "Yang"
    ],
    [
     "Shuping",
     "Tan"
    ],
    [
     "Ya",
     "Li"
    ]
   ],
   "title": "ECAPA-TDNN Based Depression Detection from Clinical Speech",
   "original": "10051",
   "page_count": 5,
   "order": 675,
   "p1": 3333,
   "pn": 3337,
   "abstract": [
    "Depression is a serious mood disorder that has become one of the major diseases that endanger human mental health. The automatic detection of depression using speech signals has become a promising approach for the early diagnosis of depression currently. However, there is still a performance gap between clinical practice and research, considering the lab-recorded corpus was used in most of the current studies. Therefore, we collected a Chinese clinical depression corpus, of which 131 participants with their speech during the Hamilton Rating Scale for Depression (HAMD) interview were included in this study. Furthermore, we developed a depression speech detection system based on a Time-Delay Neural Network (TDNN) model to distinguish depression. Our approach achieves a mean F1 score of 90.8% and an accuracy of 90.4% by five-fold cross-validation. The result suggests that the developed TDNN-based model has a potential clinical meaning in the diagnosis of depression."
   ],
   "doi": "10.21437/Interspeech.2022-10051"
  },
  "sharma22_interspeech": {
   "authors": [
    [
     "Mayank",
     "Sharma"
    ],
    [
     "Tarun",
     "Gupta"
    ],
    [
     "Kenny",
     "Qiu"
    ],
    [
     "Xiang",
     "Hao"
    ],
    [
     "Raffay",
     "Hamid"
    ]
   ],
   "title": "CNN-based Audio Event Recognition for Automated Violence Classification and Rating for Prime Video Content",
   "original": "10053",
   "page_count": 5,
   "order": 560,
   "p1": 2758,
   "pn": 2762,
   "abstract": [
    "Automated violence detection in Digital Entertainment Content (DEC) uses computer vision and natural language processing methods on visual and textual modalities. These methods face difficulty in detecting violence due to diversity, ambiguity and multilingual nature of data. Hence, we introduce a method based on audio to augment existing methods for violence and rating classification. We develop a generic Audio Event Detector model (AED) using open-source and Prime Video proprietary corpora which is used as a feature extractor. Our feature set includes global semantic embedding and sparse local audio event probabilities extracted from AED. We demonstrate that a global-local feature view of audio results in best detection performance. Next, we present a multi-modal detector by fusing several learners across modalities. Our training and evaluation set is also at least an order of magnitude larger than previous literature. Furthermore, we show that, (a) audio based approach results in superior performance compared to other baselines, (b) benefit due to audio model is more pronounced on global multi-lingual data compared to English data and (c) the multi-modal model results in 63% rating accuracy and provides the ability to backfill top 90% Stream Weighted Coverage titles in PV catalog with 88% coverage at 91% accuracy."
   ],
   "doi": "10.21437/Interspeech.2022-10053"
  },
  "zhou22d_interspeech": {
   "authors": [
    [
     "Yixuan",
     "Zhou"
    ],
    [
     "Changhe",
     "Song"
    ],
    [
     "Xiang",
     "Li"
    ],
    [
     "Luwen",
     "Zhang"
    ],
    [
     "Zhiyong",
     "Wu"
    ],
    [
     "Yanyao",
     "Bian"
    ],
    [
     "Dan",
     "Su"
    ],
    [
     "Helen",
     "Meng"
    ]
   ],
   "title": "Content-Dependent Fine-Grained Speaker Embedding for Zero-Shot Speaker Adaptation in Text-to-Speech Synthesis",
   "original": "10054",
   "page_count": 5,
   "order": 523,
   "p1": 2573,
   "pn": 2577,
   "abstract": [
    "Zero-shot speaker adaptation aims to clone an unseen speaker's voice without any adaptation time and parameters. Previous researches usually use a speaker encoder to extract a global fixed speaker embedding from reference speech, and several attempts have tried variable-length speaker embedding. However, they neglect to transfer the personal pronunciation characteristics related to phoneme content, leading to poor speaker similarity in terms of detailed speaking styles and pronunciation habits. To improve the ability of the speaker encoder to model personal pronunciation characteristics, we propose content-dependent fine-grained speaker embedding for zero-shot speaker adaptation. The corresponding local content embeddings and speaker embeddings are extracted from a reference speech, respectively. Instead of modeling the temporal relations, a reference attention module is introduced to model the content relevance between the reference speech and the input text, and to generate the fine-grained speaker embedding for each phoneme encoder output. The experimental results show that our proposed method can improve speaker similarity of synthesized speeches, especially for unseen speakers."
   ],
   "doi": "10.21437/Interspeech.2022-10054"
  },
  "wang22r_interspeech": {
   "authors": [
    [
     "Qiongqiong",
     "Wang"
    ],
    [
     "Kong Aik",
     "Lee"
    ],
    [
     "Tianchi",
     "Liu"
    ]
   ],
   "title": "Scoring of Large-Margin Embeddings for Speaker Verification: Cosine or PLDA?",
   "original": "10055",
   "page_count": 5,
   "order": 121,
   "p1": 600,
   "pn": 604,
   "abstract": [
    "The emergence of large-margin softmax cross-entropy losses in training deep speaker embedding neural networks has triggered a gradual shift from parametric back-ends to a simpler cosine similarity measure for speaker verification. Popular parametric back-ends include the probabilistic linear discriminant analysis (PLDA) and its variants. This paper investigates the properties of margin-based cross-entropy losses leading to such a shift and aims to find scoring back-ends best suited for speaker verification. In addition, we revisit the pre-processing techniques which have been widely used in the past and assess their effectiveness on large-margin embeddings. Experiments on the state-of-the-art ECAPA-TDNN networks trained with various large-margin softmax cross-entropy losses show a substantial increment in intra-speaker compactness making the conventional PLDA superfluous. In this regard, we found that constraining the within-speaker covariance matrix could improve the performance of the PLDA. It is demonstrated through a series of experiments on the VoxCeleb-1 and SITW core-core test sets with 40.8% equal error rate (EER) reduction and 35.1% minimum detection cost (minDCF) reduction. It also outperforms cosine scoring consistently with reductions in EER and minDCF by 10.9% and 4.9%, respectively."
   ],
   "doi": "10.21437/Interspeech.2022-10055"
  },
  "xin22_interspeech": {
   "authors": [
    [
     "Yifei",
     "Xin"
    ],
    [
     "Dongchao",
     "Yang"
    ],
    [
     "Yuexian",
     "Zou"
    ]
   ],
   "title": "Audio Pyramid Transformer with Domain Adaption for Weakly Supervised Sound Event Detection and Audio Classification",
   "original": "10057",
   "page_count": 5,
   "order": 314,
   "p1": 1546,
   "pn": 1550,
   "abstract": [
    "Recently, the Transformer-based model has been applied to sound event detection and audio classification tasks. However, when processing the audio spectrogram on a fine-grained scale, the computational cost is still high even with a hierarchical structure. In this paper, we introduce APT: an audio pyramid transformer with quadtree attention to reduce the computational complexity from quadratic to linear. Besides, most previous methods for weakly supervised sound event detection (WSSED) utilize the multi-instance learning (MIL) mechanism. However, MIL focuses more on the accuracy of bags (clips) rather than the instances (frames), so it tends to localize the most distinct part but not the whole sound event. To solve this problem, we provide a novel perspective that models WSSED as a domain adaption (DA) task, where the weights of the classifier trained on the source (clip) domain are shared to the target (frame) domain to enhance localization performance. Furthermore, we introduce a DAD (domain adaption detection) loss to align the feature distribution between frame and clip domain and make the classifier perceive frame domain information better. Experiments show that our APT achieves new state-of-the-art (SOTA) results on AudioSet, DCASE2017 and Urban-SED datasets. Moreover, our DA-WSSED pipeline significantly outperforms the MIL-based WSSED method."
   ],
   "doi": "10.21437/Interspeech.2022-10057"
  },
  "feng22b_interspeech": {
   "authors": [
    [
     "Tiantian",
     "Feng"
    ],
    [
     "Raghuveer",
     "Peri"
    ],
    [
     "Shrikanth",
     "Narayanan"
    ]
   ],
   "title": "User-Level Differential Privacy against Attribute Inference Attack of Speech Emotion Recognition on Federated Learning",
   "original": "10060",
   "page_count": 5,
   "order": 1023,
   "p1": 5055,
   "pn": 5059,
   "abstract": [
    "Many existing privacy-enhanced speech emotion recognition (SER) frameworks focus on perturbing the original speech data through adversarial training within a centralized machine learning setup. However, this privacy protection scheme can fail since the adversary can still access the perturbed data. In recent years, distributed learning algorithms, especially federated learning (FL), have gained popularity to protect privacy in machine learning applications. While FL provides good intuition to safeguard privacy by keeping the data on local devices, prior work has shown that privacy attacks, such as attribute inference attacks, are achievable for SER systems trained using FL. In this work, we propose to evaluate the user-level differential privacy (UDP) in mitigating the privacy leaks of the SER system in FL. UDP provides theoretical privacy guarantees with privacy parameters $\\epsilon$ and $\\delta$. Our results show that the UDP can effectively decrease attribute information leakage while keeping the utility of the SER system with the adversary accessing one model update. However, the efficacy of the UDP suffers when the FL system leaks more model updates to the adversary. We make the code publicly available to reproduce the results in \\href{https://github.com/usc-sail/fed-ser-leakage}{https://github.com/usc-sail/fed-ser-leakage}."
   ],
   "doi": "10.21437/Interspeech.2022-10060"
  },
  "zhou22e_interspeech": {
   "authors": [
    [
     "Yixuan",
     "Zhou"
    ],
    [
     "Changhe",
     "Song"
    ],
    [
     "Jingbei",
     "Li"
    ],
    [
     "Zhiyong",
     "Wu"
    ],
    [
     "Yanyao",
     "Bian"
    ],
    [
     "Dan",
     "Su"
    ],
    [
     "Helen",
     "Meng"
    ]
   ],
   "title": "Enhancing Word-Level Semantic Representation via Dependency Structure for Expressive Text-to-Speech Synthesis",
   "original": "10061",
   "page_count": 5,
   "order": 1118,
   "p1": 5518,
   "pn": 5522,
   "abstract": [
    "Exploiting rich linguistic information in raw text is crucial for expressive text-to-speech (TTS). As large scale pre-trained text representation develops, bidirectional encoder representations from Transformers (BERT) has been proven to embody semantic information and employed to TTS recently. However, original or simply fine-tuned BERT embeddings still cannot provide sufficient semantic knowledge that expressive TTS models should take into account. In this paper, we propose a word-level semantic representation enhancing method based on dependency structure and pre-trained BERT embedding. The BERT embedding of each word is reprocessed considering its specific dependencies and related words in the sentence, to generate more effective semantic representation for TTS. To better utilize the dependency structure, relational gated graph network (RGGN) is introduced to make semantic information flow and aggregate through the dependency structure. The experimental results show that the proposed method can further improve the naturalness and expressiveness of synthesized speeches on both Mandarin and English datasets."
   ],
   "doi": "10.21437/Interspeech.2022-10061"
  },
  "futami22_interspeech": {
   "authors": [
    [
     "Hayato",
     "Futami"
    ],
    [
     "Hirofumi",
     "Inaguma"
    ],
    [
     "Sei",
     "Ueno"
    ],
    [
     "Masato",
     "Mimura"
    ],
    [
     "Shinsuke",
     "Sakai"
    ],
    [
     "Tatsuya",
     "Kawahara"
    ]
   ],
   "title": "Non-autoregressive Error Correction for CTC-based ASR with Phone-conditioned Masked LM",
   "original": "10062",
   "page_count": 5,
   "order": 788,
   "p1": 3889,
   "pn": 3893,
   "abstract": [
    "Connectionist temporal classification (CTC) -based models are attractive in automatic speech recognition (ASR) because of their non-autoregressive nature. To take advantage of text-only data, language model (LM) integration approaches such as rescoring and shallow fusion have been widely used for CTC. However, they lose CTC's non-autoregressive nature because of the need for beam search, which slows down the inference speed. In this study, we propose an error correction method with phone-conditioned masked LM (PC-MLM). In the proposed method, less confident word tokens in a greedy decoded output from CTC are masked. PC-MLM then predicts these masked word tokens given unmasked words and phones supplementally predicted from CTC. We further extend it to Deletable PC-MLM in order to address insertion errors. Since both CTC and PC-MLM are non-autoregressive models, the method enables fast LM integration. Experimental evaluations on the Corpus of Spontaneous Japanese (CSJ) and TED-LIUM2 in domain adaptation setting shows that our proposed method outperformed rescoring and shallow fusion in terms of inference speed, and also in terms of recognition accuracy on CSJ."
   ],
   "doi": "10.21437/Interspeech.2022-10062"
  },
  "lee22i_interspeech": {
   "authors": [
    [
     "Jae-Hong",
     "Lee"
    ],
    [
     "Chae-Won",
     "Lee"
    ],
    [
     "Jin-Seong",
     "Choi"
    ],
    [
     "Joon-Hyuk",
     "Chang"
    ],
    [
     "Woo Kyeong",
     "Seong"
    ],
    [
     "Jeonghan",
     "Lee"
    ]
   ],
   "title": "CTRL: Continual Representation Learning to Transfer Information of Pre-trained for WAV2VEC 2.0",
   "original": "10063",
   "page_count": 5,
   "order": 688,
   "p1": 3398,
   "pn": 3402,
   "abstract": [
    "Representation models such as WAV2VEC 2.0 (W2V2) show remarkable speech recognition performance by pre-training only on unlabeled datasets and finetuning on a small amount of labeled dataset. It is crucial to train on datasets of multiple domains to obtain a richer representation of such a model. The conventional approach used for handling multiple domains is training a model on a merged dataset from scratch. However, representation learning requires excessive computation for pre-training, which becomes a severe problem as the size of the dataset increases. In this study, we present continual representation learning (CTRL), a framework that leverages continual learning methods to continually retrain the pre-trained representation model while transferring information of the previous model without the historical dataset. The framework conducts continual pre-training for pre-trained W2V2 using the redesigned continual learning method for self-supervised learning. To evaluate our framework, we continually pre-train W2V2 with CTRL in the following order: Librispeech, Wall Street Journal, and TED-LIUM V3. The results demonstrate that the proposed approach improves the speech recognition performance of all three datasets compared with that of baseline W2V2 pre-trained on Librispeech."
   ],
   "doi": "10.21437/Interspeech.2022-10063"
  },
  "chaubey22_interspeech": {
   "authors": [
    [
     "Ashutosh",
     "Chaubey"
    ],
    [
     "Sparsh",
     "Sinha"
    ],
    [
     "Susmita",
     "Ghose"
    ]
   ],
   "title": "Improved Relation Networks for End-to-End Speaker Verification and Identification",
   "original": "10064",
   "page_count": 5,
   "order": 1029,
   "p1": 5085,
   "pn": 5089,
   "abstract": [
    "Speaker identification systems in a real-world scenario are tasked to identify a speaker amongst a set of enrolled speakers given just a few samples for each enrolled speaker. This paper demonstrates the effectiveness of meta-learning and relation networks for this use case. We propose improved relation networks for speaker verification and few-shot (unseen) speaker identification. The use of relation networks facilitates joint training of the frontend speaker encoder and the backend model. Inspired by the use of prototypical networks in speaker verification and to increase the discriminability of the speaker embeddings, we train the model to classify samples in the current episode amongst all speakers present in the training set. Furthermore, we propose a new training regime for faster model convergence by extracting more information from a given meta-learning episode with negligible extra computation. We evaluate the proposed techniques on VoxCeleb, SITW and VCTK datasets on the tasks of speaker verification and unseen speaker identification. The proposed approach outperforms the existing approaches consistently on both tasks."
   ],
   "doi": "10.21437/Interspeech.2022-10064"
  },
  "wei22c_interspeech": {
   "authors": [
    [
     "Kun",
     "Wei"
    ],
    [
     "Pengcheng",
     "Guo"
    ],
    [
     "Ning",
     "Jiang"
    ]
   ],
   "title": "Improving Transformer-based Conversational ASR by Inter-Sentential Attention Mechanism",
   "original": "10066",
   "page_count": 5,
   "order": 771,
   "p1": 3804,
   "pn": 3808,
   "abstract": [
    "Transformer-based models have demonstrated their effectiveness in automatic speech recognition (ASR) tasks and even shown superior performance over the conventional hybrid framework. The main idea of Transformers is to capture the long-range global context within an utterance by self-attention layers. However, for scenarios like conversational speech, such utterance-level modeling will neglect contextual dependencies that span across utterances. In this paper, we propose to explicitly model the inter-sentential information in a Transformer based end-to-end architecture for conversational speech recognition. Specifically, for the encoder network, we capture the contexts of previous speech and incorporate such historic information into current input by a context-aware residual attention mechanism. For the decoder, the prediction of current utterance is also conditioned on the historic linguistic information through a conditional decoder framework. We show the effectiveness of our proposed method on several open-source dialogue corpora and the proposed method consistently improved the performance from the utterance-level Transformer-based ASR models."
   ],
   "doi": "10.21437/Interspeech.2022-10066"
  },
  "yang22m_interspeech": {
   "authors": [
    [
     "Zhanheng",
     "Yang"
    ],
    [
     "Hang",
     "Lv"
    ],
    [
     "Xiong",
     "Wang"
    ],
    [
     "Ao",
     "Zhang"
    ],
    [
     "Lei",
     "Xie"
    ]
   ],
   "title": "Minimizing Sequential Confusion Error in Speech Command Recognition",
   "original": "10067",
   "page_count": 5,
   "order": 647,
   "p1": 3193,
   "pn": 3197,
   "abstract": [
    "Speech command recognition (SCR) has been commonly used on resource constrained devices to achieve hands-free user experience. However, in real applications, confusion among commands with similar pronunciations often happens due to the limited capacity of small models deployed on edge devices, which drastically affects the user experience. In this paper, inspired by the advances of discriminative training in speech recognition, we propose a novel minimize sequential confusion error (MSCE) training criterion particularly for SCR, aiming to alleviate the command confusion problem. Specifically, we aim to improve the ability of discriminating the target command from other commands on the basis of MCE discriminative criteria. We define the likelihood of different commands through connectionist temporal classification (CTC). During training, we propose several strategies to use prior knowledge creating a confusing sequence set for similar-sounding command instead of creating the whole non-target command set, which can better save the training resources and effectively reduce command confusion errors. Specifically, we design and compare three different strategies for confusing set construction. By using our proposed method, we can relatively reduce the False Reject Rate~(FRR) by 33.7% at 0.01 False Alarm Rate~(FAR) and confusion errors by 18.28% on our collected speech command set."
   ],
   "doi": "10.21437/Interspeech.2022-10067"
  },
  "chen22h_interspeech": {
   "authors": [
    [
     "Kun",
     "Chen"
    ],
    [
     "Jun",
     "Wang"
    ],
    [
     "Feng",
     "Deng"
    ],
    [
     "Xiaorui",
     "Wang"
    ]
   ],
   "title": "iCNN-Transformer: An improved CNN-Transformer with Channel-spatial Attention and Keyword Prediction for Automated Audio Captioning",
   "original": "10073",
   "page_count": 5,
   "order": 845,
   "p1": 4167,
   "pn": 4171,
   "abstract": [
    "Automated audio captioning (AAC) is a task to generates text description of an audio recording. Sound events, acoustic scene and the relationship between events are described in audio captions. Currently, most AAC systems are based on encoder-decoder architecture, in which the decoder predicts the caption completely according to the features extracted from the audio clip. As a result, learning more efficient audio features allows the decoder to generate more appropriate descriptions. This paper proposes an approach to guide the generation of captioning by multi-level information extracted from audio clip. Specifically, we use two modules to obtain acoustic information for semantic expression. (1) A module that combines channel attention and spatial attention to pay more attention to important features. (2) A trained keyword prediction module to generate word-level guidance information. We apply our modules to the CNN-Transformer architecture and experiment in Clotho. The results show that the proposed approach can significantly improve the scores of various evaluation metrics and achieve the state-of-the-art performance in the Cross-entropy training stage."
   ],
   "doi": "10.21437/Interspeech.2022-10073"
  },
  "saeki22d_interspeech": {
   "authors": [
    [
     "Mao",
     "Saeki"
    ],
    [
     "Kotoka",
     "Miyagi"
    ],
    [
     "Shinya",
     "Fujie"
    ],
    [
     "Shungo",
     "Suzuki"
    ],
    [
     "Tetsuji",
     "Ogawa"
    ],
    [
     "Tetsunori",
     "Kobayashi"
    ],
    [
     "Yoichi",
     "Matsuyama"
    ]
   ],
   "title": "Confusion Detection for Adaptive Conversational Strategies of An Oral Proficiency Assessment Interview Agent",
   "original": "10075",
   "page_count": 5,
   "order": 809,
   "p1": 3988,
   "pn": 3992,
   "abstract": [
    "In this study, we present a model to detect user confusion in an online interview dialogue using conversational agents. Conversational agents have gained attention for reliable assessment of language learners' oral skills in interviews. Learners often face confusion, where they fail to understand what the system has said, and may end up unable to respond, leading to a conversational breakdown. It is thus crucial for the system to detect such a state and keep the interview going forward by repeating or rephrasing the previous system utterance. To this end, we first collected a dataset of user confusion using a psycholinguistic experimental approach and identified seven multimodal signs of confusion, some of which were unique to an online conversation. With the corresponding features, we trained a classification model of user confusion. An ablation study showed that the features related to self-talk and gaze direction were most predictive. We discuss how this model can assist a conversational agent to detect and resolve user confusion in real-time."
   ],
   "doi": "10.21437/Interspeech.2022-10075"
  },
  "han22c_interspeech": {
   "authors": [
    [
     "Chang",
     "Han"
    ],
    [
     "Weiping",
     "Tu"
    ],
    [
     "Yuhong",
     "Yang"
    ],
    [
     "Jingyi",
     "Li"
    ],
    [
     "Xinhong",
     "Li"
    ]
   ],
   "title": "Speaker- and Phone-aware Convolutional Transformer Network for Acoustic Echo Cancellation",
   "original": "10077",
   "page_count": 5,
   "order": 511,
   "p1": 2513,
   "pn": 2517,
   "abstract": [
    "Recent studies indicate the effectiveness of deep learning (DL) based methods for acoustic echo cancellation (AEC) in background noise and nonlinear distortion scenarios. However, content and speaker variations degrade the performance of such DL-based AEC models. In this study, we propose a AEC model that takes phonetic and speaker identities features as auxiliary inputs, and present a complex dual-path convolutional transformer network (DPCTNet). Given an input signal, the phonetic and speaker identities features extracted by the contrastive predictive coding network that is a self-supervised pretraining model, and the complex spectrum generated by short time Fourier transform are treated as the spectrum pattern inputs for DPCTNet. In addition, the DPCTNet applies an encoder-decoder architecture improved by inserting a dual-path transformer to effectively model the extracted inputs in a single frame and the dependence between consecutive frames. Comparative experimental results showed that the performance of AEC can be improved by explicitly considering phonetic and speaker identities features."
   ],
   "doi": "10.21437/Interspeech.2022-10077"
  },
  "kawa22_interspeech": {
   "authors": [
    [
     "Piotr",
     "Kawa"
    ],
    [
     "Marcin",
     "Plata"
    ],
    [
     "Piotr",
     "Syga"
    ]
   ],
   "title": "Attack Agnostic Dataset: Towards Generalization and Stabilization of Audio DeepFake Detection",
   "original": "10078",
   "page_count": 5,
   "order": 816,
   "p1": 4023,
   "pn": 4027,
   "abstract": [
    "Audio DeepFakes allow the creation of high-quality, convincing utterances and therefore pose a threat due to its potential applications such as impersonation or fake news. Methods for detecting these manipulations should be characterized by good generalization and stability leading to robustness against attacks conducted with techniques that are not explicitly included in the training. In this work, we introduce Attack Agnostic Dataset - a combination of two audio DeepFakes and one anti-spoofing datasets that, thanks to the disjoint use of attacks, can lead to better generalization of detection methods. We present a thorough analysis of current DeepFake detection methods and consider different audio features (front-ends). In addition, we propose a model based on LCNN with LFCC and mel-spectrogram front-end, which not only is characterized by a good generalization and stability results but also shows improvement over LFCC-based mode - we decrease standard deviation on all folds and EER in two folds by up to 5%."
   ],
   "doi": "10.21437/Interspeech.2022-10078"
  },
  "liu22o_interspeech": {
   "authors": [
    [
     "Wenjing",
     "Liu"
    ],
    [
     "Chuan",
     "Xie"
    ]
   ],
   "title": "MOS Prediction Network for Non-intrusive Speech Quality Assessment in Online Conferencing",
   "original": "10081",
   "page_count": 5,
   "order": 667,
   "p1": 3293,
   "pn": 3297,
   "abstract": [
    "Speech quality is a major indicator of the quality of service that describes the performance of speech communication network. Intrusive speech quality assessment generally requires a clean reference speech for evaluation, which is not available in applications such as online conferencing. Although the subjective measure of Mean opinion score (MOS) is widely used for assessing speech quality, the process of MOS test is time-consuming and expensive. In this paper, we propose a MOS prediction network for non-intrusive speech quality assessment in online conferencing, which consists of acoustic encoder, time-dependency network and prediction network with specific design. Accommodated with the large-scale dataset including MOS annotations from Interspeech ConferencingSpeech 2022 Challenge for supervised training, the proposed model is capable of predicting the MOS score of the degraded speech in the automatic manner, without the need for human judges and clean reference speech. Our results show that the proposed model is competitive over the baseline method of the challenge in all evaluation metrics."
   ],
   "doi": "10.21437/Interspeech.2022-10081"
  },
  "jiang22_interspeech": {
   "authors": [
    [
     "Xue",
     "Jiang"
    ],
    [
     "Xiulian",
     "Peng"
    ],
    [
     "Huaying",
     "Xue"
    ],
    [
     "Yuan",
     "Zhang"
    ],
    [
     "Yan",
     "Lu"
    ]
   ],
   "title": "Cross-Scale Vector Quantization for Scalable Neural Speech Coding",
   "original": "10084",
   "page_count": 5,
   "order": 856,
   "p1": 4222,
   "pn": 4226,
   "abstract": [
    "Bitrate scalability is a desirable feature for audio coding in real-time communications. Existing neural audio codecs usually enforce a specific bitrate during training, so different models need to be trained for each target bitrate, which increases the memory footprint at the sender and the receiver side and transcoding is often needed to support multiple receivers. In this paper, we introduce a cross-scale scalable vector quantization scheme (CSVQ), in which multi-scale features are encoded progressively with stepwise feature fusion and refinement. In this way, a coarse-level signal is reconstructed if only a portion of the bitstream is received, and progressively improves the quality as more bits are available. The proposed CSVQ scheme can be flexibly applied to any neural audio coding network with a mirrored auto-encoder structure to achieve bitrate scalability. Subjective results show that the proposed scheme outperforms the classical residual VQ (RVQ) with scalability. Moreover, the proposed CSVQ at 3 kbps outperforms Opus at 9 kbps and Lyra at 3kbps and it could provide a graceful quality boost with bitrate increase."
   ],
   "doi": "10.21437/Interspeech.2022-10084"
  },
  "li22o_interspeech": {
   "authors": [
    [
     "Kai",
     "Li"
    ],
    [
     "Sheng",
     "Li"
    ],
    [
     "Xugang",
     "Lu"
    ],
    [
     "Masato",
     "Akagi"
    ],
    [
     "Meng",
     "Liu"
    ],
    [
     "Lin",
     "Zhang"
    ],
    [
     "Chang",
     "Zeng"
    ],
    [
     "Longbiao",
     "Wang"
    ],
    [
     "Jianwu",
     "Dang"
    ],
    [
     "Masashi",
     "Unoki"
    ]
   ],
   "title": "Data Augmentation Using McAdams-Coefficient-Based Speaker Anonymization for Fake Audio Detection",
   "original": "10088",
   "page_count": 5,
   "order": 134,
   "p1": 664,
   "pn": 668,
   "abstract": [
    "Fake audio detection (FAD) is a technique to distinguish synthetic speech from natural speech. In most FAD systems, removing irrelevant features from acoustic speech while keeping only robust discriminative features is essential. Intuitively, speaker information entangled in acoustic speech should be suppressed for the FAD task. Particularly in a deep neural network (DNN)-based FAD system, the learning system may learn speaker information from a training dataset and cannot generalize well on a testing dataset. In this paper, we propose to use the speaker anonymization (SA) technique to suppress speaker information from acoustic speech before inputting it into a DNN-based FAD system. We adopted the McAdams-coefficient-based SA (MC-SA) algorithm, and this is expected that the entangled speaker information will not be involved in the DNN-based FAD learning. Based on this idea, we implemented a light convolutional neural network bidirectional long short-term memory (LCNN-BLSTM)-based FAD system and conducted experiments on the Audio Deep Synthesis Detection Challenge (ADD2022) datasets. The results showed that removing the speaker information from acoustic speech improved the relative performance in the first track of ADD2022 by 17.66%."
   ],
   "doi": "10.21437/Interspeech.2022-10088"
  },
  "aminidigehsara22_interspeech": {
   "authors": [
    [
     "Pouriya",
     "Amini Digehsara"
    ],
    [
     "João Vítor",
     "Possamai de Menezes"
    ],
    [
     "Christoph",
     "Wagner"
    ],
    [
     "Michael",
     "Bärhold"
    ],
    [
     "Petr",
     "Schaffer"
    ],
    [
     "Dirk",
     "Plettemeier"
    ],
    [
     "Peter",
     "Birkholz"
    ]
   ],
   "title": "A user-friendly headset for radar-based silent speech recognition",
   "original": "10090",
   "page_count": 5,
   "order": 979,
   "p1": 4835,
   "pn": 4839,
   "abstract": [
    "Silent speech interfaces allow speech communication to take place in the absence of the acoustic speech signal. Radar-based sensing with radio antennas on the speakers' face can be used as a non-invasive modality to measure speech articulation in such applications. One of the major challenges with this approach is the variability between different sessions, mainly due to the repositioning of the antennas on the face of the speaker. In order to reduce the impact of this influencing factor, we developed a wearable headset that can be 3D-printed with flexible materials and weighs only about 69 g. For evaluation, a radar-based word recognition experiment was performed, where five speakers recorded a speech corpus in multiple sessions, alternatively with the headset and with double-sided tape to place the antennas on the face. By using a bidirectional long short-term memory network for classification, an average intersession word accuracy of 76.50% and 68.18% was obtained using the headset and the tape, respectively. This indicates that the antenna (re-) positioning accuracy with the headset is not worse than that with the double-sided tape while providing other benefits."
   ],
   "doi": "10.21437/Interspeech.2022-10090"
  },
  "coppietersdegibson22_interspeech": {
   "authors": [
    [
     "Louise",
     "Coppieters de Gibson"
    ],
    [
     "Philip N.",
     "Garner"
    ]
   ],
   "title": "Low-Level Physiological Implications of End-to-End Learning for Speech Recognition",
   "original": "10093",
   "page_count": 5,
   "order": 151,
   "p1": 749,
   "pn": 753,
   "abstract": [
    "Current speech recognition architectures perform very well from the point of view of machine learning, hence user interaction. This suggests that they are emulating the human biological system well. We investigate whether the inference can be inverted to provide insights into that biological system; in particular the hearing mechanism. Using SincNet, we confirm that end-to-end systems do learn well known filterbank structures. However, we also show that wider band-width filters are important in the learned structure. Whilst some benefits can be gained by initialising both narrow and wide-band filters, physiological constraints suggest that such filters arise in mid-brain rather than the cochlea. We show that standard machine learning architectures must be modified to allow this process to be emulated neurally."
   ],
   "doi": "10.21437/Interspeech.2022-10093"
  },
  "feng22c_interspeech": {
   "authors": [
    [
     "Lingyun",
     "Feng"
    ],
    [
     "Jianwei",
     "Yu"
    ],
    [
     "Yan",
     "Wang"
    ],
    [
     "Songxiang",
     "Liu"
    ],
    [
     "Deng",
     "Cai"
    ],
    [
     "Haitao",
     "Zheng"
    ]
   ],
   "title": "ASR-Robust Natural Language Understanding on ASR-GLUE dataset",
   "original": "10097",
   "page_count": 5,
   "order": 224,
   "p1": 1101,
   "pn": 1105,
   "abstract": [
    "In recent years, with the increasing demand for voice interface applications, more and more attention has been paid to language understanding in speech systems. These speech-based intelligent systems usually comprise an automatic speech recognition (ASR) component and a natural language understanding (NLU) component which takes the output of the ASR component as input. Despite the rapid development of speech recognition over the past few decades, recognition errors are still inevitable, especially in noisy environments. However, the robustness of natural language understanding (NLU) systems to errors introduced by ASR is under-examined. In this paper, we propose three empirical approaches to improve the robustness of the NLU models. The first one is ASR correction which attempts to make error corrections for the mistranscriptions. The later two methods focus on simulating a noisy training scenario to train more robust NLU models. Extensive experimental results and analyses show that the proposed methods can effectively improve the robustness of NLU models."
   ],
   "doi": "10.21437/Interspeech.2022-10097"
  },
  "arai22_interspeech": {
   "authors": [
    [
     "Takayuki",
     "Arai"
    ],
    [
     "Miho",
     "Yamada"
    ],
    [
     "Megumi",
     "Okusawa"
    ]
   ],
   "title": "Syllable sequence of /a/+/ta/ can be heard as /atta/ in Japanese with visual or tactile cues",
   "original": "10099",
   "page_count": 5,
   "order": 625,
   "p1": 3083,
   "pn": 3087,
   "abstract": [
    "In our previous work, we reported that the word /atta/ with a geminate consonant differs from the syllable sequence /a/+pause+/ta/ in Japanese; specifically, there are formant transitions at the end of the first syllable in /atta/ but not in /a/+pause+/ta/. We also showed that native Japanese speakers perceived /atta/ when a facial video of /atta/ was synchronously played with an audio signal of /a/+pause+/ta/. In that study, we utilized two video clips for the two utterances in which the speaker was asked to control only the timing of the articulatory closing. In that case, there was no guarantee that the videos would be the exactly same except for the timing. Therefore, in the current study, we use a physical model of the human vocal tract with a miniature robot hand unit to achieve articulatory movements for visual cues. We also provide tactile cues to the listener's finger because we want to test whether cues of another modality affect this perception in the same framework. Our findings showed that when either visual or tactile cues were presented with an audio stimulus, listeners more frequently responded that they heard /atta/ compared to audio-only presentations."
   ],
   "doi": "10.21437/Interspeech.2022-10099"
  },
  "getman22_interspeech": {
   "authors": [
    [
     "Yaroslav",
     "Getman"
    ],
    [
     "Ragheb",
     "Al-Ghezi"
    ],
    [
     "Katja",
     "Voskoboinik"
    ],
    [
     "Tamás",
     "Grósz"
    ],
    [
     "Mikko",
     "Kurimo"
    ],
    [
     "Giampiero",
     "Salvi"
    ],
    [
     "Torbjørn",
     "Svendsen"
    ],
    [
     "Sofia",
     "Strömbergsson"
    ]
   ],
   "title": "wav2vec2-based Speech Rating System for Children with Speech Sound Disorder",
   "original": "10103",
   "page_count": 5,
   "order": 732,
   "p1": 3618,
   "pn": 3622,
   "abstract": [
    "Speaking is a fundamental way of communication, developed at a young age. Unfortunately, some children with speech sound disorder struggle to acquire this skill, hindering their ability to communicate efficiently. Speech therapies, which could aid these children in speech acquisition, greatly rely on speech practice trials and accurate feedback about their pronunciations. To enable home therapy and lessen the burden on speech-language pathologists, we need a highly accurate and automatic way of assessing the quality of speech uttered by young children. Our work focuses on exploring the applicability of state-of-the-art self-supervised, deep acoustic models, mainly wav2vec2, for this task. The empirical results highlight that these self-supervised models are superior to traditional approaches and close the gap between machine and human performance."
   ],
   "doi": "10.21437/Interspeech.2022-10103"
  },
  "muckenhirn22_interspeech": {
   "authors": [
    [
     "Hannah",
     "Muckenhirn"
    ],
    [
     "Aleksandr",
     "Safin"
    ],
    [
     "Hakan",
     "Erdogan"
    ],
    [
     "Felix",
     "de Chaumont Quitry"
    ],
    [
     "Marco",
     "Tagliasacchi"
    ],
    [
     "Scott",
     "Wisdom"
    ],
    [
     "John R.",
     "Hershey"
    ]
   ],
   "title": "CycleGAN-based Unpaired Speech Dereverberation",
   "original": "10104",
   "page_count": 5,
   "order": 40,
   "p1": 196,
   "pn": 200,
   "abstract": [
    "Typically, neural network-based speech dereverberation models are trained on paired data, composed of a dry utterance and its corresponding reverberant utterance. The main limitation of this approach is that such models can only be trained on large amounts of data and a variety of room impulse responses when the data is synthetically reverberated, since acquiring real paired data is costly. In this paper we propose a CycleGAN-based approach that enables dereverberation models to be trained on unpaired data. We quantify the impact of using unpaired data by comparing the proposed unpaired model to a paired model with the same architecture and trained on the paired version of the same dataset. We show that the performance of the unpaired model is comparable to the performance of the paired model on two different datasets, according to objective evaluation metrics. Furthermore, we run two subjective evaluations and show that both models achieve comparable subjective quality on the AMI dataset, which was not seen during training."
   ],
   "doi": "10.21437/Interspeech.2022-10104"
  },
  "he22c_interspeech": {
   "authors": [
    [
     "Mao-Kui",
     "He"
    ],
    [
     "Jun",
     "Du"
    ],
    [
     "Chin-Hui",
     "Lee"
    ]
   ],
   "title": "End-to-End Audio-Visual Neural Speaker Diarization",
   "original": "10106",
   "page_count": 5,
   "order": 297,
   "p1": 1461,
   "pn": 1465,
   "abstract": [
    "In this paper, we propose a novel end-to-end neural-network-based audio-visual speaker diarization method. Unlike most existing audio-visual methods, our audio-visual model takes audio features (e.g., FBANKs), multi-speaker lip regions of interest (ROIs), and multi-speaker i-vector embbedings as multimodal inputs. And a set of binary classification output layers produces activities of each speaker. With the finely designed end-to-end structure, the proposed method can explicitly handle the overlapping speech and distinguish between speech and non-speech accurately with multi-modal information. I-vectors are the key point to solve the alignment problem caused by visual modality error (e.g., occlusions, off-screen speakers or unreliable detection). Besides, our audio-visual model is robust to the absence of visual modality, where the diarization performance degrades significantly using the visual-only model. Evaluated on the datasets of the first multi-model information based speech processing (MISP) challenge, the proposed method achieved diarization error rates (DERs) of 10.1%/9.5% on development/eval set with reference voice activity detection (VAD) information, while audio-only and video-only system yielded DERs of 27.9%/29.0% and 14.6%/13.1% respectively."
   ],
   "doi": "10.21437/Interspeech.2022-10106"
  },
  "schuppler22_interspeech": {
   "authors": [
    [
     "Barbara",
     "Schuppler"
    ],
    [
     "Emil",
     "Berger"
    ],
    [
     "Xenia",
     "Kogler"
    ],
    [
     "Franz",
     "Pernkopf"
    ]
   ],
   "title": "Homophone Disambiguation Profits from Durational Information",
   "original": "10109",
   "page_count": 5,
   "order": 648,
   "p1": 3198,
   "pn": 3202,
   "abstract": [
    "Given the high degree of segmental reduction in conversational speech, a large number of words become homophoneous that in read speech are not. For instance, the tokens considered in this study \"ah, ach, auch, eine and \"er\" may all be reduced to [a] in conversational Austrian German. Homophones pose a serious problem for automatic speech recognition (ASR), where homophone disambiguation is typically solved using lexical context. In contrast, we propose two approaches to disambiguate homophones on the basis of prosodic and spectral features. First, we build a Random Forest classifier with a large set of acoustic features, which reaches good performance given the small data size, and allows us to gain insight into how these homophones are distinct with respect to phonetic detail. Since for the extraction of the features annotations are required, this approach would not be practical for the integration into an ASR system. We thus explored a second, convolutional neural network (CNN) based approach. The performance of this approach is on par with the one based on Random Forest, and the results indicate a high potential of this approach to facilitate homophone disambiguation when combined with a stochastic language model as part of an ASR system."
   ],
   "doi": "10.21437/Interspeech.2022-10109"
  },
  "sato22_interspeech": {
   "authors": [
    [
     "Hiroaki",
     "Sato"
    ],
    [
     "Tomoyasu",
     "Komori"
    ],
    [
     "Takeshi",
     "Mishima"
    ],
    [
     "Yoshihiko",
     "Kawai"
    ],
    [
     "Takahiro",
     "Mochizuki"
    ],
    [
     "Shoei",
     "Sato"
    ],
    [
     "Tetsuji",
     "Ogawa"
    ]
   ],
   "title": "Text-Only Domain Adaptation Based on Intermediate CTC",
   "original": "10114",
   "page_count": 5,
   "order": 450,
   "p1": 2208,
   "pn": 2212,
   "abstract": [
    "We propose a domain adaptation method that enables connectionist temporal classification (CTC)-based end-to-end (E2E) automatic speech recognition (ASR) models to adapt to a target domain using unpaired text data. The performance of ASR models deteriorates for words and topics not present in the training data, such as the latest news. Although it is difficult to collect paired speech and text data for such subjects, unpaired text data is relatively easy to obtain. Therefore, a domain adaptation method using unpaired text data is proposed for the E2E ASR model based on the intermediate CTC. This model introduces an adaptation branch to embed acoustic and linguistic information in the same latent space, allowing for domain adaptation using unpaired text data of the target domain. Experimental comparisons for multiple out-of-domain settings demonstrate that the proposed text-only domain adaptation achieves a comparable or better performance than the existing shallow-fusion-based domain adaptation, and further performance improvement is achieved by integration with shallow fusion."
   ],
   "doi": "10.21437/Interspeech.2022-10114"
  },
  "finkelstein22_interspeech": {
   "authors": [
    [
     "Lev",
     "Finkelstein"
    ],
    [
     "Heiga",
     "Zen"
    ],
    [
     "Norman",
     "Casagrande"
    ],
    [
     "Chun-an",
     "Chan"
    ],
    [
     "Ye",
     "Jia"
    ],
    [
     "Tom",
     "Kenter"
    ],
    [
     "Alex",
     "Petelin"
    ],
    [
     "Jonathan",
     "Shen"
    ],
    [
     "Vincent",
     "Wan"
    ],
    [
     "Yu",
     "Zhang"
    ],
    [
     "Yonghui",
     "Wu"
    ],
    [
     "Robert",
     "Clark"
    ]
   ],
   "title": "Training Text-To-Speech Systems From Synthetic Data: A Practical Approach For Accent Transfer Tasks",
   "original": "10115",
   "page_count": 5,
   "order": 926,
   "p1": 4571,
   "pn": 4575,
   "abstract": [
    "Transfer tasks in text-to-speech (TTS) synthesis -- where one or more aspects of the speech of one set of speakers is transferred to another set of speakers that do not feature these aspects originally -- remains a challenging task. One of the challenges is that models that have high-quality transfer capabilities can have issues in stability, making them impractical for user-facing critical tasks. This paper demonstrates that transfer can be obtained by training a robust TTS system on data generated by a less robust TTS system designed for a high-quality transfer task; in particular, a CHiVE-BERT monolingual TTS system is trained on the output of a Tacotron model designed for accent transfer. While some quality loss is inevitable with this approach, experimental results show that the models trained on synthetic data this way can produce high quality audio displaying accent transfer, while preserving speaker characteristics such as speaking style."
   ],
   "doi": "10.21437/Interspeech.2022-10115"
  },
  "stephenson22_interspeech": {
   "authors": [
    [
     "Brooke",
     "Stephenson"
    ],
    [
     "Laurent",
     "Besacier"
    ],
    [
     "Laurent",
     "Girin"
    ],
    [
     "Thomas",
     "Hueber"
    ]
   ],
   "title": "BERT, can HE predict contrastive focus? Predicting and controlling prominence in neural TTS using a language model",
   "original": "10116",
   "page_count": 5,
   "order": 685,
   "p1": 3383,
   "pn": 3387,
   "abstract": [
    "Several recent studies have tested the use of transformer language model representations to infer prosodic features for text-to-speech synthesis (TTS). While these studies have explored prosody in general, in this work, we look specifically at the prediction of contrastive focus on personal pronouns. This is a particularly challenging task as it often requires semantic, discursive and/or pragmatic knowledge to predict correctly. We collect a corpus of utterances containing contrastive focus and we evaluate the accuracy of a BERT model, finetuned to predict quantized acoustic prominence features, on these samples. We also investigate how past utterances can provide relevant information for this prediction. Furthermore, we evaluate the controllability of pronoun prominence in a TTS model conditioned on acoustic prominence features."
   ],
   "doi": "10.21437/Interspeech.2022-10116"
  },
  "yoshioka22_interspeech": {
   "authors": [
    [
     "Daiki",
     "Yoshioka"
    ],
    [
     "Yusuke",
     "Yasuda"
    ],
    [
     "Noriyuki",
     "Matsunaga"
    ],
    [
     "Yamato",
     "Ohtani"
    ],
    [
     "Tomoki",
     "Toda"
    ]
   ],
   "title": "Spoken-Text-Style Transfer with Conditional Variational Autoencoder and Content Word Storage",
   "original": "10118",
   "page_count": 5,
   "order": 927,
   "p1": 4576,
   "pn": 4580,
   "abstract": [
    "Text style transfer is the task of converting textual style while preserving content. Content preservation is still challenging in text style transfer under the training condition with non-parallel data. We improve the content preservation performance of text style transfer using a labeled non-parallel corpus, targeting interest styles for text-to-speech synthesis. We propose a content word storage mechanism to preserve \"content words”, particularly for improving content preservation, and incorporate it in the conditional variational autoencoder to capture the style information from the labeled non-parallel corpus. We have conducted a bi-directional transfer experiment of Japanese texts about \"disfluency removal/insertion” and \"standard/Kansai dialect conversion” as target styles. From the results of automatic and human evaluations, we found that 1) the proposed method improved the content preservation without compromising other performances and 2) the proposed method had different performances depending on the direction of style transfer."
   ],
   "doi": "10.21437/Interspeech.2022-10118"
  },
  "langheinrich22_interspeech": {
   "authors": [
    [
     "Ingo",
     "Langheinrich"
    ],
    [
     "Simon",
     "Stone"
    ],
    [
     "Xinyu",
     "Zhang"
    ],
    [
     "Peter",
     "Birkholz"
    ]
   ],
   "title": "Glottal inverse filtering based on articulatory synthesis and deep learning",
   "original": "10119",
   "page_count": 5,
   "order": 270,
   "p1": 1327,
   "pn": 1331,
   "abstract": [
    "We propose a new method to estimate the glottal vocal tract excitation from speech signals based on deep learning. To that end, a bidirectional recurrent neural network with long short-term memory units was trained to predict the glottal airflow derivative from the speech signal. Since natural reference data for this task is unobtainable at the required scale, we used the articulatory speech synthesizer VocalTractLab to generate a large dataset containing synchronous connected speech and glottal airflow signals for training. The trained model's performance was objectively evaluated by means of stationary synthetic signals from the OPENGLOT glottal inverse filtering benchmark dataset and by using our dataset of connected synthetic speech. Compared to the state of the art, the proposed model produced a more accurate estimation using OPENGLOT's physically synthesized signals but was less accurate for its computationally simulated signals. However, our model was much more accurate and plausible on the connected speech signals, especially for sounds with mixed excitation (e.g. fricatives) or sounds with pronounced zeros in their transfer function (e.g. nasals). Future work will introduce more variety into the training data (e.g. regarding pitch and phonation) and focus on estimating features of the glottal flow instead of the entire waveform."
   ],
   "doi": "10.21437/Interspeech.2022-10119"
  },
  "ke22_interspeech": {
   "authors": [
    [
     "Xiaoquan",
     "KE"
    ],
    [
     "Man-Wai",
     "Mak"
    ],
    [
     "Helen M.",
     "Meng"
    ]
   ],
   "title": "Automatic Selection of Discriminative Features for Dementia Detection in Cantonese-Speaking People",
   "original": "10122",
   "page_count": 5,
   "order": 439,
   "p1": 2153,
   "pn": 2157,
   "abstract": [
    "Dementia is a severe cognitive impairment that affects the health of older adults and creates a burden on their families and caretakers. This paper analyzes diverse features extracted from spoken languages and selects the most discriminative features for dementia detection. The paper presents a deep learning-based feature ranking method called dual-net feature ranking (DFR). The proposed DFR utilizes a dual-net architecture, where two networks (called operator and selector) are alternatively and cooperatively trained to simultaneously perform feature selection and dementia detection. The DFR interprets the contribution of individual features to the predictions of the selector network using all of the selector's parameters. The DFR was evaluated on the Cantonese JCCOCC-MoCA Elderly Speech Dataset. Results show that the DFR can significantly reduce feature dimensionality while identifying small feature subsets with comparable or superior performance than the whole feature set. The selected features have been uploaded to https://github.com/kexquan/AD-detection-Feature-selection."
   ],
   "doi": "10.21437/Interspeech.2022-10122"
  },
  "li22p_interspeech": {
   "authors": [
    [
     "Xian",
     "LI"
    ],
    [
     "Xiaofei",
     "Li"
    ]
   ],
   "title": "ATST: Audio Representation Learning with Teacher-Student Transformer",
   "original": "10126",
   "page_count": 5,
   "order": 846,
   "p1": 4172,
   "pn": 4176,
   "abstract": [
    "Self-supervised learning (SSL) learns knowledge from a large amount of unlabeled data, and then transfers the knowledge to a specific problem with a limited number of labeled data. SSL has achieved promising results in various domains. This work addresses the problem of segment-level general audio SSL, and proposes a new transformer-based teacher-student SSL model, named ATST. A transformer encoder is developed on a recently emerged teacher-student baseline scheme, which largely improves the modeling capability of pre-training. In addition, a new strategy for positive pair creation is designed to fully leverage the capability of transformer. Extensive experiments have been conducted, and the proposed model achieves the new state-of-the-art results on almost all of the downstream tasks."
   ],
   "doi": "10.21437/Interspeech.2022-10126"
  },
  "nam22_interspeech": {
   "authors": [
    [
     "Hyeonuk",
     "Nam"
    ],
    [
     "Seong-Hu",
     "Kim"
    ],
    [
     "Byeong-Yun",
     "Ko"
    ],
    [
     "Yong-Hwa",
     "Park"
    ]
   ],
   "title": "Frequency Dynamic Convolution: Frequency-Adaptive Pattern Recognition for Sound Event Detection",
   "original": "10127",
   "page_count": 5,
   "order": 561,
   "p1": 2763,
   "pn": 2767,
   "abstract": [
    "2D convolution is widely used in sound event detection (SED) to recognize two dimensional time-frequency patterns of sound events. However, 2D convolution enforces translation equivariance on sound events along both time and frequency axis while frequency is not shift-invariant dimension. In order to improve physical consistency of 2D convolution on SED, we propose frequency dynamic convolution which applies kernel that adapts to frequency components of input. Frequency dynamic convolution outperforms the baseline by 6.3% in DESED validation dataset in terms of polyphonic sound detection score (PSDS). It also significantly outperforms other pre-existing content-adaptive methods on SED. In addition, by comparing class-wise F1 scores of baseline and frequency dynamic convolution, we showed that frequency dynamic convolution is especially more effective for detection of non-stationary sound events with intricate time-frequency patterns. From this result, we verified that frequency dynamic convolution is superior in recognizing frequency-dependent patterns."
   ],
   "doi": "10.21437/Interspeech.2022-10127"
  },
  "shin22b_interspeech": {
   "authors": [
    [
     "Yookyung",
     "Shin"
    ],
    [
     "Younggun",
     "Lee"
    ],
    [
     "Suhee",
     "Jo"
    ],
    [
     "Yeongtae",
     "Hwang"
    ],
    [
     "Taesu",
     "Kim"
    ]
   ],
   "title": "Text-driven Emotional Style Control and Cross-speaker Style Transfer in Neural TTS",
   "original": "10131",
   "page_count": 5,
   "order": 471,
   "p1": 2313,
   "pn": 2317,
   "abstract": [
    "Expressive text-to-speech has shown improved performance in recent years. However, the style control of synthetic speech is often restricted to discrete emotion categories and requires training data recorded by the target speaker in the target style. In many practical situations, users may not have reference speech recorded in target emotion but still be interested in controlling speech style just by typing text description of desired emotional style. In this paper, we propose a text-based interface for emotional style control and cross-speaker style transfer in multi-speaker TTS. We propose the bi-modal style encoder which models the semantic relationship between text description embedding and speech style embedding with a pretrained language model. To further improve cross-speaker style transfer on disjoint, multi-style datasets, we propose the novel style loss. The experimental results show that our model can generate high-quality expressive speech even in unseen style."
   ],
   "doi": "10.21437/Interspeech.2022-10131"
  },
  "valentinibotinhao22_interspeech": {
   "authors": [
    [
     "Cassia",
     "Valentini-Botinhao"
    ],
    [
     "Manuel Sam",
     "Ribeiro"
    ],
    [
     "Oliver",
     "Watts"
    ],
    [
     "Korin",
     "Richmond"
    ],
    [
     "Gustav Eje",
     "Henter"
    ]
   ],
   "title": "Predicting pairwise preferences between TTS audio stimuli using parallel ratings data and anti-symmetric twin neural networks",
   "original": "10132",
   "page_count": 5,
   "order": 95,
   "p1": 471,
   "pn": 475,
   "abstract": [
    "Automatically predicting the outcome of subjective listening tests is a challenging task. Ratings may vary from person to person even if preferences are consistent across listeners. While previous work has focused on predicting listeners' ratings (mean opinion scores) of individual stimuli, we focus on the simpler task of predicting subjective preference given two speech stimuli for the same text. We propose a model based on anti-symmetric twin neural networks, trained on pairs of waveforms and their corresponding preference scores. We explore both attention and recurrent neural nets to account for the fact that stimuli in a pair are not time aligned. To obtain a large training set we convert listeners' ratings from MUSHRA tests to values that reflect how often one stimulus in the pair was rated higher than the other. Specifically, we evaluate performance on data obtained from twelve MUSHRA evaluations conducted over five years, containing different TTS systems, built from data of different speakers. Our results compare favourably to a state-of-the-art model trained to predict MOS scores."
   ],
   "doi": "10.21437/Interspeech.2022-10132"
  },
  "song22d_interspeech": {
   "authors": [
    [
     "Eunwoo",
     "Song"
    ],
    [
     "Ryuichi",
     "Yamamoto"
    ],
    [
     "Ohsung",
     "Kwon"
    ],
    [
     "Chan-Ho",
     "Song"
    ],
    [
     "Min-Jae",
     "Hwang"
    ],
    [
     "Suhyeon",
     "Oh"
    ],
    [
     "Hyun-Wook",
     "Yoon"
    ],
    [
     "Jin-Seob",
     "Kim"
    ],
    [
     "Jae-Min",
     "Kim"
    ]
   ],
   "title": "TTS-by-TTS 2: Data-Selective Augmentation for Neural Speech Synthesis Using Ranking Support Vector Machine with Variational Autoencoder",
   "original": "10134",
   "page_count": 5,
   "order": 393,
   "p1": 1941,
   "pn": 1945,
   "abstract": [
    "Recent advances in synthetic speech quality have enabled us to train text-to-speech (TTS) systems by using synthetic corpora. However, merely increasing the amount of synthetic data is not always advantageous for improving training efficiency. Our aim in this study is to selectively choose synthetic data that are beneficial to the training process. In the proposed method, we first adopt a variational autoencoder whose posterior distribution is utilized to extract latent features representing acoustic similarity between the recorded and synthetic corpora. By using those learned features, we then train a ranking support vector machine (RankSVM) that is well known for effectively ranking relative attributes among binary classes. By setting the recorded and synthetic ones as two opposite classes, RankSVM is used to determine how the synthesized speech is acoustically similar to the recorded data. Then, synthetic TTS data, whose distribution is close to the recorded data, are selected from large-scale synthetic corpora. By using these data for retraining the TTS model, the synthetic quality can be significantly improved. Objective and subjective evaluation results show the superiority of the proposed method over the conventional methods."
   ],
   "doi": "10.21437/Interspeech.2022-10134"
  },
  "meyer22_interspeech": {
   "authors": [
    [
     "Felix",
     "Meyer"
    ],
    [
     "Wilfried",
     "Michel"
    ],
    [
     "Mohammad",
     "Zeineldeen"
    ],
    [
     "Ralf",
     "Schlüter"
    ],
    [
     "Hermann",
     "Ney"
    ]
   ],
   "title": "Automatic Learning of Subword Dependent Model Scales",
   "original": "10136",
   "page_count": 4,
   "order": 838,
   "p1": 4133,
   "pn": 4136,
   "abstract": [
    "To improve the performance of state-of-the-art automatic speech recognition systems it is common practice to include external knowledge sources such as language models or prior corrections. This is usually done via log-linear model combination using separate scaling parameters for each model. Typically these parameters are manually optimized on some held-out data. In this work we propose to use individual scaling parameters per subword output token. We train these parameters via automatic differentiation and stochastic gradient decent optimization similar to the neural network model parameters. We show on the LibriSpeech (LBS) and Switchboard (SWB) corpora that automatic learning of two scales for a combination of attention-based encoder-decoder acoustic model and language model can be done as effectively as with manual tuning. Using subword dependent model scales which could not be tuned manually we achieve 7% improvement on LBS and 3% on SWB. We also show that joint training of scales and model parameters is possible and gives additional 6% improvement on LBS."
   ],
   "doi": "10.21437/Interspeech.2022-10136"
  },
  "fong22_interspeech": {
   "authors": [
    [
     "Jason",
     "Fong"
    ],
    [
     "Daniel",
     "Lyth"
    ],
    [
     "Gustav Eje",
     "Henter"
    ],
    [
     "Hao",
     "Tang"
    ],
    [
     "Simon",
     "King"
    ]
   ],
   "title": "Speech Audio Corrector: using speech from non-target speakers for one-off correction of mispronunciations in grapheme-input text-to-speech",
   "original": "10138",
   "page_count": 5,
   "order": 247,
   "p1": 1213,
   "pn": 1217,
   "abstract": [
    "Correct pronunciation is essential for text-to-speech (TTS) systems in production. Most production systems rely on pronouncing dictionaries to perform grapheme-to-phoneme conversion. Unlike end-to-end TTS, this enables pronunciation correction by manually altering the phoneme sequence, but the necessary dictionaries are labour-intensive to create and only exist in a few high-resourced languages. This work demonstrates that accurate TTS pronunciation control can be achieved without a dictionary. Moreover, we show that such control can be performed without requiring any model retraining or fine-tuning, merely by supplying a single correctly-pronounced reading of a word in a different voice and accent at synthesis time. Experimental results show that our proposed system successfully enables one-off correction of mispronunciations in grapheme-based TTS with maintained synthesis quality. This opens the door to production-level TTS in languages and applications where pronunciation dictionaries are unavailable."
   ],
   "doi": "10.21437/Interspeech.2022-10138"
  },
  "zuo22b_interspeech": {
   "authors": [
    [
     "Chu-Xiao",
     "Zuo"
    ],
    [
     "Jia-Yi",
     "Leng"
    ],
    [
     "Wu-Jun",
     "Li"
    ]
   ],
   "title": "Speaker-Specific Utterance Ensemble based Transfer Attack on Speaker Identification",
   "original": "10139",
   "page_count": 5,
   "order": 649,
   "p1": 3203,
   "pn": 3207,
   "abstract": [
    "Speaker identification(SI) systems based on deep neural network(DNN) have been widely applied in practical tasks. But DNN is vulnerable to imperceptibly adversarial attacks which will typically result in misjudgment and security concern. Hence, the research on adversarial attacks has become a crucial problem to verify the robustness of SI systems. Although existing works have shown that white-box attacks can break through current SI systems, few works have studied the more practical black-box attacks. Moreover, existing transfer attacks on SI systems migrated from computer vision are speaker-unrelated and lack the adaptability to speech data. In this work, we propose a new black-box attack method, called speaker-specific utterance ensemble based transfer attack(SUETA), to attack on SI systems. SUETA is the first work to generate an ensemble of multiple adversarial utterances in the unit of speakers, by utilizing the unique characteristic of speech data that different utterances of one specific speaker share the same voiceprint. Experimental results on three representative SI models show that SUETA can achieve better transfer success rate(TSR) than speaker-unrelated baselines. Furthermore, SUETA can even improve the attack success rate(ASR) of local white-box attacks."
   ],
   "doi": "10.21437/Interspeech.2022-10139"
  },
  "liu22p_interspeech": {
   "authors": [
    [
     "Chang",
     "Liu"
    ],
    [
     "Zhen-Hua",
     "Ling"
    ],
    [
     "Ling-Hui",
     "Chen"
    ]
   ],
   "title": "Pronunciation Dictionary-Free Multilingual Speech Synthesis by Combining Unsupervised and Supervised Phonetic Representations",
   "original": "10140",
   "page_count": 5,
   "order": 868,
   "p1": 4282,
   "pn": 4286,
   "abstract": [
    "This paper proposes a multilingual speech synthesis method which combines unsupervised phonetic representations (UPR) and supervised phonetic representations (SPR) to avoid reliance on the pronunciation dictionaries of target languages. In this method, a pretrained wav2vec 2.0 model is adopted to extract UPRs and a language-independent automatic speech recognition (LI-ASR) model is built with a connectionist temporal classification (CTC) loss to extract segment-level SPRs from the audio data of target languages. Then, an acoustic model is designed, which first predicts UPRs and SPRs from texts separately and then combines the predicted UPRs and SPRs to generate mel-spectrograms. The results of our experiments on six languages show that the proposed method outperformed the methods that directly predicted mel-spectrograms from character or phoneme sequences and the ablated models that utilized only UPRs or SPRs."
   ],
   "doi": "10.21437/Interspeech.2022-10140"
  },
  "zhang22r_interspeech": {
   "authors": [
    [
     "Minyue",
     "Zhang"
    ],
    [
     "Hongwei",
     "Ding"
    ]
   ],
   "title": "Impact of Background Noise and Contribution of Visual Information in Emotion Identification by Native Mandarin Speakers",
   "original": "10142",
   "page_count": 5,
   "order": 407,
   "p1": 1993,
   "pn": 1997,
   "abstract": [
    "Many studies on emotion processing considered little about the issue of ecological validity and insufficient attention has been drawn to uni-sensory and multisensory emotion perception in challenging environments. The current research explored how adding multi-talker babble noise impacts emotion perception and how visual information affects the results in comparison with the audio alone conditions. Forty native Mandarin participants (21 females and 19 males) were asked to identify the emotion according to the auditory or audiovisual information they received. Results showed that the emotion identification accuracy was significantly lower in noisy conditions than in noiseless ones, whether additional visual information was presented simultaneously or not. In noisy environments, providing multisensory emotional information greatly facilitated recognition performances even when the visual information was less reliable. To conclude, multi-talker babble noise had a corrupting effect on emotion identification, which worked in both unisensory and multisensory settings, and emotion perception is a robust multisensory situation that follows the inverse effectiveness principle."
   ],
   "doi": "10.21437/Interspeech.2022-10142"
  },
  "tamm22_interspeech": {
   "authors": [
    [
     "Bastiaan",
     "Tamm"
    ],
    [
     "Helena",
     "Balabin"
    ],
    [
     "Rik",
     "Vandenberghe"
    ],
    [
     "Hugo",
     "Van hamme"
    ]
   ],
   "title": "Pre-trained Speech Representations as Feature Extractors for Speech Quality Assessment in Online Conferencing Applications",
   "original": "10147",
   "page_count": 5,
   "order": 828,
   "p1": 4083,
   "pn": 4087,
   "abstract": [
    "Speech quality in online conferencing applications is typically assessed through human judgements in the form of the mean opinion score (MOS) metric. Since such a labor-intensive approach is not feasible for large-scale speech quality assessments in most settings, the focus has shifted towards automated MOS prediction through end-to-end training of deep neural networks (DNN). Instead of training a network from scratch, we propose to leverage the speech representations from the pre-trained wav2vec-based XLS-R model. However, the number of parameters of such a model exceeds task-specific DNNs by several orders of magnitude, which poses a challenge for resulting fine-tuning procedures on smaller datasets. Therefore, we opt to use pre-trained speech representations from XLS-R in a feature extraction rather than a fine-tuning setting, thereby significantly reducing the number of trainable model parameters. We compare our proposed XLS-R-based feature extractor to a Mel-frequency cepstral coefficient (MFCC)-based one, and experiment with various combinations of bidirectional long short term memory (Bi-LSTM) and attention pooling feedforward (AttPoolFF) networks trained on the output of the feature extractors. We demonstrate the increased performance of pre-trained XLS-R embeddings in terms a reduced root mean squared error (RMSE) on the ConferencingSpeech 2022 MOS prediction task."
   ],
   "doi": "10.21437/Interspeech.2022-10147"
  },
  "zhang22s_interspeech": {
   "authors": [
    [
     "Yuxiang",
     "Zhang"
    ],
    [
     "Zhuo",
     "Li"
    ],
    [
     "Wenchao",
     "Wang"
    ],
    [
     "Pengyuan",
     "Zhang"
    ]
   ],
   "title": "SASV Based on Pre-trained ASV System and Integrated Scoring Module",
   "original": "10149",
   "page_count": 5,
   "order": 887,
   "p1": 4376,
   "pn": 4380,
   "abstract": [
    "Based on the assumption that there is a correlation between anti-spoofing and speaker verification, a Total-Divide-Total integrated Spoofing-Aware Speaker Verification (SASV) system based on pre-trained automatic speaker verification (ASV) system and integrated scoring module is proposed and submitted to the SASV 2022 Challenge. The training and scoring of ASV and anti-spoofing countermeasure (CM) in current SASV systems are relatively independent, ignoring the correlation. In this paper, by leveraging the correlation between the two tasks, an integrated SASV system can be obtained by simply training a few more layers on the basis of the baseline pre-trained ASV subsystem. The features in pre-trained ASV system are utilized for logical access spoofing speech detection. Further, speaker embeddings extracted by the pre-trained ASV system are used to improve the performance of the CM. The integrated scoring module takes the embeddings of the ASV and anti-spoofing branches as input and preserves the correlation between the two tasks through matrix operations to produce integrated SASV scores. Submitted primary system achieved equal error rate (EER) of 3.07% on the development dataset of the SASV 2022 Challenge and 4.30% on the evaluation part, which is a 25% improvement over the baseline systems."
   ],
   "doi": "10.21437/Interspeech.2022-10149"
  },
  "chen22i_interspeech": {
   "authors": [
    [
     "Yu-Wen",
     "Chen"
    ],
    [
     "Yu",
     "Tsao"
    ]
   ],
   "title": "InQSS: a speech intelligibility and quality assessment model using a multi-task learning network",
   "original": "10153",
   "page_count": 5,
   "order": 626,
   "p1": 3088,
   "pn": 3092,
   "abstract": [
    "Speech intelligibility and quality assessment models are essential tools for researchers to evaluate and improve speech processing models. However, only a few studies have investigated multi-task models for intelligibility and quality assessment due to the limitations of available data. In this study, we released TMHINT-QI, the first Chinese speech dataset that records the quality and intelligibility scores of clean, noisy, and enhanced utterances. Then, we propose InQSS, a non-intrusive multi-task learning framework for intelligibility and quality assessment. We evaluated the InQSS on both the training-from-scratch and the pretrained models. The experimental results confirm the effectiveness of the InQSS framework. In addition, the resulting model can predict not only the intelligibility scores but also the quality scores of a speech signal."
   ],
   "doi": "10.21437/Interspeech.2022-10153"
  },
  "liu22q_interspeech": {
   "authors": [
    [
     "Zhuoya",
     "Liu"
    ],
    [
     "Mark",
     "Huckvale"
    ],
    [
     "Julian",
     "McGlashan"
    ]
   ],
   "title": "Automated Voice Pathology Discrimination from Continuous Speech Benefits from Analysis by Phonetic Context",
   "original": "10154",
   "page_count": 5,
   "order": 440,
   "p1": 2158,
   "pn": 2162,
   "abstract": [
    "In contrast to previous studies that look only at discriminating pathological voice from the normal voice, in this study we focus on the discrimination between cases of spasmodic dysphonia (SD) and vocal fold palsy (VP) using automated analysis of speech recordings. The hypothesis is that discrimination will be enhanced by studying continuous speech, since the different pathologies are likely to have different effects in different phonetic contexts. We collected audio recordings of isolated vowels and of a read passage from 60 patients diagnosed with SD (N=38) or VP (N=22). Baseline classifiers on features extracted from the recordings taken as a whole gave a cross-validated unweighted average recall of up to 75% for discriminating the two pathologies. We used an automated method to divide the read passage into phone-labelled regions and built classifiers for each phone. Results show that the discriminability of the pathologies varied with phonetic context as predicted. Since different phone contexts provide different information about the pathologies, classification is improved by fusing phone predictions, to achieve a classification accuracy of 83%. The work has implications for the differential diagnosis of voice pathologies and contributes to a better understanding of their impact on speech."
   ],
   "doi": "10.21437/Interspeech.2022-10154"
  },
  "westhausen22_interspeech": {
   "authors": [
    [
     "Nils L.",
     "Westhausen"
    ],
    [
     "Bernd T.",
     "Meyer"
    ]
   ],
   "title": "tPLCnet: Real-time Deep Packet Loss Concealment in the Time Domain Using a Short Temporal Context",
   "original": "10157",
   "page_count": 5,
   "order": 589,
   "p1": 2903,
   "pn": 2907,
   "abstract": [
    "This paper introduces a real-time time-domain packet loss concealment (PLC) neural-network (tPLCnet). It efficiently predicts lost frames from a short context buffer in a sequence-to-one (seq2one) fashion. Because of its seq2one structure, a continuous inference of the model is not required since it can be triggered when packet loss is actually detected. It is trained on 64 h of open-source speech data and packet-loss traces of real calls provided by the Audio PLC Challenge. The model with the lowest complexity described in this paper reaches a robust PLC performance and consistent improvements over the zero-filling baseline for all metrics. A configuration with higher complexity is submitted to the PLC Challenge and shows a performance increase of 1.07 compared to the zero-filling baseline in terms of PLC-MOS on the blind test set and reaches a competitive 3rd place in the challenge ranking."
   ],
   "doi": "10.21437/Interspeech.2022-10157"
  },
  "choi22e_interspeech": {
   "authors": [
    [
     "Yeonjong",
     "Choi"
    ],
    [
     "Chao",
     "Xie"
    ],
    [
     "Tomoki",
     "Toda"
    ]
   ],
   "title": "An Evaluation of Three-Stage Voice Conversion Framework for Noisy and Reverberant Conditions",
   "original": "10158",
   "page_count": 5,
   "order": 994,
   "p1": 4910,
   "pn": 4914,
   "abstract": [
    "This paper presents a new voice conversion (VC) framework capable of dealing with both additive noise and reverberation, and its performance evaluation. There have been studied some VC researches focusing on real-world circumstances where speech data are interfered with background noise and reverberation. To deal with more practical conditions where no clean target dataset is available, one possible approach is zero-shot VC, but its performance tends to degrade compared with VC using sufficient amount of target speech data. To leverage large amount of noisy-reverberant target speech data, we propose a three-stage VC framework based on denoising process using a pretrained denoising model, dereverberation process using a dereverberation model, and VC process using a nonparallel VC model based on a variational autoencoder. The experimental results show that 1) noise and reverberation additively cause significant VC performance degradation, 2) the proposed method alleviates the adverse effects caused by both noise and reverberation, and significantly outperforms the baseline directly trained on the noisy-reverberant speech data, and 3) the potential degradation introduced by the denoising and dereverberation still causes noticeable adverse effects on VC performance."
   ],
   "doi": "10.21437/Interspeech.2022-10158"
  },
  "udupa22_interspeech": {
   "authors": [
    [
     "Sathvik",
     "Udupa"
    ],
    [
     "Aravind",
     "Illa"
    ],
    [
     "Prasanta",
     "Ghosh"
    ]
   ],
   "title": "Streaming model for Acoustic to Articulatory Inversion with transformer networks",
   "original": "10159",
   "page_count": 5,
   "order": 126,
   "p1": 625,
   "pn": 629,
   "abstract": [
    "Estimating speech articulatory movements from speech acoustics is known as Acoustic to Articulatory Inversion (AAI). Recently, transformer-based AAI models have been shown to achieve state-of-art performance. However, in transformer networks, the attention is applied over the whole utterance, thereby needing to obtain the full utterance before the inference, which leads to high latency and is impractical for streaming AAI. To enable streaming during inference, evaluation could be performed on non-overlapping chucks instead of full utterance. However, due to a mismatch of attention receptive field during training and evaluation, there could be a drop in AAI performance. To overcome this scenario, in this work we perform experiments with different attention masks and use context from previous predictions during training. Experiments results revealed that using the random start mask attention with the context from previous predictions of transformer encoder performs better than the baseline results."
   ],
   "doi": "10.21437/Interspeech.2022-10159"
  },
  "dao22_interspeech": {
   "authors": [
    [
     "Mai Hoang",
     "Dao"
    ],
    [
     "Thinh",
     "Truong"
    ],
    [
     "Dat Quoc",
     "Nguyen"
    ]
   ],
   "title": "From Disfluency Detection to Intent Detection and Slot Filling",
   "original": "10161",
   "page_count": 5,
   "order": 225,
   "p1": 1106,
   "pn": 1110,
   "abstract": [
    "We present the first empirical study investigating the influence of disfluency detection on downstream tasks of intent detection and slot filling. We perform this study for Vietnamese---a low-resource language that has no previous study as well as no public dataset available for disfluency detection. First, we extend the fluent Vietnamese intent detection and slot filling dataset PhoATIS by manually adding contextual disfluencies and annotating them. Then, we conduct experiments using strong baselines for disfluency detection and joint intent detection and slot filling, which are based on pre-trained language models. We find that: (i) disfluencies produce negative effects on the performances of the downstream intent detection and slot filling tasks, and (ii) in the disfluency context, the pre-trained multilingual language model XLM-R helps produce better intent detection and slot filling performances than the pre-trained monolingual language model PhoBERT, and this is opposite to what generally found in the fluency context."
   ],
   "doi": "10.21437/Interspeech.2022-10161"
  },
  "szalay22_interspeech": {
   "authors": [
    [
     "Tuende",
     "Szalay"
    ],
    [
     "Mostafa",
     "Shahin"
    ],
    [
     "Beena",
     "Ahmed"
    ],
    [
     "Kirrie",
     "Ballard"
    ]
   ],
   "title": "Knowledge of accent differences can be used to predict speech recognition",
   "original": "10162",
   "page_count": 5,
   "order": 279,
   "p1": 1372,
   "pn": 1376,
   "abstract": [
    "If accent differences can predict speech recognition, a smaller dataset systematically representing accent differences might be sufficient and less resource intensive for adapting an automatic speech recognition (ASR) to a novel variety compared to training the ASR on a large, unsystematic dataset. However, it is not known whether ASR errors pattern according to accent differences. Therefore, we tested the performance of Google's General American (GenAm) and Standard Australian English (SAusE) ASR on both dialects using words systematically representing accent differences. Accent differences were quantified using the different number of vowel phonemes, the different phonetic quality of vowels, and differences in rhoticity (i.e., presence/absence of postvocalic /r/). Our results confirm that word recognition is significantly more accurate when ASR dialect matches the speaker dialect compared to the mismatched conditions. Our results reveal that GenAm ASR is less accurate on SAusE speakers due to the higher number of vowel phonemes and to the lack of post-vocalic /r/ in SAusE. Thus, the data need of adapting ASR from GenAm to SAusE might be reduced by using a small dataset focusing on differences in the size of vowel inventory and in rhoticity."
   ],
   "doi": "10.21437/Interspeech.2022-10162"
  },
  "stafylakis22_interspeech": {
   "authors": [
    [
     "Themos",
     "Stafylakis"
    ],
    [
     "Ladislav",
     "Mosner"
    ],
    [
     "Oldrich",
     "Plchot"
    ],
    [
     "Johan",
     "Rohdin"
    ],
    [
     "Anna",
     "Silnova"
    ],
    [
     "Lukas",
     "Burget"
    ],
    [
     "Jan",
     "Černocký"
    ]
   ],
   "title": "Training speaker embedding extractors using multi-speaker audio with unknown speaker boundaries",
   "original": "10165",
   "page_count": 5,
   "order": 122,
   "p1": 605,
   "pn": 609,
   "abstract": [
    "In this paper, we demonstrate a method for training speaker embedding extractors using weak annotation. More specifically, we are using the full VoxCeleb recordings and the name of the celebrities appearing on each video without knowledge of the time intervals the celebrities appear in the video. We show that by combining a baseline speaker diarization algorithm that requires no training or parameter tuning, a modified loss with aggregation over segments, and a two-stage training approach, we are able to train a competitive ResNet-based embedding extractor. Finally, we experiment with two different aggregation functions and analyze their behaviour in terms of their gradients."
   ],
   "doi": "10.21437/Interspeech.2022-10165"
  },
  "omahony22_interspeech": {
   "authors": [
    [
     "Johannah",
     "O'Mahony"
    ],
    [
     "Catherine",
     "Lai"
    ],
    [
     "Simon",
     "King"
    ]
   ],
   "title": "Combining conversational speech with read speech to improve prosody in Text-to-Speech synthesis",
   "original": "10167",
   "page_count": 5,
   "order": 686,
   "p1": 3388,
   "pn": 3392,
   "abstract": [
    "For isolated utterances, speech synthesis quality has improved immensely thanks to the use of sequence-to-sequence models. However, these models are generally trained on read speech and fail to generalise to unseen speaking styles. Recently, more research is focused on the synthesis of expressive and conversational speech. Conversational speech contains many prosodic phenomena that are not present in read speech. We would like to learn these prosodic patterns from data, but unfortunately, many large conversational corpora are unsuitable for speech synthesis due to low audio quality. We investigate whether a data mixing strategy can improve conversational prosody for a target voice based on monologue data from audiobooks by adding real conversational data from podcasts. We filter the podcast data to create a set of 26k question and answer pairs. We evaluate two FastPitch models: one trained on 20 hours of monologue speech from a single speaker, and another trained on 5 hours of monologue speech from that speaker plus 15 hours of questions and answers spoken by nearly 15k speakers. Results from three listening tests show that the second model generates more preferred question prosody."
   ],
   "doi": "10.21437/Interspeech.2022-10167"
  },
  "rybicka22_interspeech": {
   "authors": [
    [
     "Magdalena",
     "Rybicka"
    ],
    [
     "Jesus",
     "Villalba"
    ],
    [
     "Najim",
     "Dehak"
    ],
    [
     "Konrad",
     "Kowalczyk"
    ]
   ],
   "title": "End-to-End Neural Speaker Diarization with an Iterative Refinement of Non-Autoregressive Attention-based Attractors",
   "original": "10169",
   "page_count": 5,
   "order": 1030,
   "p1": 5090,
   "pn": 5094,
   "abstract": [
    "End-to-end neural speaker diarization (EEND) systems are currently of high interest as the approach can easily handle overlapped speech and can be trained to optimize directly the diarization decision. Recently, there have been several investigations that achieve further enhancement of the EEND system, such as proposing various network structures for the encoder module or integration of the EEND with, the well-established in speaker embedding-based diarization, clustering methods. In this paper, we propose an alternative for the EEND backend and replace the LSTM-based attractor estimator with a non-autoregressive approach based on a Transformer decoder. Moreover, we introduce an iterative method that refines the system decision and the attractors in turns. Finally, we present results derived from an additional regularization of the proposed system with the use of Additive Angular Softmax speaker classification loss. We achieve up to 15% relative improvement over baseline on 2-speaker real recordings from CALLHOME dataset and up to 18% on simulated 2-speaker mixtures."
   ],
   "doi": "10.21437/Interspeech.2022-10169"
  },
  "klejch22_interspeech": {
   "authors": [
    [
     "Ondrej",
     "Klejch"
    ],
    [
     "Electra",
     "Wallington"
    ],
    [
     "Peter",
     "Bell"
    ]
   ],
   "title": "Deciphering Speech: a Zero-Resource Approach to Cross-Lingual Transfer in ASR",
   "original": "10170",
   "page_count": 5,
   "order": 466,
   "p1": 2288,
   "pn": 2292,
   "abstract": [
    "We present a method for cross-lingual training an ASR system using absolutely no transcribed training data from the target language, and with no phonetic knowledge of the language in question. Our approach uses a novel application of a decipherment algorithm, which operates given only unpaired speech and text data from the target language. We apply this decipherment to phone sequences generated by a universal phone recogniser trained on out-of-language speech corpora, which we follow with flat-start semi-supervised training to obtain an acoustic model for the new language. To the best of our knowledge, this is the first practical approach to zero-resource cross-lingual ASR which does not rely on any hand-crafted phonetic information. We carry out experiments on read speech from the GlobalPhone corpus, and show that it is possible to learn a decipherment model on just 20 minutes of data from the target language. When used to generate pseudo-labels for semi-supervised training, we obtain WERs that range from 32.5% to just 1.9% absolute worse than the equivalent fully supervised models trained on the same data."
   ],
   "doi": "10.21437/Interspeech.2022-10170"
  },
  "tran22b_interspeech": {
   "authors": [
    [
     "Nguyen Luong",
     "Tran"
    ],
    [
     "Duong",
     "Le"
    ],
    [
     "Dat Quoc",
     "Nguyen"
    ]
   ],
   "title": "BARTpho: Pre-trained Sequence-to-Sequence Models for Vietnamese",
   "original": "10177",
   "page_count": 5,
   "order": 355,
   "p1": 1751,
   "pn": 1755,
   "abstract": [
    "We present BARTpho with two versions, BARTpho-syllable and BARTpho-word, which are the first public large-scale monolingual sequence-to-sequence models pre-trained for Vietnamese. BARTpho uses the \"large\" architecture and the pre-training scheme of the sequence-to-sequence denoising autoencoder BART, thus it is especially suitable for generative NLP tasks. We conduct experiments to compare our BARTpho with its competitor mBART on a downstream task of Vietnamese text summarization and show that: in both automatic and human evaluations, BARTpho outperforms the strong baseline mBART and improves the state-of-the-art. We further evaluate and compare BARTpho and mBART on the Vietnamese capitalization and punctuation restoration tasks and also find that BARTpho is more effective than mBART on these two tasks. We publicly release BARTpho to facilitate future research and applications of generative Vietnamese NLP tasks."
   ],
   "doi": "10.21437/Interspeech.2022-10177"
  },
  "close22_interspeech": {
   "authors": [
    [
     "George",
     "Close"
    ],
    [
     "Samuel",
     "Hollands"
    ],
    [
     "Stefan",
     "Goetze"
    ],
    [
     "Thomas",
     "Hain"
    ]
   ],
   "title": "Non-intrusive Speech Intelligibility Metric Prediction for Hearing Impaired Individuals",
   "original": "10182",
   "page_count": 5,
   "order": 705,
   "p1": 3483,
   "pn": 3487,
   "abstract": [
    "This paper proposes neural models to predict Speech Intelligibility (SI),both by prediction of established SI metrics and of human speech recognition (HSR) on the 1st Clarity Prediction Challenge. Both intrusive and non-intrusive predictors for intrusive SI metrics are trained, then fine tuned on the HSR ground truth. Results are reported on a number of SI metrics, and the model choice for the Clarity challenge submission is explained. Additionally, the relationship between the SI scores in the data and commonly used signal processing metrics which approximate SI are analysed, and some issues emerging from this relationship discussed. It is found that intrusive neural predictors of SI metrics when finetuned on the true HSR scores outperform the non neural challenge baseline."
   ],
   "doi": "10.21437/Interspeech.2022-10182"
  },
  "borsdorf22_interspeech": {
   "authors": [
    [
     "Marvin",
     "Borsdorf"
    ],
    [
     "Kevin",
     "Scheck"
    ],
    [
     "Haizhou",
     "Li"
    ],
    [
     "Tanja",
     "Schultz"
    ]
   ],
   "title": "Blind Language Separation: Disentangling Multilingual Cocktail Party Voices by Language",
   "original": "10187",
   "page_count": 5,
   "order": 52,
   "p1": 256,
   "pn": 260,
   "abstract": [
    "We introduce blind language separation (BLS) as novel research task, in which we seek to disentangle overlapping voices of multiple languages by language. BLS is expected to separate seen as well as unseen languages, which is different from the target language extraction task that works for one seen target language at a time. To develop a BLS model, we simulate a multilingual cocktail party database, of which each scene consists of two randomly selected languages, each represented by two randomly selected speakers. The database follows the recently proposed GlobalPhoneMCP database design concept that uses the audio data of the GlobalPhone 2000 Speaker Package. We show that a BLS model is able to learn the language characteristics so as to disentangle overlapping voices by language. We achieve a mean SI-SDR improvement of 12.63 dB over 231 test sets. The performance on the individual test sets varies depending on the language combination. Finally, we show that BLS can generalize well to unseen speakers and languages in the mixture."
   ],
   "doi": "10.21437/Interspeech.2022-10187"
  },
  "mirheidari22_interspeech": {
   "authors": [
    [
     "Bahman",
     "Mirheidari"
    ],
    [
     "Andre",
     "Bittar"
    ],
    [
     "Nicholas",
     "Cummins"
    ],
    [
     "Johnny",
     "Downs"
    ],
    [
     "Helen L.",
     "Fisher"
    ],
    [
     "Heidi",
     "Christensen"
    ]
   ],
   "title": "Automatic Detection of Expressed Emotion from Five-Minute Speech Samples: Challenges and Opportunities",
   "original": "10188",
   "page_count": 5,
   "order": 500,
   "p1": 2458,
   "pn": 2462,
   "abstract": [
    "We present a novel feasibility study on the automatic recognition of Expressed Emotion (EE), a family environment concept based on caregivers speaking freely about their relative/family member. We describe an automated approach for determining the degree of warmth, a key component of EE, from acoustic and text features acquired from a sample of 37 recorded interviews. These recordings, collected over 20 years ago, are derived from a nationally representative birth cohort of 2,232 British twin children and were manually coded for EE. We outline the core steps of extracting usable information from recordings with highly variable audio quality and assess the efficacy of four machine learning approaches trained with different combinations of acoustic and text features. Despite the challenges of working with this legacy data, we demonstrated that the degree of warmth can be predicted with an F1-score of 61.5%. In this paper, we summarise our learning and provide recommendations for future work using real-world speech samples."
   ],
   "doi": "10.21437/Interspeech.2022-10188"
  },
  "zhao22j_interspeech": {
   "authors": [
    [
     "Jinzheng",
     "Zhao"
    ],
    [
     "Peipei",
     "Wu"
    ],
    [
     "Xubo",
     "Liu"
    ],
    [
     "Shidrokh",
     "Goudarzi"
    ],
    [
     "Haohe",
     "Liu"
    ],
    [
     "YONG",
     "XU"
    ],
    [
     "Wenwu",
     "Wang"
    ]
   ],
   "title": "Audio Visual Multi-Speaker Tracking with Improved GCF and PMBM Filter",
   "original": "10190",
   "page_count": 5,
   "order": 751,
   "p1": 3704,
   "pn": 3708,
   "abstract": [
    "Audio and visual signals can be used jointly to provide complementary information for multi-speaker tracking. Face detectors and color histogram can provide visual measurements while Direction of Arrival (DOA) lines and global coherence field (GCF) maps can provide audio measurements. GCF, as a traditional sound source localization method, has been widely used to provide audio measurements in audio-visual speaker tracking by estimating the positions of speakers. However, GCF cannot directly deal with the scenarios of multiple speakers due to the emergence of spurious peaks on the GCF map, making it difficult to find the non-dominant speakers. To overcome this limitation, we propose a phase-aware VoiceFilter and a separation-before-localization method, which enables the audio mixture to be separated into individual speech sources while retaining their phases. This allows us to calculate the GCF map for multiple speakers, thereby their positions accurately and concurrently. Based on this method, we design an adaptive audio measurement likelihood for audio-visual multiple speaker tracking using Poisson multi-Bernoulli mixture (PMBM) filter. The experiments demonstrate that our proposed tracker achieves state-of-the-art results on the AV16.3 dataset."
   ],
   "doi": "10.21437/Interspeech.2022-10190"
  },
  "zaiem22_interspeech": {
   "authors": [
    [
     "Salah",
     "Zaiem"
    ],
    [
     "Titouan",
     "Parcollet"
    ],
    [
     "Slim",
     "Essid"
    ]
   ],
   "title": "Automatic Data Augmentation Selection and Parametrization in Contrastive Self-Supervised Speech Representation Learning",
   "original": "10191",
   "page_count": 5,
   "order": 135,
   "p1": 669,
   "pn": 673,
   "abstract": [
    "Contrastive learning enables learning useful audio and speech representations without ground-truth labels by maximizing the similarity between latent representations of similar signal segments. In this framework various data augmentation techniques are usually exploited to help enforce desired invariances within the learned representations, improving performance on various audio tasks thanks to more robust embeddings. Now, selecting the most relevant augmentations has proven crucial for better downstream performances. Thus, this work introduces a conditional independance-based method which allows for automatically selecting a suitable distribution on the choice of augmentations and their parametrization from a set of predefined ones, for contrastive self-supervised pre-training. This is performed with respect to a downstream task of interest, hence saving a costly hyper-parameter search. Experiments performed on two different downstream tasks validate the proposed approach showing better results than experimenting without augmentation or with baseline augmentations. We furthermore conduct a qualitative analysis of the automatically selected augmentations and their variation according to the considered final downstream dataset."
   ],
   "doi": "10.21437/Interspeech.2022-10191"
  },
  "lee22j_interspeech": {
   "authors": [
    [
     "Jaeuk",
     "Lee"
    ],
    [
     "Joon-Hyuk",
     "Chang"
    ]
   ],
   "title": "Advanced Speaker Embedding with Predictive Variance of Gaussian Distribution for Speaker Adaptation in TTS",
   "original": "10193",
   "page_count": 5,
   "order": 606,
   "p1": 2988,
   "pn": 2992,
   "abstract": [
    "Speaker adaptation in text-to-speech (TTS) has three goals: high-quality audio, requirement of a small amount of data for adapting to a new speaker, and fine-tuning few parameters for storage efficiency in commercial service of custom voice. In this paper, we introduce a novel adaptation method to achieve the aforementioned three goals. First, we estimate variances from a speaker embedding and add them back to the speaker embedding. Through this operation, the distribution of each speaker in latent space increases. Moreover, we design a prediction model that could generate a speaker embedding that approximately represents the new speaker's timbre. We can obtain a new speaker embedding well representing the timbre of a new speaker by the search process to the starting point of fine-tuning and the prediction model. We observe the performance change according to the number of fine-tuning parameters. Finally, we evaluate the proposed method using the mean opinion score (MOS) to demonstrate the remarkable performance of our proposed method."
   ],
   "doi": "10.21437/Interspeech.2022-10193"
  },
  "bilinski22_interspeech": {
   "authors": [
    [
     "Piotr",
     "Bilinski"
    ],
    [
     "Thomas",
     "Merritt"
    ],
    [
     "Abdelhamid",
     "Ezzerg"
    ],
    [
     "Kamil",
     "Pokora"
    ],
    [
     "Sebastian",
     "Cygert"
    ],
    [
     "Kayoko",
     "Yanagisawa"
    ],
    [
     "Roberto",
     "Barra-Chicote"
    ],
    [
     "Daniel",
     "Korzekwa"
    ]
   ],
   "title": "Creating New Voices using Normalizing Flows",
   "original": "10195",
   "page_count": 5,
   "order": 600,
   "p1": 2958,
   "pn": 2962,
   "abstract": [
    "Creating realistic and natural-sounding synthetic speech remains a big challenge for voice identities unseen during training. As there is growing interest in synthesizing voices of new speakers, here we investigate the ability of normalizing flows in text-to-speech (TTS) and voice conversion (VC) modes to extrapolate from speakers observed during training to create unseen speaker identities. Firstly, we create an approach for TTS and VC, and then we comprehensively evaluate our methods and baselines in terms of intelligibility, naturalness, speaker similarity, and ability to create new voices. We use both objective and subjective metrics to benchmark our techniques on 2 evaluation tasks: zero-shot and new voice speech synthesis. The goal of the former task is to measure the precision of the conversion to an unseen voice. The goal of the latter is to measure the ability to create new voices. Extensive evaluations demonstrate that the proposed approach systematically allows to obtain state-of-the-art performance in zero-shot speech synthesis and creates various new voices, unobserved in the training set. We consider this work to be the first attempt to synthesize new voices based on mel-spectrograms and normalizing flows, along with a comprehensive analysis and comparison of the TTS and VC modes."
   ],
   "doi": "10.21437/Interspeech.2022-10195"
  },
  "ridouane22_interspeech": {
   "authors": [
    [
     "Rachid",
     "Ridouane"
    ],
    [
     "Philipp",
     "Buech"
    ]
   ],
   "title": "Complex sounds and cross-language influence: The case of ejectives in Omani Mehri",
   "original": "10199",
   "page_count": 5,
   "order": 695,
   "p1": 3433,
   "pn": 3437,
   "abstract": [
    "Ejective consonants are known to considerably vary both cross-linguistically and within individual languages. This variability is often considered a consequence of the complex articulatory strategies involved in their production. Because they are complex, they might be particularly prone to sound change, especially under cross-language influence. In this study, we consider the production of ejectives in Mehri, a Semitic endangered language spoken in Oman where considerable influence from Arabic is expected. We provide acoustic data from seven speakers producing a list of items contrasting ejective and pulmonic alveolar and velar stops in word-initial (/#—/), word-medial (V—V), and word-final (V—#) positions. Different durational and non-durational correlates were examined. The relative importance of these correlates was quantified by the calculation of D-prime values for each. The key empirical finding is that the parameters used to signal ejectivity differ depending mainly on whether the stop is alveolar or velar. Specifically, ejective alveolar stops display characteristics of pharyngealization, similar to Arabic, but velars still maintain attributes of ejectivity in some word positions. We interpret these results as diagnostic of the sound change that is currently in progress, coupled with an ongoing context-dependent neutralization."
   ],
   "doi": "10.21437/Interspeech.2022-10199"
  },
  "eom22_interspeech": {
   "authors": [
    [
     "Youngsik",
     "Eom"
    ],
    [
     "Yeonghyeon",
     "Lee"
    ],
    [
     "Ji Sub",
     "Um"
    ],
    [
     "Hoi Rin",
     "Kim"
    ]
   ],
   "title": "Anti-Spoofing Using Transfer Learning with Variational Information Bottleneck",
   "original": "10200",
   "page_count": 5,
   "order": 722,
   "p1": 3568,
   "pn": 3572,
   "abstract": [
    "Recent advances in sophisticated synthetic speech generated from text-to-speech (TTS) or voice conversion (VC) systems cause threats to the existing automatic speaker verification (ASV) systems. Since such synthetic speech is generated from diverse algorithms, generalization ability with using limited training data is indispensable for a robust anti-spoofing system. In this work, we propose a transfer learning scheme based on the wav2vec 2.0 pretrained model with variational information bottleneck (VIB) for speech anti-spoofing task. Evaluation on the ASVspoof 2019 logical access (LA) database shows that our method improves the performance of distinguishing unseen spoofed and genuine speech, outperforming current state-of-the-art anti-spoofing systems. Furthermore, we show that the proposed system improves performance in low-resource and cross-dataset settings of anti-spoofing task significantly, demonstrating that our system is also robust in terms of data size and data distribution."
   ],
   "doi": "10.21437/Interspeech.2022-10200"
  },
  "mirheidari22b_interspeech": {
   "authors": [
    [
     "Bahman",
     "Mirheidari"
    ],
    [
     "Daniel",
     "Blackburn"
    ],
    [
     "Heidi",
     "Christensen"
    ]
   ],
   "title": "Automatic cognitive assessment: Combining sparse datasets with disparate cognitive scores",
   "original": "10205",
   "page_count": 5,
   "order": 501,
   "p1": 2463,
   "pn": 2467,
   "abstract": [
    "Automatic prediction of cognitive assessment scores through analysis of speech is a challenging task not least due to the lack of available data; this is exacerbated by datasets often being accompanied by disparate cognitive scores as diagnostic practices vary across the world. The ADReSSo 2021 challenge aimed at supporting research in this area and defined a number of tasks including a regression task (predicting Mini-Mental State Examination (MMSE) scores). It saw the successful introduction of a number of BERT-based models including our winning classification approach that successfully applied data augmentation using ASR-generated hypotheses. In this paper, we port this approach to the regression task and further present an investigation into the effect of combining smaller datasets with disparate cognitive scores. In particular, we combine the ADReSSo data with our in-house IVA dataset, which is associated with a different type of cognitive assessment: the Addenbrooke's Cognitive Examination (ACE-III). We show improved performance by converting ACE-III to MMSE scores thus enabling us to combine the two datasets. By selecting good hyper-parameters, the RMSE reduces from 4.45 to 4.40 on the ADReSSo task. Likewise, using the ADReSSo dataset to boost the IVA regression model, decreases RMSE from 3.50 to 3.00."
   ],
   "doi": "10.21437/Interspeech.2022-10205"
  },
  "woszczyk22_interspeech": {
   "authors": [
    [
     "Dominika",
     "Woszczyk"
    ],
    [
     "Anna",
     "Hedlikova"
    ],
    [
     "Alican",
     "Akman"
    ],
    [
     "Soteris",
     "Demetriou"
    ],
    [
     "Björn",
     "Schuller"
    ]
   ],
   "title": "Data Augmentation for Dementia Detection in Spoken Language.",
   "original": "10210",
   "page_count": 5,
   "order": 580,
   "p1": 2858,
   "pn": 2862,
   "abstract": [
    "Dementia is a growing problem as our society ages, and detection methods are often invasive and expensive. Recent deep-learning techniques can offer a faster diagnosis and have shown promis ing results. However, they require large amounts of labelled data which is not easily available for the task of dementia detection. One effective solution to sparse data problems is data augmenta tion, though the exact methods need to be selected carefully. To date, there has been no empirical study of data augmentation on Alzheimer's disease (AD) datasets for NLP and speech process ing. In this work, we investigate data augmentation techniques for the task of AD detection and perform an empirical evaluation of the different approaches on two kinds of models for both the text and audio domains. We use a transformer-based model for both domains, and SVM and Random Forest models for the text and audio domains, respectively. We generate additional samples using traditional as well as deep learning based methods and show that data augmentation improves performance for both the text- and audio-based models and that such results are compara ble to state-of-the-art results on the popular ADReSS set, with carefully crafted architectures and features."
   ],
   "doi": "10.21437/Interspeech.2022-10210"
  },
  "li22q_interspeech": {
   "authors": [
    [
     "Yuhan",
     "Li"
    ],
    [
     "Ying",
     "Shen"
    ],
    [
     "Dongqing",
     "Wang"
    ],
    [
     "Lin",
     "Zhang"
    ]
   ],
   "title": "SiD-WaveFlow: A Low-Resource Vocoder Independent of Prior Knowledge",
   "original": "10222",
   "page_count": 5,
   "order": 328,
   "p1": 1616,
   "pn": 1620,
   "abstract": [
    "Flow-based neural vocoders have demonstrated their effectiveness in generating high-fidelity speech in real-time. However, most flow-based vocoders are computationally heavy models which rely on large amounts of speech for model training. Witnessing the limitations of these vocoders, a new flowbased vocoder, namely Semi-inverse DynamicWaveFlow (SiDWaveFlow), for low-resource speech synthesis is proposed. SiDWaveFlow can generate high-quality speech in real-time with the constraint of limited training data. Specifically, in SiDWaveFlow, a module named Semi-inverse Dynamic Transformation (SiDT) is proposed to improve the synthesis quality as well as the computational efficiency by replacing the affine coupling layers (ACL) used in WaveGlow. In addition, a preemphasis operation is introduced to the training process of SiD-WaveFlow to further improve the quality of the synthesized speech. Experimental results have corroborated that SiDWaveFlow can generate speech with better quality compared with its counterparts. Particularly, the TTS system integrating SiD-WaveFlow vocoder achieves 3.416 and 2.968 mean MOS on CSMSC and LJ Speech data sets, respectively. Besides, SiDWaveFlow converges much faster than WaveGlow at the training stage. Last but not least, SiD-WaveFlow is a lightweight model and can generate speech on edge devices with a much faster inference speed. The source code and demos are available at https://slptongji.github.io/."
   ],
   "doi": "10.21437/Interspeech.2022-10222"
  },
  "zhang22t_interspeech": {
   "authors": [
    [
     "Shimin",
     "Zhang"
    ],
    [
     "Ziteng",
     "Wang"
    ],
    [
     "Yukai",
     "Ju"
    ],
    [
     "Yihui",
     "Fu"
    ],
    [
     "Yueyue",
     "Na"
    ],
    [
     "Qiang",
     "Fu"
    ],
    [
     "Lei",
     "Xie"
    ]
   ],
   "title": "Personalized Acoustic Echo Cancellation for Full-duplex Communications",
   "original": "10225",
   "page_count": 5,
   "order": 512,
   "p1": 2518,
   "pn": 2522,
   "abstract": [
    "Deep neural networks (DNNs) have shown promising results for acoustic echo cancellation (AEC). But the DNN-based AEC models let through all near-end speakers including the interfering speech. In light of recent studies on personalized speech enhancement, we investigate the feasibility of personalized acoustic echo cancellation (PAEC) in this paper for full-duplex communications, where background noise and interfering speakers may coexist with acoustic echoes. Specifically, we first propose a novel backbone neural network termed as gated temporal convolutional neural network (GTCNN) that outperforms state-of-the-art AEC models in performance. Speaker embeddings like d-vectors are further adopted as auxiliary information to guide the GTCNN to focus on the target speaker. A special case in PAEC is that speech snippets of both parties on the call are enrolled. Experimental results show that auxiliary information from either the near-end speaker or the far-end speaker can improve the DNN-based AEC performance. Nevertheless, there is still much room for improvement in the utilization of the finite-dimensional speaker embeddings."
   ],
   "doi": "10.21437/Interspeech.2022-10225"
  },
  "zhang22u_interspeech": {
   "authors": [
    [
     "Bowen",
     "Zhang"
    ],
    [
     "Songjun",
     "Cao"
    ],
    [
     "Xiaoming",
     "Xhang"
    ],
    [
     "Yike",
     "Zhang"
    ],
    [
     "Long",
     "Ma"
    ],
    [
     "Takahiro",
     "Shinozaki"
    ]
   ],
   "title": "Censer: Curriculum Semi-supervised Learning for Speech Recognition Based on Self-supervised Pre-training",
   "original": "10226",
   "page_count": 5,
   "order": 539,
   "p1": 2653,
   "pn": 2657,
   "abstract": [
    "Recent studies have shown that the benefits provided by self-supervised pre-training and self-training (pseudo-labeling) are complementary. Semi-supervised fine-tuning strategies under the pre-training framework, however, remain insufficiently studied. Besides, modern semi-supervised speech recognition algorithms either treat unlabeled data indiscriminately or filter out noisy samples with a confidence threshold. The dissimilarities among different unlabeled data are often ignored. In this paper, we propose Censer, a semi-supervised speech recognition algorithm based on self-supervised pre-training to maximize the utilization of unlabeled data. The pre-training stage of Censer adopts wav2vec2.0 and the fine-tuning stage employs an improved semi-supervised learning algorithm from slimIPL, which leverages unlabeled data progressively according to their pseudo labels' qualities. We also incorporate a temporal pseudo label pool and an exponential moving average to control the pseudo labels' update frequency and to avoid model divergence. Experimental results on Libri-Light and LibriSpeech datasets manifest our proposed method achieves better performance compared to existing approaches while being more unified."
   ],
   "doi": "10.21437/Interspeech.2022-10226"
  },
  "dalhouse22_interspeech": {
   "authors": [
    [
     "Jovan M.",
     "Dalhouse"
    ],
    [
     "Katunobu",
     "Itou"
    ]
   ],
   "title": "Cross-Lingual Transfer Learning Approach to Phoneme Error Detection via Latent Phonetic Representation",
   "original": "10228",
   "page_count": 5,
   "order": 635,
   "p1": 3133,
   "pn": 3137,
   "abstract": [
    "Extensive research has been conducted on CALL systems for Pronunciation Error detection to automate language improvement through self-evaluation. However, many of these previous approaches have relied on HMM or Neural Network Hybrid Models which, although have proven to be effective, often utilize phonetically labelled L2 speech data which is expensive and often scarce. This paper discusses a ”zero-shot” transfer learning approach to detect phonetic errors in L2 English speech by Japanese Native speakers using solely unaligned phonetically labelled Native Language speech. The proposed method introduces a simple base architecture which utilizes the XLSR-Wav2Vec2.0 model pre-trained on unlabelled multilingual speech. Phoneme mapping for each language is determined based on difference of articulation of similar phonemes. This method achieved a Phonetic Error Rate of 0.214 on erroneous L2 speech after fine-tuning on 70 hours of speech with low resource automated phonetic labelling, and proved to additionally model phonemes of the Native Language of the L2 speaker effectively without the need for L2 speech fine-tuning."
   ],
   "doi": "10.21437/Interspeech.2022-10228"
  },
  "wang22s_interspeech": {
   "authors": [
    [
     "Xiaoyu",
     "Wang"
    ],
    [
     "Xiangyu",
     "Kong"
    ],
    [
     "Xiulian",
     "Peng"
    ],
    [
     "Yan",
     "Lu"
    ]
   ],
   "title": "Multi-Modal Multi-Correlation Learning for Audio-Visual Speech Separation",
   "original": "10229",
   "page_count": 5,
   "order": 181,
   "p1": 886,
   "pn": 890,
   "abstract": [
    "In this paper we propose a multi-modal multi-correlation learning framework targeting at the task of audio-visual speech separation. Although previous efforts have been extensively put on combining audio and visual modalities, most of them solely adopt a straightforward concatenation of audio and visual features. To exploit the real useful information behind these two modalities, we define two key correlations which are: (1) identity correlation (between timbre and facial attributes); (2) phonetic correlation (between phoneme and lip motion). These two correlations together comprise the complete information, which shows a certain superiority in separating target speaker's voice especially in some hard cases, such as the same gender or similar content. For implementation, contrastive learning or adversarial training approach is applied to maximize these two correlations. Both of them work well, while adversarial training shows its advantage by avoiding some limitations of contrastive learning. Compared with previous research, our solution demonstrates clear improvement on experimental metrics without additional complexity. Further analysis reveals the validity of the proposed architecture and its good potential for future extension."
   ],
   "doi": "10.21437/Interspeech.2022-10229"
  },
  "zhao22k_interspeech": {
   "authors": [
    [
     "Zihan",
     "Zhao"
    ],
    [
     "Yanfeng",
     "Wang"
    ],
    [
     "Yu",
     "Wang"
    ]
   ],
   "title": "Multi-level Fusion of Wav2vec 2.0 and BERT for Multimodal Emotion Recognition",
   "original": "10230",
   "page_count": 5,
   "order": 957,
   "p1": 4725,
   "pn": 4729,
   "abstract": [
    "The research and applications of multimodal emotion recognition have become increasingly popular recently. However, multimodal emotion recognition faces the challenge of lack of data. To solve this problem, we propose to use transfer learning which leverages state-of-the-art pre-trained models including wav2vec 2.0 and BERT for this task. Multi-level fusion approaches including coattention-based early fusion and late fusion with the models trained on both embeddings are explored. Also, a multi-granularity framework which extracts not only frame-level speech embeddings but also segment-level embeddings including phone, syllable and word-level speech embeddings is proposed to further boost the performance. By combining our coattention-based early fusion model and late fusion model with the multi-granularity feature extraction framework, we obtain result that outperforms best baseline approaches by 1.3% unweighted accuracy (UA) on the IEMOCAP dataset."
   ],
   "doi": "10.21437/Interspeech.2022-10230"
  },
  "mdhaffar22_interspeech": {
   "authors": [
    [
     "Salima",
     "Mdhaffar"
    ],
    [
     "Jarod",
     "Duret"
    ],
    [
     "Titouan",
     "Parcollet"
    ],
    [
     "Yannick",
     "Estève"
    ]
   ],
   "title": "End-to-end model for named entity recognition from speech without paired training data",
   "original": "10231",
   "page_count": 5,
   "order": 825,
   "p1": 4068,
   "pn": 4072,
   "abstract": [
    "Recent works showed that end-to-end neural approaches tend to become very popular for spoken language understanding (SLU). Through the term end-to-end, one considers the use of a single model optimized to extract semantic information directly from the speech signal. A major issue for such models is the lack of paired audio and textual data with semantic annotation. This paper proposes an approach to build an end-to-end neural model to extract semantic information in a scenario in which zero paired audio data is available. Our approach is based on the use of an external model trained to generate a sequence of vectorial representations from text. These representations mimic the hidden representations that could be generated inside an end-to-end automatic speech recognition (ASR) model by processing a speech signal. A SLU neural module is then trained using these representations as input and the annotated text as output. Last, the SLU module replaces the top layers of the ASR model to achieve the construction of the end-to-end model. Our experiments on named entity recognition, carried out on the QUAERO corpus, show that this approach is very promising, getting better results than a comparable cascade approach or than the use of synthetic voices"
   ],
   "doi": "10.21437/Interspeech.2022-10231"
  },
  "scharf22_interspeech": {
   "authors": [
    [
     "Maximilian Karl",
     "Scharf"
    ],
    [
     "Sabine",
     "Hochmuth"
    ],
    [
     "Lena L.N.",
     "Wong"
    ],
    [
     "Birger",
     "Kollmeier"
    ],
    [
     "Anna",
     "Warzybok"
    ]
   ],
   "title": "Lombard Effect for Bilingual Speakers in Cantonese and English: importance of spectro-temporal features",
   "original": "10235",
   "page_count": 5,
   "order": 280,
   "p1": 1377,
   "pn": 1381,
   "abstract": [
    "For a better understanding of the mechanisms underlying speech perception and the contribution of different signal features, computational models of speech recognition have a long tradition in hearing research. Due to the diverse range of situations in which speech needs to be recognized, these models need to be generalizable across many acoustic conditions, speakers, and languages. This contribution examines the importance of different features for speech recognition predictions of plain and Lombard speech for English in comparison to Cantonese in stationary and modulated noise. While Cantonese is a tonal language that encodes information in spectro-temporal features, the Lombard effect is known to be associated with spectral changes in the speech signal. These contrasting properties of tonal languages and the Lombard effect form an interesting basis for the assessment of speech recognition models. Here, an automatic speech recognition-based (ASR) model using spectral or spectro-temporal features is evaluated with empirical data. The results indicate that spectro-temporal features are crucial in order to predict the speaker-specific speech recognition threshold SRT50 in both Cantonese and English as well as to account for the improvement of speech recognition in modulated noise, while effects due to Lombard speech can already be predicted by spectral features."
   ],
   "doi": "10.21437/Interspeech.2022-10235"
  },
  "chen22j_interspeech": {
   "authors": [
    [
     "Zikai",
     "Chen"
    ],
    [
     "Lin",
     "Wu"
    ],
    [
     "Junjie",
     "Pan"
    ],
    [
     "Xiang",
     "Yin"
    ]
   ],
   "title": "An Automatic Soundtracking System for Text-to-Speech Audiobooks",
   "original": "10236",
   "page_count": 5,
   "order": 96,
   "p1": 476,
   "pn": 480,
   "abstract": [
    "Background music (BGM) plays an essential role in audiobooks, which can enhance the immersive experience of audiences and help them better understand the story. However, well-designed BGM still requires human effort in the text-to-speech (TTS) audiobook production, which is quite time-consuming and costly. In this paper, we introduce an automatic soundtracking system for TTS-based audiobooks. The proposed system divides the soundtracking process into three tasks: plot partition, plot classification, and music selection. The experiments shows that both our plot partition module and plot classification module outperform baselines by a large margin. Furthermore, TTS-based audiobooks produced with our proposed automatic soundtracking system achieves comparable performance to that produced with the human soundtracking system. To our best of knowledge, this is the first work of automatic soundtracking system for audiobooks. Demos are available on https://acst1223.github.io/interspeech2022/main."
   ],
   "doi": "10.21437/Interspeech.2022-10236"
  },
  "markitantov22_interspeech": {
   "authors": [
    [
     "Maxim",
     "Markitantov"
    ],
    [
     "Elena",
     "Ryumina"
    ],
    [
     "Dmitry",
     "Ryumin"
    ],
    [
     "Alexey",
     "Karpov"
    ]
   ],
   "title": "Biometric Russian Audio-Visual Extended MASKS (BRAVE-MASKS) Corpus: Multimodal Mask Type Recognition Task",
   "original": "10240",
   "page_count": 5,
   "order": 356,
   "p1": 1756,
   "pn": 1760,
   "abstract": [
    "In this paper, we present a new multimodal corpus called Biometric Russian Audio-Visual Extended MASKS (BRAVE-MASKS), which is designed to analyze voice and facial characteristics of persons wearing various masks, as well as to develop automatic systems for bimodal verification and identification of speakers. In particular, we tackle the multimodal mask type recognition task (6 classes). As a result, audio, visual and multimodal systems were developed, which showed UAR of 54.83%, 72.02% and 82.01%, respectively, on the Test set. These performances are the baseline for the BRAVE-MASKS corpus to compare the follow-up approaches with the proposed systems."
   ],
   "doi": "10.21437/Interspeech.2022-10240"
  },
  "kim22k_interspeech": {
   "authors": [
    [
     "Eesung",
     "Kim"
    ],
    [
     "Jae-Jin",
     "Jeon"
    ],
    [
     "Hyeji",
     "Seo"
    ],
    [
     "Hoon",
     "Kim"
    ]
   ],
   "title": "Automatic Pronunciation Assessment using Self-Supervised Speech Representation Learning",
   "original": "10245",
   "page_count": 5,
   "order": 287,
   "p1": 1411,
   "pn": 1415,
   "abstract": [
    "Self-supervised learning (SSL) approaches such as wav2vec 2.0 and HuBERT models have shown promising results in various downstream tasks in the speech community. In particular, speech representations learned by SSL models have been shown to be effective for encoding various speech-related characteristics. In this context, we propose a novel automatic pronunciation assessment method based on SSL models. First, the proposed method fine-tunes the pre-trained SSL models with connectionist temporal classification to adapt the English pronunciation of English-as-a-second-language (ESL) learners in a data environment. Then, the layer-wise contextual representations are extracted from all across the transformer layers of the SSL models. Finally, the automatic pronunciation score is estimated using bidirectional long short-term memory with the layer-wise contextual representations and the corresponding text. We show that the proposed SSL model-based methods outperform the baselines, in terms of the Pearson correlation coefficient, on datasets of Korean ESL learner children and Speechocean762. Furthermore, we analyze how different representations of transformer layers in the SSL model affect the performance of the pronunciation assessment task."
   ],
   "doi": "10.21437/Interspeech.2022-10245"
  },
  "du22c_interspeech": {
   "authors": [
    [
     "Zongyang",
     "Du"
    ],
    [
     "Berrak",
     "Sisman"
    ],
    [
     "Kun",
     "Zhou"
    ],
    [
     "Haizhou",
     "Li"
    ]
   ],
   "title": "Disentanglement of Emotional Style and Speaker Identity for Expressive Voice Conversion",
   "original": "10249",
   "page_count": 5,
   "order": 529,
   "p1": 2603,
   "pn": 2607,
   "abstract": [
    "Expressive voice conversion performs identity conversion for emotional speakers by jointly converting speaker identity and emotional style. Due to the hierarchical structure of speech emotion, it is challenging to disentangle the emotional style for different speakers. Inspired by the recent success of speaker disentanglement with variational autoencoder (VAE), we propose an any-to-any expressive voice conversion framework, that is called StyleVC. StyleVC is designed to disentangle linguistic content, speaker identity, pitch, and emotional style information. We study the use of style encoder to model emotional style explicitly. At run-time, StyleVC converts both speaker identity and emotional style for arbitrary speakers. Experiments validate the effectiveness of our proposed framework in both objective and subjective evaluations."
   ],
   "doi": "10.21437/Interspeech.2022-10249"
  },
  "shin22c_interspeech": {
   "authors": [
    [
     "Wooseok",
     "Shin"
    ],
    [
     "Hyun Joon",
     "Park"
    ],
    [
     "Jin Sob",
     "Kim"
    ],
    [
     "Byung Hoon",
     "Lee"
    ],
    [
     "Sung Won",
     "Han"
    ]
   ],
   "title": "Multi-View Attention Transfer for Efficient Speech Enhancement",
   "original": "10251",
   "page_count": 5,
   "order": 244,
   "p1": 1198,
   "pn": 1202,
   "abstract": [
    "Recent deep learning models have achieved high performance in speech enhancement; however, it is still challenging to obtain a fast and low-complexity model without significant performance degradation. Previous knowledge distillation studies on speech enhancement could not solve this problem because their output distillation methods do not fit the speech enhancement task in some aspects. In this study, we propose multi-view attention transfer (MV-AT), a feature-based distillation, to obtain efficient speech enhancement models in the time domain. Based on the multi-view features extraction model, MV-AT transfers multi-view knowledge of the teacher network to the student network without additional parameters. The experimental results show that the proposed method consistently improved the performance of student models of various sizes on the Valentini and deep noise suppression (DNS) datasets. MANNER-S-8.1GF with our proposed method, a lightweight model for efficient deployment, achieved 15.4x and 4.71x fewer parameters and floating-point operations (FLOPs), respectively, compared to the baseline model with similar performance."
   ],
   "doi": "10.21437/Interspeech.2022-10251"
  },
  "zhang22v_interspeech": {
   "authors": [
    [
     "Chenggang",
     "Zhang"
    ],
    [
     "JinJiang",
     "Liu"
    ],
    [
     "Xueliang",
     "Zhang"
    ]
   ],
   "title": "LCSM: A Lightweight Complex Spectral Mapping Framework for Stereophonic Acoustic Echo Cancellation",
   "original": "10252",
   "page_count": 5,
   "order": 513,
   "p1": 2523,
   "pn": 2527,
   "abstract": [
    "The traditional adaptive algorithms will face the non-uniqueness problem when dealing with stereophonic acoustic echo cancellation (SAEC). In this paper, we first propose an efficient multi-input and multi-output (MIMO) scheme based on deep learning to filter out echoes from all microphone signals at once. Then, we employ a lightweight complex spectral mapping framework (LCSM) for end-to-end SAEC without decorrelation preprocessing to the loudspeaker signals. Inplace convolution and channel-wise spatial modeling are utilized to ensure the near-end signal information is preserved. Finally, a cross-domain loss function is designed for better generalization capability. Experiments are evaluated on a variety of untrained conditions and results demonstrate that the LCSM significantly outperforms previous methods. Moreover, the proposed causal framework only has 0.55 million parameters, much less than the similar deep learning-based methods, which is important for the resource-limited devices."
   ],
   "doi": "10.21437/Interspeech.2022-10252"
  },
  "chen22k_interspeech": {
   "authors": [
    [
     "Jun",
     "Chen"
    ],
    [
     "Wei",
     "Rao"
    ],
    [
     "Zilin",
     "Wang"
    ],
    [
     "Zhiyong",
     "Wu"
    ],
    [
     "Yannan",
     "Wang"
    ],
    [
     "Tao",
     "Yu"
    ],
    [
     "Shidong",
     "Shang"
    ],
    [
     "Helen",
     "Meng"
    ]
   ],
   "title": "Speech Enhancement with Fullband-Subband Cross-Attention Network",
   "original": "10257",
   "page_count": 5,
   "order": 199,
   "p1": 976,
   "pn": 980,
   "abstract": [
    "FullSubNet has been shown its promising performance on speech enhancement by utilizing both fullband and subband information. However, the relationship between fullband and subband in FullSubNet is achieved by simply concatenating the output of fullband model and subband units. It only supplements the subband units with a small quantity of global information and has not considered the interaction between fullband and subband. This paper proposes a fullband-subband cross-attention (FSCA) module to interactively fuse the global and local information and applies to FullSubNet. This new framework is called as FS-CANet. Moreover, different from FullSubNet, the proposed FS-CANet optimize the fullband extractor by temporal convolutional network (TCN) blocks to further reduce the model size. Experimental results on DNS Challenge - Interspeech 2021 dataset show that the proposed FS-CANet outperforms other state-of-the-art speech enhancement approaches, and demonstrate the effectiveness of fullband-subband cross-attention."
   ],
   "doi": "10.21437/Interspeech.2022-10257"
  },
  "yang22n_interspeech": {
   "authors": [
    [
     "Zhanheng",
     "Yang"
    ],
    [
     "Sining",
     "Sun"
    ],
    [
     "Jin",
     "Li"
    ],
    [
     "Xiaoming",
     "Zhang"
    ],
    [
     "Xiong",
     "Wang"
    ],
    [
     "Long",
     "Ma"
    ],
    [
     "Lei",
     "Xie"
    ]
   ],
   "title": "CaTT-KWS: A Multi-stage Customized Keyword Spotting Framework based on Cascaded Transducer-Transformer",
   "original": "10258",
   "page_count": 5,
   "order": 341,
   "p1": 1681,
   "pn": 1685,
   "abstract": [
    "Customized keyword spotting~(KWS) has great potential to be deployed on edge devices to achieve hands-free user experience. However, in real applications, false alarm (FA) would be a serious problem for spotting dozens or even hundreds of keywords, which drastically affects user experience. To solve this problem, in this paper, we leverage the recent advances in transducer and transformer based acoustic models and propose a new multi-stage customized KWS framework named Cascaded Transducer-Transformer KWS~(CaTT-KWS), which includes a transducer based keyword detector, a frame-level phone predictor based force alignment module and a transformer based decoder. Specifically, the streaming transducer module is used to spot keyword candidates in audio stream. Then force alignment is implemented using the phone posteriors predicted by the phone predictor to finish the first stage keyword verification and refine the time boundaries of keyword. Finally, the transformer decoder further verifies the triggered keyword. Our proposed CaTT-KWS framework reduces FA rate effectively without obviously hurting keyword recognition accuracy. Specifically, we can get impressively 0.13 FA per hour on a challenging dataset, with over 90\\% relative reduction on FA comparing to the transducer based detection model, while keyword recognition accuracy only drops less than 2%."
   ],
   "doi": "10.21437/Interspeech.2022-10258"
  },
  "zhang22w_interspeech": {
   "authors": [
    [
     "Li",
     "Zhang"
    ],
    [
     "Yue",
     "Li"
    ],
    [
     "Huan",
     "Zhao"
    ],
    [
     "Qing",
     "Wang"
    ],
    [
     "Lei",
     "Xie"
    ]
   ],
   "title": "Backend Ensemble for Speaker Verification and Spoofing Countermeasure",
   "original": "10259",
   "page_count": 5,
   "order": 888,
   "p1": 4381,
   "pn": 4385,
   "abstract": [
    "This paper describes the NPU system submitted to Spoofing Aware Speaker Verification Challenge 2022. We particularly focus on the \\textit{backend ensemble} for speaker verification and spoofing countermeasure from three aspects. Firstly, besides simple concatenation, we propose circulant matrix transformation and stacking for speaker embeddings and countermeasure embeddings. With the stacking operation of newly-defined circulant embeddings, we almost explore all the possible interactions between speaker embeddings and countermeasure embeddings. Secondly, we attempt different convolution neural networks to selectively fuse the embeddings' salient regions into channels with convolution kernels. Finally, we design parallel attention in 1D convolution neural networks to learn the global correlation in channel dimensions as well as to learn the important parts in feature dimensions. Meanwhile, we embed squeeze-and-excitation attention in 2D convolutional neural networks to learn the global dependence among speaker embeddings and countermeasure embeddings. Experimental results demonstrate that all the above methods are effective. After fusion of four well-trained models enhanced by the mentioned methods, the best SASV-EER, SPF-EER and SV-EER we achieve are 0.559%, 0.354% and 0.857% on the evaluation set respectively. Together with the above contributions, our submission system achieves the fifth place in this challenge."
   ],
   "doi": "10.21437/Interspeech.2022-10259"
  },
  "yang22o_interspeech": {
   "authors": [
    [
     "Zhengdong",
     "Yang"
    ],
    [
     "Wangjin",
     "Zhou"
    ],
    [
     "Chenhui",
     "Chu"
    ],
    [
     "Sheng",
     "Li"
    ],
    [
     "Raj",
     "Dabre"
    ],
    [
     "Raphael",
     "Rubino"
    ],
    [
     "Yi",
     "Zhao"
    ]
   ],
   "title": "Fusion of Self-supervised Learned Models for MOS Prediction",
   "original": "10262",
   "page_count": 5,
   "order": 1103,
   "p1": 5443,
   "pn": 5447,
   "abstract": [
    "We participated in the mean opinion score (MOS) prediction challenge, 2022. This challenge aims to predict MOS scores of synthetic speech on two tracks, the main track and a more challenging sub-track: out-of-domain (OOD). To improve the accuracy of the predicted scores, we have explored several model fusion-related strategies and proposed a fused framework in which seven pretrained self-supervised learned (SSL) models have been engaged. These pretrained SSL models are derived from three ASR frameworks, including Wav2Vec, Hubert, and WavLM. For the OOD track, we followed the 7 SSL models selected on main track and adopted a semi-supervised learning method to exploit the unlabeled data. According to the official analysis results, we have achieved the highest LCC, SRCC, and KTAU scores at the system level on main track, as well as the best performance on the LCC, SRCC, and KTAU evaluation metrics at the utterance level on OOD track. Compared with the basic SSL models, the prediction accuracy of the fused system has been largely improved, especially on OOD sub-track."
   ],
   "doi": "10.21437/Interspeech.2022-10262"
  },
  "wang22t_interspeech": {
   "authors": [
    [
     "Rui",
     "Wang"
    ],
    [
     "Qibing",
     "Bai"
    ],
    [
     "Junyi",
     "Ao"
    ],
    [
     "Long",
     "Zhou"
    ],
    [
     "Zhixiang",
     "Xiong"
    ],
    [
     "Zhihua",
     "Wei"
    ],
    [
     "Yu",
     "Zhang"
    ],
    [
     "Tom",
     "Ko"
    ],
    [
     "Haizhou",
     "Li"
    ]
   ],
   "title": "LightHuBERT: Lightweight and Configurable Speech Representation Learning with Once-for-All Hidden-Unit BERT",
   "original": "10269",
   "page_count": 5,
   "order": 342,
   "p1": 1686,
   "pn": 1690,
   "abstract": [
    "Self-supervised speech representation learning has shown promising results in various speech processing tasks. However, the pre-trained models, e.g., HuBERT, are storage-intensive Transformers, limiting their scope of applications under low-resource settings. To this end, we propose LightHuBERT, a once-for-all Transformer compression framework, to find the desired architectures automatically by pruning structured parameters. More precisely, we create a Transformer-based supernet that is nested with thousands of weight-sharing subnets and design a two-stage distillation strategy to leverage the contextualized latent representations from HuBERT. Experiments on automatic speech recognition (ASR) and the SUPERB benchmark show the proposed LightHuBERT enables over 10^9 architectures concerning the embedding dimension, attention dimension, head number, feed-forward network ratio, and network depth. LightHuBERT outperforms the original HuBERT on ASR and five SUPERB tasks with the HuBERT size, achieves comparable performance to the teacher model in most tasks with a reduction of 29% parameters, and obtains a 3.5x compression ratio in three SUPERB tasks, e.g., automatic speaker verification, keyword spotting, and intent classification, with a slight accuracy loss. The code and pre-trained models are available at https://github.com/mechanicalsea/lighthubert."
   ],
   "doi": "10.21437/Interspeech.2022-10269"
  },
  "fatehi22_interspeech": {
   "authors": [
    [
     "Kavan",
     "Fatehi"
    ],
    [
     "Mercedes",
     "Torres Torres"
    ],
    [
     "Ayse",
     "Kucukyilmaz"
    ]
   ],
   "title": "ScoutWav: Two-Step Fine-Tuning on Self-Supervised Automatic Speech Recognition for Low-Resource Environments",
   "original": "10270",
   "page_count": 5,
   "order": 713,
   "p1": 3523,
   "pn": 3527,
   "abstract": [
    "Recent improvements in Automatic Speech Recognition (ASR) systems obtain extraordinary results. However, there are specific domains where training data can be either limited or not representative enough, which are known as Low-Resource Environments (LRE). In this paper, we present ScoutWav, a network that integrates context-based word boundaries with self-supervised learning, wav2vec 2.0, to present a low-resource ASR model. First, we pre-train a model on High-Resource Environment (HRE) datasets and then fine-tune with the LRE datasets to obtain context-based word boundaries. The resulting word boundaries are used for fine-tuning with a pre-trained and iteratively refined wav2vec 2.0 to learn appropriate representations for the downstream ASR task. Our refinement strategy for wav2vec 2.0 comes determined by using canonical correlation analysis (CCA) to detect which layers need updating. This dynamic refinement allows wav2vec 2.0 to learn more descriptive LRE-based representations. Finally, the learned representations in the two-step fine-tuned wav2vec 2.0 framework are fed back to the Scout Network for the downstream task. We carried out experiments with two different LRE datasets: I-CUBE and UASpeech. Our experiments demonstrate that using the target domain word boundary after pre-training and automatic layer analysis, ScoutWav shows up to 12% relative WER reduction on the LR data."
   ],
   "doi": "10.21437/Interspeech.2022-10270"
  },
  "mostaani22_interspeech": {
   "authors": [
    [
     "Zohreh",
     "Mostaani"
    ],
    [
     "Mathew Magimai",
     "Doss"
    ]
   ],
   "title": "On Breathing Pattern Information in Synthetic Speech",
   "original": "10271",
   "page_count": 5,
   "order": 562,
   "p1": 2768,
   "pn": 2772,
   "abstract": [
    "The respiratory system is an integral part of human speech production. As a consequence, there is a close relation between respiration and speech signal, and the produced speech signal carries breathing pattern related information. Speech can also be generated using speech synthesis systems. In this paper, we investigate whether synthetic speech carries breathing pattern related information in the same way as natural human speech. We address this research question in the framework of logical-access presentation attack detection using embeddings extracted from neural networks pre-trained for speech breathing pattern estimation. Our studies on ASVSpoof 2019 challenge data show that there is a clear distinction between the extracted breathing pattern embedding of natural human speech and synthesized speech, indicating that speech synthesis systems tend to not carry breathing pattern related information in the same way as human speech. Whilst, this is not the case with voice conversion of natural human speech."
   ],
   "doi": "10.21437/Interspeech.2022-10271"
  },
  "ribeiro22_interspeech": {
   "authors": [
    [
     "Vinicius",
     "Ribeiro"
    ],
    [
     "Yves",
     "Laprie"
    ]
   ],
   "title": "Autoencoder-Based Tongue Shape Estimation During Continuous Speech",
   "original": "10272",
   "page_count": 5,
   "order": 18,
   "p1": 86,
   "pn": 90,
   "abstract": [
    "Vocal tract shape estimation is a necessary step for articulatory speech synthesis. However, the literature on the topic is scarce, and most current methods lack adequacy to many physical constraints related to speech production. This study proposes an alternative approach to the task to solve specific issues faced in the previous work, especially those related to critical articulators. We present an autoencoder-based method for tongue shape estimation during continuous speech. An autoencoder is trained to learn the data's encoding and serves as an auxiliary network for the principal one, which maps phonemes to the shapes. Instead of predicting the exact points in the target curve, the neural network learns how to predict the curve's main components, i.e., the autoencoder's representation. We show how this approach allows imposing critical articulators' constraints, controlling the tongue shape through the latent space, and generating a smooth output without relying on any postprocessing method."
   ],
   "doi": "10.21437/Interspeech.2022-10272"
  },
  "dang22_interspeech": {
   "authors": [
    [
     "Ting",
     "Dang"
    ],
    [
     "Thomas",
     "Quinnell"
    ],
    [
     "Cecilia",
     "Mascolo"
    ]
   ],
   "title": "Exploring Semi-supervised Learning for Audio-based COVID-19 Detection using FixMatch",
   "original": "10274",
   "page_count": 5,
   "order": 502,
   "p1": 2468,
   "pn": 2472,
   "abstract": [
    "While there has been recent success in audio-based COVID-19 detection, challenges still exist in developing more reliable and generalised models due to the limited amount of high quality labelled audio recordings. With a substantial amount of unlabelled audio recordings available, exploring semi-supervised learning (SSL) may benefit COVID-19 detection by incorporating this extra data. In this paper, we propose a SSL framework which adjusted FixMatch, one of the most advanced SSL approaches, to audio signals and explored its effectiveness in COVID-19 detection. The proposed framework is validated with a crowd-sourced audio database collected from our app, and showed superior performance over supervised models with a maximum of 7.2\\% relative improvement. Furthermore, we demonstrated that the proposed framework significantly benefits model development using imbalanced datasets, which is a common challenge in clinical data. It can also improve model generalisation. This potentially paves a new pathway of utilising unlabelled data effectively to build more accurate and reliable COVID-19 detection tools."
   ],
   "doi": "10.21437/Interspeech.2022-10274"
  },
  "yang22p_interspeech": {
   "authors": [
    [
     "Haoquan",
     "Yang"
    ],
    [
     "Liqun",
     "Deng"
    ],
    [
     "Yu Ting",
     "Yeung"
    ],
    [
     "Nianzu",
     "Zheng"
    ],
    [
     "Yong",
     "Xu"
    ]
   ],
   "title": "Streamable Speech Representation Disentanglement and Multi-Level Prosody Modeling for Live One-Shot Voice Conversion",
   "original": "10277",
   "page_count": 5,
   "order": 524,
   "p1": 2578,
   "pn": 2582,
   "abstract": [
    "This paper takes efforts to tackle the challenge of \"live” one-shot voice conversion (VC), which performs conversion across arbitrary speakers in a streaming way while retaining high intelligibility and naturalness. We propose a hybrid unsupervised and supervised learning based VC model with a two-stage model training strategy. Specially, we first employ an unsupervised disentanglement framework to separate speech representations of different granularity using mutual information constraint and vector quantization technique. Then we augment linguistic content modeling with a supervised ASR acoustic encoder. To perform live conversion, we design the model with streamable neural networks and run the model in streaming mode with sliding windows. Experimental results demonstrate that our proposed method achieves comparable performance on speech naturalness, intelligibility and speaker similarity with offline VC solutions, with sufficient efficiency for practical real-time applications. Audio samples are available online for demonstration."
   ],
   "doi": "10.21437/Interspeech.2022-10277"
  },
  "li22r_interspeech": {
   "authors": [
    [
     "Jin",
     "Li"
    ],
    [
     "Xin",
     "Fang"
    ],
    [
     "Fan",
     "Chu"
    ],
    [
     "Tian",
     "Gao"
    ],
    [
     "Yan",
     "Song"
    ],
    [
     "Rong Li",
     "Dai"
    ]
   ],
   "title": "Acoustic Feature Shuffling Network for Text-independent Speaker Verification",
   "original": "10278",
   "page_count": 5,
   "order": 970,
   "p1": 4790,
   "pn": 4794,
   "abstract": [
    "Deep embedding learning methods have shown state-of-the-art performance for text-independent speaker verification(SV) tasks, compared to the traditional i-vectors. Existing methods mainly focus on designing frame-level feature extraction structures, utterance-level aggregation methods and loss functions to learn effective speaker embeddings. However, due to the locality property of frame-level extraction, the resulting embeddings will be different if we shuffle the sequential order of the input utterance. On the contrary, the conventional i-vector methods are order-insensitive. In this paper, we propose an acoustic feature shuffling network to learn the order-insensitive speaker embeddings via a joint learning method. Specifically, the input utterance is first organized into multi-scale segments. Then, these segments are randomly shuffled to form the input of the deep embedding learning architecture. A symmetric Kullback-Leibler(KL-)Divergence loss, in addition to the Cross-Entropy (CE) loss, is used to force the learned architecture to be order-insensitive. Experimental results of benchmark Voxceleb corpus demonstrate the effectiveness of the proposed acoustic feature shuffling network."
   ],
   "doi": "10.21437/Interspeech.2022-10278"
  },
  "yu22_interspeech": {
   "authors": [
    [
     "Cheng",
     "Yu"
    ],
    [
     "Szu-wei",
     "Fu"
    ],
    [
     "Tsun-An",
     "Hsieh"
    ],
    [
     "Yu",
     "Tsao"
    ],
    [
     "Mirco",
     "Ravanelli"
    ]
   ],
   "title": "OSSEM: one-shot speaker adaptive speech enhancement using meta learning",
   "original": "10283",
   "page_count": 5,
   "order": 200,
   "p1": 981,
   "pn": 985,
   "abstract": [
    "Although deep learning (DL) has achieved notable progress in speech enhancement (SE), further research is still required for a DL-based SE system to adapt effectively and efficiently to particular speakers. In this study, we propose a novel meta-learning-based speaker-adaptive SE approach (called OSSEM) that aims to achieve SE model adaptation in a one-shot manner. OOSSEM consists of a modified transformer SE network and a speaker specific masking (SSM) network. In practice, the SSM network uses enrolled speaker embeddings extracted using ECAPA-TDNN to adjust input features through masking. To evaluate OSSEM, we design a modified Voice Bank-DEMAND dataset containing the first noisy utterances from speakers in the test set for model adaptation and the remaining utterances for testing performance. Furthermore, we set the constraints to be able to perform the SE process in real time, thereby designing OSSEM as a causal SE system. The experimental results first show that OSSEM can effectively adapt the SE model to a specific speaker using only one of his/her noisy utterances, thereby improving SE results. Meanwhile, OSSEM exhibits competitive performance compared to state-of-the-art causal SE systems."
   ],
   "doi": "10.21437/Interspeech.2022-10283"
  },
  "suzuki22_interspeech": {
   "authors": [
    [
     "Naoaki",
     "Suzuki"
    ],
    [
     "Satoshi",
     "Nakamura"
    ]
   ],
   "title": "Representing 'how you say' with 'what you say': English corpus of focused speech and text reflecting corresponding implications",
   "original": "10284",
   "page_count": 5,
   "order": 1008,
   "p1": 4980,
   "pn": 4984,
   "abstract": [
    "In speech communication, how something is said (paralinguistic information) is as crucial as what is said (linguistic information). As a type of paralinguistic information, English speech uses sentence stress, the heaviest prominence within a sentence, to convey emphasis. While different placements of sentence stress communicate different emphatic implications, current speech translation systems return the same translations if the utterances are linguistically identical, losing paralinguistic information. Concentrating on focus, a type of emphasis, we propose mapping paralinguistic information into the linguistic domain within the source language using lexical and grammatical devices. This method enables us to translate the paraphrased text representations instead of the transcription of the original speech and obtain translations that preserve paralinguistic information. As a first step, we present the collection of an English corpus containing speech that differed in the placement of focus along with the corresponding text, which was designed to reflect the implied meaning of the speech. Also, analyses of our corpus demonstrated that mapping of focus from the paralinguistic domain into the linguistic domain involved various lexical and grammatical methods. The data and insights from our analysis will further advance research into paralinguistic translation. The corpus will be published via LDC and our website."
   ],
   "doi": "10.21437/Interspeech.2022-10284"
  },
  "zhang22x_interspeech": {
   "authors": [
    [
     "Shuai",
     "Zhang"
    ],
    [
     "Jiangyan",
     "Yi"
    ],
    [
     "Zhengkun",
     "Tian"
    ],
    [
     "Jianhua",
     "Tao"
    ],
    [
     "Yu Ting",
     "Yeung"
    ],
    [
     "Liqun",
     "Deng"
    ]
   ],
   "title": "reducing multilingual context confusion for end-to-end code-switching automatic speech recognition",
   "original": "10286",
   "page_count": 5,
   "order": 789,
   "p1": 3894,
   "pn": 3898,
   "abstract": [
    "Code-switching deals with alternative languages in communication process. Training end-to-end (E2E) automatic speech recognition (ASR) systems for code-switching is especially challenging as code-switching training data are always insufficient to combat the increased multilingual context confusion due to the presence of more than one language. We propose a language-related attention mechanism to reduce multilingual context confusion for the E2E code-switching ASR model based on the Equivalence Constraint (EC) Theory. The linguistic theory requires that any monolingual fragment that occurs in the code-switching sentence must occur in one of the monolingual sentences. The theory establishes a bridge between monolingual data and code-switching data. We leverage this linguistics theory to design the code-switching E2E ASR model. The proposed model efficiently transfers language knowledge from rich monolingual data to improve the performance of the code-switching ASR model. We evaluate our model on ASRU 2019 Mandarin-English code-switching challenge dataset. Compared to the baseline model, our proposed model achieves a 17.12% relative error reduction."
   ],
   "doi": "10.21437/Interspeech.2022-10286"
  },
  "wei22d_interspeech": {
   "authors": [
    [
     "Yuheng",
     "Wei"
    ],
    [
     "Junzhao",
     "Du"
    ],
    [
     "Hui",
     "Liu"
    ],
    [
     "Qian",
     "Wang"
    ]
   ],
   "title": "CTFALite: Lightweight Channel-specific Temporal and Frequency Attention Mechanism for Enhancing the Speaker Embedding Extractor",
   "original": "10288",
   "page_count": 5,
   "order": 69,
   "p1": 341,
   "pn": 345,
   "abstract": [
    "Attention mechanism provides an effective and plug-and-play feature enhancement module for speaker embedding extractors. Attention-based pooling layers have been widely used to aggregate a sequence of frame-level feature vectors into an utterance-level speaker embedding. Besides, convolution attention mechanisms are introduced into convolution blocks to improve the sensibility of speaker embedding extractors to those features with more discriminative speaker characteristics. However, it is still a challenging problem to make a good trade off between performance and model complexity for convolution attention models, especially for speaker recognition systems on low-resource edge computing nodes (smartphone, embedded devices, etc.). In this paper, we propose a lightweight convolution attention model named as CTFALite, which learns channel-specific temporal attention and frequency attention by leveraging both of the global context information and the local cross-channel dependencies. Experiment results demonstrate the effectiveness of CTFALite for improving performance. The further analysis about computational resource consumption shows that CTFALite achieves a better trade-off between performance and computational complexity, compared to other competing lightweight convolution attention mechanisms."
   ],
   "doi": "10.21437/Interspeech.2022-10288"
  },
  "gorai22_interspeech": {
   "authors": [
    [
     "Takeru",
     "Gorai"
    ],
    [
     "Daisuke",
     "Saito"
    ],
    [
     "Nobuaki",
     "Minematsu"
    ]
   ],
   "title": "Text-to-speech synthesis using spectral modeling based on non-negative autoencoder",
   "original": "10290",
   "page_count": 5,
   "order": 329,
   "p1": 1621,
   "pn": 1625,
   "abstract": [
    "This paper proposes a statistical parametric speech synthesis system that uses non-negative autoencoder (NAE) for spectral modeling. NAE is a model that extends non-negative matrix factorization (NMF) as neural networks. In the proposed method, we employ latent variables in NAE as acoustic features. Reconstruction of spectral information and estimation of latent variables are simultaneously trained. The non-negativity of latent variables in NAE is expected to contribute to dimensionality reduction such that the fine structure of the spectral envelopes is preserved. Experimental results demonstrates the effectiveness of the proposed framework. We also study multispeaker modeling where each of NAEs corresponds to each single speaker. In addition, a neural source-filter (NSF) model was applied to the waveform generation. When a neural vocoder is trained with natural acoustic features and tested with synthesized features, quality degradation occurs due to the mismatch between training and test data. In order to mitigate the mismatch, this system uses features obtained by reconstructing natural speech using NAE for training. Experimental results show that reconstructed features are similar to synthesized features, and as a result, the quality of the synthesized speech is improved."
   ],
   "doi": "10.21437/Interspeech.2022-10290"
  },
  "mallolragolta22_interspeech": {
   "authors": [
    [
     "Adria",
     "Mallol-Ragolta"
    ],
    [
     "Helena",
     "Cuesta"
    ],
    [
     "Emilia",
     "Gomez"
    ],
    [
     "Björn",
     "Schuller"
    ]
   ],
   "title": "Multi-Type Outer Product-Based Fusion of Respiratory Sounds for Detecting COVID-19",
   "original": "10291",
   "page_count": 5,
   "order": 441,
   "p1": 2163,
   "pn": 2167,
   "abstract": [
    "This work presents an outer product-based approach to fuse the embedded representations learnt from the spectrograms of cough, breath, and speech samples for the automatic detection of COVID-19. To extract deep learnt representations from the spectrograms, we compare the performance of specific Convolutional Neural Networks (CNNs) trained from scratch and ResNet18-based CNNs fine-tuned for the task at hand. Furthermore, we investigate whether the patients' sex and the use of contextual attention mechanisms are beneficial. Our experiments use the dataset released as part of the Second Diagnosing COVID-19 using Acoustics (DiCOVA) Challenge. The results suggest the suitability of fusing breath and speech information to detect COVID-19. An Area Under the Curve (AUC) of 84.06 % is obtained on the test partition when using specific CNNs trained from scratch with contextual attention mechanisms. When using ResNet18-based CNNs for feature extraction, the baseline model scores the highest performance with an AUC of 84.26 %."
   ],
   "doi": "10.21437/Interspeech.2022-10291"
  },
  "lim22_interspeech": {
   "authors": [
    [
     "Dan",
     "Lim"
    ],
    [
     "Sunghee",
     "Jung"
    ],
    [
     "Eesung",
     "Kim"
    ]
   ],
   "title": "JETS: Jointly Training FastSpeech2 and HiFi-GAN for End to End Text to Speech",
   "original": "10294",
   "page_count": 5,
   "order": 5,
   "p1": 21,
   "pn": 25,
   "abstract": [
    "In neural text-to-speech (TTS), two-stage system or a cascade of separately learned models have shown synthesis quality close to human speech. For example, FastSpeech2 transforms an input text to a mel-spectrogram and then HiFi-GAN generates a raw waveform from a mel-spectogram where they are called an acoustic feature generator and a neural vocoder respectively. However, their training pipeline is somewhat cumbersome in that it requires a fine-tuning and an accurate speech-text alignment for optimal performance. In this work, we present end-to-end text-to-speech (E2E-TTS) model which has a simplified training pipeline and outperforms a cascade of separately learned models. Specifically, our proposed model is jointly trained FastSpeech2 and HiFi-GAN with an alignment module. Since there is no acoustic feature mismatch between training and inference, it does not requires fine-tuning. Furthermore, we remove dependency on an external speech-text alignment tool by adopting an alignment learning objective in our joint training framework. Experiments on LJSpeech corpus shows that the proposed model outperforms publicly available, state-of-the-art implementations of ESPNet2-TTS on subjective evaluation (MOS) and some objective evaluations."
   ],
   "doi": "10.21437/Interspeech.2022-10294"
  },
  "magistro22_interspeech": {
   "authors": [
    [
     "Giuseppe",
     "Magistro"
    ],
    [
     "Claudia",
     "Crocco"
    ]
   ],
   "title": "Phonetic erosion and information structure in function words: the case of mia",
   "original": "10305",
   "page_count": 5,
   "order": 19,
   "p1": 91,
   "pn": 95,
   "abstract": [
    "The purpose of this paper is to examine the prosodic correlates of a grammaticalisation process that leads to the formation of a function word. In particular, our case study will tackle the pattern of negation renewal known as Jespersen's Cycle (JC). In JC, a negative reinforcer carrying contrastive meaning grammaticalises to a function word denoting polar negation. We want to show that this change fits in with prosodic change: specifically, the grammaticalised item undergoes prosodic reduction. We test the latter hypothesis on the peculiar Italo-Romance dialect Gazzolese, where mia, the particle undergoing JC, can be used both as the erstwhile contrastive function and as a function word denoting negation (it can appear, for example, in Broad Focus statements). The results confirm that when mia is used as a function word, it displays a shorter duration, a reduced intensity excursion, and does not associate with a pitch accent, in comparison to the original contrastive context. These results show that the change in function word can be appreciated on different phonetic/phonological levels: the metrical one and the intonational one, mediated through the role of the lexical item within information structure."
   ],
   "doi": "10.21437/Interspeech.2022-10305"
  },
  "chen22l_interspeech": {
   "authors": [
    [
     "Qi",
     "Chen"
    ],
    [
     "BingHuai",
     "Lin"
    ],
    [
     "YanLu",
     "Xie"
    ]
   ],
   "title": "An Alignment Method Leveraging Articulatory Features for Mispronunciation Detection and Diagnosis in L2 English",
   "original": "10309",
   "page_count": 5,
   "order": 880,
   "p1": 4342,
   "pn": 4346,
   "abstract": [
    "Mispronunciation Detection and Diagnosis (MD&D) technology is used for detecting mispronunciations and providing feedback. Most MD&D systems are based on phoneme recognition. However, few studies have made use of the predefined reference text which has been provided to second language (L2) learners while practicing pronunciation. In this paper, we propose a novel alignment method based on linguistic knowledge of articulatory manners and places to align the phone sequences of the reference text with L2 learners speech. After getting the alignment results, we concatenate the corresponding phoneme embedding and the acoustic features of each speech frame as input. This method makes reasonable use of the reference text information as extra input. Experimental results show that the model can implicitly learn valid information in the reference text by this method. Meanwhile, it avoids introducing misleading information in the reference text, which will cause false acceptance (FA). Besides, the method incorporates articulatory features, which helps the model recognize phonemes. We evaluate the method on the L2-ARCTIC dataset and it turns out that our approach improves the F1-score over the state-of-the-art system by 4.9% relative."
   ],
   "doi": "10.21437/Interspeech.2022-10309"
  },
  "wen22_interspeech": {
   "authors": [
    [
     "Yan",
     "Wen"
    ],
    [
     "Zhenchun",
     "Lei"
    ],
    [
     "Yingen",
     "Yang"
    ],
    [
     "Changhong",
     "Liu"
    ],
    [
     "Minglei",
     "Ma"
    ]
   ],
   "title": "Multi-Path GMM-MobileNet Based on Attack Algorithms and Codecs for Synthetic Speech and Deepfake Detection",
   "original": "10312",
   "page_count": 5,
   "order": 971,
   "p1": 4795,
   "pn": 4799,
   "abstract": [
    "The generalization ability of the speech spoofing detection system in real unseen sources is a great challenge. Spoofed speech from different attack algorithms or codecs has different feature distribution, which is the variant of the genuine speech. The conventional GMM describes the common distribution of all speech feature. But the GMM does not pay attention to the specificity of speech generated using an attack algorithm or codec, which may be useful to model the feature distribution of speech from unknown source. We propose the multi-path GMM-MobileNet model, which includes the GMMs trained on genuine and spoofed speech generated using various attack algorithms or codecs respectively. The 1-D variant of the MobileNet structure is used to extract embedding vector, and the multi-path structure is used to improve the generalization ability. On ASVspoof 2021 LA task, the M-GMM-MobileNet achieves a minimum t-DCF of 0.3231 and an EER of 6.80%, which relatively reduce by 6.2% and 26.6% compared with the LFCC-LCNN baseline. On the ASVspoof 2021 DF task, the M-GMM-MobileNet achieves an EER of 16.86%, which relatively reduce by 24.7% compared with the RawNet2 baseline. Compared with the systems on the ASVspoof 2021 DF leaderboard, our model is competitive."
   ],
   "doi": "10.21437/Interspeech.2022-10312"
  },
  "shu22_interspeech": {
   "authors": [
    [
     "Xiaofeng",
     "Shu"
    ],
    [
     "Yanjie",
     "Chen"
    ],
    [
     "Chuxiang",
     "Shang"
    ],
    [
     "Yan",
     "Zhao"
    ],
    [
     "Chengshuai",
     "Zhao"
    ],
    [
     "Yehang",
     "Zhu"
    ],
    [
     "Chuanzeng",
     "Huang"
    ],
    [
     "Yuxuan",
     "Wang"
    ]
   ],
   "title": "Non-intrusive Speech Quality Assessment with a Multi-Task Learning based Subband Adaptive Attention Temporal Convolutional Neural Network",
   "original": "10315",
   "page_count": 5,
   "order": 668,
   "p1": 3298,
   "pn": 3302,
   "abstract": [
    "In terms of subjective evaluations, speech quality has been generally described by a mean opinion score (MOS). In recent years, non-intrusive speech quality assessment shows an active progress by leveraging deep learning techniques. In this paper, we propose a new multi-task learning based model, termed as subband adaptive attention temporal convolutional neural net- work (SAA-TCN), to perform non-intrusive speech quality assessment with the help of MOS value interval detector (VID) auxiliary task. Instead of using fullband magnitude spectrogram, the proposed model takes subband magnitude spectrogram as the input to reduce model parameters and prevent over- fitting. To effectively utilize the energy distribution information along the subband frequency dimension, subband adaptive attention (SAA) is employed to enhance the TCN model. Experimental results reveal that the proposed method achieves a superior performance on predicting the MOS values. In ConferencingSpeech 2022 Challenge, our method achieves a mean Pearson's correlation coefficient (PCC) score of 0.763 and out- performs the challenge baseline method by 0.233."
   ],
   "doi": "10.21437/Interspeech.2022-10315"
  },
  "wang22u_interspeech": {
   "authors": [
    [
     "Chao",
     "Wang"
    ],
    [
     "Zhonghao",
     "Li"
    ],
    [
     "Benlai",
     "Tang"
    ],
    [
     "Xiang",
     "Yin"
    ],
    [
     "Yuan",
     "Wan"
    ],
    [
     "Yibiao",
     "Yu"
    ],
    [
     "Zejun",
     "Ma"
    ]
   ],
   "title": "Towards high-fidelity singing voice conversion with acoustic reference and contrastive predictive coding",
   "original": "10316",
   "page_count": 5,
   "order": 869,
   "p1": 4287,
   "pn": 4291,
   "abstract": [
    "Recently, phonetic posteriorgrams (PPGs) based methods have been quite popular in non-parallel singing voice conversion systems. However, due to the lack of acoustic information in PPGs, style and naturalness of the converted singing voices are still limited. To solve these problems, in this paper, we utilize an acoustic ref- erence encoder to implicitly model singing characteristics. We experiment with different auxiliary features, including mel spectrograms, HuBERT, and the middle hidden feature (PPG-Mid) of pretrained automatic speech recognition (ASR) model, as the input of the reference encoder, and finally find the HuBERT feature is the best choice. In addition, we use contrastive predictive coding(CPC) module to further smooth the voices by predicting future observations in latent space. Experiments show that, compared with the baseline models, our proposed model can significantly improve the naturalness of converted singing voices and the similarity with the target singer. Moreover, our proposed model can also make the speakers with just speech data sing."
   ],
   "doi": "10.21437/Interspeech.2022-10316"
  },
  "wei22e_interspeech": {
   "authors": [
    [
     "Kun",
     "Wei"
    ],
    [
     "Yike",
     "Zhang"
    ],
    [
     "Sining",
     "Sun"
    ],
    [
     "Lei",
     "Xie"
    ],
    [
     "Long",
     "Ma"
    ]
   ],
   "title": "Leveraging Acoustic Contextual Representation by Audio-textual Cross-modal Learning for Conversational ASR",
   "original": "10326",
   "page_count": 5,
   "order": 207,
   "p1": 1016,
   "pn": 1020,
   "abstract": [
    "Leveraging context information is an intuitive idea to improve performance on conversational automatic speech recognition (ASR). Previous works usually adopt recognized hypotheses of historical utterances as preceding context, which may bias the current recognized hypothesis due to the inevitable historical recognition errors. To avoid this problem, we propose an audio-textual cross-modal representation extractor to learn contextual representations directly from preceding speech. Specifically, it consists of two modal-related encoders, extracting high-level latent features from speech or text, and a cross-modal encoder, which aims to learn the correlation between speech and text. For each modal-related encoder, we randomly mask some tokens of its input or the whole input sequence, then we perform a token-missing or modal-missing prediction and a modal-level CTC loss on cross-modal encoder. Thus, the model captures not only the bi-directional context dependencies in a specific modality but also relationships between different modalities. Then, the extractor will be frozen to extract the textual representations of preceding speech during the training of the conversational ASR system through attention mechanism. The effectiveness of the proposed approach is validated on several Mandarin conversation corpora and the highest character error rate (CER) reduction up to 16% is achieved on the MagicData dataset."
   ],
   "doi": "10.21437/Interspeech.2022-10326"
  },
  "do22_interspeech": {
   "authors": [
    [
     "Cong-Thanh",
     "Do"
    ],
    [
     "Mohan",
     "Li"
    ],
    [
     "Rama",
     "Doddipatla"
    ]
   ],
   "title": "Multiple-hypothesis RNN-T Loss for Unsupervised Fine-tuning and Self-training of Neural Transducer",
   "original": "10330",
   "page_count": 5,
   "order": 901,
   "p1": 4446,
   "pn": 4450,
   "abstract": [
    "This paper proposes a new approach to perform unsupervised fine-tuning and self-training using unlabeled speech data for recurrent neural network (RNN)-Transducer (RNN-T) end-to-end (E2E) automatic speech recognition (ASR) systems. Conventional systems perform fine-tuning/self-training using ASR hypothesis as the targets when using unlabeled audio data and are susceptible to the ASR performance of the base model. Here in order to alleviate the influence of ASR errors while using unlabeled data, we propose a multiple-hypothesis RNN-T loss that incorporates multiple ASR 1-best hypotheses into the loss function. For the fine-tuning task, ASR experiments on Librispeech show that the multiple-hypothesis approach achieves a relative reduction of 14.2% word error rate (WER) when compared to the single-hypothesis approach, on the test_other set. For the self-training task, ASR models are trained using supervised data from Wall Street Journal (WSJ), Aurora-4 along with CHiME-4 real noisy data as unlabeled data. The multiple-hypothesis approach yields a relative reduction of 3.3% WER on the CHiME-4's single-channel real noisy evaluation set when compared with the single-hypothesis approach."
   ],
   "doi": "10.21437/Interspeech.2022-10330"
  },
  "ludusan22_interspeech": {
   "authors": [
    [
     "Bogdan",
     "Ludusan"
    ],
    [
     "Marin",
     "Schröer"
    ],
    [
     "Petra",
     "Wagner"
    ]
   ],
   "title": "Investigating phonetic convergence of laughter in conversation",
   "original": "10332",
   "page_count": 5,
   "order": 271,
   "p1": 1332,
   "pn": 1336,
   "abstract": [
    "Laughter is one of the most encountered paralinguistic phenomena in conversation. Similarly to other communicative elements, evidence for laughter convergence, in particular for its temporal distribution and its acoustic marking, has been found between interlocutors. We investigate here whether segmental-level convergence effects, previously observed for speech, may be also found in the case of laughter. Using a corpus of dyadic interactions, we evaluate phonetic convergence of the vocalic part of laughs, by means of distances between the formant values. This was carried out for two proposed measures of convergence: global – at the level of the entire conversation, and local – considering consecutive laughs. Our global measure results reveal that interlocutors converge towards the end of the interaction, compared to its beginning, although important individual variation exists. With respect to the local measure, our findings show a lack of phonetic convergence (or divergence) between conversational partners."
   ],
   "doi": "10.21437/Interspeech.2022-10332"
  },
  "triantafyllopoulos22_interspeech": {
   "authors": [
    [
     "Andreas",
     "Triantafyllopoulos"
    ],
    [
     "Markus",
     "Fendler"
    ],
    [
     "Anton",
     "Batliner"
    ],
    [
     "Maurice",
     "Gerczuk"
    ],
    [
     "Shahin",
     "Amiriparian"
    ],
    [
     "Thomas",
     "Berghaus"
    ],
    [
     "Björn W.",
     "Schuller"
    ]
   ],
   "title": "Distinguishing between pre- and post-treatment in the speech of patients with chronic obstructive pulmonary disease",
   "original": "10333",
   "page_count": 5,
   "order": 733,
   "p1": 3623,
   "pn": 3627,
   "abstract": [
    "Chronic obstructive pulmonary disease (COPD) causes lung inflammation and airflow blockage leading to a variety of respiratory symptoms; it is also a leading cause of death and affects millions of individuals around the world. Patients often require treatment and hospitalisation, while no cure is currently available. As COPD predominantly affects the respiratory system, speech and non-linguistic vocalisations present a major avenue for measuring the effect of treatment. In this work, we present results on a new COPD dataset of 20 patients, showing that, by employing personalisation through speaker-level feature normalisation, we can distinguish between pre- and post-treatment speech with an unweighted average recall (UAR) of up to 82% in (nested) leave-one-speaker-out cross-validation. We further identify the most important features and link them to pathological voice properties, thus enabling an auditory interpretation of treatment effects. Monitoring tools based on such approaches may help objectivise the clinical status of COPD patients and facilitate personalised treatment plans."
   ],
   "doi": "10.21437/Interspeech.2022-10333"
  },
  "bhat22_interspeech": {
   "authors": [
    [
     "Chitralekha",
     "Bhat"
    ],
    [
     "Ashish",
     "Panda"
    ],
    [
     "Helmer",
     "Strik"
    ]
   ],
   "title": "Improved ASR Performance for Dysarthric Speech Using Two-stage DataAugmentation",
   "original": "10335",
   "page_count": 5,
   "order": 10,
   "p1": 46,
   "pn": 50,
   "abstract": [
    "Machine learning (ML) and Deep Neural Networks (DNN) have greatly aided the problem of Automatic Speech Recognition (ASR). However, accurate ASR for dysarthric speech remains a serious challenge. Dearth of usable data remains a problem in applying ML and DNN techniques for dysarthric speech recognition. In the current research, we address this challenge using a novel two-stage data augmentation scheme, a combination of static and dynamic data augmentation techniques that are designed by leveraging an understanding of the characteristics of dysarthric speech. Deep Autoencoder (DAE)-based healthy speech modification and various perturbations comprise static augmentations, whereas SpecAugment techniques modified to specifically augment dysarthric speech comprise the dynamic data augmentation. The objective of this work is to improve the ASR performance for dysarthric speech using the two-stage data augmentation scheme. An end-to-end ASR using a Transformer acoustic model is used to evaluate the data augmentation scheme on speech from the UA dysarthric speech corpus. We achieve an absolute improvement of 16% in word error rate (WER) over a baseline with no augmentation, with a final WER of 20.6%."
   ],
   "doi": "10.21437/Interspeech.2022-10335"
  },
  "bandarupalli22_interspeech": {
   "authors": [
    [
     "Tarun Sai",
     "Bandarupalli"
    ],
    [
     "Shakti",
     "Rath"
    ],
    [
     "Nirmesh",
     "Shah"
    ],
    [
     "Onoe",
     "Naoyuki"
    ],
    [
     "Sriram",
     "Ganapathy"
    ]
   ],
   "title": "Semi-supervised Acoustic and Language Modeling for Hindi ASR",
   "original": "10336",
   "page_count": 5,
   "order": 714,
   "p1": 3528,
   "pn": 3532,
   "abstract": [
    "This paper describes the submission made by our team to the Hindi Gram Vaani ASR challenge. This challenge involves building an ASR system for spontaneous telephonic recordings. The challenge is unique because of the small amount of labelled data available for model development. On top of that, the acoustic variabilities such as spontaneity of natural conversations, rich diversity of Hindi across India and varied backgrounds present in the corpus make it much more challenging. We participated in two of the three tracks where the first track involves 100 hours of labelled speech only and the second track involves 1000 hours of additional unlabelled corpus along with 100 hours of labelled speech. A Kaldi based hybrid model has been developed for the first and second track involving TDNN-F character based acoustic model, N-gram first pass decoding, RNN-LM re-scoring and system combinations. On the other hand, for the second track, an E2E conformer based system has been trained on representations obtained from a contrastive predictive coding (CPC) model. The results obtained for both the tracks are significantly better than the baseline results published by the challenge organizers on the development set consisting of 5 hours of audio."
   ],
   "doi": "10.21437/Interspeech.2022-10336"
  },
  "comini22_interspeech": {
   "authors": [
    [
     "Giulia",
     "Comini"
    ],
    [
     "Goeric",
     "Huybrechts"
    ],
    [
     "Manuel Sam",
     "Ribeiro"
    ],
    [
     "Adam",
     "Gabryś"
    ],
    [
     "Jaime",
     "Lorenzo-Trueba"
    ]
   ],
   "title": "Low-data? No problem: low-resource, language-agnostic conversational text-to-speech via F0-conditioned data augmentation",
   "original": "10338",
   "page_count": 5,
   "order": 394,
   "p1": 1946,
   "pn": 1950,
   "abstract": [
    "The availability of data in expressive styles across languages is limited, and recording sessions are costly and time consuming. To overcome these issues, we demonstrate how to build low-resource, neural text-to-speech (TTS) voices with only 1 hour of conversational speech, when no other conversational data are available in the same language. Assuming the availability of non-expressive speech data in that language, we propose a 3-step technology: 1) we train an F0-conditioned voice conversion (VC) model as data augmentation technique; 2) we train an F0 predictor to control the conversational flavour of the voice-converted synthetic data; 3) we train a TTS system that consumes the augmented data. We prove that our technology enables F0 controllability, is scalable across speakers and languages and is competitive in terms of naturalness over a state-of-the-art baseline model, another augmented method which does not make use of F0 information."
   ],
   "doi": "10.21437/Interspeech.2022-10338"
  },
  "kim22l_interspeech": {
   "authors": [
    [
     "Dong-Hyun",
     "Kim"
    ],
    [
     "Jae-Hong",
     "Lee"
    ],
    [
     "Ji-Hwan",
     "Mo"
    ],
    [
     "Joon-Hyuk",
     "Chang"
    ]
   ],
   "title": "W2V2-Light: A Lightweight Version of Wav2vec 2.0 for Automatic Speech Recognition",
   "original": "10339",
   "page_count": 5,
   "order": 616,
   "p1": 3038,
   "pn": 3042,
   "abstract": [
    "Wav2vec 2.0 (W2V2) has shown remarkable speech recognition performance by pre-training only with unlabeled data and fine-tuning with a small amount of labeled data. However, the practical application of W2V2 is hindered by hardware memory limitations, as it contains 317 million parameters. To ad- dress this issue, we propose W2V2-Light, a lightweight version of W2V2. We introduce two simple sharing methods to reduce the memory consumption as well as the computational costs of W2V2. Compared to W2V2, our model has 91% lesser parameters and a speedup of 1.31 times with minor degradation in downstream task performance. Moreover, by quantifying the stability of representations, we provide an empirical insight into why our model is capable of maintaining competitive performance despite the significant reduction in memory"
   ],
   "doi": "10.21437/Interspeech.2022-10339"
  },
  "kuang22_interspeech": {
   "authors": [
    [
     "Fangjun",
     "Kuang"
    ],
    [
     "Liyong",
     "Guo"
    ],
    [
     "Wei",
     "Kang"
    ],
    [
     "Long",
     "Lin"
    ],
    [
     "Mingshuang",
     "Luo"
    ],
    [
     "Zengwei",
     "Yao"
    ],
    [
     "Daniel",
     "Povey"
    ]
   ],
   "title": "Pruned RNN-T for fast, memory-eﬀicient ASR training",
   "original": "10340",
   "page_count": 5,
   "order": 422,
   "p1": 2068,
   "pn": 2072,
   "abstract": [
    "The RNN-Transducer (RNN-T) framework for speech recognition has been growing in popularity, particularly for deployed real-time ASR systems, because it combines high accuracy with naturally streaming recognition. One of the drawbacks of RNN-T is that its loss function is relatively slow to compute, and can use a lot of memory. Excessive GPU memory usage can make it impractical to use RNN-T loss in cases where the vocabulary size is large: for example, for Chinese character-based ASR. We introduce a method for faster and more memory- eﬀicient RNN-T loss computation. We first obtain pruning bounds for the RNN-T recursion using a simple joiner network that is linear in the encoder and decoder embeddings; we can evaluate this without using much memory. We then use those pruning bounds to evaluate the full, non-linear joiner network. The code is open-sourced and publicly available."
   ],
   "doi": "10.21437/Interspeech.2022-10340"
  },
  "li22s_interspeech": {
   "authors": [
    [
     "Zhuo",
     "Li"
    ],
    [
     "Runqiu",
     "Xiao"
    ],
    [
     "Hangting",
     "Chen"
    ],
    [
     "Zhenduo",
     "Zhao"
    ],
    [
     "Zihan",
     "Zhang"
    ],
    [
     "Wenchao",
     "Wang"
    ]
   ],
   "title": "The HCCL System for the NIST SRE21",
   "original": "10342",
   "page_count": 5,
   "order": 752,
   "p1": 3709,
   "pn": 3713,
   "abstract": [
    "This paper describes the systems developed by the HCCL team for the NIST 2021 speaker recognition evaluation (NIST SRE21). We first explore various state-of-the-art speaker embedding extractors combined with a novel circle loss to obtain discriminative deep speaker embeddings. Considering that cross-channel and cross-linguistic speaker recognition are the key challenges of SRE21, we introduce several techniques to reduce the cross-domain mismatch. Specifically, Codec and speech enhancement are directly applied to the raw speech to eliminate the codecs and the environment noise mismatch. We denote these methods that work directly on raw audio to eliminate the relatively explicit mismatch collectively as data adaptation methods. Experiments show that data adaption methods achieve 15\\% improvements over our baseline. Furthermore, some popular back-ends domain adaptation algorithms are deployed on speaker embeddings to alleviate speaker performance degradation caused by the implicit mismatch. Score calibration is a major failure for us in SRE21. The reason is that score calibration with excessive parameters easily leads to overfitting."
   ],
   "doi": "10.21437/Interspeech.2022-10342"
  },
  "suzuki22b_interspeech": {
   "authors": [
    [
     "Yu",
     "Suzuki"
    ],
    [
     "Tsuneo",
     "Kato"
    ],
    [
     "Akihiro",
     "Tamura"
    ]
   ],
   "title": "Automatic Prosody Evaluation of L2 English Read Speech in Reference to Accent Dictionary with Transformer Encoder",
   "original": "10344",
   "page_count": 5,
   "order": 905,
   "p1": 4466,
   "pn": 4470,
   "abstract": [
    "Automatic prosody evaluation models for second language (L2) read speech are classified into two categories: reference-based and reference-free. Reference-based models refer to native speakers' speech of the uttered text while reference-free models do not. Conventional reference-free models do not even take the uttered text into account. We propose an automatic prosody evaluation model that takes the uttered text into account by estimating native speakers' prosodic patterns using a Transformer encoder. The Transformer encoder used in Fast-Speech 2 estimates a sequence of native speakers' prosodic features in a phoneme-segment level, and a subsequent neural network module evaluates an L2 learner's utterance by comparing the sequence of prosodic features with the estimated sequence of native speakers' utterances. We evaluated the model by Spearman's correlation between the objective and subjective scores on L2 English sentence speech read by Japanese university students. The experimental results indicated that our model achieved a higher subjective-objective score correlation than that with a reference-free model and even higher than an inter-rater score correlation."
   ],
   "doi": "10.21437/Interspeech.2022-10344"
  },
  "tan22_interspeech": {
   "authors": [
    [
     "Daxin",
     "Tan"
    ],
    [
     "Guangyan",
     "Zhang"
    ],
    [
     "Tan",
     "Lee"
    ]
   ],
   "title": "Environment Aware Text-to-Speech Synthesis",
   "original": "10348",
   "page_count": 5,
   "order": 97,
   "p1": 481,
   "pn": 485,
   "abstract": [
    "This study aims at designing an environment-aware text-to-speech (TTS) system that can generate speech to suit specific acoustic environments. It is also motivated by the desire to leverage massive data of speech audio from heterogeneous sources in TTS system development. The key idea is to model the acoustic environment in speech audio as a factor of data variability and incorporate it as a condition in the process of neural network based speech synthesis. Two embedding extractors are trained with two purposely constructed datasets for characterization and disentanglement of speaker and environment factors in speech. A neural network model is trained to generate speech from extracted speaker and environment embeddings. Objective and subjective evaluation results demonstrate that the proposed TTS system is able to effectively disentangle speaker and environment factors and synthesize speech audio that carries designated speaker characteristics and environment attribute. Audio samples are available online for demonstration."
   ],
   "doi": "10.21437/Interspeech.2022-10348"
  },
  "gao22c_interspeech": {
   "authors": [
    [
     "Zhenke",
     "Gao"
    ],
    [
     "Manwai",
     "Mak"
    ],
    [
     "Weiwei",
     "Lin"
    ]
   ],
   "title": "UNet-DenseNet for Robust Far-Field Speaker Verification",
   "original": "10350",
   "page_count": 5,
   "order": 753,
   "p1": 3714,
   "pn": 3718,
   "abstract": [
    "Far-field speaker verification (SV) has always been critical but challenging. Data augmentation is commonly used to overcome the problems arising from far-field microphones, such as high background noise levels and reverberation effects. On top of data augmentation, this paper tackles these problems by introducing a UNet-based speech enhancement (SE) module as a front-end processor for the speaker embedding module. To prevent the SE module from distorting speaker information, we propose two improvements to the speech enhancement–speaker embedding pipeline. (1) A UNet-DenseNet joint training scheme in which the UNet is optimized by both the MSE and speaker classification losses. (2) A semi-joint training scheme that stops the UNet training but continues the DenseNet training when overfitting of the UNet is detected. Extensive experiments on noise-contaminated Voxceleb1 and the VOiCES Challenge 2019 demonstrate the effectiveness of the two training schemes."
   ],
   "doi": "10.21437/Interspeech.2022-10350"
  },
  "yang22q_interspeech": {
   "authors": [
    [
     "Wei",
     "Yang"
    ],
    [
     "Satoru",
     "Fukayama"
    ],
    [
     "Panikos",
     "Heracleous"
    ],
    [
     "Jun",
     "Ogata"
    ]
   ],
   "title": "Exploiting Fine-tuning of Self-supervised Learning Models for Improving Bi-modal Sentiment Analysis and Emotion Recognition",
   "original": "10354",
   "page_count": 5,
   "order": 408,
   "p1": 1998,
   "pn": 2002,
   "abstract": [
    "Speech-based multimodal affective computing has recently attracted significant research attention. Previous experimental results have shown that the audio-only approach exhibits inferior performance than the text-only approach in sentiment analysis and emotion recognition tasks. In this paper, we propose a new strategy to improve the performance of uni-modal and bi-modal affective computing systems via fine-tuning of two pre-trained self-supervised learning models (Text-RoBERTa and Speech-RoBERTa). We fine-tune the models on sentiment analysis and emotion recognition tasks using a shallow architecture, and apply crossmodal attention fusion to the models for further learning and final prediction or classification. We evaluate our proposed method on the CMU-MOSI, CMU-MOSEI and IEMOCAP datasets. The experimental results demonstrate that our approach exhibits superior performance for all benchmarks compared to existing state-of-the-art results, establishing the effectiveness of the proposed method."
   ],
   "doi": "10.21437/Interspeech.2022-10354"
  },
  "kothapally22_interspeech": {
   "authors": [
    [
     "Vinay",
     "Kothapally"
    ],
    [
     "YONG",
     "XU"
    ],
    [
     "Meng",
     "Yu"
    ],
    [
     "Shi-Xiong",
     "ZHANG"
    ],
    [
     "Dong",
     "Yu"
    ]
   ],
   "title": "Joint Neural AEC and Beamforming with Double-Talk Detection",
   "original": "10358",
   "page_count": 5,
   "order": 514,
   "p1": 2528,
   "pn": 2532,
   "abstract": [
    "Acoustic echo cancellation (AEC) in full-duplex communication systems eliminates acoustic feedback. However, nonlinear distortions induced by audio devices, background noise, reverberation, and double-talk reduce the efficiency of conventional AEC systems. Several hybrid AEC models were proposed to address this, which use deep learning models to suppress residual echo from standard adaptive filtering. This paper proposes deep learning-based joint AEC and beamforming model (JAECBF) building on our previous self-attentive recurrent neural network (RNN) beamformer. The proposed network consists of two modules: (i) multi-channel neural-AEC, and (ii) joint AEC-RNN beamformer with a double-talk detection (DTD) that computes time-frequency (T-F) beamforming weights. We train the proposed model in an end-to-end approach to eliminate background noise and echoes from far-end audio devices, which include nonlinear distortions. From experimental evaluations, we find the proposed network outperforms other multi-channel AEC and denoising systems in terms of speech recognition rate and overall speech quality."
   ],
   "doi": "10.21437/Interspeech.2022-10358"
  },
  "zhang22y_interspeech": {
   "authors": [
    [
     "Wangyou",
     "Zhang"
    ],
    [
     "Zhuo",
     "Chen"
    ],
    [
     "Naoyuki",
     "Kanda"
    ],
    [
     "Shujie",
     "Liu"
    ],
    [
     "Jinyu",
     "Li"
    ],
    [
     "Sefik",
     "Emre Eskimez"
    ],
    [
     "Takuya",
     "Yoshioka"
    ],
    [
     "Xiong",
     "Xiao"
    ],
    [
     "Zhong",
     "Meng"
    ],
    [
     "Yanmin",
     "Qian"
    ],
    [
     "Furu",
     "Wei"
    ]
   ],
   "title": "Separating Long-Form Speech with Group-wise Permutation Invariant Training",
   "original": "10362",
   "page_count": 5,
   "order": 1091,
   "p1": 5383,
   "pn": 5387,
   "abstract": [
    "Multi-talker conversational speech processing has drawn many interests for various applications such as meeting transcription. Speech separation is often required to handle overlapped speech that is commonly observed in conversation. Although the original utterance-level permutation invariant training-based continuous speech separation approach has proven to be effective in various conditions, it lacks the ability to leverage the long-span relationship of utterances and is computationally inefficient due to the highly overlapped sliding windows. To overcome these drawbacks, we propose a novel training scheme named Group-PIT, which allows direct training of the speech separation models on the long-form speech with a low computational cost for label assignment. Two different speech separation approaches with Group-PIT are explored, including direct long-span speech separation and short-span speech separation with long-span tracking. Experiments on simulated meeting-style data demonstrate the effectiveness of our proposed approaches, especially in dealing with a very long speech input."
   ],
   "doi": "10.21437/Interspeech.2022-10362"
  },
  "yue22b_interspeech": {
   "authors": [
    [
     "Yanyan",
     "Yue"
    ],
    [
     "Jun",
     "Du"
    ],
    [
     "Mao-Kui",
     "He"
    ],
    [
     "YuTing",
     "Yeung"
    ],
    [
     "Renyu",
     "Wang"
    ]
   ],
   "title": "Online Speaker Diarization with Core Samples Selection",
   "original": "10363",
   "page_count": 5,
   "order": 298,
   "p1": 1466,
   "pn": 1470,
   "abstract": [
    "We propose a novel online speaker diarization approach based on the VBx algorithm which works well on the offline speaker diarization tasks. To efficiently process long-time recordings, we perform the online diarization in a block-wise manner. First, we devise a core samples updating strategy utilizing time penalty function, which can preserve important historical information with a low memory cost. Then we select clustering samples from core samples by stratified sampling to enhance the variability among samples and retain sufficient speaker identity information, which helps VBx to improve classification accuracy on a small amount of data. Finally, we solve the label ambiguity problem by a global constrained clustering algorithm. We evaluate our system on DIHARD and AMI datasets. The experimental results demonstrate that our online approach achieves superior performance compared with the state-of-the-art."
   ],
   "doi": "10.21437/Interspeech.2022-10363"
  },
  "ao22_interspeech": {
   "authors": [
    [
     "Junyi",
     "Ao"
    ],
    [
     "Ziqiang",
     "Zhang"
    ],
    [
     "Long",
     "Zhou"
    ],
    [
     "Shujie",
     "Liu"
    ],
    [
     "Haizhou",
     "Li"
    ],
    [
     "Tom",
     "Ko"
    ],
    [
     "Lirong",
     "Dai"
    ],
    [
     "Jinyu",
     "Li"
    ],
    [
     "Yao",
     "Qian"
    ],
    [
     "Furu",
     "Wei"
    ]
   ],
   "title": "Pre-Training Transformer Decoder for End-to-End ASR Model with Unpaired Speech Data",
   "original": "10368",
   "page_count": 5,
   "order": 540,
   "p1": 2658,
   "pn": 2662,
   "abstract": [
    "This paper studies a novel pre-training technique with unpaired speech data, Speech2C, for encoder-decoder based automatic speech recognition (ASR). Within a multi-task learning framework, we introduce two pre-training tasks for the encoder-decoder network using acoustic units, i.e., pseudo codes, derived from an offline clustering model. One is to predict the pseudo codes via masked language modeling in encoder output, like HuBERT model, while the other lets the decoder learn to reconstruct pseudo codes autoregressively instead of generating textual scripts. In this way, the decoder learns to reconstruct original speech information with codes before learning to generate correct text. Comprehensive experiments on the LibriSpeech corpus show that the proposed Speech2C can relatively reduce the word error rate (WER) by 19.2\\% over the method without decoder pre-training, and also outperforms significantly the state-of-the-art wav2vec 2.0 and HuBERT on fine-tuning subsets of 10h and 100h. We release our code and model at https://github.com/microsoft/SpeechT5/tree/main/Speech2C."
   ],
   "doi": "10.21437/Interspeech.2022-10368"
  },
  "defino22_interspeech": {
   "authors": [
    [
     "Verdiana",
     "De Fino"
    ],
    [
     "Lionel",
     "Fontan"
    ],
    [
     "Julien",
     "Pinquier"
    ],
    [
     "Isabelle",
     "Ferrané"
    ],
    [
     "Sylvain",
     "Detey"
    ]
   ],
   "title": "Prediction of L2 speech proficiency based on multi-level linguistic features",
   "original": "10369",
   "page_count": 5,
   "order": 820,
   "p1": 4043,
   "pn": 4047,
   "abstract": [
    "This study investigates the possibility to use automatic, multi-level features for the prediction of L2 speech proficiency. The method was applied on a corpus containing audio recordings and transcripts for 38 Japanese learners of French who participated in a semi-spontaneous oral production task. Each learner's speech proficiency level was assessed by three experienced French teachers. Audio recordings were processed to extract features related to the pronunciation skills and phonetic fluency of the learners, while the transcripts were used to measure their lexical, syntactic, and discursive abilities in French. A Lasso regression using a leave-one-out cross-validation procedure was used to select relevant features and to accurately predict speech proficiency scores. The results show that five features related to the phonetic fluency (speech rate), lexical abilities (lexical density), discourse planning and elaboration skills (number of hesitation and false starts, mean utterance length) of the learners can be used to predict speech proficiency ratings (r=0.71, mean absolute error on a 5-point scale: 0.53)."
   ],
   "doi": "10.21437/Interspeech.2022-10369"
  },
  "triantafyllopoulos22b_interspeech": {
   "authors": [
    [
     "Andreas",
     "Triantafyllopoulos"
    ],
    [
     "Johannes",
     "Wagner"
    ],
    [
     "Hagen",
     "Wierstorf"
    ],
    [
     "Maximilian",
     "Schmitt"
    ],
    [
     "Uwe",
     "Reichel"
    ],
    [
     "Florian",
     "Eyben"
    ],
    [
     "Felix",
     "Burkhardt"
    ],
    [
     "Björn W.",
     "Schuller"
    ]
   ],
   "title": "Probing speech emotion recognition transformers for linguistic knowledge",
   "original": "10371",
   "page_count": 5,
   "order": 30,
   "p1": 146,
   "pn": 150,
   "abstract": [
    "Large, pre-trained neural networks consisting of self-attention layers (transformers) have recently achieved state-of-the-art results on several speech emotion recognition (SER) datasets. These models are typically pre-trained in self-supervised manner with the goal to improve automatic speech recognition performance -- and thus, to understand linguistic information. In this work, we investigate the extent in which this information is exploited during SER fine-tuning. Using a reproducible methodology based on open-source tools, we synthesise prosodically neutral speech utterances while varying the sentiment of the text. Valence predictions of the transformer model are very reactive to positive and negative sentiment content, as well as negations, but not to intensifiers or reducers, while none of those linguistic features impact arousal or dominance. These findings show that transformers can successfully leverage linguistic information to improve their valence predictions, and that linguistic analysis should be included in their testing."
   ],
   "doi": "10.21437/Interspeech.2022-10371"
  },
  "dutta22b_interspeech": {
   "authors": [
    [
     "Debottam",
     "Dutta"
    ],
    [
     "Debarpan",
     "Bhattacharya"
    ],
    [
     "Sriram",
     "Ganapathy"
    ],
    [
     "Amir Hossein",
     "Poorjam"
    ],
    [
     "Deepak",
     "Mittal"
    ],
    [
     "Maneesh",
     "Singh"
    ]
   ],
   "title": "Acoustic Representation Learning on Breathing and Speech Signals for COVID-19 Detection",
   "original": "10376",
   "page_count": 5,
   "order": 581,
   "p1": 2863,
   "pn": 2867,
   "abstract": [
    "In this paper, we describe an approach for representation learning of audio signals for the task of COVID-19 detection. The raw audio samples are processed with a bank of 1-D convolutional filters that are parameterized as cosine modulated Gaussian functions. The choice of these kernels allows the interpretation of the filterbanks as smooth band-pass filters. The filtered outputs are pooled, log-compressed and used in a self-attention based relevance weighting mechanism. The relevance weighting emphasizes the key regions of the time-frequency decomposition that are important for the downstream task. The subsequent layers of the model consist of a recurrent architecture and the models are trained for a COVID-19 detection task. In our experiments on the Coswara data set, we show that the proposed model achieves significant performance improvements over the baseline system as well as other representation learning approaches. Further, the approach proposed is shown to be uniformly applicable for speech and breathing signals and for transfer learning from a larger data set."
   ],
   "doi": "10.21437/Interspeech.2022-10376"
  },
  "bassan22_interspeech": {
   "authors": [
    [
     "Shahaf",
     "Bassan"
    ],
    [
     "Yossi",
     "Adi"
    ],
    [
     "Jeffrey",
     "Rosenschein"
    ]
   ],
   "title": "Unsupervised Symbolic Music Segmentation using Ensemble Temporal Prediction Errors",
   "original": "10379",
   "page_count": 5,
   "order": 493,
   "p1": 2423,
   "pn": 2427,
   "abstract": [
    "Symbolic music segmentation is the process of dividing symbolic melodies into smaller meaningful groups, such as melodic phrases. We proposed an unsupervised method for segmenting symbolic music. The proposed model is based on an ensemble of temporal prediction error models. During training, each model predicts the next token to identify musical phrase changes. While at test time, we perform a peak detection algorithm to select segment candidates. Finally, we aggregate the predictions of each of the models participating in the ensemble to predict the final segmentation. Results suggest the proposed method reaches state-of-the-art performance on the Essen Folksong dataset under the unsupervised setting when considering F-Score and R-value. We additionally provide an ablation study to better assess the contribution of each of the model components to the final results. As expected, the proposed method is inferior to the supervised setting, which leaves room for improvement in future research considering closing the gap between unsupervised and supervised methods."
   ],
   "doi": "10.21437/Interspeech.2022-10379"
  },
  "wu22h_interspeech": {
   "authors": [
    [
     "Xianchao",
     "Wu"
    ]
   ],
   "title": "Deep Sparse Conformer for Speech Recognition",
   "original": "10384",
   "page_count": 5,
   "order": 423,
   "p1": 2073,
   "pn": 2077,
   "abstract": [
    "Conformer has achieved impressive results in Automatic Speech Recognition (ASR) by leveraging transformer's capturing of content-based global interactions and convolutional neural network's exploiting of local features. In Conformer, two macaron-like feed-forward layers with half-step residual connections sandwiches the multi-head self-attention and convolution modules followed by a post layer normalization. We improve Conformer's long-sequence representation ability in two directions, sparser and deeper. We adapt a sparse self-attention mechanism with O(LlogL) in time complexity and memory usage. A deep normalization strategy is utilized when performing residual connections to ensure our training of hundred-level Conformer blocks. On the Japanese CSJ-500h dataset, this deep sparse Conformer achieves respectively CERs of 5.52%, 4.03% and 4.50% on the three evaluation sets and 4.16%, 2.84% and 3.20% when ensembling five deep sparse Conformer variants from 12 to 16, 17, 50, and finally 100 encoder layers."
   ],
   "doi": "10.21437/Interspeech.2022-10384"
  },
  "mumtaz22_interspeech": {
   "authors": [
    [
     "Deebha",
     "Mumtaz"
    ],
    [
     "Ajit",
     "Jena"
    ],
    [
     "Vinit",
     "Jakhetiya"
    ],
    [
     "Karan",
     "Nathwani"
    ],
    [
     "Sharath Chandra",
     "Guntuku"
    ]
   ],
   "title": "Transformer-based quality assessment model for generalized user-generated multimedia audio content",
   "original": "10386",
   "page_count": 5,
   "order": 136,
   "p1": 674,
   "pn": 678,
   "abstract": [
    "In this paper, we propose a computational measure for the quality of audio in user-generated multimedia (UGM) in accordance with the human perceptual system. To this end, we first extend the previously proposed IIT-JMU-UGM Audio dataset by including samples with more diverse context, content, distortion types, and intensities, along with implicitly distorted audio that reflect realistic scenarios. We conduct subjective testing on the extended database containing 2075 audio clips to obtain the mean opinion scores for each sample. We then introduce transformer-based learning to the domain of audio quality assessment, which is trained on three vital audio features: Mel-frequency cepstral coefficients, chroma, and Mel-scaled spectrogram. The proposed non-intrusive transformer-based model is compared against state-of-the-art methods and found to outperform Simple RNN, LSTM, and GRU models by over 4%. The database and the source code will be made public upon acceptance."
   ],
   "doi": "10.21437/Interspeech.2022-10386"
  },
  "lee22k_interspeech": {
   "authors": [
    [
     "Seonwoo",
     "Lee"
    ],
    [
     "Sunhee",
     "Kim"
    ],
    [
     "Minhwa",
     "Chung"
    ]
   ],
   "title": "A Study on the Phonetic Inventory Development of Children with Cochlear Implants for 5 Years after Implantation",
   "original": "10387",
   "page_count": 5,
   "order": 734,
   "p1": 3628,
   "pn": 3632,
   "abstract": [
    "This paper investigates longitudinal phonetic inventories of vowels and consonants of Korean-speaking children with cochlear implants (CIs). They are based on speech data of 7 children with CI over 5 years PI to examine the entire speech production development. Phones produced at least twice by more than 50% children in spontaneous and imitation speech from 6 months to 5 years post-implantation (PI) are compiled in the inventories. The results show and differences and similarities between children with CI and with normal hearing (NH). The vowel and consonant inventories at 6 months PI are larger than those of NH children at 1 year of age whose hearing experience is longer, including liquid [ɾ] and fricative [s]. It can be attributed to biological maturation of CI children. As in children with NH, there is an explosive increase in phonetic inventories during a year after 1-year of robust hearing experience and the inventories are almost complete after 3 years of PI. Phonetic inventories at each time are expected to be references to assess the developmental appropriateness in speech production and guides to direct habilitation goals."
   ],
   "doi": "10.21437/Interspeech.2022-10387"
  },
  "kulkarni22_interspeech": {
   "authors": [
    [
     "Ajinkya",
     "Kulkarni"
    ],
    [
     "Vincent",
     "Colotte"
    ],
    [
     "Denis",
     "Jouvet"
    ]
   ],
   "title": "Analysis of expressivity transfer in non-autoregressive end-to-end multispeaker TTS systems",
   "original": "10388",
   "page_count": 5,
   "order": 928,
   "p1": 4581,
   "pn": 4585,
   "abstract": [
    "The main objective of this work is to study the expres- sivity transfer in a speaker's voice for which no expressive speech data is available in non-autoregressive end-to-end TTS systems. We investigated the expressivity transfer capability of probability density estimation based on deep generative mod- els, namely Generative Flow (Glow) and diffusion probabilis- tic models (DPM). The usage of deep generative models pro- vides better log likelihood estimates and tractability of the sys- tem, subsequently providing high-quality speech synthesis with faster inference speed. Furthermore, we propose the usage of various expressivity encoders, which assist in expressivity transfer in the text-to-speech (TTS) system. More precisely, we used self-attention statistical pooling and multi-scale ex- pressivity encoder architectures for creating a meaningful rep- resentation of expressivity. In addition to traditional subjective metrics used for speech synthesis evaluation, we incorporated cosine-similarity to measure the strength of attributes associ- ated with speaker and expressivity. The performance of a non- autoregressive TTS system with a multi-scale expressivity en- coder showed better expressivity transfer on Glow and DPM- based decoders. Thus, illustrating the ability of multi-scale ar- chitecture to apprehend the underlying attributes of expressivity from multiple acoustic features."
   ],
   "doi": "10.21437/Interspeech.2022-10388"
  },
  "bhattacharya22_interspeech": {
   "authors": [
    [
     "Debarpan",
     "Bhattacharya"
    ],
    [
     "Debottam",
     "Dutta"
    ],
    [
     "Neeraj",
     "Sharma"
    ],
    [
     "Srikanth Raj",
     "Chetupalli"
    ],
    [
     "Pravin",
     "Mote"
    ],
    [
     "Sriram",
     "Ganapathy"
    ],
    [
     "Chandrakiran",
     "C"
    ],
    [
     "Sahiti",
     "Nori"
    ],
    [
     "Suhail",
     "K K"
    ],
    [
     "Sadhana",
     "Gonuguntla"
    ],
    [
     "Murali",
     "Alagesan"
    ]
   ],
   "title": "Analyzing the impact of SARS-CoV-2 variants on respiratory sound signals",
   "original": "10389",
   "page_count": 5,
   "order": 503,
   "p1": 2473,
   "pn": 2477,
   "abstract": [
    "The COVID-19 outbreak resulted in multiple waves of infections that have been associated with different SARS-CoV-2 variants. Studies have reported differential impact of the variants on respiratory health of patients. We explore whether acoustic signals, collected from COVID-19 subjects, show computationally distinguishable acoustic patterns suggesting a possibility to predict the underlying virus variant. We analyze the Coswara dataset which is collected from three subject pools, namely, i) healthy, ii) COVID-19 subjects recorded during the delta variant dominant period, and iii) data from COVID-19 subjects recorded during the omicron surge. Our findings suggest that multiple sound categories, such as cough, breathing, and speech, indicate significant acoustic feature differences when comparing COVID-19 subjects with omicron and delta variants. The classification areas-under-the-curve are significantly above chance for differentiating subjects infected by omicron from those infected by delta. Using a score fusion from multiple sound categories, we obtained an area-under-the-curve of 89% and 52.4% sensitivity at 95% specificity. Additionally, a hierarchical three class approach was used to classify the acoustic data into healthy and COVID-19 positive, and further COVID-19 subjects into delta and omicron variants providing high level of 3-class classification accuracy. These results suggest new ways for designing sound based COVID-19 diagnosis approaches."
   ],
   "doi": "10.21437/Interspeech.2022-10389"
  },
  "fara22_interspeech": {
   "authors": [
    [
     "Salvatore",
     "Fara"
    ],
    [
     "Stefano",
     "Goria"
    ],
    [
     "Emilia",
     "Molimpakis"
    ],
    [
     "Nicholas",
     "Cummins"
    ]
   ],
   "title": "Speech and the n-Back task as a lens into depression. How combining both may allow us to isolate different core symptoms of depression",
   "original": "10393",
   "page_count": 5,
   "order": 387,
   "p1": 1911,
   "pn": 1915,
   "abstract": [
    "Embedded in any speech signal is a rich combination of cognitive, neuromuscular and physiological information. This richness makes speech a powerful signal in relation to a range of different health conditions, including major depressive disorders (MDD). One pivotal issue in speech-depression research is the assumption that depressive severity is the dominant measurable effect. However, given the heterogeneous clinical profile of MDD, it may actually be the case that speech alterations are more strongly associated with subsets of key depression symptoms. This paper presents strong evidence in support of this argument. First, we present a novel large, cross-sectional, multi-modal dataset collected at Thymia. We then present a set of machine learning experiments that demonstrate that combining speech with features from an n-Back working memory assessment improves classifier performance when predicting the popular eight-item Patient Health Questionnaire depression scale (PHQ-8). Finally, we present a set of experiments that highlight the association between different speech and n-Back markers at the PHQ-8 item level. Specifically, we observe that somatic and psychomotor symptoms are more strongly associated with n-Back performance scores, whilst the other items: anhedonia, depressed mood, change in appetite, feelings of worthlessness and trouble concentrating are more strongly associated with speech changes."
   ],
   "doi": "10.21437/Interspeech.2022-10393"
  },
  "liu22r_interspeech": {
   "authors": [
    [
     "Junpeng",
     "Liu"
    ],
    [
     "Yanyan",
     "Zou"
    ],
    [
     "Yuxuan",
     "Xi"
    ],
    [
     "Shengjie",
     "Li"
    ],
    [
     "Mian",
     "Ma"
    ],
    [
     "Zhuoye",
     "Ding"
    ],
    [
     "Bo",
     "Long"
    ]
   ],
   "title": "Negative Guided Abstractive Dialogue Summarization",
   "original": "10395",
   "page_count": 5,
   "order": 659,
   "p1": 3253,
   "pn": 3257,
   "abstract": [
    "The goal of the abstractive dialogue summarization task is to generate a shorter form of a long conversation while retaining its most salient information, which plays an important role in speech. Unlike the well-structured text, such as scientific articles and news, dialogues often comprise of utterances coming from multiple interlocutors, where the conversations are often informal, verbose, repetitive, and sprinkled with false-starts, backchanneling, reconfirmations, hesitations as well as speaker interruptions, which might introduce much noisy information and thus brings new challenges of summarizing dialogues. In this work, we extend the widely-used sequence-to-sequence summarization framework with a negative guided mechanism, which allows models to explicitly perceive the unnecessary pieces (i.e., noise) of a dialogue and thus focus more on the salient information. Specifically, the negative guided mechanism has two main components, negative example construction and negative guided loss. We explore two different ways to constructing the negative examples and further calculate the negative loss. Extensive experiments on the benchmark datasets demonstrate that our method significantly outperforms the baselines with regard to both semantic matching and factual consistent based metrics. We also elicit the human efforts to prove the performance gains."
   ],
   "doi": "10.21437/Interspeech.2022-10395"
  },
  "machado22_interspeech": {
   "authors": [
    [
     "Carolina Lins",
     "Machado"
    ],
    [
     "Volker",
     "Dellwo"
    ],
    [
     "Lei",
     "He"
    ]
   ],
   "title": "Idiosyncratic lingual articulation of American English /æ/ and /ɑ/ using network analysis",
   "original": "10397",
   "page_count": 5,
   "order": 152,
   "p1": 754,
   "pn": 758,
   "abstract": [
    "Formant dynamics are believed to reflect the characteristic articulatory behavior of a speaker. The present study aims to explore individual articulatory behaviors when producing American English /æ/ and /ɑ/. The two vowels differ in the degree of inherent spectral change, a property believed to carry information about vowel-phoneme identity, which may be reflected in the articulatory movements. We measured first and second formants together with tongue blade and dorsum trajectories from 20 speakers producing 330 words in citation forms. Using the network analysis, the relationships between acoustic and kinematic variables were revealed. In particular, between-speaker articulatory behaviors were most dissimilar in /ɑ/ which requires less inherent spectral change. Moreover, when networks of speakers with similar formant patterns were compared, it was revealed that their articulatory behaviors also shared similarities, although they seemed to be organized in characteristic ways. These findings contribute to our understanding of the complex interaction between articulatory variables and the acoustic outcome."
   ],
   "doi": "10.21437/Interspeech.2022-10397"
  },
  "tu22_interspeech": {
   "authors": [
    [
     "Zehai",
     "Tu"
    ],
    [
     "Ning",
     "Ma"
    ],
    [
     "Jon",
     "Barker"
    ]
   ],
   "title": "Exploiting Hidden Representations from a DNN-based Speech Recogniser for Speech Intelligibility Prediction in Hearing-impaired Listeners",
   "original": "10399",
   "page_count": 5,
   "order": 706,
   "p1": 3488,
   "pn": 3492,
   "abstract": [
    "An accurate objective speech intelligibility prediction algorithms is of great interest for many applications such as speech enhancement for hearing aids. Most algorithms measures the signal-to-noise ratios or correlations between the acoustic features of clean reference signals and degraded signals. However, these hand-picked acoustic features are usually not explicitly correlated with recognition. Meanwhile, deep neural network (DNN) based automatic speech recogniser (ASR) is approaching human performance in some speech recognition tasks. This work leverages the hidden representations from DNN-based ASR as features for speech intelligibility prediction in hearing-impaired listeners. The experiments based on a hearing aid intelligibility database show that the proposed method could make better prediction than a widely used short-time objective intelligibility (STOI) based binaural measure."
   ],
   "doi": "10.21437/Interspeech.2022-10399"
  },
  "hao22_interspeech": {
   "authors": [
    [
     "Junyong",
     "Hao"
    ],
    [
     "Shunzhou",
     "Ye"
    ],
    [
     "Cheng",
     "Lu"
    ],
    [
     "Fei",
     "Dong"
    ],
    [
     "Jingang",
     "Liu"
    ],
    [
     "Dong",
     "Pi"
    ]
   ],
   "title": "Soft-label Learn for No-Intrusive Speech Quality Assessment",
   "original": "10400",
   "page_count": 5,
   "order": 669,
   "p1": 3303,
   "pn": 3307,
   "abstract": [
    "Mean opinion score (MOS) is a widely used subjective metric to assess the quality of speech, and usually involves multiple human to judge each speech file. To reduce the labor cost of MOS, no-intrusive speech quality assessment methods have been extensively studied. However, due to the highly subjective bias of speech quality label, the performance of models to accurately represent speech quality scores is difficult to be trained. In this paper, we propose a convolutional self-attention neural network (Conformer) for MOS score prediction of conference speech to effectively alleviate the disadvantage of subjective bias on model training. In addition to this novel architecture, we further improve the generalization and accuracy of the predictor by utilizing attention label pooling and soft-label learning. We demonstrate that our proposed method achieves RMSE cost of 0.458 and PLCC score of 0.792 on evaluation test datasets of Conferencing Speech 2022 Challenge"
   ],
   "doi": "10.21437/Interspeech.2022-10400"
  },
  "zhang22z_interspeech": {
   "authors": [
    [
     "Xueshuai",
     "Zhang"
    ],
    [
     "Jiakun",
     "Shen"
    ],
    [
     "Jun",
     "Zhou"
    ],
    [
     "Pengyuan",
     "Zhang"
    ],
    [
     "Yonghong",
     "Yan"
    ],
    [
     "Zhihua",
     "Huang"
    ],
    [
     "Yanfen",
     "Tang"
    ],
    [
     "Yu",
     "Wang"
    ],
    [
     "Fujie",
     "Zhang"
    ],
    [
     "Shaoxing",
     "Zhang"
    ],
    [
     "Aijun",
     "Sun"
    ]
   ],
   "title": "Robust Cough Feature Extraction and Classification Method for COVID-19 Cough Detection Based on Vocalization Characteristics",
   "original": "10401",
   "page_count": 5,
   "order": 442,
   "p1": 2168,
   "pn": 2172,
   "abstract": [
    "A fast, efficient and accurate detection method of COVID-19 remains a critical challenge. Many cough-based COVID-19 detection researches have shown competitive results through artificial intelligence. However, the lack of analysis on vocalization characteristics of cough sounds limits the further improvement of detection performance. In this paper, we propose two novel acoustic features of cough sounds and a convolutional neural network structure for COVID-19 detection. First, a time-frequency differential feature is proposed to characterize dynamic information of cough sounds in time and frequency domain. Then, an energy ratio feature is proposed to calculate the energy difference caused by the phonation characteristics in different cough phases. Finally, a convolutional neural network with two parallel branches which is pre-trained on a large amount of unlabeled cough data is proposed for classification. Experiment results show that our proposed method achieves state-of-the-art performance on Coswara dataset for COVID-19 detection. The results on an external clinical dataset Virufy also show the better generalization ability of our proposed method."
   ],
   "doi": "10.21437/Interspeech.2022-10401"
  },
  "chen22m_interspeech": {
   "authors": [
    [
     "Chengxin",
     "Chen"
    ],
    [
     "Pengyuan",
     "Zhang"
    ]
   ],
   "title": "CTA-RNN: Channel and Temporal-wise Attention RNN leveraging Pre-trained ASR Embeddings for Speech Emotion Recognition",
   "original": "10403",
   "page_count": 5,
   "order": 958,
   "p1": 4730,
   "pn": 4734,
   "abstract": [
    "Previous research has looked into ways to improve speech emotion recognition (SER) by utilizing both acoustic and linguistic cues of speech. However, the potential association between state-of-the-art ASR models and the SER task has yet to be investigated. In this paper, we propose a novel channel and temporal-wise attention RNN (CTA-RNN) architecture based on the intermediate representations of pre-trained ASR models. Specifically, the embeddings of a large-scale pre-trained end-to-end ASR encoder contain both acoustic and linguistic information, as well as the ability to generalize to different speakers, making them well suited for downstream SER task. To further exploit the embeddings from different layers of the ASR encoder, we propose a novel CTA-RNN architecture to capture the emotional salient parts of embeddings in both the channel and temporal directions. We evaluate our approach on two popular benchmark datasets, IEMOCAP and MSP-IMPROV, using both within-corpus and cross-corpus settings. Experimental results show that our proposed method can achieve excellent performance in terms of accuracy and robustness."
   ],
   "doi": "10.21437/Interspeech.2022-10403"
  },
  "yang22r_interspeech": {
   "authors": [
    [
     "Chenyu",
     "Yang"
    ],
    [
     "Yu",
     "Wang"
    ]
   ],
   "title": "Robust End-to-end Speaker Diarization with Generic Neural Clustering",
   "original": "10404",
   "page_count": 5,
   "order": 299,
   "p1": 1471,
   "pn": 1475,
   "abstract": [
    "End-to-end speaker diarization approaches have shown exceptional performance over the traditional modular approaches. To further improve the performance of the end-to-end speaker diarization for real speech recordings, recently works have been proposed which integrate unsupervised clustering algorithms with the end-to-end neural diarization models. However, these methods have a number of drawbacks: 1) The unsupervised clustering algorithms cannot leverage the supervision from the available datasets; 2) The K-means-based unsupervised algorithms that are explored often suffer from the constraint violation problem; 3) There is unavoidable mismatch between the supervised training and the unsupervised inference. In this paper, a robust generic neural clustering approach is proposed that can be integrated with any chunk-level predictor to accomplish a fully supervised end-to-end speaker diarization model. Also, by leveraging the sequence modelling ability of a recurrent neural network, the proposed neural clustering approach can dynamically estimate the number of speakers during inference. Experimental show that when integrating an attractor-based chunk-level predictor, the proposed neural clustering approach can yield better Diarization Error Rate (DER) than the constrained K-means-based clustering approaches under the mismatched conditions."
   ],
   "doi": "10.21437/Interspeech.2022-10404"
  },
  "kocour22_interspeech": {
   "authors": [
    [
     "Martin",
     "Kocour"
    ],
    [
     "Katerina",
     "Zmolikova"
    ],
    [
     "Lucas",
     "Ondel"
    ],
    [
     "Jan",
     "Svec"
    ],
    [
     "Marc",
     "Delcroix"
    ],
    [
     "Tsubasa",
     "Ochiai"
    ],
    [
     "Lukas",
     "Burget"
    ],
    [
     "Jan",
     "Cernocky"
    ]
   ],
   "title": "Revisiting joint decoding based multi-talker speech recognition with DNN acoustic model",
   "original": "10406",
   "page_count": 5,
   "order": 1003,
   "p1": 4955,
   "pn": 4959,
   "abstract": [
    "In typical multi-talker speech recognition systems, a neural network-based acoustic model predicts senone state posteriors for each speaker. These are later used by a single-talker decoder which is applied on each speaker-specific output stream separately. In this work, we argue that such a scheme is sub-optimal and propose a principled solution that decodes all speakers jointly. We modify the acoustic model to predict joint state posteriors for all speakers, enabling the network to express uncertainty about the attribution of parts of the speech signal to the speakers. We employ a joint decoder that can make use of this uncertainty together with higher-level language information. For this, we revisit decoding algorithms used in factorial generative models in early multi-talker speech recognition systems. In contrast with these early works, we replace the GMM acoustic model with DNN, which provides greater modeling power and simplifies part of the inference. We demonstrate the advantage of joint decoding in proof of concept experiments on a mixed-TIDIGITS dataset."
   ],
   "doi": "10.21437/Interspeech.2022-10406"
  },
  "tu22b_interspeech": {
   "authors": [
    [
     "Zehai",
     "Tu"
    ],
    [
     "Ning",
     "Ma"
    ],
    [
     "Jon",
     "Barker"
    ]
   ],
   "title": "Unsupervised Uncertainty Measures of Automatic Speech Recognition for Non-intrusive Speech Intelligibility Prediction",
   "original": "10408",
   "page_count": 5,
   "order": 707,
   "p1": 3493,
   "pn": 3497,
   "abstract": [
    "Non-intrusive intelligibility prediction is important for its application in realistic scenarios, where a clean reference signal is difficult to access. The construction of many non-intrusive predictors require either ground truth intelligibility labels or clean reference signals for supervised learning. In this work, we leverage an unsupervised uncertainty estimation method for predicting speech intelligibility, which does not require intelligibility labels or reference signals to train the predictor. Our experiments demonstrate that the uncertainty from state-of-the-art end-to-end automatic speech recognition (ASR) models is highly correlated with speech intelligibility. The proposed method is evaluated on two databases and the results show that the unsupervised uncertainty measures of ASR models are more correlated with speech intelligibility from listening results than the predictions made by widely used intrusive methods."
   ],
   "doi": "10.21437/Interspeech.2022-10408"
  },
  "svec22_interspeech": {
   "authors": [
    [
     "Jan",
     "Švec"
    ],
    [
     "Jan",
     "Lehečka"
    ],
    [
     "Luboš",
     "Šmídl"
    ]
   ],
   "title": "Deep LSTM Spoken Term Detection using Wav2Vec 2.0 Recognizer",
   "original": "10409",
   "page_count": 5,
   "order": 382,
   "p1": 1886,
   "pn": 1890,
   "abstract": [
    "In recent years, the standard hybrid DNN-HMM speech recognizers are outperformed by the end-to-end speech recognition systems. One of the very promising approaches is the grapheme Wav2Vec 2.0 model, which uses the self-supervised pretraining approach combined with transfer learning of the fine-tuned speech recognizer. Since it lacks the pronunciation vocabulary and language model, the approach is suitable for tasks where obtaining such models is not easy or almost impossible. In this paper, we use the Wav2Vec speech recognizer in the task of spoken term detection over a large set of spoken documents. The method employs a deep LSTM network which maps the recognized hypothesis and the searched term into a shared pronunciation embedding space in which the term occurrences and the assigned scores are easily computed. The paper describes a bootstrapping approach that allows the transfer of the knowledge contained in traditional pronunciation vocabulary of DNN-HMM hybrid ASR into the context of grapheme-based Wav2Vec. The proposed method outperforms the previously published system based on the combination of the DNN-HMM hybrid ASR and phoneme recognizer by a large margin on the MALACH data in both English and Czech languages."
   ],
   "doi": "10.21437/Interspeech.2022-10409"
  },
  "jiang22b_interspeech": {
   "authors": [
    [
     "Wenbin",
     "Jiang"
    ],
    [
     "Tao",
     "Liu"
    ],
    [
     "Kai",
     "Yu"
    ]
   ],
   "title": "Efficient Speech Enhancement with Neural Homomorphic Synthesis",
   "original": "10411",
   "page_count": 5,
   "order": 201,
   "p1": 986,
   "pn": 990,
   "abstract": [
    "Most of the existing deep neural network based speech enhancement methods usually operate on short-time Fourier transform domain or alternatively learned features without employing the speech production model. In this work, we present an efficient speech enhancement algorithm using the speech source-filter model. Concretely, we separate the framed speech into excitation and vocal tract components by homomorphic filtering, adopt two convolutional recurrent networks for estimating the reference magnitude of the separated components, and synthesize the minimum phase signal with the estimated components. Lastly, the enhanced speech is obtained by a post-processing procedure, including using the noisy phase and overlap-addition. Experimental results demonstrated that the proposed method yields a comparable performance with the state-of-the-art complex-valued neural network based method. In addition, we conducted extensive experiments and found that the proposed method is more efficient with a compact model."
   ],
   "doi": "10.21437/Interspeech.2022-10411"
  },
  "bergsma22_interspeech": {
   "authors": [
    [
     "Boris",
     "Bergsma"
    ],
    [
     "Minhao",
     "Yang"
    ],
    [
     "Milos",
     "Cernak"
    ]
   ],
   "title": "PEAF: Learnable Power Efficient Analog Acoustic Features for Audio Recognition",
   "original": "10412",
   "page_count": 5,
   "order": 77,
   "p1": 381,
   "pn": 385,
   "abstract": [
    "At the end of Moore’s law, new computing paradigms are required to prolong the battery life of wearable and IoT smart audio devices. Theoretical analysis and physical validation have shown that analog signal processing (ASP) can be more power-efficient than its digital counterpart in the realm of low-to-medium signal-to-noise ratio applications. In addition, ASP allows a direct interface with an analog microphone without a power-hungry analog-to-digital converter. Here, we present power-efficient analog acoustic features (PEAF) that are validated by fabricated CMOS chips for running audio recognition. Linear, non-linear, and learnable PEAF variants are evaluated on two speech processing tasks that are demanded in many battery-operated devices: wake word detection (WWD) and keyword spotting (KWS). Compared to digital acoustic features, higher power efficiency with competitive classification accuracy can be obtained. A novel theoretical framework based on information theory is established to analyze the information flow in each individual stage of the feature extraction pipeline. The analysis identifies the information bottleneck and helps improve the KWS accuracy by up to 7%. This work may pave the way to building more power-efficient smart audio devices with best-in-class inference performance."
   ],
   "doi": "10.21437/Interspeech.2022-10412"
  },
  "tao22_interspeech": {
   "authors": [
    [
     "Dehua",
     "Tao"
    ],
    [
     "Tan",
     "Lee"
    ],
    [
     "Harold",
     "Chui"
    ],
    [
     "Sarah",
     "Luk"
    ]
   ],
   "title": "Characterizing Therapist's Speaking Style in Relation to Empathy in Psychotherapy",
   "original": "10416",
   "page_count": 5,
   "order": 409,
   "p1": 2003,
   "pn": 2007,
   "abstract": [
    "In conversation-based psychotherapy, therapists use verbal techniques to help clients express thoughts and feelings, and change behavior. In particular, how well therapists convey empathy is an essential quality index of psychotherapy sessions and is associated with psychotherapy outcome. In this paper, we analyze the prosody of therapist speech and attempt to associate the therapist's speaking style with subjectively perceived empathy. An automatic speech and text processing system is developed to segment long recordings of psychotherapy sessions into pause-delimited utterances with text transcriptions. Data-driven clustering is applied to the utterances from different therapists in multiple sessions. For each cluster, a typological representation of utterance genre is derived based on quantized prosodic feature parameters. Prominent speaking styles of the therapist can be observed and interpreted from salient utterance genres that are correlated with empathy. Using the salient utterance genres, an accuracy of 71% is achieved in classifying psychotherapy sessions into \"high\" and \"low\" empathy level. Analysis of results suggests that empathy level tends to be (1) low if therapists speak long utterances slowly or speak short utterances quickly; and (2) high if therapists talk to clients with a steady tone and volume."
   ],
   "doi": "10.21437/Interspeech.2022-10416"
  },
  "wang22v_interspeech": {
   "authors": [
    [
     "Yajian",
     "Wang"
    ],
    [
     "Jun",
     "Du"
    ],
    [
     "Hang",
     "Chen"
    ],
    [
     "Qing",
     "Wang"
    ],
    [
     "Chin-Hui",
     "Lee"
    ]
   ],
   "title": "Deep Segment Model for Acoustic Scene Classification",
   "original": "10418",
   "page_count": 5,
   "order": 847,
   "p1": 4177,
   "pn": 4181,
   "abstract": [
    "In most state-of-the-art acoustic scene classification (ASC) techniques, convolutional neural networks (CNNs) are adopted due to their extraordinary ability in learning local deep features. However, the CNN-based approach is unable to effectively describe the structure of sound events in an audio clip, which is a key element in distinguishing acoustic scenes with similar characteristics, whereas the acoustic segment model (ASM) based approach shows its superiority. To take full advantage of these two types of approaches, we proposed a novel deep segment model (DSM) for ASC. DSM employs a fully convolutional neural network (FCNN) as a deep feature extractor and then guides the ASM to better capture semantic information among sound events. Specifically, the FCNN-based encoder is trained with the multi-task of classifying both three coarse-grained acoustic scenes and ten fine-grained acoustic scenes to extract multi-level acoustic features. Moreover, an entropy-based decision fusion strategy is designed to further utilize the complementarity of FCNN-based and DSM-based systems. The final system achieves an accuracy of 80.4\\% in the DCASE2021 Task1b audio dataset, yielding a relative error rate reduction of about 15\\% over the FCNN-based system."
   ],
   "doi": "10.21437/Interspeech.2022-10418"
  },
  "huang22h_interspeech": {
   "authors": [
    [
     "Yu-Lin",
     "Huang"
    ],
    [
     "Bo-Hao",
     "Su"
    ],
    [
     "Y.-W. Peter",
     "Hong"
    ],
    [
     "Chi-Chun",
     "Lee"
    ]
   ],
   "title": "An Attention-Based Method for Guiding Attribute-Aligned Speech Representation Learning",
   "original": "10419",
   "page_count": 5,
   "order": 1018,
   "p1": 5030,
   "pn": 5034,
   "abstract": [
    "The rich personal information contained in speech signal can lead to privacy leakage and unfair prediction for speech based technology. In this work, we propose a feature-scoring variational autoencoder (FS-VAE) to handle these issues by performing attribute alignment for speech representation learning. FS-VAE performs attribute alignment by using attention-based scoring machines guided by two additional penalty terms. After obtaining the attribute-aligned representation, we can then choose and mask the nodes containing specific attribute of interest based on the requirement in the downstream tasks. We evaluate our methods on tasks of PP-SER (identity-free emotion recognition) and PP-SV (emotion-less speaker verification). Our proposed method achieves better utility maintenance and competitive privacy protection compared to the most recent attribute-aligned representation learning method."
   ],
   "doi": "10.21437/Interspeech.2022-10419"
  },
  "javanmardi22_interspeech": {
   "authors": [
    [
     "Farhad",
     "Javanmardi"
    ],
    [
     "Sudarsana Reddy",
     "Kadiri"
    ],
    [
     "Manila",
     "Kodali"
    ],
    [
     "Paavo",
     "Alku"
    ]
   ],
   "title": "Comparing 1-dimensional and 2-dimensional spectral feature representations in voice pathology detection using machine learning and deep learning classifiers",
   "original": "10420",
   "page_count": 5,
   "order": 443,
   "p1": 2173,
   "pn": 2177,
   "abstract": [
    "The present study investigates the use of 1-dimensional (1-D) and 2-dimensional (2-D) spectral feature representations in voice pathology detection with several classical machine learning (ML) and recent deep learning (DL) classifiers. Four popularly used spectral feature representations (static mel-frequency cepstral coefficients (MFCCs), dynamic MFCCs, spectrogram and mel-spectrogram) are derived in both the 1-D and 2-D form from voice signals. Three widely used ML classifiers (support vector machine (SVM), random forest (RF) and Adaboost) and three DL classifiers (deep neural network (DNN), long short-term memory (LSTM) network, and convolutional neural network (CNN)) are used with the 1-D feature representations. In addition, CNN classifiers are built using the 2-D feature representations. The popularly used HUPA database is considered in the pathology detection experiments. Experimental results revealed that using the CNN classifier with the 2-D feature representations yielded better accuracy compared to using the ML and DL classifiers with the 1-D feature representations. The best performance was achieved using the 2-D CNN classifier based on dynamic MFCCs that showed a detection accuracy of 81%."
   ],
   "doi": "10.21437/Interspeech.2022-10420"
  },
  "ma22_interspeech": {
   "authors": [
    [
     "Guodong",
     "Ma"
    ],
    [
     "Pengfei",
     "Hu"
    ],
    [
     "Nurmemet",
     "Yolwas"
    ],
    [
     "Shen",
     "Huang"
    ],
    [
     "Hao",
     "Huang"
    ]
   ],
   "title": "PM-MMUT: Boosted Phone-mask Data Augmentation using Multi-Modeling Unit Training for Phonetic-Reduction-Robust E2E Speech Recognition",
   "original": "10422",
   "page_count": 5,
   "order": 208,
   "p1": 1021,
   "pn": 1025,
   "abstract": [
    "Consonant and vowel reduction are often encountered in speech, which might cause performance degradation in automatic speech recognition (ASR). Our recently proposed learning strategy based on masking, Phone Masking Training (PMT), alleviates the impact of such phenomenon in Uyghur ASR. Although PMT achieves remarkably improvements, there still exists room for further gains due to the granularity mismatch between the masking unit of PMT (phoneme) and the modeling unit (word-piece). To boost the performance of PMT, we propose multi-modeling unit training (MMUT) architecture fusion with PMT (PM-MMUT). The idea of MMUT framework is to split the Encoder into two parts including acoustic feature sequences to phoneme-level representation (AF-to-PLR) and phoneme-level representation to word-piece-level representation (PLR-to-WPLR). It allows AF-to-PLR to be optimized by an intermediate phoneme-based CTC loss to learn the rich phoneme-level context information brought by PMT. Experimental results on Uyghur ASR show that the proposed approaches outperform obviously the pure PMT. We also conduct experiments on the 960-hour Librispeech benchmark using ESPnet1, which achieves about 10% relative WER reduction on all the test set without LM fusion comparing with the latest official ESPnet1 pre-trained model."
   ],
   "doi": "10.21437/Interspeech.2022-10422"
  },
  "nabe22_interspeech": {
   "authors": [
    [
     "Mamady",
     "NABE"
    ],
    [
     "Julien",
     "Diard"
    ],
    [
     "Jean-Luc",
     "Schwartz"
    ]
   ],
   "title": "Isochronous is beautiful? Syllabic event detection in a neuro-inspired oscillatory model is facilitated by isochrony in speech",
   "original": "10426",
   "page_count": 5,
   "order": 946,
   "p1": 4671,
   "pn": 4675,
   "abstract": [
    "Oscillation-based neuro-computational models of speech perception are grounded in the capacity of human brain oscillations to track the speech signal. Consequently, one would expect this tracking to be more efficient for more regular signals. In this paper, we address the question of the contribution of isochrony to event detection by neuro-computational models of speech perception. We consider a simple model of event detection proposed in the literature, based on oscillatory processes driven by the acoustic envelope, that was previously shown to efficiently detect syllabic events in various languages. We first evaluate its performance in the detection of syllabic events for French, and show that \"perceptual centers\" associated to vowel onsets are more robustly detected than syllable onsets. Then we show that isochrony in natural speech improves the performance of event detection in the oscillatory model. We also evaluate the model's robustness to acoustic noise. Overall, these results show the importance of bottom-up resonance mechanism for event detection; however, they suggest that bottom-up processing of acoustic envelope is not able to perfectly detect events relevant to speech temporal segmentation, highlighting the potential and complementary role of top-down, predictive knowledge."
   ],
   "doi": "10.21437/Interspeech.2022-10426"
  },
  "liu22s_interspeech": {
   "authors": [
    [
     "Baiyun",
     "Liu"
    ],
    [
     "Qi",
     "Song"
    ],
    [
     "Mingxue",
     "Yang"
    ],
    [
     "Wuwen",
     "Yuan"
    ],
    [
     "Tianbao",
     "Wang"
    ]
   ],
   "title": "PLCNet: Real-time Packet Loss Concealment with Semi-supervised Generative Adversarial Network",
   "original": "10428",
   "page_count": 5,
   "order": 116,
   "p1": 575,
   "pn": 579,
   "abstract": [
    "Packet loss is one of the main reasons for speech quality degradation in voice over internet phone (VOIP) calls. However, the existing packet loss concealment (PLC) algorithms are hard to generate high-quality speech signal while maintaining low computational complexity. In this paper, a causal wave-to-wave non-autoregressive lightweight PLC model (PLCNet) is proposed, which can do real-time streaming process with low latency. In addition, we introduce multiple multi-resolution discriminators and semi-supervised training strategy to improve the ability of the encoder part to extract global features while enabling the decoder part to accurately reconstruct waveforms where packets are lost. Contrary to autoregressive model, PLCNet can guarantee the smoothness and continuity of the speech phase before and after packet loss without any smoothing operations. Experimental results show that PLCNet achieves significant improvements in perceptual quality and intelligibility over three classical PLC methods and three state-of-the-art deep PLC methods. In the INTERSPEECH 2022 PLC Challenge, our approach has ranked the 3rd place on PLCMOS (3.829) and the 3rd place on the final score (0.798)."
   ],
   "doi": "10.21437/Interspeech.2022-10428"
  },
  "vitormenezes22_interspeech": {
   "authors": [
    [
     "Joao",
     "Vitor Menezes"
    ],
    [
     "Pouriya",
     "Amini Digehsara"
    ],
    [
     "Christoph",
     "Wagner"
    ],
    [
     "Marco",
     "Mütze"
    ],
    [
     "Michael",
     "Bärhold"
    ],
    [
     "Petr",
     "Schaffer"
    ],
    [
     "Dirk",
     "Plettemeier"
    ],
    [
     "Peter",
     "Birkholz"
    ]
   ],
   "title": "Evaluation of different antenna types and positions in a stepped frequency continuous-wave radar-based silent speech interface",
   "original": "10431",
   "page_count": 5,
   "order": 735,
   "p1": 3633,
   "pn": 3637,
   "abstract": [
    "Silent speech interfaces (SSIs) are subject of growing interest, as they can enable speech communication even in the absence of the acoustic signal. Among sensing techniques used in SSIs, radar sensing has many desirable characteristics, such as non-invasiveness and comfort. Although promising results have been achieved with radar-based SSIs, some of its crucial parameters are yet to be investigated, e.g., the optimal type and position of the antennas. To fill this gap, this study investigated the performance of a radar-based SSI with 3 antenna types attached to 3 positions on the speaker's cheek (9 setups). A corpus of 25 phonemes uttered under co-articulation effects was recorded with the 9 setups by 2 native German speakers and then classified with respect to the phonemes. A linear mixed-effect model was fitted to the resulting recognition rates and likelihood ratio tests showed significance for the effects of antenna type and position. The two monopole-type antennas performed better than the Vivaldi-type antenna (2.7% ± 2.8% and 6.2% ± 3.0% improvement), and the two positions closer to the speaker's lips performed better than the most distant position (decrease of 2.8% ± 0.9%). This provides more solid foundation for the development of this type of SSI."
   ],
   "doi": "10.21437/Interspeech.2022-10431"
  },
  "hollands22_interspeech": {
   "authors": [
    [
     "Samuel",
     "Hollands"
    ],
    [
     "Daniel",
     "Blackburn"
    ],
    [
     "Heidi",
     "Christensen"
    ]
   ],
   "title": "Evaluating the Performance of State-of-the-Art ASR Systems on Non-Native English using Corpora with Extensive Language Background Variation",
   "original": "10433",
   "page_count": 5,
   "order": 803,
   "p1": 3958,
   "pn": 3962,
   "abstract": [
    "This investigation is an exploration into the performance of several different ASR systems in dealing with non-native English using corpora with extensive language background variation. This study takes two corpora amounting to 191 different native language (L1) backgrounds and looks at how these systems are able to process non-native English (L2) speech. A transformer based ASR system and a CRDNN architecture are both tested, trained on Librispeech and Commonvoice for a three way cross comparison. In addition Google's Speech-to-Text API and AWS Transcribe were investigated in order to evaluate popular mainstream approaches given their current degree of impact in deployed systems. Experiments reveal deficits in the range of 10%-15% mean WER performance difference between L1 and L2 speech. Results indicate ASR systems trained on particular varieties of L2 speech may be effective in improving WERs with outcomes in this paper demonstrating several Google ASR models trained on varieties of African L2 English outperforming L1 trained ASR for under-represented dialect groups in the United Kingdom. Further research is proposed to explore the plausibility of this approach and to critically approach WER as a metric for ASR evaluation, striving instead towards metrics with greater emphasis on evaluating language for communication."
   ],
   "doi": "10.21437/Interspeech.2022-10433"
  },
  "braun22_interspeech": {
   "authors": [
    [
     "Franziska",
     "Braun"
    ],
    [
     "Markus",
     "Förstel"
    ],
    [
     "Bastian",
     "Oppermann"
    ],
    [
     "Andreas",
     "Erzigkeit"
    ],
    [
     "Hartmut",
     "Lehfeld"
    ],
    [
     "Thomas",
     "Hillemacher"
    ],
    [
     "Korbinian",
     "Riedhammer"
    ]
   ],
   "title": "Automated Evaluation of Standardized Dementia Screening Tests",
   "original": "10436",
   "page_count": 5,
   "order": 504,
   "p1": 2478,
   "pn": 2482,
   "abstract": [
    "For dementia screening and monitoring, standardized tests play a key role in clinical routine since they aim at minimizing subjectivity by measuring performance on a variety of cognitive tasks. In this paper, we report a study consisting of a semi-standardized history taking followed by two standardized neuropsychological tests, namely the SKT and the CERAD-NB. The tests include basic tasks such as naming objects, learning word lists, but also widely used tools such as the MMSE. Most of the tasks are performed verbally and should thus be suitable for automated scoring based on transcripts. For the first batch of 30 patients, we analyze the correlation between expert manual evaluations and automatic evaluations based on manual and automatic transcriptions. For both SKT and CERAD-NB, we observe high to perfect correlations using manual transcripts; for certain tasks with lower correlation, the automatic scoring is stricter than the human reference since it is limited to the audio. Using automatic transcriptions, correlations drop as expected and are related to recognition accuracy; however, we still observe high correlations of up to 0.98 (SKT) and 0.85 (CERAD-NB). We show that using word alternatives helps to mitigate recognition errors and subsequently improves correlation with expert scores."
   ],
   "doi": "10.21437/Interspeech.2022-10436"
  },
  "chen22n_interspeech": {
   "authors": [
    [
     "Chun-Yu",
     "Chen"
    ],
    [
     "Yun-Shao",
     "Lin"
    ],
    [
     "Chi-Chun",
     "Lee"
    ]
   ],
   "title": "Emotion-Shift Aware CRF for Decoding Emotion Sequence in Conversation",
   "original": "10438",
   "page_count": 5,
   "order": 234,
   "p1": 1148,
   "pn": 1152,
   "abstract": [
    "Emotion recognition in conversation (ERC) is an increasingly important topic as it improves user experiences when adopting speech technology in our daily life. In this work, we propose an emotion-shift aware decoder based on formulation of conditional random field (CRF) to address the perennial issue of poor performances when handling emotion shift in dialogues. We conduct speech emotion recognition experiments on the IEMOCAP and the NNIME and achieve a 74.47% unweighted accuracy, which is the current state-of-the-art performance in the four class emotion recognition on the IEMOCAP. This is also the first work for ERC on the NNIME that obtains an outstanding performance of 61.02\\% weighted accuracy."
   ],
   "doi": "10.21437/Interspeech.2022-10438"
  },
  "lehecka22_interspeech": {
   "authors": [
    [
     "Jan",
     "Lehečka"
    ],
    [
     "Jan",
     "Švec"
    ],
    [
     "Ales",
     "Prazak"
    ],
    [
     "Josef",
     "Psutka"
    ]
   ],
   "title": "Exploring Capabilities of Monolingual Audio Transformers using Large Datasets in Automatic Speech Recognition of Czech",
   "original": "10439",
   "page_count": 5,
   "order": 371,
   "p1": 1831,
   "pn": 1835,
   "abstract": [
    "In this paper, we present our progress in pretraining Czech monolingual audio transformers from a large dataset containing more than 80 thousand hours of unlabeled speech, and subsequently fine-tuning the model on automatic speech recognition tasks using a combination of in-domain data and almost 6 thousand hours of out-of-domain transcribed speech. We are presenting a large palette of experiments with various fine-tuning setups evaluated on two public datasets (CommonVoice and VoxPopuli) and one extremely challenging dataset from the MALACH project. Our results show that monolingual Wav2Vec 2.0 models are robust ASR systems, which can take advantage of large labeled and unlabeled datasets and successfully compete with state-of-the-art LVCSR systems. Moreover, Wav2Vec models proved to be good zero-shot learners when no training data are available for the target ASR task."
   ],
   "doi": "10.21437/Interspeech.2022-10439"
  },
  "shao22b_interspeech": {
   "authors": [
    [
     "Qijie",
     "Shao"
    ],
    [
     "Jinghao",
     "Yan"
    ],
    [
     "Jian",
     "Kang"
    ],
    [
     "Pengcheng",
     "Guo"
    ],
    [
     "Xian",
     "Shi"
    ],
    [
     "Pengfei",
     "Hu"
    ],
    [
     "Lei",
     "Xie"
    ]
   ],
   "title": "Linguistic-Acoustic Similarity Based Accent Shift for Accent Recognition",
   "original": "10444",
   "page_count": 5,
   "order": 754,
   "p1": 3719,
   "pn": 3723,
   "abstract": [
    "General accent recognition (AR) models tend to directly extract low-level information from spectrums, which always significantly overfit on speakers or channels. Considering accent can be regarded as a series of shifts relative to native pronunciation, distinguishing accents will be an easier task with accent shift as input. But due to the lack of native utterance as an anchor, estimating the accent shift is difficult. In this paper, we propose linguistic-acoustic similarity based accent shift (LASAS) for AR tasks. For an accent speech utterance, after mapping the corresponding text vector to multiple accent-associated spaces as anchors, its accent shift could be estimated by the similarities between the acoustic embedding and those anchors. Then, we concatenate the accent shift with a dimension-reduced text vector to obtain a linguistic-acoustic bimodal representation. Compared with pure acoustic embedding, the bimodal representation is richer and more clear by taking full advantage of both linguistic and acoustic information, which can effectively improve AR performance. Experiments on Accented English Speech Recognition Challenge (AESRC) dataset show that our method achieves 77.42% accuracy on Test set, obtaining a 6.94% relative improvement over a competitive system in the challenge."
   ],
   "doi": "10.21437/Interspeech.2022-10444"
  },
  "kakoulidis22_interspeech": {
   "authors": [
    [
     "Panagiotis",
     "Kakoulidis"
    ],
    [
     "Nikolaos",
     "Ellinas"
    ],
    [
     "Georgios",
     "Vamvoukakis"
    ],
    [
     "Konstantinos",
     "Markopoulos"
    ],
    [
     "June Sig",
     "Sung"
    ],
    [
     "Gunu",
     "Jho"
    ],
    [
     "Pirros",
     "Tsiakoulis"
    ],
    [
     "Aimilios",
     "Chalamandaris"
    ]
   ],
   "title": "Karaoker: Alignment-free singing voice synthesis with speech training data",
   "original": "10446",
   "page_count": 5,
   "order": 607,
   "p1": 2993,
   "pn": 2997,
   "abstract": [
    "Existing singing voice synthesis models (SVS)are usually trained on singing data anddepend on either error-prone time-alignment and duration features or explicit music score information. In this paper, we propose Karaoker, a multispeaker Tacotron-based model conditioned on voice characteristic features that is trained exclusively on spoken data without requiring time-alignments. Karaoker synthesizes singing voice and transfers style following a multi-dimensional template extracted from a source waveform of an unseen singer/speaker. The model is jointly conditioned with a single deep convolutional encoder on continuous data including pitch, intensity, harmonicity, formants, cepstral peak prominence and octaves. We extend the text-to-speech training objective with feature reconstruction, classification and speaker identification tasks that guide the model to an accurate result. In addition to multi-tasking, we also employ a Wasserstein GAN training scheme as well as new losses on the acoustic model's output to further refine the quality of the model."
   ],
   "doi": "10.21437/Interspeech.2022-10446"
  },
  "r22_interspeech": {
   "authors": [
    [
     "Kirandevraj",
     "R"
    ],
    [
     "Vinod Kumar",
     "Kurmi"
    ],
    [
     "Vinay",
     "Namboodiri"
    ],
    [
     "C V",
     "Jawahar"
    ]
   ],
   "title": "Generalized Keyword Spotting using ASR embeddings",
   "original": "10450",
   "page_count": 5,
   "order": 26,
   "p1": 126,
   "pn": 130,
   "abstract": [
    "Keyword Spotting (KWS) detects a set of pre-defined spoken keywords. Building a KWS system for an arbitrary set requires massive training datasets. We propose to use the text transcripts from an Automatic Speech Recognition (ASR) system alongside triplets for KWS training. The intermediate representation from the ASR system trained on a speech corpus is used as acoustic word embeddings for keywords. Triplet loss is added to the Connectionist Temporal Classification (CTC) loss in the ASR while training. This method achieves an Average Precision (AP) of 0.843 over 344 words unseen by the model trained on the TIMIT dataset. In contrast, the Multi-View recurrent method that learns jointly on the text and acoustic embeddings achieves only 0.218 for out-of-vocabulary words. This method is also applied to low-resource languages such as Tamil by converting Tamil characters to English using transliteration. This is a very challenging novel task for which we provide a dataset of transcripts for the keywords. Despite our model not generalizing well, we achieve a benchmark AP of 0.321 on over 38 words unseen by the model on the MSWC Tamil keyword set. The model also produces an accuracy of 96.2% for classification tasks on the Google Speech Commands dataset."
   ],
   "doi": "10.21437/Interspeech.2022-10450"
  },
  "landini22_interspeech": {
   "authors": [
    [
     "Federico",
     "Landini"
    ],
    [
     "Alicia",
     "Lozano-Diez"
    ],
    [
     "Mireia",
     "Diez"
    ],
    [
     "Lukáš",
     "Burget"
    ]
   ],
   "title": "From Simulated Mixtures to Simulated Conversations as Training Data for End-to-End Neural Diarization",
   "original": "10451",
   "page_count": 5,
   "order": 1031,
   "p1": 5095,
   "pn": 5099,
   "abstract": [
    "End-to-end neural diarization (EEND) is nowadays one of the most prominent research topics in speaker diarization. EEND presents an attractive alternative to standard cascaded diarization systems since a single system is trained at once to deal with the whole diarization problem. Several EEND variants and approaches are being proposed, however, all these models require large amounts of annotated data for training but available annotated data are scarce. Thus, EEND works have used mostly simulated mixtures for training. However, simulated mixtures do not resemble real conversations in many aspects. In this work we present an alternative method for creating synthetic conversations that resemble real ones by using statistics about distributions of pauses and overlaps estimated on genuine conversations. Furthermore, we analyze the effect of the source of the statistics, different augmentations and amounts of data. We demonstrate that our approach performs substantially better than the original one, while reducing the dependence on the fine-tuning stage. Experiments are carried out on 2-speaker telephone conversations of Callhome and DIHARD 3. Together with this publication, we release our implementations of EEND and the method for creating simulated conversations."
   ],
   "doi": "10.21437/Interspeech.2022-10451"
  },
  "su22_interspeech": {
   "authors": [
    [
     "Bo-Hao",
     "Su"
    ],
    [
     "Chi-Chun",
     "Lee"
    ]
   ],
   "title": "Vaccinating SER to Neutralize Adversarial Attacks with Self-Supervised Augmentation Strategy",
   "original": "10453",
   "page_count": 5,
   "order": 235,
   "p1": 1153,
   "pn": 1157,
   "abstract": [
    "Speech emotion recognition (SER) is being actively developed in multiple real-world application scenarios, and users tend to become intimately connected to these services. However, most existing SER models are vulnerable against a growing diverse set of adversarial attacks. The degraded performances can lead to dreadful user experiences. In this work, we propose a self-supervised augmentation defense (SSAD) strategy to learn a single purify network acts as a general front-end to neutralize adversarial distortions without knowing the types of attack beforehand. We show that our approach can robustly defend against two different gradient-based attacks at various intensities on the well-known IEMOCAP. Further, by examining metrics of protection efficacy and recovery rate, our approach shows a consistent protection behavior to prevent adverse outcomes and is capable to recover samples that are wrongly-predicted before purification."
   ],
   "doi": "10.21437/Interspeech.2022-10453"
  },
  "kukk22_interspeech": {
   "authors": [
    [
     "Kunnar",
     "Kukk"
    ],
    [
     "Tanel",
     "Alumäe"
    ]
   ],
   "title": "Improving Language Identification of Accented Speech",
   "original": "10455",
   "page_count": 5,
   "order": 262,
   "p1": 1288,
   "pn": 1292,
   "abstract": [
    "Language identification from speech is a common preprocessing step in many spoken language processing systems. In recent years, this field has seen a fast progress, mostly due to the use of self-supervised models pretrained on multilingual data and the use of large training corpora. This paper shows that for speech with a non-native or regional accent, the accuracy of spoken language identification systems drops dramatically, and that the accuracy of identifying the language is inversely correlated with the strength of the accent. We also show that using the output of a lexicon-free speech recognition system of the particular language helps to improve language identification performance on accented speech by a large margin, without sacrificing accuracy on native speech. We obtain relative error rate reductions ranging from to 35 to 63% over the state-of-the-art model across several non-native speech datasets."
   ],
   "doi": "10.21437/Interspeech.2022-10455"
  },
  "chien22_interspeech": {
   "authors": [
    [
     "Jen-Tzung",
     "Chien"
    ],
    [
     "Yu-Han",
     "Huang"
    ]
   ],
   "title": "Bayesian Transformer Using Disentangled Mask Attention",
   "original": "10457",
   "page_count": 5,
   "order": 357,
   "p1": 1761,
   "pn": 1765,
   "abstract": [
    "Transformer conducts self attention which has achieved state-of-the-art performance in many applications. Multi-head attention in transformer basically gathers the features from individual tokens in input sequence to form the mapping to output sequence. There are twofold weaknesses in transformer. First, due to the natural property that attention mechanism would mix up the features of different tokens in input and output sequences, it is likely that the representation of input tokens contains redundant information. Second, the patterns of attention weights between different heads tend to be similar, the model capacity is bounded. To strengthen the sequential learning, this paper presents a variational disentangled mask attention in transformer where the redundant features are enhanced with semantic information. Latent disentanglement in multi-head attention is learned. The attention weights are filtered by a mask which is optimized by semantic clustering. The proposed attention mechanism is then implemented according to a Bayesian learning for clustered disentanglement. The experiments on machine translation show the merit of the disentangled mask attention."
   ],
   "doi": "10.21437/Interspeech.2022-10457"
  },
  "robach22_interspeech": {
   "authors": [
    [
     "Jana",
     "Roßbach"
    ],
    [
     "Rainer",
     "Huber"
    ],
    [
     "Saskia",
     "Röttges"
    ],
    [
     "Christopher F.",
     "Hauth"
    ],
    [
     "Thomas",
     "Biberger"
    ],
    [
     "Thomas",
     "Brand"
    ],
    [
     "Bernd T.",
     "Meyer"
    ],
    [
     "Jan",
     "Rennies"
    ]
   ],
   "title": "Speech Intelligibility Prediction for Hearing-Impaired Listeners with the LEAP Model",
   "original": "10460",
   "page_count": 5,
   "order": 708,
   "p1": 3498,
   "pn": 3502,
   "abstract": [
    "The prediction of speech recognition is an important tool for the optimization of speech enhancement algorithms. The first Clarity Prediction Challenge was organized to find the most accurate prediction models for listeners with hearing-impairment and stimuli processed by different speech enhancement algorithms. The modified binaural short-time objective intelligibility (MBSTOI) represents the baseline. Our challenge contribution is based on a model for predicting listening effort. Predictions are obtained non-intrusively using only the output signals from the hearing aid processors. The challenge is split into a closed data set where all listeners and enhancement algorithms are included in training and testing, and an open data set where some listeners and one algorithm are missing in the training set. For the closed set, an individual mapping from the model output to speech intelligibility scores is used whereas for the open set the same mapping is applied for all data points. The model achieves a prediction accuracy of 25.88% root mean squared error (RMSE) (MBSTOI: 28.52%) and a correlation of 0.70 for the closed set. The open set results in an RMSE of 32.07% (MBSTOI: 36.52%) and a correlation of 0.54. The proposed non-intrusive model outperforms the intrusive MBSTOI for both data sets."
   ],
   "doi": "10.21437/Interspeech.2022-10460"
  },
  "sapru22_interspeech": {
   "authors": [
    [
     "Ashtosh",
     "Sapru"
    ]
   ],
   "title": "Using Data Augmentation and Consistency Regularization to Improve Semi-supervised Speech Recognition",
   "original": "10462",
   "page_count": 5,
   "order": 1035,
   "p1": 5115,
   "pn": 5119,
   "abstract": [
    "State-of-the-art automatic speech recognition (ASR) networks use attention mechanism and optimize transducer loss on labeled acoustic data. Recently, Semi-Supervised Learning (SSL) techniques that leverage large amount of unlabeled data have become an active area of interest to improve the performance of ASR networks. In this paper we approach SSL based on the framework of consistency regularization, where data augmentation transforms are used to make ASR network predictions invariant to perturbations in the acoustic data. To increase data diversity we present a combination technique that randomly fuses multiple waveform and feature transforms. For each unlabeled acoustic waveform, two versions, i.e., a weakly augmented and a strongly augmented version of the unaugmented input are generated. During training, a semi-supervised loss is assigned that enforces consistent outputs between the weak and strong augmentations of the unlabeled input. Moreover, we employ model averaging technique to generate stable outputs over time. We compare and demonstrate the benefits of the proposed approach against standard SSL strategies like iterative self-labeling. We leverage over 100000 hours of unlabeled data to train the ASR network using streaming transducer loss and reach improvements in the range of 8%-12% over self-labeling baseline."
   ],
   "doi": "10.21437/Interspeech.2022-10462"
  },
  "toya22_interspeech": {
   "authors": [
    [
     "Teruki",
     "Toya"
    ],
    [
     "Wenyu",
     "Zhu"
    ],
    [
     "Maori",
     "Kobayashi"
    ],
    [
     "Kenichi",
     "Nakamura"
    ],
    [
     "Masashi",
     "Unoki"
    ]
   ],
   "title": "Method for improving the word intelligibility of presented speech using bone-conduction headphones",
   "original": "10463",
   "page_count": 5,
   "order": 153,
   "p1": 759,
   "pn": 763,
   "abstract": [
    "Bone-conduction (BC) headphones enable listeners to hear sounds through BC while leaving the ear canal (EC) open to enable surrounding air-conducted (AC) sound to pass through at the same time. However, the intelligibility of presented speech using BC headphones is degraded by BC transmission, especially in noisy environments. This paper proposes a method for improving the word intelligibility of presented BC speech under noisy conditions. The method consists of two types of emphasis: higher-frequency emphasis and consonant emphasis. In the higher-frequency emphasis, frequency components attenuated due to BC transmission were compensated by the inverse-filtering of the transfer function obtained from the regio-temporalis (RT) vibration or the EC radiated sound. In the consonant emphasis, consonant sections with 20-ms short-formant trajectories of subsequent vowels in speech signals were locally amplified by a constant gain. The results of word intelligibility tests showed that both types of emphasis had significant improvements in comparison with no-emphasis. Moreover, we found that the proposed method had the best improvements under all conditions."
   ],
   "doi": "10.21437/Interspeech.2022-10463"
  },
  "xie22_interspeech": {
   "authors": [
    [
     "Yi",
     "Xie"
    ],
    [
     "Jonathan J.",
     "Macoskey"
    ],
    [
     "Martin",
     "Radfar"
    ],
    [
     "Feng-Ju",
     "Chang"
    ],
    [
     "Brian",
     "King"
    ],
    [
     "Ariya",
     "Rastrow"
    ],
    [
     "Athanasios",
     "Mouchtaris"
    ],
    [
     "Grant",
     "Strimel"
    ]
   ],
   "title": "Compute Cost Amortized Transformer for Streaming ASR",
   "original": "10465",
   "page_count": 5,
   "order": 617,
   "p1": 3043,
   "pn": 3047,
   "abstract": [
    "We present a streaming, Transformer-based end-to-end automatic speech recognition (ASR) architecture which achieves efficient neural inference through compute cost amortization. Our architecture creates sparse computation pathways dynamically at inference time, resulting in selective use of compute resources throughout decoding, enabling significant reductions in compute with minimal impact on accuracy. The fully differentiable architecture is trained end-to-end with an accompanying lightweight arbitrator mechanism operating at the frame-level to make dynamic decisions on each input while a tunable loss function is used to regularize the overall level of compute against predictive performance. We report empirical results from experiments using the compute amortized Transformer-Transducer (T-T) model conducted on LibriSpeech data. Our best model can achieve a 60% compute cost reduction with only a 3% relative word error rate (WER) increase."
   ],
   "doi": "10.21437/Interspeech.2022-10465"
  },
  "liu22t_interspeech": {
   "authors": [
    [
     "Tao",
     "Liu"
    ],
    [
     "Shuai",
     "Fan"
    ],
    [
     "Xu",
     "Xiang"
    ],
    [
     "Hongbo",
     "Song"
    ],
    [
     "Shaoxiong",
     "Lin"
    ],
    [
     "Jiaqi",
     "Sun"
    ],
    [
     "Tianyuan",
     "Han"
    ],
    [
     "Siyuan",
     "Chen"
    ],
    [
     "Binwei",
     "Yao"
    ],
    [
     "Sen",
     "Liu"
    ],
    [
     "Yifei",
     "Wu"
    ],
    [
     "Yanmin",
     "Qian"
    ],
    [
     "Kai",
     "Yu"
    ]
   ],
   "title": "MSDWild: Multi-modal Speaker Diarization Dataset in the Wild",
   "original": "10466",
   "page_count": 5,
   "order": 300,
   "p1": 1476,
   "pn": 1480,
   "abstract": [
    "Speaker diarization in real-world acoustic environments is a challenging task of increasing interest from both academia and industry. Although it has been widely accepted that incorporating visual information benefits audio processing tasks such as speech recognition, there is currently no fully released dataset that can be used for benchmarking multi-modal speaker diarization performance in real-world environments. In this paper, we release MSDWild, a benchmark dataset for multi-modal speaker diarization in the wild. The dataset is collected from public videos, covering rich real-world scenarios and languages. All video clips are naturally shot videos without over-editing such as lens switching. Audio and video are both released. In particular, MSDWild has a large portion of the naturally overlapped speech, forming an excellent testbed for cocktail-party problem research. Furthermore, we also conduct baseline experiments on the dataset using audio-only, visual-only, and audio-visual speaker diarization."
   ],
   "doi": "10.21437/Interspeech.2022-10466"
  },
  "sonowal22_interspeech": {
   "authors": [
    [
     "Sukanya",
     "Sonowal"
    ],
    [
     "Anish",
     "Tamse"
    ]
   ],
   "title": "Novel Augmentation Schemes for Device Robust Acoustic Scene Classification",
   "original": "10468",
   "page_count": 5,
   "order": 848,
   "p1": 4182,
   "pn": 4186,
   "abstract": [
    "For audio classification tasks, one has access to the recordings from only a few microphones while the system could be deployed for a wider range of microphones. This paper discusses augmentation methods for audio scene recognition with the aim of improving performance on recordings from unseen microphones. The proposed augmentation schemes can be broadly classified into two categories. The first category which is called the frequency response augmentation technique, aims to artificially generate ‘new' microphone frequency responses. This is achieved by collecting microphone impulse responses from a publicly available library and applying image augmentation techniques on them to create a more diverse set of frequency responses. The train data is then augmented with these artificially generated frequency responses. The second category consists of the amplitude augmentation and random frame drop methods which are simple yet effective in further boosting the performance. We test all these augmentation methods on various architectures and observe a good classification accuracy of 76.0% on the DCASE 2020 Task 1a set. Especially on unseen devices our best reported accuracy, without using any model ensembles, is 74.24%."
   ],
   "doi": "10.21437/Interspeech.2022-10468"
  },
  "weise22b_interspeech": {
   "authors": [
    [
     "Andreas",
     "Weise"
    ],
    [
     "Rivka",
     "Levitan"
    ]
   ],
   "title": "Investigating the influence of personality on acoustic-prosodic entrainment",
   "original": "10470",
   "page_count": 5,
   "order": 627,
   "p1": 3093,
   "pn": 3097,
   "abstract": [
    "It has long been observed that humans in interaction tend to adapt their behavior to become more similar to their interlocutors. Yet the reasons why such entrainment arises in some conversations and not others remain poorly understood. Early work suggests an influence of different personality traits on the degree of entrainment speakers engage in. However, some of these results have never been replicated to test their generalizability. Moreover, a recent finding draws into question whether these and other effects are strong enough to create differences between speakers that persist across multiple conversations. We investigate a variety of personality traits for their influence on a local form of acoustic-prosodic entrainment in two kinds of spontaneous conversation. To our knowledge, this is the first attempt to detect such effects across several interactions per subject and the first attempt to replicate some influential early work in a more natural context. We find virtually no impact of personality, suggesting that prior results might not generalize to a more natural context or entrainment on other linguistic variables."
   ],
   "doi": "10.21437/Interspeech.2022-10470"
  },
  "um22_interspeech": {
   "authors": [
    [
     "Ji Sub",
     "Um"
    ],
    [
     "Yeunju",
     "Choi"
    ],
    [
     "Hoi Rin",
     "Kim"
    ]
   ],
   "title": "ACNN-VC: Utilizing Adaptive Convolution Neural Network for One-Shot Voice Conversion",
   "original": "10473",
   "page_count": 5,
   "order": 608,
   "p1": 2998,
   "pn": 3002,
   "abstract": [
    "Voice conversion (VC) converts speaker characteristics of a source speaker to ones of a target speaker without modifying the linguistic content. To overcome limitations of the existing VC systems for target speakers unseen during training, many researchers have recently studied one-shot voice conversion. Although many papers have shown that voice conversion can be performed even with only one utterance of an unseen target speaker, it sounds still far from the target speaker's voice. To enhance the similarity of the generated speech, we implement an adaptive convolution neural network (ACNN) for the voice conversion system in two ways. Firstly, we utilize ACNNs with a normalization method to adapt speaker-related information in denormalization process. The second method is to build an architecture with ACNNs that have various receptive fields to generate a voice closer to the target speaker while considering temporal patterns. We evaluated two methods through objective and subjective evaluation metrics. Results show that the converted speech is better than the previous methods in terms of the speaker similarity while keeping the naturalness score."
   ],
   "doi": "10.21437/Interspeech.2022-10473"
  },
  "lee22l_interspeech": {
   "authors": [
    [
     "Hung-Shin",
     "Lee"
    ],
    [
     "Pin-Tuan",
     "Huang"
    ],
    [
     "Yao-Fei",
     "Cheng"
    ],
    [
     "Hsin-Min",
     "Wang"
    ]
   ],
   "title": "Chain-based Discriminative Autoencoders for Speech Recognition",
   "original": "10474",
   "page_count": 5,
   "order": 424,
   "p1": 2078,
   "pn": 2082,
   "abstract": [
    "In our previous work, we proposed a discriminative autoencoder (DcAE) for speech recognition. DcAE combines two training schemes into one. First, since DcAE aims to learn encoder-decoder mappings, the squared error between the reconstructed speech and the input speech is minimized. Second, in the code layer, frame-based phonetic embeddings are obtained by minimizing the categorical cross-entropy between ground truth labels and predicted triphone-state scores. DcAE is developed based on the Kaldi toolkit by treating various TDNN models as encoders. In this paper, we further propose three new versions of DcAE. First, a new objective function that considers both categorical cross-entropy and mutual information between ground truth and predicted triphone-state sequences is used. The resulting DcAE is called a chain-based DcAE (c-DcAE). For application to robust speech recognition, we further extend c-DcAE to hierarchical and parallel structures, resulting in hc-DcAE and pc-DcAE. In these two models, both the error between the reconstructed noisy speech and the input noisy speech and the error between the enhanced speech and the reference clean speech are taken into the objective function. Experimental results on the WSJ and Aurora-4 corpora show that our DcAE models outperform baseline systems."
   ],
   "doi": "10.21437/Interspeech.2022-10474"
  },
  "cheng22c_interspeech": {
   "authors": [
    [
     "Jingwen",
     "Cheng"
    ],
    [
     "Yuchen",
     "Yan"
    ],
    [
     "Yingming",
     "Gao"
    ],
    [
     "Xiaoli",
     "Feng"
    ],
    [
     "Yannan",
     "Wang"
    ],
    [
     "Jinsong",
     "Zhang"
    ]
   ],
   "title": "A study of production error analysis for Mandarin-speaking Children with Hearing Impairment",
   "original": "10477",
   "page_count": 5,
   "order": 980,
   "p1": 4840,
   "pn": 4844,
   "abstract": [
    "Investigating the speech acquisition of hearing-impaired children attracts considerable attentions in recent years. Previous studies that investigate Mandarin-speaking children with hearing impairment mostly focus on production of some specific phonemes. Besides, the phonemes are sometimes embedded in a limited number of speech materials or uttered by only a few speakers. In this study, we analyzed the pronunciation errors of all Mandarin vowels and consonant produced by 60 pre- or post-lingually hearing-impaired children. We designed a set of speech materials that consisted of 153 monosyllable and 145 disyllable commonly used words and had a comprehensive phonetic coverage. The analysis shows that monophthongs were produced less accurately than diphthongs and triphthongs. Bilabials and nasals and plosives were relatively easier for hearing-impaired children to acquire than other consonants with respect to articulation manner and place, respectively. The Mandarin affricates had the lowest accuracy. Substitution is the most frequent error patterns for initial consonants while deletion is the common error for final consonants. The findings of this study can shed light on pronunciation teaching of hearing-impaired children. Besides, the corpus can benefit developing computer-assisted speech assessment system."
   ],
   "doi": "10.21437/Interspeech.2022-10477"
  },
  "chao22_interspeech": {
   "authors": [
    [
     "Rong",
     "Chao"
    ],
    [
     "Cheng",
     "Yu"
    ],
    [
     "Szu-wei",
     "Fu"
    ],
    [
     "Xugang",
     "Lu"
    ],
    [
     "Yu",
     "Tsao"
    ]
   ],
   "title": "Perceptual Contrast Stretching on Target Feature for Speech Enhancement",
   "original": "10478",
   "page_count": 5,
   "order": 1104,
   "p1": 5448,
   "pn": 5452,
   "abstract": [
    "Speech enhancement (SE) performance has improved considerably owing to the use of deep learning models as a base function. Herein, we propose a perceptual contrast stretching (PCS) approach to further improve SE performance. The PCS is derived based on the critical band importance function and is applied to modify the targets of the SE model. Specifically, the contrast of target features is stretched based on perceptual importance, thereby improving the overall SE performance. Compared with post-processing-based implementations, incorporating PCS into the training phase preserves performance and reduces online computation. Notably, PCS can be combined with different SE model architectures and training criteria. Furthermore, PCS does not affect the causality or convergence of SE model training. Experimental results on the VoiceBank-DEMAND dataset show that the proposed method can achieve state-of-the-art performance on both causal (PESQ score = 3.07) and noncausal (PESQ score = 3.35) SE tasks."
   ],
   "doi": "10.21437/Interspeech.2022-10478"
  },
  "kons22_interspeech": {
   "authors": [
    [
     "Zvi",
     "Kons"
    ],
    [
     "Hagai",
     "Aronowitz"
    ],
    [
     "Edmilson",
     "Morais"
    ],
    [
     "Matheus",
     "Damasceno"
    ],
    [
     "Hong-Kwang",
     "Kuo"
    ],
    [
     "Samuel",
     "Thomas"
    ],
    [
     "George",
     "Saon"
    ]
   ],
   "title": "Extending RNN-T-based speech recognition systems with emotion and language classification",
   "original": "10480",
   "page_count": 4,
   "order": 110,
   "p1": 546,
   "pn": 549,
   "abstract": [
    "Speech transcription, emotion recognition, and language identification are usually considered to be three different tasks. Each one requires a different model with a different architecture and training process. We propose using a recurrent neural network transducer (RNN-T)-based speech-to-text (STT) system as a common component that can be used for emotion recognition and language identification as well as for speech recognition. Our work extends the STT system for emotion classification through minimal changes, and shows successful results on the IEMOCAP and MELD datasets. In addition, we demonstrate that by adding a lightweight component to the RNN-T module, it can also be used for language identification. In our evaluations, this new classifier demonstrates state-of-the-art accuracy for the NIST-LRE-07 dataset."
   ],
   "doi": "10.21437/Interspeech.2022-10480"
  },
  "chen22o_interspeech": {
   "authors": [
    [
     "Hang",
     "Chen"
    ],
    [
     "Jun",
     "Du"
    ],
    [
     "Yusheng",
     "Dai"
    ],
    [
     "Chin-Hui",
     "Lee"
    ],
    [
     "Sabato Marco",
     "Siniscalchi"
    ],
    [
     "Shinji",
     "Watanabe"
    ],
    [
     "Odette",
     "Scharenborg"
    ],
    [
     "Jingdong",
     "Chen"
    ],
    [
     "Baocai",
     "Yin"
    ],
    [
     "Jia",
     "Pan"
    ]
   ],
   "title": "Audio-Visual Speech Recognition in MISP2021 Challenge: Dataset Release and Deep Analysis",
   "original": "10483",
   "page_count": 5,
   "order": 358,
   "p1": 1766,
   "pn": 1770,
   "abstract": [
    "In this paper, we present the updated Audio-Visual Speech Recognition (AVSR) corpus of MISP2021 challenge, a large-scale audio-visual Chinese conversational corpus consisting of 141h audio and video data collected by far/middle/near microphones and far/middle cameras in 34 real-home TV rooms. To our best knowledge, our corpus is the first distant multi-microphone conversational Chinese audio-visual corpus and the first large vocabulary continuous Chinese lip-reading dataset in the adverse home-tv scenario. Moreover, we make a deep analysis of the corpus and conduct a comprehensive ablation study of all audio and video data in the audio-only/video-only/audio-visual systems. Error analysis shows video modality supplement acoustic information degraded by noise to reduce deletion errors and provide discriminative information in overlapping speech to reduce substitution errors. Finally, we also design a set of experiments such as frontend, data augmentation and end-to-end models for providing the direction of potential future work. The corpus and the code are released to promote the research not only in speech area but also for the computer vision area and cross-disciplinary research."
   ],
   "doi": "10.21437/Interspeech.2022-10483"
  },
  "flechl22_interspeech": {
   "authors": [
    [
     "Martin",
     "Flechl"
    ],
    [
     "Shou-Chun",
     "Yin"
    ],
    [
     "Junho",
     "Park"
    ],
    [
     "Peter",
     "Skala"
    ]
   ],
   "title": "End-to-end speech recognition modeling from de-identified data",
   "original": "10484",
   "page_count": 5,
   "order": 281,
   "p1": 1382,
   "pn": 1386,
   "abstract": [
    "De-identification of data used for automatic speech recognition modeling is a critical component in protecting privacy, especially in the medical domain. However, simply removing all personally identifiable information (PII) from end-to-end model training data leads to a significant performance degradation in particular for the recognition of names, dates, locations, and words from similar categories. We propose and evaluate a two-step method for partially recovering this loss. First, PII is identified, and each occurrence is replaced with a random word sequence of the same category. Then, corresponding audio is produced via text-to-speech or by splicing together matching audio fragments extracted from the corpus. These artificial audio/label pairs, together with speaker turns from the original data without PII, are used to train models. We evaluate the performance of this method on in-house data of medical conversations and observe a recovery of almost the entire performance degradation in the general word error rate while still maintaining a strong diarization performance. Our main focus is the improvement of recall and precision in the recognition of PII-related words. Depending on the PII category, between 50% - 90% of the performance degradation can be recovered using our proposed method."
   ],
   "doi": "10.21437/Interspeech.2022-10484"
  },
  "cardinale22_interspeech": {
   "authors": [
    [
     "Franklin Alvarez",
     "Cardinale"
    ],
    [
     "Waldo",
     "Nogueira"
    ]
   ],
   "title": "Predicting Speech Intelligibility using the Spike Acativity Mutual Information Index",
   "original": "10488",
   "page_count": 5,
   "order": 709,
   "p1": 3503,
   "pn": 3507,
   "abstract": [
    "The spike activity mutual information index (SAMII) is presented as a new intrusive objective metric to predict speech intelligibility. A target speech signal and speech-in-noise signal are processed by a state-of-the-art computational model of the peripheral auditory system. It simulates the neural activity in a population of auditory nerve fibers (ANFs), which are grouped into critical bands covering the speech frequency range. The mutual information between the neural activity of both signals is calculated using analysis windows of 20 ms. Then, the mutual information is averaged along these analysis windows to obtain SAMII. SAMII is also extended to binaural scenarios by calculating the index for the left ear, right ear, and both ears, choosing the best case for predicting intelligibility. SAMII was developed based on the first clarity prediction challenge training dataset and compared to the modified binaural short-time objective intelligibility (MBSTOI) as baseline. Scores are reported in root mean squared error (RMSE) between measured and predicted data using the clarity challenge test dataset. SAMII scored 35.16\\%, slightly better than the MBSTOI which obtained 36.52\\%. This work leads to the conclusion that SAMII is a reliable objective metric when ``low-level\" representations of the speech, such as spike activity, are used."
   ],
   "doi": "10.21437/Interspeech.2022-10488"
  },
  "prabhu22_interspeech": {
   "authors": [
    [
     "Navin Raj",
     "Prabhu"
    ],
    [
     "Guillaume",
     "Carbajal"
    ],
    [
     "Nale",
     "Lehmann-Willenbrock"
    ],
    [
     "Timo",
     "Gerkmann"
    ]
   ],
   "title": "End-To-End Label Uncertainty Modeling for Speech-based Arousal Recognition Using Bayesian Neural Networks",
   "original": "10490",
   "page_count": 5,
   "order": 31,
   "p1": 151,
   "pn": 155,
   "abstract": [
    "Emotions are subjective constructs. Recent end-to-end speech emotion recognition systems are typically agnostic to the subjective nature of emotions, despite their state-of-the-art performance. In this work, we introduce an end-to-end Bayesian neural network architecture to capture the inherent subjectivity in the arousal dimension of emotional expressions. To the best of our knowledge, this work is the first to use Bayesian neural networks for speech emotion recognition. At training, the network learns a distribution of weights to capture the inherent uncertainty related to subjective arousal annotations. To this end, we introduce a loss term that enables the model to be explicitly trained on a distribution of annotations, rather than training them exclusively on mean or gold-standard labels. We evaluate the proposed approach on the AVEC'16 dataset. Qualitative and quantitative analysis of the results reveals that the proposed model can aptly capture the distribution of subjective arousal annotations, with state-of-the-art results in mean and standard deviation estimations for uncertainty modeling."
   ],
   "doi": "10.21437/Interspeech.2022-10490"
  },
  "pandey22_interspeech": {
   "authors": [
    [
     "Ashutosh",
     "Pandey"
    ],
    [
     "DeLiang",
     "Wang"
    ]
   ],
   "title": "Attentive Training: A New Training Framework for Talker-independent Speaker Extraction",
   "original": "10491",
   "page_count": 5,
   "order": 41,
   "p1": 201,
   "pn": 205,
   "abstract": [
    "Listening in a multitalker scenario, we typically attend to a single talker through auditory selective attention. Inspired by human selective attention, we propose attentive training: a new training framework for talker-independent speaker extraction with an intrinsic selection mechanism. In the real world, multiple talkers very unlikely start speaking at the same time. Based on this observation, we train a deep neural network to create a representation for the first speaker and utilize it to extract or track that speaker from a multitalker noisy mixture. Experimental results demonstrate the superiority of attentive training over widely used permutation invariant training for talker-independent speaker extraction, especially in mismatched conditions in terms of the number of speakers, speaker interaction patterns, and the amount of speaker overlaps."
   ],
   "doi": "10.21437/Interspeech.2022-10491"
  },
  "yin22b_interspeech": {
   "authors": [
    [
     "Haoran",
     "Yin"
    ],
    [
     "Meng",
     "Ge"
    ],
    [
     "Yanjie",
     "Fu"
    ],
    [
     "Gaoyan",
     "Zhang"
    ],
    [
     "Longbiao",
     "Wang"
    ],
    [
     "Lei",
     "Zhang"
    ],
    [
     "Lin",
     "Qiu"
    ],
    [
     "Jianwu",
     "Dang"
    ]
   ],
   "title": "MIMO-DoAnet: Multi-channel Input and Multiple Outputs DoA Network with Unknown Number of Sound Sources",
   "original": "10493",
   "page_count": 5,
   "order": 182,
   "p1": 891,
   "pn": 895,
   "abstract": [
    "Recent neural network based Direction of Arrival (DoA) estimation algorithms have performed well on unknown number of sound sources scenarios. These algorithms are usually achieved by mapping the multi-channel audio input to the single output (i.e. overall spatial pseudo-spectrum (SPS) of all sources), that is called MISO. However, such MISO algorithms strongly depend on empirical threshold setting and the angle assumption that the angles between the sound sources are greater than a fixed angle. To address these limitations, we propose a novel multi-channel input and multiple outputs DoA network called MIMO-DoAnet. Unlike the general MISO algorithms, MIMO-DoAnet predicts the SPS coding of each sound source with the help of the informative spatial covariance matrix. By doing so, the threshold task of detecting the number of sound sources becomes an easier task of detecting whether there is a sound source in each output, and the serious interaction between sound sources disappears during inference stage. Experimental results show that MIMO-DoAnet achieves relative 18.6% and absolute 13.3%, relative 34.4% and absolute 20.2% F1 score improvement compared with the MISO baseline system in 3, 4 sources scenes. The results also demonstrate MIMO-DoAnet alleviates the threshold setting problem and solves the angle assumption problem effectively."
   ],
   "doi": "10.21437/Interspeech.2022-10493"
  },
  "zeng22_interspeech": {
   "authors": [
    [
     "Chang",
     "Zeng"
    ],
    [
     "Lin",
     "Zhang"
    ],
    [
     "Meng",
     "Liu"
    ],
    [
     "Junichi",
     "Yamagishi"
    ]
   ],
   "title": "Spoofing-Aware Attention based ASV Back-end with Multiple Enrollment Utterances and a Sampling Strategy for the SASV Challenge 2022",
   "original": "10495",
   "page_count": 5,
   "order": 585,
   "p1": 2883,
   "pn": 2887,
   "abstract": [
    "Current state-of-the-art automatic speaker verification (ASV) systems are vulnerable to presentation attacks, and several countermeasures (CMs), which distinguish bona fide trials from spoofing ones, have been explored to protect ASV. However, ASV systems and CMs are generally developed and optimized independently without considering their inter-relationship. In this paper, we propose a new spoofing-aware ASV back-end module that efficiently computes a combined ASV score based on speaker similarity and CM score. In addition to the learnable fusion function of the two scores, the proposed back-end module has two types of attention components, scaled-dot and feed-forward self-attention, so that intra-relationship information of multiple enrollment utterances can also be learned at the same time. Moreover, a new effective trials-sampling strategy is designed for simulating new spoofing-aware verification scenarios introduced in the Spoof-Aware Speaker Verification (SASV) challenge 2022. Combining the two types of scores using the proposed back-end optimized by using the sampling strategies, it is confirmed that the SASV-EER can be significantly reduced from 22.91\\% to 1.19\\% on the evaluation set of the ASVSpoof 2019 LA database."
   ],
   "doi": "10.21437/Interspeech.2022-10495"
  },
  "elbanna22_interspeech": {
   "authors": [
    [
     "Gasser",
     "Elbanna"
    ],
    [
     "Alice",
     "Biryukov"
    ],
    [
     "Neil",
     "Scheidwasser-Clow"
    ],
    [
     "Lara",
     "Orlandic"
    ],
    [
     "Pablo",
     "Mainar"
    ],
    [
     "Mikolaj",
     "Kegler"
    ],
    [
     "Pierre",
     "Beckmann"
    ],
    [
     "Milos",
     "Cernak"
    ]
   ],
   "title": "Hybrid Handcrafted and Learnable Audio Representation for Analysis of Speech Under Cognitive and Physical Load",
   "original": "10498",
   "page_count": 5,
   "order": 78,
   "p1": 386,
   "pn": 390,
   "abstract": [
    "As a neurophysiological response to threat or adverse conditions, stress can affect cognition, emotion and behaviour with potentially detrimental effects on health in the case of sustained exposure. Since the affective content of speech is inherently modulated by an individual's physical and mental state, a substantial body of research has been devoted to the study of paralinguistic correlates of stress-inducing task load. Historically, voice stress analysis (VSA) has been conducted using conventional digital signal processing (DSP) techniques. Despite the development of modern methods based on deep neural networks (DNNs), accurately detecting stress in speech remains difficult due to the wide variety of stressors and considerable variability in the individual stress perception. To that end, we introduce a set of five datasets for task load detection in speech. The voice recordings were collected as either cognitive or physical stress was induced in the cohort of volunteers, with a cumulative number of more than a hundred speakers. We used the datasets to design and evaluate a novel self-supervised audio representation that leverages the effectiveness of handcrafted features (DSP-based) and the complexity of data-driven DNN representations. Notably, the proposed approach outperformed both extensive handcrafted feature sets and novel DNN-based audio representation learning approaches."
   ],
   "doi": "10.21437/Interspeech.2022-10498"
  },
  "tan22b_interspeech": {
   "authors": [
    [
     "Hao",
     "Tan"
    ],
    [
     "Junjian",
     "Zhang"
    ],
    [
     "Huan",
     "Zhang"
    ],
    [
     "Le",
     "Wang"
    ],
    [
     "Yaguan",
     "Qian"
    ],
    [
     "Zhaoquan",
     "Gu"
    ]
   ],
   "title": "NRI-FGSM: An Efficient Transferable Adversarial Attack for Speaker Recognition Systems",
   "original": "10499",
   "page_count": 5,
   "order": 889,
   "p1": 4386,
   "pn": 4390,
   "abstract": [
    "Deep neural network (DNN), though widely applied in Speaker Recognition Systems (SRS), is vulnerable to adversarial attacks which are hard to detect by humans. The black-box model vulnerability against adversarial attacks is crucial for the robustness of SRS, especially for the latest models such as x-vector and ECAPA-TDNN. The state-of-the-art transferable adversarial attack methods start with generating the adversarial audio from white-box SRS, then utilizing this audio to attack the black-box SRS. However, these methods often have a lower success rate in SRS than in the image processing domain. To improve the attack performance on SRS, we propose an efficient Nesterov accelerate and RMSProp optimization-based Iterative-Fast Gradient Sign Method (NRI-FGSM), which integrates the Nesterov Accelerated Gradient method and the Root Mean Squared Propagation optimization method with adaptive step size. Through extensive experiments on both closed-set speaker recognition (CSR) and open-set speaker recognition (OSR) tasks, our method achieves higher attack success rates of 97.8% for CSR and 61.9% for OSR tasks than others, and meanwhile maintains a lower perturbation rate with signal-to-noise ratio (SNR) and perceptual evaluation of speech quality (PESQ) metrics. It is worth mentioning that our work is the first to attack the ECAPA-TDNN SRS model successfully."
   ],
   "doi": "10.21437/Interspeech.2022-10499"
  },
  "xiao22_interspeech": {
   "authors": [
    [
     "Yang",
     "Xiao"
    ],
    [
     "Nana",
     "Hou"
    ],
    [
     "Eng Siong",
     "Chng"
    ]
   ],
   "title": "Rainbow Keywords: Efficient Incremental Learning for Online Spoken Keyword Spotting",
   "original": "10500",
   "page_count": 5,
   "order": 763,
   "p1": 3764,
   "pn": 3768,
   "abstract": [
    "Catastrophic forgetting is a thorny challenge when updating keyword spotting (KWS) models after deployment. This problem will be more challenging if KWS models are further required for edge devices due to their limited memory. To alleviate such an issue, we propose a novel diversity-aware incremental learning method named Rainbow Keywords (RK). Specifically, the proposed RK approach introduces a diversity-aware sampler to select a diverse set from historical and incoming keywords by calculating classification uncertainty. As a result, the RK approach can incrementally learn new tasks without forgetting prior knowledge. Besides, the RK approach also proposes data augmentation and knowledge distillation loss function for efficient memory management on the edge device. Experimental results show that the proposed RK approach achieves 4.2% absolute improvement in terms of average accuracy over the best baseline on Google Speech Command dataset with less required memory. The scripts are available on GitHub"
   ],
   "doi": "10.21437/Interspeech.2022-10500"
  },
  "ou22_interspeech": {
   "authors": [
    [
     "yangyang",
     "Ou"
    ],
    [
     "Peng",
     "Zhang"
    ],
    [
     "Jing",
     "Zhang"
    ],
    [
     "Hui",
     "Gao"
    ],
    [
     "Xing",
     "Ma"
    ]
   ],
   "title": "Incorporating Dual-Aware with Hierarchical Interactive Memory Networks for Task-Oriented Dialogue",
   "original": "10501",
   "page_count": 5,
   "order": 551,
   "p1": 2713,
   "pn": 2717,
   "abstract": [
    "Recent years, end-to-end task-oriented dialogue systems have made a remarkable breakthrough. However, existing dialogue models tend to equally summarize all the history as the context representation and apply memory networks to incorporate external knowledge. They neglect to highlight the latest request of users, which will cause the dialogue system to generate improper responses. In addition, it is insufficient for the original memory networks to interact between memories only at hop-level and difficult to extract more useful knowledge information. To address these issues, we propose a novel neural model which incorporates Dual-Aware with Hierarchical Interactive Memory Networks (DA-HIMN). The dual-aware constituting static request-aware and dynamic KB-aware is responsible for capturing the latest request of users and collecting related knowledge information. Furthermore, we design a hierarchical interaction mechanism to augment the memory networks at layer-level to more adequately learn the knowledge representation. Our experimental results demonstrate that our model outperforms the baseline model on two task-oriented dialogue datasets in several evaluation metrics."
   ],
   "doi": "10.21437/Interspeech.2022-10501"
  },
  "du22d_interspeech": {
   "authors": [
    [
     "Yicheng",
     "Du"
    ],
    [
     "Aditya Arie",
     "Nugraha"
    ],
    [
     "Kouhei",
     "Sekiguchi"
    ],
    [
     "Yoshiaki",
     "Bando"
    ],
    [
     "Mathieu",
     "Fontaine"
    ],
    [
     "Kazuyoshi",
     "Yoshii"
    ]
   ],
   "title": "Direction-Aware Joint Adaptation of Neural Speech Enhancement and Recognition in Real Multiparty Conversational Environments",
   "original": "10508",
   "page_count": 5,
   "order": 592,
   "p1": 2918,
   "pn": 2922,
   "abstract": [
    "This paper describes noisy speech recognition for an augmented reality headset that helps verbal communication with in real multiparty conversational environments. A major approach that has actively been studied in simulated environments is to sequentially perform speech enhancement and automatic speech recognition (ASR) based on deep neural networks (DNNs) trained in a supervised manner. In our task, however, such a pretrained system fails to work due to the mismatch between the training and test conditions and the head movements of the user. To enhance only the utterances of a target speaker, we use beamforming based on a DNN-based speech mask estimator that can adaptively extract the speech components corresponding to a head-relative particular direction. We propose a semi-supervised adaptation method that jointly updates the mask estimator and the ASR model at run-time using clean speech signals with ground-truth transcriptions and noisy speech signals with highly-confident estimated transcriptions. Comparative experiments using the state-of-the-art distant speech recognition system show that the proposed method significantly improves the ASR performance."
   ],
   "doi": "10.21437/Interspeech.2022-10508"
  },
  "chen22p_interspeech": {
   "authors": [
    [
     "Chen",
     "Chen"
    ],
    [
     "Nana",
     "Hou"
    ],
    [
     "Yuchen",
     "Hu"
    ],
    [
     "Heqing",
     "Zou"
    ],
    [
     "Xiaofeng",
     "Qi"
    ],
    [
     "Eng Siong",
     "Chng"
    ]
   ],
   "title": "Interactive Auido-text Representation for Automated Audio Captioning with Contrastive Learning",
   "original": "10510",
   "page_count": 5,
   "order": 563,
   "p1": 2773,
   "pn": 2777,
   "abstract": [
    "Automated Audio captioning (AAC) is a cross-modal task that generates natural language to describe the content of input audio. Most prior works usually extract single-modality acoustic features and are therefore sub-optimal for the cross-modal decoding task. In this work, we propose a novel AAC system called CLIP-AAC to learn interactive cross-modality representation with both acoustic and textual information. Specifically, the proposed CLIP-AAC introduces an audio-head and a text-head in the pre-trained encoder to extract audio-text information. Furthermore, we also apply contrastive learning to narrow the domain difference by learning the correspondence between the audio signal and its paired captions. Experimental results show that the proposed CLIP-AAC approach surpasses the best baseline by a significant margin on the Clotho dataset in terms of NLP evaluation metrics. The ablation study indicates that both the pre-trained model and contrastive learning contribute to the performance gain of the AAC model."
   ],
   "doi": "10.21437/Interspeech.2022-10510"
  },
  "kadiri22_interspeech": {
   "authors": [
    [
     "Sudarsana Reddy",
     "Kadiri"
    ],
    [
     "Farhad",
     "Javanmardi"
    ],
    [
     "Paavo",
     "Alku"
    ]
   ],
   "title": "Convolutional Neural Networks for Classification of Voice Qualities from Speech and Neck Surface Accelerometer Signals",
   "original": "10513",
   "page_count": 5,
   "order": 1065,
   "p1": 5253,
   "pn": 5257,
   "abstract": [
    "Prior studies in the automatic classification of voice quality have mainly studied support vector machine (SVM) classifiers using the acoustic speech signal as input. Recently, one voice quality classification study was published using neck surface accelerometer (NSA) and speech signals as inputs and using SVMs with hand-crafted glottal source features. The present study examines simultaneously recorded NSA and speech signals in the classification of three voice qualities (breathy, modal, and pressed) using convolutional neural networks (CNNs) as classifier. The study has two goals: (1) to investigate which of the two signals (NSA vs. speech) is more useful in the classification task, and (2) to compare whether deep learning -based CNN classifiers with spectrogram and mel-spectrogram features are able to improve the classification accuracy compared to SVM classifiers using hand-crafted glottal source features. The results indicated that the NSA signal showed better classification of the voice qualities compared to the speech signal, and that the CNN classifier outperformed the SVM classifiers with large margins. The best mean classification accuracy was achieved with mel-spectrogram as input to the CNN classifier (93.8% for NSA and 90.6% for speech)."
   ],
   "doi": "10.21437/Interspeech.2022-10513"
  },
  "huckvale22_interspeech": {
   "authors": [
    [
     "Mark",
     "Huckvale"
    ],
    [
     "Gaston",
     "Hilkhuysen"
    ]
   ],
   "title": "ELO-SPHERES intelligibility prediction model for the Clarity Prediction Challenge 2022",
   "original": "10521",
   "page_count": 5,
   "order": 797,
   "p1": 3934,
   "pn": 3938,
   "abstract": [
    "This paper describes and evaluates the ELO-SPHERES project sentence intelligibility model for the Clarity Prediction Challenge 2022. The aim of the model is to make predictions of the intelligibility of enhanced speech to hearing impaired listeners. Input to the model are binaural processed audio of short sentences generated in a simulated noisy and reverberant environment together with the original source audio. Output of the model is a prediction of the intelligibility of each sentence in terms of percentage words correct for a known hearing-impaired listener characterized by a pure-tone audiogram. Models are evaluated in terms of the root mean squared error of prediction. We approached this problem in three stages: (i) evaluation of the influences of the scene metadata on scores, (ii) construction of classifiers for estimation of scene metadata from audio, and (iii) training a non-linear regression model on the challenge data and evaluation using 5-fold cross validation. On the test data, a baseline system using only the standard short-time objective intelligibility metric on the better ear achieved a RMS prediction error of 27%, while our model that also took into account given and estimated scene data achieved an RMS error of 22%."
   ],
   "doi": "10.21437/Interspeech.2022-10521"
  },
  "geng22_interspeech": {
   "authors": [
    [
     "YANZHANG",
     "GENG"
    ],
    [
     "Heng",
     "Wang"
    ],
    [
     "Tao",
     "Zhang"
    ],
    [
     "Xin",
     "Zhao"
    ]
   ],
   "title": "A speech enhancement method for long-range speech acquisition task",
   "original": "10523",
   "page_count": 5,
   "order": 1105,
   "p1": 5453,
   "pn": 5457,
   "abstract": [
    "Using audio devices to collect speech from far is a significant and challenging task. The speech energy severely decays if the speaker is far away from the audio acquisition device, resulting in poor speech quality. Usually, a long-range speech acquisition task can be seen as a speech enhancement problem in a specific application scenario. We proposed a speech enhancement method called Paraboloid with Microphone Array (PMA) to enhance long-range speech. Firstly, a parabolic reflector is employed to enhance the target signal acoustically. Meanwhile, the Linearly Constrained Minimum Variance(LCMV) algorithm based on a microphone array is used to jam the target speech and estimate the noise. Then a mapping relationship between the above two output noises is established using an improved Long Short-Term Memory(LSTM) neural network. At last, with the mapping model, the final enhanced speech is obtained by reducing the LCMV estimated noise from the acoustically enhanced speech remained noise. Computer simulations show that the proposed method can effectively enhance speech from the speaker within 50 meters. Besides, the outdoor experiment with a realistic PMA and the subjective listening test also confirmed the effectiveness of the PMA in a real-world scenario."
   ],
   "doi": "10.21437/Interspeech.2022-10523"
  },
  "fu22c_interspeech": {
   "authors": [
    [
     "Yanjie",
     "Fu"
    ],
    [
     "Meng",
     "Ge"
    ],
    [
     "Haoran",
     "Yin"
    ],
    [
     "Xinyuan",
     "Qian"
    ],
    [
     "Longbiao",
     "Wang"
    ],
    [
     "Gaoyan",
     "Zhang"
    ],
    [
     "Jianwu",
     "Dang"
    ]
   ],
   "title": "Iterative Sound Source Localization for Unknown Number of Sources",
   "original": "10525",
   "page_count": 5,
   "order": 183,
   "p1": 896,
   "pn": 900,
   "abstract": [
    "Sound source localization aims to seek the direction of arrival (DOA) of all sound sources from the observed multi-channel audio. For the practical problem of unknown number of sources, existing localization algorithms attempt to predict a likelihood-based coding (i.e., spatial spectrum) and employ a pre-determined threshold to detect the source number and corresponding DOA value. However, these threshold-based algorithms are not stable since they are limited by the careful choice of threshold. To address this problem, we propose an iterative sound source localization approach called ISSL, which can iteratively extract each source's DOA without threshold until the termination criterion is met. Unlike threshold-based algorithms, ISSL designs an active source detector network based on binary classifier to accept residual spatial spectrum and decide whether to stop the iteration. By doing so, our ISSL can deal with an arbitrary number of sources, even more than the number of sources seen during the training stage. The experimental results show that our ISSL achieves significant performance improvements in both DOA estimation and source number detection compared with the existing threshold-based algorithms."
   ],
   "doi": "10.21437/Interspeech.2022-10525"
  },
  "guzik22_interspeech": {
   "authors": [
    [
     "Mateusz",
     "Guzik"
    ],
    [
     "Konrad",
     "Kowalczyk"
    ]
   ],
   "title": "NTF of Spectral and Spatial Features for Tracking and Separation of Moving Sound Sources in Spherical Harmonic Domain",
   "original": "10526",
   "page_count": 5,
   "order": 53,
   "p1": 261,
   "pn": 265,
   "abstract": [
    "This paper presents a novel Non-negative Tensor Factorization (NTF) based approach to tracking and separation of moving sound sources, formulated in the Spherical Harmonic Domain (SHD). In particular, at first, we redefine an already existing Ambisonic NTF by introducing time-dependence into the Spatial Covariance Matrix (SCM) model. Next, we further extend the time-dependent SCM by incorporating a newly proposed NTF model of the spatial features, thereby introducing spatial components. To exploit the relationship between the positions of sound sources in adjacent time frames, resulting from the naturally occurring continuity of the movement itself, we impose local smoothness on time-dependent components of the spatial features. To this end, we propose a suitable posterior probability with Gibbs prior, and finally we derive the corresponding update rules. The experimental evaluation is based on first-order Ambisonic recordings of speech utterances and musical instruments in several scenarios with moving sources."
   ],
   "doi": "10.21437/Interspeech.2022-10526"
  },
  "li22t_interspeech": {
   "authors": [
    [
     "Yi",
     "Li"
    ],
    [
     "Xiaoming",
     "Jiang"
    ]
   ],
   "title": "Common and differential acoustic representation of interpersonal and tactile iconic perception of Mandarin vowels",
   "original": "10531",
   "page_count": 5,
   "order": 628,
   "p1": 3098,
   "pn": 3102,
   "abstract": [
    "Many have found that sound sequences can convey various domains of meaning, yet the mechanisms underlying such apparent associations between particular sound sequences and meanings in speech are underexplored. The study aimed to examine the common and different acoustic cues crucial for tactile and social sound symbolism based on the vowels in Mandarin. With the full spectrum of vowels tested, the study provided rich sound variations to demonstrate the role of multiple acoustic cues in encoding different meanings at the same time. Forty native Chinese listened to vowel sequences and rated on the interpersonal (e.g., encouraging-authoritative) and tactile (e.g., light-heavy) attributes based on these sounds. Acoustic analysis was performed on the vowel sequences and XGBoost algorithm was further applied to simulate tactile/social perception. The results showed that duration and pitch are two features crucial for representing both tactile and social sound symbolism. The second formant and nasality of vowels are specifically important for tactile representation while the third formant is fundamental for social representation. Machine learning models can achieve an above-chance performance using limited yet important acoustic features of vowels. Findings of this study have implications for multi-modal speech perception and iconic sound-meaning mappings in Mandarin."
   ],
   "doi": "10.21437/Interspeech.2022-10531"
  },
  "sarkar22_interspeech": {
   "authors": [
    [
     "Eklavya",
     "Sarkar"
    ],
    [
     "RaviShankar",
     "Prasad"
    ],
    [
     "Mathew Magimai",
     "Doss"
    ]
   ],
   "title": "Unsupervised Voice Activity Detection by Modeling Source and System Information using Zero Frequency Filtering",
   "original": "10535",
   "page_count": 5,
   "order": 937,
   "p1": 4626,
   "pn": 4630,
   "abstract": [
    "Voice activity detection (VAD) is an important pre-processing step for speech technology applications. The task consists of deriving segment boundaries of audio signals which contain voicing information. In recent years, it has been shown that voice source and vocal tract system information can be extracted using zero-frequency filtering (ZFF) without making any explicit model assumptions about the speech signal. This paper investigates the potential of zero-frequency filtering for jointly modeling voice source and vocal tract system information, and proposes two approaches for VAD. The first approach demarcates voiced regions using a composite signal composed of different zero-frequency filtered signals. The second approach feeds the composite signal as input to the rVAD algorithm. These approaches are compared with other supervised and unsupervised VAD methods in the literature, and are evaluated on the Aurora-2 database, across a range of SNRs (20 to -5 dB). Our studies show that the proposed ZFF-based methods perform comparable to state-of-art VAD methods and are more invariant to added degradation and different channel characteristics."
   ],
   "doi": "10.21437/Interspeech.2022-10535"
  },
  "furukawa22_interspeech": {
   "authors": [
    [
     "Kei",
     "Furukawa"
    ],
    [
     "Takeshi",
     "Kishiyama"
    ],
    [
     "Satoshi",
     "Nakamura"
    ]
   ],
   "title": "Applying Syntax–Prosody Mapping Hypothesis and Prosodic Well-Formedness Constraints to Neural Sequence-to-Sequence Speech Synthesis",
   "original": "10541",
   "page_count": 5,
   "order": 1066,
   "p1": 5258,
   "pn": 5262,
   "abstract": [
    "End-to-end text-to-speech synthesis (TTS), which generates speech sounds directly from strings of texts or phonemes, has improved the quality of speech synthesis over the conventional TTS. However, most previous studies have been evaluated based on subjective naturalness and have not objectively examined whether they can reproduce pitch patterns of phonological phenomena such as downstep, rhythmic boost, and initial lowering that reflect syntactic structures in Japanese. These phenomena can be linguistically explained by phonological constraints and the syntax–prosody mapping hypothesis (SPMH), which assumes projections from syntactic structures to phonological hierarchy. Although some experiments in psycholinguistics have verified the validity of the SPMH, it is crucial to investigate whether it can be implemented in TTS. To synthesize linguistic phenomena involving syntactic or phonological constraints, we propose a model using phonological symbols based on the SPMH and prosodic well-formedness constraints. Experimental results showed that the proposed method synthesized similar pitch patterns to those reported in linguistics experiments for the phenomena of initial lowering and rhythmic boost. The proposed model efficiently synthesizes phonological phenomena in the test data that were not explicitly included in the training data."
   ],
   "doi": "10.21437/Interspeech.2022-10541"
  },
  "eranovic22_interspeech": {
   "authors": [
    [
     "Jovan",
     "Eranovic"
    ],
    [
     "Daniel",
     "Pape"
    ],
    [
     "Magda",
     "Stroińska"
    ],
    [
     "Elisabet",
     "Service"
    ],
    [
     "Marijana",
     "Matkovski"
    ]
   ],
   "title": "Effects of Noise on Speech Perception and Spoken Word Comprehension",
   "original": "10543",
   "page_count": 5,
   "order": 629,
   "p1": 3103,
   "pn": 3107,
   "abstract": [
    "The aim of the study was to find out which of the three categories of noise acting as maskers (energetic: masking portions of the target speech with its energy; informational: both target and masker compete for the listener's attention; degraded: reverberated or filtered speech) is most detrimental to speech perception and spoken word comprehension. To that end, participants completed three tasks with and without added noise – listening span, listening comprehension, and shadowing – where shadowing is considered primarily a task relying on speech perception, with the other two tasks considered to rely on word comprehension and semantic inference. The study found informational masking to be most detrimental to speech perception, while energetic masking and sound degradation were most detrimental to spoken word comprehension. The results also imply that masking categories must be used with caution, since not all maskers belonging to one category had the same effect on performance."
   ],
   "doi": "10.21437/Interspeech.2022-10543"
  },
  "tao22b_interspeech": {
   "authors": [
    [
     "Dehua",
     "Tao"
    ],
    [
     "Tan",
     "Lee"
    ],
    [
     "Harold",
     "Chui"
    ],
    [
     "Sarah",
     "Luk"
    ]
   ],
   "title": "Hierarchical Attention Network for Evaluating Therapist Empathy in Counseling Session",
   "original": "10550",
   "page_count": 5,
   "order": 410,
   "p1": 2008,
   "pn": 2012,
   "abstract": [
    "Counseling typically takes the form of spoken conversation between a therapist and a client. The empathy level expressed by the therapist is considered to be an essential quality factor of counseling outcome. This paper proposes a hierarchical recurrent network combined with two-level attention mechanisms to determine the therapist's empathy level solely from the acoustic features of conversational speech in a counseling session. The experimental results show that the proposed model can achieve an accuracy of 72.1% in classifying the therapist's empathy level as being \"high\" or \"low\". It is found that the speech from both the therapist and the client are contributing to predicting the empathy level that is subjectively rated by an expert observer. By analyzing speaker turns assigned with high attention weights, it is observed that 2 to 6 consecutive turns should be considered together to provide useful clues for detecting empathy, and the observer tends to take the whole session into consideration when rating the therapist empathy, instead of relying on a few specific speaker turns."
   ],
   "doi": "10.21437/Interspeech.2022-10550"
  },
  "mahadeokar22_interspeech": {
   "authors": [
    [
     "Jay",
     "Mahadeokar"
    ],
    [
     "Yangyang",
     "Shi"
    ],
    [
     "Ke",
     "Li"
    ],
    [
     "Duc",
     "Le"
    ],
    [
     "Jiedan",
     "Zhu"
    ],
    [
     "Vikas",
     "Chandra"
    ],
    [
     "Ozlem",
     "Kalinli"
    ],
    [
     "Michael",
     "Seltzer"
    ]
   ],
   "title": "Streaming parallel transducer beam search with fast slow cascaded encoders",
   "original": "10551",
   "page_count": 5,
   "order": 425,
   "p1": 2083,
   "pn": 2087,
   "abstract": [
    "Streaming ASR with strict latency constraints is required in many speech recognition applications. In order to achieve the required latency, streaming ASR models sacrifice accuracy compared to non-streaming ASR models due to lack of future input context. Previous research has shown that streaming and non-streaming ASR for RNN Transducers can be unified by cascading causal and non-causal encoders. This work improves upon this cascaded encoders framework by leveraging two streaming non-causal encoders with variable input context sizes that can produce outputs at different audio intervals (e.g. fast and slow). We propose a novel parallel time-synchronous beam search algorithm for transducers that decodes from fast-slow encoders, where the slow encoder corrects the mistakes generated from the fast encoder. The proposed algorithm, achieves up to 20% WER reduction with a slight increase in token emission delays on the public Librispeech dataset and in-house datasets. We also explore techniques to reduce the computation by distributing processing between the fast and slow encoders. Lastly, we explore sharing the parameters in the fast encoder to reduce the memory footprint. This enables low latency processing on edge devices with low computation cost and a low memory footprint."
   ],
   "doi": "10.21437/Interspeech.2022-10551"
  },
  "delvaux22_interspeech": {
   "authors": [
    [
     "Veronique",
     "Delvaux"
    ],
    [
     "Audrey",
     "Lavallée"
    ],
    [
     "Fanny",
     "Degouis"
    ],
    [
     "Xavier",
     "Saloppe"
    ],
    [
     "Jean-Louis",
     "Nandrino"
    ],
    [
     "Thierry",
     "Pham"
    ]
   ],
   "title": "Telling self-defining memories: An acoustic study of natural emotional speech productions",
   "original": "10554",
   "page_count": 5,
   "order": 272,
   "p1": 1337,
   "pn": 1341,
   "abstract": [
    "Vocal cues in emotion encoding are rarely studied based on real-life, naturalistic emotional speech. In the present study, 20 speakers aged 25-35 were recorded while orally telling 5 successive self-defining autobiographic memories (SDM). By definition, this task is highly emotional, although emotional load and emotion regulation are expected to vary across SDM. Seven acoustic parameters were extracted: MeanF0, MedianFo, StandardDeviationF0, MinF0, MaxF0, Duration and SpeechRate. All SDM were manually transcribed, then their emotional lexicon was analysed using Emotaix. First, speech productions were examined in reference with SDM characteristics (specificity, integrative meaning and affective valence) as determined by 3 independent investigators. Results showed that overall the speech parameters did not change over the time course of the experiment, or as a function of integrative meaning. Specific memories were recounted at a higher speech rate and at greater length than non specific ones. SDM with positive affective valence were shorter and included less variability in fundamental frequency than negative SDM. Second, emotionally-charged (positive vs. negative; high vs. low arousal) vs. emotionally-neutral utterances as to Emotaix classification were compared over all SDM. Only a few significant effects were observed, which led us to discuss the role of emotion regulation in the SDM task."
   ],
   "doi": "10.21437/Interspeech.2022-10554"
  },
  "kanagawa22_interspeech": {
   "authors": [
    [
     "Hiroki",
     "Kanagawa"
    ],
    [
     "Yusuke",
     "Ijima"
    ],
    [
     "Hiroyuki",
     "Toda"
    ]
   ],
   "title": "Joint Modeling of Multi-Sample and Subband Signals for Fast Neural Vocoding on CPU",
   "original": "10556",
   "page_count": 5,
   "order": 330,
   "p1": 1626,
   "pn": 1630,
   "abstract": [
    "In this work, we propose a fast and high quality neural vocoder for CPU implementation. The main approaches to realize fast inference via an autoregressive model are 1) a subband-based vocoder and 2) multiple samples prediction. Our previous work demonstrated that the combination worked well up to two samples simultaneous generation without quality degradation. To further increase the number of simultaneous samples while maintaining quality, we focus on the existence of an association between subband signals and multiple samples. Our proposed vocoder jointly models these associations with a multivariate Gaussian. Experimentals show that our proposed four-sample vocoder is 1.47 times faster than the conventional two-sample equivalent. For both the acoustic features extracted from natural speech and those predicted by TTS, the proposed method realizes generation with up to four samples without any significant degradation in naturalness. This vocoder also matched the naturalness comparable of the two-sample conventional method."
   ],
   "doi": "10.21437/Interspeech.2022-10556"
  },
  "tsunoo22_interspeech": {
   "authors": [
    [
     "Emiru",
     "Tsunoo"
    ],
    [
     "Yosuke",
     "Kashiwagi"
    ],
    [
     "Chaitanya Prasad",
     "Narisetty"
    ],
    [
     "Shinji",
     "Watanabe"
    ]
   ],
   "title": "Residual Language Model for End-to-end Speech Recognition",
   "original": "10557",
   "page_count": 5,
   "order": 790,
   "p1": 3899,
   "pn": 3903,
   "abstract": [
    "End-to-end automatic speech recognition suffers from adaptation to unknown target domain speech despite being trained with a large amount of paired audio--text data. Recent studies estimate a linguistic bias of the model as the internal language model (LM). To effectively adapt to the target domain, the internal LM is subtracted from the posterior during inference and fused with an external target-domain LM. However, this fusion complicates the inference and the estimation of the internal LM may not always be accurate. In this paper, we propose a simple external LM fusion method for domain adaptation, which considers the internal LM estimation in its training. We directly model the residual factor of the external and internal LMs, namely the residual LM. To stably train the residual LM, we propose smoothing the estimated internal LM and optimizing it with a combination of cross-entropy and mean-squared-error losses, which consider the statistical behaviors of the internal LM in the target domain data. We experimentally confirmed that the proposed residual LM performs better than the internal LM estimation in most of the cross-domain and intra-domain scenarios."
   ],
   "doi": "10.21437/Interspeech.2022-10557"
  },
  "cui22c_interspeech": {
   "authors": [
    [
     "Fan",
     "Cui"
    ],
    [
     "Liyong",
     "Guo"
    ],
    [
     "Quandong",
     "Wang"
    ],
    [
     "Peng",
     "Gao"
    ],
    [
     "Yujun",
     "Wang"
    ]
   ],
   "title": "Exploring representation learning for small-footprint keyword spotting",
   "original": "10558",
   "page_count": 5,
   "order": 660,
   "p1": 3258,
   "pn": 3262,
   "abstract": [
    "In this paper, we investigate representation learning for low-resource keyword spotting (KWS). The main challenges of KWS are limited labeled data and limited available device resources. To address those challenges, we explore representation learning for KWS by self-supervised contrastive learning and self-training with pretrained model. First, local-global contrastive siamese networks (LGCSiam) are designed to learn similar utterance-level representations for similar audio samplers by proposed local-global contrastive loss without requiring ground-truth. Second, a self-supervised pretrained Wav2Vec 2.0 model is applied as a constraint module (WVC) to force the KWS model to learn frame-level acoustic representations. By the LGCSiam and WVC modules, the proposed small-footprint KWS model can be pretrained with unlabeled data. Experiments on speech commands dataset show that the self-training WVC module and the self-supervised LGCSiam module significantly improve accuracy, especially in the case of training on a small labeled dataset."
   ],
   "doi": "10.21437/Interspeech.2022-10558"
  },
  "audhkhasi22_interspeech": {
   "authors": [
    [
     "Kartik",
     "Audhkhasi"
    ],
    [
     "Yinghui",
     "Huang"
    ],
    [
     "Bhuvana",
     "Ramabhadran"
    ],
    [
     "Pedro J.",
     "Moreno"
    ]
   ],
   "title": "Analysis of Self-Attention Head Diversity for Conformer-based Automatic Speech Recognition",
   "original": "10560",
   "page_count": 5,
   "order": 209,
   "p1": 1026,
   "pn": 1030,
   "abstract": [
    "Attention layers are an integral part of modern end-to-end automatic speech recognition systems, for instance as part of the Transformer or Conformer architecture. Attention is typically multi-headed, where each head has an independent set of learned parameters and operates on the same input feature sequence. The output of multi-headed attention is a fusion of the outputs from the individual heads. We empirically analyze the diversity between representations produced by the different attention heads and demonstrate that the heads become highly correlated during the course of training. We investigate a few approaches to increasing attention head diversity, including using different attention mechanisms for each head and auxiliary training loss functions to promote head diversity. We show that introducing diversity-promoting auxiliary loss functions during training is a more effective approach, and obtain WER improvements of up to 6% relative on the Librispeech corpus. Finally, we draw a connection between the diversity of attention heads and the similarity of the gradients of head parameters."
   ],
   "doi": "10.21437/Interspeech.2022-10560"
  },
  "zhang22aa_interspeech": {
   "authors": [
    [
     "Sichen",
     "Zhang"
    ],
    [
     "Aijun",
     "Li"
    ]
   ],
   "title": "Acquisition of Two Consecutive Neutral Tones in Mandarin-Speaking Preschoolers: Phonological Representation and Phonetic Realization",
   "original": "10561",
   "page_count": 5,
   "order": 630,
   "p1": 3108,
   "pn": 3112,
   "abstract": [
    "Building on the description of acoustic patterns of neutral tone produced by Mandarin-speaking adults in literature and the corresponding proposals of phonological-phonetic mapping mechanism, the present study examined the production of two consecutive neutral tones by Mandarin-speaking preschool children. Data was extracted from spontaneous conversation between preschoolers and their caretakers in CASS-CHILD, a child language corpus of Mandarin-speaking children. Results show the developmental process of consecutive tones production in terms of F0 contour, pitch range, and duration. Our findings are in accord with established research on the early mastery of tone features and synchronous development among lexical tone categories in prosody acquisition. Furthermore, it is revealed from this study that preschoolers perform well on acquiring phonological representation of neutral tone and sandhi. Even so, they may have difficulties in accurately realizing consecutive neutral tones in the following conditions: 1) when the preceding full syllable is Tone 2 or Tone 3, which in themselves trouble young speakers; 2) when word prosody interacts with phrase-level prosody, for example, domain-final lengthening at intonational phrase boundaries. In addition, acoustic parameters, pitch and duration in our case, interfere one another in preschoolers' off-standard realization of consecutive neutral tones."
   ],
   "doi": "10.21437/Interspeech.2022-10561"
  },
  "mai22_interspeech": {
   "authors": [
    [
     "Long",
     "Mai"
    ],
    [
     "Julie",
     "Carson-Berndsen"
    ]
   ],
   "title": "Unsupervised domain adaptation for speech recognition with unsupervised error correction",
   "original": "10565",
   "page_count": 5,
   "order": 1036,
   "p1": 5120,
   "pn": 5124,
   "abstract": [
    "The transcription quality of automatic speech recognition (ASR) systems degrades significantly when transcribing audios coming from unseen domains. We propose an unsupervised error correction method for unsupervised ASR domain adaption, aiming to recover transcription errors caused by domain mismatch. Unlike existing correction methods that rely on transcribed audios for training, our approach requires only unlabeled data of the target domains in which a pseudo-labeling technique is applied to generate correction training samples. To reduce over-fitting to the pseudo data, we also propose an encoder-decoder correction model that can take into account additional information such as dialogue context and acoustic features. Experiment results show that our method obtains a significant word error rate (WER) reduction over non-adapted ASR systems. The correction model can also be applied on top of other adaptation approaches to bring an additional improvement of 10% relatively."
   ],
   "doi": "10.21437/Interspeech.2022-10565"
  },
  "liu22u_interspeech": {
   "authors": [
    [
     "Danni",
     "Liu"
    ],
    [
     "Changhan",
     "Wang"
    ],
    [
     "Hongyu",
     "Gong"
    ],
    [
     "Xutai",
     "Ma"
    ],
    [
     "Yun",
     "Tang"
    ],
    [
     "Juan",
     "Pino"
    ]
   ],
   "title": "From Start to Finish: Latency Reduction Strategies for Incremental Speech Synthesis in Simultaneous Speech-to-Speech Translation",
   "original": "10568",
   "page_count": 5,
   "order": 359,
   "p1": 1771,
   "pn": 1775,
   "abstract": [
    "Speech-to-speech translation (S2ST) converts input speech to speech in another language. A challenge of delivering S2ST in real time is the accumulated delay between the translation and speech synthesis modules. While recently incremental text-to-speech (iTTS) models have shown large quality improvements, they typically require additional future text inputs to reach optimal performance. In this work, we minimize the initial waiting time of iTTS by adapting the upstream speech translator to generate high-quality pseudo lookahead for the speech synthesizer. After mitigating the initial delay, we demonstrate that the duration of synthesized speech also plays a crucial role on latency. We formalize this as a latency metric and the present a simple yet effective duration-scaling approach for latency reduction. Our approaches consistently reduce latency by 0.2-0.5 second without sacrificing speech translation quality."
   ],
   "doi": "10.21437/Interspeech.2022-10568"
  },
  "rattcliffe22_interspeech": {
   "authors": [
    [
     "Dino",
     "Rattcliffe"
    ],
    [
     "You",
     "Wang"
    ],
    [
     "Alex",
     "Mansbridge"
    ],
    [
     "Penny",
     "Karanasou"
    ],
    [
     "Alexis",
     "Moinet"
    ],
    [
     "Marius",
     "Cotescu"
    ]
   ],
   "title": "Cross-lingual Style Transfer with Conditional Prior VAE and Style Loss",
   "original": "10572",
   "page_count": 5,
   "order": 929,
   "p1": 4586,
   "pn": 4590,
   "abstract": [
    "In this work we improve the style representation for cross-lingual style transfer. Specifically, we improve the Spanish representation across four styles, Newscaster, DJ, Excited, and Disappointed, whilst maintaining a single speaker identity for which we only have English samples. This is achieved using Learned Conditional Prior VAE (LCPVAE), a hierarchical Variational Auto Encoder (VAE) approach. A secondary VAE is introduced, conditioned on one-hot encoded style information, resulting in a structured embedding space of the primary VAE. This places utterances of the same style in similar locations of the latent space irrespective of language. We also experiment with extending this model by incorporating a style loss. We perform subjective evaluations for style similarity using native Spanish speakers, and show an average relative improvement over the baseline of 3.5% with statistical significance (p-value<0.01) across all four styles. Interestingly the more expressive styles achieve a higher relative improvement of 4.4% compared to 2.6% for styles that are closer to neutral speech. We also demonstrate that this is whilst maintaining speaker similarity and in-lingual performance in all styles. Accent performance is maintained in three out of four styles with the exception of Excited, while naturalness performance is maintained in News and Disappointed styles."
   ],
   "doi": "10.21437/Interspeech.2022-10572"
  },
  "zheng22c_interspeech": {
   "authors": [
    [
     "Huahuan",
     "Zheng"
    ],
    [
     "keyu",
     "An"
    ],
    [
     "Zhijian",
     "Ou"
    ],
    [
     "Chen",
     "Huang"
    ],
    [
     "Ke",
     "Ding"
    ],
    [
     "Guanglu",
     "Wan"
    ]
   ],
   "title": "An Empirical Study of Language Model Integration for Transducer based Speech Recognition",
   "original": "10576",
   "page_count": 5,
   "order": 791,
   "p1": 3904,
   "pn": 3908,
   "abstract": [
    "Utilizing text-only data with an external language model (ELM) in end-to-end RNN-Transducer (RNN-T) for speech recognition is challenging. Recently, a class of methods such as density ratio (DR) and internal language model estimation (ILME) have been developed, outperforming the classic shallow fusion (SF) method. The basic idea behind these methods is that RNN-T posterior should first subtract the implicitly learned internal language model (ILM) prior, in order to integrate the ELM. While recent studies suggest that RNN-T only learns some low-order language model information, the DR method uses a well-trained neural language model with full context, which may be inappropriate for the estimation of ILM and deteriorate the integration performance. Based on the DR method, we propose a low-order density ratio method (LODR) by replacing the estimation with a low-order weak language model. Extensive empirical experiments are conducted on both in-domain and cross-domain scenarios on English LibriSpeech & Tedlium-2 and Chinese WenetSpeech & AISHELL-1 datasets. It is shown that LODR consistently outperforms SF in all tasks, while performing generally close to ILME and better than DR in most tests."
   ],
   "doi": "10.21437/Interspeech.2022-10576"
  },
  "roy22_interspeech": {
   "authors": [
    [
     "Anwesha",
     "Roy"
    ],
    [
     "Varun",
     "Belagali"
    ],
    [
     "Prasanta",
     "Ghosh"
    ]
   ],
   "title": "Air tissue boundary segmentation using regional loss in real-time Magnetic Resonance Imaging video for speech production",
   "original": "10579",
   "page_count": 5,
   "order": 631,
   "p1": 3113,
   "pn": 3117,
   "abstract": [
    "The SegNet model has been shown to provide the best performance in air-tissue boundary (ATB) segmentation in real-time Magnetic Resonance Imaging (rtMRI) videos in seen subject conditions. The SegNet model uses overall binary cross entropy as the loss function. However, such a global loss function does not give enough emphasis on regions which are more prone to errors. In this work, together with global loss, we explore the use of regional loss functions which focus on areas of the contours which have been analysed as error prone in the past. Evaluation is done using global Dynamic Time Warping (DTW) distance as well as regional metrics. The regional metrics used are EVEL and VELrDTW for contour1, and ETB and TBrDTW for contour2. We show that using such combinations of regional and global losses improves the regional, as well as global, evaluation metrics. For the best combination of losses, the two regional metrics show an improvement of 37.2% and 25.3% for contour1 and 23.9% and 28.4% for contour2, over a baseline model which uses only global loss. Global DTW distance, on the other hand, improves by 11.2% for contour1 and 5.6% for contour2."
   ],
   "doi": "10.21437/Interspeech.2022-10579"
  },
  "ide22_interspeech": {
   "authors": [
    [
     "Yuta",
     "Ide"
    ],
    [
     "Susumu",
     "Saito"
    ],
    [
     "Teppei",
     "Nakano"
    ],
    [
     "Tetsuji",
     "Ogawa"
    ]
   ],
   "title": "Can Humans Correct Errors From System? Investigating Error Tendencies in Speaker Identification Using Crowdsourcing",
   "original": "10580",
   "page_count": 5,
   "order": 1032,
   "p1": 5100,
   "pn": 5104,
   "abstract": [
    "An attempt was made to clarify the effectiveness of crowdsourcing on reducing errors in automatic speaker identification (ASID). It is possible to efficiently reduce errors by manually revalidating the unreliable results given by ASID systems. Ideally, errors should be corrected appropriately, and correct answers should not be miscorrected. In addition, a low false acceptance rate is desirable in authentication, but a high false rejection rate should be avoided from a usability viewpoint. It, however, is not certain that humans can achieve such an ideal SID, and in the case of crowdsourcing, the existence of malicious workers cannot be ignored. This study, therefore, investigates whether manual verification of error-prone inputs by crowd workers can reduce ASID errors and whether the resulting corrections are ideal. Experimental investigations on Amazon Mechanical Turk, in which 426 qualified workers identified 256 speech pairs from VoxCeleb data, demonstrated that crowdsourced verification can significantly reduce the number of false acceptances without increasing the number of false rejections compared to the results from the ASID system."
   ],
   "doi": "10.21437/Interspeech.2022-10580"
  },
  "parry22_interspeech": {
   "authors": [
    [
     "Jack",
     "Parry"
    ],
    [
     "Eric",
     "DeMattos"
    ],
    [
     "Anita",
     "Klementiev"
    ],
    [
     "Axel",
     "Ind"
    ],
    [
     "Daniela",
     "Morse-Kopp"
    ],
    [
     "Georgia",
     "Clarke"
    ],
    [
     "Dimitri",
     "Palaz"
    ]
   ],
   "title": "Speech Emotion Recognition in the Wild using Multi-task and Adversarial Learning",
   "original": "10581",
   "page_count": 5,
   "order": 236,
   "p1": 1158,
   "pn": 1162,
   "abstract": [
    "Speech Emotion Recognition (SER) is an important and challenging task, especially when deploying systems in the wild i.e. on unseen data, as they tend to generalise poorly. One promising approach to improve the generalisation capabilities of SER systems is to incorporate attributes of the speech signal, such as corpus or speaker information, which can be a source of overfitting or confusion for the model. In this paper, we investigate using multi-task learning, where attribute prediction is given as an auxiliary task to the model, and adversarial learning, where the model is explicitly trained to incorrectly predict attributes. We compare two adversarial learning approaches: gradient reversal and an adversarial discriminator. We evaluate these approaches in a cross-corpus training setting using two unseen corpora as test sets. We use four attributes -- corpus, speaker, gender and language -- and evaluate all possible combinations of these attributes. We show that both multi-task learning and adversarial learning improve SER performance in the wild, with the gradient reversal approach being the most consistent across attributes and test sets."
   ],
   "doi": "10.21437/Interspeech.2022-10581"
  },
  "rathod22_interspeech": {
   "authors": [
    [
     "Jash",
     "Rathod"
    ],
    [
     "Nauman",
     "Dawalatabad"
    ],
    [
     "SHATRUGHAN",
     "SINGH"
    ],
    [
     "Dhananjaya",
     "Gowda"
    ]
   ],
   "title": "Multi-stage Progressive Compression of Conformer Transducer for On-device Speech Recognition",
   "original": "10582",
   "page_count": 5,
   "order": 343,
   "p1": 1691,
   "pn": 1695,
   "abstract": [
    "The smaller memory bandwidth in smart devices prompts development of smaller Automatic Speech Recognition (ASR) models. To obtain a smaller model, one can employ the model compression techniques. Knowledge distillation (KD) is a popular model compression approach that has shown to achieve smaller model size with relatively lesser degradation in the model performance. In this approach, knowledge is distilled from a trained large size teacher model to a smaller size student model. Also, the transducer based models have recently shown to perform well for on-device streaming ASR task, while the conformer models are efficient in handling long term dependencies. Hence in this work we employ a streaming transducer architecture with conformer as the encoder. We propose a multi-stage progressive approach to compress the conformer transducer model using KD. We progressively update our teacher model with the distilled student model in a multi-stage setup. On standard LibriSpeech dataset, our experimental results have successfully achieved compression rates greater than 60% without significant degradation in the performance compared to the larger teacher model."
   ],
   "doi": "10.21437/Interspeech.2022-10582"
  },
  "vyas22_interspeech": {
   "authors": [
    [
     "Apoorv",
     "Vyas"
    ],
    [
     "Wei-Ning",
     "Hsu"
    ],
    [
     "Michael",
     "Auli"
    ],
    [
     "Alexei",
     "Baevski"
    ]
   ],
   "title": "On-demand compute reduction with stochastic wav2vec 2.0",
   "original": "10584",
   "page_count": 5,
   "order": 618,
   "p1": 3048,
   "pn": 3052,
   "abstract": [
    "Squeeze and Efficient Wav2vec (SEW) is a recently proposed architecture that squeezes the input to the transformer encoder for compute efficient pre-training and inference with wav2vec 2.0 (W2V2) models. In this work, we propose stochastic compression for on-demand compute reduction for W2V2 models. As opposed to using a fixed squeeze factor, we sample it uniformly during training. We further introduce query and key-value pooling mechanisms that can be applied to each transformer layer for further compression. Our results for models pre-trained on 960h Librispeech dataset and fine-tuned on 10h of transcribed data show that using the same stochastic model, we get a smooth trade-off between word error rate (WER) and inference time with only marginal WER degradation compared to the W2V2 and SEW models trained for a specific setting. We further show that we can fine-tune the same stochastically pre-trained model to a specific configuration to recover the WER difference resulting in significant computational savings on pre-training models from scratch."
   ],
   "doi": "10.21437/Interspeech.2022-10584"
  },
  "zhou22f_interspeech": {
   "authors": [
    [
     "Shaohuan",
     "Zhou"
    ],
    [
     "Shun",
     "Lei"
    ],
    [
     "Weiya",
     "You"
    ],
    [
     "Deyi",
     "Tuo"
    ],
    [
     "Yuren",
     "You"
    ],
    [
     "Zhiyong",
     "Wu"
    ],
    [
     "Shiyin",
     "Kang"
    ],
    [
     "Helen",
     "Meng"
    ]
   ],
   "title": "Towards Improving the Expressiveness of Singing Voice Synthesis with BERT Derived Semantic Information",
   "original": "10585",
   "page_count": 5,
   "order": 870,
   "p1": 4292,
   "pn": 4296,
   "abstract": [
    "This paper presents an end-to-end high-quality singing voice synthesis (SVS) system that uses bidirectional encoder representation from Transformers (BERT) derived semantic embeddings to improve the expressiveness of the synthesized singing voice. Based on the main architecture of recently proposed VISinger, we put forward several specific designs for expressive singing voice synthesis. First, different from the previous SVS models, we use text representation of lyrics extracted from pre-trained BERT as additional input to the model. The representation contains information about semantics of the lyrics, which could help SVS system produce more expressive and natural voice. Second, we further introduce an energy predictor to stabilize the synthesized voice and model the wider range of energy variations that also contribute to the expressiveness of singing voice. Last but not the least, to attenuate the off-key issues, the pitch predictor is re-designed to predict the real to note pitch ratio. Both objective and subjective experimental results prove that the proposed SVS system can produce singing voice with higher-quality outperforming VISinger."
   ],
   "doi": "10.21437/Interspeech.2022-10585"
  },
  "pierre22_interspeech": {
   "authors": [
    [
     "Champion",
     "Pierre"
    ],
    [
     "Anthony",
     "Larcher"
    ],
    [
     "Denis",
     "Jouvet"
    ]
   ],
   "title": "Are disentangled representations all you need to build speaker anonymization systems?",
   "original": "10586",
   "page_count": 5,
   "order": 567,
   "p1": 2793,
   "pn": 2797,
   "abstract": [
    "Speech signals contain a lot of sensitive information, such as the speaker's identity, which raises privacy concerns when speech data get collected. Speaker anonymization aims to transform a speech signal to remove the source speaker's identity while leaving the spoken content unchanged. Current methods perform the transformation by relying on content/speaker disentanglement and voice conversion. Usually, an acoustic model from an automatic speech recognition system extracts the content representation while an x-vector system extracts the speaker representation. Prior work has shown that the extracted features are not perfectly disentangled. This paper tackles how to improve features disentanglement, and thus the converted anonymized speech. We propose enhancing the disentanglement by removing speaker information from the acoustic model using vector quantization. Evaluation done using the VoicePrivacy 2022 toolkit showed that vector quantization helps conceal the original speaker identity while maintaining utility for speech recognition."
   ],
   "doi": "10.21437/Interspeech.2022-10586"
  },
  "yang22s_interspeech": {
   "authors": [
    [
     "Zijian",
     "Yang"
    ],
    [
     "Yingbo",
     "Gao"
    ],
    [
     "Alexander",
     "Gerstenberger"
    ],
    [
     "Jintao",
     "Jiang"
    ],
    [
     "Ralf",
     "Schlüter"
    ],
    [
     "Hermann",
     "Ney"
    ]
   ],
   "title": "Self-Normalized Importance Sampling for Neural Language Modeling",
   "original": "10588",
   "page_count": 5,
   "order": 792,
   "p1": 3909,
   "pn": 3913,
   "abstract": [
    "To mitigate the problem of having to traverse over the full vocabulary in the softmax normalization of a neural language model, sampling-based training criteria are proposed and investigated in the context of large vocabulary word-based neural language models. These training criteria typically enjoy the benefit of faster training and testing, at a cost of slightly degraded performance in terms of perplexity and almost no visible drop in word error rate. While noise contrastive estimation is one of the most popular choices, recently we show that other sampling-based criteria can also perform well, as long as an extra correction step is done, where the intended class posterior probability is recovered from the raw model outputs. In this work, we propose self-normalized importance sampling. Compared to our previous work, the criteria considered in this work are self-normalized and there is no need to further conduct a correction step. Through self-normalized language model training as well as lattice rescoring experiments, we show that our proposed self-normalized importance sampling is competitive in both research-oriented and production-oriented automatic speech recognition tasks."
   ],
   "doi": "10.21437/Interspeech.2022-10588"
  },
  "fougeron22_interspeech": {
   "authors": [
    [
     "Cécile",
     "Fougeron"
    ],
    [
     "Nicolas",
     "Audibert"
    ],
    [
     "Ina",
     "Kodrasi"
    ],
    [
     "Parvaneh",
     "Janbakhshi"
    ],
    [
     "Michaela",
     "Pernon"
    ],
    [
     "Nathalie",
     "Leveque"
    ],
    [
     "Stephanie",
     "Borel"
    ],
    [
     "Marina",
     "Laganaro"
    ],
    [
     "Herve",
     "Bourlard"
    ],
    [
     "Frederic",
     "Assal"
    ]
   ],
   "title": "Comparison of 5 methods for the evaluation of intelligibility in mild to moderate French dysarthric speech",
   "original": "10590",
   "page_count": 5,
   "order": 446,
   "p1": 2188,
   "pn": 2192,
   "abstract": [
    "Altered quality of the phonetic-acoustic information in the speech signal in the case of motor speech disorders may reduce its intelligibility. Monitoring intelligibility is part of the standard clinical assessment of patients. It is also a valuable tool to index the evolution of the speech disorder. However, measuring intelligibility raises methodological debates concerning: the type of linguistic material on which the assessment is based (non-words, words, continuous speech), the evaluation protocol and type of scores (scale-based rating, transcription or recognition tests), and the advantages and disadvantages of listener vs. automatic-based approaches (subjective vs. objective, expertise level, types of models used). In this paper, the intelligibility of the speech of 32 French patients presenting mild to moderate dysarthria and 17 elderly speakers is assessed with five different methods: impressionistic clinician judgment on continuous speech, number of words recognized in an interactive face-to-face setting and in an on-line testing of the same material by 75 judges, automatic feature-based and automatic speech recognition-based methods (both on short sentences). The implications of the different methods for clinical practice are discussed."
   ],
   "doi": "10.21437/Interspeech.2022-10590"
  },
  "li22u_interspeech": {
   "authors": [
    [
     "Yan",
     "Li"
    ],
    [
     "Ying",
     "Chen"
    ],
    [
     "Xinya",
     "Zhang"
    ],
    [
     "Yanyang",
     "Chen"
    ],
    [
     "Jiazheng",
     "Wang"
    ]
   ],
   "title": "Effects of Language Contact on Vowel Nasalization in Wenzhou and Rugao Dialects",
   "original": "10591",
   "page_count": 5,
   "order": 1067,
   "p1": 5263,
   "pn": 5267,
   "abstract": [
    "This study examined the nasality of pre-nasal vowels /a, o, e/ in Wenzhou—a dialect of Wu contacting with Mandarin and Min, and pre-nasal vowels /ɔ, ə/ and nasal vowels /a, e, ʊ, i/ in Rugao—a dialect of Mandarin contacting with Wu. Three age groups of native speakers were recruited as the talkers for each dialect. The acoustic parameter A1-P0, i.e., the differential between A1 (the amplitude of the first formant) and P0 (the extra peak below the first formant) was measured to evaluate the nasality for non-high vowels, and A1-P1, i.e., the differential between A1 and P1 (the extra peak between the first two formants) for high vowels. The statistical results reveal that the degree of nasalization varied across age group, vowel type, and point of vowel duration. Younger speakers produced overall more nasalization than the middle-aged and older speakers in Wenzhou /o/ and Rugao /ə, e, ʊ, i/. The midpoint of vowel showed less nasalization than the start and end points in non-high vowels in both Wenzhou and Rugao dialects. These results suggest different effects of language contact on sound change in Chinese dialects."
   ],
   "doi": "10.21437/Interspeech.2022-10591"
  },
  "li22v_interspeech": {
   "authors": [
    [
     "Jinchao",
     "Li"
    ],
    [
     "Shuai",
     "Wang"
    ],
    [
     "Yang",
     "Chao"
    ],
    [
     "Xunying",
     "Liu"
    ],
    [
     "Helen",
     "Meng"
    ]
   ],
   "title": "Context-aware Multimodal Fusion for Emotion Recognition",
   "original": "10592",
   "page_count": 5,
   "order": 411,
   "p1": 2013,
   "pn": 2017,
   "abstract": [
    "Automatic emotion recognition (AER) is an inherently complex multimodal task that aims to automatically determine the emotional state of a given expression. Recent works have witnessed the benefits of upstream pretrained models in both audio and textual modalities for the AER task. However, efforts are still needed to effectively integrate features across multiple modalities, devoting due considerations to granularity mismatch and asynchrony in time steps. In this work, we first validate the effectiveness of the upstream models in a unimodal setup and empirically find that partial fine-tuning of the pretrained model in the feature space can significantly boost performance. Moreover, we take the context of the current sentence to model a more accurate emotional state. Based on the unimodal setups, we further propose several multimodal fusion methods to combine high-level features from the audio and text modalities. Experiments are carried out on the IEMOCAP dataset in a 4-category classification problem and compared with state-of-the-art methods in recent literature. Results show that the proposed models gave a superior performance of up to 84.45% and 80.36% weighted accuracy scores respectively in Session 5 and 5-fold cross-validation settings."
   ],
   "doi": "10.21437/Interspeech.2022-10592"
  },
  "kim22m_interspeech": {
   "authors": [
    [
     "Donghyeon",
     "Kim"
    ],
    [
     "Bowon",
     "Lee"
    ]
   ],
   "title": "Phase Vocoder For Time Stretch Based On Center Frequency Estimation",
   "original": "10593",
   "page_count": 5,
   "order": 895,
   "p1": 4416,
   "pn": 4420,
   "abstract": [
    "In this paper, we propose a phase correction algorithm for phase vocoders based on center frequency estimation. Phase shift estimation in the phase vocoder for time stretch is one of the important problems, for which there exists no consistent solution. In many cases, a simple method consisting of STFT, phase correction, and ISTFT cannot produce high-quality results, so methods such as using multiple FFT sizes and sine wave synthesis through oscillator bank are proposed, which unfortunately is computationally expensive. The proposed method uses a phase correction algorithm that can generate high-quality results while enabling a high degree of parallelism for faster execution. It uses a simple structure that includes only STFT, phase correction, and ISTFT as essential elements. The phase correction algorithm uses peak phase-locking and a method to find an appropriate dominant peak frequency, and it requires only a single sized FFT and has the advantage that it can be easily applied to various applications thanks to its structural similarity to the classical phase vocoder."
   ],
   "doi": "10.21437/Interspeech.2022-10593"
  },
  "li22w_interspeech": {
   "authors": [
    [
     "Yuntao",
     "Li"
    ],
    [
     "Hanchu",
     "Zhang"
    ],
    [
     "Yutian",
     "Li"
    ],
    [
     "Sirui",
     "Wang"
    ],
    [
     "Wei",
     "Wu"
    ],
    [
     "Yan",
     "Zhang"
    ]
   ],
   "title": "Pay More Attention to History: A Context Modeling Strategy for Conversational Text-to-SQL",
   "original": "10596",
   "page_count": 5,
   "order": 552,
   "p1": 2718,
   "pn": 2722,
   "abstract": [
    "Conversational text-to-SQL aims at converting multi-turn natural language queries into their corresponding SQL (Structured Query Language) representations. One of the most intractable problems of conversational text-to-SQL is modelling the semantics of multi-turn queries and gathering the proper information required for the current query. This paper shows that explicitly modelling the semantic changes by adding each turn and the summarization of the whole context can bring better performance on converting conversational queries into SQLs. In particular, we propose two conversational modelling tasks in both turn grain and conversation grain. These two tasks simply work as auxiliary training tasks to help with multi-turn conversational semantic parsing. We conducted empirical studies and achieved new state-of-the-art results on the large-scale open-domain conversational text-to-SQL dataset. The results demonstrate that the proposed mechanism significantly improves the performance of multi-turn semantic parsing."
   ],
   "doi": "10.21437/Interspeech.2022-10596"
  },
  "yi22b_interspeech": {
   "authors": [
    [
     "Gaoxiong",
     "Yi"
    ],
    [
     "Wei",
     "Xiao"
    ],
    [
     "Yiming",
     "Xiao"
    ],
    [
     "Babak",
     "Naderi"
    ],
    [
     "Sebastian",
     "Möller"
    ],
    [
     "Wafaa",
     "Wardah"
    ],
    [
     "Gabriel",
     "Mittag"
    ],
    [
     "Ross",
     "Culter"
    ],
    [
     "Zhuohuang",
     "Zhang"
    ],
    [
     "Donald S.",
     "Williamson"
    ],
    [
     "Fei",
     "Chen"
    ],
    [
     "Fuzheng",
     "Yang"
    ],
    [
     "Shidong",
     "Shang"
    ]
   ],
   "title": "ConferencingSpeech 2022 Challenge: Non-intrusive Objective Speech Quality Assessment (NISQA) Challenge for Online Conferencing Applications",
   "original": "10597",
   "page_count": 5,
   "order": 670,
   "p1": 3308,
   "pn": 3312,
   "abstract": [
    "With the advances in speech communication systems such as online conferencing applications, we can seamlessly work with people regardless of where they are. However, during online meetings, the speech quality can be significantly affected by background noise, reverberation, packet loss, network jitter, etc. Because of its nature, the speech quality is traditionally assessed in subjective tests in laboratories and lately also in crowdsourcing following the international standards from ITU-T Rec. P.800 series. However, those approaches are costly and cannot be applied to customer data. Therefore, an effective objective assessment approach is needed to evaluate or monitor the speech quality of the ongoing conversation. The ConferencingSpeech 2022 challenge targets the non-intrusive deep neural network models for the speech quality assessment task. We open-sourced a training corpus with more than 86K speech clips in different languages, with a wide range of synthesized and live degradations and their corresponding subjective quality scores through crowdsourcing. 18 teams submitted their models for evaluation in this challenge. The blind test sets included about 4300 clips from wide ranges of degradations. This paper describes the challenge, the datasets, and the evaluation methods and reports the final results."
   ],
   "doi": "10.21437/Interspeech.2022-10597"
  },
  "hu22f_interspeech": {
   "authors": [
    [
     "Shichao",
     "Hu"
    ],
    [
     "Bin",
     "Zhang"
    ],
    [
     "Jinhong",
     "Lu"
    ],
    [
     "Yiliang",
     "Jiang"
    ],
    [
     "Wucheng",
     "Wang"
    ],
    [
     "Lingcheng",
     "Kong"
    ],
    [
     "Weifeng",
     "Zhao"
    ],
    [
     "Tao",
     "Jiang"
    ]
   ],
   "title": "WideResNet with Joint Representation Learning and Data Augmentation for Cover Song Identification",
   "original": "10600",
   "page_count": 5,
   "order": 849,
   "p1": 4187,
   "pn": 4191,
   "abstract": [
    "Cover song identification~(CSI) has been a challenging task and an import topic in music information retrieval~(MIR) community. In recent years, CSI problems have been extensively studied based on deep learning methods. In this paper, we propose a novel framework for CSI based on a joint representation learning method inspired by multi-task learning. In specific, we propose a joint learning strategy which combines classification and metric learning for optimizing the cover song model based on WideResNet, called LyraC-Net. Classification objective learns separable embeddings from different classes, while metric learning optimizes embedding similarity by decreasing the inter-class distance and increasing the intra-classs separability. This joint optimization strategy is expected to learn a more robust cover song representation than methods with single training objectives. For the metric learning, prototypical network is introduced to stabilize and accelerate the training process, together with triplet loss. Furthermore, we introduce SpecAugment, a popular augmentation method in speech recognition, to further improve the performance. Experiment results show that our proposed method achieves promising results and outperforms other recent CSI methods in the evaluations."
   ],
   "doi": "10.21437/Interspeech.2022-10600"
  },
  "huang22i_interspeech": {
   "authors": [
    [
     "Wen Chin",
     "Huang"
    ],
    [
     "Dejan",
     "Markovic"
    ],
    [
     "Alexander",
     "Richard"
    ],
    [
     "Israel Dejene",
     "Gebru"
    ],
    [
     "Anjali",
     "Menon"
    ]
   ],
   "title": "End-to-End Binaural Speech Synthesis",
   "original": "10603",
   "page_count": 5,
   "order": 248,
   "p1": 1218,
   "pn": 1222,
   "abstract": [
    "In this work, we present an end-to-end binaural speech synthesis system that combines a low-bitrate audio codec with a powerful binaural decoder that is capable of accurate speech binauralization while faithfully reconstructing environmental factors like ambient noise or reverb. The network is a modified vector-quantized variational autoencoder, trained with several carefully designed objectives, including an adversarial loss. We evaluate the proposed system on an internal binaural dataset with objective metrics and a perceptual study. Results show that the proposed approach matches the ground truth data more closely than previous methods. In particular, we demonstrate the capability of the adversarial loss in capturing environment effects needed to create an authentic auditory scene."
   ],
   "doi": "10.21437/Interspeech.2022-10603"
  },
  "rakotomalala22_interspeech": {
   "authors": [
    [
     "Tsiky",
     "Rakotomalala"
    ],
    [
     "Pierre",
     "Baraduc"
    ],
    [
     "Pascal",
     "Perrier"
    ]
   ],
   "title": "Trajectories predicted by optimal speech motor control using LSTM networks",
   "original": "10604",
   "page_count": 5,
   "order": 127,
   "p1": 630,
   "pn": 634,
   "abstract": [
    "The question of optimality and its role in trajectory formation is at the core of important debates in motor control research. We present the first speech control model that associates Optimal Feedback Control (OFC) for planning and execution of movements with a biomechanical model of the vocal tract. Simulated trajectories in the VCV sequences are compared with trajectories generated using the GEPPETO model that drives the same 2D biomechanical model; in GEPPETO, the scope of optimality is limited to movement planning and to phoneme-related target motor commands. In our OFC model commands are estimated via the minimisation of a cost that combines neuromuscular effort, and a penalty on accuracy of the auditory patterns reached for the phonemes. The biomechanics of the plant are implemented by an LSTM trained on simulations of a finite element model of the tongue. The comparison of the OFC model with GEPPETO relies on the time variation of the motor commands, the shape of the articulatory trajectories, and on auditory trajectories in the F1-F2 planes."
   ],
   "doi": "10.21437/Interspeech.2022-10604"
  },
  "tanveer22_interspeech": {
   "authors": [
    [
     "Md Iftekhar",
     "Tanveer"
    ],
    [
     "Diego",
     "Casabuena"
    ],
    [
     "Jussi",
     "Karlgren"
    ],
    [
     "Rosie",
     "Jones"
    ]
   ],
   "title": "Unsupervised Speaker Diarization that is Agnostic to Language, Overlap-Aware, and Tuning Free",
   "original": "10605",
   "page_count": 5,
   "order": 301,
   "p1": 1481,
   "pn": 1485,
   "abstract": [
    "Podcasts are conversational in nature and speaker changes are frequent---requiring speaker diarization for content understanding. We propose an unsupervised technique for speaker diarization without relying on language-specific components. The algorithm is overlap-aware and does not require information about the number of speakers. Our approach shows 79% improvement on purity scores (34% on F-score) against the Google Cloud Platform solution on podcast data."
   ],
   "doi": "10.21437/Interspeech.2022-10605"
  },
  "pandey22b_interspeech": {
   "authors": [
    [
     "Ayushi",
     "Pandey"
    ],
    [
     "Sébastien",
     "Le Maguer"
    ],
    [
     "Julie",
     "Carson-Berndsen"
    ],
    [
     "Naomi",
     "Harte"
    ]
   ],
   "title": "Production characteristics of obstruents in WaveNet and older TTS systems",
   "original": "10606",
   "page_count": 5,
   "order": 483,
   "p1": 2373,
   "pn": 2377,
   "abstract": [
    "Segmental properties of Text-to-Speech (TTS) synthesizers have been studied for their influence on various perceived attributes of synthesized speech. However, they have received very limited attention for modern, neural vocoder-based TTS. In this paper, we compare segmental properties of WaveNET vocoder voices with a natural voice, and the best-performing non-neural synthesizers of the 2013 Blizzard Challenge. We extended the 2013 dataset with two new voices generated using a WaveNET vocoder. Acoustic-phonetic features of obstruent consonants and their neighbouring vowels were compared between the natural voice and each of these TTS systems. Statistical analysis was conducted using the Kruskal-Wallis test, and Dunn's test. Compared to the reference natural voice, we find that the WaveNET vocoder performs very well in modelling vowels, but features like F0 at onset and spectral tilt show significant deviations from the natural voice. Among consonants, neural voices deviate most from natural in the context of voiceless fricatives. Compared to other TTS systems, several features (like vowel dispersions, and consonant duration) which had shown strong deviations from natural, were found to not differ from natural in the WaveNET vocoder systems."
   ],
   "doi": "10.21437/Interspeech.2022-10606"
  },
  "rana22_interspeech": {
   "authors": [
    [
     "Fareeha S.",
     "Rana"
    ],
    [
     "Daniel",
     "Pape"
    ],
    [
     "Elisabet",
     "Service"
    ]
   ],
   "title": "The effect of increasing acoustic and linguistic complexity on auditory processing: an EEG study",
   "original": "10607",
   "page_count": 5,
   "order": 821,
   "p1": 4048,
   "pn": 4052,
   "abstract": [
    "This study uses electroencephalography (EEG) to explore the pre-attentive processing of auditory stimuli with a systematic (stepwise) increase in linguistic complexity. Participants heard repetitive similar stimuli interspersed with rare deviants that differed in pitch. Responses to the deviants were compared among non-speech complex harmonic waves and speech syllables. Syllables were generated using VocalTractLab [1], a software that allows for the synthesis of natural-sounding, highly controllable artificial speech. Complex waves were generated using Praat [2], with their intensity envelopes and harmonic structure matched to those of the syllables. Thus, the two stimulus types were acoustically very comparable, differing only in linguistic characteristics like presence/absence of modelled vocal tract and laryngeal influences. We hypothesized that both deviants would evoke a Mismatch Negativity (MMN) brain response. We further explored how MMN size varied with the nature of the stimulus. Results supported our hypothesis. An MMN was observed for both pitch deviants. The MMN was larger in amplitude for the complex waves than the syllables, suggesting differences between how linguistic syllable stimuli are processed compared to non-linguistic complex waves. This study demonstrates how an increase in acoustic and linguistic complexity reflects in the MMN response and provides support for domain-specific theories of auditory processing."
   ],
   "doi": "10.21437/Interspeech.2022-10607"
  },
  "jose22_interspeech": {
   "authors": [
    [
     "Christin",
     "Jose"
    ],
    [
     "Joe",
     "Wang"
    ],
    [
     "Grant",
     "Strimel"
    ],
    [
     "Mohammad Omar",
     "Khursheed"
    ],
    [
     "Yuriy",
     "Mishchenko"
    ],
    [
     "Brian",
     "Kulis"
    ]
   ],
   "title": "Latency Control for Keyword Spotting",
   "original": "10608",
   "page_count": 5,
   "order": 383,
   "p1": 1891,
   "pn": 1895,
   "abstract": [
    "Conversational agents commonly utilize keyword spotting (KWS) to initiate voice interaction with the user. For user experience and privacy considerations, existing approaches to KWS largely focus on accuracy, which can often come at the expense of introduced latency. To address this tradeoff, we propose a novel approach to control KWS model latency and which generalizes to any loss function without explicit knowledge of the keyword endpoint. Through a single, tunable hyperparameter, our approach enables one to balance detection latency and accuracy for the targeted application. Empirically, we show that our approach gives superior performance under latency constraints when compared to existing methods. Namely, we make a substantial 25 % relative false accepts improvement for a fixed latency target when compared to the baseline state-of-the-art. We also show that when our approach is used in conjunction with a max-pooling loss, we are able to improve relative false accepts by 25 % at a fixed latency when compared to cross entropy loss."
   ],
   "doi": "10.21437/Interspeech.2022-10608"
  },
  "chang22e_interspeech": {
   "authors": [
    [
     "Kai-Wei",
     "Chang"
    ],
    [
     "Wei-Cheng",
     "Tseng"
    ],
    [
     "Shang-Wen",
     "Li"
    ],
    [
     "Hung-yi",
     "Lee"
    ]
   ],
   "title": "An Exploration of Prompt Tuning on Generative Spoken Language Model for Speech Processing Tasks",
   "original": "10610",
   "page_count": 5,
   "order": 1013,
   "p1": 5005,
   "pn": 5009,
   "abstract": [
    "Speech representations learned from Self-supervised learning (SSL) models have been found beneficial for various speech processing tasks. However, utilizing SSL representations usually requires fine-tuning the pre-trained models or designing task-specific downstream models and loss functions, causing much memory usage and human labor. On the other hand, prompting in Natural Language Processing (NLP) is an efficient and widely used technique to leverage pre-trained language models (LMs). Nevertheless, such a paradigm is little studied in the speech community. We report in this paper the first exploration of the prompt tuning paradigm for speech processing tasks based on Generative Spoken Language Model (GSLM). Experiment results show that the prompt tuning technique achieves competitive performance in speech classification tasks with fewer trainable parameters than fine-tuning specialized downstream models. We further study the technique in challenging sequence generation tasks. Prompt tuning also demonstrates its potential, while the limitation and possible research directions are discussed in this paper."
   ],
   "doi": "10.21437/Interspeech.2022-10610"
  },
  "biadsy22_interspeech": {
   "authors": [
    [
     "Fadi",
     "Biadsy"
    ],
    [
     "Youzheng",
     "Chen"
    ],
    [
     "Xia",
     "Zhang"
    ],
    [
     "Oleg",
     "Rybakov"
    ],
    [
     "Andrew",
     "Rosenberg"
    ],
    [
     "Pedro",
     "Moreno"
    ]
   ],
   "title": "A Scalable Model Specialization Framework for Training and Inference using Submodels and its Application to Speech Model Personalization",
   "original": "10613",
   "page_count": 5,
   "order": 1037,
   "p1": 5125,
   "pn": 5129,
   "abstract": [
    "Model fine-tuning and adaptation have become a common approach for model specialization for downstream tasks or domains. Fine-tuning the entire model or a subset of the parameters using light-weight adaptation has shown considerable success across different specialization tasks. Fine-tuning a model for a large number of domains typically requires starting a new training job for every domain posing scaling limitations. Once these models are trained, deploying them also poses significant scalability challenges for inference for real-time applications. In this paper, building upon prior light-weight adaptation techniques, we propose a modular framework that enables us to substantially improve scalability for model training and inference. We introduce Submodels that can be quickly and dynamically loaded for on-the-fly inference. We also propose multiple approaches for training those Submodels in parallel using an embedding space in the same training job. We test our framework on an extreme use-case which is speech model personalizaion for atypical speech, requiring a Submodel for each user. We obtain 128X Submodel throughput with a fixed computation budget without a loss of accuracy. We also show that learning a speaker-embedding space can scale further and reduce the amount of personalization training data required per speaker."
   ],
   "doi": "10.21437/Interspeech.2022-10613"
  },
  "zaidi22_interspeech": {
   "authors": [
    [
     "Mohd Abbas",
     "Zaidi"
    ],
    [
     "Beomseok",
     "Lee"
    ],
    [
     "Sangha",
     "Kim"
    ],
    [
     "Chanwoo",
     "Kim"
    ]
   ],
   "title": "Cross-Modal Decision Regularization for Simultaneous Speech Translation",
   "original": "10617",
   "page_count": 5,
   "order": 24,
   "p1": 116,
   "pn": 120,
   "abstract": [
    "Simultaneous translation systems start producing the output while processing the partial source sentence in the incoming input stream. These systems need to decide when to read more input and when to write the output. The decisions taken by the model depend on the structure of source/target language and the information contained in the partial input sequence. Hence, read/write decision policy remains the same across different input modalities, i.e., speech and text. This motivates us to leverage the text transcripts corresponding to the speech input for improving simultaneous speech-to-text translation (SimulST). We propose Cross-Modal Decision Regularization (CMDR) to improve the decision policy of SimulST systems by using the simultaneous text-to-text translation (SimulMT) task. We also extend several techniques from the offline speech translation domain to explore the role of SimulMT task in improving SimulST performance. Overall, we achieve 34.66% / 4.5 BLEU improvement over the baseline model across different latency regimes for the MuST-C English-German (EnDe) SimulST task."
   ],
   "doi": "10.21437/Interspeech.2022-10617"
  },
  "schraner22_interspeech": {
   "authors": [
    [
     "Yanick",
     "Schraner"
    ],
    [
     "Christian",
     "Scheller"
    ],
    [
     "Michel",
     "Plüss"
    ],
    [
     "Lukas",
     "Neukom"
    ],
    [
     "Manfred",
     "Vogel"
    ]
   ],
   "title": "Comparison of Unsupervised Learning and Supervised Learning with Noisy Labels for Low-Resource Speech Recognition",
   "original": "10620",
   "page_count": 5,
   "order": 987,
   "p1": 4875,
   "pn": 4879,
   "abstract": [
    "Supervised training of end-to-end speech recognition systems usually requires large amounts of transcribed speech data to achieve reasonable performance. This hinders its application to problems where the availability of annotated data is low since manual labeling is costly. Often, however, large amounts of speech data with imperfect transcriptions are available, which can be automatically aligned to generate noisy labels. In this work, we compare how supervised learning on noisy data from forced alignment compares to semi-supervised learning and self-supervised representation learning. The latter two have shown great success in improving speech recognition using unlabeled data. We employ noisy student training for semi-supervised learning and wav2vec 2.0 for self-supervised representation learning. We compare these methods on 2324 hours of Swiss German audio with automatically aligned Standard German text. Using speech data with noisy labels for supervised learning leads to a word error rate (WER) of 26.4% on our test set. Using the same data for wav2vec pretraining leads to a WER of 27.8%. With noisy student training, we achieve a WER of 30.3%."
   ],
   "doi": "10.21437/Interspeech.2022-10620"
  },
  "lam22_interspeech": {
   "authors": [
    [
     "Perry",
     "Lam"
    ],
    [
     "Huayun",
     "Zhang"
    ],
    [
     "Nancy",
     "Chen"
    ],
    [
     "Berrak",
     "Sisman"
    ]
   ],
   "title": "EPIC TTS Models: Empirical Pruning Investigations Characterizing Text-To-Speech Models",
   "original": "10626",
   "page_count": 5,
   "order": 166,
   "p1": 823,
   "pn": 827,
   "abstract": [
    "Neural models are known to be over-parameterized, and recent work has shown that sparse text-to-speech (TTS) models can outperform dense models. Although a plethora of sparse methods has been proposed for other domains, such methods have rarely been applied in TTS. In this work, we seek to answer the question: what are the characteristics of selected sparse techniques on the performance and model complexity? We compare a Tacotron2 baseline and the results of applying five techniques. We then evaluate the performance via the factors of naturalness, intelligibility and prosody, while reporting model size and training time. Complementary to prior research, we find that pruning before or during training can achieve similar performance to pruning after training and can be trained much faster, while removing entire neurons degrades performance much more than removing parameters. To our best knowledge, this is the first work that compares sparsity paradigms in text-to-speech synthesis."
   ],
   "doi": "10.21437/Interspeech.2022-10626"
  },
  "chang22f_interspeech": {
   "authors": [
    [
     "Chih-Chiang",
     "Chang"
    ],
    [
     "Hung-yi",
     "Lee"
    ]
   ],
   "title": "Exploring Continuous Integrate-and-Fire for Adaptive Simultaneous Speech Translation",
   "original": "10627",
   "page_count": 5,
   "order": 1047,
   "p1": 5175,
   "pn": 5179,
   "abstract": [
    "Simultaneous speech translation (SimulST) is a challenging task aiming to translate streaming speech before the complete input is observed. A SimulST system generally includes two components: the pre-decision that aggregates the speech information and the policy that decides to read or write. While recent works had proposed various strategies to improve the pre-decision, they mainly adopt the fixed wait-k policy, leaving the adaptive policies rarely explored. This paper proposes to model the adaptive policy by adapting the Continuous Integrate-and-Fire (CIF). Compared with monotonic multihead attention (MMA), our method has the advantage of simpler computation, superior quality at low latency, and better generalization to long utterances. We conduct experiments on the MuST-C V2 dataset and show the effectiveness of our approach."
   ],
   "doi": "10.21437/Interspeech.2022-10627"
  },
  "sharon22_interspeech": {
   "authors": [
    [
     "Rini",
     "Sharon"
    ],
    [
     "Heet",
     "Shah"
    ],
    [
     "Debdoot",
     "Mukherjee"
    ],
    [
     "Vikram",
     "Gupta"
    ]
   ],
   "title": "Multilingual and Multimodal Abuse Detection",
   "original": "10629",
   "page_count": 5,
   "order": 938,
   "p1": 4631,
   "pn": 4635,
   "abstract": [
    "The presence of abusive content on social media platforms is undesirable as it severely impedes healthy and safe social media interactions. While automatic abuse detection has been widely explored in textual domain, audio abuse detection still remains unexplored. In this paper, we attempt abuse detection in conversational audio from a multimodal perspective in a multilingual social media setting. Our key hypothesis is that along with the modelling of audio, incorporating discriminative information from other modalities can be highly beneficial for this task. Our proposed method, MADA, explicitly focuses on two modalities other than the audio itself, namely, the underlying emotions expressed in the abusive audio and the semantic information encapsulated in the corresponding textual form. Observations prove that MADA demonstrates gains over audio-only approaches on the ADIMA dataset. We test the proposed approach on 10 different languages and observe consistent gains in the range 0.6%-5.2% by leveraging multiple modalities. We also perform extensive ablation experiments for studying the contributions of every modality and observe the best results while leveraging all the modalities together. Additionally, we perform experiments to empirically confirm that there is a strong correlation between underlying emotions and abusive behaviour. Code is available at https://github.com/ShareChatAI/MADA"
   ],
   "doi": "10.21437/Interspeech.2022-10629"
  },
  "abderrazek22_interspeech": {
   "authors": [
    [
     "Sondes",
     "Abderrazek"
    ],
    [
     "Corinne",
     "Fredouille"
    ],
    [
     "Alain",
     "Ghio"
    ],
    [
     "Muriel",
     "Lalain"
    ],
    [
     "Christine",
     "Meunier"
    ],
    [
     "Virginie",
     "Woisard"
    ]
   ],
   "title": "Validation of the Neuro-Concept Detector framework for the characterization of speech disorders: A comparative study including Dysarthria and Dysphonia",
   "original": "10631",
   "page_count": 5,
   "order": 736,
   "p1": 3638,
   "pn": 3642,
   "abstract": [
    "Recently, we have proposed a general analytical framework, called Neuro-based Concept Detector (NCD), to interpret the deep representations of a DNN. Based on the activation patterns of hidden neurons, this framework highlights the ability of neurons to detect a specific concept related to the final task. Its main strength is to provide an interpretability tool for any type of DNN performing a classification task, whatever the application domain. Thanks to NCD, we have demonstrated the emergence of phonetic features in the classification layers of a CNN-based model for French phone classification. The emergence of this concept, of great interest in the field of clinical phonetics, has been studied considering healthy speech. Applied to Head and Neck Cancers, we have shown that this framework automatically reflects the level of impairment of the phonetic features produced by a patient, which is supported by the strong correlations with perceptual assessments performed by clinical experts. The objective of the work presented here is to validate the proposed framework by confronting it to new populations of patients, but with very different pathologies (neurodegenerative diseases/ Dysarthria and vocal dysfunction/ Dysphonia). The robustness of the approach to the phonetic content variability of read text is also studied."
   ],
   "doi": "10.21437/Interspeech.2022-10631"
  },
  "bartolewska22_interspeech": {
   "authors": [
    [
     "Julitta",
     "Bartolewska"
    ],
    [
     "Stanisław",
     "Kacprzak"
    ],
    [
     "Konrad",
     "Kowalczyk"
    ]
   ],
   "title": "Refining DNN-based Mask Estimation using CGMM-based EM Algorithm for Multi-channel Noise Reduction",
   "original": "10632",
   "page_count": 5,
   "order": 593,
   "p1": 2923,
   "pn": 2927,
   "abstract": [
    "In this paper, we present a method that allows to further improve speech enhancement obtained with recently introduced Deep Neural Network (DNN) models. We propose a multi-channel refinement method of time-frequency masks obtained with single-channel DNNs, which consists of an iterative Complex Gaussian Mixture Model (CGMM) based algorithm, followed by optimum spatial filtration. We validate our approach on time-frequency masks estimated with three recent deep learning models, namely DCUnet, DCCRN, and FullSubNet. We show that our method with the proposed mask refinement procedure allows to improve the accuracy of estimated masks, in terms of the Area Under the ROC Curve (AUC) measure, and as a consequence the overall speech quality of the enhanced speech signal, as measured by PESQ improvement, and that the improvement is consistent across all three DNN models."
   ],
   "doi": "10.21437/Interspeech.2022-10632"
  },
  "lemaguer22_interspeech": {
   "authors": [
    [
     "Sébastien",
     "Le Maguer"
    ],
    [
     "Simon",
     "King"
    ],
    [
     "Naomi",
     "Harte"
    ]
   ],
   "title": "Back to the Future: Extending the Blizzard Challenge 2013",
   "original": "10633",
   "page_count": 5,
   "order": 484,
   "p1": 2378,
   "pn": 2382,
   "abstract": [
    "Nowadays, speech synthesis technology is synonymous with the use of Deep Learning. To understand more about how synthesis systems have progressed with the advent of Deep Learning requires open-sourced speech resources that connect past and present technologies. This would allow direct comparisons. This paper presents such a resource by extending the 2013 edition of the Blizzard Challenge. Using this extension, we compare top-tier systems from the past to modern technologies in a controlled setting. From this edition, we selected the best representative of each historical synthesis technology, to which we added four systems representing combinations of modern acoustic models and neural vocoders. A large scale subjective evaluation was conducted to evaluate naturalness. Our results show that, as expected, modern technologies generate more natural synthetic speech. However, these systems are still not perceived to be as natural as the human voice. Crucially, we also observed that the Mean Opinion Score (MOS) of the historical systems dropped a full MOS point from their scores in the original edition. This demonstrates the relative nature of MOS: it should generally not be reported as an absolute value despite its origin as an absolute category rating."
   ],
   "doi": "10.21437/Interspeech.2022-10633"
  },
  "yang22t_interspeech": {
   "authors": [
    [
     "Zijiang",
     "Yang"
    ],
    [
     "Xin",
     "Jing"
    ],
    [
     "Andreas",
     "Triantafyllopoulos"
    ],
    [
     "Meishu",
     "Song"
    ],
    [
     "Ilhan",
     "Aslan"
    ],
    [
     "Björn W.",
     "Schuller"
    ]
   ],
   "title": "An Overview & Analysis of Sequence-to-Sequence Emotional Voice Conversion",
   "original": "10636",
   "page_count": 5,
   "order": 995,
   "p1": 4915,
   "pn": 4919,
   "abstract": [
    "Emotional voice conversion (EVC) focuses on converting a speech utterance from a source to a target emotion; it can thus be a key enabling technology for human-computer interaction applications and beyond. However, EVC remains an unsolved research problem with several challenges. In particular, as speech rate and rhythm are two key factors of emotional conversion, models have to generate output sequences of differing length. Sequence-to-sequence modelling is recently emerging as a competitive paradigm for models that can overcome those challenges. In an attempt to stimulate further research in this promising new direction, recent sequence-to-sequence EVC papers were systematically investigated and reviewed from six perspectives: their motivation, training strategies, model architectures, datasets, model inputs, and evaluation methods. This information is organised to provide the research community with an easily digestible overview of the current state-of-the-art. Finally, we discuss existing challenges of sequence-to-sequence EVC."
   ],
   "doi": "10.21437/Interspeech.2022-10636"
  },
  "patel22_interspeech": {
   "authors": [
    [
     "Tanvina",
     "Patel"
    ],
    [
     "Odette",
     "Scharenborg"
    ]
   ],
   "title": "Using cross-model learnings for the Gram Vaani ASR Challenge 2022",
   "original": "10639",
   "page_count": 5,
   "order": 988,
   "p1": 4880,
   "pn": 4884,
   "abstract": [
    "In the diverse and multilingual land of India, Hindi is spoken as a first language by a majority of its population. Efforts are made to obtain data in terms of audio, transcriptions, dictionary, etc. to develop speech-technology applications in Hindi. Similarly, the Gram-Vaani ASR Challenge 2022 provides spontaneous telephone speech, with natural back-ground and regional variations in Hindi. The challenge provides: 100 hours of labeled train-set, 5 hours of labeled dev-set and 1000 hours of unlabeled data-set. For the 'Closed Challenge', we trained an End-to-End (E2E) Conformer model using speed perturbations, SpecAugment techniques and use VTLN to handle any unknown speaker groups in the blind evaluation set. On the dev-set, we achieved a 30.3% WER compared to the 34.8% WER by the Challenge E2E baseline. For the 'Self Supervised Closed Challenge', a semi-supervised learning approach is used. We generate pseudo-transcripts for the unlabeled data using a hybrid TDNN-3gram LM model and trained an E2E model. This is then used as a seed for retraining the E2E model with high confidence data. Cross-model learning and refining of the E2E model gave 25.3% WER on the dev-set compared to ~33-35% WER by the Challenge baseline that use wav2vec models."
   ],
   "doi": "10.21437/Interspeech.2022-10639"
  },
  "luu22_interspeech": {
   "authors": [
    [
     "Chau",
     "Luu"
    ],
    [
     "Steve",
     "Renals"
    ],
    [
     "Peter",
     "Bell"
    ]
   ],
   "title": "Investigating the contribution of speaker attributes to speaker separability using disentangled speaker representations",
   "original": "10643",
   "page_count": 5,
   "order": 123,
   "p1": 610,
   "pn": 614,
   "abstract": [
    "Deep speaker embeddings have been shown to encode a wide variety of attributes relating to a speaker. The aim of this work is to separate out some of these attributes in the embedding space, disentangling these sources of speaker variation into subsets of the embedding dimensions. This is achieved modifying the training procedure of a typical speaker embedding network, which is typically only trained to classify speakers. This work instead adds pairs of attribute specific task heads to operate on complementary subsets of the speaker embedding dimensions. While specific dimensions are encouraged to encode an attribute, for example gender, the other dimensions are penalized for containing this information using an adversarial loss. We show that this method is effective in factorizing out multiple attributes in the embedding space, successfully disentangling gender, nationality and age. Using the disentangled representations, we investigate how much removing this information impacts speaker verification and diarization performance, showing that gender is a significant source of separation in the deep speaker embedding space, with nationality and age also contributing to a lesser degree."
   ],
   "doi": "10.21437/Interspeech.2022-10643"
  },
  "gao22d_interspeech": {
   "authors": [
    [
     "Yan",
     "Gao"
    ],
    [
     "Javier",
     "Fernandez-Marques"
    ],
    [
     "Titouan",
     "Parcollet"
    ],
    [
     "Abhinav",
     "Mehrotra"
    ],
    [
     "Nicholas",
     "Lane"
    ]
   ],
   "title": "Federated Self-supervised Speech Representations: Are We There Yet?",
   "original": "10644",
   "page_count": 5,
   "order": 772,
   "p1": 3809,
   "pn": 3813,
   "abstract": [
    "The ubiquity of microphone-enabled devices has lead to large amounts of unlabelled audio data being produced at the edge. The integration of self-supervised learning (SSL) and federated learning (FL) into one coherent system can potentially offer data privacy guarantees while also advancing the quality and robustness of speech representations. In this paper, we provide a first-of-its-kind systematic study of the feasibility and complexities for training speech SSL models under FL scenarios from the perspective of algorithms, hardware, and systems limits. Despite the high potential of their combination, we find existing system constraints and algorithmic behaviour make SSL and FL systems nearly impossible to build today. Yet critically, our results indicate specific performance bottlenecks and research opportunities that would allow this situation to be reversed. While our analysis suggests that, given existing trends in hardware, hybrid SSL and FL speech systems will not be viable until 2027, we believe this study can act as a roadmap to accelerate work towards reaching this milestone much earlier."
   ],
   "doi": "10.21437/Interspeech.2022-10644"
  },
  "nguyen22c_interspeech": {
   "authors": [
    [
     "Thi Thu Trang",
     "NGUYEN"
    ],
    [
     "Trung Duc Anh",
     "Dang"
    ],
    [
     "Quoc Viet",
     "Vu"
    ],
    [
     "Woomyoung",
     "Park"
    ]
   ],
   "title": "Building Vietnamese Conversational Smart Home Dataset and Natural Language Understanding Model",
   "original": "10645",
   "page_count": 5,
   "order": 1048,
   "p1": 5180,
   "pn": 5184,
   "abstract": [
    "Natural Language Understanding (NLU), which includes intent detection and slot tagging, plays an important role in any dialog system. This paper aims at building a first-ever conversational smart home dataset SmartNLU and NLU model for Vietnamese. Raw data were collected by asking participants provide or confirm the intents of and slot values in the user says that they sent or received in a smart home conversation until all were matched, using a Wizard-of-Oz set-up of a web tool. The data were then cleaned and processed to build templates of user says with empty slots. The entity strategy, which filled all slot values by the round-robin algorithm to templates, was empirically chosen to generate user says from collected templates, which made a total of 3,492/1,176/1,198 user says correspondingly for the training/validating/test sets. The dataset has been released for a challenge carried out in AIHub, and published for the community. Several state-of-the-art joint NLU models were experimented on the released dataset. The proposed NLU model, which added PhoBERT to the DIET architecture of Rasa framework, gave the best results. The sentence accuracy of the DIET+PhoBERT was considerably higher than (i.e. 4.3% to 11.7%) the one of others."
   ],
   "doi": "10.21437/Interspeech.2022-10645"
  },
  "mohapatra22_interspeech": {
   "authors": [
    [
     "Debasish",
     "Mohapatra"
    ],
    [
     "Mario",
     "Fleischer"
    ],
    [
     "Victor",
     "Zappi"
    ],
    [
     "Peter",
     "Birkholz"
    ],
    [
     "Sidney",
     "Fels"
    ]
   ],
   "title": "Three-dimensional finite-difference time-domain acoustic analysis of simplified vocal tract shapes",
   "original": "10649",
   "page_count": 5,
   "order": 154,
   "p1": 764,
   "pn": 768,
   "abstract": [
    "The finite-difference time-domain (FDTD) method has been widely used for vocal tract acoustic modelling due to its simplicity and low computational cost. Nevertheless, the method suffers from high discretization error while approximating realistic vocal tract geometries using orthogonal grid elements. Alternatively, simplified vocal tract shapes having regular contours can be used for articulatory models. These geometries can be generated from one-dimensional (1D) area functions, which approximate vocal tracts as concatenated tubes with different cross-sections. To this aim, we modify an existing 3D FDTD model for faster acoustic simulation and synthesize five English vowels with various simplified vocal tract shapes. We implement six geometrical shapes for each vowel, consisting of circular, elliptical and square cross-sections with centric and eccentric tube segment configurations. Vowel transfer functions obtained from these FDTD simulations are compared with a highly accurate finite element (FE) scheme. The acoustic formants of the FDTD model agree well with the corresponding FEM approach for most vowels. The influence of vocal tracts with different geometry approximations remains insignificant for frequencies below $5$~kHz. However, vocal tracts with elliptical or eccentric configurations have produced higher-order acoustic modes. This paper characterizes the acoustic properties of simplified vocal tract shapes using the 3D FDTD scheme."
   ],
   "doi": "10.21437/Interspeech.2022-10649"
  },
  "zhou22g_interspeech": {
   "authors": [
    [
     "Hengshun",
     "Zhou"
    ],
    [
     "Jun",
     "Du"
    ],
    [
     "Gongzhen",
     "Zou"
    ],
    [
     "Zhaoxu",
     "Nian"
    ],
    [
     "Chin-Hui",
     "Lee"
    ],
    [
     "Sabato Marco",
     "Siniscalchi"
    ],
    [
     "Shinji",
     "Watanabe"
    ],
    [
     "Odette",
     "Scharenborg"
    ],
    [
     "Jingdong",
     "Chen"
    ],
    [
     "Shifu",
     "Xiong"
    ],
    [
     "Jian-Qing",
     "Gao"
    ]
   ],
   "title": "Audio-Visual Wake Word Spotting in MISP2021 Challenge: Dataset Release and Deep Analysis",
   "original": "10650",
   "page_count": 5,
   "order": 226,
   "p1": 1111,
   "pn": 1115,
   "abstract": [
    "In this paper, we describe and release publicly the audio-visual wake word spotting (WWS) database in the MISP2021 Challenge, which covers a range of scenarios of audio and video data collected by near-, mid-, and far-field microphone arrays, and cameras, to create a shared and publicly available database for WWS. The database and the code are released, which will be a valuable addition to the community for promoting WWS research using multi-modality information in realistic and complex conditions. Moreover, we investigated the different data augmentation methods for single modalities on an end-to-end WWS network. A set of audio-visual fusion experiments and analysis were conducted to observe the assistance from visual information to acoustic information based on different audio and video field configurations. The results showed that the fusion system generally improves over the single-modality (audio- or video-only) system, especially under complex noisy conditions."
   ],
   "doi": "10.21437/Interspeech.2022-10650"
  },
  "peng22c_interspeech": {
   "authors": [
    [
     "Puyuan",
     "Peng"
    ],
    [
     "David",
     "Harwath"
    ]
   ],
   "title": "Word Discovery in Visually Grounded, Self-Supervised Speech Models",
   "original": "10652",
   "page_count": 5,
   "order": 573,
   "p1": 2823,
   "pn": 2827,
   "abstract": [
    "We present a method for visually-grounded spoken term discovery. After training either a HuBERT or wav2vec2.0 model to associate spoken captions with natural images, we show that powerful word segmentation and clustering capability emerges within the model's self-attention heads. Our experiments reveal that this ability is not present to nearly the same extent in the base HuBERT and wav2vec2.0 models, suggesting that the visual grounding task is a crucial component of the word discovery capability we observe. We also evaluate our method on the Buckeye word segmentation and ZeroSpeech spoken term discovery tasks, where we outperform all currently published methods on several metrics. Code and model weights are available at \\href{https://github.com/jasonppy/word-discovery}{https://github.com/jasonppy/word-discovery}"
   ],
   "doi": "10.21437/Interspeech.2022-10652"
  },
  "welker22_interspeech": {
   "authors": [
    [
     "Simon",
     "Welker"
    ],
    [
     "Julius",
     "Richter"
    ],
    [
     "Timo",
     "Gerkmann"
    ]
   ],
   "title": "Speech Enhancement with Score-Based Generative Models in the Complex STFT Domain",
   "original": "10653",
   "page_count": 5,
   "order": 594,
   "p1": 2928,
   "pn": 2932,
   "abstract": [
    "Score-based generative models (SGMs) have recently shown impressive results for difficult generative tasks such as the unconditional and conditional generation of natural images and audio signals. In this work, we extend these models to the complex short-time Fourier transform (STFT) domain, proposing a novel training task for speech enhancement using a complex-valued deep neural network. We derive this training task within the formalism of stochastic differential equations (SDEs), thereby enabling the use of predictor-corrector samplers. We provide alternative formulations inspired by previous publications on using generative diffusion models for speech enhancement, avoiding the need for any prior assumptions on the noise distribution and making the training task purely generative which, as we show, results in improved enhancement performance."
   ],
   "doi": "10.21437/Interspeech.2022-10653"
  },
  "li22x_interspeech": {
   "authors": [
    [
     "Yuntao",
     "Li"
    ],
    [
     "Can",
     "Xu"
    ],
    [
     "Huang",
     "Hu"
    ],
    [
     "Lei",
     "Sha"
    ],
    [
     "Yan",
     "Zhang"
    ],
    [
     "Daxin",
     "Jiang"
    ]
   ],
   "title": "Small Changes Make Big Differences: Improving Multi-turn Response Selection in Dialogue Systems via Fine-Grained Contrastive Learning",
   "original": "10656",
   "page_count": 5,
   "order": 553,
   "p1": 2723,
   "pn": 2727,
   "abstract": [
    "Retrieve-based dialogue response selection aims to find a proper response from a candidate set given a multi-turn context. The sequence representations generated by pre-trained language models (PLMs) play key roles in the learning of matching degree between the dialogue contexts and the responses. However, context-response pairs sharing the same context but different responses tend to have a greater similarity in the sequence representations calculated by PLMs, which makes it hard to distinguish positive responses from negative ones. Motivated by this, we propose a novel Fine-Grained Contrastive (FGC) learning method for the response selection task based on PLMs. This FGC learning strategy helps PLMs to generate more distinguishable pair representations of each dialogue at fine grains, and further make better predictions on choosing positive responses. Empirical studies on two benchmark datasets demonstrate that the proposed FGC learning method can generally and significantly improve the model performance of existing PLM-based matching models."
   ],
   "doi": "10.21437/Interspeech.2022-10656"
  },
  "weiran22_interspeech": {
   "authors": [
    [
     "Wang",
     "Weiran"
    ],
    [
     "Tongzhou",
     "Chen"
    ],
    [
     "Tara",
     "Sainath"
    ],
    [
     "Ehsan",
     "Variani"
    ],
    [
     "Rohit",
     "Prabhavalkar"
    ],
    [
     "W. Ronny",
     "Huang"
    ],
    [
     "Bhuvana",
     "Ramabhadran"
    ],
    [
     "Neeraj",
     "Gaur"
    ],
    [
     "Sepand",
     "Mavandadi"
    ],
    [
     "Cal",
     "Peyser"
    ],
    [
     "Trevor",
     "Strohman"
    ],
    [
     "Yanzhang",
     "He"
    ],
    [
     "David",
     "Rybach"
    ]
   ],
   "title": "Improving Rare Word Recognition with LM-aware MWER Training",
   "original": "10660",
   "page_count": 5,
   "order": 210,
   "p1": 1031,
   "pn": 1035,
   "abstract": [
    "Language models (LMs) significantly improve the recognition accuracy of end-to-end (E2E) models on words rarely seen during training, when used in either the shallow fusion or the rescoring setups. In this work, we introduce LMs in the learning of hybrid autoregressive transducer (HAT) models in the discriminative training framework, to mitigate the training versus inference gap regarding the use of LMs. For the shallow fusion setup, we use LMs during both hypotheses generation and loss computation, and the LM-aware MWER-trained model achieves 10\\% relative improvement over the model trained with standard MWER on voice search test sets containing rare words. For the rescoring setup, we learn a small neural module to generate per-token fusion weights in a data-dependent manner. This model achieves the same rescoring WER as regular MWER-trained model, but without the need for sweeping fusion weights."
   ],
   "doi": "10.21437/Interspeech.2022-10660"
  },
  "oh22_interspeech": {
   "authors": [
    [
     "Miran",
     "Oh"
    ],
    [
     "Yoonjeong",
     "Lee"
    ]
   ],
   "title": "Dynamic Vertical Larynx Actions Under Prosodic Focus",
   "original": "10661",
   "page_count": 5,
   "order": 20,
   "p1": 96,
   "pn": 100,
   "abstract": [
    "Recently, Lee (2018) observes that one vertical larynx movement (VLM) is associated with an Accentual Phrase (AP) in Seoul Korean. The current study builds on these findings by investigating the effect of prosodic focus on vertical larynx actions. Target sentences were designed to produce four APs (e.g., Joohyun sold six yards of shabby garden field; AP[Joohyun-SUBJ] AP[shabby garden field] AP[six yards-OBJ] AP[sold-DECL], presented in Korean) and were used to elicit focus on the initial word of the object phrase (e.g., six). Articulatory data on VLM is obtained from five Seoul Korean speakers using real-time MRI. Results indicate that quantifiable VLMs observed for each sentence range from 3 to 6 movements, with 4 movements per sentence being the most frequent. Sentences with focus have more instances of VLM per sentence than those without. Focused sentences exhibit significantly greater vertical larynx displacement around the region of focus than the control. Our findings have implications for prosodic planning and pitch resetting, and ongoing analyses examine how VLMs align with Accentual Phrases in Seoul Korean and correlate with fundamental frequency."
   ],
   "doi": "10.21437/Interspeech.2022-10661"
  },
  "quamer22_interspeech": {
   "authors": [
    [
     "Waris",
     "Quamer"
    ],
    [
     "Anurag",
     "Das"
    ],
    [
     "John",
     "Levis"
    ],
    [
     "Evgeny",
     "Chukharev-Hudilainen"
    ],
    [
     "Ricardo",
     "Gutierrez-Osuna"
    ]
   ],
   "title": "Zero-Shot Foreign Accent Conversion without a Native Reference",
   "original": "10664",
   "page_count": 5,
   "order": 996,
   "p1": 4920,
   "pn": 4924,
   "abstract": [
    "Previous approaches for foreign accent conversion (FAC) either need a reference utterance from a native speaker (L1) during synthesis, or are dedicated one-to-one systems that must be trained separately for each non-native (L2) speaker. To address both issues, we propose a new FAC system that can transform L2 speech directly from previously unseen speakers. The system consists of two independent modules: a translator and a synthesizer, which operate on bottleneck features derived from phonetic posteriorgrams. The translator is trained to map bottleneck features in L2 utterances into those from a parallel L1 utterance. The synthesizer is a many-to-many system that maps input bottleneck features into the corresponding Mel-spectrograms, conditioned on an embedding from the L2 speaker. During inference, both modules operate in sequence to take an unseen L2 utterance and generate a native-accented Mel-spectrogram. Perceptual experiments show that our system achieves a large reduction (67%) in non-native accentedness compared to a state-of-the-art reference-free system (28.9%) that builds a dedicated model for each L2 speaker. Moreover, 80% of the listeners rated the synthesized utterances to have the same voice identity as the L2 speaker."
   ],
   "doi": "10.21437/Interspeech.2022-10664"
  },
  "wang22w_interspeech": {
   "authors": [
    [
     "Shijun",
     "Wang"
    ],
    [
     "Hamed",
     "Hemati"
    ],
    [
     "Jón",
     "Guðnason"
    ],
    [
     "Damian",
     "Borth"
    ]
   ],
   "title": "Generative Data Augmentation Guided by Triplet Loss for Speech Emotion Recognition",
   "original": "10667",
   "page_count": 5,
   "order": 79,
   "p1": 391,
   "pn": 395,
   "abstract": [
    "Speech Emotion Recognition (SER) is crucial for human-computer interaction but still remains a challenging problem because of two major obstacles: data scarcity and imbalance. Many datasets for SER are substantially imbalanced, where data utterances of one class (most often Neutral) are much more frequent than those of other classes. Furthermore, only a few data resources are available for many existing spoken languages. To address these problems, we exploit a GAN-based augmentation model guided by a triplet network, to improve SER performance given imbalanced and insufficient training data. We conduct experiments and demonstrate: 1) With a highly imbalanced dataset, our augmentation strategy significantly improves the SER performance (+8\\% recall score compared with the baseline). 2) Moreover, in a cross-lingual benchmark, where we train a model with enough source language utterances but very few target language utterances (around 50 in our experiments), our augmentation strategy brings benefits for the SER performance of all three target languages."
   ],
   "doi": "10.21437/Interspeech.2022-10667"
  },
  "bradshaw22_interspeech": {
   "authors": [
    [
     "Leah",
     "Bradshaw"
    ],
    [
     "Eleanor",
     "Chodroff"
    ],
    [
     "Lena",
     "Jäger"
    ],
    [
     "Volker",
     "Dellwo"
    ]
   ],
   "title": "Fundamental Frequency Variability over Time in Telephone Interactions",
   "original": "10669",
   "page_count": 5,
   "order": 21,
   "p1": 101,
   "pn": 105,
   "abstract": [
    "Speech signals contain substantial fundamental frequency (f0) variability. Even within a single utterance, speakers modify f0 to create different intonational patterns. Previous studies have identified markers of increased f0 variability, such as the introduction of a new topic or greetings, but these are limited in the scope of their analyses. In the present study, we investigate f0 variability over the course of a telephone conversation, with a focus on the initial and medial utterances within the exchange. We examined f0 standard deviation of each utterance in over 2000 telephone conversations from 509 American English speakers from the Switchboard corpus. Findings showed that on average, speakers exhibit more f0 variability in the opening compared to mid-conversation utterances. Further, findings suggest that the inclusion of a greeting word in an initial turn, e.g., \"hello” or \"hi”, corresponds to an increase in f0 standard deviation. These results suggest that speakers employed more variable f0 in the initial few turns of a telephone conversation. The interpretation of this finding is multifaceted and may be linked to several communicative goals, including the placement of identity markers in conversation or the attraction of attention, or the role of openings as boundary markers."
   ],
   "doi": "10.21437/Interspeech.2022-10669"
  },
  "shirian22_interspeech": {
   "authors": [
    [
     "AMIR",
     "SHIRIAN"
    ],
    [
     "Krishna",
     "Somandepalli"
    ],
    [
     "Victor",
     "Sanchez"
    ],
    [
     "Tanaya",
     "Guha"
    ]
   ],
   "title": "Visually-aware Acoustic Event Detection using Heterogeneous Graphs",
   "original": "10670",
   "page_count": 5,
   "order": 494,
   "p1": 2428,
   "pn": 2432,
   "abstract": [
    "Perception of auditory events is inherently multimodal relying on both audio and visual cues. The majority of existing multimodal approaches usually process each modality using modality-specific models and then fuse the embeddings to encode the joint information. Different from that, we employ a heterogeneous graph that explicitly captures the spatial and temporal relationships between the modalities captures detailed information and rich semantics. We propose a heterogeneous graph approach to address the task of visually-aware acoustic event detection which serves as a compact, efficient and scalable way to represent data is in the form of graphs. Through the heterogeneous graphs, we efficiently model the intra- and inter-modality relationship both at spatial and temporal domains. Our model can easily be adapted to different scales of events through relevant hyperparameters. Experiments on a large benchmark dataset, called AudioSet, shows that our model achieves state-of-the-art performance."
   ],
   "doi": "10.21437/Interspeech.2022-10670"
  },
  "teixeira22_interspeech": {
   "authors": [
    [
     "Francisco",
     "Teixeira"
    ],
    [
     "Alberto",
     "Abad"
    ],
    [
     "Bhiksha",
     "Raj"
    ],
    [
     "Isabel",
     "Trancoso"
    ]
   ],
   "title": "Towards End-to-End Private Automatic Speaker Recognition",
   "original": "10672",
   "page_count": 5,
   "order": 568,
   "p1": 2798,
   "pn": 2802,
   "abstract": [
    "The development of privacy-preserving automatic speaker verification systems has been the focus of a number of studies with the intent of allowing users to authenticate themselves without risking the privacy of their voice. However, current privacy-preserving methods assume that the template voice representations (or speaker embeddings) used for authentication are extracted locally by the user. This poses two important issues: first, knowledge of the speaker embedding extraction model may create security and robustness liabilities for the authentication system, as this knowledge might help attackers in crafting adversarial examples able to mislead the system; second, from the point of view of a service provider the speaker embedding extraction model is arguably one of the most valuable components in the system and, as such, disclosing it would be highly undesirable. In this work, we show how speaker embeddings can be extracted while keeping both the speaker's voice and the service provider's model private, using Secure Multiparty Computation. Further, we show that it is possible to obtain reasonable trade-offs between security and computational cost. This work is complementary to those showing how authentication may be performed privately, and thus can be considered as another step towards fully private automatic speaker recognition."
   ],
   "doi": "10.21437/Interspeech.2022-10672"
  },
  "gibson22_interspeech": {
   "authors": [
    [
     "Mark",
     "Gibson"
    ],
    [
     "Marcel",
     "Schlechtweg"
    ],
    [
     "Beatriz",
     "Blecua Falgueras"
    ],
    [
     "Judit",
     "Ayala Alcalde"
    ]
   ],
   "title": "Language-specific interactions of vowel discrimination in noise",
   "original": "10673",
   "page_count": 5,
   "order": 632,
   "p1": 3118,
   "pn": 3122,
   "abstract": [
    "Our pilot vowel discrimination experiment addresses the competition between attentional focus and language exposure in two noise conditions using two groups of participants (L1 English-speakers (L1-EN) taking a perception test in Spanish and L1 Spanish (L1-SP) speakers taking a perception test in Spanish). Our noise conditions included three signal-to-noise ratio (SNR) conditions (-12, -6 and 0 decibels (dB)) and conditions using automatically generated multi-speaker background babble for 1-12 speakers. Our results show notable confusion by both groups in discriminating back round vowels [o] and [u] regardless of L1 or language exposure. We attribute this confusion to the fact that tongue height, detectible through F1, is obfuscated by F3 (lip rounding). In the absence of a visual input by which a listener can discriminate mid and high vowels by a control parameter such as lip aperture (or jaw angle), listeners experience notable difficulty in discerning vowel categories regardless of L1 or exposure to a target L2. Our results are consistent with the notion that both attentional focus and language exposure may provide advantages to vowel discrimination in noise, but compete in bottom-up/top-down protocols."
   ],
   "doi": "10.21437/Interspeech.2022-10673"
  },
  "hernandez22_interspeech": {
   "authors": [
    [
     "Abner",
     "Hernandez"
    ],
    [
     "Paula Andrea",
     "Pérez-Toro"
    ],
    [
     "Elmar",
     "Noeth"
    ],
    [
     "Juan Rafael",
     "Orozco-Arroyave"
    ],
    [
     "Andreas",
     "Maier"
    ],
    [
     "Seung Hee",
     "Yang"
    ]
   ],
   "title": "Cross-lingual Self-Supervised Speech Representations for Improved Dysarthric Speech Recognition",
   "original": "10674",
   "page_count": 5,
   "order": 11,
   "p1": 51,
   "pn": 55,
   "abstract": [
    "State-of-the-art automatic speech recognition (ASR) systems perform well on healthy speech. However, the performance on impaired speech still remains an issue. The current study explores the usefulness of using Wav2Vec self-supervised speech representations as features for training an ASR system for dysarthric speech. Dysarthric speech recognition is particularly difficult as several aspects of speech such as articulation, prosody and phonation can be impaired. Specifically, we train an acoustic model with features extracted from Wav2Vec, Hubert, and the cross-lingual XLSR model. Results suggest that speech representations pretrained on large unlabelled data can improve word error rate (WER) performance. In particular, features from the multilingual model led to lower WERs than Fbanks or models trained on a single language. Improvements were seen in English speakers with cerebral palsy caused dysarthria (UASpeech corpus), Spanish speakers with Parkinsonian dysarthria (PC-GITA corpus) and Italian speakers with paralysis-based dysarthria (EasyCall corpus). Compared to using Fbank features, XLSR-based features reduced WERs by 6.8%, 22.0%, and 7.0% for the UASpeech, PC-GITA, and EasyCall corpus, respectively."
   ],
   "doi": "10.21437/Interspeech.2022-10674"
  },
  "ardaillon22_interspeech": {
   "authors": [
    [
     "Luc",
     "Ardaillon"
    ],
    [
     "Nathalie",
     "Henrich"
    ],
    [
     "Olivier",
     "Perrotin"
    ]
   ],
   "title": "Voicing decision based on phonemes classification and spectral moments for whisper-to-speech conversion",
   "original": "10675",
   "page_count": 5,
   "order": 459,
   "p1": 2253,
   "pn": 2257,
   "abstract": [
    "Cordectomized or laryngectomized patients recover the ability to speak thanks to devices able to produce a natural-sounding voice source in real time. However, constant voicing can impair the naturalness and intelligibility of reconstructed speech. Voicing decision, consisting in identifying whether an uttered phone should be voiced or not, is investi- gated here as an automatic process in the context of whisper-to-speech (W2S) conversion systems. Whereas state-of-the-art approaches apply DNN techniques on high-dimensional acoustic features, we seek here a low-resource alternative approach for a perceptually-meaningful mapping between acoustic features and voicing decision, suitable for real-time applications. Our method first classifies whisper signal frames into phoneme classes based on their spectral centroid and spread, and then discriminate voiced phonemes from their unvoiced counterpart based on class-dependent spectral centroid thresholds. We compared our method to a simpler approach using a single centroid threshold on several databases of annotated whispers in both single-speaker and multi-speaker training setups. While both approaches reach voicing accuracy higher than 91%, the proposed method allows to avoid some systematic voicing decision errors, which may allow users to learn to adapt their speech in real-time to compensate for remaining voicing errors."
   ],
   "doi": "10.21437/Interspeech.2022-10675"
  },
  "chatzoudis22_interspeech": {
   "authors": [
    [
     "Gerasimos",
     "Chatzoudis"
    ],
    [
     "Manos",
     "Plitsis"
    ],
    [
     "Spyridoula",
     "Stamouli"
    ],
    [
     "Athanasia–Lida",
     "Dimou"
    ],
    [
     "Nassos",
     "Katsamanis"
    ],
    [
     "Vassilis",
     "Katsouros"
    ]
   ],
   "title": "Zero-Shot Cross-lingual Aphasia Detection using Automatic Speech Recognition",
   "original": "10681",
   "page_count": 5,
   "order": 444,
   "p1": 2178,
   "pn": 2182,
   "abstract": [
    "Aphasia is a common speech and language disorder, typically caused by a brain injury or a stroke, that affects millions of people worldwide. Detecting and assessing Aphasia in patients is a difficult, time-consuming process, and numerous attempts to automate it have been made, the most successful using machine learning models trained on aphasic speech data. Like in many medical applications, aphasic speech data is scarce and the problem is exacerbated in so-called ``low resource\" languages, which are, for this task, most languages excluding English. We attempt to leverage available data in English and achieve zero-shot aphasia detection in low-resource languages such as Greek and French, by using language-agnostic linguistic features. Current cross-lingual aphasia detection approaches rely on manually extracted transcripts. We propose an end-to-end pipeline using pre-trained Automatic Speech Recognition (ASR) models that share cross-lingual speech representations and are fine-tuned for our desired low-resource languages. To further boost our ASR model's performance, we also combine it with a language model. We show that our ASR-based end-to-end pipeline offers comparable results to previous setups using human-annotated transcripts."
   ],
   "doi": "10.21437/Interspeech.2022-10681"
  },
  "li22y_interspeech": {
   "authors": [
    [
     "Mohan",
     "Li"
    ],
    [
     "Rama Sanand",
     "Doddipatla"
    ],
    [
     "Catalin",
     "Zorila"
    ]
   ],
   "title": "Self-regularised Minimum Latency Training for Streaming Transformer-based Speech Recognition",
   "original": "10682",
   "page_count": 5,
   "order": 426,
   "p1": 2088,
   "pn": 2092,
   "abstract": [
    "This paper proposes a self-regularised minimum latency training (SR-MLT) method for streaming Transformer-based automatic speech recognition (ASR) systems. In previous works, latency was optimised by truncating the online attention weights based on the hard alignments obtained from conventional ASR models, without taking into account the potential loss of ASR accuracy. On the contrary, here we present a strategy to obtain the alignments as a part of the model training without external supervision. The alignments produced by the proposed method are dynamically regularised on the training data, such that the latency reduction does not result in the loss of ASR accuracy. SR-MLT is applied as a fine-tuning step on the pre-trained Transformer models that are based on either monotonic chunkwise attention (MoChA) or cumulative attention (CA) algorithms for online decoding. ASR experiments on the AIShell-1 and Librispeech datasets show that when applied on a decent pre-trained MoChA or CA baseline model, SR-MLT can effectively reduce the latency with the relative gains ranging from 11.8% to 39.5%. Furthermore, we also demonstrate that under certain accuracy levels, the models trained with SR-MLT can achieve lower latency when compared to those supervised using external hard alignments."
   ],
   "doi": "10.21437/Interspeech.2022-10682"
  },
  "dejong22_interspeech": {
   "authors": [
    [
     "Dorina",
     "de Jong"
    ],
    [
     "Aldo",
     "Pastore"
    ],
    [
     "Noël",
     "Nguyen"
    ],
    [
     "Alessandro",
     "D'Ausilio"
    ]
   ],
   "title": "Speech imitation skills predict automatic phonetic convergence: a GMM-UBM study on L2",
   "original": "10684",
   "page_count": 5,
   "order": 155,
   "p1": 769,
   "pn": 773,
   "abstract": [
    "Phonetic convergence is the observation that two interlocutors adapt their speech towards one another on an acoustic-phonetic level. It happens automatically and unconsciously, but people can also deliberately imitate others when asked to do so. Here, we investigate to what degree people converge to their interlocutor in a scripted dialogue when they are and when they are not explicitly requested to imitate their interlocutor. More specifically, we collected two separate data sets, where Italian- and French-native participants read English sentences aloud in alternating speaking turns. The results of both groups with different language backgrounds were compared against each other. We used a Gaussian mixture model – universal background model (GMM-UBM) to assess phonetic convergence on the sentence level. The GMM-UBM configuration was optimized to make the best distinction between speakers on validation data. We found that people start to converge to one another while interacting compared to the baseline and even more substantially when explicitly asked to do so. Results are robust across data sets. More importantly, the degree of implicit convergence people display is related to how good of an explicit imitator they are, supporting the claim that the two phenomena are based on the same neurocognitive process."
   ],
   "doi": "10.21437/Interspeech.2022-10684"
  },
  "girish22_interspeech": {
   "authors": [
    [
     "K V Vijay",
     "Girish"
    ],
    [
     "Srikanth",
     "Konjeti"
    ],
    [
     "Jithendra",
     "Vepa"
    ]
   ],
   "title": "Interpretabilty of Speech Emotion Recognition modelled using Self-Supervised Speech and Text Pre-Trained Embeddings",
   "original": "10685",
   "page_count": 5,
   "order": 911,
   "p1": 4496,
   "pn": 4500,
   "abstract": [
    "Speech emotion recognition (SER) is useful in many applications and is approached using signal processing techniques in the past and deep learning techniques recently. Human emotions are complex in nature and can vary widely within an utterance. The SER accuracy has improved using various multi- modal techniques but there is still some gap in understanding the model behaviour and expressing these complex emotions in a human interpretable form. In this work, we propose and define interpretability measures represented as a Human Level Indicator Matrix for an utterance and showcase it's effectiveness in both qualitative and quantitative terms. A word level interpretability is presented using an attention based sequence modelling of self-supervised speech and text pre-trained embeddings. Prosody features are also combined with the proposed model to see the efficacy at the word and utterance level. We provide insights into sub-utterance level emotion predictions for complex utterances where the emotion classes change within the utterance. We evaluate the model and provide the interpretations on the publicly available IEMOCAP dataset."
   ],
   "doi": "10.21437/Interspeech.2022-10685"
  },
  "vanniekerk22_interspeech": {
   "authors": [
    [
     "Daniel",
     "Van Niekerk"
    ],
    [
     "Anqi",
     "Xu"
    ],
    [
     "Branislav",
     "Gerazov"
    ],
    [
     "Paul Konstantin",
     "Krug"
    ],
    [
     "Peter",
     "Birkholz"
    ],
    [
     "Yi",
     "Xu"
    ]
   ],
   "title": "Exploration strategies for articulatory synthesis of complex syllable onsets",
   "original": "10689",
   "page_count": 5,
   "order": 128,
   "p1": 635,
   "pn": 639,
   "abstract": [
    "High-quality articulatory speech synthesis has many potential applications in speech science and technology. However, developing appropriate mappings from linguistic specification to articulatory gestures is difficult and time consuming. In this paper we construct an optimisation-based framework as a first step towards learning these mappings without manual intervention. We demonstrate the production of CCV syllables and discuss the quality of the articulatory gestures with reference to coarticulation."
   ],
   "doi": "10.21437/Interspeech.2022-10689"
  },
  "banno22_interspeech": {
   "authors": [
    [
     "Stefano",
     "Bannò"
    ],
    [
     "Bhanu",
     "Balusu"
    ],
    [
     "Mark",
     "Gales"
    ],
    [
     "Kate",
     "Knill"
    ],
    [
     "Konstantinos",
     "Kyriakopoulos"
    ]
   ],
   "title": "View-Specific Assessment of L2 Spoken English",
   "original": "10691",
   "page_count": 5,
   "order": 906,
   "p1": 4471,
   "pn": 4475,
   "abstract": [
    "The growing demand for learning English as a second language has increased interest in automatic approaches for assessing and improving spoken language proficiency. A significant challenge in this field is to provide interpretable scores and informative feedback to learners through individual viewpoints of learners' proficiency, as opposed to holistic scores. Thus far, holistic scoring remains commonly applied in large-scale commercial tests. As a result, an issue with more detailed evaluation is that human graders are generally trained to provide holistic scores. This paper investigates whether view-specific systems can be trained when only holistic scores are available. To enable this process, view-specific networks are defined where both their inputs and structure are adapted to focus on specific facets of proficiency. It is shown that it is possible to train such systems on holistic scores, such that they provide view-specific scores at evaluation time. View-specific networks are designed in this way for pronunciation, rhythm, text, use of parts of speech and grammatical accuracy. The relationships between the predictions of each system are investigated on the spoken part of the Linguaskill proficiency test. It is shown that the view-specific predictions are complementary in nature and capture different information about proficiency."
   ],
   "doi": "10.21437/Interspeech.2022-10691"
  },
  "li22z_interspeech": {
   "authors": [
    [
     "Katrina Kechun",
     "Li"
    ],
    [
     "Julia",
     "Schwarz"
    ],
    [
     "Jasper Hong",
     "Sim"
    ],
    [
     "Yixin",
     "Zhang"
    ],
    [
     "Elizabeth",
     "Buchanan-Worster"
    ],
    [
     "Brechtje",
     "Post"
    ],
    [
     "Kirsty",
     "McDougall"
    ]
   ],
   "title": "Recording and timing vocal responses in online experimentation",
   "original": "10697",
   "page_count": 5,
   "order": 822,
   "p1": 4053,
   "pn": 4057,
   "abstract": [
    "Cued shadowing is a psycholinguistic task that captures the response speed and accuracy of participants' vocal repetition of target words. Due to its simplicity, the paradigm is widely used as a naturalistic measure of speech processing. While the COVID-19 pandemic has driven the adaptation of many lab-based experiments to internet-based data collection, cued shadowing is not straightforward to adapt due to various challenges, including the precision of timing, efficient extraction of response latencies, and control over data quality. The current paper presents solutions to these challenges and describes the methodology for conducting cued shadowing of audio-video stimuli online with children and adults. The performance of two (semi-)automatic speech onset detection tools and two experimental designs are evaluated. The technique developed enables millisecond precision in response time measurement and has great potential for the inclusion of minority and hard-to-reach communities in future speech perception and production research."
   ],
   "doi": "10.21437/Interspeech.2022-10697"
  },
  "elhajal22_interspeech": {
   "authors": [
    [
     "Karl",
     "El Hajal"
    ],
    [
     "Milos",
     "Cernak"
    ],
    [
     "Pablo",
     "Mainar"
    ]
   ],
   "title": "MOSRA: Joint Mean Opinion Score and Room Acoustics Speech Quality Assessment",
   "original": "10698",
   "page_count": 5,
   "order": 671,
   "p1": 3313,
   "pn": 3317,
   "abstract": [
    "The acoustic environment can degrade speech quality during communication (e.g., video call, remote presentation, outside voice recording), and its impact is often unknown. Objective metrics for speech quality have proven challenging to develop given the multi-dimensionality of factors that affect speech quality and the difficulty of collecting labeled data. Hypothesizing the impact of acoustics on speech quality, this paper presents MOSRA: a non-intrusive multi-dimensional speech quality metric that can predict room acoustics parameters (SNR, STI, T60, DRR, and C50) alongside the overall mean opinion score (MOS) for speech quality. By explicitly optimizing the model to learn these room acoustics parameters, we can extract more informative features and improve the generalization for the MOS task when the training data is limited. Furthermore, we also show that this joint training method enhances the blind estimation of room acoustics, improving the performance of current state-of-the-art models. An additional side-effect of this joint prediction is the improvement in the explainability of the predictions, which is a valuable feature for many applications."
   ],
   "doi": "10.21437/Interspeech.2022-10698"
  },
  "dinarelli22_interspeech": {
   "authors": [
    [
     "Marco",
     "Dinarelli"
    ],
    [
     "Marco",
     "Naguib"
    ],
    [
     "François",
     "Portet"
    ]
   ],
   "title": "Toward Low-Cost End-to-End Spoken Language Understanding",
   "original": "10702",
   "page_count": 5,
   "order": 554,
   "p1": 2728,
   "pn": 2732,
   "abstract": [
    "Recent advances in spoken language understanding benefited from Self-Supervised models trained on large speech corpora. For French, the LeBenchmark project has made such models available and has led to impressive progress on several tasks including spoken language understanding. These advances have a non-negligible cost in terms of computation time and energy consumption. In this paper, we compare several learning strategies aiming at reducing such cost while keeping competitive performance. We measure the cost in terms of training time and electric energy consumption, hopefully promoting a comprehensive evaluation procedure. The experiments are performed on the FSC and MEDIA corpora, and show that it is possible to reduce the learning cost while maintaining state-of-the-art performance."
   ],
   "doi": "10.21437/Interspeech.2022-10702"
  },
  "meyer22b_interspeech": {
   "authors": [
    [
     "Sarina",
     "Meyer"
    ],
    [
     "Florian",
     "Lux"
    ],
    [
     "Pavel",
     "Denisov"
    ],
    [
     "Julia",
     "Koch"
    ],
    [
     "Pascal",
     "Tilli"
    ],
    [
     "Ngoc Thang",
     "Vu"
    ]
   ],
   "title": "Speaker Anonymization with Phonetic Intermediate Representations",
   "original": "10703",
   "page_count": 5,
   "order": 997,
   "p1": 4925,
   "pn": 4929,
   "abstract": [
    "In this work, we propose a speaker anonymization pipeline that leverages high quality automatic speech recognition and synthesis systems to generate speech conditioned on phonetic transcriptions and anonymized speaker embeddings. Using phones as the intermediate representation ensures near complete elimination of speaker identity information from the input while preserving the original phonetic content as much as possible. Our experimental results on LibriSpeech and VCTK corpora reveal two key findings: 1) although automatic speech recognition produces imperfect transcriptions, our neural speech synthesis system can handle such errors, making our system feasible and robust, and 2) combining speaker embeddings from different resources is beneficial and their appropriate normalization is crucial. Overall, our final best system outperforms significantly the baselines provided in the Voice Privacy Challenge 2020 in terms of privacy robustness against a lazy-informed attacker while maintaining high intelligibility and naturalness of the anonymized speech."
   ],
   "doi": "10.21437/Interspeech.2022-10703"
  },
  "batra22_interspeech": {
   "authors": [
    [
     "Mudit D.",
     "Batra"
    ],
    [
     "",
     "JAYESH"
    ],
    [
     "C.S.",
     "Ramalingam"
    ]
   ],
   "title": "Robust Pitch Estimation Using Multi-Branch CNN-LSTM and 1-Norm LP Residual",
   "original": "10704",
   "page_count": 5,
   "order": 723,
   "p1": 3573,
   "pn": 3577,
   "abstract": [
    "Pitch and voicing determination are important in many speech and audio signal processing applications. Even in the clean signal case their estimation can pose problems, and more so when noise is present. In this paper we propose a Multi-Branch CNN-LSTM based Temporal Neural Network for pitch and voicing determination. In addition, rather than using the raw waveform, we use the ℓ1-norm based LP residual as the input signal. These changes have made the proposed method more robust to SNR degradation, i.e., even though there is a slight fall in accuracy in the clean signal case, there is a 2.9% absolute increase in RPA for the 0 dB case when compared with the CREPE algorithm. More importantly, when the RPA tolerance is tightened, the fall in accuracy is smaller. This robustness has been achieved with only 1.79M parameters, which is an order of magnitude less than what is used in CREPE."
   ],
   "doi": "10.21437/Interspeech.2022-10704"
  },
  "wang22x_interspeech": {
   "authors": [
    [
     "Xiaofei",
     "Wang"
    ],
    [
     "Dongmei",
     "Wang"
    ],
    [
     "Naoyuki",
     "Kanda"
    ],
    [
     "Sefik",
     "Emre Eskimez"
    ],
    [
     "Takuya",
     "Yoshioka"
    ]
   ],
   "title": "Leveraging Real Conversational Data for Multi-Channel Continuous Speech Separation",
   "original": "10706",
   "page_count": 5,
   "order": 773,
   "p1": 3814,
   "pn": 3818,
   "abstract": [
    "Existing multi-channel continuous speech separation (CSS) models are heavily dependent on supervised data - either simulated data which causes data mismatch between the training and real-data testing, or the real transcribed overlapping data, which is difficult to be acquired, hindering further improvements in the conversational/meeting transcription tasks. In this paper, we propose a three-stage training scheme for the CSS model that can leverage both supervised data and extra large-scale unsupervised real-world conversational data. The scheme consists of two conventional training approaches---pre-training using simulated data and ASR-loss-based training using transcribed data---and a novel continuous semi-supervised training between the two, in which the CSS model is further trained by using real data based on the teacher-student learning framework. We apply this scheme to an array-geometry-agnostic CSS model, which can use the multi-channel data collected from any microphone array. Large-scale meeting transcription experiments are carried out on both Microsoft internal meeting data and the AMI meeting corpus. The steady improvement by each training stage has been observed, showing the effect of the proposed method that enables to leverage real conversational data for CSS model training."
   ],
   "doi": "10.21437/Interspeech.2022-10706"
  },
  "ali22_interspeech": {
   "authors": [
    [
     "Mohamed Nabih",
     "Ali"
    ],
    [
     "Alessio",
     "Brutti"
    ],
    [
     "Falavigna",
     "Daniele"
    ]
   ],
   "title": "Enhancing Embeddings for Speech Classification in Noisy Conditions",
   "original": "10707",
   "page_count": 5,
   "order": 595,
   "p1": 2933,
   "pn": 2937,
   "abstract": [
    "Robustness against noise is critical for several speech applications in real-world environments. In general, to improve the robustness, a speech enhancement front-end is integrated as a preprocessing stage, often jointly trained with the network backend to reduce the impact of distortions and artifacts on the performance. Recently, the use of speech representation computed using pre-trained models on large amounts of data, as Wav2Vec, has proved to be effective in a variety of speech processing and classification tasks. However, the performance of these models, although very robust, deteriorates in presence of environmental noise. In this paper, we investigate how enhancement can be applied in neural speech classification architectures employing pre-trained speech embeddings. We investigate two approaches: one applies time-domain enhancement prior to extracting the embeddings; the other employs a convolutional neural network to map the noisy embeddings to the corresponding clean ones. Exhaustive experiments on the Fluent Speech Commands and Google Speech Commands corpora, contaminated with noises from the Microsoft Scalable Noisy Speech Dataset, sheds light and provide insights about the most promising enhancement training approaches."
   ],
   "doi": "10.21437/Interspeech.2022-10707"
  },
  "li22aa_interspeech": {
   "authors": [
    [
     "Xinjian",
     "Li"
    ],
    [
     "Florian",
     "Metze"
    ],
    [
     "David R.",
     "Mortensen"
    ],
    [
     "Alan W",
     "Black"
    ],
    [
     "Shinji",
     "Watanabe"
    ]
   ],
   "title": "ASR2K: Speech Recognition for Around 2000 Languages without Audio",
   "original": "10712",
   "page_count": 5,
   "order": 989,
   "p1": 4885,
   "pn": 4889,
   "abstract": [
    "Most recent speech recognition models rely on large supervised datasets, which are unavailable for many low-resource languages. In this work, we present a speech recognition pipeline that does not require any audio for the target language. The only assumption is that we have access to raw text datasets or a set of n-gram statistics. Our speech pipeline consists of three components: acoustic, pronunciation, and language models. Unlike the standard pipeline, our acoustic and pronunciation models use multilingual models without any supervision. The language model is built using n-gram statistics or the raw text dataset. We build speech recognition for 1909 languages by combining it with Crubadan: a large endangered languages n-gram database. Furthermore, we test our approach on 129 languages across two datasets: Common Voice and CMU Wilderness dataset. We achieve 50% CER and 74% WER on the Wilderness dataset with Crubadan statistics only and improve them to 45% CER and 69% WER when using only 10000 raw text utterances."
   ],
   "doi": "10.21437/Interspeech.2022-10712"
  },
  "singh22_interspeech": {
   "authors": [
    [
     "Arshdeep",
     "Singh"
    ],
    [
     "Mark D.",
     "Plumbley"
    ]
   ],
   "title": "A Passive Similarity based CNN Filter Pruning for Efficient Acoustic Scene Classification",
   "original": "10714",
   "page_count": 5,
   "order": 495,
   "p1": 2433,
   "pn": 2437,
   "abstract": [
    "We present a method to develop low-complexity convolutional neural networks (CNNs) for acoustic scene classification (ASC). The large size and high computational complexity of typical CNNs is a bottleneck for their deployment on resource-constrained devices. We propose a passive filter pruning framework, where a few convolutional filters from the CNNs are eliminated to yield compressed CNNs. Our hypothesis is that similar filters produce similar responses and give redundant information allowing such filters to be eliminated from the network. To identify similar filters, a cosine distance based greedy algorithm is proposed. A fine-tuning process is then performed to regain much of the performance lost due to filter elimination. To perform efficient fine-tuning, we analyze how the performance varies as the number of fine-tuning training examples changes. An experimental evaluation of the proposed framework is performed on the publicly available DCASE 2021 Task 1A baseline network trained for ASC. The proposed method is simple, reduces computations per inference by 27%, with 25% fewer parameters, with less than 1% drop in accuracy."
   ],
   "doi": "10.21437/Interspeech.2022-10714"
  },
  "weiran22b_interspeech": {
   "authors": [
    [
     "Wang",
     "Weiran"
    ],
    [
     "Ke",
     "Hu"
    ],
    [
     "Tara",
     "Sainath"
    ]
   ],
   "title": "Streaming Align-Refine for Non-autoregressive Deliberation",
   "original": "10715",
   "page_count": 5,
   "order": 344,
   "p1": 1696,
   "pn": 1700,
   "abstract": [
    "We propose a streaming non-autoregressive (non-AR) decoding algorithm to deliberate the hypothesis alignment of a streaming RNN-T model. Our algorithm facilitates a simple greedy decoding procedure, and at the same time is capable of producing the decoding result at each frame with limited right context, thus enjoying both high efficiency and low latency. These advantages are achieved by converting the offline Align-Refine algorithm to be streaming-compatible, with a novel transformer decoder architecture that performs local self-attentions for both text and audio, and a time-aligned cross-attention at each layer. Furthermore, we perform discriminative training of our model with the minimum word error rate (MWER) criterion, which has not been done in the non-AR decoding literature. Experiments on voice search datasets and Librispeech show that with reasonable right context, our streaming model performs as well as the offline counterpart, and discriminative training leads to further WER gain when the first-pass model has small capacity."
   ],
   "doi": "10.21437/Interspeech.2022-10715"
  },
  "spinu22_interspeech": {
   "authors": [
    [
     "Laura",
     "Spinu"
    ],
    [
     "Ioana",
     "Vasilescu"
    ],
    [
     "Lori",
     "Lamel"
    ],
    [
     "Jason",
     "Lilley"
    ]
   ],
   "title": "Voicing neutralization in Romanian fricatives across different speech styles",
   "original": "10716",
   "page_count": 5,
   "order": 273,
   "p1": 1342,
   "pn": 1346,
   "abstract": [
    "Romance languages such as Italian or Spanish preserve fricative voicing contrasts in word-final position, while their neutralization has been reported for European Portuguese, but the behavior of Romanian fricatives remains understudied. Previous work with Romanian fricatives suggests a pattern of final devoicing but, due to the specific properties of the corpus analyzed, it is unclear if this is limited to the presence of secondary palatalization and/or the result of morphological conditioning. In this study, we apply speech processing tools to investigate the acoustic characteristics of the voicing contrast in fricatives in contemporary spoken Romanian. We examine a corpus of prepared speech from newscasts and semi-spontaneous TV debates (86 speakers) and compare our results to previous findings from a corpus of controlled experimental speech (31 speakers). Our classification tool employs cepstral coefficients and hidden Markov model (HMM)-defined temporal regions to identify the properties of these segments. Our findings conform to typological predictions regarding partial devoicing in coda position, especially at more posterior places, but we find little support for voicing neutralization in Romanian fricatives more generally. Our study thus documents the properties of Romanian fricatives and contributes to our understanding of the dynamics of contrast maintenance in phonological systems."
   ],
   "doi": "10.21437/Interspeech.2022-10716"
  },
  "tzinis22_interspeech": {
   "authors": [
    [
     "Efthymios",
     "Tzinis"
    ],
    [
     "Gordon",
     "Wichern"
    ],
    [
     "Aswin Shanmugam",
     "Subramanian"
    ],
    [
     "Paris",
     "Smaragdis"
    ],
    [
     "Jonathan",
     "Le Roux"
    ]
   ],
   "title": "Heterogeneous Target Speech Separation",
   "original": "10717",
   "page_count": 5,
   "order": 364,
   "p1": 1796,
   "pn": 1800,
   "abstract": [
    "We introduce a new paradigm for single-channel target source separation where the sources of interest can be distinguished using non-mutually exclusive concepts (e.g., loudness, gender, language, spatial location, etc). Our proposed heterogeneous separation framework can seamlessly leverage datasets with large distribution shifts and learn cross-domain representations under a variety of concepts used as conditioning. Our experiments show that training separation models with heterogeneous conditions facilitates the generalization to new concepts with unseen out-of-domain data while also performing substantially higher than single-domain specialist models. Notably, such training leads to more robust learning of new harder source separation discriminative concepts and can yield improvements over permutation invariant training with oracle source selection. We analyze the intrinsic behavior of source separation training with heterogeneous metadata and propose ways to alleviate emerging problems with challenging separation conditions. We release the collection of preparation recipes for all datasets used to further promote research towards this challenging task."
   ],
   "doi": "10.21437/Interspeech.2022-10717"
  },
  "manghat22_interspeech": {
   "authors": [
    [
     "Sreeram",
     "Manghat"
    ],
    [
     "Sreeja",
     "Manghat"
    ],
    [
     "Tanja",
     "Schultz"
    ]
   ],
   "title": "Normalization of code-switched text for speech synthesis",
   "original": "10719",
   "page_count": 5,
   "order": 871,
   "p1": 4297,
   "pn": 4301,
   "abstract": [
    "In multilingual communities, code-switching is a common phenomenon. Due to the increase in usage of social media, high level of code-switching is present in social media text as well. These code-switched social media texts are often seen written in monolingual script. Text normalization techniques of the conventional Text-to-Speech (TTS) and machine translation systems may not be able to handle such code-switched texts. Malayalam is a low resource Indic language. Conversational Malayalam contains high level of inter-sentential, intra-sentential as well as intra-word code-switching with English. This paper specifies the techniques for handling Malayalam-English code-switched text data. Evaluation results of experiments conducted on Malayalam-English code-switched data is also presented."
   ],
   "doi": "10.21437/Interspeech.2022-10719"
  },
  "zeineldeen22_interspeech": {
   "authors": [
    [
     "Mohammad",
     "Zeineldeen"
    ],
    [
     "Jingjing",
     "Xu"
    ],
    [
     "Christoph",
     "Lüscher"
    ],
    [
     "Ralf",
     "Schlüter"
    ],
    [
     "Hermann",
     "Ney"
    ]
   ],
   "title": "Improving the Training Recipe for a Robust Conformer-based Hybrid Model",
   "original": "10723",
   "page_count": 5,
   "order": 211,
   "p1": 1036,
   "pn": 1040,
   "abstract": [
    "Speaker adaptation is important to build robust automatic speech recognition (ASR) systems. In this work, we investigate various methods for speaker adaptive training (SAT) based on feature-space approaches for a conformer-based acoustic model (AM) on the Switchboard 300h dataset. We propose a method, called Weighted-Simple-Add, which adds weighted speaker information vectors to the input of the multi-head self-attention module of the conformer AM. Using this method for SAT, we achieve 3.5% and 4.5% relative improvement in terms of WER on the CallHome part of Hub5'00 and Hub5'01 respectively. Moreover, we build on top of our previous work where we proposed a novel and competitive training recipe for a conformer-based hybrid AM. We extend and improve this recipe where we achieve 11% relative improvement in terms of word-error-rate (WER) on Switchboard 300h Hub5'00 dataset. We also make this recipe efficient by reducing the total number of parameters by 34% relative."
   ],
   "doi": "10.21437/Interspeech.2022-10723"
  },
  "hutin22_interspeech": {
   "authors": [
    [
     "Mathilde",
     "Hutin"
    ],
    [
     "Martine",
     "Adda-Decker"
    ],
    [
     "Lori",
     "Lamel"
    ],
    [
     "Ioana",
     "Vasilescu"
    ]
   ],
   "title": "When Phonetics Meets Morphology: Intervocalic Voicing Within and Across Words in Romance Languages",
   "original": "10725",
   "page_count": 5,
   "order": 696,
   "p1": 3438,
   "pn": 3442,
   "abstract": [
    "Intervocalic voicing is a process whereby a voiceless segment such as /ptk/ is realized as partially or totally voiced [bdg] when occurring between two vowels. It supposedly happens across-the-board in connected speech, where phonetics is blind to morphological boundaries (in our case, word-edges) but only word-internal intervocalic voicing actually phonologizes, as in Lat. vita → Spa. vida. This means that a change currently happening can be identified if phonetic variation patterns differently at word-edges and word-internally. We provide an analysis of ~1000h of automatically aligned connected speech in five Romance languages to investigate intervocalic voicing of /ptk/ – as well as resistance to devoicing of /bdg/ – as a function of the stop's position in the word, i.e., internal (VCV), initial (V#CV), final (VC#V) and in isolation (V#C#V). Results show that voicing alternations in Portuguese are sensitive to word-edges while French and Romanian are sensitive to the right word-edge only and Italian shows no difference at word-edges or word-internally. However, the surprising result is that word-edges do not only show resistance to intervocalic voicing, but even tend towards devoicing of voiced stops."
   ],
   "doi": "10.21437/Interspeech.2022-10725"
  },
  "kuang22b_interspeech": {
   "authors": [
    [
     "Jianjing",
     "Kuang"
    ],
    [
     "May Pik Yu",
     "Chan"
    ],
    [
     "Nari",
     "Rhee"
    ],
    [
     "Mark",
     "Liberman"
    ],
    [
     "Hongwei",
     "Ding"
    ]
   ],
   "title": "The mapping between syntactic and prosodic phrasing in English and Mandarin",
   "original": "10726",
   "page_count": 5,
   "order": 697,
   "p1": 3443,
   "pn": 3447,
   "abstract": [
    "To achieve a better understanding of the relationship between syntactic parsing and prosodic phrasing in speech production cross-linguistically, we investigated how syntactic constituents map onto a high-dimensional acoustic space of prosodic phrasing in two read-speech corpora of Mandarin and English with syntactic annotations. The left and right edges of the constituents from the syntactic parsings were used as a proxy for the relative strength of the syntactic boundaries. A wide range of acoustic cues capturing pauses, duration cues, F0, energy, and voice quality cues were extracted. Our results showed that there is a clear correlation between the strength of syntactic boundary and prosodic phrasing, and the syntax-prosody mapping is much stronger for the right boundaries than for the left boundaries. Moreover, the prosodic realization of syntactic boundaries is gradient (especially for right boundaries), and acoustic cues scale up or down collectively to indicate different extents of phrasing, rather than being specific to certain levels of phrasing. We discuss the findings' implications in relation to the prosodic hierarchy and the nature of the prosody-syntax interface."
   ],
   "doi": "10.21437/Interspeech.2022-10726"
  },
  "lu22c_interspeech": {
   "authors": [
    [
     "Yen-Ju",
     "Lu"
    ],
    [
     "Xuankai",
     "Chang"
    ],
    [
     "Chenda",
     "Li"
    ],
    [
     "Wangyou",
     "Zhang"
    ],
    [
     "Samuele",
     "Cornell"
    ],
    [
     "Zhaoheng",
     "Ni"
    ],
    [
     "Yoshiki",
     "Masuyama"
    ],
    [
     "Brian",
     "Yan"
    ],
    [
     "Robin",
     "Scheibler"
    ],
    [
     "Zhong-Qiu",
     "Wang"
    ],
    [
     "Yu",
     "Tsao"
    ],
    [
     "Yanmin",
     "Qian"
    ],
    [
     "Shinji",
     "Watanabe"
    ]
   ],
   "title": "ESPnet-SE++: Speech Enhancement for Robust Speech Recognition, Translation, and Understanding",
   "original": "10727",
   "page_count": 5,
   "order": 1106,
   "p1": 5458,
   "pn": 5462,
   "abstract": [
    "This paper presents recent progress on integrating speech separation and enhancement (SSE) into the ESPnet toolkit. Compared with the previous ESPnet-SE work, numerous features have been added, including recent state-of-the-art speech enhancement models with their respective training and evaluation recipes. Importantly, a new interface has been designed to flexibly combine speech enhancement front-ends with other tasks, including automatic speech recognition (ASR), speech translation (ST), and spoken language understanding (SLU). To showcase such integration, we performed experiments on carefully designed synthetic datasets for noisy-reverberant multi-channel ST and SLU tasks, which can be used as benchmark corpora for future research. In addition to these new tasks, we also use CHiME-4 and WSJ0-2Mix to benchmark multi- and single-channel SE approaches. Results show that the integration of SE front-ends with back-end tasks is a promising research direction even for tasks besides ASR, especially in the multi-channel scenario. The code is available online at \\url{https://github.com/ESPnet/ESPnet}. The multi-channel ST and SLU datasets, which are another contribution of this work, are released on HuggingFace."
   ],
   "doi": "10.21437/Interspeech.2022-10727"
  },
  "nguyen22d_interspeech": {
   "authors": [
    [
     "Tuan Nam",
     "Nguyen"
    ],
    [
     "Ngoc-Quan",
     "Pham"
    ],
    [
     "Alexander",
     "Waibel"
    ]
   ],
   "title": "Accent Conversion using Pre-trained Model and Synthesized Data from Voice Conversion",
   "original": "10729",
   "page_count": 5,
   "order": 525,
   "p1": 2583,
   "pn": 2587,
   "abstract": [
    "Accent conversion (AC) aims to generate synthetic audios by changing the pronunciation pattern and prosody of source speakers (in source audios) while preserving voice quality and linguistic content. There has not been a parallel corpus that contains pairs of audios having the same contents yet coming from the same speakers in different accents, the authors hence work on a solution to synthesize one as training input. The training pipeline is conducted via two steps. First, a voice conversion (VC) model is constructed to synthesize a training data set, containing pairs of audios in the same voice but two different accents. Second, an AC model is trained with the synthesized data to convert a source accented speech to a target accented speech. Given the recognized success of self-supervised learning speech representation (wav2vec 2.0) on certain speech problems such as VC, speech recognition, speech translation, and speech-to-speech translation, we adopt this architecture with some customization to train the AC model in the second step. With just 9-hour synthesized training data, the encoder initialized by the weight of the pre-trained wav2vec 2.0 model outperforms the LSTM-based encoder."
   ],
   "doi": "10.21437/Interspeech.2022-10729"
  },
  "turetzky22_interspeech": {
   "authors": [
    [
     "Arnon",
     "Turetzky"
    ],
    [
     "Tzvi",
     "Michelson"
    ],
    [
     "Yossi",
     "Adi"
    ],
    [
     "Shmuel",
     "Peleg"
    ]
   ],
   "title": "Deep Audio Waveform Prior",
   "original": "10735",
   "page_count": 5,
   "order": 596,
   "p1": 2938,
   "pn": 2942,
   "abstract": [
    "Convolutional neural network contain strong priors for generating natural looking images [1]. These priors enable image denoising, super resolution, and inpainting in an unsupervised manner. Previous attempts to demonstrate similar ideas in audio, namely deep audio priors, (i) use hand picked architectures such as harmonic convolutions, (ii) only work with spectrogram input, and (iii) have been used mostly for eliminating Gaussian noise [2]. In this work we show that existing SOTA architectures for audio source separation contain deep priors even when working with the raw waveform. Deep priors can be discovered by training a neural network to generate a single corrupted signal when given white noise as input. A network with relevant deep priors is likely to generate a cleaner version of the signal before converging on the corrupted signal. We demonstrate this restoration effect with several corruptions: background noise, reverberations, and a gap in the signal (audio inpainting)."
   ],
   "doi": "10.21437/Interspeech.2022-10735"
  },
  "sklyar22_interspeech": {
   "authors": [
    [
     "Ilya",
     "Sklyar"
    ],
    [
     "Anna",
     "Piunova"
    ],
    [
     "Christian",
     "Osendorfer"
    ]
   ],
   "title": "Separator-Transducer-Segmenter: Streaming Recognition and Segmentation of Multi-party Speech",
   "original": "10738",
   "page_count": 5,
   "order": 902,
   "p1": 4451,
   "pn": 4455,
   "abstract": [
    "Streaming recognition and segmentation of multi-party conversations with overlapping speech is crucial for the next generation of voice assistant applications. In this work we address its challenges discussed in the previous work on multi-turn recurrent neural network transducer (MT-RNN-T) with a novel approach, separator-transducer-segmenter (STS), that enables tighter integration of speech separation, recognition and segmentation in a single model. First, we propose a new segmentation modeling strategy through start-of-turn and end-of-turn tokens that improves segmentation without recognition accuracy degradation. Second, we further improve both speech recognition and segmentation accuracy through an emission regularization method, FastEmit, and multi-task training with speech activity information as an additional training signal. Third, we experiment with end-of-turn emission latency penalty to improve end-point detection for each speaker turn. Finally, we establish a novel framework for segmentation analysis of multi-party conversations through emission latency metrics. With our best model, we report 4.6% abs. turn counting accuracy improvement and 17% rel. word error rate (WER) improvement on LibriCSS dataset compared to the previously published work."
   ],
   "doi": "10.21437/Interspeech.2022-10738"
  },
  "yadavalli22_interspeech": {
   "authors": [
    [
     "Aditya",
     "Yadavalli"
    ],
    [
     "Ganesh",
     "Mirishkar"
    ],
    [
     "Anil Kumar",
     "Vuppala"
    ]
   ],
   "title": "Multi-Task End-to-End Model for Telugu Dialect and Speech Recognition",
   "original": "10739",
   "page_count": 5,
   "order": 282,
   "p1": 1387,
   "pn": 1391,
   "abstract": [
    "Conventional Automatic Speech Recognition (ASR) systems are susceptible to dialect variations within a language, thereby adversely affecting the ASR. Therefore, the current practice is to use dialect-specific ASRs. However, dialect-specific information or data is hard to obtain making it difficult to build dialect-specific ASRs. Furthermore, it is cumbersome to maintain multiple dialect-specific ASR systems for each language. We build a unified multi-dialect End-to-End ASR that removes the need for a dialect recognition block and the need to maintain multiple dialect-specific ASRs for three Telugu regional dialects: Telangana, Coastal Andhra, and Rayalaseema. We find that pooling the data and training a multi-dialect ASR benefits the low-resource dialect the most -- an improvement of over 9.71% in relative Word Error Rate (WER). Subsequently, we experiment with multi-task ASRs where the primary task is to transcribe the audio and the secondary task is to predict the dialect. We do this by adding a Dialect ID to the output targets. Such a model outperforms naive multi-dialect ASRs by up to 8.24% in relative WER. Additionally, we test this model on a dialect recognition task and find that it outperforms strong baselines by 6.14% in accuracy."
   ],
   "doi": "10.21437/Interspeech.2022-10739"
  },
  "kuhlmann22_interspeech": {
   "authors": [
    [
     "Michael",
     "Kuhlmann"
    ],
    [
     "Fritz",
     "Seebauer"
    ],
    [
     "Janek",
     "Ebbers"
    ],
    [
     "Petra",
     "Wagner"
    ],
    [
     "Reinhold",
     "Haeb-Umbach"
    ]
   ],
   "title": "Investigation into Target Speaking Rate Adaptation for Voice Conversion",
   "original": "10740",
   "page_count": 5,
   "order": 998,
   "p1": 4930,
   "pn": 4934,
   "abstract": [
    "Disentangling speaker and content attributes of a speech signal into separate latent representations followed by decoding the content with an exchanged speaker representation is a popular approach for voice conversion, which can be trained with non-parallel and unlabeled speech data. However, previous approaches perform disentanglement only implicitly via some sort of information bottleneck or normalization, where it is usually hard to find a good trade-off between voice conversion and content reconstruction. Further, previous works usually do not consider an adaptation of the speaking rate to the target speaker or they put some major restrictions to the data or use case. Therefore, the contribution of this work is two-fold. First, we employ an explicit and fully unsupervised disentanglement approach, which has previously only been used for representation learning, and show that it allows to obtain both superior voice conversion and content reconstruction. Second, we investigate simple and generic approaches to linearly adapt the length of a speech signal, and hence the speaking rate, to a target speaker and show that the proposed adaptation allows to increase the speaking rate similarity with respect to the target speaker."
   ],
   "doi": "10.21437/Interspeech.2022-10740"
  },
  "vanderreydt22_interspeech": {
   "authors": [
    [
     "Geoffroy",
     "Vanderreydt"
    ],
    [
     "François",
     "REMY"
    ],
    [
     "Kris",
     "Demuynck"
    ]
   ],
   "title": "Transfer Learning from Multi-Lingual Speech Translation Benefits Low-Resource Speech Recognition",
   "original": "10744",
   "page_count": 5,
   "order": 619,
   "p1": 3053,
   "pn": 3057,
   "abstract": [
    "In this article, we propose a simple yet effective approach to train an end-to-end speech recognition system on languages with limited resources by leveraging a large pre-trained wav2vec2.0 model fine-tuned on a multi-lingual speech translation task. We show that the weights of this model form an excellent initialization for Connectionist Temporal Classification (CTC) speech recognition, a different but closely related task. We explore the benefits of this initialization for various languages, both in-domain and out-of-domain for the speech translation task. Our experiments on the CommonVoice dataset confirm that our approach performs significantly better in-domain, and is often better out-of-domain too. This method is particularly relevant for Automatic Speech Recognition (ASR) with limited data and/or compute budget during training."
   ],
   "doi": "10.21437/Interspeech.2022-10744"
  },
  "ghosh22b_interspeech": {
   "authors": [
    [
     "Sreyan",
     "Ghosh"
    ],
    [
     "Samden",
     "Lepcha"
    ],
    [
     "S",
     "Sakshi"
    ],
    [
     "Rajiv Ratn",
     "Shah"
    ],
    [
     "Srinivasan",
     "Umesh"
    ]
   ],
   "title": "DeToxy: A Large-Scale Multimodal Dataset for Toxicity Classification in Spoken Utterances",
   "original": "10752",
   "page_count": 5,
   "order": 1049,
   "p1": 5185,
   "pn": 5189,
   "abstract": [
    "Toxic speech is regarded as one of the crucial issues plaguing online social media today. Most recent work on toxic speech detection is constrained to the modality of text and written conversations with very limited work on toxicity detection from spoken utterances or using the modality of speech. In this paper, we introduce a new dataset DeToxy, the first publicly available toxicity annotated dataset for the English language. DeToxy is sourced from various openly available speech databases and consists of over 2 million utterances. We believe that our dataset would act as a benchmark for the relatively new and unexplored Spoken Language Processing (SLP) task of detecting toxicity from spoken utterances and boost further research in this space. Finally, we also provide strong unimodal baselines for our dataset and compare traditional two-step cascade and End-to-End (E2E) approaches. Our experiments show that in the case of spoken utterances, text-based approaches are largely dependent on gold human-annotated transcripts for their performance and also suffer from the problem of keyword bias. However, the presence of speech files in DeToxy helps facilitates the development of E2E speech models which alleviate both the above-stated problems by better capturing speech clues."
   ],
   "doi": "10.21437/Interspeech.2022-10752"
  },
  "chernyak22_interspeech": {
   "authors": [
    [
     "Bronya Roni",
     "Chernyak"
    ],
    [
     "Talia",
     "Ben Simon"
    ],
    [
     "Yael",
     "Segal"
    ],
    [
     "Jeremy",
     "Steffman"
    ],
    [
     "Eleanor",
     "Chodroff"
    ],
    [
     "Jennifer",
     "Cole"
    ],
    [
     "Joseph",
     "Keshet"
    ]
   ],
   "title": "DeepFry: Identifying Vocal Fry Using Deep Neural Networks",
   "original": "10756",
   "page_count": 5,
   "order": 724,
   "p1": 3578,
   "pn": 3582,
   "abstract": [
    "Vocal fry or creaky voice refers to a voice quality characterized by irregular glottal opening and low pitch. It occurs in diverse languages and is prevalent in American English, where it is used not only to mark phrase finality, but also sociolinguistic factors and affect. Due to its irregular periodicity, creaky voice challenges automatic speech processing and recognition systems, particularly for languages where creak is frequently used. This paper proposes a deep learning model to detect creaky voice in fluent speech. The model is composed of an encoder and a classifier trained together. The encoder takes the raw waveform and learns a representation using a convolutional neural network. The classifier is implemented as a multi-headed fully-connected network trained to detect creaky voice, voicing, and pitch, where the last two are used to refine creak prediction. The model is trained and tested on speech of American English speakers, annotated for creak by trained phoneticians. We evaluated the performance of our system using two encoders: one is tailored for the task, and the other is based on a state-of-the-art unsupervised representation. Results suggest our best-performing system has improved recall and F1 scores compared to previous methods on unseen data."
   ],
   "doi": "10.21437/Interspeech.2022-10756"
  },
  "mariotte22_interspeech": {
   "authors": [
    [
     "Theo",
     "Mariotte"
    ],
    [
     "Anthony",
     "Larcher"
    ],
    [
     "Silvio",
     "Montrésor"
    ],
    [
     "Jean-Hugh",
     "Thomas"
    ]
   ],
   "title": "Microphone Array Channel Combination Algorithms for Overlapped Speech Detection",
   "original": "10758",
   "page_count": 5,
   "order": 939,
   "p1": 4636,
   "pn": 4640,
   "abstract": [
    "Overlapped speech occurs when multiple speakers are simultaneously active. This may lead to severe performance degradation in automatic speech processing systems such as speaker diarization. Overlapped speech detection (OSD) aims at detecting time segments in which several speakers are simultaneously active. Recent deep neural network architectures have shown impressive results in the close-talk scenario. However, performance tends to deteriorate in the context of distant speech. Microphone arrays are often considered under these conditions to record signals including spatial information. This paper investigates the use of the self-attention channel combinator (SACC) system as a feature extractor for OSD. This model is also extended in the complex space (cSACC) to improve the interpretability of the approach. Results show that distant OSD performance with self-attentive models gets closer to the near-field condition. A detailed analysis of the cSACC combination-weights is also conducted showing that the self-attention module focuses attention on the speakers' direction."
   ],
   "doi": "10.21437/Interspeech.2022-10758"
  },
  "liebig22_interspeech": {
   "authors": [
    [
     "Leon",
     "Liebig"
    ],
    [
     "Christoph",
     "Wagner"
    ],
    [
     "Alexander",
     "Mainka"
    ],
    [
     "Peter",
     "Birkholz"
    ]
   ],
   "title": "An investigation of regression-based prediction of the femininity or masculinity in speech of transgender people",
   "original": "10759",
   "page_count": 5,
   "order": 947,
   "p1": 4676,
   "pn": 4680,
   "abstract": [
    "Transgender individuals often seek for voice modification to more closely have their voice matched with their new sex, and avoid potential stigmatization or even discrimination. Whereas treatment options such as voice therapy or surgery exist, a quantitative measure of the treatment outcome is missing. In this paper, we therefore propose a novel regression-based method to predict the perceived femininity or masculinity of a speaker's voice. To this end, 86 speakers (34 male, 35 female, 17 transgender) were recorded reading aloud a German standard passage. Subsequently a group of 28 laypersons and 13 experts rated the femininity/masculinity of these speech samples. Each spoken utterance was automatically analysed with respect to nine different pitch-, resonance- and voice quality-related acoustic features. The ratings were the targets for three prediction models (linear, logistic and decision tree regression) based on the extracted features. The results show that, generally, f0 and the vocal tract length (VTL) are the main predictors. Furthermore, the continuous outcome logistic regression model with f0, smoothed cepstral peak prominence (CPPS), Jitter and VTL as input features performed best and achieved promising results with a cross-validated root mean-squared error of 0.117 on the normalized ratings [0,1]."
   ],
   "doi": "10.21437/Interspeech.2022-10759"
  },
  "zaidi22b_interspeech": {
   "authors": [
    [
     "Julian",
     "Zaïdi"
    ],
    [
     "Hugo",
     "Seuté"
    ],
    [
     "Benjamin",
     "van Niekerk"
    ],
    [
     "Marc-André",
     "Carbonneau"
    ]
   ],
   "title": "Daft-Exprt: Cross-Speaker Prosody Transfer on Any Text for Expressive Speech Synthesis",
   "original": "10761",
   "page_count": 5,
   "order": 930,
   "p1": 4591,
   "pn": 4595,
   "abstract": [
    "This paper presents Daft-Exprt, a multi-speaker acoustic model advancing the state-of-the-art for cross-speaker prosody transfer on any text. This is one of the most challenging, and rarely directly addressed, task in speech synthesis, especially for highly expressive data. Daft-Exprt uses FiLM conditioning layers to strategically inject different prosodic information in all parts of the architecture. The model explicitly encodes traditional low-level prosody features such as pitch, loudness and duration, but also higher level prosodic information that helps generating convincing voices in highly expressive styles. Speaker identity and prosodic information are disentangled through an adversarial training strategy that enables accurate prosody transfer across speakers. Experimental results show that Daft-Exprt significantly outperforms strong baselines on inter-text cross-speaker prosody transfer tasks, while yielding naturalness comparable to state-of-the-art expressive models. Moreover, results indicate that the model discards speaker identity information from the prosody representation, and consistently generate speech with the desired voice. We publicly release our code and provide speech samples from our experiments."
   ],
   "doi": "10.21437/Interspeech.2022-10761"
  },
  "antony22_interspeech": {
   "authors": [
    [
     "Ansen",
     "Antony"
    ],
    [
     "Sumanth Reddy",
     "Kota"
    ],
    [
     "Akhilesh",
     "Lade"
    ],
    [
     "Spoorthy",
     "V"
    ],
    [
     "Shashidhar G.",
     "Koolagudi"
    ]
   ],
   "title": "An Improved Transformer Transducer Architecture for Hindi-English Code Switched Speech Recognition",
   "original": "10763",
   "page_count": 5,
   "order": 633,
   "p1": 3123,
   "pn": 3127,
   "abstract": [
    "Due to the extensive usage of technology in many languages throughout the world, interest in Automatic Speech Recognition (ASR) systems for Code-Switching (CS) in speech has grown in recent years. Several studies have shown that End-to-End (E2E) ASR is easier to adopt and works much better in monolingual settings. E2E systems are likewise widely recognised for requiring massive quantities of labelled speech data. Since there is a scarcity in the availability of large amount of CS speech, E2E ASR takes longer computation time and does not offer promising results. In this work, an E2E ASR model system using a transformer-transducer architecture is introduced for code-switched Hindi-English speech, and also addressed training data scarcity by leveraging the vastly available monolingual data. Specifically, the language-specific modules in the Transformer are pre-trained by leveraging the vastly available single language speech datasets. The proposed method also provides a Word Error Rate (WER) of 29.63% and a Transliterated Word Error Rate (T-WER) of 27.42% which is better than the state-of-the-art by 2.19%."
   ],
   "doi": "10.21437/Interspeech.2022-10763"
  },
  "nikitaras22_interspeech": {
   "authors": [
    [
     "Karolos",
     "Nikitaras"
    ],
    [
     "Georgios",
     "Vamvoukakis"
    ],
    [
     "Nikolaos",
     "Ellinas"
    ],
    [
     "Konstantinos",
     "Klapsas"
    ],
    [
     "Konstantinos",
     "Markopoulos"
    ],
    [
     "Spyros",
     "Raptis"
    ],
    [
     "June Sig",
     "Sung"
    ],
    [
     "Gunu",
     "Jho"
    ],
    [
     "Aimilios",
     "Chalamandaris"
    ],
    [
     "Pirros",
     "Tsiakoulis"
    ]
   ],
   "title": "Fine-grained Noise Control for Multispeaker Speech Synthesis",
   "original": "10765",
   "page_count": 5,
   "order": 167,
   "p1": 828,
   "pn": 832,
   "abstract": [
    "A text-to-speech (TTS) model typically factorizes speech attributes such as content, speaker and prosody into disentangled representations. Recent works aim to additionally model the acoustic conditions explicitly, in order to disentangle the primary speech factors, i.e. linguistic content, prosody and timbre from any residual factors, such as recording conditions and background noise. This paper proposes unsupervised, interpretable and fine-grained noise and prosody modeling. We incorporate adversarial training, representation bottleneck and utterance-to-frame modeling in order to learn frame-level noise representations. To the same end, we perform fine-grained prosody modeling via a Fully Hierarchical Variational AutoEncoder (FVAE) which additionally results in more expressive speech synthesis. Experimental results support our claims and ablation studies verify the importance of each proposed component. Audio samples are available in our demo page."
   ],
   "doi": "10.21437/Interspeech.2022-10765"
  },
  "becerra22_interspeech": {
   "authors": [
    [
     "Helard",
     "Becerra"
    ],
    [
     "Alessandro",
     "Ragano"
    ],
    [
     "Andrew",
     "Hines"
    ]
   ],
   "title": "Exploring the influence of fine-tuning data on wav2vec 2.0 model for blind speech quality prediction",
   "original": "10766",
   "page_count": 5,
   "order": 829,
   "p1": 4088,
   "pn": 4092,
   "abstract": [
    "Recent studies have shown how self-supervised models can produce accurate speech quality predictions. Speech representations generated by the pre-trained wav2vec 2.0 model allows constructing robust predicting models using small amounts of annotated data. This opens the possibility of developing strong models in scenarios where labelled data is scarce. It is known that fine-tuning improves the model's performance; however, it is unclear how the data (e.g., language, amount of samples) used for fine-tuning is influencing that performance. In this paper, we explore how using different speech corpus to fine-tune the wav2vec 2.0 can influence its performance. We took four speech datasets containing degradations found in common conferencing applications and fine-tuned wav2vec 2.0 targeting different languages and data size scenarios. The fine-tuned models were tested across all four conferencing datasets plus an additional dataset containing synthetic speech and they were compared against three external baseline models. Results showed that fine-tuned models were able to compete with baseline models. Larger fine-tune data guarantee better performance; meanwhile, diversity in language helped the models deal with specific languages. Further research is needed to evaluate other wav2vec 2.0 models pre-trained with multi-lingual datasets and to develop prediction models that are more resilient to language diversity."
   ],
   "doi": "10.21437/Interspeech.2022-10766"
  },
  "sato22b_interspeech": {
   "authors": [
    [
     "Hiroshi",
     "Sato"
    ],
    [
     "Tsubasa",
     "Ochiai"
    ],
    [
     "Marc",
     "Delcroix"
    ],
    [
     "Keisuke",
     "Kinoshita"
    ],
    [
     "Takafumi",
     "Moriya"
    ],
    [
     "Naoki",
     "Makishima"
    ],
    [
     "Mana",
     "Ihori"
    ],
    [
     "Tomohiro",
     "Tanaka"
    ],
    [
     "Ryo",
     "Masumura"
    ]
   ],
   "title": "Strategies to Improve Robustness of Target Speech Extraction to Enrollment Variations",
   "original": "10767",
   "page_count": 5,
   "order": 203,
   "p1": 996,
   "pn": 1000,
   "abstract": [
    "Target speech extraction is a technique to extract the target speaker's voice from mixture signals using a pre-recorded enrollment utterance that characterize the voice characteristics of the target speaker. One major difficulty of target speech extraction lies in handling variability in ``intra-speaker'' characteristics, i.e., characteristics mismatch between target speech and an enrollment utterance. While most conventional approaches focus on improving average performance given a set of enrollment utterances, here we propose to guarantee the worst performance, which we believe is of great practical importance. In this work, we propose an evaluation metric called worst-enrollment source-to-distortion ratio (SDR) to quantitatively measure the robustness towards enrollment variations. We also introduce a novel training scheme that aims at directly optimizing the worst-case performance by focusing on training with difficult enrollment cases where extraction does not perform well. In addition, we investigate the effectiveness of auxiliary speaker identification loss (SI-loss) as another way to improve robustness over enrollments. Experimental validation reveals the effectiveness of both worst-enrollment target training and SI-loss training to improve robustness against enrollment variations, by increasing speaker discriminability."
   ],
   "doi": "10.21437/Interspeech.2022-10767"
  },
  "gudmalwar22_interspeech": {
   "authors": [
    [
     "Ashishkumar",
     "Gudmalwar"
    ],
    [
     "Biplove",
     "Basel"
    ],
    [
     "Anirban",
     "Dutta"
    ],
    [
     "Ch V",
     "Rama Rao"
    ]
   ],
   "title": "The Magnitude and Phase based Speech Representation Learning using Autoencoder for Classifying Speech Emotions using Deep Canonical Correlation Analysis",
   "original": "10769",
   "page_count": 5,
   "order": 237,
   "p1": 1163,
   "pn": 1167,
   "abstract": [
    "Speech Emotion Recognition (SER) from human speech utterances is a task of identifying emotions irrespective of their semantic content. It has an important role in making human-machine interaction natural. Conventional SER approaches emphasize more on magnitude spectrum for feature extraction and ignore phase information. Recent studies reveal that phase information has a significant part in analyzing speech acoustics. This work explores speech representation learning from magnitude and phase information using autoencoder for SER task. We trained the UNET autoencoder using Mel Frequency Cepstral Coefficients (MFCCs) and Modified Group Delay Function (MODGD) for learning representations. The encoder part of the trained UNET autoencoder is used as input to the neural network classifier and fine-tuned it concerning four emotions separately for MFCCs and MODGD. The learned representation for both MFCCs and MODGD are combined and given input to Support Vector Machine (SVM) for classification. The Deep Canonical Correlation Analysis (DCCA) is used to maximize the correlation between magnitude and phase information to improve the conventional SER system's performance. The performance analysis is carried out using the IEMOCAP database. The experimental results show improvement over MFCC features and existing approaches for unimodal SER."
   ],
   "doi": "10.21437/Interspeech.2022-10769"
  },
  "schoburgcarrillodemira22_interspeech": {
   "authors": [
    [
     "Rodrigo",
     "Schoburg Carrillo de Mira"
    ],
    [
     "Alexandros",
     "Haliassos"
    ],
    [
     "Stavros",
     "Petridis"
    ],
    [
     "Björn W.",
     "Schuller"
    ],
    [
     "Maja",
     "Pantic"
    ]
   ],
   "title": "SVTS: Scalable Video-to-Speech Synthesis",
   "original": "10770",
   "page_count": 5,
   "order": 372,
   "p1": 1836,
   "pn": 1840,
   "abstract": [
    "Video-to-speech synthesis (also known as lip-to-speech) refers to the translation of silent lip movements into the corresponding audio. This task has received an increasing amount of attention due to its self-supervised nature (i.e., can be trained without manual labelling) combined with the ever-growing collection of audio-visual data available online. Despite these strong motivations, contemporary video-to-speech works focus mainly on small- to medium-sized corpora with substantial constraints in both vocabulary and setting. In this work, we introduce a scalable video-to-speech framework consisting of two components: a video-to-spectrogram predictor and a pre-trained neural vocoder, which converts the mel-frequency spectrograms into waveform audio. We achieve state-of-the art results for GRID and considerably outperform previous approaches on LRW. More importantly, by focusing on spectrogram prediction using a simple feedforward model, we can efficiently and effectively scale our method to very large and unconstrained datasets: To the best of our knowledge, we are the first to show intelligible results on the challenging LRS3 dataset."
   ],
   "doi": "10.21437/Interspeech.2022-10770"
  },
  "talkar22_interspeech": {
   "authors": [
    [
     "Tanya",
     "Talkar"
    ],
    [
     "Christina",
     "Manxhari"
    ],
    [
     "James",
     "Williamson"
    ],
    [
     "Kara M.",
     "Smith"
    ],
    [
     "Thomas",
     "Quatieri"
    ]
   ],
   "title": "Speech Acoustics in Mild Cognitive Impairment and Parkinson's Disease With and Without Concurrent Drawing Tasks",
   "original": "10772",
   "page_count": 5,
   "order": 460,
   "p1": 2258,
   "pn": 2262,
   "abstract": [
    "Parkinson's disease (PD) is characterized by motor dysfunction; however, non-motor symptoms such as cognitive decline also have a dramatic impact on quality of life. Current assessments to diagnose cognitive impairment take many hours and require high clinician involvement. Thus, there is a need to develop new tools leading to quick and accurate determination of cognitive impairment to allow for appropriate, timely interventions. In this paper, individuals with PD, designated as either having no cognitive impairment (NCI) or mild cognitive impairment (MCI), undergo a speech-based protocol, involving reading or listing items within a category, performed either with or without a concurrent drawing task. From the speech recordings, we extract motor coordination-based features, derived from correlations across acoustic features representative of speech production subsystems. The correlation-based features are utilized in gaussian mixture models to discriminate between individuals designated NCI or MCI in both the single and dual task paradigms. Features derived from the laryngeal and respiratory subsystems, in particular, discriminate between these two groups with AUCs > 0.80. These results suggest that cognitive impairment can be detected using speech from both single and dual task paradigms, and that cognitive impairment may manifest as differences in vocal fold vibration stability."
   ],
   "doi": "10.21437/Interspeech.2022-10772"
  },
  "liao22_interspeech": {
   "authors": [
    [
     "Sishi",
     "Liao"
    ],
    [
     "Phil",
     "Hoole"
    ],
    [
     "Conceição",
     "Cunha"
    ],
    [
     "Esther",
     "Kunay"
    ],
    [
     "Aletheia",
     "Cui"
    ],
    [
     "Lia Saki Bučar",
     "Shigemori"
    ],
    [
     "Felicitas",
     "Kleber"
    ],
    [
     "Dirk",
     "Voit"
    ],
    [
     "Jens",
     "Frahm"
    ],
    [
     "Jonathan",
     "Harrington"
    ]
   ],
   "title": "Nasal Coda Loss in the Chengdu Dialect of Mandarin: Evidence from RT-MRI",
   "original": "10775",
   "page_count": 5,
   "order": 274,
   "p1": 1347,
   "pn": 1351,
   "abstract": [
    "In the Chengdu dialect of Mandarin, the /(V)an/ rime words have been described to have undergone a nasal loss process in the last decades. However, no acoustical or physiological evidence has been provided so far. In this study, we investigate this sound change process by directly looking at the velum gesture in the target words from 4 Chengdu speakers. By means of real-time Magnetic Resonance Imaging (rt-MRI), the velum opening signal was captured along with synchronized and noise suppressed audio. The maximum degree of velum opening was compared between tautosyllabic and heterosyllabic VN sequences for different vowels (N = /n, ŋ/). Nasal consonant loss was most evident for tautosyllabic /(V)an/ rime words. This sound change, together with the observed diachronic vowel raising in /(V)an/ rimes, is compatible with other research showing a preference for low vowel raising before nasal consonants. This phonetically motivated oral vowel, which is a consequence of nasal coda loss and vowel raising, would form a new phonological contrast in this dialect e.g., from /pa, pan/ to /pa, pɛ/."
   ],
   "doi": "10.21437/Interspeech.2022-10775"
  },
  "baumann22_interspeech": {
   "authors": [
    [
     "Ilja",
     "Baumann"
    ],
    [
     "Dominik",
     "Wagner"
    ],
    [
     "Sebastian",
     "Bayerl"
    ],
    [
     "Tobias",
     "Bocklet"
    ]
   ],
   "title": "Nonwords Pronunciation Classification in Language Development Tests for Preschool Children",
   "original": "10777",
   "page_count": 5,
   "order": 737,
   "p1": 3643,
   "pn": 3647,
   "abstract": [
    "This work aims to automatically evaluate whether the language development of children is age-appropriate. Validated speech and language tests are used for this purpose to test the auditory memory. In this work, the task is to determine whether spoken nonwords have been uttered correctly. We compare different approaches that are motivated to model specific language structures: Low-level features (FFT), speaker embeddings (ECAPA-TDNN), grapheme-motivated embeddings (wav2vec 2.0), and phonetic embeddings in form of senones (ASR acoustic model). Each of the approaches provides input for VGG-like 5-layer CNN classifiers. We also examine the adaptation per nonword. The evaluation of the proposed systems was performed using recordings from different kindergartens of spoken nonwords. ECAPA-TDNN and low-level FFT features do not explicitly model phonetic information; wav2vec2.0 is trained on grapheme labels, our ASR acoustic model features contain (sub-)phonetic information. We found that the more granular the phonetic modeling is, the higher are the achieved recognition rates. The best system trained on ASR acoustic model features with VTLN achieved an accuracy of 89.4% and an area under the ROC (Receiver Operating Characteristic) curve (AUC) of 0.923. This corresponds to an improvement in accuracy of 20.2% and AUC of 0.309 relative compared to the FFT-baseline."
   ],
   "doi": "10.21437/Interspeech.2022-10777"
  },
  "tzudir22_interspeech": {
   "authors": [
    [
     "Moakala",
     "Tzudir"
    ],
    [
     "Priyankoo",
     "Sarmah"
    ],
    [
     "S R Mahadeva",
     "Prasanna"
    ]
   ],
   "title": "Prosodic Information in Dialect Identification of a Tonal Language: The case of Ao",
   "original": "10779",
   "page_count": 5,
   "order": 456,
   "p1": 2238,
   "pn": 2242,
   "abstract": [
    "Dialect identification has been explored profusely in major languages, such as Arabic, Chinese and Spanish. This paper presents an automatic dialect identification system in the Ao language using prosodic features. Ao is a low-resource Tibeto-Burman tonal language spoken in Nagaland in the North-Eastern part of India. It consists of three distinct dialects: Chungli, Mongsen and Changki. Prosodic characteristics are believed to have an essential role in tonal languages. In this direction, the current work focuses to investigate the prosodic characteristics to build a discriminative system in identifying the three Ao dialects. The statistical and Low-Level Descriptors (LLD) of prosodic features are used in this work. The prosodic features such as F0, loudness, shimmer, jitter, voiced and unvoiced segment length, etc., are utilized in this study. The experiments are conducted using SVM and attention-based Bi-GRU classifiers in trisyllabic words and passage-level datasets, respectively. The combination of prosodic features outperforms the MFCC (baseline) feature. The Voice Quality and Temporal (VQT) feature set is the best performing prosodic feature. The statistical analysis also shows that the VQT features are statistically significant. The performances of SVM and attention-based Bi-GRU classifiers indicate the significance of prosodic information in classifying the three Ao dialects."
   ],
   "doi": "10.21437/Interspeech.2022-10779"
  },
  "fras22_interspeech": {
   "authors": [
    [
     "Mieszko",
     "Fras"
    ],
    [
     "Marcin",
     "Witkowski"
    ],
    [
     "Konrad",
     "Kowalczyk"
    ]
   ],
   "title": "Convolutive Weighted Multichannel Wiener Filter Front-end for Distant Automatic Speech Recognition in Reverberant Multispeaker Scenarios",
   "original": "10780",
   "page_count": 5,
   "order": 597,
   "p1": 2943,
   "pn": 2947,
   "abstract": [
    "The performance of automatic speech recognition (ASR) systems strongly deteriorates when the desired speech signal is contaminated with room reverberation and when the speech of interfering speakers overlaps. To achieve acceptable word error rates (WER) by distant ASR in multispeaker reverberant scenarios, source separation and dereverberation can be performed as front-end processing. An existing optimum filter suitable for this task is the recently proposed weighted power minimization distortionless response convolutional beamformer (WPD). In this paper, we introduce a novel speech enhancement front-end for improving the accuracy of back-end ASR in scenarios with multiple reverberant overlapping speakers. The convolutional weighted multichannel Wiener filter (CW-MWF) is optimum for the joint separation and dereverberation task, and it is derived from the convolutional weighted minimum mean square error (CW-MMSE) optimization criterion, presented recently by the current authors. The WER results of performed experiments indicate superior performance of the CW-MWF in real and simulated rooms, irrespective of the method used for filter parameter estimation and the DNN model used for back-end ASR."
   ],
   "doi": "10.21437/Interspeech.2022-10780"
  },
  "deoliveira22_interspeech": {
   "authors": [
    [
     "Danilo",
     "de Oliveira"
    ],
    [
     "Tal",
     "Peer"
    ],
    [
     "Timo",
     "Gerkmann"
    ]
   ],
   "title": "Efficient Transformer-based Speech Enhancement Using Long Frames and STFT Magnitudes",
   "original": "10781",
   "page_count": 5,
   "order": 598,
   "p1": 2948,
   "pn": 2952,
   "abstract": [
    "The SepFormer architecture shows very good results in speech separation. Like other learned-encoder models, it uses short frames, as they have been shown to obtain better performance in these cases. This results in a large number of frames at the input, which is problematic; since the SepFormer is transformer-based, its computational complexity drastically increases with longer sequences. In this paper, we employ the SepFormer in a speech enhancement task and show that by replacing the learned-encoder features with a magnitude short-time Fourier transform (STFT) representation, we can use long frames without compromising perceptual enhancement performance. We obtained equivalent quality and intelligibility evaluation scores while reducing the number of operations by a factor of approximately 8 for a 10-second utterance."
   ],
   "doi": "10.21437/Interspeech.2022-10781"
  },
  "young22_interspeech": {
   "authors": [
    [
     "Nathan Joel",
     "Young"
    ],
    [
     "David",
     "Britain"
    ],
    [
     "Adrian",
     "Leemann"
    ]
   ],
   "title": "A blueprint for using deepfakes in sociolinguistic matched-guise experiments",
   "original": "10782",
   "page_count": 5,
   "order": 1068,
   "p1": 5268,
   "pn": 5272,
   "abstract": [
    "Matched-guise paradigms, which are used extensively in speaker and accent evaluation studies, have long been hampered by empirical holes. We offer a solution by incorporating deepfake technology, which greatly reduces the number of potential confounds. We constructed a sociophonetic experiment whereby high-rising terminal (a.k.a. \"uptalk”) – and the lack thereof – was superimposed onto a deepfaked \"beautiful” and \"less beautiful” female guise. The resulting four guises were incorporated into a 2x2-factor between-subjects experiment tested on female evaluators. Each evaluator assessed their respective guise against a list of prescribed attributes and offered free-form comments. Results align with studies on high-rising terminal as well as intuitions concerning conventional beauty, which validates the technique and motivates its wider adoption."
   ],
   "doi": "10.21437/Interspeech.2022-10782"
  },
  "yang22u_interspeech": {
   "authors": [
    [
     "Samuel",
     "Yang"
    ],
    [
     "Scott",
     "Wisdom"
    ],
    [
     "Chet",
     "Gnegy"
    ],
    [
     "Richard F.",
     "Lyon"
    ],
    [
     "Sagar",
     "Savla"
    ]
   ],
   "title": "Listening with Googlears: Low-Latency Neural Multiframe Beamforming and Equalization for Hearing Aids",
   "original": "10783",
   "page_count": 5,
   "order": 798,
   "p1": 3939,
   "pn": 3943,
   "abstract": [
    "Understanding speech in the presence of noise with hearing aids can be challenging. Here we describe our entry, submission E003, to the 2021 Clarity Enhancement Challenge Round 1 (CEC1), a machine learning challenge for improving hearing aid processing. We apply and evaluate a deep neural network speech enhancement model with a low-latency recursive least squares (RLS) adaptive beamformer, and a linear equalizer, to improve speech intelligibility in the presence of speech or noise interferers. The enhancement network is trained only on the CEC1 data, and all processing obeys the 5~ms latency requirement. We quantify the improvement using the CEC1 provided hearing loss model and Modified Binaural Short-Time Objective Intelligibility (MBSTOI) score (ranging from 0 to 1, higher being better). On the CEC1 test set, we achieve a mean of 0.644 and median of 0.652 compared to the 0.310 mean and 0.314 median for the baseline. In the CEC1 subjective listener intelligibility assessment, for scenes with noise interferers, we achieve the second highest improvement in intelligibility from 33.2% to 85.5%, but for speech interferers, we see more mixed results, potentially from listener confusion."
   ],
   "doi": "10.21437/Interspeech.2022-10783"
  },
  "benway22_interspeech": {
   "authors": [
    [
     "Nina",
     "Benway"
    ],
    [
     "Jonathan L.",
     "Preston"
    ],
    [
     "Elaine",
     "Hitchcock"
    ],
    [
     "Asif",
     "Salekin"
    ],
    [
     "Harshit",
     "Sharma"
    ],
    [
     "Tara",
     "McAllister"
    ]
   ],
   "title": "PERCEPT-R: An Open-Access American English Child/Clinical Speech Corpus Specialized for the Audio Classification of /ɹ/",
   "original": "10785",
   "page_count": 5,
   "order": 738,
   "p1": 3648,
   "pn": 3652,
   "abstract": [
    "We present the PERCEPT-R corpus, a labeled corpus of child speakers of American English with typical speech and residual speech sound disorders affecting rhotics. We demonstrate the utility of age-and-gender normalized formants extracted from PERCEPT-R in training support vector classifiers to predict ground-truth perceptual judgments of \"rhotic” (i.e., dialect-typical) and \"derhotic” phones for novel speakers (mean of participant-specific f-metrics = .83; SD = .18, N = 281)."
   ],
   "doi": "10.21437/Interspeech.2022-10785"
  },
  "lin22d_interspeech": {
   "authors": [
    [
     "Rongmei",
     "Lin"
    ],
    [
     "Yonghui",
     "Xiao"
    ],
    [
     "Tien-Ju",
     "Yang"
    ],
    [
     "Ding",
     "Zhao"
    ],
    [
     "Li",
     "Xiong"
    ],
    [
     "Giovanni",
     "Motta"
    ],
    [
     "Francoise",
     "Beaufays"
    ]
   ],
   "title": "Federated Pruning: Improving Neural Network Efficiency with Federated Learning",
   "original": "10787",
   "page_count": 5,
   "order": 345,
   "p1": 1701,
   "pn": 1705,
   "abstract": [
    "Automatic Speech Recognition models require large amount of speech data for training, and the collection of such data often leads to privacy concerns. Federated learning has been widely used and is considered to be an effective decentralized tech- nique by collaboratively learning a shared prediction model while keeping the data local on different clients devices. How- ever, the limited computation and communication resources on clients devices present practical difficulties for large models. To overcome such challenges, we propose Federated Pruning to train a reduced model under the federated setting, while main- taining similar performance compared to the full model. More- over, the vast amount of clients data can also be leveraged to im- prove the pruning results compared to centralized training. We explore different pruning schemes and provide empirical evi- dence of the effectiveness of our methods."
   ],
   "doi": "10.21437/Interspeech.2022-10787"
  },
  "ding22b_interspeech": {
   "authors": [
    [
     "Shaojin",
     "Ding"
    ],
    [
     "Wang",
     "Weiran"
    ],
    [
     "Ding",
     "Zhao"
    ],
    [
     "Tara",
     "Sainath"
    ],
    [
     "Yanzhang",
     "He"
    ],
    [
     "Robert",
     "David"
    ],
    [
     "Rami",
     "Botros"
    ],
    [
     "Xin",
     "Wang"
    ],
    [
     "Rina",
     "Panigrahy‎"
    ],
    [
     "Qiao",
     "Liang"
    ],
    [
     "Dongseong",
     "Hwang"
    ],
    [
     "Ian",
     "McGraw"
    ],
    [
     "Rohit",
     "Prabhavalkar"
    ],
    [
     "Trevor",
     "Strohman"
    ]
   ],
   "title": "A Unified Cascaded Encoder ASR Model for Dynamic Model Sizes",
   "original": "10791",
   "page_count": 5,
   "order": 346,
   "p1": 1706,
   "pn": 1710,
   "abstract": [
    "In this paper, we propose a dynamic cascaded encoder Automatic Speech Recognition (ASR) model, which unifies models for different deployment scenarios. Moreover, the model can significantly reduce model size and power consumption without loss of quality. Namely, with the dynamic cascaded encoder model, we explore three techniques to maximally boost the performance of each model size: 1) Use separate decoders for each sub-model while sharing the encoders; 2) Use funnel-pooling to improve the encoder efficiency; 3) Balance the size of causal and non-causal encoders to improve quality and fit deployment constraints. Overall, the proposed large-medium model has 30% smaller size and reduces power consumption by 33%, compared to the baseline cascaded encoder model. The triple-size model that unifies the large, medium, and small models achieves 37% total size reduction with minimal quality loss, while substantially reducing the engineering efforts of having separate models."
   ],
   "doi": "10.21437/Interspeech.2022-10791"
  },
  "kumar22b_interspeech": {
   "authors": [
    [
     "Ayush",
     "Kumar"
    ],
    [
     "Vijit",
     "Malik"
    ],
    [
     "Jithendra",
     "Vepa"
    ]
   ],
   "title": "Does Utterance entails Intent?: Evaluating Natural Language Inference Based Setup for Few-Shot Intent Detection",
   "original": "10794",
   "page_count": 5,
   "order": 912,
   "p1": 4501,
   "pn": 4505,
   "abstract": [
    "Intent Detection is one of the core tasks of dialog systems. Few- shot Intent Detection is challenging due to limited number of annotated utterances for novel classes. Generalized Few-shot intent detection is more realistic but challenging setup which aims to discriminate the joint label space of both novel intents which have few examples each and existing intents consisting of enough labeled data. Large label spaces and fewer number of shots increase the complexity of the task. In this work, we employ a simple and effective method based on Natural Language Inference that leverages the semantics in the class- label names to learn and predict the novel classes. Our method achieves state-of-the-art results on 1-shot and 5-shot intent detection task with gains ranging from 2-8% points in F1 score on four benchmark datasets. Our method also outperforms existing approaches on a more practical setting of generalized few-shot intent detection with gains up to 20% F1 score. We show that the suggested approach performs well across single and multi domain datasets with the number of class labels from as few as 7 to as high as 150."
   ],
   "doi": "10.21437/Interspeech.2022-10794"
  },
  "baby22_interspeech": {
   "authors": [
    [
     "Deepak",
     "Baby"
    ],
    [
     "Pasquale",
     "D'Alterio"
    ],
    [
     "Valentin",
     "Mendelev"
    ]
   ],
   "title": "Incremental learning for RNN-Transducer based speech recognition models",
   "original": "10795",
   "page_count": 5,
   "order": 15,
   "p1": 71,
   "pn": 75,
   "abstract": [
    "This paper investigates an incremental learning framework for a real-world voice assistant employing RNN-Transducer based automatic speech recognition (ASR) model. Such a model needs to be regularly updated to keep up with changing distribution of customer requests. We demonstrate that a simple fine-tuning approach with a combination of old and new training data can be used to incrementally update the model spending only several hours of training time and without any degradation on old data. This paper explores multiple rounds of incremental updates on the ASR model with monthly training data. Results show that the proposed approach achieves 5-6\\% relative WER improvement over the models trained from scratch on the monthly evaluation datasets. In addition, we explore if it is possible to improve recognition of specific new words. We simulate multiple rounds of incremental updates with handful of training utterances per word (both real and synthetic) and show that the recognition of the new words improves dramatically but with a minor degradation on general data. Finally, we demonstrate that the observed degradation on general data can be mitigated by interleaving monthly updates with updates targeting specific words."
   ],
   "doi": "10.21437/Interspeech.2022-10795"
  },
  "berrebbi22_interspeech": {
   "authors": [
    [
     "Dan",
     "Berrebbi"
    ],
    [
     "Jiatong",
     "Shi"
    ],
    [
     "Brian",
     "Yan"
    ],
    [
     "Osbel",
     "López-Francisco"
    ],
    [
     "Jonathan",
     "Amith"
    ],
    [
     "Shinji",
     "Watanabe"
    ]
   ],
   "title": "Combining Spectral and Self-Supervised Features for Low Resource Speech Recognition and Translation",
   "original": "10796",
   "page_count": 5,
   "order": 715,
   "p1": 3533,
   "pn": 3537,
   "abstract": [
    "Self-Supervised Learning (SSL) models have been successfully applied in various deep learning-based speech tasks, particularly those with a limited amount of data. However, the quality of SSL representations depends highly on the relatedness between the SSL training domain(s) and the target data domain. On the contrary, spectral feature (SF) extractors such as log Mel-filterbanks are hand-crafted non-learnable components, and could be more robust to domain shifts. The present work examines the assumption that combining non-learnable SF extractors to SSL models is an effective approach to low resource speech tasks. We propose a learnable and interpretable framework to combine SF and SSL representations. The proposed framework outperforms significantly both baseline and SSL models on Automatic Speech Recognition (ASR) and Speech Translation (ST) tasks on three low resource datasets. We additionally design a mixture of experts based combination model. This last model reveals that the relative contribution of SSL models over conventional SF extractors is very small in case of domain mismatch between SSL training set and the target language data."
   ],
   "doi": "10.21437/Interspeech.2022-10796"
  },
  "siuzdak22_interspeech": {
   "authors": [
    [
     "Hubert",
     "Siuzdak"
    ],
    [
     "Piotr",
     "Dura"
    ],
    [
     "Pol",
     "van Rijn"
    ],
    [
     "Nori",
     "Jacoby"
    ]
   ],
   "title": "WavThruVec: Latent speech representation as intermediate features for neural speech synthesis",
   "original": "10797",
   "page_count": 5,
   "order": 168,
   "p1": 833,
   "pn": 837,
   "abstract": [
    "Recent advances in neural text-to-speech research have been dominated by two-stage pipelines utilizing low-level intermediate speech representation such as mel-spectrograms. However, such predetermined features are fundamentally limited, because they do not allow to exploit the full potential of a data-driven approach through learning hidden representations. For this reason, several end-to-end methods have been proposed. However, such models are harder to train and require a large number of high-quality recordings with transcriptions. Here, we propose WavThruVec - a two-stage architecture that resolves the bottleneck by using high-dimensional wav2vec 2.0 embeddings as intermediate speech representation. Since these hidden activations provide high-level linguistic features, they are more robust to noise. That allows us to utilize annotated speech datasets of a lower quality to train the first-stage module. At the same time, the second-stage component can be trained on large-scale untranscribed audio corpora, as wav2vec 2.0 embeddings are already time-aligned. This results in an increased generalization capability to out-of-vocabulary words, as well as to a better generalization to unseen speakers. We show that the proposed model not only matches the quality of state-of-the-art neural models, but also presents useful properties enabling tasks like voice conversion or zero-shot synthesis."
   ],
   "doi": "10.21437/Interspeech.2022-10797"
  },
  "ravi22_interspeech": {
   "authors": [
    [
     "Vijay",
     "Ravi"
    ],
    [
     "Jinhan",
     "Wang"
    ],
    [
     "Jonathan",
     "Flint"
    ],
    [
     "Abeer",
     "Alwan"
    ]
   ],
   "title": "A Step Towards Preserving Speakers’ Identity While Detecting Depression Via Speaker Disentanglement",
   "original": "10798",
   "page_count": 5,
   "order": 676,
   "p1": 3338,
   "pn": 3342,
   "abstract": [
    "Preserving a patient's identity is a challenge for automatic, speech-based diagnosis of mental health disorders. In this paper, we address this issue by proposing adversarial disentanglement of depression characteristics and speaker identity. The model used for depression classification is trained in a speaker-identity-invariant manner by minimizing depression prediction loss and maximizing speaker prediction loss during training. The effectiveness of the proposed method is demonstrated on two datasets - DAIC-WOZ (English) and CONVERGE (Mandarin), with three feature sets (Mel-spectrograms, raw-audio signals, and the last-hidden-state of Wav2vec2.0), using a modified DepAudioNet model. With adversarial training, depression classification improves for every feature when compared to the baseline. Wav2vec2.0 features with adversarial learning resulted in the best performance (F1-score of 69.2% for DAIC-WOZ and 91.5% for CONVERGE). Analysis of the class-separability measure (J-ratio) of the hidden states of the DepAudioNet model shows that when adversarial learning is applied, the backend model loses some speaker-discriminability while it improves depression-discriminability. These results indicate that there are some components of speaker identity that may not be useful for depression detection and minimizing their effects provides a more accurate diagnosis of the underlying disorder and can safeguard a speaker's identity."
   ],
   "doi": "10.21437/Interspeech.2022-10798"
  },
  "toussaint22_interspeech": {
   "authors": [
    [
     "Wiebke",
     "Toussaint"
    ],
    [
     "Lauriane",
     "Gorce"
    ],
    [
     "Aaron Yi",
     "Ding"
    ]
   ],
   "title": "Design Guidelines for Inclusive Speaker Verification Evaluation Datasets",
   "original": "10799",
   "page_count": 5,
   "order": 263,
   "p1": 1293,
   "pn": 1297,
   "abstract": [
    "Speaker verification (SV) provides billions of voice-enabled devices with access control, and ensures the security of voice-driven technologies. As a type of biometrics, it is necessary that SV is unbiased, with consistent and reliable performance across speakers irrespective of their demographic, social and economic attributes. Current SV evaluation practices are insufficient for evaluating bias: they are over-simplified and aggregate users, not representative of usage scenarios encountered in deployment, and consequences of errors are not accounted for. This paper proposes design guidelines for constructing SV evaluation datasets that address these short-comings. We propose a schema for grading the difficulty of utterance pairs, and present an algorithm for generating inclusive SV datasets. We empirically validate our proposed method in a set of experiments on the VoxCeleb1 dataset. Our results confirm that the count of utterance pairs/speaker, and the difficulty grading of utterance pairs have a significant effect on evaluation performance and variability. Our work contributes to the development of SV evaluation practices that are inclusive and fair."
   ],
   "doi": "10.21437/Interspeech.2022-10799"
  },
  "le22_interspeech": {
   "authors": [
    [
     "Duc",
     "Le"
    ],
    [
     "Akshat",
     "Shrivastava"
    ],
    [
     "Paden D.",
     "Tomasello"
    ],
    [
     "Suyoun",
     "Kim"
    ],
    [
     "Aleksandr",
     "Livshits"
    ],
    [
     "Ozlem",
     "Kalinli"
    ],
    [
     "Michael",
     "Seltzer"
    ]
   ],
   "title": "Deliberation Model for On-Device Spoken Language Understanding",
   "original": "10800",
   "page_count": 5,
   "order": 702,
   "p1": 3468,
   "pn": 3472,
   "abstract": [
    "We propose a novel deliberation-based approach to end-to-end (E2E) spoken language understanding (SLU), where a streaming automatic speech recognition (ASR) model produces the first-pass hypothesis and a second-pass natural language understanding (NLU) component generates the semantic parse by conditioning on both ASR's text and audio embeddings. By formulating E2E SLU as a generalized decoder, our system is able to support complex compositional semantic structures. Furthermore, the sharing of parameters between ASR and NLU makes the system especially suitable for resource-constrained (on-device) environments; our proposed approach consistently outperforms strong pipeline NLU baselines by 0.60% to 0.65% on the spoken version of the TOPv2 dataset (STOP). We demonstrate that the fusion of text and audio features, coupled with the system's ability to rewrite the first-pass hypothesis, makes our approach more robust to ASR errors. Finally, we show that our approach can significantly reduce the degradation when moving from natural speech to synthetic speech training, but more work is required to make text-to-speech (TTS) a viable solution for scaling up E2E SLU."
   ],
   "doi": "10.21437/Interspeech.2022-10800"
  },
  "wang22y_interspeech": {
   "authors": [
    [
     "Pu",
     "Wang"
    ],
    [
     "Hugo",
     "Van hamme"
    ]
   ],
   "title": "Bottleneck Low-rank Transformers for Low-resource Spoken Language Understanding",
   "original": "10801",
   "page_count": 5,
   "order": 254,
   "p1": 1248,
   "pn": 1252,
   "abstract": [
    "End-to-end spoken language understanding (SLU) systems benefit from pretraining on large corpora, followed by fine-tuning on application-specific data. The resulting models are too large for on-edge applications. For instance, BERT-based systems contain over 110M parameters. Observing the model is over-parameterized, we propose lean transformer structure where the dimension of the attention mechanism is automatically reduced using group sparsity. We propose a variant where the learned attention subspace is transferred to an attention bottleneck layer. In a low-resource setting and without pre-training, the resulting compact SLU model achieves accuracies competitive with pre-trained large models."
   ],
   "doi": "10.21437/Interspeech.2022-10801"
  },
  "adigwe22_interspeech": {
   "authors": [
    [
     "Adaeze O.",
     "Adigwe"
    ],
    [
     "Esther",
     "Klabbers"
    ]
   ],
   "title": "Strategies for developing a Conversational Speech Dataset for Text-To-Speech Synthesis",
   "original": "10802",
   "page_count": 5,
   "order": 472,
   "p1": 2318,
   "pn": 2322,
   "abstract": [
    "There have been many efforts to improve the quality of speech synthesis systems in conversational AI. Although state-of-the-art systems are capable of producing natural-sounding speech, the generated speech often lacks prosodic variation and is not always suited to the task. In this paper, we examine data collection methods for dialogue data to use as training data for our acoustic models. We collect speech using three different setups: (1) Random read-aloud sentences; (2) Performed dialogues; (3) Semi-Spontaneous dialogues. We analyze prosodic and textual properties of the data collected in these setups and make some recommendations to collect data for speech synthesis in conversational AI settings."
   ],
   "doi": "10.21437/Interspeech.2022-10802"
  },
  "wallbridge22_interspeech": {
   "authors": [
    [
     "Sarenne Carrol",
     "Wallbridge"
    ],
    [
     "Catherine",
     "Lai"
    ],
    [
     "Peter",
     "Bell"
    ]
   ],
   "title": "Investigating perception of spoken dialogue acceptability through surprisal",
   "original": "10808",
   "page_count": 5,
   "order": 913,
   "p1": 4506,
   "pn": 4510,
   "abstract": [
    "Surprisal is used throughout computational psycholinguistics to model a range of language processing behaviour. There is growing evidence that language model (LM) estimates of surprisal correlate with human performance on a range of written language comprehension tasks. Although communicative interaction is arguably the primary form of language use, most studies of surprisal are based on monological, written data. Towards the goal of understanding perception in spontaneous, natural language, we present an exploratory investigation into whether the relationship between human comprehension behaviour and LM-estimated surprisal holds when applied to dialogue, considering both written dialogue, and the lexical component of spoken dialogue. We use a novel judgement task of dialogue utterance acceptability to ask two questions: \"How well can people make predictions about written dialogue and transcripts of spoken dialogue?” and \"Does surprisal correlate with these acceptability judgements?”. We demonstrate that people can make accurate predictions about upcoming dialogue and that their ability differs between spoken transcripts and written conversation. We investigate the relationship between global and local operationalisations of surprisal and human acceptability judgements, finding a combination of both to provide the most predictive power"
   ],
   "doi": "10.21437/Interspeech.2022-10808"
  },
  "ding22c_interspeech": {
   "authors": [
    [
     "Shaojin",
     "Ding"
    ],
    [
     "Phoenix",
     "Meadowlark‎"
    ],
    [
     "Yanzhang",
     "He"
    ],
    [
     "Lukasz",
     "Lew"
    ],
    [
     "Shivani",
     "Agrawal"
    ],
    [
     "Oleg",
     "Rybakov‎"
    ]
   ],
   "title": "4-bit Conformer with Native Quantization Aware Training for Speech Recognition",
   "original": "10809",
   "page_count": 5,
   "order": 347,
   "p1": 1711,
   "pn": 1715,
   "abstract": [
    "Reducing the latency and model size has always been a significant research problem for live Automatic Speech Recognition (ASR) application scenarios. Along this direction, model quantization has become an increasingly popular approach to compress neural networks and reduce computation cost. Most of the existing practical ASR systems apply post-training 8-bit quantization. To achieve a higher compression rate without introducing additional performance regression, in this study, we propose to develop 4-bit ASR models with native quantization aware training, which leverages native integer operations to effectively optimize both training and inference. We conducted two experiments on state-of-the-art Conformer-based ASR models to evaluate our proposed quantization technique. First, we explored the impact of different precisions for both weight and activation quantization on the LibriSpeech dataset, and obtained a lossless 4-bit Conformer model with 7.7x size reduction compared to the float32 model. Following this, we for the first time investigated and revealed the viability of 4-bit quantization on a practical ASR system that is trained with large-scale datasets, and produced a lossless Conformer ASR model with mixed 4-bit and 8-bit weights that has 5x size reduction compared to the float32 model."
   ],
   "doi": "10.21437/Interspeech.2022-10809"
  },
  "bai22b_interspeech": {
   "authors": [
    [
     "Yu",
     "Bai"
    ],
    [
     "Ferdy",
     "Hubers"
    ],
    [
     "Catia",
     "Cucchiarini"
    ],
    [
     "Roeland",
     "van Hout"
    ],
    [
     "Helmer",
     "Strik"
    ]
   ],
   "title": "The Effects of Implicit and Explicit Feedback in an ASR-based Reading Tutor for Dutch First-graders",
   "original": "10810",
   "page_count": 5,
   "order": 907,
   "p1": 4476,
   "pn": 4480,
   "abstract": [
    "Literacy skills are pivotal for children to become schooled and educated and to engage with written texts in everyday life. Learning to read is a primary skill that children acquire at school. Supervised, supportive guided reading aloud may help children improve their reading skills. However, such a support- ive context is usually difficult to realize at school since teachers do not have enough time to give directed individual feedback. An ASR-based reading tutor could be a solution in such a press- ing situation, as such a tutor can ‘listen' to children reading, provide individual feedback on errors, and store information on reading practice into logfiles. In this study, we investigated the effectiveness of an ASR- based Reading Tutor for Dutch first graders, in which different forms of feedback were implemented. We collected data from 525 first-graders in 44 schools, with 263 pupils who received explicit feedback and 262 pupils who received implicit feed- back. Analyses based on mixed linear regression models indi- cate positive effects of both feedback forms on reading accuracy and a trade-off relationship between accuracy and speed."
   ],
   "doi": "10.21437/Interspeech.2022-10810"
  },
  "buech22_interspeech": {
   "authors": [
    [
     "Philipp",
     "Buech"
    ],
    [
     "Simon",
     "Roessig"
    ],
    [
     "Lena",
     "Pagel"
    ],
    [
     "Doris",
     "Muecke"
    ],
    [
     "Anne",
     "Hermes"
    ]
   ],
   "title": "ema2wav: doing articulation by Praat",
   "original": "10813",
   "page_count": 5,
   "order": 275,
   "p1": 1352,
   "pn": 1356,
   "abstract": [
    "In this paper, we present ema2wav, a software conversion tool for electromagnetic articulographic data, producing multi-channel WAVE files. The data can be converted either by executing a stand-alone Python script or by using a user-friendly GUI. ema2wav allows the display and extraction of EMA trajectories as well as data smoothing and the computation of derivatives and Euclidean distance between sensors. A great asset of this converter is that it allows the research community to process EMA data in widespread and easy-to-use open-source programs like Praat. It is completely platform-independent and is thus a very promising alternative for e.g., students, teachers and researchers in experimental linguistics who have either limited access to software licenses and/or seek for an easy way to maintain open solutions for their research."
   ],
   "doi": "10.21437/Interspeech.2022-10813"
  },
  "wang22z_interspeech": {
   "authors": [
    [
     "Jinhan",
     "Wang"
    ],
    [
     "Vijay",
     "Ravi"
    ],
    [
     "Jonathan",
     "Flint"
    ],
    [
     "Abeer",
     "Alwan"
    ]
   ],
   "title": "Unsupervised Instance Discriminative Learning for Depression Detection from Speech Signals",
   "original": "10814",
   "page_count": 5,
   "order": 412,
   "p1": 2018,
   "pn": 2022,
   "abstract": [
    "Major Depressive Disorder (MDD) is a severe illness that affects millions of people, and it is critical to diagnose this disorder as early as possible. Detecting depression from voice signals can be of great help to physicians and can be done without the need of any invasive procedure. Since relevant labelled data are scarce, we propose a modified Instance Discriminative Learning (IDL) method, an unsupervised pre-training technique, to extract augment-invariant and instance-spread-out embeddings. In terms of learning augment-invariant embeddings, various data augmentation methods for speech are investigated, and time-masking is found to provide the best performance. To learn instance-spread-out embeddings, we explore methods for sampling instances for a training batch (distinct speaker-based and random sampling). It is found that the distinct speaker-based sampling provides better performance than the random one, and we hypothesize that this result is because relevant speaker information is preserved in the embedding. Additionally, we propose a novel sampling strategy, Pseudo Instance-based Sampling (PIS), based on clustering algorithms, to enhance spread-out characteristics of the embeddings. Experiments are conducted with DepAudioNet on DAIC-WOZ (English) and CONVERGE (Mandarin) datasets, and statistically significant improvements are observed in the detection of MDD relative to the baseline with no pre-training."
   ],
   "doi": "10.21437/Interspeech.2022-10814"
  },
  "zhang22ba_interspeech": {
   "authors": [
    [
     "Zhaoyan",
     "Zhang"
    ],
    [
     "Jason",
     "Zhang"
    ],
    [
     "Jody",
     "Kreiman"
    ]
   ],
   "title": "Effects of laryngeal manipulations on voice gender perception",
   "original": "10815",
   "page_count": 5,
   "order": 376,
   "p1": 1856,
   "pn": 1860,
   "abstract": [
    "This study aims to identify laryngeal manipulations that would allow a male to approximate a female-sounding voice, and that can be targeted in voice feminization surgery or therapy. Synthetic voices were generated using a three-dimensional vocal fold model with parametric variations in vocal fold geometry, stiffness, adduction, and subglottal pressure. The vocal tract was kept constant in order to focus on the contribution of laryngeal manipulations. Listening subjects were asked to judge if a voice sounded male or female, or if they were unsure. Results showed the expected large effect of the fundamental frequency (F0) and a moderate effect of spectral shape on gender perception. A mismatch between F0 and spectral shape cues (e.g., low F0 paired with high H1-H2) contributed to ambiguity in gender perception, particularly for voices with F0 in the intermediate range between those of typical adult males and females. Physiologically, the results showed that a female-sounding voice can be produced by decreasing vocal fold thickness and increasing vocal fold transverse stiffness in the coronal plane, changes in which modified both F0 and spectral shape. In contrast, laryngeal manipulations with limited impact on F0 or spectral shape were shown to be less effective in modifying gender perception."
   ],
   "doi": "10.21437/Interspeech.2022-10815"
  },
  "dheram22_interspeech": {
   "authors": [
    [
     "PRANAV",
     "DHERAM"
    ],
    [
     "Murugesan",
     "Ramakrishnan"
    ],
    [
     "Anirudh",
     "Raju"
    ],
    [
     "I-Fan",
     "Chen"
    ],
    [
     "Brian",
     "King"
    ],
    [
     "Katherine",
     "Powell"
    ],
    [
     "Melissa",
     "Saboowala"
    ],
    [
     "Karan",
     "Shetty"
    ],
    [
     "Andreas",
     "Stolcke"
    ]
   ],
   "title": "Toward Fairness in Speech Recognition: Discovery and mitigation of performance disparities",
   "original": "10816",
   "page_count": 5,
   "order": 258,
   "p1": 1268,
   "pn": 1272,
   "abstract": [
    "As for other forms of AI, speech recognition has recently been examined with respect to performance disparities across different user cohorts. One approach to achieve fairness in speech recognition is to (1) identify speaker cohorts that suffer from subpar performance and (2) apply fairness mitigation measures targeting the cohorts so discovered. In this paper, we report on initial findings with both discovery and mitigation of performance disparities using data from a product-scale AI assistant speech recognition system. We compare cohort discovery based on geographic and demographic information to a more scalable method that groups speakers without human labels, using speaker embedding technology. For fairness mitigation, we find that oversampling of underrepresented cohorts, as well as modeling speaker cohort membership by additional input variables, is able to reduce the gap between top- and bottom-performing cohorts, without deteriorating overall recognition accuracy. Index Terms: speech recognition, performance fairness, cohort discovery."
   ],
   "doi": "10.21437/Interspeech.2022-10816"
  },
  "tran22c_interspeech": {
   "authors": [
    [
     "Kelvin",
     "Tran"
    ],
    [
     "Lingfeng",
     "Xu"
    ],
    [
     "Gabriela",
     "Stegmann"
    ],
    [
     "Julie",
     "Liss"
    ],
    [
     "Visar",
     "Berisha"
    ],
    [
     "Rene",
     "Utianski"
    ]
   ],
   "title": "Investigating the Impact of Speech Compression on the Acoustics of Dysarthric Speech",
   "original": "10817",
   "page_count": 5,
   "order": 461,
   "p1": 2263,
   "pn": 2267,
   "abstract": [
    "Acoustic analysis plays an important role in the assessment of dysarthria. Out of a public health necessity, telepractice has become increasingly adopted as the modality in which clinical care is given. While there are differences in software among telepractice platforms, they all use some form of speech compression to preserve bandwidth, with the most common algorithm being the Opus codec. Opus has been optimized for compression of speech from the general (mostly healthy) population. As a result, for speech-language pathologists, this begs the question: is the remotely transmitted speech signal a faithful representation of dysarthric speech? Existing high-fidelity audio recordings from 20 speakers of various dysarthria types were encoded at three different bit rates defined within Opus to simulate different internet bandwidth conditions. Acoustic measures of articulation, voice, and prosody were extracted, and mixed-effect models were used to evaluate the impact of bandwidth conditions on the measures. Significant differences in cepstral peak prominence, degree of voice breaks, jitter, vowel space area, pitch, and vowel space area were observed after Opus processing, providing insight into the types of acoustic measures that are susceptible to speech compression algorithms."
   ],
   "doi": "10.21437/Interspeech.2022-10817"
  },
  "huang22j_interspeech": {
   "authors": [
    [
     "W. Ronny",
     "Huang"
    ],
    [
     "Cal",
     "Peyser"
    ],
    [
     "Tara",
     "Sainath"
    ],
    [
     "Ruoming",
     "Pang"
    ],
    [
     "Trevor D.",
     "Strohman"
    ],
    [
     "Shankar",
     "Kumar"
    ]
   ],
   "title": "Sentence-Select: Large-Scale Language Model Data Selection for Rare-Word Speech Recognition",
   "original": "10820",
   "page_count": 5,
   "order": 139,
   "p1": 689,
   "pn": 693,
   "abstract": [
    "Language model fusion helps smart assistants recognize words which are rare in acoustic data but abundant in text-only corpora (typed search logs). However, such corpora have properties that hinder downstream performance, including being (1) too large, (2) beset with domain-mismatched content, and (3) heavy-headed rather than heavy-tailed (excessively many duplicate search queries such as \"weather\"). We show that three simple strategies for selecting language modeling data can dramatically improve rare-word recognition without harming overall performance. First, to address the heavy-headedness, we downsample the data according to a soft log function, which tunably reduces high frequency (head) sentences. Second, to encourage rare-word exposure, we explicitly filter for words rare in the acoustic data. Finally, we tackle domain-mismatch via perplexity-based contrastive selection, filtering for examples matched to the target domain. We down-select a large corpus of web search queries by a factor of 53x and achieve better LM perplexities than without down-selection. When shallow-fused with a state-of-the-art, production speech engine, our LM achieves WER reductions of up to 24\\% relative on rare-word sentences (without changing overall WER) compared to a baseline LM trained on the raw corpus. These gains are further validated through favorable side-by-side evaluations on live voice search traffic."
   ],
   "doi": "10.21437/Interspeech.2022-10820"
  },
  "barker22_interspeech": {
   "authors": [
    [
     "Jon",
     "Barker"
    ],
    [
     "Michael",
     "Akeroyd"
    ],
    [
     "Trevor J.",
     "Cox"
    ],
    [
     "John F.",
     "Culling"
    ],
    [
     "Jennifer",
     "Firth"
    ],
    [
     "Simone",
     "Graetzer"
    ],
    [
     "Holly",
     "Griffiths"
    ],
    [
     "Lara",
     "Harris"
    ],
    [
     "Graham",
     "Naylor"
    ],
    [
     "Zuzanna",
     "Podwinska"
    ],
    [
     "Eszter",
     "Porter"
    ],
    [
     "Rhoddy Viveros",
     "Munoz"
    ]
   ],
   "title": "The 1st Clarity Prediction Challenge: A machine learning challenge for hearing aid intelligibility prediction",
   "original": "10821",
   "page_count": 5,
   "order": 710,
   "p1": 3508,
   "pn": 3512,
   "abstract": [
    "This paper reports on the design and outcomes of the 1st Clarity Prediction Challenge (CPC1) for predicting the intelligibility of hearing aid processed signals heard by individuals with a hearing impairment. The challenge was designed to promote the development of new intelligibility measures suitable for use in developing hearing aid algorithms. Participants were supplied with listening test data compromising 7233 responses from 27 individuals. Data was split between training and test sets in a manner that fostered a machine learning approach and allowed both closed-set (known listeners) and open-set (unseen listener/unseen system) evaluation. The paper provides a description of the challenge design including the datasets, the hearing aid algorithms applied, the listeners and the perceptual tests. The challenge attracted submissions from 15 systems. The results are reviewed and the paper summarises, compares and contrasts approaches."
   ],
   "doi": "10.21437/Interspeech.2022-10821"
  },
  "lebourdais22_interspeech": {
   "authors": [
    [
     "Martin",
     "Lebourdais"
    ],
    [
     "Marie",
     "Tahon"
    ],
    [
     "Antoine",
     "LAURENT"
    ],
    [
     "Sylvain",
     "Meignier"
    ]
   ],
   "title": "Overlapped speech and gender detection with WavLM pre-trained features",
   "original": "10825",
   "page_count": 5,
   "order": 1014,
   "p1": 5010,
   "pn": 5014,
   "abstract": [
    "This article focuses on overlapped speech and gender detection in order to study interactions between women and men in French audiovisual media (Gender Equality Monitoring project). In this application context, we need to automatically segment the speech signal according to speakers gender, and to identify when at least two speakers speak at the same time. We propose to use WavLM model which has the advantage of being pre-trained on a huge amount of speech data, to build an overlapped speech detection (OSD) and a gender detection (GD) systems. In this study, we use two different corpora. The DIHARD III corpus which is well adapted for the OSD task but lack gender information. The ALLIES corpus fits with the project application context. Our best OSD system is a Temporal Convolutional Network (TCN) with WavLM pre-trained features as input, which reaches a new state-of-the-art F1-score performance on DIHARD. A neural GD is trained with WavLM inputs on a gender balanced subset of the French broadcast news ALLIES data, and obtains an accuracy of 94.9%. This work opens new perspectives for human science researchers regarding the differences of representation between women and men in French media."
   ],
   "doi": "10.21437/Interspeech.2022-10825"
  },
  "zezario22_interspeech": {
   "authors": [
    [
     "Ryandhimas Edo",
     "Zezario"
    ],
    [
     "Szu-wei",
     "Fu"
    ],
    [
     "Fei",
     "Chen"
    ],
    [
     "Chiou-Shann",
     "Fuh"
    ],
    [
     "Hsin-Min",
     "Wang"
    ],
    [
     "Yu",
     "Tsao"
    ]
   ],
   "title": "MTI-Net: A Multi-Target Speech Intelligibility Prediction Model",
   "original": "10828",
   "page_count": 5,
   "order": 1107,
   "p1": 5463,
   "pn": 5467,
   "abstract": [
    "Recently, deep learning (DL)-based non-intrusive speech assessment models have attracted great attention. Many studies report that these DL-based models yield satisfactory assessment performance and good flexibility, but their performance in unseen environments remains a challenge. Furthermore, compared to quality scores, fewer studies elaborate deep learning models to estimate intelligibility scores. This study proposes a multi-task speech intelligibility prediction model, called MTI-Net, for simultaneously predicting human and machine intelligibility measures. Specifically, given a speech utterance, MTI-Net is designed to predict human subjective listening test results and word error rate (WER) scores. We also investigate several methods that can improve the prediction performance of MTI-Net. First, we compare different features (including low-level features and embeddings from self-supervised learning (SSL) models) and prediction targets of MTI-Net. Second, we explore the effect of transfer learning and multi-tasking learning on training MTI-Net. Finally, we examine the potential advantages of fine-tuning SSL embeddings. Experimental results demonstrate the effectiveness of using cross-domain features, multi-task learning, and fine-tuning SSL embeddings. Furthermore, it is confirmed that the intelligibility and WER scores predicted by MTI-Net are highly correlated with the ground-truth scores."
   ],
   "doi": "10.21437/Interspeech.2022-10828"
  },
  "diener22_interspeech": {
   "authors": [
    [
     "Lorenz",
     "Diener"
    ],
    [
     "Sten",
     "Sootla"
    ],
    [
     "Solomiya",
     "Branets"
    ],
    [
     "Ando",
     "Saabas"
    ],
    [
     "Robert",
     "Aichner"
    ],
    [
     "Ross",
     "Cutler"
    ]
   ],
   "title": "INTERSPEECH 2022 Audio Deep Packet Loss Concealment Challenge",
   "original": "10829",
   "page_count": 5,
   "order": 117,
   "p1": 580,
   "pn": 584,
   "abstract": [
    "Audio Packet Loss Concealment (PLC) is the hiding of gaps in audio streams caused by data transmission failures in packet switched networks. This is a common problem, and of increasing importance as end-to-end VoIP telephony and teleconference systems become the default and ever more widely used form of communication in business as well as in personal usage. This paper presents the INTERSPEECH 2022 Audio Deep Packet Loss Concealment challenge. We first give an overview of the PLC problem, and introduce some classical approaches to PLC as well as recent work. We then present the open source dataset released as part of this challenge as well as the evaluation methods and metrics used to determine the winner. We also briefly introduce PLCMOS, a novel data-driven metric that can be used to quickly evaluate the performance PLC systems. Finally, we present the results of the INTERSPEECH 2022 Audio Deep PLC Challenge, and provide a summary of important takeaways."
   ],
   "doi": "10.21437/Interspeech.2022-10829"
  },
  "buech22b_interspeech": {
   "authors": [
    [
     "Philipp",
     "Buech"
    ],
    [
     "Rachid",
     "Ridouane"
    ],
    [
     "Anne",
     "Hermes"
    ]
   ],
   "title": "Pharyngealization in Amazigh: Acoustic and articulatory marking over time",
   "original": "10831",
   "page_count": 5,
   "order": 698,
   "p1": 3448,
   "pn": 3452,
   "abstract": [
    "Pharyngealization refers to a secondary articulation whereby a set of consonants is produced with the backing of the tongue towards the pharyngeal wall. This typologically rare phenomenon appears in some Afroasiatic languages, including Arabic and Amazigh. While the phonetic characteristics of pharyngealization in Arabic have been investigated in-depth, the comparative data currently available on Amazigh is particularly scant. The present study aims to fill this gap and provides the first comprehensive study on the phonetic characteristics of pharyngealization in an Amazigh language. The empirical data comes from acoustic and articulatory recordings of six Tashlhiyt speakers producing a set of singleton and geminate plain coronals and their pharyngealized counterparts in intervocalic position. We analyze formant trajectories of the vowels surrounding plain and pharyngealized consonants and articulatory trajectories during the consonantal movements and examine how the manner of articulation, voicing, and length shape variability in the way the contrast is implemented. Results show that pharyngealization is principally characterized by a large drop of F2 of adjacent vowels at the acoustic level and an extensive lowering of the tongue body at the articulatory level. These attributes are consistent across voicing, length and manner of articulation."
   ],
   "doi": "10.21437/Interspeech.2022-10831"
  },
  "helwani22_interspeech": {
   "authors": [
    [
     "Karim",
     "Helwani"
    ],
    [
     "Erfan",
     "Soltanmohammadi"
    ],
    [
     "Michael Mark",
     "Goodwin"
    ],
    [
     "Arvindh",
     "Krishnaswamy"
    ]
   ],
   "title": "Clock Skew Robust Acoustic Echo Cancellation",
   "original": "10833",
   "page_count": 5,
   "order": 515,
   "p1": 2533,
   "pn": 2537,
   "abstract": [
    "Traditional acoustic echo cancelers require that the reference and microphone signals have exactly the same sampling frequency. In this paper, we present a novel Kalman filtering approach to acoustic echo cancellation (AEC) which blindly accounts for the clock skew between the playback and recording devices without the need for exchanging time stamps when they have independent clocks. The proposed Kalman-filter AEC em- ploys a state-space formulation for the clock-skew problem and is implemented in a subband-domain based on a new variation of the complex modified discrete cosine transform which allows for arbitrary hop size and therefore enhanced time resolution. We show that the proposed algorithm is robust to Gaussian and super-Gaussian near-end noises and provide experimental results which demonstrate the state-of-the-art echo cancellation performance of the approach under clock-skew conditions."
   ],
   "doi": "10.21437/Interspeech.2022-10833"
  },
  "yadav22_interspeech": {
   "authors": [
    [
     "Sarthak",
     "Yadav"
    ],
    [
     "Neil",
     "Zeghidour"
    ]
   ],
   "title": "Learning neural audio features without supervision",
   "original": "10834",
   "page_count": 5,
   "order": 80,
   "p1": 396,
   "pn": 400,
   "abstract": [
    "Deep audio classification, traditionally cast as training a deep neural network on top of mel-filterbanks in a supervised fashion, has recently benefited from two independent lines of work. The first one explores \"learnable frontends'', i.e., neural modules that produce a learnable time-frequency representation, to overcome limitations of fixed features. The second one uses self-supervised learning to leverage unprecedented scales of pre-training data. In this work, we study the feasibility of combining both approaches, i.e., pre-training learnable frontend jointly with the main architecture for downstream classification. First, we show that pretraining two previously proposed frontends (SincNet and LEAF) on Audioset drastically improves linear-probe performance over fixed mel-filterbanks, suggesting that learnable time-frequency representations can benefit self-supervised pre-training even more than supervised training. Surprisingly, randomly initialized learnable filterbanks outperform mel-scaled initialization in the self-supervised setting, a counter-intuitive result that questions the appropriateness of strong priors when designing learnable filters. Through exploratory analysis of the learned frontend components, we uncover crucial differences in properties of these frontends when used in a supervised and self-supervised setting, especially the affinity of self-supervised filters to diverge significantly from the mel-scale to model a broader range of frequencies."
   ],
   "doi": "10.21437/Interspeech.2022-10834"
  },
  "damania22_interspeech": {
   "authors": [
    [
     "Ronit",
     "Damania"
    ],
    [
     "Christopher",
     "Homan"
    ],
    [
     "Emily",
     "Prud'hommeaux"
    ]
   ],
   "title": "Combining Simple but Novel Data Augmentation Methods for Improving Conformer ASR",
   "original": "10835",
   "page_count": 5,
   "order": 990,
   "p1": 4890,
   "pn": 4894,
   "abstract": [
    "In this paper, we propose several novel data augmentation methods for improving the performance of automatic speech recognition (ASR) in low-resource settings. Using a 100-hour subset of English LibriSpeech to simulate a low-resource setting, we compare the well-known SpecAugment approach to these new methods, along with several other competitive baselines. We then apply the most promising combinations of models and augmentation methods to three genuinely under-resourced languages using the 40-hour Gujarati, Tamil, Telugu datasets from the 2021 Interspeech Low Resource Automatic Speech Recognition Challenge for Indian Languages. Our data augmentation approaches, coupled with state-of-the-art acoustic model architectures and language models, yield reductions in word error rate over SpecAugment and other competitive baselines for the LibriSpeech-100 dataset, showing a particular advantage over prior models for the ``other'', more challenging, dev and test sets. Extending this work to the low-resource Indian languages, we see large improvements over the baseline models and results comparable to large multilingual models."
   ],
   "doi": "10.21437/Interspeech.2022-10835"
  },
  "edozezario22_interspeech": {
   "authors": [
    [
     "Ryandhimas",
     "Edo Zezario"
    ],
    [
     "Fei",
     "Chen"
    ],
    [
     "Chiou-Shann",
     "Fuh"
    ],
    [
     "Hsin-Min",
     "Wang"
    ],
    [
     "Yu",
     "Tsao"
    ]
   ],
   "title": "MBI-Net: A Non-Intrusive Multi-Branched Speech Intelligibility Prediction Model for Hearing Aids",
   "original": "10838",
   "page_count": 5,
   "order": 799,
   "p1": 3944,
   "pn": 3948,
   "abstract": [
    "Improving the user's hearing ability to understand speech in noisy environments is critical to the development of hearing aid (HA) devices. For this, it is important to derive a metric that can fairly predict speech intelligibility for HA users. A straightforward approach is to conduct a subjective listening test and use the test results as an evaluation metric. However, conducting large-scale listening tests is time-consuming and expensive. Therefore, several evaluation metrics were derived as surrogates for subjective listening test results. In this study, we propose a multi-branched speech intelligibility prediction model (MBI-Net), for predicting the subjective intelligibility scores of HA users. MBI-Net consists of two branches of models, with each branch consisting of a hearing loss model, a cross-domain feature extraction module, and a speech intelligibility prediction model, to process speech signals from one channel. The outputs of the two branches are fused through a linear layer to obtain predicted speech intelligibility scores. Experimental results confirm the effectiveness of MBI-Net, which produces higher prediction scores than the baseline system in Track 1 and Track 2 on the Clarity Prediction Challenge 2022 dataset."
   ],
   "doi": "10.21437/Interspeech.2022-10838"
  },
  "chang22g_interspeech": {
   "authors": [
    [
     "Xuankai",
     "Chang"
    ],
    [
     "Takashi",
     "Maekaku"
    ],
    [
     "Yuya",
     "Fujita"
    ],
    [
     "Shinji",
     "Watanabe"
    ]
   ],
   "title": "End-to-End Integration of Speech Recognition, Speech Enhancement, and Self-Supervised Learning Representation",
   "original": "10839",
   "page_count": 5,
   "order": 774,
   "p1": 3819,
   "pn": 3823,
   "abstract": [
    "This work presents our end-to-end (E2E) automatic speech recognition (ASR) model targetting at robust speech recognition, called Integraded speech Recognition with enhanced speech Input for Self-supervised learning representation (IRIS). Compared with conventional E2E ASR models, the proposed E2E model integrates two important modules including a speech enhancement (SE) module and a self-supervised learning representation (SSLR) module. The SE module enhances the noisy speech. Then the SSLR module extracts features from enhanced speech to be used for speech recognition (ASR). To train the proposed model, we establish an efficient learning scheme. Evaluation results on the monaural CHiME-4 task show that the IRIS model achieves the best performance reported in the literature for the single-channel CHiME-4 benchmark (2.0% for the real development and 3.6% for the real test) thanks to the powerful pre-trained SSLR module and the fine-tuned SE module."
   ],
   "doi": "10.21437/Interspeech.2022-10839"
  },
  "koch22_interspeech": {
   "authors": [
    [
     "Julia",
     "Koch"
    ],
    [
     "Florian",
     "Lux"
    ],
    [
     "Nadja",
     "Schauffler"
    ],
    [
     "Toni",
     "Bernhart"
    ],
    [
     "Felix",
     "Dieterle"
    ],
    [
     "Jonas",
     "Kuhn"
    ],
    [
     "Sandra",
     "Richter"
    ],
    [
     "Gabriel",
     "Viehhauser"
    ],
    [
     "Ngoc",
     "Thang Vu"
    ]
   ],
   "title": "PoeticTTS - Controllable Poetry Reading for Literary Studies",
   "original": "10841",
   "page_count": 5,
   "order": 249,
   "p1": 1223,
   "pn": 1227,
   "abstract": [
    "Speech synthesis for poetry is challenging due to specific intonation patterns inherent to poetic speech. In this work, we propose an approach to synthesise poems with almost human like naturalness in order to enable literary scholars to systematically examine hypotheses on the interplay between text, spoken realisation, and the listener's perception of poems. To meet these special requirements for literary studies, we resynthesise poems by cloning prosodic values from a human reference recitation, and afterwards make use of fine-grained prosody control to manipulate the synthetic speech in a human-in-the-loop setting to alter the recitation w.r.t. specific phenomena. We find that finetuning our TTS model on poetry captures poetic intonation patterns to a large extent which is beneficial for prosody cloning and manipulation and verify the success of our approach both in an objective evaluation as well as in human studies."
   ],
   "doi": "10.21437/Interspeech.2022-10841"
  },
  "deadman22_interspeech": {
   "authors": [
    [
     "Jack",
     "Deadman"
    ],
    [
     "Jon",
     "Barker"
    ]
   ],
   "title": "Modelling Turn-taking in Multispeaker Parties for Realistic Data Simulation",
   "original": "10842",
   "page_count": 5,
   "order": 54,
   "p1": 266,
   "pn": 270,
   "abstract": [
    "Simulation plays a crucial role in developing components of automatic speech recognition systems such as enhancement and diarization. In source separation and target-speaker extraction, datasets with high degrees of temporal overlap are used both in training and evaluation. However, this contrasts with the fact that people tend to avoid such overlap in real conversations. It is well known that artifacts introduced from pre-processing with no overlapping speech can be detrimental to recognition performance. This work proposes a finite-state based generative method trained on timing information in speech corpora, which leads to two main contributions. First, a method for generating arbitrary large datasets which follow desired statistics of real parties. Second, features extracted from the models are shown to have a correlation with speaker extraction performance. This leads to the contribution of quantifying how much difficulty in a mixture is due to turn-taking, factoring out other complexities in the signal. Models which treat speakers as independent produce poor generation and representation results. We improve upon this by proposing models which have states conditioned on whether another person is speaking."
   ],
   "doi": "10.21437/Interspeech.2022-10842"
  },
  "paturi22_interspeech": {
   "authors": [
    [
     "Rohit",
     "Paturi"
    ],
    [
     "Sundararajan",
     "Srinivasan"
    ],
    [
     "Katrin",
     "Kirchhoff"
    ],
    [
     "Daniel",
     "Garcia-Romero"
    ]
   ],
   "title": "Directed speech separation for automatic speech recognition of long form conversational speech",
   "original": "10843",
   "page_count": 5,
   "order": 1092,
   "p1": 5388,
   "pn": 5392,
   "abstract": [
    "Many of the recent advances in speech separation are primarily aimed at synthetic mixtures of short audio utterances with high degrees of overlap. Most of these approaches need an additional stitching step to stitch the separated speech chunks for long form audio. Since most of the approaches involve Permutation Invariant training (PIT), the order of separated speech chunks is nondeterministic and leads to difficulty in accurately stitching homogenous speaker chunks for downstream tasks like Automatic Speech Recognition (ASR). Also, most of these models are trained with synthetic mixtures and do not generalize to real conversational data. In this paper, we propose a speaker conditioned separator trained on speaker embeddings extracted directly from the mixed signal using an over-clustering based approach. This model naturally regulates the order of the separated chunks without the need for an additional stitching step. We also introduce a data sampling strategy with real and synthetic mixtures which generalizes well to real conversation speech. With this model and data sampling technique, we show significant improvements in speaker-attributed word error rate (SA-WER) on Hub5 data."
   ],
   "doi": "10.21437/Interspeech.2022-10843"
  },
  "radfar22_interspeech": {
   "authors": [
    [
     "Martin",
     "Radfar"
    ],
    [
     "Rohit",
     "Barnwal"
    ],
    [
     "Rupak Vignesh",
     "Swaminathan"
    ],
    [
     "Feng-Ju",
     "Chang"
    ],
    [
     "Grant P.",
     "Strimel"
    ],
    [
     "Nathan",
     "Susanj"
    ],
    [
     "Athanasios",
     "Mouchtaris"
    ]
   ],
   "title": "ConvRNN-T: Convolutional Augmented Recurrent Neural Network Transducers for Streaming Speech Recognition",
   "original": "10844",
   "page_count": 5,
   "order": 898,
   "p1": 4431,
   "pn": 4435,
   "abstract": [
    "The recurrent neural network transducer (RNN-T) is a prominent streaming end-to-end (E2E) ASR technology. In RNN-T, the acoustic encoder commonly consists of stacks of LSTMs. Very recently, as an alternative to LSTM layers, the Conformer architecture was introduced where the encoder of RNN-T is replaced with a modified Transformer encoder composed of convolutional layers at the frontend and between attention layers. In this paper, we introduce a new streaming ASR model, Convolutional Augmented Recurrent Neural Network Transducers (ConvRNN-T) in which we augment the LSTM-based RNN-T with a novel convolutional frontend consisting of local and global context CNN encoders. ConvRNN-T takes advantage of causal 1-D convolutional layers, squeeze-and-excitation, dilation, and residual blocks to provide both global and local audio context representation to LSTM layers. We show ConvRNN-T outperforms RNN-T, Conformer, and ContextNet on Librispeech and in-house data. In addition, ConvRNN-T offers less computational complexity compared to Conformer. ConvRNN-T's superior accuracy along with its low footprint make it a promising candidate for on-device streaming ASR technologies."
   ],
   "doi": "10.21437/Interspeech.2022-10844"
  },
  "lee22m_interspeech": {
   "authors": [
    [
     "Yoonjeong",
     "Lee"
    ],
    [
     "Jody",
     "Kreiman"
    ]
   ],
   "title": "Linguistic versus biological factors governing acoustic voice variation",
   "original": "10847",
   "page_count": 4,
   "order": 129,
   "p1": 640,
   "pn": 643,
   "abstract": [
    "This study presents a cross-linguistic investigation of acoustic voice spaces in English, Seoul Korean, and White Hmong, which differ in whether they phonologically contrast phonation type and/or tone. The overarching hypothesis is that acoustic variability in voice will be shaped by biological factors, linguistic factors, and individual idiosyncrasies. By employing principal component analysis on speakers' read speech productions, we identify how individual and population voice spaces are acoustically structured for speakers of these three languages. Results revealed several factors that consistently account for acoustic variability across speakers and languages, but also factors that vary with language-specific phonology."
   ],
   "doi": "10.21437/Interspeech.2022-10847"
  },
  "chetupalli22_interspeech": {
   "authors": [
    [
     "Srikanth Raj",
     "Chetupalli"
    ],
    [
     "Emanuël",
     "Habets"
    ]
   ],
   "title": "Speech Separation for an Unknown Number of Speakers Using Transformers With Encoder-Decoder Attractors",
   "original": "10849",
   "page_count": 5,
   "order": 1093,
   "p1": 5393,
   "pn": 5397,
   "abstract": [
    "Speaker-independent speech separation for single-channel mixtures with an unknown number of multiple speakers in the waveform domain is considered in this paper. To deal with the unknown number of sources, we incorporate an encoder-decoder attractor (EDA) module into a speech separation network. The neural network architecture consists of a trainable encoder-decoder pair and a masking network. The mask network in the proposed approach is inspired by the transformer-based SepFormer separation system. It contains a dual-path block and a triple path block, each block modeling both short-time and long-time dependencies in the signal. The EDA module first summarises the dual-path block output using an LSTM encoder and generates one attractor vector per speaker in the mixture using an LSTM decoder. The attractors are combined with the dual-path block output to generate speaker channels, which are processed jointly by the triple-path block to predict the mask. Further, a linear-sigmoid layer, with attractors as the input, predicts a binary output to indicate a stopping criterion for attractor generation. The proposed approach is evaluated on the WSJ0-mix dataset with mixtures of up to five speakers. State-of-the-art results are obtained in the speech separation quality and speaker counting for all the mixtures."
   ],
   "doi": "10.21437/Interspeech.2022-10849"
  },
  "meyer22c_interspeech": {
   "authors": [
    [
     "Josh",
     "Meyer"
    ],
    [
     "David",
     "Adelani"
    ],
    [
     "Edresson",
     "Casanova"
    ],
    [
     "Alp",
     "Öktem"
    ],
    [
     "Daniel",
     "Whitenack"
    ],
    [
     "Julian",
     "Weber"
    ],
    [
     "Salomon",
     "KABONGO KABENAMUALU"
    ],
    [
     "Elizabeth",
     "Salesky"
    ],
    [
     "Iroro",
     "Orife"
    ],
    [
     "Colin",
     "Leong"
    ],
    [
     "Perez",
     "Ogayo"
    ],
    [
     "Chris",
     "Chinenye Emezue"
    ],
    [
     "Jonathan",
     "Mukiibi"
    ],
    [
     "Salomey",
     "Osei"
    ],
    [
     "Apelete",
     "AGBOLO"
    ],
    [
     "Victor",
     "Akinode"
    ],
    [
     "Bernard",
     "Opoku"
    ],
    [
     "Olanrewaju",
     "Samuel"
    ],
    [
     "Jesujoba",
     "Alabi"
    ],
    [
     "Shamsuddeen Hassan",
     "Muhammad"
    ]
   ],
   "title": "BibleTTS: a large, high-fidelity, multilingual, and uniquely African speech corpus",
   "original": "10850",
   "page_count": 5,
   "order": 485,
   "p1": 2383,
   "pn": 2387,
   "abstract": [
    "BibleTTS is a large, high-quality, open speech dataset for ten languages spoken in Sub-Saharan Africa. The corpus contains up to 86 hours of aligned, studio quality 48kHz single speaker recordings per language, enabling the development of high-quality text-to-speech models. The ten languages represented are: Akuapem Twi, Asante Twi, Chichewa, Ewe, Hausa, Kikuyu, Lingala, Luganda, Luo, and Yoruba. This corpus is a derivative work of Bible recordings made and released by the Open.Bible project from Biblica. We have aligned, cleaned, and filtered the original recordings, and additionally hand-checked a subset of the alignments for each language. We present results for text-to-speech models with Coqui TTS. The data is released under a commercial-friendly CC-BY-SA license."
   ],
   "doi": "10.21437/Interspeech.2022-10850"
  },
  "kapelonis22_interspeech": {
   "authors": [
    [
     "Eleftherios",
     "Kapelonis"
    ],
    [
     "Efthymios",
     "Georgiou"
    ],
    [
     "Alexandros",
     "Potamianos"
    ]
   ],
   "title": "A Multi-Task BERT Model for Schema-Guided Dialogue State Tracking",
   "original": "10852",
   "page_count": 5,
   "order": 555,
   "p1": 2733,
   "pn": 2737,
   "abstract": [
    "Task-oriented dialogue systems often employ a Dialogue State Tracker (DST) to successfully complete conversations. Recent state-of-the-art DST implementations rely on schemata of diverse services to improve model robustness and handle zero-shot generalization to new domains [1], however such methods [2, 3] typically require multiple large scale transformer models and long input sequences to perform well. We propose a single multi-task BERT-based model that jointly solves the three DST tasks of intent prediction, requested slot prediction and slot filling. Moreover, we propose an efficient and parsimonious encoding of the dialogue history and service schemata that is shown to further improve performance. Evaluation on the SGD dataset shows that our approach outperforms the baseline SGP-DST by a large margin and performs well compared to the state-of-the-art, while being significantly more computationally efficient. Extensive ablation studies are performed to examine the contributing factors to the success of our model."
   ],
   "doi": "10.21437/Interspeech.2022-10852"
  },
  "laptev22_interspeech": {
   "authors": [
    [
     "Aleksandr",
     "Laptev"
    ],
    [
     "Somshubra",
     "Majumdar"
    ],
    [
     "Boris",
     "Ginsburg"
    ]
   ],
   "title": "CTC Variations Through New WFST Topologies",
   "original": "10854",
   "page_count": 5,
   "order": 212,
   "p1": 1041,
   "pn": 1045,
   "abstract": [
    "This paper presents novel Weighted Finite-State Transducer (WFST) topologies to implement Connectionist Temporal Classification (CTC)-like algorithms for automatic speech recognition. Three new CTC variants are proposed: (1) the \"compact- CTC”, in which direct transitions between units are replaced with 〈ε〉 back-off transitions; (2) the \"minimal-CTC”, that only adds 〈blank〉 self-loops when used in WFST-composition; and (3) the \"selfless-CTC” variants, which disallows self-loop for non-blank units. Compact-CTC allows for 1.5 times smaller WFST decoding graphs and reduces memory consumption by two times when training CTC models with the LF-MMI objective without hurting the recognition accuracy. Minimal-CTC reduces graph size and memory consumption by two and four times for the cost of a small accuracy drop. Using selfless-CTC can improve the accuracy for wide context window models."
   ],
   "doi": "10.21437/Interspeech.2022-10854"
  },
  "vanrijn22_interspeech": {
   "authors": [
    [
     "Pol",
     "van Rijn"
    ],
    [
     "Silvan",
     "Mertes"
    ],
    [
     "Dominik",
     "Schiller"
    ],
    [
     "Piotr",
     "Dura"
    ],
    [
     "Hubert",
     "Siuzdak"
    ],
    [
     "Peter M. C.",
     "Harrison"
    ],
    [
     "Elisabeth",
     "André"
    ],
    [
     "Nori",
     "Jacoby"
    ]
   ],
   "title": "VoiceMe: Personalized voice generation in TTS",
   "original": "10855",
   "page_count": 5,
   "order": 526,
   "p1": 2588,
   "pn": 2592,
   "abstract": [
    "Novel text-to-speech systems can generate entirely new voices that were not seen during training. However, it remains a difficult task to efficiently create personalized voices from a high-dimensional speaker space. In this work, we use speaker embeddings from a state-of-the-art speaker verification model (SpeakerNet) trained on thousands of speakers to condition a TTS model. We employ a human sampling paradigm to explore this speaker latent space. We show that users can create voices that fit well to photos of faces, art portraits, and cartoons. We recruit online participants to collectively manipulate the voice of a speaking face. We show that (1) a separate group of human raters confirms that the created voices match the faces, (2) speaker gender apparent from the face is well-recovered in the voice, and (3) people are consistently moving towards the real voice prototype for the given face. Our results demonstrate that this technology can be applied in a wide number of applications including character voice development in audiobooks and games, personalized speech assistants, and individual voices for people with speech impairment."
   ],
   "doi": "10.21437/Interspeech.2022-10855"
  },
  "klapsas22_interspeech": {
   "authors": [
    [
     "Konstantinos",
     "Klapsas"
    ],
    [
     "Nikolaos",
     "Ellinas"
    ],
    [
     "Karolos",
     "Nikitaras"
    ],
    [
     "Georgios",
     "Vamvoukakis"
    ],
    [
     "Panagiotis",
     "Kakoulidis"
    ],
    [
     "Konstantinos",
     "Markopoulos"
    ],
    [
     "Spyros",
     "Raptis"
    ],
    [
     "June Sig",
     "Sung"
    ],
    [
     "Gunu",
     "Jho"
    ],
    [
     "Aimilios",
     "Chalamandaris"
    ],
    [
     "Pirros",
     "Tsiakoulis"
    ]
   ],
   "title": "Self supervised learning for robust voice cloning",
   "original": "10856",
   "page_count": 5,
   "order": 999,
   "p1": 4935,
   "pn": 4939,
   "abstract": [
    "Voice cloning is a difficult task which requires robust and informative features incorporated in a high quality TTS system in order to effectively copy an unseen speaker's voice. In our work, we utilize features learned in a self-supervised framework via the Bootstrap Your Own Latent (BYOL) method, which is shown to produce high quality speech representations when specific audio augmentations are applied to the vanilla algorithm. We further extend the augmentations in the training procedure to aid the resulting features to capture the speaker identity and to make them robust to noise and acoustic conditions. The learned features are used as pre-trained utterance-level embeddings and as inputs to a Non-Attentive Tacotron based architecture, aiming to achieve multispeaker speech synthesis without utilizing additional speaker features. This method enables us to train our model in an unlabeled multispeaker dataset as well as use unseen speaker embeddings to copy a speaker's voice. Subjective and objective evaluations are used to validate the proposed model, as well as the robustness to the acoustic conditions of the target utterance."
   ],
   "doi": "10.21437/Interspeech.2022-10856"
  },
  "liu22v_interspeech": {
   "authors": [
    [
     "Yuchen",
     "Liu"
    ],
    [
     "Li-Chia",
     "Yang"
    ],
    [
     "Alexander",
     "Pawlicki"
    ],
    [
     "Marko",
     "Stamenovic"
    ]
   ],
   "title": "CCATMos: Convolutional Context-aware Transformer Network for Non-intrusive Speech Quality Assessment",
   "original": "10857",
   "page_count": 5,
   "order": 672,
   "p1": 3318,
   "pn": 3322,
   "abstract": [
    "Speech quality assessment has been a critical component in many voice communication related applications such as telephony and online conferencing. Traditional intrusive speech assessment requires the clean reference of the degraded utterance to provide an accurate quality measurement. This requirement limits the usability of these methods in real-world scenarios. On the other hand, non-intrusive subjective measurement is the \"golden standard\" in evaluating speech quality as human listeners can intrinsically evaluate the quality of any degraded speech with ease. In this paper, we propose a novel end-to-end model structure called Convolutional Context-Aware Transformer (CCAT) network to predict the mean opinion score (MOS) of human raters. We evaluate our model on three MOS-annotated datasets spanning multiple languages and distortion types and submit our results to the ConferencingSpeech 2022 Challenge. Our experiments show that CCAT provides promising MOS predictions compared to current state-of-art non-intrusive speech assessment models with average Pearson correlation coefficient (PCC) increasing from 0.530 to 0.697 and average RMSE decreasing from 0.768 to 0.570 compared to the baseline model on the challenge evaluation test set."
   ],
   "doi": "10.21437/Interspeech.2022-10857"
  },
  "sartzetaki22_interspeech": {
   "authors": [
    [
     "Christina",
     "Sartzetaki"
    ],
    [
     "Georgios",
     "Paraskevopoulos"
    ],
    [
     "Alexandros",
     "Potamianos"
    ]
   ],
   "title": "Extending Compositional Attention Networks for Social Reasoning in Videos",
   "original": "10858",
   "page_count": 5,
   "order": 227,
   "p1": 1116,
   "pn": 1120,
   "abstract": [
    "We propose a novel deep architecture for the task of reasoning about social interactions in videos. We leverage the multistep reasoning capabilities of Compositional Attention Networks (MAC) [1], and propose a multimodal extension (MAC-X). MAC-X is based on a recurrent cell that performs iterative mid-level fusion of input modalities (visual, auditory, text) over multiple reasoning steps, by use of a temporal attention mechanism. We then combine MAC-X with LSTMs for temporal input processing in an end-to-end architecture. Our ablation studies show that the proposed MAC-X architecture can effectively leverage multimodal input cues using mid-level fusion mechanisms. We apply MAC-X to the task of Social Video Question Answering in the Social IQ dataset and obtain a 2.5% absolute improvement in terms of binary accuracy over the current state-of-the-art. Index Terms: Video Question Answering, Social Reasoning, Compositional Attention Networks, MAC"
   ],
   "doi": "10.21437/Interspeech.2022-10858"
  },
  "kadandale22_interspeech": {
   "authors": [
    [
     "Venkatesh Shenoy",
     "Kadandale"
    ],
    [
     "Juan F.",
     "Montesinos"
    ],
    [
     "Gloria",
     "Haro"
    ]
   ],
   "title": "VocaLiST: An Audio-Visual Synchronisation Model for Lips and Voices",
   "original": "10861",
   "page_count": 5,
   "order": 634,
   "p1": 3128,
   "pn": 3132,
   "abstract": [
    "In this paper, we address the problem of lip-voice synchronisation in videos containing human face and voice. Our approach is based on determining if the lips motion and the voice in a video are synchronised or not, depending on their audio-visual correspondence score. We propose an audio-visual cross-modal transformer-based model that outperforms several baseline models in the audio-visual synchronisation task on the standard lip-reading speech benchmark dataset LRS2. While the existing methods focus mainly on lip synchronisation in speech videos, we also consider the special case of the singing voice. The singing voice is a more challenging use case for synchronisation due to sustained vowel sounds. We also investigate the relevance of lip synchronisation models trained on speech datasets in the context of singing voice. Finally, we use the frozen visual features learned by our lip synchronisation model in the singing voice separation task to outperform a baseline audio-visual model which was trained end-to-end. The demos, source code, and the pre-trained models are available on https://ipcv.github.io/VocaLiST/"
   ],
   "doi": "10.21437/Interspeech.2022-10861"
  },
  "zhu22d_interspeech": {
   "authors": [
    [
     "Youxiang",
     "Zhu"
    ],
    [
     "Xiaohui",
     "Liang"
    ],
    [
     "John A.",
     "Batsis"
    ],
    [
     "Robert M.",
     "Roth"
    ]
   ],
   "title": "Domain-aware Intermediate Pretraining for Dementia Detection with Limited Data",
   "original": "10862",
   "page_count": 5,
   "order": 445,
   "p1": 2183,
   "pn": 2187,
   "abstract": [
    "Detecting dementia using human speech is promising but faces a limited data challenge. While recent research has shown general pretrained models (e.g., BERT) can be applied to improve dementia detection, the pretrained model can hardly be fine-tuned with the available small dementia dataset as that would raise the overfitting problem. In this paper, we propose a domain-aware intermediate pretraining to enable a pretraining process using a domain-similar dataset that is selected by incorporating the knowledge from the dementia dataset. Specifically, we use pseudo-perplexity to find an effective pretraining dataset, and then propose dataset-level and sample-level domain-aware intermediate pretraining techniques. We further employ information units (IU) from previous dementia research and define an IU-pseudo-perplexity to reduce calculation complexity. We confirm the effectiveness of perplexity by showing a strong correlation between perplexity and accuracy using 9 datasets and models from the GLUE benchmark. We show that our domain-aware intermediate pretraining improves detection accuracy in almost all cases. Our results suggested that the difference in text-based perplexity values between patients with Alzheimer's Dementia and Healthy Control is still small, and the perplexity incorporating acoustic features (e.g., pause) may make the pretraining more effective."
   ],
   "doi": "10.21437/Interspeech.2022-10862"
  },
  "antonova22_interspeech": {
   "authors": [
    [
     "Alexandra",
     "Antonova"
    ],
    [
     "Evelina",
     "Bakhturina"
    ],
    [
     "Boris",
     "Ginsburg"
    ]
   ],
   "title": "Thutmose Tagger: Single-pass neural model for Inverse Text Normalization",
   "original": "10864",
   "page_count": 5,
   "order": 111,
   "p1": 550,
   "pn": 554,
   "abstract": [
    "Inverse text normalization (ITN) is an essential post-processing step in automatic speech recognition (ASR). It converts numbers, dates, abbreviations, and other semiotic classes from the spoken form generated by ASR to their written forms. One can consider ITN as a Machine Translation task and use neural sequence-to-sequence models to solve it. Unfortunately, such neural models are prone to hallucinations that could lead to unacceptable errors. To mitigate this issue, we propose a single-pass token classifier model that regards ITN as a tagging task. The model assigns a replacement fragment to every input token or marks it for deletion or copying without changes. We present a method of dataset preparation, based on granular alignment of ITN examples. The proposed model is less prone to hallucination errors. The model is trained on the Google Text Normalization dataset and achieves state-of-the-art sentence accuracy on both English and Russian test sets. One-to-one correspondence between tags and input words improves the interpretability of the model's predictions, simplifies debugging, and allows for post-processing corrections. The model is simpler than sequence-to-sequence models and easier to optimize in production settings. The model and the code to prepare the dataset is published as part of NeMo project."
   ],
   "doi": "10.21437/Interspeech.2022-10864"
  },
  "dieck22_interspeech": {
   "authors": [
    [
     "Teena tom",
     "Dieck"
    ],
    [
     "Paula Andrea",
     "Pérez-Toro"
    ],
    [
     "Tomas",
     "Arias"
    ],
    [
     "Elmar",
     "Noeth"
    ],
    [
     "Philipp",
     "Klumpp"
    ]
   ],
   "title": "Wav2vec behind the Scenes: How end2end Models learn Phonetics",
   "original": "10865",
   "page_count": 5,
   "order": 1038,
   "p1": 5130,
   "pn": 5134,
   "abstract": [
    "End2end models became extremely popular in recent years. Whilst they excel at tasks like acoustic modelling or full-fledged speech recognition, the decision making process can be quite complex to retrace due to their black-box character. As end2end models learn high-level feature extraction on-the-fly, outputs from hidden layers from within the network had been used as feature vectors in various studies to perform transfer learning. It is therefore crucial to understand how extracted hidden activations transport information collected from the signal. Furthermore, is the traditional categorization into feature extractor and temporal analysis still applicable on the sub-parts of end2end models? By the example of Wav2vec 2.0, we show how an acoustic model learns to perform a frequency analysis on a speech waveform. Our experiments also show that phonetic information about speech production is preserved in extracted feature vectors. Ultimately, our findings highlight how different parts of an end2end model encode information on an entirely different level. Whilst the influence of gender is quite large on early feature vectors, it vanished after temporal contextualization. At the same time, hidden activations which included context information were superimposed by language-related patterns."
   ],
   "doi": "10.21437/Interspeech.2022-10865"
  },
  "rose22_interspeech": {
   "authors": [
    [
     "Richard",
     "Rose"
    ],
    [
     "Olivier",
     "Siohan"
    ]
   ],
   "title": "End-to-End multi-talker audio-visual ASR using an active speaker attention module",
   "original": "10866",
   "page_count": 5,
   "order": 574,
   "p1": 2828,
   "pn": 2832,
   "abstract": [
    "This paper presents a new approach for end-to-end audio-visual multi-talker speech recognition. The approach, referred to here as the visual context attention model (VCAM), is important because it uses the available video information to assign decoded text to one of multiple visible faces. This essentially resolves the label ambiguity issue associated with most multi-talker modeling approaches which can decode multiple label strings but cannot assign the label strings to the correct speakers. This is implemented as a transformer-transducer based end-to-end model and evaluated using a two speaker simulated audio-visual overlapping speech dataset created from YouTube videos. It is shown in the paper that the VCAM model improves performance with respect to previously reported audio-only and audio-visual multi-talker ASR systems. It is also shown that the attention model, trained end-to-end with the multi-talker A/V ASR model, is able perform active speaker detection on the two speaker overlapped speech test set."
   ],
   "doi": "10.21437/Interspeech.2022-10866"
  },
  "sardhaei22_interspeech": {
   "authors": [
    [
     "Nasim Mahdinazhad",
     "Sardhaei"
    ],
    [
     "Marzena",
     "Zygis"
    ],
    [
     "Hamid",
     "Sharifzadeh"
    ]
   ],
   "title": "How do our eyebrows respond to masks and whispering? The case of Persians",
   "original": "10867",
   "page_count": 5,
   "order": 413,
   "p1": 2023,
   "pn": 2027,
   "abstract": [
    "Whispering is one of the mechanisms of human communication to convey linguistic information. Due to the lack of vocal fold vibration, whispering acoustically differs from the voiced speech in the absence of fundamental frequency which is one of the main prosodic correlates of intonation. This study addresses the importance of facial cues with respect to acoustic cues of intonation. Specifically, we aim to probe how eyebrow velocity and furrowing change when people whisper and wear face masks, also, when they are supposed to produce a prosodic modulation as it is the case in polar questions with rising intonation. To this end, we run an experiment with 10 Persian speakers. The results show the greater mean speed when speakers whisper indicating a compensation effect for the lack of F0 in whispering. We also found a more pronounced movement of both eyebrows when the speakers wear a mask. Finally, our results reveal greater eyebrow motions in questions suggesting the question is a more marked utterance type than a statement. No significant effect of eyebrow furrowing was found. However, eyebrow movements were positively correlated with the eyebrow widening suggesting a mutual link between these two movement types."
   ],
   "doi": "10.21437/Interspeech.2022-10867"
  },
  "cao22b_interspeech": {
   "authors": [
    [
     "Beiming",
     "Cao"
    ],
    [
     "Kristin",
     "Teplansky"
    ],
    [
     "Nordine",
     "Sebkhi"
    ],
    [
     "Arpan",
     "Bhavsar"
    ],
    [
     "Omer",
     "Inan"
    ],
    [
     "Robin",
     "Samlan"
    ],
    [
     "Ted",
     "Mau"
    ],
    [
     "Jun",
     "Wang"
    ]
   ],
   "title": "Data Augmentation for End-to-end Silent Speech Recognition for Laryngectomees",
   "original": "10868",
   "page_count": 5,
   "order": 739,
   "p1": 3653,
   "pn": 3657,
   "abstract": [
    "Silent speech recognition (SSR) predicts textual information from silent articulation, which is an algorithm design in silent speech interfaces (SSIs). SSIs have the potential of recovering the speech ability of individuals who lost their voice but can still articulate (e.g. laryngectomees). Due to the logistic difficulties in articulatory data collection, current SSR studies suffer limited amount of dataset. Data augmentation aims to increase the training data amount by introducing variations into the existing dataset, but has rarely been investigated in SSR for laryngectomees. In this study, we investigated the effectiveness of multiple data augmentation approaches for SSR including consecutive and intermittent time masking, articulatory dimension masking, sinusoidal noise injection and randomly scaling. Different experimental setups including speaker-dependent, speaker-independent, and speaker-adaptive were used. The SSR models were end-to-end speech recognition models trained with connectionist temporal classification (CTC). Electromagnetic articulography (EMA) datasets collected from multiple healthy speakers and laryngectomees were used. The experimental results have demonstrated that the data augmentation approaches explored performed differently, but generally improved SSR performance. Especially, the consecutive time masking has brought significant improvement on SSR for both healthy speakers and laryngectomees."
   ],
   "doi": "10.21437/Interspeech.2022-10868"
  },
  "chan22b_interspeech": {
   "authors": [
    [
     "May Pik Yu",
     "Chan"
    ],
    [
     "June",
     "Choe"
    ],
    [
     "Aini",
     "Li"
    ],
    [
     "Yiran",
     "Chen"
    ],
    [
     "Xin",
     "Gao"
    ],
    [
     "Nicole",
     "Holliday"
    ]
   ],
   "title": "Training and typological bias in ASR performance for world Englishes",
   "original": "10869",
   "page_count": 5,
   "order": 259,
   "p1": 1273,
   "pn": 1277,
   "abstract": [
    "The use of automatic speech recognition (ASR) has been increasing to promote inclusion and accessibility. Nonetheless, prior work on ASR finds performance gaps conditioned by specific gender and racial groups, revealing systematic biases in modern ASR systems. However, work has focused on native varieties of English, glossing over its impact on a wider range of ASR users, namely second language speakers of English. The present work compares the performance of the transcription system Otter, on 24 varieties of English, 21 of them are non-native varieties. We compare the word and phone error rate (WER/PER) of accent varieties that are claimed to be supported by Otter and those that are unsupported. Results show that English varieties that are supported have lower WERs compared to that of unsupported varieties. However, there are still systematic differences in performance conditioned by linguistic structure in both supported and unsupported Englishes. Specifically, Otter performs better on English varieties from non-tonal first language speakers. We conclude that while inclusion of more varieties of English in the training data set for ASR may promote inclusivity, there may still be biases inherent to the linguistic structure that should not be overlooked."
   ],
   "doi": "10.21437/Interspeech.2022-10869"
  },
  "martikainen22_interspeech": {
   "authors": [
    [
     "Katariina",
     "Martikainen"
    ],
    [
     "Jussi",
     "Karlgren"
    ],
    [
     "Khiet",
     "Truong"
    ]
   ],
   "title": "Exploring audio-based stylistic variation in podcasts",
   "original": "10871",
   "page_count": 5,
   "order": 477,
   "p1": 2343,
   "pn": 2347,
   "abstract": [
    "Podcast are a growing spoken medium that is listened to for various reasons in various situations (e.g., for entertainment or educational purposes, on the train or at home) consisting of various types of audio such as (unstructured) speech, music, and other sounds. Traditionally, search and recommendation of spoken content focuses on topical content, derived from text transcriptions, ignoring paralinguistic aspects in spoken language. Instead, we propose to model these paralinguistic aspects, such as speaking style, in podcasts to address both the heterogeneity of type of audio in podcasts and user needs to enable enriched access to this medium. In this paper, we take a first step towards this goal and explore audio-based stylistic variation in podcasts by 1) investigating what facets of stylistic variation are salient and of interest to listeners, and 2) gathering more insights into the kind of stylistic variation that is currently feasible to model with open-source audio tools and that is present in podcasts. We find that much of the stylistic variation mentioned by the users is related to speaking style and music, and we show, using open-source tools, how audio-based stylistic aspects vary across episodes, shows, and genres."
   ],
   "doi": "10.21437/Interspeech.2022-10871"
  },
  "yadav22b_interspeech": {
   "authors": [
    [
     "Hemant",
     "Yadav"
    ],
    [
     "Akshat",
     "Gupta"
    ],
    [
     "Sai Krishna",
     "Rallabandi"
    ],
    [
     "Alan W",
     "Black"
    ],
    [
     "Rajiv Ratn",
     "Shah"
    ]
   ],
   "title": "Intent classification using pre-trained language agnostic embeddings for low resource languages",
   "original": "10873",
   "page_count": 5,
   "order": 703,
   "p1": 3473,
   "pn": 3477,
   "abstract": [
    "Building Spoken Language Understanding (SLU) systems that do not rely on language specific Automatic Speech Recognition (ASR) is an important yet less explored problem in language processing. In this paper, we present a comparative study aimed at employing a pre-trained language agnostic acoustic model to perform SLU in low resource scenarios. Specifically, we use three different embedding settings extracted using Allosaurus, a pre-trained universal phone decoder: (1) Phone-labels (2) Panphone, and (3) Allo embeddings (proposed by us). These embeddings are then used in identifying the spoken intent. We perform experiments across three different languages: English, Sinhala, and Tamil each with different data sizes to simulate high, medium, and low resource scenarios. Our system improves on the state-of-the-art (SOTA) intent classification accuracy by absolute 2.11% for Sinhala and 7.00% for Tamil and achieves competitive results in English. Furthermore, we also present a quantitative analysis to show how the performance scales with the number of training examples."
   ],
   "doi": "10.21437/Interspeech.2022-10873"
  },
  "krug22_interspeech": {
   "authors": [
    [
     "Paul Konstantin",
     "Krug"
    ],
    [
     "Peter",
     "Birkholz"
    ],
    [
     "Branislav",
     "Gerazov"
    ],
    [
     "Daniel Rudolph",
     "van Niekerk"
    ],
    [
     "Anqi",
     "Xu"
    ],
    [
     "Yi",
     "Xu"
    ]
   ],
   "title": "Articulatory Synthesis for Data Augmentation in Phoneme Recognition",
   "original": "10874",
   "page_count": 5,
   "order": 250,
   "p1": 1228,
   "pn": 1232,
   "abstract": [
    "While numerous studies on automatic speech recognition have been published in recent years describing data augmentation strategies based on time or frequency domain signal processing, few works exist on the artificial extensions of training data sets using purely synthetic speech data. In this work, the German Kiel corpus was augmented with synthetic data generated with the state-of-the-art articulatory synthesizer VocalTractLab. It is shown that the additional synthetic data can lead to a significantly better performance in single-phoneme recognition in certain cases, while at the same time, the performance can also decrease in other cases, depending on the degree of acoustic naturalness of the synthetic phonemes. As a result, this work can potentially guide future studies to improve the quality of articulatory synthesis via the link between synthetic speech production and automatic speech recognition."
   ],
   "doi": "10.21437/Interspeech.2022-10874"
  },
  "georges22_interspeech": {
   "authors": [
    [
     "Marc-Antoine",
     "Georges"
    ],
    [
     "Jean-Luc",
     "Schwartz"
    ],
    [
     "Thomas",
     "Hueber"
    ]
   ],
   "title": "Self-supervised speech unit discovery from articulatory and acoustic features using VQ-VAE",
   "original": "10876",
   "page_count": 5,
   "order": 156,
   "p1": 774,
   "pn": 778,
   "abstract": [
    "The human perception system is often assumed to recruit motor knowledge when processing auditory speech inputs. Using articulatory modeling and deep learning, this study examines how this articulatory information can be used for discovering speech units in a self-supervised setting. We used vector-quantized variational autoencoders (VQ-VAE) to learn discrete representations from articulatory and acoustic speech data. In line with the zero-resource paradigm, an ABX test was then used to investigate how the extracted representations encode phonetically relevant properties. Experiments were conducted on three different corpora in English and French. We found that articulatory information rather organises the latent representations in terms of place of articulation whereas the speech acoustics mainly structure the latent space in terms of manner of articulation. We show that an optimal fusion of the two modalities can lead to a joint representation of these phonetic dimensions more accurate than each modality considered individually. Since articulatory information is usually not available in a practical situation, we finally investigate the benefit it provides when inferred from the speech acoustics in a self-supervised manner."
   ],
   "doi": "10.21437/Interspeech.2022-10876"
  },
  "sadekova22_interspeech": {
   "authors": [
    [
     "Tasnima",
     "Sadekova"
    ],
    [
     "Vladimir",
     "Gogoryan"
    ],
    [
     "Ivan",
     "Vovk"
    ],
    [
     "Vadim",
     "Popov"
    ],
    [
     "Mikhail",
     "Kudinov"
    ],
    [
     "Jiansheng",
     "Wei"
    ]
   ],
   "title": "A Unified System for Voice Cloning and Voice Conversion through Diffusion Probabilistic Modeling",
   "original": "10879",
   "page_count": 5,
   "order": 609,
   "p1": 3003,
   "pn": 3007,
   "abstract": [
    "Text-to-speech and voice conversion are two common speech generation tasks typically solved using different models. In this paper, we present a novel approach to voice cloning and any-to-any voice conversion relying on a single diffusion probabilistic model with two encoders each operating on its input domain and a shared decoder. Extensive human evaluation shows that the proposed model can copy a target speaker's voice by means of speaker adaptation better than other known multimodal systems of such kind and the quality of the speech synthesized by our system in both voice cloning and voice conversion modes is comparable with that of recently proposed algorithms for the corresponding single tasks. Besides, it takes as few as 3 minutes of GPU time to adapt our model to a new speaker with only 15 seconds of untranscribed audio which makes it attractive for practical applications."
   ],
   "doi": "10.21437/Interspeech.2022-10879"
  },
  "pereztoro22_interspeech": {
   "authors": [
    [
     "Paula Andrea",
     "Pérez-Toro"
    ],
    [
     "Philipp",
     "Klumpp"
    ],
    [
     "Abner",
     "Hernandez"
    ],
    [
     "Tomas",
     "Arias"
    ],
    [
     "Patricia",
     "Lillo"
    ],
    [
     "Andrea",
     "Slachevsky"
    ],
    [
     "Adolfo Martín",
     "García"
    ],
    [
     "Maria",
     "Schuster"
    ],
    [
     "Andreas K.",
     "Maier"
    ],
    [
     "Elmar",
     "Noeth"
    ],
    [
     "Juan Rafael",
     "Orozco-Arroyave"
    ]
   ],
   "title": "Alzheimer's Detection from English to Spanish Using Acoustic and Linguistic Embeddings",
   "original": "10883",
   "page_count": 5,
   "order": 505,
   "p1": 2483,
   "pn": 2487,
   "abstract": [
    "Cross-lingual approaches are growing in popularity in the machine learning domain, where large amounts of data are required to obtain better generalizations. Moreover, one of the biggest problems is the availability of clinical speech data, where most of the resources are in English. For instance, not many available Alzheimer's Disease (AD) corpora in different languages can be found in the literature. Despite the phonological and phonemic differences between Spanish and English, fortunately, there are also similarities between these two languages, e.g., around 40% of all words in English have a related word in Spanish. In this work, we want to investigate the feasibility of combining information from English and Spanish languages to discriminate AD. Two datasets were considered: part of the Pitt Corpus, which is composed of English speakers, and a Spanish AD dataset composed of speakers from Chile. We based our analysis on known acoustic (Wav2Vec) and word (BERT, RoBERTa) embeddings using different classifiers. Strong language dependencies were found, even using multilingual representations. We observed that linguistic information was more important for classifying English AD (F-Score=0.76) and acoustic for Spanish AD (F-Score=0.80). Using knowledge transferred from English to Spanish achieved F-scores of up to 0.85 for discriminating AD."
   ],
   "doi": "10.21437/Interspeech.2022-10883"
  },
  "wells22_interspeech": {
   "authors": [
    [
     "Dan",
     "Wells"
    ],
    [
     "Hao",
     "Tang"
    ],
    [
     "Korin",
     "Richmond"
    ]
   ],
   "title": "Phonetic Analysis of Self-supervised Representations of English Speech",
   "original": "10884",
   "page_count": 5,
   "order": 725,
   "p1": 3583,
   "pn": 3587,
   "abstract": [
    "We present an analysis of discrete units discovered via self-supervised representation learning on English speech. We focus on units produced by a pre-trained HuBERT model due to its wide adoption in ASR, speech synthesis, and many other tasks. Whereas previous work has evaluated the quality of such quantization models in aggregate over all phones for a given language, we break our analysis down into broad phonetic classes, taking into account specific aspects of their articulation when considering their alignment to discrete units. We find that these units correspond to sub-phonetic events, and that fine dynamics such as the distinct closure and release portions of plosives tend to be represented by sequences of discrete units. Our work provides a reference for the phonetic properties of discrete units discovered by HuBERT, facilitating analyses of many speech applications based on this model."
   ],
   "doi": "10.21437/Interspeech.2022-10884"
  },
  "singla22_interspeech": {
   "authors": [
    [
     "Karan",
     "Singla"
    ],
    [
     "Shahab",
     "Jalalvand"
    ],
    [
     "Yeon-Jun",
     "Kim"
    ],
    [
     "Ryan",
     "Price"
    ],
    [
     "Daniel",
     "Pressel"
    ],
    [
     "Srinivas",
     "Bangalore"
    ]
   ],
   "title": "Seq-2-Seq based Refinement of ASR Output for Spoken Name Capture",
   "original": "10885",
   "page_count": 5,
   "order": 804,
   "p1": 3963,
   "pn": 3967,
   "abstract": [
    "Person name capture from human speech is a difficult task in human-machine conversations. In this paper, we propose a novel approach to capture the person names from the caller utterances in response to the prompt \"say and spell your first/last name\". Inspired from work on spell correction, disfluency removal and text normalization, we propose a lightweight Seq-2-Seq system which generates a name spell from a varying user input. Our proposed method outperforms the strong baseline which is based on LM-driven rule-based approach."
   ],
   "doi": "10.21437/Interspeech.2022-10885"
  },
  "rutowski22_interspeech": {
   "authors": [
    [
     "Tomasz",
     "Rutowski"
    ],
    [
     "Amir",
     "Harati"
    ],
    [
     "Elizabeth",
     "Shriberg"
    ],
    [
     "Yang",
     "Lu"
    ],
    [
     "Piotr",
     "Chlebek"
    ],
    [
     "Ricardo",
     "Oliveira"
    ]
   ],
   "title": "Toward Corpus Size Requirements for Training and Evaluating Depression Risk Models Using Spoken Language",
   "original": "10888",
   "page_count": 5,
   "order": 677,
   "p1": 3343,
   "pn": 3347,
   "abstract": [
    "Mental health risk prediction is a growing field in the speech community, but many studies are based on small corpora. This study illustrates how variations in test and train set sizes impact performance in a controlled study. Using a corpus of over 65K labeled data points, results from a fully crossed design of different train/test size combinations are provided. Two model types are included: one based on language and the other on speech acoustics. Both use methods current in this domain. An age-mismatched test set was also included. Results show that (1) test sizes below 1K samples gave noisy results, even for larger training set sizes; (2) training set sizes of at least 2K were needed for stable results; (3) NLP and acoustic models behaved similarly with train/test size variations, and (4) the mismatched test set showed the same patterns as the matched test set. Additional factors are discussed, including label priors, model strength and pre-training, unique speakers, and data lengths. While no single study can specify exact size requirements, results demonstrate the need for appropriately sized train and test sets for future studies of mental health risk prediction from speech and language."
   ],
   "doi": "10.21437/Interspeech.2022-10888"
  },
  "vovk22_interspeech": {
   "authors": [
    [
     "Ivan",
     "Vovk"
    ],
    [
     "Tasnima",
     "Sadekova"
    ],
    [
     "Vladimir",
     "Gogoryan"
    ],
    [
     "Vadim",
     "Popov"
    ],
    [
     "Mikhail",
     "Kudinov"
    ],
    [
     "Jiansheng",
     "Wei"
    ]
   ],
   "title": "Fast Grad-TTS: Towards Efficient Diffusion-Based Speech Generation on CPU",
   "original": "10889",
   "page_count": 5,
   "order": 169,
   "p1": 838,
   "pn": 842,
   "abstract": [
    "Recently, score-based diffusion probabilistic modeling has shown encouraging results in various tasks outperforming other popular generative modeling frameworks in terms of quality. However, to unlock its potential and make diffusion models feasible from the practical point of view, special efforts should be made to enable more efficient iterative sampling procedure on CPU devices. In this paper, we focus on applying the most promising techniques from recent literature on diffusion modeling to Grad-TTS, a diffusion-based text-to-speech system, in order to accelerate it. We compare various reverse diffusion sampling schemes, the technique of progressive distillation, GAN-based diffusion modeling and score-based generative modeling in latent space. Experimental results demonstrate that it is possible to speed Grad-TTS up to 4.5 times compared to vanilla Grad-TTS and achieve real time factor 0.15 on CPU while keeping synthesis quality competitive with that of conventional text-to-speech baselines."
   ],
   "doi": "10.21437/Interspeech.2022-10889"
  },
  "arora22_interspeech": {
   "authors": [
    [
     "Siddhant",
     "Arora"
    ],
    [
     "Siddharth",
     "Dalmia"
    ],
    [
     "Xuankai",
     "Chang"
    ],
    [
     "Brian",
     "Yan"
    ],
    [
     "Alan W",
     "Black"
    ],
    [
     "Shinji",
     "Watanabe"
    ]
   ],
   "title": "Two-Pass Low Latency End-to-End Spoken Language Understanding",
   "original": "10890",
   "page_count": 5,
   "order": 704,
   "p1": 3478,
   "pn": 3482,
   "abstract": [
    "End-to-end (E2E) models are becoming increasingly popular for spoken language understanding (SLU) systems and are beginning to achieve competitive performance to pipeline-based approaches. However, recent work has shown that these models struggle to generalize to new phrasings for the same intent indicating that models cannot understand the semantic content of the given utterance. In this work, we incorporated language models pre-trained on unlabeled text data inside E2E-SLU frameworks to build strong semantic representations. Incorporating both semantic and acoustic information can increase the inference time, leading to high latency when deployed for applications like voice assistants. We developed a 2-pass SLU system that makes low latency prediction using acoustic information from the few seconds of the audio in the first pass and makes higher quality prediction in the second pass by combining semantic and acoustic representations. We take inspiration from prior work on 2-pass end-to-end speech recognition systems that attends on both audio and first-pass hypothesis using a deliberation network. The proposed 2-pass SLU system outperforms the acoustic-based SLU model on the Fluent Speech Commands Challenge Set and SLURP dataset and reduces latency, thus improving user experience. Our code and models are publicly available as part of the ESPnet-SLU toolkit."
   ],
   "doi": "10.21437/Interspeech.2022-10890"
  },
  "hori22_interspeech": {
   "authors": [
    [
     "Chiori",
     "Hori"
    ],
    [
     "Takaaki",
     "Hori"
    ],
    [
     "Jonathan",
     "Le Roux"
    ]
   ],
   "title": "Low-Latency Online Streaming VideoQA Using Audio-Visual Transformers",
   "original": "10891",
   "page_count": 5,
   "order": 914,
   "p1": 4511,
   "pn": 4515,
   "abstract": [
    "To apply scene-aware interaction technology to real-time dialog systems, we propose an online low-latency response generation framework for scene-aware interaction using a video question answering setup. This paper extends our prior work on low-latency video captioning to build a novel approach that can optimize the timing to generate each answer under a trade-off between latency of generation and quality of answer. For video QA, the timing detector is now in charge of finding a timing for the question-relevant event, instead of determining when the system has seen enough to generate a general caption as in the video captioning case. Our audio visual scene-aware dialog system built for the 10th Dialog System Technology Challenge was extended to exploit a low-latency function. Experiments with the MSRVTT-QA and AVSD datasets show that our approach achieves between 97% and 99% of the answer quality of the upper bound given by a pre-trained Transformer using the entire video clips, using less than 40% of frames from the beginning."
   ],
   "doi": "10.21437/Interspeech.2022-10891"
  },
  "wu22i_interspeech": {
   "authors": [
    [
     "Peter",
     "Wu"
    ],
    [
     "Shinji",
     "Watanabe"
    ],
    [
     "Louis",
     "Goldstein"
    ],
    [
     "Alan W",
     "Black"
    ],
    [
     "Gopala Krishna",
     "Anumanchipalli"
    ]
   ],
   "title": "Deep Speech Synthesis from Articulatory Representations",
   "original": "10892",
   "page_count": 5,
   "order": 157,
   "p1": 779,
   "pn": 783,
   "abstract": [
    "In the articulatory synthesis task, speech is synthesized from input features containing information about the physical behavior of the human vocal tract. This task provides a promising direction for speech synthesis research, as the articulatory space is compact, smooth, and interpretable. Current works have highlighted the potential for deep learning models to perform articulatory synthesis. However, it remains unclear whether these models can achieve the efficiency and fidelity of the human speech production system. To help bridge this gap, we propose a time-domain articulatory synthesis methodology and demonstrate its efficacy with both electromagnetic articulography (EMA) and synthetic articulatory feature inputs. Our model is computationally efficient and achieves a transcription word error rate (WER) of 18.5% for the EMA-to-speech task, yielding an improvement of 11.6% compared to prior work. Through interpolation experiments, we also highlight the generalizability and interpretability of our approach."
   ],
   "doi": "10.21437/Interspeech.2022-10892"
  },
  "liu22w_interspeech": {
   "authors": [
    [
     "Xubo",
     "Liu"
    ],
    [
     "Haohe",
     "Liu"
    ],
    [
     "Qiuqiang",
     "Kong"
    ],
    [
     "Xinhao",
     "Mei"
    ],
    [
     "Jinzheng",
     "Zhao"
    ],
    [
     "Qiushi",
     "Huang"
    ],
    [
     "Mark D.",
     "Plumbley"
    ],
    [
     "Wenwu",
     "Wang"
    ]
   ],
   "title": "Separate What You Describe: Language-Queried Audio Source Separation",
   "original": "10894",
   "page_count": 5,
   "order": 365,
   "p1": 1801,
   "pn": 1805,
   "abstract": [
    "In this paper, we introduce the task of language-queried audio source separation (LASS), which aims to separate a target source from an audio mixture based on a natural language query of the target source (e.g., ''a man tells a joke followed by people laughing''). A unique challenge in LASS is associated with the complexity of natural language description and its relation with the audio sources. To address this issue, we proposed LASS-Net, an end-to-end neural network that is learned to jointly process acoustic and linguistic information, and separate the target source that is consistent with the language query from an audio mixture. We evaluate the performance of our proposed system with a dataset created from the AudioCaps dataset. Experimental results show that LASS-Net achieves considerable improvements over baseline methods. Furthermore, we observe that LASS-Net achieves promising generalization results when using diverse human-annotated descriptions as queries, indicating its potential use in real-world scenarios. The separated audio samples and source code are available at https://liuxubo717.github.io/LASS-demopage."
   ],
   "doi": "10.21437/Interspeech.2022-10894"
  },
  "amid22_interspeech": {
   "authors": [
    [
     "Ehsan",
     "Amid"
    ],
    [
     "Om Dipakbhai",
     "Thakkar"
    ],
    [
     "Arun",
     "Narayanan"
    ],
    [
     "Rajiv",
     "Mathews"
    ],
    [
     "Francoise",
     "Beaufays"
    ]
   ],
   "title": "Extracting Targeted Training Data from ASR Models, and How to Mitigate It",
   "original": "10895",
   "page_count": 5,
   "order": 569,
   "p1": 2803,
   "pn": 2807,
   "abstract": [
    "Recent work has designed methods to demonstrate that model updates in ASR training can leak potentially sensitive attributes of the utterances used in computing the updates. In this work, we design the first method to demonstrate information leakage about training data from trained ASR models. We design Noise Masking, a fill-in-the-blank style method for extracting targeted parts of training data from trained ASR models. We demonstrate the success of Noise Masking by using it in four settings for extracting names from the LibriSpeech dataset used for training a state-of-the-art Conformer model. In particular, we show that we are able to extract the correct names from masked training utterances with 11.8% accuracy, while the model outputs some name from the train set 55.2% of the time. Further, we show that even in a setting that uses synthetic audio and partial transcripts from the test set, our method achieves 2.5% correct name accuracy (47.7% any name success rate). Lastly, we design Word Dropout, a data augmentation method that we show when used in training along with Multistyle TRaining (MTR), provides comparable utility as the baseline, along with significantly mitigating extraction via Noise Masking across the four evaluated settings."
   ],
   "doi": "10.21437/Interspeech.2022-10895"
  },
  "baskar22b_interspeech": {
   "authors": [
    [
     "Murali Karthick",
     "Baskar"
    ],
    [
     "Tim",
     "Herzig"
    ],
    [
     "Diana",
     "Nguyen"
    ],
    [
     "Mireia",
     "Diez"
    ],
    [
     "Tim",
     "Polzehl"
    ],
    [
     "Lukas",
     "Burget"
    ],
    [
     "Jan",
     "Černocký"
    ]
   ],
   "title": "Speaker adaptation for Wav2vec2 based dysarthric ASR",
   "original": "10896",
   "page_count": 5,
   "order": 689,
   "p1": 3403,
   "pn": 3407,
   "abstract": [
    "Dysarthric speech recognition has posed major challenges due to lack of training data and heavy mismatch in speaker characteristics. Recent ASR systems have benefited from readily available pretrained models such as wav2vec2 to improve the recognition performance. Speaker adaptation using fMLLR and xvectors have provided major gains for dysarthric speech with very little adaptation data. However, integration of wav2vec2 with fMLLR features or xvectors during wav2vec2 finetuning is yet to be explored. In this work, we propose a simple adaptation network for fine-tuning wav2vec2 using fMLLR features. The adaptation network is also flexible to handle other speaker adaptive features such as xvectors. Experimental analysis show steady improvements using our proposed approach across all impairment severity levels and attains 57.72\\% WER for high severity in UASpeech dataset. We also performed experiments on German dataset to substantiate the consistency of our proposed approach across diverse domains."
   ],
   "doi": "10.21437/Interspeech.2022-10896"
  },
  "tian22e_interspeech": {
   "authors": [
    [
     "Zuoyu",
     "Tian"
    ],
    [
     "Xiao",
     "Dong"
    ],
    [
     "Feier",
     "Gao"
    ],
    [
     "Haining",
     "Wang"
    ],
    [
     "Charles",
     "Lin"
    ]
   ],
   "title": "Mandarin Tone Sandhi Realization: Evidence from Large Speech Corpora",
   "original": "10897",
   "page_count": 5,
   "order": 1069,
   "p1": 5273,
   "pn": 5277,
   "abstract": [
    "There has been a long-standing debate on the acoustical difference between second tone and sandhi third tone in Mandarin Chinese. Except [1], most of the studies focus on the tonal realization in carefully controlled speech, and the relationship between demographic features and tonal realization has not been addressed by existing studies using large-scale speech data. In this paper, we investigate the tonal realization in three large speech corpora of Mandarin Chinese, which contain around 2,300 speakers from different regions of China. The relation between demographic factors (e.g., gender, age, and place of origin) and tonal realization is addressed. The results show that even though the sandhi third tone is close to the second tone in terms of acoustic features, it still displays significant differences in large data. We also report some demographic influences on tonal realization, including age, gender, and places of origin."
   ],
   "doi": "10.21437/Interspeech.2022-10897"
  },
  "kataria22_interspeech": {
   "authors": [
    [
     "Saurabh",
     "Kataria"
    ],
    [
     "Jesús",
     "Villalba"
    ],
    [
     "Laureano",
     "Moro-Velázquez"
    ],
    [
     "Najim",
     "Dehak"
    ]
   ],
   "title": "Joint domain adaptation and speech bandwidth extension using time-domain GANs for speaker verification",
   "original": "10900",
   "page_count": 5,
   "order": 124,
   "p1": 615,
   "pn": 619,
   "abstract": [
    "Speech systems developed for a particular choice of acoustic domain and sampling frequency do not translate easily to others. The usual practice is to learn domain adaptation and bandwidth extension models independently. Contrary to this, we propose to learn both tasks together. Particularly, we learn to map narrowband conversational telephone speech to wideband microphone speech. We developed parallel and non-parallel learning solutions which utilize both paired and unpaired data. We first discuss joint and disjoint training of multiple generative models for our tasks. Then, we propose a two-stage learning solution using a pre-trained domain adaptation system for pre-processing in bandwidth extension training. We evaluated our schemes on a Speaker Verification downstream task. We used the JHU-MIT experimental setup for NIST SRE21, which comprises SRE16, SRE-CTS Superset, and SRE21. Our results prove that learning both tasks is better than learning just one. On SRE16, our best system achieves 22% relative improvement in Equal Error Rate w.r.t. a direct learning baseline and 8% w.r.t. a strong bandwidth expansion system."
   ],
   "doi": "10.21437/Interspeech.2022-10900"
  },
  "huo22_interspeech": {
   "authors": [
    [
     "Zhouyuan",
     "Huo"
    ],
    [
     "Dongseong",
     "Hwang"
    ],
    [
     "Khe Chai",
     "Sim"
    ],
    [
     "Shefali",
     "Garg"
    ],
    [
     "Ananya",
     "Misra"
    ],
    [
     "Nikhil",
     "Siddhartha‎"
    ],
    [
     "Trevor",
     "Strohman"
    ],
    [
     "Francoise",
     "Beaufays"
    ]
   ],
   "title": "Incremental Layer-Wise Self-Supervised Learning for Efficient Unsupervised Speech Domain Adaptation On Device",
   "original": "10904",
   "page_count": 5,
   "order": 981,
   "p1": 4845,
   "pn": 4849,
   "abstract": [
    "Streaming end-to-end speech recognition models have been widely applied to mobile devices and show significant improvement in efficiency. These models are typically trained on the server using transcribed speech data. However, the server data distribution can be very different from the data distribution on user devices, which could affect the model performance. There are two main challenges for on device training, limited reliable labels and limited training memory. While self-supervised learning algorithms can mitigate the mismatch between domains using unlabeled data, they are not applicable on mobile devices directly because of the memory constraint. In this paper, we propose an incremental layer-wise self-supervised learning algorithm for efficient unsupervised speech domain adaptation on mobile devices, in which only one layer is updated at a time. Extensive experimental results demonstrate that the proposed algorithm achieves a $24.2\\%$ relative Word Error Rate (WER) improvement on the target domain compared to a supervised baseline and costs $95.7\\%$ less training memory than the end-to-end self-supervised learning algorithm."
   ],
   "doi": "10.21437/Interspeech.2022-10904"
  },
  "parikh22_interspeech": {
   "authors": [
    [
     "Rahil",
     "Parikh"
    ],
    [
     "Harshavardhan",
     "Sundar"
    ],
    [
     "Ming",
     "Sun"
    ],
    [
     "Chao",
     "Wang"
    ],
    [
     "Spyros",
     "Matsoukas"
    ]
   ],
   "title": "Impact of Acoustic Event Tagging on Scene Classification in a Multi-Task Learning Framework",
   "original": "10905",
   "page_count": 5,
   "order": 850,
   "p1": 4192,
   "pn": 4196,
   "abstract": [
    "Acoustic events are sounds with well-defined spectro-temporal characteristics which can be associated with the physical objects generating them. Acoustic scenes are collections of such acoustic events in no specific temporal order. Given this natural linkage between events and scenes, a common belief is that the ability to classify events must help in the classification of scenes. This has led to several efforts attempting to do well on Acoustic Event Tagging (AET) and Acoustic Scene Classification (ASC) using a multi-task network. However, in these efforts, improvement in one task does not guarantee an improvement in the other, suggesting a tension between ASC and AET. It is unclear if improvements in AET translates to improvements in ASC. We explore this conundrum through an extensive empirical study and show that under certain conditions, using AET as an auxiliary task in the multi-task network consistently improves ASC performance. Additionally, ASC performance further improves with the AET data-set size and is not sensitive to the choice of events or the number of events in the AET data-set. We conclude that this improvement in ASC performance comes from the regularization effect of using AET and not from the network's improved ability to discern between acoustic events."
   ],
   "doi": "10.21437/Interspeech.2022-10905"
  },
  "wang22aa_interspeech": {
   "authors": [
    [
     "Yu",
     "Wang"
    ],
    [
     "Mark",
     "Cartwright"
    ],
    [
     "Juan Pablo",
     "Bello"
    ]
   ],
   "title": "Active Few-Shot Learning for Sound Event Detection",
   "original": "10907",
   "page_count": 5,
   "order": 315,
   "p1": 1551,
   "pn": 1555,
   "abstract": [
    "Few-shot learning has shown promising results in sound event detection where the model can learn to recognize novel classes assuming a few labeled examples (typically five) are available at inference time. Most research studies simulate this process by sampling support examples randomly and uniformly from all test data with the target class label. However, in many real-world scenarios, users might not even have five examples at hand or these examples may be from a limited context and not representative, resulting in model performance lower than expected. In this work, we relax these assumptions, and to recover model performance, we propose to use active learning techniques to efficiently sample additional informative support examples at inference time. We developed a novel dataset simulating the long-term temporal characteristics of sound events in real-world environmental soundscapes. Then we ran a series of experiments with this dataset to explore the modeling and sampling choices that arise when combining few-shot learning and active learning, including different training schemes, sampling strategies, models, and temporal windows in sampling."
   ],
   "doi": "10.21437/Interspeech.2022-10907"
  },
  "bayerl22b_interspeech": {
   "authors": [
    [
     "Sebastian Peter",
     "Bayerl"
    ],
    [
     "Dominik",
     "Wagner"
    ],
    [
     "Elmar",
     "Noeth"
    ],
    [
     "Korbinian",
     "Riedhammer"
    ]
   ],
   "title": "Detecting Dysfluencies in Stuttering Therapy Using wav2vec 2.0",
   "original": "10908",
   "page_count": 5,
   "order": 582,
   "p1": 2868,
   "pn": 2872,
   "abstract": [
    "Stuttering is a varied speech disorder that harms an individual's communication ability. Persons who stutter (PWS) often use speech therapy to cope with their condition. Improving speech recognition systems for people with such non-typical speech or tracking the effectiveness of speech therapy would require systems that can detect dysfluencies while at the same time being able to detect speech techniques acquired in therapy. This paper shows that fine-tuning wav2vec 2.0 [1] for the classification of stuttering on a sizeable English corpus containing stuttered speech, in conjunction with multi-task learning, boosts the effectiveness of the general-purpose wav2vec 2.0 features for detecting stuttering in speech; both within and across languages. We evaluate our method on FluencyBank , [2] and the German therapy-centric Kassel State of Fluency (KSoF) [3] dataset by training Support Vector Machine classifiers using features extracted from the fine- tuned models for six different stuttering-related event types: blocks, prolongations, sound repetitions, word repetitions, interjections, and – specific to therapy – speech modifications. Using embeddings from the fine-tuned models leads to rel- ative classification performance gains up to 27% w.r.t. F1-score."
   ],
   "doi": "10.21437/Interspeech.2022-10908"
  },
  "huang22k_interspeech": {
   "authors": [
    [
     "W. Ronny",
     "Huang"
    ],
    [
     "Steve",
     "Chien"
    ],
    [
     "Om Dipakbhai",
     "Thakkar"
    ],
    [
     "Rajiv",
     "Mathews"
    ]
   ],
   "title": "Detecting Unintended Memorization in Language-Model-Fused ASR",
   "original": "10909",
   "page_count": 5,
   "order": 570,
   "p1": 2808,
   "pn": 2812,
   "abstract": [
    "End-to-end (E2E) models are often being accompanied by language models (LMs) via shallow fusion for boosting their overall quality as well as recognition of rare words. At the same time, several prior works show that LMs are susceptible to unintentionally memorizing rare or unique sequences in the training data. In this work, we design a framework for detecting memorization of random textual sequences (which we call canaries) in the LM training data when one has only black-box (query) access to LM-fused speech recognizer, as opposed to direct access to the LM. On a production-grade Conformer RNN-T E2E model fused with a Transformer LM, we show that detecting memorization of singly-occurring canaries from the LM training data of 300M examples is possible. Motivated to protect privacy, we also show that such memorization gets significantly reduced by per-example gradient-clipped LM training without compromising overall quality."
   ],
   "doi": "10.21437/Interspeech.2022-10909"
  },
  "lee22n_interspeech": {
   "authors": [
    [
     "Boram",
     "Lee"
    ],
    [
     "Naomi",
     "Yamaguchi"
    ],
    [
     "Cécile",
     "Fougeron"
    ]
   ],
   "title": "Why is Korean lenis stop difficult to perceive for L2 Korean learners?",
   "original": "10912",
   "page_count": 5,
   "order": 377,
   "p1": 1861,
   "pn": 1865,
   "abstract": [
    "This study investigates how French learners, whose native language has only a binary laryngeal contrast, acquire the Korean three-way laryngeal contrast in stops by focusing on cue weighting. We tested how 21 French learners of Korean identify fortis/lenis/aspirated Korean stops over eight monthly sessions. Learners were the most successful at identifying aspirated stops. The identification of lenis stops was the most challenging, with no improvement over the 8 months, whereas the perception of aspirated and fortis stops improved. In order to explain the difficulty with lenis stops, we tested the relative contribution of VOT and f0 to the perception of the contrast on synthesized stimuli. Learners showed different cue-weighting strategies: VOT was used to distinguish between aspirated and fortis/lenis, and f0 was used to differentiate between fortis and lenis, implying a two-way rather than a three-way contrast. Furthermore, the larger weight given to VOT compared to f0 by the French learners can explain the poor identification of lenis stops, whose main contrastive cue in Korean is a lowering of f0 on the following vowel. Based on these findings, the acquisition of the three-way contrast necessitates a reorganization of the cues' relative weight."
   ],
   "doi": "10.21437/Interspeech.2022-10912"
  },
  "breiner22_interspeech": {
   "authors": [
    [
     "Theresa",
     "Breiner"
    ],
    [
     "Swaroop",
     "Ramaswamy"
    ],
    [
     "Ehsan",
     "Variani"
    ],
    [
     "Shefali",
     "Garg"
    ],
    [
     "Rajiv",
     "Mathews"
    ],
    [
     "Khe Chai",
     "Sim"
    ],
    [
     "Kilol",
     "Gupta"
    ],
    [
     "Mingqing",
     "Chen"
    ],
    [
     "Lara",
     "McConnaughey"
    ]
   ],
   "title": "UserLibri: A Dataset for ASR Personalization Using Only Text",
   "original": "10915",
   "page_count": 5,
   "order": 140,
   "p1": 694,
   "pn": 698,
   "abstract": [
    "Personalization of speech models on mobile devices (on-device personalization) is an active area of research, but more often than not, mobile devices have more text-only data than paired audio-text data. We explore training a personalized language model on text-only data, used during inference to improve speech recognition performance for that user. We experiment on a user-clustered LibriSpeech corpus, supplemented with personalized text-only data for each user from Project Gutenberg. We release this User-Specific LibriSpeech (UserLibri) dataset to aid future personalization research. LibriSpeech audio-transcript pairs are grouped into 55 users from the test-clean dataset and 52 users from test-other. We are able to lower the average word error rate per user across both sets in streaming and nonstreaming models, including an improvement of 2.5 for the harder set of test-other users when streaming."
   ],
   "doi": "10.21437/Interspeech.2022-10915"
  },
  "farooq22_interspeech": {
   "authors": [
    [
     "Muhammad Umar",
     "Farooq"
    ],
    [
     "Thomas",
     "Hain"
    ]
   ],
   "title": "Investigating the Impact of Crosslingual Acoustic-Phonetic Similarities on Multilingual Speech Recognition",
   "original": "10916",
   "page_count": 5,
   "order": 780,
   "p1": 3849,
   "pn": 3853,
   "abstract": [
    "Multilingual speech recognition systems mostly benefit low resource languages but suffer degradation in the performance of several languages relative to their monolingual counterparts. Limited studies have focused on understanding the languages behaviour in the multilingual speech recognition setups. In this paper, a novel data-driven approach is proposed to investigate the cross-lingual acoustic-phonetic similarities. This technique measures the similarities between posterior distributions from various monolingual acoustic models against a target speech signal. Deep neural networks are trained as mapping networks to transform the distributions from different acoustic models into a directly comparable form. The analysis observes that the languages ‘closeness' can not be truly estimated by the volume of overlapping phonemes set. Entropy analysis of the proposed mapping networks exhibits that a language with lesser overlap can be more amenable to cross-lingual transfer, and hence more beneficial in the multilingual setup. Finally, the proposed posterior transformation approach is leveraged to fuse monolingual models for a target language. A relative improvement of ∼8% over monolingual counterpart is achieved."
   ],
   "doi": "10.21437/Interspeech.2022-10916"
  },
  "chen22q_interspeech": {
   "authors": [
    [
     "Szu-Jui",
     "Chen"
    ],
    [
     "Jiamin",
     "Xie"
    ],
    [
     "John H.L.",
     "Hansen"
    ]
   ],
   "title": "FeaRLESS: Feature Refinement Loss for Ensembling Self-Supervised Learning Features in Robust End-to-end Speech Recognition",
   "original": "10917",
   "page_count": 5,
   "order": 620,
   "p1": 3058,
   "pn": 3062,
   "abstract": [
    "Self-supervised learning representations (SSLR) have become robust features for downstream tasks of many fields. Recently, several SSLRs have shown promising results on many automatic speech recognition (ASR) benchmark corpora. However, previous studies have only shown performance for solitary SSLRs as an input feature for ASR models. In this study, we propose to investigate the effectiveness of diverse SSLR combinations using various fusion methods within end-to-end (E2E) ASR models. In addition, we will show there are correlations between these extracted SSLRs. As such, we further propose a feature refinement loss for decorrelation to efficiently combine the set of input features. For evaluation, we show the proposed \"fearless learning features\" perform better than systems without the proposed feature refinement loss for both WSJ and Fearless Steps Challenge (FSC) corpora."
   ],
   "doi": "10.21437/Interspeech.2022-10917"
  },
  "serdyuk22_interspeech": {
   "authors": [
    [
     "Dmitriy",
     "Serdyuk"
    ],
    [
     "Otavio",
     "Braga"
    ],
    [
     "Olivier",
     "Siohan"
    ]
   ],
   "title": "Transformer-Based Video Front-Ends for Audio-Visual Speech Recognition for Single and Muti-Person Video",
   "original": "10920",
   "page_count": 5,
   "order": 575,
   "p1": 2833,
   "pn": 2837,
   "abstract": [
    "Audio-visual automatic speech recognition (AV-ASR) extends the speech recognition by introducing the video modality. In particular, the information contained in the motion of the speaker's mouth is used to augment the audio features. The video modality is traditionally processed with a 3D convolutional neural network (e.g. 3D version of VGG). Recently, image transformer networks [1] demonstrated the ability to extract rich visual features for the image classification task. In this work, we propose to replace the 3D convolution with a video transformer video feature extractor. We train our baselines and the proposed model on a large scale corpus of the YouTube videos. Then we evaluate the performance on a labeled subset of YouTube as well as on the public corpus LRS3-TED. Our best model video-only model achieves the performance of 34.9% WER on YTDEV18 and 19.3% on LRS3-TED which is a 10% and 9% relative improvements over the convolutional baseline. We achieve the state of the art performance of the audio-visual recognition on the LRS3-TED after fine-tuning our model (1.6% WER). We observe an average relative improvement of 2% over a series of multi-person datasets."
   ],
   "doi": "10.21437/Interspeech.2022-10920"
  },
  "alenin22_interspeech": {
   "authors": [
    [
     "Alexander",
     "Alenin"
    ],
    [
     "Nikita",
     "Torgashov"
    ],
    [
     "Anton",
     "Okhotnikov"
    ],
    [
     "Rostislav",
     "Makarov"
    ],
    [
     "Ivan",
     "Yakovlev"
    ]
   ],
   "title": "A Subnetwork Approach for Spoofing Aware Speaker Verification",
   "original": "10921",
   "page_count": 5,
   "order": 586,
   "p1": 2888,
   "pn": 2892,
   "abstract": [
    "This paper describes the ID R&D team submission to the Spoofing Aware Speaker Verification (SASV) challenge. Firstly, we present an approach that utilizes automatic speaker verification (ASV) system together with countermeasures (CM) subsystem in a single computational graph, called an anti-spoofing subnetwork. Subnetwork is a small network operating on feature maps from larger parent network, in this case trained for a speaker verification task. While requiring a small number of additionally trained parameters, subnetwork approach showed great performance in spoofing attacks detection task. Secondly, we cover training strategies for independently trained ASV and CM systems. In addition, we present a SASV-EER optimization approach using a fusion of multiple systems outputs and quality measurement functions (QMFs). Our best fusion achieves 0.136% EER on SASV-2022 evaluation set, while the smallest single-model system with 11.6M parameters achieves 0.223% EER."
   ],
   "doi": "10.21437/Interspeech.2022-10921"
  },
  "maniati22_interspeech": {
   "authors": [
    [
     "Georgia",
     "Maniati"
    ],
    [
     "Alexandra",
     "Vioni"
    ],
    [
     "Nikolaos",
     "Ellinas"
    ],
    [
     "Karolos",
     "Nikitaras"
    ],
    [
     "Konstantinos",
     "Klapsas"
    ],
    [
     "June Sig",
     "Sung"
    ],
    [
     "Gunu",
     "Jho"
    ],
    [
     "Aimilios",
     "Chalamandaris"
    ],
    [
     "Pirros",
     "Tsiakoulis"
    ]
   ],
   "title": "SOMOS: The Samsung Open MOS Dataset for the Evaluation of Neural Text-to-Speech Synthesis",
   "original": "10922",
   "page_count": 5,
   "order": 486,
   "p1": 2388,
   "pn": 2392,
   "abstract": [
    "In this work, we present the SOMOS dataset, the first large-scale mean opinion scores (MOS) dataset consisting of solely neural text-to-speech (TTS) samples. It can be employed to train automatic MOS prediction systems focused on the assessment of modern synthesizers, and can stimulate advancements in acoustic model evaluation. It consists of 20K synthetic utterances of the LJ Speech voice, a public domain speech dataset which is a common benchmark for building neural acoustic models and vocoders. Utterances are generated from 200 different TTS systems including a variety of vanilla neural acoustic models as well as models which allow prosodic variations. An LPCNet vocoder is used for all systems, so that the variations in the final samples depend only on the acoustic models. The synthesized utterances provide a balanced and adequate domain, length and phoneme coverage. MOS naturalness evaluations are collected via crowdsourcing on Amazon Mechanical Turk. We present in detail the design of the SOMOS dataset, as well as provide baseline results by training and evaluating state-of-the-art MOS prediction models, while we show the problems that these models face when assigned to evaluate TTS samples."
   ],
   "doi": "10.21437/Interspeech.2022-10922"
  },
  "nespoli22_interspeech": {
   "authors": [
    [
     "Francesco",
     "Nespoli"
    ],
    [
     "Daniel",
     "Barreda"
    ],
    [
     "Patrick",
     "Naylor"
    ]
   ],
   "title": "Relative Acoustic Features for Distance Estimation in Smart-Homes",
   "original": "10925",
   "page_count": 5,
   "order": 146,
   "p1": 724,
   "pn": 728,
   "abstract": [
    "Any audio recording encapsulates the unique fingerprint of the associated acoustic environment, namely the background noise and reverberation. Considering the scenario of a room equipped with a fixed smart speaker device with one or more microphones and a wearable smart device (watch, glasses or smartphone), we employed the improved proportionate normalized least mean square adaptive filter to estimate the relative room impulse response mapping the audio recordings of the two devices. We performed inter-device distance estimation by exploiting a new set of features obtained extending the definition of some acoustic attributes of the room impulse response to its relative version. In combination with the sparseness measure of the estimated relative room impulse response, the relative features allow precise inter-device distance estimation which can be exploited for tasks such as best microphone selection or acoustic scene analysis. Experimental results from simulated rooms of different dimensions and reverberation times demonstrate the effectiveness of this computationally lightweight approach for smart home acoustic ranging applications."
   ],
   "doi": "10.21437/Interspeech.2022-10925"
  },
  "parikh22b_interspeech": {
   "authors": [
    [
     "Rahil",
     "Parikh"
    ],
    [
     "Nadee",
     "Seneviratne"
    ],
    [
     "Ganesh",
     "Sivaraman"
    ],
    [
     "Shihab",
     "Shamma"
    ],
    [
     "Carol",
     "Espy-Wilson"
    ]
   ],
   "title": "Acoustic To Articulatory Speech Inversion Using Multi-Resolution Spectro-Temporal Representations Of Speech Signals",
   "original": "10926",
   "page_count": 5,
   "order": 948,
   "p1": 4681,
   "pn": 4685,
   "abstract": [
    "Multi-resolution spectro-temporal features of a speech signal represent how the brain perceives sounds by tuning cortical cells to different spectral and temporal modulations. These features produce a higher dimensional representation of the speech signals. The purpose of this paper is to evaluate how well the auditory cortex representation of speech signals contribute to estimate articulatory features of those corresponding signals. Since obtaining articulatory features from acoustic features of speech signals has been a challenging topic of interest for different speech communities, we investigate the possibility of using this multi-resolution representation of speech signals as acoustic features. We used U. of Wisconsin X-ray Microbeam (XRMB) database of clean speech signals to train a feed-forward deep neural network (DNN) to estimate articulatory trajectories of six tract variables. The optimal set of multi-resolution spectro-temporal features to train the model were chosen using appropriate scale and rate vector parameters to obtain the best performing model. Experiments achieved a correlation of 0.675 with ground-truth tract variables. We compared the performance of this speech inversion system with prior experiments conducted using Mel Frequency Cepstral Coefficients (MFCCs)."
   ],
   "doi": "10.21437/Interspeech.2022-10926"
  },
  "baird22_interspeech": {
   "authors": [
    [
     "Alice",
     "Baird"
    ],
    [
     "Panagiotis",
     "Tzirakis"
    ],
    [
     "Jeff",
     "Brooks"
    ],
    [
     "Lauren",
     "Kim"
    ],
    [
     "Michael",
     "Opara"
    ],
    [
     "Chris",
     "Gregory"
    ],
    [
     "Jacob",
     "Metrick"
    ],
    [
     "Garrett",
     "Boseck"
    ],
    [
     "Dacher",
     "Keltner"
    ],
    [
     "Alan",
     "Cowen"
    ]
   ],
   "title": "State & Trait Measurement from Nonverbal Vocalizations: A Multi-Task Joint Learning Approach",
   "original": "10927",
   "page_count": 5,
   "order": 414,
   "p1": 2028,
   "pn": 2032,
   "abstract": [
    "Humans infer a wide array of meanings from expressive nonverbal vocalizations, \\eg laughs, cries, and sighs. Thus far, computational research has primarily focused on the coarse classification of vocalizations such as laughs, but that approach overlooks significant variations in the meaning of distinct laughs (e.g., amusement, awkwardness, triumph) and the rich array of more nuanced vocalizations people form. Nonverbal vocalizations are shaped by the emotional state an individual chooses to convey, their wellbeing, and (as with the voice more broadly) their identity-related traits. In the present study, we utilize a large-scale dataset comprising more than 35 hours of densely labeled vocal bursts to model emotionally expressive states and demographic traits from nonverbal vocalizations. We compare a single-task and multi-task deep learning architecture to explore how models can leverage acoustic co-dependencies that may exist between the expression of 10 emotions by vocal bursts and the demographic traits of the speaker. Results show that nonverbal vocalizations can be reliably leveraged to predict emotional expression, age, and country of origin. In a multi-task setting, our experiments show that joint learning of emotional expression and demographic traits appears to yield robust results, primarily beneficial for the classification of a speaker's country of origin."
   ],
   "doi": "10.21437/Interspeech.2022-10927"
  },
  "boeddeker22_interspeech": {
   "authors": [
    [
     "Christoph",
     "Boeddeker"
    ],
    [
     "Tobias",
     "Cord-Landwehr"
    ],
    [
     "Thilo",
     "von Neumann"
    ],
    [
     "Reinhold",
     "Haeb-Umbach"
    ]
   ],
   "title": "An Initialization Scheme for Meeting Separation with Spatial Mixture Models",
   "original": "10929",
   "page_count": 5,
   "order": 55,
   "p1": 271,
   "pn": 275,
   "abstract": [
    "Spatial mixture model (SMM) supported acoustic beamforming has been extensively used for the separation of simultaneously active speakers. However, it has hardly been considered for the separation of meeting data, that are characterized by long recordings and only partially overlapping speech. In this contribution, we show that the fact that often only a single speaker is active can be utilized for a clever initialization of an SMM that employs time-varying class priors. In experiments on LibriCSS we show that the proposed initialization scheme achieves a significantly lower Word Error Rate (WER) on a downstream speech recognition task than a random initialization of the class probabilities by drawing from a Dirichlet distribution. With the only requirement that the number of speakers has to be known, we obtain a WER of 5.9 %, which is comparable to the best reported WER on this data set. Furthermore, the estimated speaker activity from the mixture model serves as a diarization based on spatial information."
   ],
   "doi": "10.21437/Interspeech.2022-10929"
  },
  "roux22_interspeech": {
   "authors": [
    [
     "Thibault Bañeras",
     "Roux"
    ],
    [
     "Mickael",
     "Rouvier"
    ],
    [
     "Jane",
     "Wottawa"
    ],
    [
     "Richard",
     "Dufour"
    ]
   ],
   "title": "Qualitative Evaluation of Language Model Rescoring in Automatic Speech Recognition",
   "original": "10931",
   "page_count": 5,
   "order": 805,
   "p1": 3968,
   "pn": 3972,
   "abstract": [
    "Evaluating automatic speech recognition (ASR) systems is a classical but difficult and still open problem, which often boils down to focusing only on the word error rate (WER). However, this metric suffers from many limitations and does not allow an in-depth analysis of automatic transcription errors. In this paper, we propose to study and understand the impact of language models rescoring in ASR systems by means of several metrics often used in other natural language processing (NLP) tasks in addition to the WER. In particular, we introduce two measures related to morpho-syntactic and semantic aspects of transcribed words: 1) the POSER (Part-of-speech Error Rate), which should highlight the grammatical aspects, and 2) the EmbER (Embedding Error Rate), a measurement that modifies the WER by providing a weighting according to the semantic distance of the wrongly transcribed words. These metrics illustrate the linguistic contributions of the language models that are applied during an a posteriori rescoring step on transcription hypotheses, more on the grammatical aspects of the sentences than on the semantical ones."
   ],
   "doi": "10.21437/Interspeech.2022-10931"
  },
  "kim22n_interspeech": {
   "authors": [
    [
     "MISEUL",
     "KIM"
    ],
    [
     "ZHENYU",
     "PIAO"
    ],
    [
     "Seyun",
     "Um"
    ],
    [
     "Ran",
     "Lee"
    ],
    [
     "Jaemin",
     "Joh"
    ],
    [
     "Seungshin",
     "Lee"
    ],
    [
     "Hong-Goo",
     "Kang"
    ]
   ],
   "title": "Light-Weight Speaker Verification with Global Context Information",
   "original": "10932",
   "page_count": 5,
   "order": 1033,
   "p1": 5105,
   "pn": 5109,
   "abstract": [
    "In this paper, we propose a light-weight speaker verification (SV) system that utilizes the characteristics of utterance-level global features. Many recent SV tasks employ convolutional neural networks (CNNs) to extract representative speaker features from the given input utterances. However, their inherent receptive field size on the feature extraction process is limited by the localized structure of the convolutional layers. To effectively extract utterance-level global speaker representations, we introduce a novel architecture combining a CNN with a self-attention network that is able to utilize the relationship between local and aggregated global features. The global features are continuously updated at every analysis block using a point-wise attentive summation to the local features. We also adopt a densely connected CNN structure (DenseNet) to reliably estimate speaker-related local features with a small number of model parameters. Our proposed model shows higher speaker verification performance with EER 1.935% with significantly small number of parameters, 1.2M, which is 16% reduced model size than the baseline models."
   ],
   "doi": "10.21437/Interspeech.2022-10932"
  },
  "su22b_interspeech": {
   "authors": [
    [
     "Jing",
     "Su"
    ],
    [
     "Longxiang",
     "Zhang"
    ],
    [
     "Hamid Reza",
     "Hassanzadeh"
    ],
    [
     "Thomas",
     "Schaaf"
    ]
   ],
   "title": "Extract and Abstract with BART for Clinical Notes from Doctor-Patient Conversations",
   "original": "10935",
   "page_count": 5,
   "order": 506,
   "p1": 2488,
   "pn": 2492,
   "abstract": [
    "Reducing the burden of documentation physicians are required to do with speech understanding is a challenging and worthwhile goal with the potential to improve care. When transcripts of doctor-patient conversations are available, automatic summarization with deep neural networks is one promising solution to reducing documentation workload. We develop an ``extract-and-abstract'' approach to automatic generation of the History of Present Illness (HPI) section in clinical notes with BART: we train a classifier on annotated data to predict a clinical section each utterance is most relevant to; we then utilize the trained classifier to select only utterances from conversations relevant to HPI to be considered as input to BART for summarization; we experiment with additional filtering methods on selected utterances to further reduce input truncation due to the token limit of BART model. Results show that the generated summaries from our approach improve in both ROUGE scores and extracted medical concepts over previously published results. Considering the improvement is achieved with a relatively small set of doctor-patient conversations, we expect further improvement with more labeled data in the future."
   ],
   "doi": "10.21437/Interspeech.2022-10935"
  },
  "chen22r_interspeech": {
   "authors": [
    [
     "Zhehuai",
     "Chen"
    ],
    [
     "Yu",
     "Zhang"
    ],
    [
     "Andrew",
     "Rosenberg"
    ],
    [
     "Bhuvana",
     "Ramabhadran"
    ],
    [
     "Pedro J.",
     "Moreno"
    ],
    [
     "Ankur",
     "Bapna"
    ],
    [
     "Heiga",
     "Zen"
    ]
   ],
   "title": "MAESTRO: Matched Speech Text Representations through Modality Matching",
   "original": "10937",
   "page_count": 5,
   "order": 830,
   "p1": 4093,
   "pn": 4097,
   "abstract": [
    "We present Maestro, a self-supervised training method to unify representations learnt from speech and text modalities. Self-supervised learning from speech signals aims to learn the latent structure inherent in the signal, while self-supervised learning from text attempts to capture lexical information. Learning aligned representations from unpaired speech and text sequences is a challenging task. Previous work either implicitly enforced the representations learnt from these two modalities to be aligned in the latent space through multi-tasking and parameter sharing or explicitly through conversion of modalities via speech synthesis. While the former suffers from interference between the two modalities, the latter introduces additional complexity. In this paper, we propose Maestro, a novel algorithm to learn unified representations from both these modalities simultaneously that can transfer to diverse downstream tasks such as Automated Speech Recognition(ASR) and Speech Translation(ST). Maestro learns unified representations through sequence alignment, duration prediction and matching embeddings in the learned space through an aligned masked-language model loss. We establish a new state-of-the-art (SOTA) on VoxPopuli multilingual ASR with a 8% relative reduction in Word Error Rate (WER), multi-domain SpeechStew ASR (3.7%relative) and 21 languages to English multilingual ST on CoVoST 2 with an improvement of 2.8BLEU averaged over 21 languages."
   ],
   "doi": "10.21437/Interspeech.2022-10937"
  },
  "jia22b_interspeech": {
   "authors": [
    [
     "Ye",
     "Jia"
    ],
    [
     "Yifan",
     "Ding"
    ],
    [
     "Ankur",
     "Bapna"
    ],
    [
     "Colin",
     "Cherry"
    ],
    [
     "Yu",
     "Zhang"
    ],
    [
     "Alexis",
     "Conneau"
    ],
    [
     "Nobu",
     "Morioka"
    ]
   ],
   "title": "Leveraging unsupervised and weakly-supervised data to improve direct speech-to-speech translation",
   "original": "10938",
   "page_count": 5,
   "order": 349,
   "p1": 1721,
   "pn": 1725,
   "abstract": [
    "End-to-end speech-to-speech translation (S2ST) without relying on intermediate text representations is a rapidly emerging frontier of research. Recent works have demonstrated that the performance of such direct S2ST systems is approaching that of conventional cascade S2ST when trained on comparable datasets. However, in practice, the performance of direct S2ST is bounded by the availability of paired S2ST training data. In this work, we explore multiple approaches for leveraging much more widely available unsupervised and weakly-supervised speech and text data to improve the performance of direct S2ST based on Translatotron 2. With our most effective approaches, the average translation quality of direct S2ST on 21 language pairs on the CVSS-C corpus is improved by +13.6 BLEU (or +113% relatively), as compared to the previous state-of-the-art trained without additional data. The improvements on low-resource language are even more significant (+398% relatively on average). Our comparative studies suggest future research directions for S2ST and speech representation learning."
   ],
   "doi": "10.21437/Interspeech.2022-10938"
  },
  "teytaut22_interspeech": {
   "authors": [
    [
     "Yann",
     "TEYTAUT"
    ],
    [
     "Baptiste",
     "Bouvier"
    ],
    [
     "Axel",
     "Roebel"
    ]
   ],
   "title": "A study on constraining Connectionist Temporal Classification for temporal audio alignment",
   "original": "10940",
   "page_count": 5,
   "order": 1015,
   "p1": 5015,
   "pn": 5019,
   "abstract": [
    "Connectionist Temporal Classification (CTC) has become a standard for deep learning-based temporal alignment allowing relevant probabilistic distributions to be learned. However, by nature, CTC is a transcription objective that can be minimized without guaranteeing any alignment properties. This work aims to study several constraints to help CTC generating alignments. With a fully convolutional architecture coupled with multi-head attention, we investigate the task of phonetic alignment for clean speech and singing signals. The focus is set on the impact of additional losses, namely spectral envelope reconstruction, temporal structure invariance and guided monotony. Results show that, once scaled to have identical temporal dependence, combining all of these constraints produces best performances."
   ],
   "doi": "10.21437/Interspeech.2022-10940"
  },
  "ablimit22_interspeech": {
   "authors": [
    [
     "Ayimnisagul",
     "Ablimit"
    ],
    [
     "Karen",
     "Scholz"
    ],
    [
     "Tanja",
     "Schultz"
    ]
   ],
   "title": "Deep Learning Approaches for Detecting Alzheimer’s Dementia from Conversational Speech of ILSE Study",
   "original": "10942",
   "page_count": 5,
   "order": 678,
   "p1": 3348,
   "pn": 3352,
   "abstract": [
    "Automatic screening of Alzheimer's Dementia (AD) can have significant impact on society and the well-being of the patients. Early detection of AD from spontaneous speech offers great potential for inexpensive and convenient casual testing. We propose our deep neural network architecture that leverages acoustic, linguistic, and demographic features to build a model for dementia screening for the biographic interview speech corpus of Interdisciplinary Longitudinal Study on Adult Development and Aging (ILSE). We oversample non-sequential and sequential features using well-known oversampling techniques and adapted data augmentation techniques to overcome the challenge of the imbalanced dataset, since the distribution of the diagnostic groups in ILSE corresponds to the prevalence of dementia. Our system achieves 70.6% of unweighted average recall on a 3-class classification problem. Moreover, we also investigate the feature importances to the model prediction to identify the most relevant indicators for AD detection, which may contribute to interpreting signs of cognitive decline and thus supporting clinicians in the diagnosis of dementia."
   ],
   "doi": "10.21437/Interspeech.2022-10942"
  },
  "perez22_interspeech": {
   "authors": [
    [
     "Matthew",
     "Perez"
    ],
    [
     "Mimansa",
     "Jaiswal"
    ],
    [
     "Minxue",
     "Niu"
    ],
    [
     "Cristina",
     "Gorrostieta"
    ],
    [
     "Matthew",
     "Roddy"
    ],
    [
     "Kye",
     "Taylor"
    ],
    [
     "Reza",
     "Lotfian"
    ],
    [
     "John",
     "Kane"
    ],
    [
     "Emily Mower",
     "Provost"
    ]
   ],
   "title": "Mind the gap: On the value of silence representations to lexical-based speech emotion recognition",
   "original": "10943",
   "page_count": 5,
   "order": 32,
   "p1": 156,
   "pn": 160,
   "abstract": [
    "Speech timing and non-speech regions (here referred to as ``silence\"), often play a critical role in the perception of spoken language. Silence represents an important paralinguistic component in communication. For example, some of its functions include conveying emphasis, dramatization, or even sarcasm. In speech emotion recognition (SER), there has been relatively little work on investigating the utility of silence and no work regarding the effect of silence on linguistics. In this work, we present a novel framework which investigates fusing linguistic and silence representations for emotion recognition in naturalistic speech using the MSP-Podcast dataset. We investigate two methods to represent silence in SER models; the first approach uses utterance-level statistics, while the second learns a silence token embedding within a transformer language model. Our results show that modeling silence does improve SER performance and that modeling silence as a token in a transformer language model significantly improves performance on MSP-Podcast achieving a concordance correlation coefficient of .191 and .453 for activation and valence respectively. In addition, we perform analyses on the attention of silence and find that silence emphasizes the attention of its surrounding words."
   ],
   "doi": "10.21437/Interspeech.2022-10943"
  },
  "novak22_interspeech": {
   "authors": [
    [
     "Mirek",
     "Novak"
    ],
    [
     "Pavlos",
     "Papadopoulos"
    ]
   ],
   "title": "RNN-T lattice enhancement by grafting of pruned paths",
   "original": "10945",
   "page_count": 5,
   "order": 1004,
   "p1": 4960,
   "pn": 4964,
   "abstract": [
    "Recurrent Neural Network Transducers (RNN-T) - a streaming variant of end-to-end models - became very popular in recent years. Since RNN-T networks condition the future output label sequence on all previous labels, the natural search space is represented by a tree. In contrast, hybrid systems employ limited-context language models, where the natural search space is a network, i.e. a lattice. While this lattice represents a rich set of hypotheses n-best list, the search tree produced by RNN-T is more limited. In this work, we introduce a heuristic method which preserves some of the pruned hypotheses by attaching them, or grafting them to the surviving hypotheses in the search tree. We achieved up to 21% oracle WERR without degrading the best-path WER. There is no impact on CPU cost, in fact we can speed up the decoder by using a smaller beam size, and still get improvements on oracle WER."
   ],
   "doi": "10.21437/Interspeech.2022-10945"
  },
  "salais22_interspeech": {
   "authors": [
    [
     "Léane",
     "Salais"
    ],
    [
     "Pablo",
     "Arias"
    ],
    [
     "Clément",
     "Le Moine"
    ],
    [
     "Victor",
     "Rosi"
    ],
    [
     "Yann",
     "TEYTAUT"
    ],
    [
     "Nicolas",
     "Obin"
    ],
    [
     "Axel",
     "Roebel"
    ]
   ],
   "title": "Production Strategies of Vocal Attitudes",
   "original": "10947",
   "page_count": 5,
   "order": 1009,
   "p1": 4985,
   "pn": 4989,
   "abstract": [
    "Humans have an impressive ability to communicate precise social intentions and desires with their voice — through vocal attitudes. Previous studies have shown how isolated acoustic features such as pitch can convey social attitudes, but have mostly worked with single attitudes and have not controlled for inter-speaker variability. Thus, the vocal behaviours used to produce social attitudes remain mostly unknown. That is the aim of the current study, to uncover the anatomic production strategies that speakers use to communicate vocal attitudes. To do this, we analysed recordings from N=20 French speakers producing dominant, friendly, seductive and distant speech. For each of these attitudes, we investigated their vocal fold behaviour, vocal tract actuation and phonetic speech structure, with the support of deep alignment methods, and compared them with group statistics. We notably produced high-level representations of speakers' articulation (e.g. Vowel Space Density) and speech rhythm. Our results reveal speakers' prototypical strategies to produce vocal attitudes, and highlight how vocal behaviours can communicate social signals. We expect these results to provide an objective validation method for deep voice attitude conversions."
   ],
   "doi": "10.21437/Interspeech.2022-10947"
  },
  "jin22_interspeech": {
   "authors": [
    [
     "Minho",
     "Jin"
    ],
    [
     "Chelsea",
     "Ju"
    ],
    [
     "Zeya",
     "Chen"
    ],
    [
     "Yi Chieh",
     "Liu"
    ],
    [
     "Jasha",
     "Droppo"
    ],
    [
     "Andreas",
     "Stolcke"
    ]
   ],
   "title": "Adversarial Reweighting for Speaker Verification Fairness",
   "original": "10948",
   "page_count": 5,
   "order": 972,
   "p1": 4800,
   "pn": 4804,
   "abstract": [
    "We address performance fairness for speaker verification using the adversarial reweighting (ARW) method. ARW is reformulated for speaker verification with metric learning, and shown to improve results across different subgroups of gender and nationality, without requiring annotation of subgroups in the training data. An adversarial network learns a weight for each training sample in the batch so that the main learner is forced to focus on poorly performing instances. Using a min-max optimization algorithm, this method improves overall speaker verification fairness. We present three different ARW formulations: accumulated pairwise similarity, pseudo-labeling, and pairwise weighting, and measure their performance in terms of equal error rate (EER) on the VoxCeleb corpus. Results show that the pairwise weighting method can achieve 1.08% overall EER, 1.25% for male and 0.67% for female speakers, with relative EER reductions of 7.7%, 10.1% and 3.0%, respectively. For nationality subgroups, the proposed algorithm showed 1.04% EER for US speakers, 0.76% for UK speakers, and 1.22% for all others. The absolute EER gap between gender groups was reduced from 0.70% to 0.58%, while the standard deviation over nationality groups decreased from 0.21 to 0.19."
   ],
   "doi": "10.21437/Interspeech.2022-10948"
  },
  "brueggeman22_interspeech": {
   "authors": [
    [
     "Avamarie",
     "Brueggeman"
    ],
    [
     "John H.L.",
     "Hansen"
    ]
   ],
   "title": "Speaker Trait Enhancement for Cochlear Implant Users: A Case Study for Speaker Emotion Perception",
   "original": "10951",
   "page_count": 5,
   "order": 462,
   "p1": 2268,
   "pn": 2272,
   "abstract": [
    "Despite significant progress in areas such as speech recognition, cochlear implant users still experience challenges related to identifying various speaker traits such as gender, age, emotion, accent, etc. In this study, we focus on emotion as one trait. We propose the use of emotion intensity conversion to perceptually enhance emotional speech with the goal of improving speech emotion recognition for cochlear implant users. To this end, we utilize a parallel speech dataset containing emotion and intensity labels to perform conversion from normal to high intensity emotional speech. A non-negative matrix factorization method is integrated to perform emotion intensity conversion via spectral mapping. We evaluate our emotional speech enhancement using a support vector machine model for emotion recognition. In addition, we perform an emotional speech recognition listener experiment with normal hearing listeners using vocoded audio. It is suggested that such enhancement will benefit speaker trait perception for cochlear implant users."
   ],
   "doi": "10.21437/Interspeech.2022-10951"
  },
  "xue22d_interspeech": {
   "authors": [
    [
     "Jian",
     "Xue"
    ],
    [
     "Peidong",
     "Wang"
    ],
    [
     "Jinyu",
     "Li"
    ],
    [
     "Matt",
     "Post"
    ],
    [
     "Yashesh",
     "Gaur"
    ]
   ],
   "title": "Large-Scale Streaming End-to-End Speech Translation with Neural Transducers",
   "original": "10953",
   "page_count": 5,
   "order": 661,
   "p1": 3263,
   "pn": 3267,
   "abstract": [
    "Neural transducers have been widely used in automatic speech recognition (ASR). In this paper, we introduce it to streaming end-to-end speech translation (ST), which aims to convert audio signals to texts in other languages directly. Compared with cascaded ST that performs ASR followed by text-based machine translation (MT), the proposed Transformer transducer (TT)-based ST model drastically reduces inference latency, exploits speech information, and avoids error propagation from ASR to MT. To improve the modeling capacity, we propose attention pooling for the joint network in TT. In addition, we extend TT-based ST to multilingual ST, which generates texts of multiple languages at the same time. Experimental results on a large-scale 50 thousand (K) hours pseudo-labeled training set show that TT-based ST not only significantly reduces inference time but also outperforms non-streaming cascaded ST for English-German translation."
   ],
   "doi": "10.21437/Interspeech.2022-10953"
  },
  "albesano22_interspeech": {
   "authors": [
    [
     "Dario",
     "Albesano"
    ],
    [
     "Jesús",
     "Andrés-Ferrer"
    ],
    [
     "Nicola",
     "Ferri"
    ],
    [
     "Puming",
     "Zhan"
    ]
   ],
   "title": "On the Prediction Network Architecture in RNN-T for ASR",
   "original": "10954",
   "page_count": 5,
   "order": 427,
   "p1": 2093,
   "pn": 2097,
   "abstract": [
    "RNN-T models have gained popularity in the literature and in commercial systems because of their competitiveness and capability of operating in online streaming mode. In this work, we conduct an extensive study comparing several prediction network architectures for both monotonic and original RNN-T models. We compare 4 types of prediction networks based on a common state-of-the-art Conformer encoder and report results obtained on the Librispeech and an internal medical conversation data set. Our study covers both offline batch-mode and online streaming scenarios. In contrast to some previous works, our results show that Transformer does not always outperform LSTM when used as prediction network along with Conformer encoder. Inspired by our scoreboard, we propose a new simple prediction network architecture, $N$-Concat, that outperforms the others in our online streaming benchmark. Transformer and n-gram reduced architectures perform very similarly yet with some important distinct behaviour in terms of previous context. Overall we obtained up to 4.1 % relative WER improvement compared to our LSTM baseline, while reducing prediction network parameters by nearly an order of magnitude (8.4 times)."
   ],
   "doi": "10.21437/Interspeech.2022-10954"
  },
  "ekstedt22_interspeech": {
   "authors": [
    [
     "Erik",
     "Ekstedt"
    ],
    [
     "Gabriel",
     "Skantze"
    ]
   ],
   "title": "Voice Activity Projection: Self-supervised Learning of Turn-taking Events",
   "original": "10955",
   "page_count": 5,
   "order": 1050,
   "p1": 5190,
   "pn": 5194,
   "abstract": [
    "The modeling of turn-taking in dialog can be viewed as the modeling of the dynamics of voice activity of the interlocutors. We extend prior work and define the predictive task of Voice Activity Projection, a general, self-supervised objective, as a way to train turn-taking models without the need of labeled data. We highlight a theoretical weakness with prior approaches, arguing for the need of modeling the dependency of voice activity events in the projection window. We propose four zero-shot tasks, related to the prediction of upcoming turn-shifts and backchannels, and show that the proposed model outperforms prior work."
   ],
   "doi": "10.21437/Interspeech.2022-10955"
  },
  "zhou22h_interspeech": {
   "authors": [
    [
     "Xiaozhou",
     "Zhou"
    ],
    [
     "Ruying",
     "Bao"
    ],
    [
     "William M.",
     "Campbell"
    ]
   ],
   "title": "Phonetic Embedding for ASR Robustness in Entity Resolution",
   "original": "10956",
   "page_count": 5,
   "order": 662,
   "p1": 3268,
   "pn": 3272,
   "abstract": [
    "Entity Resolution (ER) in spoken dialog systems can suffer from phonetic variation in search queries caused by Automatic Speech Recognition (ASR) errors. In this paper, we propose a phonetic embedding technique to improve the robustness of the ER system to this variation, which includes a phonetic embedding model, a training-data augmentation and sampling method, and an ASR robustness evaluation methodology. We test the technique on two use cases: voice search for videos and for books in the e-commerce domain. Combined with a semantic embedding neural vector search (NVS) model, phonetic embedding reduces the error rate of retrieval by 7.07% relative for video, by 4.23% for books compared to NVS not using phonetic embedding, and by 49.9% for video, and by 35.3% for books compared to a lexical search baseline."
   ],
   "doi": "10.21437/Interspeech.2022-10956"
  },
  "faria22_interspeech": {
   "authors": [
    [
     "Arlo",
     "Faria"
    ],
    [
     "Adam",
     "Janin"
    ],
    [
     "Sidhi",
     "Adkoli"
    ],
    [
     "Korbinian",
     "Riedhammer"
    ]
   ],
   "title": "Toward Zero Oracle Word Error Rate on the Switchboard Benchmark",
   "original": "10959",
   "page_count": 5,
   "order": 806,
   "p1": 3973,
   "pn": 3977,
   "abstract": [
    "The \"Switchboard benchmark” is a very well-known test set in automatic speech recognition (ASR) research, establishing record-setting performance for systems that claim human-level transcription accuracy. This work highlights lesser-known practical considerations of this evaluation, demonstrating major improvements in word error rate (WER) by correcting the reference transcriptions and deviating from the official scoring methodology. In this more detailed and reproducible scheme, even commercial ASR systems can score below 5% WER and the established record for a research system is lowered to 2.3%. An alternative metric of transcript precision is proposed, which does not penalize deletions and appears to be more discriminating for human vs. machine performance. While commercial ASR systems are still below this threshold, a research system is shown to clearly surpass the accuracy of commercial human speech recognition. This work also explores using standardized scoring tools to compute oracle WER by selecting the best among a list of alternatives. A phrase alternatives representation is compared to utterance-level N-best lists and word-level data structures; using dense lattices and adding out-of-vocabulary words, this achieves an oracle WER of 0.18%"
   ],
   "doi": "10.21437/Interspeech.2022-10959"
  },
  "baade22_interspeech": {
   "authors": [
    [
     "Alan",
     "Baade"
    ],
    [
     "Puyuan",
     "Peng"
    ],
    [
     "David",
     "Harwath"
    ]
   ],
   "title": "MAE-AST: Masked Autoencoding Audio Spectrogram Transformer",
   "original": "10961",
   "page_count": 5,
   "order": 496,
   "p1": 2438,
   "pn": 2442,
   "abstract": [
    "In this paper, we propose a simple yet powerful improvement over the recent Self-Supervised Audio Spectrogram Transformer (SSAST) model for speech and audio classification. Specifically, we leverage the insight that the SSAST uses a very high masking ratio (75%) during pretraining, meaning that the vast majority of self-attention compute is performed on mask tokens. We address this by integrating the encoder-decoder architecture from Masked Autoencoders are Scalable Vision Learners (MAE) into the SSAST, where a deep encoder operates on only unmasked input, and a shallow decoder operates on encoder outputs and mask tokens. We find that MAE-like pretraining can provide a 3x speedup and 2x memory usage reduction over the vanilla SSAST using current audio pretraining strategies with ordinary model and input sizes. When fine-tuning on downstream tasks, which only uses the encoder, we find that our approach outperforms the SSAST on a variety of downstream tasks. We further conduct comprehensive evaluations into different strategies of pretraining and explore differences in MAE-style pretraining between the visual and audio domains."
   ],
   "doi": "10.21437/Interspeech.2022-10961"
  },
  "thakker22_interspeech": {
   "authors": [
    [
     "Manthan",
     "Thakker"
    ],
    [
     "Sefik Emre",
     "Eskimez"
    ],
    [
     "Takuya",
     "Yoshioka"
    ],
    [
     "Huaming",
     "Wang"
    ]
   ],
   "title": "Fast Real-time Personalized Speech Enhancement: End-to-End Enhancement Network (E3Net) and Knowledge Distillation",
   "original": "10962",
   "page_count": 5,
   "order": 202,
   "p1": 991,
   "pn": 995,
   "abstract": [
    "This paper investigates how to improve the runtime speed of personalized speech enhancement (PSE) networks while maintaining the model quality. Our approach includes two aspects: architecture and knowledge distillation (KD). We propose an end-to-end enhancement (E3Net) model architecture, which is 3× faster than a baseline STFT-based model. Besides, we use KD techniques to develop compressed student models without significantly degrading quality. In addition, we investigate using noisy data without reference clean signals for training the student models, where we combine KD with multi-task learning (MTL) using automatic speech recognition (ASR) loss. Our results show that E3Net provides better speech and transcription quality with a lower target speaker over-suppression (TSOS) rate than the baseline model. Furthermore, we show that the KD methods can yield student models that are 2 − 4× faster than the teacher and provides reasonable quality. Combining KD and MTL improves the ASR and TSOS metrics without degrading the speech quality."
   ],
   "doi": "10.21437/Interspeech.2022-10962"
  },
  "thienpondt22_interspeech": {
   "authors": [
    [
     "Jenthe",
     "Thienpondt"
    ],
    [
     "Kris",
     "Demuynck"
    ]
   ],
   "title": "Transfer Learning for Robust Low-Resource Children's Speech ASR with Transformers and Source-Filter Warping",
   "original": "10964",
   "page_count": 5,
   "order": 451,
   "p1": 2213,
   "pn": 2217,
   "abstract": [
    "Automatic Speech Recognition (ASR) systems are known to exhibit difficulties when transcribing children's speech. This can mainly be attributed to the absence of large children's speech corpora to train robust ASR models and the resulting domain mismatch when decoding children's speech with systems trained on adult data. In this paper, we propose multiple enhancements to alleviate these issues. First, we propose a data augmentation technique based on the source-filter model of speech to close the domain gap between adult and children's speech. This enables us to leverage the data availability of adult speech corpora by making these samples perceptually similar to children's speech. Second, using this augmentation strategy, we apply transfer learning on a Transformer model pre-trained on adult data. This model follows the recently introduced XLS-R architecture, a wav2vec 2.0 model pre-trained on several cross-lingual adult speech corpora to learn general and robust acoustic frame-level representations. Adopting this model for the ASR task using adult data augmented with the proposed source-filter warping strategy and a limited amount of in-domain children's speech significantly outperforms previous state-of-the-art results on the PF-STAR British English Children's Speech corpus with a 4.86% WER on the official test set."
   ],
   "doi": "10.21437/Interspeech.2022-10964"
  },
  "audibert22_interspeech": {
   "authors": [
    [
     "Nicolas",
     "Audibert"
    ],
    [
     "Cécile",
     "Fougeron"
    ]
   ],
   "title": "Intra-speaker phonetic variation in read speech: comparison with inter-speaker variability in a controlled population",
   "original": "10965",
   "page_count": 5,
   "order": 963,
   "p1": 4755,
   "pn": 4759,
   "abstract": [
    "Our knowledge of speech is historically built on data averaged across speakers or comparing different speakers. We therefore know little about the variability of speech produced by the same speaker: to what extent does it vary from one repetition to another and on what dimensions? In this study, we document the stability of speech and voice characteristics in 9 French speakers, on the reading of two texts recorded over ten sessions on different days over a two-month period. 21 features related to temporal, spectral, f0 and harmonicity aspects as well as their modulation between consecutive chunks are studied. The stability of these features between sessions is evaluated in comparison with their variability between speakers. Results show that short-term variability of energy in the 0-1kHz band, mean F0 and the slope of the LTAS vary the most between sessions for a given speaker, and are also among the speech and voice features that vary the most between speakers in this small cohort, while modulation features between consecutive chunks remain more stable across sessions."
   ],
   "doi": "10.21437/Interspeech.2022-10965"
  },
  "miller22_interspeech": {
   "authors": [
    [
     "Tyler",
     "Miller"
    ],
    [
     "David",
     "Harwath"
    ]
   ],
   "title": "Exploring Few-Shot Fine-Tuning Strategies for Models of Visually Grounded Speech",
   "original": "10966",
   "page_count": 5,
   "order": 288,
   "p1": 1416,
   "pn": 1420,
   "abstract": [
    "In this paper, we study models of visually-grounded speech (VGS) in a few-shot setting. Beginning with a model that was pre-trained to associate natural images with speech waveforms describing the images, we probe the model's ability to learn to recognize novel words and their visual referents from a limited number of additional examples. We define new splits for the SpokenCOCO dataset to facilitate few-shot word and object acquisition, explore various few-shot fine-tuning strategies in an effort to mitigate the catastrophic forgetting phenomenon, and identify several techniques that work well in this respect."
   ],
   "doi": "10.21437/Interspeech.2022-10966"
  },
  "le22b_interspeech": {
   "authors": [
    [
     "Giang",
     "Le"
    ],
    [
     "Chilin",
     "Shih"
    ],
    [
     "Yan",
     "Tang"
    ]
   ],
   "title": "A Laryngographic Study on the Voice Quality of Northern Vietnamese Tones under the Lombard Effect",
   "original": "10970",
   "page_count": 5,
   "order": 1070,
   "p1": 5278,
   "pn": 5282,
   "abstract": [
    "While acoustic changes in Lombard speech are well-documented, articulatory changes are less well-studied. This study presents recent findings of Northern Vietnamese tone production in noise from an articulatory perspective, based on laryngographic evidence of voice quality. From the time domain Lx signals, jitter and shimmer were found to decrease at higher noise levels while the mean F0 and harmonics-to-noise ratio increased. Prior hypotheses predicted that creaky tones might demonstrate different degrees of glottal vibratory changes compared to the modal tones. Interaction effects were detected between tone and noise levels, and a detailed examination showed that differences in Lombard effects were even found within creaky tones themselves. Furthermore, the laryngographic spectral tilt flattened at increasingly higher noise levels in the frequency domain. These findings point towards reduced creakiness in the voice quality of speech produced in noise, and provide evidence that hyper-articulation starts at the sound source."
   ],
   "doi": "10.21437/Interspeech.2022-10970"
  },
  "romana22_interspeech": {
   "authors": [
    [
     "Amrit",
     "Romana"
    ],
    [
     "Minxue",
     "Niu"
    ],
    [
     "Matthew",
     "Perez"
    ],
    [
     "Angela",
     "Roberts"
    ],
    [
     "Emily Mower",
     "Provost"
    ]
   ],
   "title": "Enabling Off-the-Shelf Disfluency Detection and Categorization for Pathological Speech",
   "original": "10971",
   "page_count": 5,
   "order": 388,
   "p1": 1916,
   "pn": 1920,
   "abstract": [
    "A speech disfluency, such as a filled pause, repetition, or revision, disrupts the typical flow of speech. Disfluency modeling has grown as a research area, as recent work has shown that these disfluencies may help in assessing health conditions. For example, for individuals with cognitive impairment, changes in disfluencies may indicate worsening symptoms. However, work on disfluency modeling has focused heavily on detection and less on categorization. Work that has focused on categorization has suffered with two specific classes: repetitions and revisions. In this paper, we evaluate how BERT (Bidirectional Encoder Representations from Transformers) compares to other models on disfluency detection and categorization. We also propose adding a second fine-tuning task where BERT learns to distance repetitions and revisions from their repairs with triplet loss. We find that BERT and BERT with triplet loss outperform previous work on disfluency detection and categorization, particularly for repetitions and revisions. In this paper we present the first analysis of how these models can be fine-tuned on widely available disfluency data, and then used in an off-the-shelf manner on small corpora of pathological speech."
   ],
   "doi": "10.21437/Interspeech.2022-10971"
  },
  "peterson22_interspeech": {
   "authors": [
    [
     "Kay",
     "Peterson"
    ],
    [
     "Audrey",
     "Tong"
    ],
    [
     "Yan",
     "Yu"
    ]
   ],
   "title": "OpenASR21: The Second Open Challenge for Automatic Speech Recognition of Low-Resource Languages",
   "original": "10972",
   "page_count": 5,
   "order": 991,
   "p1": 4895,
   "pn": 4899,
   "abstract": [
    "In 2021, the National Institute of Standards and Technology (NIST), in cooperation with the Intelligence Advanced Research Project Activity (IARPA), conducted OpenASR21, the second cycle of an open challenge series of automatic speech recognition (ASR) technology for low-resource languages. The OpenASR21 Challenge was offered for 15 low-resource languages. Five of these languages were new in 2021. OpenASR21 also introduced a case-sensitive scoring track on a wider set of data genres for three of the new languages, as a proxy for assessing ASR performance on proper nouns. The paper gives an overview of the challenge setup and results. Fifteen teams from seven countries made at least one required valid submission. 504 submissions were scored. Results show that ASR performance under a severely constrained training condition is still a challenge, with the best Word Error Rate (WER) ranging from 32\\% (Swahili) to 68\\% (Farsi). However, improvements over OpenASR20 were made by augmenting training data with perturbation and text-to-speech techniques along with system combination."
   ],
   "doi": "10.21437/Interspeech.2022-10972"
  },
  "kirkland22_interspeech": {
   "authors": [
    [
     "Ambika",
     "Kirkland"
    ],
    [
     "Harm",
     "Lameris"
    ],
    [
     "Eva",
     "Szekely"
    ],
    [
     "Joakim",
     "Gustafson"
    ]
   ],
   "title": "Where's the uh, hesitation? The interplay between filled pause location, speech rate and fundamental frequency in perception of confidence",
   "original": "10973",
   "page_count": 5,
   "order": 1010,
   "p1": 4990,
   "pn": 4994,
   "abstract": [
    "Much of the research investigating the perception of speaker certainty has relied on either attempting to elicit prosodic features in read speech, or artificial manipulation of recorded audio. Our novel method of controlling prosody in synthesized spontaneous speech provides a powerful tool for studying speech perception and can provide better insight into the interacting effects of prosodic features on perception while also paving the way for conversational systems which are more effectively able to engage in and respond to social behaviors. Here we have used this method to examine the combined impact of filled pause location, speech rate and f0 on the perception of speaker confidence. We found an additive effect of all three features. The most confident-sounding utterances had no filler, low f0 and high speech rate, while the least confident-sounding utterances had a medial filled pause, high f0 and low speech rate. Insertion of filled pauses had the strongest influence, but pitch and speaking rate could be used to more finely control the uncertainty cues in spontaneous speech synthesis."
   ],
   "doi": "10.21437/Interspeech.2022-10973"
  },
  "panchapagesan22_interspeech": {
   "authors": [
    [
     "Sankaran",
     "Panchapagesan"
    ],
    [
     "Arun",
     "Narayanan"
    ],
    [
     "Turaj Zakizadeh",
     "Shabestary"
    ],
    [
     "Shuai",
     "Shao"
    ],
    [
     "Nathan",
     "Howard"
    ],
    [
     "Alex",
     "Park"
    ],
    [
     "James",
     "Walker"
    ],
    [
     "Alexander",
     "Gruenstein"
    ]
   ],
   "title": "A Conformer-based Waveform-domain Neural Acoustic Echo Canceller Optimized for ASR Accuracy",
   "original": "10974",
   "page_count": 5,
   "order": 516,
   "p1": 2538,
   "pn": 2542,
   "abstract": [
    "Acoustic Echo Cancellation (AEC) is essential for accurate recognition of queries spoken to a smart speaker that is playing out audio. Previous work has shown that a neural AEC model operating on log-mel spectral features (denoted \"logmel” hereafter) can greatly improve Automatic Speech Recognition (ASR) accuracy when optimized with an auxiliary loss utilizing a pre-trained ASR model encoder. In this paper, we develop a conformer-based waveform-domain neural AEC model inspired by the \"TasNet” architecture. The model is trained by jointly optimizing Negative Scale-Invariant SNR (SISNR) and ASR losses on a large speech dataset. On a realistic rerecorded test set, we find that cascading a linear adaptive AEC and a waveform-domain neural AEC is very effective, giving 56-59% word error rate (WER) reduction over the linear AEC alone. On this test set, the 1.6M parameter waveform-domain neural AEC also improves over a larger 6.5M parameter logmel-domain neural AEC model by 20-29% in easy to moderate conditions. By operating on smaller frames, the waveform neural model is able to perform better at smaller sizes and is better suited for applications where memory is limited."
   ],
   "doi": "10.21437/Interspeech.2022-10974"
  },
  "joshi22_interspeech": {
   "authors": [
    [
     "Sonal",
     "Joshi"
    ],
    [
     "Saurabh",
     "Kataria"
    ],
    [
     "Yiwen",
     "Shao"
    ],
    [
     "Piotr",
     "Żelasko"
    ],
    [
     "Jesús",
     "Villalba"
    ],
    [
     "Sanjeev",
     "Khudanpur"
    ],
    [
     "Najim",
     "Dehak"
    ]
   ],
   "title": "Defense against Adversarial Attacks on Hybrid Speech Recognition System using Adversarial Fine-tuning with Denoiser",
   "original": "10977",
   "page_count": 5,
   "order": 1019,
   "p1": 5035,
   "pn": 5039,
   "abstract": [
    "Adversarial attacks are a threat to automatic speech recognition (ASR) systems, and it becomes imperative to propose defenses to protect them. In this paper, we perform experiments to show that K2 conformer hybrid ASR is strongly affected by white-box adversarial attacks. We propose three defenses–denoiser pre-processor, adversarially fine-tuning ASR model, and adversarially fine-tuning joint model of ASR and denoiser. Our evaluation shows denoiser pre-processor (trained on offline adversarial examples) fails to defend against adaptive white-box attacks. However, adversarially fine-tuning the denoiser using a tandem model of denoiser and ASR offers more robustness. We evaluate two variants of this defense–one updating parameters of both models and the second keeping ASR frozen. The joint model offers a mean absolute decrease of 19.3% ground truth (GT) WER with reference to baseline against fast gradient sign method (FGSM) attacks with different L∞ norms. The joint model with frozen ASR parameters gives the best defense against projected gradient descent (PGD) with 7 iterations, yielding a mean absolute increase of 22.3% GT WER with reference to baseline; and against PGD with 500 iterations, yielding a mean absolute decrease of 45.08% GT WER and an increase of 68.05% adversarial target WER."
   ],
   "doi": "10.21437/Interspeech.2022-10977"
  },
  "gent22_interspeech": {
   "authors": [
    [
     "Helen",
     "Gent"
    ],
    [
     "Chase",
     "Adams"
    ],
    [
     "Yan",
     "Tang"
    ],
    [
     "Chilin",
     "Shih"
    ]
   ],
   "title": "Deep Learning for Prosody-Based Irony Classification in Spontaneous Speech",
   "original": "10978",
   "page_count": 5,
   "order": 810,
   "p1": 3993,
   "pn": 3997,
   "abstract": [
    "Recognizing irony in speech and text can be challenging even for humans. For natural language processing (NLP) applications, irony recognition presents a unique challenge as irony alters the sentiment and meaning of the words themselves. Combining phonological insights from past literature on irony prosody and deep learning modeling, this research presents a new approach to irony classification in naturalistic speech data. A new corpus consisting of nearly five hours of irony-annotated, naturalistic, conversational speech data has been constructed for this study. A wide array of utterance-level and time-series acoustic features were extracted from this data and utilized in the training and fine-tuning of a series of deep learning approaches for irony classification. The best-performing model achieved an area under the curve of 0.811 in the speaker dependent condition, and 0.738 in the speaker independent condition, outperforming most irony classification models in the existing literature. In addition to the myriad real-world applications for this approach, its contribution to the understanding of prosodically-encoded augmentation of semantic content constitutes a significant step forward for research in the fields of linguistics and NLP."
   ],
   "doi": "10.21437/Interspeech.2022-10978"
  },
  "zygis22_interspeech": {
   "authors": [
    [
     "Marzena",
     "Zygis"
    ],
    [
     "Sarah",
     "Wesolek"
    ],
    [
     "Nina",
     "Hosseini-Kivanani"
    ],
    [
     "Manfred",
     "Krifka"
    ]
   ],
   "title": "The Prosody of Cheering in Sport Events",
   "original": "10982",
   "page_count": 5,
   "order": 1071,
   "p1": 5283,
   "pn": 5287,
   "abstract": [
    "Motivational speaking usually conveys a highly emotional message and its purpose is to invite action. The goal of this paper is to investigate the prosodic realization of one particular type of cheering, namely inciting cheering for single addressees in sport events (here, long-distance running), using the name of that person. 31 native speakers of German took part in the experiment. They were asked to cheer up an individual marathon runner in a sporting event represented by video by producing his or her name (1-5 syllable long). For reasons of comparison, the participants also produced the same names in isolation and carrier sentences. Our results reveal that speakers use different strategies to meet their motivational communicative goals: while some speakers produced the runners' names by dividing them into syllables, others pronounced the names as quickly as possible putting more emphasis on the first syllable. A few speakers followed a mixed strategy. Contrary to our expectations, it was not the intensity that mostly contributes to the differences between the different speaking styles (cheering vs. neutral), at least in the methods we were using. Rather, participants employed higher fundamental frequency and longer duration when cheering for marathon runners."
   ],
   "doi": "10.21437/Interspeech.2022-10982"
  },
  "joshi22b_interspeech": {
   "authors": [
    [
     "Sonal",
     "Joshi"
    ],
    [
     "Saurabh",
     "Kataria"
    ],
    [
     "Jesús",
     "Villalba"
    ],
    [
     "Najim",
     "Dehak"
    ]
   ],
   "title": "AdvEst: Adversarial Perturbation Estimation to Classify and Detect Adversarial Attacks against Speaker Identification",
   "original": "10985",
   "page_count": 5,
   "order": 1024,
   "p1": 5060,
   "pn": 5064,
   "abstract": [
    "Adversarial attacks pose a severe security threat to the state-of-the-art speaker identification systems, thereby making it vital to propose countermeasures against them. Building on our previous work that used representation learning to classify and detect adversarial attacks, we propose an improvement to it using AdvEst, a method to estimate adversarial perturbation. First, we empirically validate our claim that training representation learning network using adversarial perturbations as opposed to adversarial examples (consisting of a combination of clean signal and adversarial perturbation) is beneficial because it potentially eliminates nuisance information. At inference time, we use a time-domain denoiser to estimate the adversarial perturbations from adversarial examples. Using our improved representation learning approach to obtain attack embeddings (signatures), we evaluate their performance for three applications: known attack classification, attack verification, and unknown attack detection. We show that common attacks in the literature (Fast Gradient Sign Method (FGSM), Projected Gradient Descent (PGD), Carlini-Wagner (CW) with different Lp threat models) can be classified with an accuracy of ∼96%. We also detect unknown attacks with an equal error rate (EER) of ∼9%, an absolute improvement of ∼12% from our previous work."
   ],
   "doi": "10.21437/Interspeech.2022-10985"
  },
  "cordero22_interspeech": {
   "authors": [
    [
     "Maria del Mar",
     "Cordero"
    ],
    [
     "Ambre",
     "Denis-Noël"
    ],
    [
     "Elsa",
     "Spinelli"
    ],
    [
     "Fanny",
     "Meunier"
    ]
   ],
   "title": "Neural correlates of acoustic and semantic cues during speech segmentation in French",
   "original": "10986",
   "page_count": 5,
   "order": 823,
   "p1": 4058,
   "pn": 4062,
   "abstract": [
    "Natural speech is highly complex and variable. Spoken language, in contrast to written language, does not have any clear word boundaries. Adult listeners can exploit different types of information to segment the continuous stream such as acoustic and semantic information. However, the weight of these cues, when co-occurring, remains to be determined. Behavioural tasks are not conclusive on this point as they focus participants' attention on certain sources of information, thus biasing the results. Here we looked at the processing of homophonic utterances such as \"l'amie\" vs \"la mie\" (both /lami/) which include fine acoustic differences and for which the meaning changes depending on segmentation. To examine the perceptual resolution of such ambiguities when semantic information is available, we measured the online processing of sentences containing such sequences in an ERP experiment involving no active task. In a congruent context, semantic information matched the acoustic signal of the word \"amie\". However, in the incongruent condition, the semantic information carried by the sentence and the acoustic signal were leading to different lexical candidates. Our results suggest a preponderant weight of semantic information that takes over the acoustic information."
   ],
   "doi": "10.21437/Interspeech.2022-10986"
  },
  "siahkoohi22_interspeech": {
   "authors": [
    [
     "Ali",
     "Siahkoohi"
    ],
    [
     "Michael",
     "Chinen"
    ],
    [
     "Tom",
     "Denton"
    ],
    [
     "W. Bastiaan",
     "Kleijn"
    ],
    [
     "Jan",
     "Skoglund"
    ]
   ],
   "title": "Ultra-Low-Bitrate Speech Coding with Pretrained Transformers",
   "original": "10988",
   "page_count": 5,
   "order": 896,
   "p1": 4421,
   "pn": 4425,
   "abstract": [
    "Speech coding facilitates the transmission of speech over low-bandwidth networks with minimal distortion. Neural-network based speech codecs have recently demonstrated significant improvements in quality over traditional approaches. While this new generation of codecs is capable of synthesizing high-fidelity speech, their use of recurrent or convolutional layers often restricts their effective receptive fields, which prevents them from compressing speech efficiently. We propose to further reduce the bitrate of neural speech codecs through the use of pretrained Transformers, capable of exploiting long-range dependencies in the input signal due to their inductive bias. As such, we use a pretrained Transformer in tandem with a convolutional encoder, which is trained end-to-end with a quantizer and a generative adversarial net decoder. Our numerical experiments show that supplementing the convolutional encoder of a neural speech codec with Transformer speech embeddings yields a speech codec with a bitrate of 600 bps that outperforms the original neural speech codec in synthesized speech quality when trained at the same bitrate. Subjective human evaluations suggest that the quality of the resulting codec is comparable or better than that of conventional codecs operating at three to four times the rate."
   ],
   "doi": "10.21437/Interspeech.2022-10988"
  },
  "shinohara22_interspeech": {
   "authors": [
    [
     "Yusuke",
     "Shinohara"
    ],
    [
     "Shinji",
     "Watanabe"
    ]
   ],
   "title": "Minimum latency training of sequence transducers for streaming end-to-end speech recognition",
   "original": "10989",
   "page_count": 5,
   "order": 428,
   "p1": 2098,
   "pn": 2102,
   "abstract": [
    "Sequence transducers, such as the RNN-T and the Conformer-T, are one of the most promising models of end-to-end speech recognition, especially in streaming scenarios where both latency and accuracy are important. Although various methods, such as alignment-restricted training and FastEmit, have been studied to reduce the latency, latency reduction is often accompanied with a significant degradation in accuracy. We argue that this suboptimal performance might be caused because none of the prior methods explicitly model and reduce the latency. In this paper, we propose a new training method to explicitly model and reduce the latency of sequence transducer models. First, we define the expected latency at each diagonal line on the lattice, and show that its gradient can be computed efficiently within the forward-backward algorithm. Then we augment the transducer loss with this expected latency, so that an optimal trade-off between latency and accuracy is achieved. Experimental results on the WSJ dataset show that the proposed minimum latency training reduces the latency of causal Conformer-T from 220 ms to 27 ms within a WER degradation of 0.7%, and outperforms conventional alignment-restricted training (110 ms) and FastEmit (67 ms) methods."
   ],
   "doi": "10.21437/Interspeech.2022-10989"
  },
  "wang22ba_interspeech": {
   "authors": [
    [
     "Gary",
     "Wang"
    ],
    [
     "Andrew",
     "Rosenberg"
    ],
    [
     "Bhuvana",
     "Ramabhadran"
    ],
    [
     "Fadi",
     "Biadsy"
    ],
    [
     "Jesse",
     "Emond"
    ],
    [
     "Yinghui",
     "Huang"
    ],
    [
     "Pedro J.",
     "Moreno"
    ]
   ],
   "title": "Non-Parallel Voice Conversion for ASR Augmentation",
   "original": "10990",
   "page_count": 5,
   "order": 690,
   "p1": 3408,
   "pn": 3412,
   "abstract": [
    "Automatic speech recognition (ASR) needs to be robust to speaker differences. Voice Conversion (VC) modifies speaker characteristics of input speech. This is an attractive feature for ASR data augmentation. In this paper, we demonstrate that voice conversion can be used as a data augmentation technique to improve ASR performance, even on LibriSpeech, which contains 2,456 speakers. For ASR augmentation, it is necessary that the VC model be robust to a wide range of input speech. This motivates the use of a non-autoregressive, non-parallel VC model, and the use of a pretrained ASR encoder within the VC model. This work suggests that despite including many speakers, speaker diversity may remain a limitation to ASR quality. Finally, interrogation of our VC performance has provided useful metrics for objective evaluation of VC quality."
   ],
   "doi": "10.21437/Interspeech.2022-10990"
  },
  "fox22_interspeech": {
   "authors": [
    [
     "Jennifer",
     "Fox"
    ],
    [
     "Natalie",
     "Delworth"
    ]
   ],
   "title": "Improving Contextual Recognition of Rare Words with an Alternate Spelling Prediction Model",
   "original": "10991",
   "page_count": 5,
   "order": 793,
   "p1": 3914,
   "pn": 3918,
   "abstract": [
    "Contextual ASR, which takes a list of bias terms as input along with audio, has drawn recent interest as ASR use becomes more widespread. We are releasing contextual biasing lists to accompany the Earnings21 dataset, creating a public benchmark for this task. We present baseline results on this benchmark using a pretrained end-to-end ASR model from the WeNet toolkit. We show results for shallow fusion contextual biasing applied to two different decoding algorithms. Our baseline results confirm observations that end-to-end models struggle in particular with words that are rarely or never seen during training, and that existing shallow fusion techniques do not adequately address this problem. We propose an alternate spelling prediction model that improves recall of rare words by 34.7% relative and of out-of-vocabulary words by 97.2% relative, compared to contextual biasing without alternate spellings. This model is conceptually similar to ones used in prior work, but is simpler to implement as it does not rely on either a pronunciation dictionary or an existing text-to-speech system."
   ],
   "doi": "10.21437/Interspeech.2022-10991"
  },
  "zhu22e_interspeech": {
   "authors": [
    [
     "Ge",
     "Zhu"
    ],
    [
     "Juan-Pablo",
     "Caceres"
    ],
    [
     "Justin",
     "Salamon"
    ]
   ],
   "title": "Filler Word Detection and Classification: A Dataset and Benchmark",
   "original": "10992",
   "page_count": 5,
   "order": 764,
   "p1": 3769,
   "pn": 3773,
   "abstract": [
    "Filler words such as uh' orum' are sounds or words people use to signal they are pausing to think. Finding and removing filler words from recordings is a common and tedious task in media editing. Automatically detecting and classifying filler words could greatly aid in this task, but few studies have been published on this problem to date.A key reason is the absence of a dataset with annotated filler words for model training and evaluation.In this work, we present a novel speech dataset, PodcastFillers, with 35K annotated filler words and 50K annotations of other sounds that commonly occur in podcasts such as breaths, laughter, and word repetitions.We propose a pipeline that leverages VAD and ASR to detect filler candidates and a classifier to distinguish between filler word types.We evaluate our proposed pipeline on PodcastFillers, compare to several baselines, and present a detailed ablation study.In particular, we evaluate the importance of using ASR and how it compares to a transcription-free approach resembling keyword spotting. We show that our pipeline obtains state-of-the-art results, and that leveraging ASR strongly outperforms a keyword spotting approach. We make PodcastFillers publicly available, in the hope that our work serves as a benchmark for future research."
   ],
   "doi": "10.21437/Interspeech.2022-10992"
  },
  "ashokumar22_interspeech": {
   "authors": [
    [
     "Monica",
     "Ashokumar"
    ],
    [
     "Jean-Luc",
     "Schwartz"
    ],
    [
     "Takayuki",
     "Ito"
    ]
   ],
   "title": "Orofacial somatosensory inputs in speech perceptual training modulate speech production",
   "original": "10993",
   "page_count": 4,
   "order": 158,
   "p1": 784,
   "pn": 787,
   "abstract": [
    "Somatosensory inputs are important to acquire or learn precise control of movement [1]. In the case of speech, receiving somatosensory inputs together with corresponding speech sounds may be a key to formulate or calibrate the speech production system [2]. We here examined whether speech production can be modulated by perceptual training with repetitive exposure to paired auditory-somatosensory stimulation in the absence of actual production of the sound. We carried out a perceptual training using a vowel identification task with /e/-/eu/ continuum. The speech sounds were accompanied with somatosensory stimulation, in which a facial skin-stretch was applied in the backward direction. The vowels /e/ and /eu/ were recorded prior to and following the training and the first three formants were compared. Results showed that the third formant of /e/ was increased following the training, and the rest of formant was not changed. Since the current somatosensory stimulation was related to the articulatory movement for the production of /e/ (lip-spreading), repetitive exposure to somatosensory stimulation in addition to the sound may specifically change the articulatory behavior for the production of /e/. The results suggest that perceptual training with specific pairs of auditory-somatosensory inputs can be important to formulate production mechanisms."
   ],
   "doi": "10.21437/Interspeech.2022-10993"
  },
  "kim22o_interspeech": {
   "authors": [
    [
     "Tae-Woo",
     "Kim"
    ],
    [
     "Min-Su",
     "Kang"
    ],
    [
     "Gyeong-Hoon",
     "Lee"
    ]
   ],
   "title": "Adversarial Multi-Task Learning for Disentangling Timbre and Pitch in Singing Voice Synthesis",
   "original": "10994",
   "page_count": 5,
   "order": 610,
   "p1": 3008,
   "pn": 3012,
   "abstract": [
    "Recently, deep learning-based generative models have been introduced to generate singing voices. One approach is to predict the parametric vocoder features consisting of explicit speech parameters. This approach has the advantage that the meaning of each feature is explicitly distinguished. Another approach is to predict mel-spectrograms for a neural vocoder. However, parametric vocoders have limitations of voice quality and the mel-spectrogram features are difficult to model because the timbre and pitch information are entangled. In this study, we propose a singing voice synthesis model with multi-task learning to use both approaches -- acoustic features for a parametric vocoder and mel-spectrograms for a neural vocoder. By using the parametric vocoder features as auxiliary features, the proposed model can efficiently disentangle and control the timbre and pitch components of the mel-spectrogram. Moreover, a generative adversarial network framework is applied to improve the quality of singing voices in a multi-singer model. Experimental results demonstrate that our proposed model can generate more natural singing voices than the single-task models, while performing better than the conventional parametric vocoder-based model."
   ],
   "doi": "10.21437/Interspeech.2022-10994"
  },
  "botelho22_interspeech": {
   "authors": [
    [
     "Catarina",
     "Botelho"
    ],
    [
     "Tanja",
     "Schultz"
    ],
    [
     "Alberto",
     "Abad"
    ],
    [
     "Isabel",
     "Trancoso"
    ]
   ],
   "title": "Challenges of using longitudinal and cross-domain corpora on studies of pathological speech",
   "original": "10995",
   "page_count": 5,
   "order": 389,
   "p1": 1921,
   "pn": 1925,
   "abstract": [
    "Several promising works have reported very exciting results in the field of speech in health, however there are still issues to address before deploying such systems into clinical applications. One of such issues is to ensure the generalisability and reliability of results. With this in mind, in this work, we perform a comparative analysis of healthy speech in two scenarios: (1) collected for six different datasets spoken in the same language, and (2) collected across different times in a single longitudinal corpus. We show that feature sets typically used for disease detection from speech (eGeMAPS, ComParE, pause-related features, ECAPA-TDNN embeddings and i-vectors) encode much information about the dataset or about changing recording conditions over time, in longitudinal studies. We support our results with classification results largely above chance level for both scenarios, and through unsupervised clustering experiments, where we observe that data naturally clusters according to dataset."
   ],
   "doi": "10.21437/Interspeech.2022-10995"
  },
  "sawhney22_interspeech": {
   "authors": [
    [
     "Ramit",
     "Sawhney"
    ],
    [
     "Megh",
     "Thakkar"
    ],
    [
     "Vishwa",
     "Shah"
    ],
    [
     "Puneet",
     "Mathur"
    ],
    [
     "Vasu",
     "Sharma"
    ],
    [
     "Dinesh",
     "Manocha"
    ]
   ],
   "title": "PISA: PoIncaré Saliency-Aware Interpolative Augmentation",
   "original": "11001",
   "page_count": 5,
   "order": 541,
   "p1": 2663,
   "pn": 2667,
   "abstract": [
    "Saliency-aware portion-wise mixup has proven to be an effective data augmentation technique for different modalities and tasks. However, it involves calculating the saliency over gradient vectors in the Euclidean space, representations that often possess complicated geometries and inherent hierarchical structure. We propose PISA, saliency-aware interpolative regularization operating in the hyperbolic space, to better capture the complex geometries of representations. To this end, we also formulate a saliency-aware mixup for speech signals. PISA outperforms existing state-of-the-art interpolative augmentation methods on 7 benchmark and low-resource datasets from the domains of speech signal processing and computer vision. PISA results in more stable training than existing data augmentation methods while being robust to adversarial attacks. It can be generalized across modalities, models and downstream tasks."
   ],
   "doi": "10.21437/Interspeech.2022-11001"
  },
  "lee22o_interspeech": {
   "authors": [
    [
     "Wo Jae",
     "Lee"
    ],
    [
     "Emanuele",
     "Coviello"
    ]
   ],
   "title": "A Multimodal Strategy for Singing Language Identification",
   "original": "11007",
   "page_count": 5,
   "order": 457,
   "p1": 2243,
   "pn": 2247,
   "abstract": [
    "Identification of the language of performance of songs is important for applications such as personalized recommendations, discovery, and search. In this paper, we present an automated multimodal approach to identify the singing language of songs that scales to millions of songs. The proposed model uses a variety of song-level features, including a consumption embedding derived from sessions listening data from a music streaming service, segment-level vocals embedding computed from the vocal track of a song, and generic timbral features. Our experimental results show that our approach outperforms benchmark models in the signing-language identification task, and demonstrates the benefit of the multimodal approach through an ablation study. In addition, we present a data augmentation technique to increase the robustness of the model to missing data modalities."
   ],
   "doi": "10.21437/Interspeech.2022-11007"
  },
  "goncalves22_interspeech": {
   "authors": [
    [
     "Lucas",
     "Goncalves"
    ],
    [
     "Carlos",
     "Busso"
    ]
   ],
   "title": "Improving Speech Emotion Recognition Using Self-Supervised Learning with Domain-Specific Audiovisual Tasks",
   "original": "11012",
   "page_count": 5,
   "order": 238,
   "p1": 1168,
   "pn": 1172,
   "abstract": [
    "ISpeech emotion recognition (SER) is a challenging task due to the limited availability of real-world labeled datasets. Since it is easier to find unlabeled data, the use of self-supervised learning (SSL) has become an attractive alternative. This study proposes new pre-text tasks for SSL to improve SER. While our target application is SER, the proposed pre-text tasks includes audiovisual formulations, leveraging the relationship between acoustic and facial features. Our proposed approach introduces three new unimodal and multimodal pre-text tasks that are carefully designed to learn better representations for predicting emotional cues from speech. Task 1 predicts energy variations (high or low) from a speech sequence. Task 2 uses speech features to predict facial activation (high or low) based on facial landmark movements. Task 3 performs a multi-class emotion recognition task on emotional labels obtained from combinations of action units (AUs) detected across a video sequence. We pre-train a network with 60.92 hours of unlabeled data, fine-tuning the model for the downstream SER task. The results on the CREMA-D dataset show that the model pre-trained on the proposed domain-specific pre-text tasks significantly improves the precision (up to 5.1%), recall (up to 4.5%), and F1-scores (up to 4.9%) of our SER system."
   ],
   "doi": "10.21437/Interspeech.2022-11012"
  },
  "liu22x_interspeech": {
   "authors": [
    [
     "Haohe",
     "Liu"
    ],
    [
     "Woosung",
     "Choi"
    ],
    [
     "Xubo",
     "Liu"
    ],
    [
     "Qiuqiang",
     "Kong"
    ],
    [
     "Qiao",
     "Tian"
    ],
    [
     "DeLiang",
     "Wang"
    ]
   ],
   "title": "Neural Vocoder is All You Need for Speech Super-resolution",
   "original": "11017",
   "page_count": 5,
   "order": 857,
   "p1": 4227,
   "pn": 4231,
   "abstract": [
    "Speech super-resolution (SR) is a task to increase speech sampling rate by generating high-frequency components. Existing speech SR methods are trained in constrained experimental settings, such as a fixed upsampling ratio. These strong constraints can potentially lead to poor generalization ability in mismatched real-world cases. In this paper, we propose a neural vocoder based speech super-resolution method (NVSR) that can handle a variety of input resolution and upsampling ratios. NVSR consists of a mel-bandwidth extension module, a neural vocoder module, and a post-processing module. Our proposed system achieves state-of-the-art results on the VCTK multi-speaker benchmark. On 44.1 kHz target resolution, NVSR outperforms WSRGlow and Nu-wave by 8% and 37% respectively on log spectral distance and achieves a significantly better perceptual quality. We also demonstrate that prior knowledge in the pre-trained vocoder is crucial for speech SR by performing mel-bandwidth extension with a simple replication-padding method. Samples can be found at https://haoheliu.github.io/nvsr."
   ],
   "doi": "10.21437/Interspeech.2022-11017"
  },
  "nagamine22_interspeech": {
   "authors": [
    [
     "Takayuki",
     "Nagamine"
    ]
   ],
   "title": "Acquisition of allophonic variation in second language speech: An acoustic and articulatory study of English laterals by Japanese speakers",
   "original": "11020",
   "page_count": 5,
   "order": 130,
   "p1": 644,
   "pn": 648,
   "abstract": [
    "Acquisition of positional allophonic variation is seen as the foundation of a successful L2 speech learning. However, previous research has mostly focused on the phonemic contrast between English /l/ and /r/, providing little evidence in the acquisition of positional allophones, such as those in English /l/. The current study investigates the acoustics and articulation of allophonic variations in English laterals produced by Japanese speakers, focusing on the effects of syllabic positions and flanking vowels. Acoustic and articulatory data were obtained from five Japanese speakers in a simultaneous audio and high-speed ultrasound tongue imaging recording set-up while they read sentences containing syllable-initial and -final tokens of English /l/ in four different vowel contexts. Acoustic analysis was conducted on 500 tokens using linear-mixed effects modelling and the articulatory data were analysed using generalised additive mixed modelling. Syllable position and vowel context had significant effects on acoustics, while midsagittal tongue shape was more influenced by vowel context, with fewer positional effects. The results demonstrate that differences in acoustics not always be mirrored exactly by midsagittal tongue shape, suggesting multidimensionality of articulation in second language speech."
   ],
   "doi": "10.21437/Interspeech.2022-11020"
  },
  "zheng22d_interspeech": {
   "authors": [
    [
     "Weiyi",
     "Zheng"
    ],
    [
     "Alex",
     "Xiao"
    ],
    [
     "Gil",
     "Keren"
    ],
    [
     "Duc",
     "Le"
    ],
    [
     "Frank",
     "Zhang"
    ],
    [
     "Christian",
     "Fuegen"
    ],
    [
     "Ozlem",
     "Kalinli"
    ],
    [
     "Yatharth",
     "Saraf"
    ],
    [
     "Abdelrahman",
     "Mohamed"
    ]
   ],
   "title": "Scaling ASR Improves Zero and Few Shot Learning",
   "original": "11023",
   "page_count": 5,
   "order": 1039,
   "p1": 5135,
   "pn": 5139,
   "abstract": [
    "With 4.5 million hours of English speech from 10 different sources across 120 countries and models of up to 10 billion parameters, we explore the frontiers of scale for automatic speech recognition. We propose some empirical data selection techniques to efficiently find the most valuable samples in pseudo-labeled datasets. To efficiently scale model sizes, we leverage various optimizations such as sparse transducer loss and parameter sharding. By training universal English ASR models with up to 10B parameters, we push the limits of speech recognition performance across many domains. Furthermore, our models were able to generalize well to novel domains and styles of speech, exceeding previous results across multiple in-house and public benchmarks. On AphasiaBank, a dataset from speakers with disorders due to brain damage, our best zero-shot and few-shot models achieve 22% and 60% relative improvement, respectively. The same universal model reaches equivalent performance with 500x less in-domain data on the SPGISpeech financial-domain dataset."
   ],
   "doi": "10.21437/Interspeech.2022-11023"
  },
  "corey22_interspeech": {
   "authors": [
    [
     "Ryan",
     "Corey"
    ],
    [
     "Manan",
     "Mittal"
    ],
    [
     "Kanad",
     "Sarkar"
    ],
    [
     "Andrew C.",
     "Singer"
    ]
   ],
   "title": "Cooperative Speech Separation With a Microphone Array and Asynchronous Wearable Devices",
   "original": "11025",
   "page_count": 5,
   "order": 1094,
   "p1": 5398,
   "pn": 5402,
   "abstract": [
    "We consider the problem of separating speech from several talkers in background noise using a fixed microphone array and a set of wearable devices. Wearable devices can provide reliable information about speech from their wearers, but they typically cannot be used directly for multichannel source separation due to network delay, sample rate offsets, and relative motion. Instead, the wearable microphone signals are used to compute the speech presence probability for each talker at each time-frequency index. Those parameters, which are robust against small sample rate offsets and relative motion, are used to track the second-order statistics of the speech sources and background noise. The fixed array then separates the speech signals using an adaptive linear time-varying multichannel Wiener filter. The proposed method is demonstrated using real-room recordings from three human talkers with binaural earbud microphones and an eight-microphone tabletop array."
   ],
   "doi": "10.21437/Interspeech.2022-11025"
  },
  "liu22y_interspeech": {
   "authors": [
    [
     "Haohe",
     "Liu"
    ],
    [
     "Xubo",
     "Liu"
    ],
    [
     "Qiuqiang",
     "Kong"
    ],
    [
     "Qiao",
     "Tian"
    ],
    [
     "Yan",
     "Zhao"
    ],
    [
     "DeLiang",
     "Wang"
    ],
    [
     "Chuanzeng",
     "Huang"
    ],
    [
     "Yuxuan",
     "Wang"
    ]
   ],
   "title": "VoiceFixer: A Unified Framework for High-Fidelity Speech Restoration",
   "original": "11026",
   "page_count": 5,
   "order": 858,
   "p1": 4232,
   "pn": 4236,
   "abstract": [
    "Speech restoration aims to remove distortions in speech signals. Prior methods mainly focus on a single type of distortion, such as speech denoising or dereverberation. However, speech signals can be degraded by several different distortions simultaneously in the real world. It is thus important to extend speech restoration models to deal with multiple distortions. In this paper, we introduce VoiceFixer, a unified framework for high-fidelity speech restoration. VoiceFixer restores speech from multiple distortions (e.g., noise, reverberation, clipping) and can expand degraded speech (e.g., noisy speech) with a low bandwidth to 44.1 kHz full-bandwidth high-fidelity speech. We design VoiceFixer based on (1) an analysis stage that predicts intermediate-level features from the degraded speech, and (2) a synthesis stage that generates waveform using a neural vocoder. Both objective and subjective evaluations show that VoiceFixer is effective on severely degraded speech, such as real-world historical speech recordings. Samples of VoiceFixer are available at https://haoheliu.github.io/voicefixer."
   ],
   "doi": "10.21437/Interspeech.2022-11026"
  },
  "teng22_interspeech": {
   "authors": [
    [
     "Zhongwei",
     "Teng"
    ],
    [
     "Quchen",
     "Fu"
    ],
    [
     "Jules",
     "White"
    ],
    [
     "Maria",
     "Powell"
    ],
    [
     "Douglas",
     "Schmidt"
    ]
   ],
   "title": "SA-SASV: An End-to-End Spoof-Aggregated Spoofing-Aware Speaker Verification System",
   "original": "11029",
   "page_count": 5,
   "order": 890,
   "p1": 4391,
   "pn": 4395,
   "abstract": [
    "Research in the past several years has boosted the performance of automatic speaker verification systems and countermeasure systems to deliver low Equal Error Rates (EERs) on each system. However, research on joint optimization of both systems is still limited. The Spoofing-Aware Speaker Verification (SASV) 2022 challenge was proposed to encourage the development of integrated SASV systems with new metrics to evaluate joint model performance. This paper proposes an ensemble-free end-to-end solution, known as Spoof-Aggregated-SASV (SA-SASV) to build a SASV system with multi-task classifiers, which are optimized by multiple losses and has more flexible requirements in training set. The proposed system is trained on the ASVSpoof 2019 LA dataset, a spoof verification dataset with small number of bonafide speakers. Results of SASV-EER indicate that the model performance can be further improved by training in complete automatic speaker verification and countermeasure datasets."
   ],
   "doi": "10.21437/Interspeech.2022-11029"
  },
  "gao22e_interspeech": {
   "authors": [
    [
     "Heting",
     "Gao"
    ],
    [
     "Junrui",
     "Ni"
    ],
    [
     "Kaizhi",
     "Qian"
    ],
    [
     "Yang",
     "Zhang"
    ],
    [
     "Shiyu",
     "Chang"
    ],
    [
     "Mark",
     "Hasegawa-Johnson"
    ]
   ],
   "title": "WavPrompt: Towards Few-Shot Spoken Language Understanding with Frozen Language Models",
   "original": "11031",
   "page_count": 5,
   "order": 556,
   "p1": 2738,
   "pn": 2742,
   "abstract": [
    "Large-scale auto-regressive language models pretrained on massive text have demonstrated their impressive ability to perform new natural language tasks with only a few text examples, without the need for fine-tuning. Recent studies further show that such a few-shot learning ability can be extended to the text-image setting by training an encoder to encode the images into embeddings functioning like the text embeddings of the language model. Interested in exploring the possibility of transferring the few-shot learning ability to the audio-text setting, we propose a novel speech understanding framework, WavPrompt, where we finetune a wav2vec model to generate a sequence of audio embeddings understood by the language model. We show that WavPrompt is a few-shot learner that can perform speech understanding tasks better than a naive text baseline. We conduct detailed ablation studies on different components and hyperparameters to empirically identify the best model configuration. In addition, we conduct a non-speech understanding experiment to show WavPrompt can extract more information than just the transcriptions. The source code is available at https://github.com/Hertin/WavPrompt."
   ],
   "doi": "10.21437/Interspeech.2022-11031"
  },
  "popuri22_interspeech": {
   "authors": [
    [
     "Sravya",
     "Popuri"
    ],
    [
     "Peng-Jen",
     "Chen"
    ],
    [
     "Changhan",
     "Wang"
    ],
    [
     "Juan",
     "Pino"
    ],
    [
     "Yossi",
     "Adi"
    ],
    [
     "Jiatao",
     "Gu"
    ],
    [
     "Wei-Ning",
     "Hsu"
    ],
    [
     "Ann",
     "Lee"
    ]
   ],
   "title": "Enhanced Direct Speech-to-Speech Translation Using Self-supervised Pre-training and Data Augmentation",
   "original": "11032",
   "page_count": 5,
   "order": 1051,
   "p1": 5195,
   "pn": 5199,
   "abstract": [
    "Direct speech-to-speech translation (S2ST) models suffer from data scarcity issues as there exists little parallel S2ST data, compared to the amount of data available for conventional cascaded systems that consist of automatic speech recognition (ASR), machine translation (MT), and text-to-speech (TTS) synthesis. In this work, we explore self-supervised pre-training with unlabeled speech data and data augmentation to tackle this issue. We take advantage of a recently proposed speech-to-unit translation (S2UT) framework that encodes target speech into discrete representations, and transfer pre-training and efficient partial finetuning techniques that work well for speech-to-text translation (S2T) to the S2UT domain by studying both speech encoder and discrete unit decoder pre-training. Our experiments on Spanish- English translation show that self-supervised pre-training consistently improves model performance compared with multitask learning with an average 6.6-12.1 BLEU gain, and it can be further combined with data augmentation techniques that apply MT to create weakly supervised training data."
   ],
   "doi": "10.21437/Interspeech.2022-11032"
  },
  "hwang22c_interspeech": {
   "authors": [
    [
     "Dongseong",
     "Hwang"
    ],
    [
     "Khe Chai",
     "Sim"
    ],
    [
     "Zhouyuan",
     "Huo"
    ],
    [
     "Trevor",
     "Strohman"
    ]
   ],
   "title": "Pseudo Label Is Better Than Human Label",
   "original": "11034",
   "page_count": 5,
   "order": 289,
   "p1": 1421,
   "pn": 1425,
   "abstract": [
    "State-of-the-art automatic speech recognition (ASR) systems are trained with tens of thousands of hours of labeled speech data. Human transcription is expensive and time consuming. Factors such as the quality and consistency of the transcription can greatly affect the performance of the ASR models trained with these data. In this paper, we show that we can train a strong teacher model to produce high quality pseudo labels by utilizing recent self-supervised and semi-supervised learning techniques. Specifically, we use JUST (Joint Unsupervised/Supervised Training) and iterative noisy student teacher training to train a 600 million parameter bi-directional teacher model. This model achieved 4.0% word error rate (WER) on a voice search task, 11.1% relatively better than a baseline. We further show that by using this strong teacher model to generate high-quality pseudo labels for training, we can achieve 13.6% relative WER reduction (5.9% to 5.1%) for a streaming model compared to using human labels."
   ],
   "doi": "10.21437/Interspeech.2022-11034"
  },
  "bittar22_interspeech": {
   "authors": [
    [
     "Alexandre",
     "Bittar"
    ],
    [
     "Philip N.",
     "Garner"
    ]
   ],
   "title": "Bayesian Recurrent Units and the Forward-Backward Algorithm",
   "original": "11035",
   "page_count": 5,
   "order": 839,
   "p1": 4137,
   "pn": 4141,
   "abstract": [
    "Using Bayes's theorem, we derive a unit-wise recurrence as well as a backward recursion similar to the forward-backward algorithm. The resulting Bayesian recurrent units can be integrated as recurrent neural networks within deep learning frameworks, while retaining a probabilistic interpretation from the direct correspondence with hidden Markov models. Whilst the contribution is mainly theoretical, experiments on speech recognition indicate that adding the derived units at the end of state-of-the-art recurrent architectures can improve the performance at a very low cost in terms of trainable parameters."
   ],
   "doi": "10.21437/Interspeech.2022-11035"
  },
  "yuan22b_interspeech": {
   "authors": [
    [
     "Ruibin",
     "Yuan"
    ],
    [
     "Yuxuan",
     "Wu"
    ],
    [
     "Jacob",
     "Li"
    ],
    [
     "Jaxter",
     "Kim"
    ]
   ],
   "title": "DeID-VC: Speaker De-identification via Zero-shot Pseudo Voice Conversion",
   "original": "11036",
   "page_count": 5,
   "order": 527,
   "p1": 2593,
   "pn": 2597,
   "abstract": [
    "The widespread adoption of speech-based online services raises security and privacy concerns regarding the data that they use and share. If the data were compromised, attackers could exploit user speech to bypass speaker verification systems or even impersonate users. To mitigate this, we propose DeID-VC, a speaker de-identification system that converts a real speaker to pseudo speakers, thus removing or obfuscating the speaker-dependent attributes from a spoken voice. The key components of DeID-VC include a Variational Autoencoder (VAE) based Pseudo Speaker Generator (PSG) and a voice conversion Autoencoder (AE) under zero-shot settings. With the help of PSG, DeID-VC can assign unique pseudo speakers at speaker level or even at utterance level. Also, two novel learning objectives are added to bridge the gap between training and inference of zero-shot voice conversion. We present our experimental results with word error rate (WER) and equal error rate (EER), along with three subjective metrics to evaluate the generated output of DeID-VC. The result shows that our method substantially improved intelligibility (WER 10% lower) and de-identification effectiveness (EER 5% higher) compared to our baseline."
   ],
   "doi": "10.21437/Interspeech.2022-11036"
  },
  "wang22ca_interspeech": {
   "authors": [
    [
     "Zihan",
     "Wang"
    ],
    [
     "Christer",
     "Gobl"
    ]
   ],
   "title": "Contribution of the glottal flow residual in affect-related voice transformation",
   "original": "11038",
   "page_count": 5,
   "order": 1072,
   "p1": 5288,
   "pn": 5292,
   "abstract": [
    "This paper explores the contribution of the glottal flow residual in affect-related voice transformation. This signal, which is defined as the difference between the output of the inverse filter estimating the glottal flow signal and the modelled source signal, was analysed using multiple regression analysis. Results show that the strength of the residual varies as a function of the source parameters and this variation is frequency dependent: low frequency energy in the residual is mainly determined by the glottal excitation strength, whereas mid to high frequencies are more influenced by the glottal pulse shape. A method for modelling the residual is presented, which enables modifications based on the changes in source parameters used for voice transformation. This method makes it possible to use the residual as part of the voice source signal when transforming the voice quality in expressive speech synthesis. The result of a listening test, involving the transformation of a neutral voice to an angry or a sad voice, shows that including the glottal flow residual can improve the perceived naturalness of the synthesis. However, the fact that the transformed utterances are still relatively degraded indicates that other factors also need to be considered."
   ],
   "doi": "10.21437/Interspeech.2022-11038"
  },
  "yang22v_interspeech": {
   "authors": [
    [
     "Mu",
     "Yang"
    ],
    [
     "Kevin",
     "Hirschi"
    ],
    [
     "Stephen Daniel",
     "Looney"
    ],
    [
     "Okim",
     "Kang"
    ],
    [
     "John H.L.",
     "Hansen"
    ]
   ],
   "title": "Improving Mispronunciation Detection with Wav2vec2-based Momentum Pseudo-Labeling for Accentedness and Intelligibility Assessment",
   "original": "11039",
   "page_count": 5,
   "order": 908,
   "p1": 4481,
   "pn": 4485,
   "abstract": [
    "Current leading mispronunciation detection and diagnosis (MDD) systems achieve promising performance via end-to-end phoneme recognition. One challenge of such end-to-end solutions is the scarcity of human-annotated phonemes on natural L2 speech. In this work, we leverage unlabeled L2 speech via a pseudo-labeling (PL) procedure and extend the fine-tuning approach based on pre-trained self-supervised learning (SSL) models. Specifically, we use Wav2vec 2.0 as our SSL model, and fine-tune it using original labeled L2 speech samples plus the created pseudo-labeled L2 speech samples. Our pseudo labels are dynamic and are produced by an ensemble of the online model on-the-fly, which ensures that our model is robust to pseudo label noise. We show that fine-tuning with pseudo labels achieves a 5.35% phoneme error rate reduction and 2.48% MDD F1 score improvement over a labeled-samples-only fine-tuning baseline. The proposed PL method is also shown to outperform conventional offline PL methods. Compared to the state-of-the-art MDD systems, our MDD solution produces a more accurate and consistent phonetic error diagnosis. In addition, we conduct an open test on a separate UTD-4Accents dataset, where our system recognition outputs show a strong correlation with human perception, based on accentedness and intelligibility."
   ],
   "doi": "10.21437/Interspeech.2022-11039"
  },
  "chou22_interspeech": {
   "authors": [
    [
     "Huang-Cheng",
     "Chou"
    ],
    [
     "Chi-Chun",
     "Lee"
    ],
    [
     "Carlos",
     "Busso"
    ]
   ],
   "title": "Exploiting Co-occurrence Frequency of Emotions in Perceptual Evaluations To Train A Speech Emotion Classifier",
   "original": "11041",
   "page_count": 5,
   "order": 33,
   "p1": 161,
   "pn": 165,
   "abstract": [
    "Previous studies on speech emotion recognition (SER) with categorical emotions have often formulated the task as a single-label classification problem, where the emotions are considered orthogonal to each other. However, previous studies have indicated that emotions can co-occur, especially for more ambiguous emotional sentences (e.g., a mixture of happiness and surprise). Some studies have regarded SER problems as a multi-label task, predicting multiple emotional classes. However, this formulation does not leverage the relation between emotions during training, since emotions are assumed to be independent. This study explores the idea that emotional classes are not necessarily independent and its implications on training SER models. In particular, we calculate the frequency of co-occurring emotions from perceptual evaluations in the train set to generate a matrix with class-dependent penalties, punishing more mistakes between distant emotional classes. We integrate the penalization matrix into three existing label-learning approaches (hard-label, multi-label, and distribution-label learning) using the proposed modified loss. We train SER models using the penalty loss and commonly used cost functions for SER tasks. The evaluation of our proposed penalization matrix on the MSP-Podcast corpus shows important relative improvements in macro F1-score for hard-label learning (17.12%), multi-label learning (12.79%), and distribution-label learning (25.8%)."
   ],
   "doi": "10.21437/Interspeech.2022-11041"
  },
  "kothare22_interspeech": {
   "authors": [
    [
     "Hardik",
     "Kothare"
    ],
    [
     "Michael",
     "Neumann"
    ],
    [
     "Jackson",
     "Liscombe"
    ],
    [
     "Oliver",
     "Roesler"
    ],
    [
     "William",
     "Burke"
    ],
    [
     "Andrew",
     "Exner"
    ],
    [
     "Sandy",
     "Snyder"
    ],
    [
     "Andrew",
     "Cornish"
    ],
    [
     "Doug",
     "Habberstad"
    ],
    [
     "David",
     "Pautler"
    ],
    [
     "David",
     "Suendermann-Oeft"
    ],
    [
     "Jessica",
     "Huber"
    ],
    [
     "Vikram",
     "Ramanarayanan"
    ]
   ],
   "title": "Statistical and clinical utility of multimodal dialogue-based speech and facial metrics for Parkinson's disease assessment",
   "original": "11048",
   "page_count": 5,
   "order": 740,
   "p1": 3658,
   "pn": 3662,
   "abstract": [
    "We present a framework for characterising the statistical and clinical relevance of speech and facial metrics in Parkinson's disease (PD) extracted by a multimodal conversational platform. 38 people with PD (pPD) and 22 controls were recruited in an ongoing study and were asked to complete four interactive sessions, a week apart from each other. In each session, a virtual conversational agent, Tina, guided participants through a battery of standard tasks designed to elicit speech and facial behaviours. Speech and facial metrics were automatically extracted in real time, several of which showed statistically significant differences between pPD and controls. We explored which of these differences were greater than measurement error, a threshold defined as the minimally detectable change (MDC). Furthermore, we computed the minimal clinically important difference (MCID) with respect to the Communicative Participation Item Bank short form (CPIB-S) scale for these select metrics. Our results show that differences in metrics like duration and fundamental frequency (F0) of speech are captured beyond measurement error. We also discuss several confounding factors that need to be taken into consideration before making any clinical interpretation of changes in these metrics."
   ],
   "doi": "10.21437/Interspeech.2022-11048"
  },
  "hard22_interspeech": {
   "authors": [
    [
     "Andrew",
     "Hard"
    ],
    [
     "Kurt",
     "Partridge"
    ],
    [
     "Neng",
     "Chen"
    ],
    [
     "Sean",
     "Augenstein"
    ],
    [
     "Aishanee",
     "Shah"
    ],
    [
     "Hyun Jin",
     "Park"
    ],
    [
     "Alex",
     "Park"
    ],
    [
     "Sara",
     "Ng"
    ],
    [
     "Jessica",
     "Nguyen"
    ],
    [
     "Ignacio",
     "Lopez-Moreno"
    ],
    [
     "Rajiv",
     "Mathews"
    ],
    [
     "Francoise",
     "Beaufays"
    ]
   ],
   "title": "Production federated keyword spotting via distillation, filtering, and joint federated-centralized training",
   "original": "11050",
   "page_count": 5,
   "order": 16,
   "p1": 76,
   "pn": 80,
   "abstract": [
    "We trained a keyword spotting model using federated learning on real user devices and observed significant improvements when the model was deployed for inference on phones. To compensate for data domains that are missing from on-device training caches, we employed joint federated-centralized training. And to learn in the absence of curated labels on-device, we formulated a confidence filtering strategy based on user-feedback signals for federated distillation. These techniques created models that significantly improved quality metrics in offline evaluations and user-experience metrics in live A/B experiments."
   ],
   "doi": "10.21437/Interspeech.2022-11050"
  },
  "kilgour22_interspeech": {
   "authors": [
    [
     "Kevin",
     "Kilgour"
    ],
    [
     "Beat",
     "Gfeller"
    ],
    [
     "Qingqing",
     "Huang"
    ],
    [
     "Aren",
     "Jansen"
    ],
    [
     "Scott",
     "Wisdom"
    ],
    [
     "Marco",
     "Tagliasacchi"
    ]
   ],
   "title": "Text-Driven Separation of Arbitrary Sounds",
   "original": "11052",
   "page_count": 5,
   "order": 1095,
   "p1": 5403,
   "pn": 5407,
   "abstract": [
    "We propose a method of separating a desired sound source from a single-channel mixture, based on either a textual description or a short audio sample of the target source. This is achieved by combining two distinct models. The first model, SoundFilter, is trained to jointly embed both an audio clip and its textual description to the same embedding in a shared representation. The second model,SoundFilter, takes a mixed source audio clip as an input and separates it based on a conditioning vector from the shared text-audio representation defined by SoundWords, making the model agnostic to the conditioning modality. Evaluating on multiple datasets, we show that our approach can achieve an SI-SDR of 9.1 dB for mixtures of two arbitrary sounds when conditioned on text and 10.1 dB when conditioned on audio. We also show that SoundWords is effective at learning co-embeddings and that our multi-modal training approach improves the performance of SoundFilter"
   ],
   "doi": "10.21437/Interspeech.2022-11052"
  },
  "chen22s_interspeech": {
   "authors": [
    [
     "Long",
     "Chen"
    ],
    [
     "Yixiong",
     "Meng"
    ],
    [
     "Venkatesh",
     "Ravichandran"
    ],
    [
     "Andreas",
     "Stolcke"
    ]
   ],
   "title": "Graph-based Multi-View Fusion and Local Adaptation: Mitigating Within-Household Confusability for Speaker Identification",
   "original": "11053",
   "page_count": 5,
   "order": 973,
   "p1": 4805,
   "pn": 4809,
   "abstract": [
    "Speaker identification (SID) in the household scenario (e.g., for smart speakers) is an important but challenging problem due to limited number of labeled (enrollment) utterances, confusable voices, and demographic imbalances. Conventional speaker recognition systems generalize from a large random sample of speakers, causing the recognition to underperform for households drawn from specific cohorts or otherwise exhibiting high confusability. In this work, we propose a graph-based semi-supervised learning approach to improve household-level SID accuracy and robustness with locally adapted graph normalization and multi-signal fusion with multi-view graphs. Unlike other work on household SID, fairness, and signal fusion, this work focuses on speaker label inference (scoring) and provides a simple solution to realize household-specific adaptation and multi-signal fusion without tuning the embeddings or training a fusion network. Experiments on the VoxCeleb dataset demonstrate that our approach consistently improves the performance across households with different customer cohorts and degrees of confusability."
   ],
   "doi": "10.21437/Interspeech.2022-11053"
  },
  "cho22b_interspeech": {
   "authors": [
    [
     "Yeonjin",
     "Cho"
    ],
    [
     "Sara",
     "Ng"
    ],
    [
     "Trang",
     "Tran"
    ],
    [
     "Mari",
     "Ostendorf"
    ]
   ],
   "title": "Leveraging Prosody for Punctuation Prediction of Spontaneous Speech",
   "original": "11061",
   "page_count": 5,
   "order": 112,
   "p1": 555,
   "pn": 559,
   "abstract": [
    "This paper introduces a new neural model for punctuation prediction that incorporates prosodic features to improve automatic punctuation prediction in transcriptions of spontaneous speech. We explore the benefit of intonation and energy features over simply using pauses. In addition, the work poses the question of how to represent interruption points associated with disfluencies in spontaneous speech. In experiments on the Switchboard corpus, we find that prosodic information improved punctuation prediction fidelity for both hand transcripts and ASR output. Explicit modeling of interruption points can benefit prediction of standard punctuation, particularly if the convention associates interruptions with commas."
   ],
   "doi": "10.21437/Interspeech.2022-11061"
  },
  "trinh22_interspeech": {
   "authors": [
    [
     "Viet Anh",
     "Trinh"
    ],
    [
     "Pegah",
     "Ghahremani"
    ],
    [
     "Brian",
     "King"
    ],
    [
     "Jasha",
     "Droppo"
    ],
    [
     "Andreas",
     "Stolcke"
    ],
    [
     "Roland",
     "Maas"
    ]
   ],
   "title": "Reducing Geographic Disparities in Automatic Speech Recognition via Elastic Weight Consolidation",
   "original": "11063",
   "page_count": 5,
   "order": 264,
   "p1": 1298,
   "pn": 1302,
   "abstract": [
    "We present an approach to reduce the performance disparity between geographic regions without degrading performance on the overall user population for ASR. A popular approach is to fine-tune the model with data from regions where the ASR model has a higher word error rate (WER). However, when the ASR model is adapted to get better performance on these high-WER regions, its parameters wander from the previous optimal values, which can lead to worse performance in other regions. In our proposed method, we utilize the elastic weight consolidation (EWC) regularization loss to identify directions in parameters space along which the ASR weights can vary to improve for high-error regions, while still maintaining performance on the speaker population overall. Our results demonstrate that EWC can reduce the word error rate (WER) in the region with highest WER by 3.2% relative while reducing the overall WER by 1.3% relative. We also evaluate the role of language and acoustic models in ASR fairness and propose a clustering algorithm to identify WER disparities based on geographic region."
   ],
   "doi": "10.21437/Interspeech.2022-11063"
  },
  "miao22_interspeech": {
   "authors": [
    [
     "Xiaoxiao",
     "Miao"
    ],
    [
     "Xin",
     "Wang"
    ],
    [
     "Erica",
     "Cooper"
    ],
    [
     "Junichi",
     "Yamagishi"
    ],
    [
     "Natalia",
     "Tomashenko"
    ]
   ],
   "title": "Analyzing Language-Independent Speaker Anonymization Framework under Unseen Conditions",
   "original": "11065",
   "page_count": 5,
   "order": 897,
   "p1": 4426,
   "pn": 4430,
   "abstract": [
    "In our previous work, we proposed a language-independent speaker anonymization system based on self-supervised learning models. Although the system can anonymize speech data of any language, the anonymization was imperfect, and the speech content of the anonymized speech was distorted. This limitation is more severe when the input speech is from a domain unseen in the training data. This study analyzed the bottleneck of the anonymization system under unseen conditions. It was found that the domain (e.g., language and channel) mismatch between the training and test data affected the neural waveform vocoder and anonymized speaker vectors, which limited the performance of the whole system. Increasing the training data diversity for the vocoder was found to be helpful to reduce its implicit language and channel dependency. Furthermore, a simple correlation-alignment-based domain adaption strategy was found to be significantly effective to alleviate the mismatch on the anonymized speaker vectors. Audio samples and source code are available online."
   ],
   "doi": "10.21437/Interspeech.2022-11065"
  },
  "ploujnikov22_interspeech": {
   "authors": [
    [
     "Artem",
     "Ploujnikov"
    ],
    [
     "Mirco",
     "Ravanelli"
    ]
   ],
   "title": "SoundChoice: Grapheme-to-Phoneme Models with Semantic Disambiguation",
   "original": "11066",
   "page_count": 5,
   "order": 98,
   "p1": 486,
   "pn": 490,
   "abstract": [
    "End-to-end speech synthesis models directly convert the input characters into an audio representation (e.g., spectrograms). Despite their impressive performance, such models have difficulty disambiguating the pronunciations of identically spelled words. To mitigate this issue, a separate Grapheme-to-Phoneme (G2P) model can be employed to convert the characters into phonemes before synthesizing the audio. This paper proposes SoundChoice, a novel G2P architecture that processes entire sentences rather than operating at the word level. The proposed architecture takes advantage of a weighted homograph loss (that improves disambiguation), exploits curriculum learning (that gradually switches from wordlevel to sentence-level G2P), and integrates word embeddings from BERT (for further performance improvement). Moreover, the model inherits the best practices in speech recognition, including multi-task learning with Connectionist Temporal Classification (CTC) and beam search with an embedded language model. As a result, SoundChoice achieves a Phoneme Error Rate (PER) of 2.65% on whole-sentence transcription using data from LibriSpeech and Wikipedia."
   ],
   "doi": "10.21437/Interspeech.2022-11066"
  },
  "raju22_interspeech": {
   "authors": [
    [
     "Anirudh",
     "Raju"
    ],
    [
     "Milind",
     "Rao"
    ],
    [
     "Gautam",
     "Tiwari"
    ],
    [
     "PRANAV",
     "DHERAM"
    ],
    [
     "Bryan",
     "Anderson"
    ],
    [
     "Zhe",
     "Zhang"
    ],
    [
     "Chul",
     "Lee"
    ],
    [
     "Bach",
     "Bui"
    ],
    [
     "Ariya",
     "Rastrow"
    ]
   ],
   "title": "On joint training with interfaces for spoken language understanding",
   "original": "11067",
   "page_count": 5,
   "order": 255,
   "p1": 1253,
   "pn": 1257,
   "abstract": [
    "Spoken language understanding (SLU) systems extract both text transcripts and semantics associated with intents and slots from input speech utterances. SLU systems usually consist of (1) an automatic speech recognition (ASR) module (2) an interface module that exposes relevant outputs from ASR, and (3) a natural language understanding (NLU) module. Interfaces in SLU systems carry information on text transcriptions or richer information like neural embeddings from ASR to NLU. In this paper, we study how interfaces affect joint-training for spoken language understanding. Most notably, we obtain the state-of-the-art results on the publicly available 50-hr SLURP [1] dataset. We first leverage large-size pretrained ASR and NLU models that are connected by a text interface, and then jointly train both models via a sequence loss function. For scenarios where pretrained models are not utilized, the best results are obtained through a joint sequence loss training using richer neural interfaces. Finally, we show the overall diminishing impact of leveraging pretrained models with increased training data size."
   ],
   "doi": "10.21437/Interspeech.2022-11067"
  },
  "liu22z_interspeech": {
   "authors": [
    [
     "Alexander H.",
     "Liu"
    ],
    [
     "Cheng-I",
     "Lai"
    ],
    [
     "Wei-Ning",
     "Hsu"
    ],
    [
     "Michael",
     "Auli"
    ],
    [
     "Alexei",
     "Baevski"
    ],
    [
     "James",
     "Glass"
    ]
   ],
   "title": "Simple and Effective Unsupervised Speech Synthesis",
   "original": "11071",
   "page_count": 5,
   "order": 170,
   "p1": 843,
   "pn": 847,
   "abstract": [
    "We introduce the first unsupervised speech synthesis system based on a simple, yet effective recipe. The framework leverages recent work in unsupervised speech recognition as well as existing neural-based speech synthesis. Using only unlabeled speech audio and unlabeled text as well as a lexicon, our method enables speech synthesis without the need for a human-labeled corpus. Experiments demonstrate the unsupervised system can synthesize speech similar to a supervised counterpart in terms of naturalness and intelligibility measured by human evaluation."
   ],
   "doi": "10.21437/Interspeech.2022-11071"
  },
  "bakhturina22_interspeech": {
   "authors": [
    [
     "Evelina",
     "Bakhturina"
    ],
    [
     "Yang",
     "Zhang"
    ],
    [
     "Boris",
     "Ginsburg"
    ]
   ],
   "title": "Shallow Fusion of Weighted Finite-State Transducer and Language Model for Text Normalization",
   "original": "11074",
   "page_count": 5,
   "order": 99,
   "p1": 491,
   "pn": 495,
   "abstract": [
    "Text normalization (TN) systems in production are largely rule-based using weighted finite-state transducers (WFST). However, WFST-based systems struggle with ambiguous input when the normalized form is context-dependent. On the other hand, neural text normalization systems can take context into account but they suffer from unrecoverable errors and require labeled normalization datasets, which are hard to collect. We propose a new hybrid approach that combines the benefits of rule-based and neural systems. First, a non-deterministic WFST outputs all normalization candidates, and then a neural language model picks the best one -- similar to shallow fusion for automatic speech recognition. While the WFST prevents unrecoverable errors, the language model resolves contextual ambiguity. We show for English that the approach is effective and easy to extend. It achieves comparable or better results than existing state-of-the-art TN models."
   ],
   "doi": "10.21437/Interspeech.2022-11074"
  },
  "bando22_interspeech": {
   "authors": [
    [
     "Yoshiaki",
     "Bando"
    ],
    [
     "Takahiro",
     "Aizawa"
    ],
    [
     "Katsutoshi",
     "Itoyama"
    ],
    [
     "Kazuhiro",
     "Nakadai"
    ]
   ],
   "title": "Weakly-Supervised Neural Full-Rank Spatial Covariance Analysis for a Front-End System of Distant Speech Recognition",
   "original": "11077",
   "page_count": 5,
   "order": 775,
   "p1": 3824,
   "pn": 3828,
   "abstract": [
    "This paper presents a weakly-supervised multichannel neural speech separation method for distant speech recognition (DSR) of real conversational speech mixtures. A blind source separation (BSS) method called neural full-rank spatial covariance analysis (FCA) can precisely separate multichannel speech mixtures by using a deep spectral model without any supervision. The neural FCA, however, requires that the number of sound sources is fixed and known in advance. This requirement complicates its utilization for a front-end system of DSR for multi-speaker conversations, in which the number of speakers changes dynamically. In this paper, we propose an extension of neural FCA to handle a dynamically changing number of sound sources by taking temporal voice activities of target speakers as auxiliary information. We train a source separation network in a weakly-supervised manner using a dataset of multichannel audio mixtures and their voice activities. Experimental results with the CHiME-6 dataset, whose task is to recognize conversations at dinner parties, show that our method outperformed a conventional BSS-based system in word error rates."
   ],
   "doi": "10.21437/Interspeech.2022-11077"
  },
  "nidadavolu22_interspeech": {
   "authors": [
    [
     "Phani Sankar",
     "Nidadavolu"
    ],
    [
     "Na",
     "Xu"
    ],
    [
     "Nick",
     "Jutila"
    ],
    [
     "Ravi Teja",
     "Gadde"
    ],
    [
     "Aswarth Abhilash",
     "Dara"
    ],
    [
     "Joseph",
     "Savold"
    ],
    [
     "Sapan",
     "Patel"
    ],
    [
     "Aaron",
     "Hoff"
    ],
    [
     "Veerdhawal",
     "Pande"
    ],
    [
     "Kevin",
     "Crews"
    ],
    [
     "Ankur",
     "Gandhe"
    ],
    [
     "Ariya",
     "Rastrow"
    ],
    [
     "Roland",
     "Maas"
    ]
   ],
   "title": "RefTextLAS: Reference Text Biased Listen, Attend, and Spell Model For Accurate Reading Evaluation",
   "original": "11078",
   "page_count": 5,
   "order": 881,
   "p1": 4347,
   "pn": 4351,
   "abstract": [
    "We present an automatic reading evaluator that listens to novice young readers and offers feedback based on the reading accuracy. In order to not discourage the reader, the model should not misrecognize correctly read tokens (false rejects), which may come at the expense of tolerating some reading mistakes (false accepts). To minimize the former, we explore two approaches to provide reference text – the text user is supposed to read – as context to automatic speech recognition (ASR) models: 1) a finite state transducer (FST) based error detection procedure, that restricts the grammar to tokens from reference text and an out of vocabulary (OOV) catch-all token, and 2) RefTextLAS, an attention-based end-to-end (E2E) ASR model, that takes tokens from reference text as an additional input. Our biasing approaches reduce false reject rate (FRR) by 38-56% on an in-house dataset compared to a baseline hybrid model, whose language model (LM) is trained on book texts, with 65-82% compromise on false accept rate (FAR). To reduce FAR, we present an ensemble approach that uses both baseline and RefTextLAS models to determine reading accuracy. The ensemble approach limits the relative degradation on FAR to 26.4% while providing a 42.7% improvement on FRR."
   ],
   "doi": "10.21437/Interspeech.2022-11078"
  },
  "huang22l_interspeech": {
   "authors": [
    [
     "Jinmiao",
     "Huang"
    ],
    [
     "Waseem",
     "Gharbieh"
    ],
    [
     "Qianhui",
     "Wan"
    ],
    [
     "Han Suk",
     "Shim"
    ],
    [
     "Hyun Chul",
     "Lee"
    ]
   ],
   "title": "QbyE-MLPMixer: Query-by-Example Open-Vocabulary Keyword Spotting using MLPMixer",
   "original": "11080",
   "page_count": 5,
   "order": 1052,
   "p1": 5200,
   "pn": 5204,
   "abstract": [
    "Current keyword spotting systems are typically trained with a large amount of pre-defined keywords. Recognizing keywords in an open-vocabulary setting is essential for personalizing smart device interaction. Towards this goal, we propose a pure MLP-based neural network that is based on MLPMixer - an MLP model architecture that effectively replaces the attention mechanism in Vision Transformers. We investigate different ways of adapting the MLPMixer architecture to the QbyE open-vocabulary keyword spotting task. Comparisons with the state-of-the-art RNN and CNN models show that our method achieves better performance in challenging situations (10dB and 6dB environments) on both the publicly available Hey-Snips dataset and a larger scale internal dataset with 400 speakers. Our proposed model also has a smaller number of parameters and MACs compared to the baseline models."
   ],
   "doi": "10.21437/Interspeech.2022-11080"
  },
  "vuong22_interspeech": {
   "authors": [
    [
     "Tyler",
     "Vuong"
    ],
    [
     "Richard",
     "Stern"
    ]
   ],
   "title": "Improved Modulation-Domain Loss for Neural-Network-based Speech Enhancement",
   "original": "11082",
   "page_count": 5,
   "order": 42,
   "p1": 206,
   "pn": 210,
   "abstract": [
    "We describe an improved modulation-domain loss for deeplearning- based speech enhancement systems (SE). We utilized a simple self-supervised speech reconstruction task to learn a set of spectro-temporal receptive fields (STRFs). Similar to the recently developed spectro-temporal modulation error, the learned STRFs are used to calculate a weighted mean-squared error in the modulation domain for training a speech enhancement system. Experiments show that training the SE systems using the improved modulation-domain loss consistently improves the objective prediction of speech quality and intelligibility. Additionally, we show that the SE systems improve the word error rate of a state-of-the-art automatic speech recognition system at low SNRs."
   ],
   "doi": "10.21437/Interspeech.2022-11082"
  },
  "bu22_interspeech": {
   "authors": [
    [
     "Suliang",
     "Bu"
    ],
    [
     "Yunxin",
     "Zhao"
    ],
    [
     "Tuo",
     "Zhao"
    ]
   ],
   "title": "Steering vector correction in MVDR beamformer for speech enhancement",
   "original": "11084",
   "page_count": 5,
   "order": 1108,
   "p1": 5468,
   "pn": 5472,
   "abstract": [
    "How to correct inaccurate steering vectors (SV) is an important issue for many beamformers. A SV consists of two essential components: weights and phases. In this work, we propose two novel methods to correct respectively a SV's weights and phases, under anechoic or low reverberant conditions. In an anechoic condition, the SV's weights are constant across frequency bins, and we derive an analytic solution to update weights. In a low reverberant condition, we use a constrained polynomial regression to fit SV's weights across frequency bins, which is further cast as a solvable convex optimization problem. To correct SV's phases, we exploit the linear phase relation across frequency bins in a SV of a microphone channel, and solve the optimization problem mainly by Newton's method. We have evaluated our proposed approach on simulated multi-channel noisy speech based on CHiME-3 and LibriSpeech, and obtained promising results in PESQ and STOI of MVDR enhanced speech."
   ],
   "doi": "10.21437/Interspeech.2022-11084"
  },
  "dhamyal22_interspeech": {
   "authors": [
    [
     "Hira",
     "Dhamyal"
    ],
    [
     "Bhiksha",
     "Raj"
    ],
    [
     "Rita",
     "Singh"
    ]
   ],
   "title": "Positional Encoding for Capturing Modality Specific Cadence for Emotion Detection",
   "original": "11085",
   "page_count": 5,
   "order": 34,
   "p1": 166,
   "pn": 170,
   "abstract": [
    "Emotion detection from a single modality, such as an audio or text stream, has been known to be a challenging task. While encouraging results have been obtained by using joint evidence from multiple streams, combining such evidence in optimal ways is an open challenge. In this paper, we claim that although the multi-modalities like audio, phoneme sequence ids and word sequence ids are related to each other, they also have their individual local 'cadence', which is important to be modelled for the task of emotion recognition. We model the local cadence by using separate `positional encodings' for each modality in a transformer architecture. Our results show that emotion detection based on this strategy is better than when the modality specific cadence is ignored or normalized out by using a shared positional encoding. We also find that capturing the modality interdependence is not as important as is capturing of the local cadence of individual modalities. We conduct our experiments on the IEMOCAP and CMU-MOSI datasets to demonstrate the effectiveness of the proposed methodology for combining multi-modal evidence."
   ],
   "doi": "10.21437/Interspeech.2022-11085"
  },
  "virkar22_interspeech": {
   "authors": [
    [
     "Yogesh",
     "Virkar"
    ],
    [
     "Marcello",
     "Federico"
    ],
    [
     "Robert",
     "Enyedi"
    ],
    [
     "Roberto",
     "Barra-Chicote"
    ]
   ],
   "title": "Prosodic alignment for off-screen automatic dubbing",
   "original": "11089",
   "page_count": 5,
   "order": 100,
   "p1": 496,
   "pn": 500,
   "abstract": [
    "The goal of automatic dubbing is to perform speech-to-speech translation while achieving audiovisual coherence. This entails isochrony, i.e., translating the original speech by also matching its prosodic structure into phrases and pauses, especially when the speaker's mouth is visible. In previous work, we introduced a prosodic alignment model to address isochrone or on-screen dubbing. In this work, we extend the prosodic alignment model to also address off-screen dubbing that requires less stringent synchronization constraints. We conduct experiments on four dubbing directions – English to French, Italian, German and Spanish – on a publicly available collection of TED Talks and on publicly available YouTube videos. Empirical results show that compared to our previous work the extended prosodic alignment model provides significantly better subjective viewing experience on videos in which on-screen and off-screen automatic dubbing is applied for sentences with speakers mouth visible and not visible, respectively."
   ],
   "doi": "10.21437/Interspeech.2022-11089"
  },
  "gharbieh22_interspeech": {
   "authors": [
    [
     "Waseem",
     "Gharbieh"
    ],
    [
     "Jinmiao",
     "Huang"
    ],
    [
     "Qianhui",
     "Wan"
    ],
    [
     "Han Suk",
     "Shim"
    ],
    [
     "Hyun Chul",
     "Lee"
    ]
   ],
   "title": "DyConvMixer: Dynamic Convolution Mixer Architecture for Open-Vocabulary Keyword Spotting",
   "original": "11090",
   "page_count": 5,
   "order": 1053,
   "p1": 5205,
   "pn": 5209,
   "abstract": [
    "User-defined keyword spotting research has been gaining popularity in recent years. An open-vocabulary keyword spotting system with high accuracy and low power consumption remains a challenging problem. In this paper, we propose the DyConvMixer model for tackling the problem. By leveraging dynamic convolution alongside a convolutional equivalent of the MLPMixer architecture, we obtain and efficient and effective model that has less than 200K parameters and uses less than 11M MACs. Despite the fact that our model is less than half the size of state-of-the-art RNN and CNN models, it shows competitive results on the publicly available Hey-Snips and Hey-Snapdragon datasets. In addition, we discussed the importance of designing an effective evaluation system. We detailed our evaluation pipeline for comparison with future work."
   ],
   "doi": "10.21437/Interspeech.2022-11090"
  },
  "yang22w_interspeech": {
   "authors": [
    [
     "Muqiao",
     "Yang"
    ],
    [
     "Ian",
     "Lane"
    ],
    [
     "Shinji",
     "Watanabe"
    ]
   ],
   "title": "Online Continual Learning of End-to-End Speech Recognition Models",
   "original": "11093",
   "page_count": 5,
   "order": 542,
   "p1": 2668,
   "pn": 2672,
   "abstract": [
    "Continual Learning, also known as Lifelong Learning, aims to continually learn from new data as it becomes available. While prior research on continual learning in automatic speech recognition has focused on the adaptation of models across multiple different speech recognition tasks, in this paper we proposed an experimental setting for \\textit{online continual learning} for automatic speech recognition of a single task. Specifically focusing on the case where additional training data for the same task becomes available incrementally over time, we demonstrate the effectiveness of performing incremental model updates to end-to-end speech recognition models with an online Gradient Episodic Memory (GEM) method. Moreover, we show that with online continual learning and a selective sampling strategy, we can maintain an accuracy that is similar to retraining a model from scratch while requiring significantly lower computation costs. We have also verified our method with self-supervised learning (SSL) features."
   ],
   "doi": "10.21437/Interspeech.2022-11093"
  },
  "sadhu22_interspeech": {
   "authors": [
    [
     "Samik",
     "Sadhu"
    ],
    [
     "Hynek",
     "Hermansky"
    ]
   ],
   "title": "Complex Frequency Domain Linear Prediction: A Tool to Compute Modulation Spectrum of Speech",
   "original": "11095",
   "page_count": 5,
   "order": 650,
   "p1": 3208,
   "pn": 3212,
   "abstract": [
    "Conventional Frequency Domain Linear Prediction (FDLP) technique models the squared Hilbert envelope of speech with varied degrees of approximation which can be sampled at the required frame rate and used as features for Automatic Speech Recognition (ASR). Although previously the complex cepstrum of the conventional FDLP model has been used as compact frame-wise speech features, it has lacked interpretability in the context of the Hilbert envelope. In this paper, we propose a modification of the conventional FDLP model that allows easy interpretability of the complex cepstrum as temporal modulations in an all-pole model approximation of the power of the speech signal."
   ],
   "doi": "10.21437/Interspeech.2022-11095"
  },
  "shao22c_interspeech": {
   "authors": [
    [
     "Yiwen",
     "Shao"
    ],
    [
     "Jesus",
     "Villalba"
    ],
    [
     "Sonal",
     "Joshi"
    ],
    [
     "Saurabh",
     "Kataria"
    ],
    [
     "Sanjeev",
     "Khudanpur"
    ],
    [
     "Najim",
     "Dehak"
    ]
   ],
   "title": "Chunking Defense for Adversarial Attacks on ASR",
   "original": "11096",
   "page_count": 5,
   "order": 1021,
   "p1": 5045,
   "pn": 5049,
   "abstract": [
    "While deep learning has lead to dramatic improvements in automatic speech recognition (ASR) systems in the past few years, it has also made them vulnerable to adversarial attacks. These attacks may be designed to either make ASR fail in producing the correct transcription or worse, output an adversary-chosen sentence. In this work, we propose a defense based on independently processing random or fixed size chunks of the speech input in the hope of \"containing” the cumulative effect of the adversarial perturbations. This approach does not require any additional training of the ASR system, or any defensive preprocessing of the input. It can be easily applied to any ASR systems with little loss in performance under benign conditions, while improving adversarial robustness. We perform experiments on the Librispeech data set with different adversarial attack budgets, and show that the proposed defense achieves consistent improvement on two different ASR systems/models"
   ],
   "doi": "10.21437/Interspeech.2022-11096"
  },
  "seneviratne22_interspeech": {
   "authors": [
    [
     "Nadee",
     "Seneviratne"
    ],
    [
     "Carol",
     "Espy-Wilson"
    ]
   ],
   "title": "Multimodal Depression Severity Score Prediction Using Articulatory Coordination Features and Hierarchical Attention Based Text Embeddings",
   "original": "11099",
   "page_count": 5,
   "order": 679,
   "p1": 3353,
   "pn": 3357,
   "abstract": [
    "Multimodal approaches to predict depression severity is a highly researched problem. We present a multimodal depression severity score prediction system that uses articulatory coordination features (ACFs) derived from vocal tract variables (TVs) and text transcriptions obtained from an automatic speech recognition tool that yields improvements of the root mean squared errors compared to unimodal classifiers (14.8% and 11% for audio and text, respectively). A multi-stage convolutional recurrent neural network was trained using a staircase regression (ST-R) approach with the TV based ACFs. The ST-R approach helps to better capture the quasi-numerical nature of the depression severity scores. A text model is trained using the Hierarchical Attention Network (HAN) architecture. The multimodal system is developed by combining embeddings from the session-level audio model and the HAN text model with a session-level auxiliary feature vector containing timing measures of the speech signal. We also show that this model tracks the severity of depression for most of the subjects reasonably well and we analyze the underlying reasons for the cases with significant deviations of the predictions from the ground-truth score."
   ],
   "doi": "10.21437/Interspeech.2022-11099"
  },
  "patterson22_interspeech": {
   "authors": [
    [
     "Katharine",
     "Patterson"
    ],
    [
     "Kevin",
     "Wilson"
    ],
    [
     "Scott",
     "Wisdom"
    ],
    [
     "John R.",
     "Hershey"
    ]
   ],
   "title": "Distance-Based Sound Separation",
   "original": "11100",
   "page_count": 5,
   "order": 184,
   "p1": 901,
   "pn": 905,
   "abstract": [
    "We propose the novel task of distance-based sound separation, where sounds are separated based only on their distance from a single microphone. In the context of assisted listening devices, proximity provides a simple criterion for sound selection in noisy environments that would allow the user to focus on sounds relevant to a local conversation. We demonstrate the feasibility of this approach by training a neural network to separate near sounds from far sounds in single channel synthetic reverberant mixtures, relative to a threshold distance defining the boundary between near and far. With a single nearby speaker and four distant speakers, the model improves scale-invariant signal to noise ratio by 4.4 dB for near sounds and 6.8 dB for far sounds."
   ],
   "doi": "10.21437/Interspeech.2022-11100"
  },
  "lee22p_interspeech": {
   "authors": [
    [
     "Yeonghyeon",
     "Lee"
    ],
    [
     "Kangwook",
     "Jang"
    ],
    [
     "Jahyun",
     "Goo"
    ],
    [
     "Youngmoon",
     "Jung"
    ],
    [
     "Hoi Rin",
     "Kim"
    ]
   ],
   "title": "FitHuBERT: Going Thinner and Deeper for Knowledge Distillation of Speech Self-Supervised Models",
   "original": "11112",
   "page_count": 5,
   "order": 726,
   "p1": 3588,
   "pn": 3592,
   "abstract": [
    "Large-scale speech self-supervised learning (SSL) has emerged to the main field of speech processing, however, the problem of computational cost arising from its vast size makes a high entry barrier to academia. In addition, existing distillation techniques of speech SSL models compress the model by reducing layers, which induces performance degradation in linguistic pattern recognition tasks such as phoneme recognition (PR). In this paper, we propose FitHuBERT, which makes thinner in dimension throughout almost all model components and deeper in layer compared to prior speech SSL distillation works. Moreover, we employ a time-reduction layer to speed up inference time and propose a method of hint-based distillation for less performance degradation. Our method reduces the model to 23.8% in size and 35.9% in inference time compared to HuBERT. Also, we achieve 12.1% word error rate and 13.3% phoneme error rate on the SUPERB benchmark which is superior than prior work."
   ],
   "doi": "10.21437/Interspeech.2022-11112"
  },
  "mei22_interspeech": {
   "authors": [
    [
     "Xinhao",
     "Mei"
    ],
    [
     "Xubo",
     "Liu"
    ],
    [
     "Jianyuan",
     "Sun"
    ],
    [
     "Mark",
     "Plumbley"
    ],
    [
     "Wenwu",
     "Wang"
    ]
   ],
   "title": "On Metric Learning for Audio-Text Cross-Modal Retrieval",
   "original": "11115",
   "page_count": 5,
   "order": 840,
   "p1": 4142,
   "pn": 4146,
   "abstract": [
    "Audio-text retrieval aims at retrieving a target audio clip or caption from a pool of candidates given a query in another modality. Solving such cross-modal retrieval task is challenging because it not only requires learning robust feature representations for both modalities, but also requires capturing the fine-grained alignment between these two modalities. Existing cross-modal retrieval models are mostly optimized by metric learning objectives as both of them attempt to map data to an embedding space, where similar data are close together and dissimilar data are far apart. Unlike other cross-modal retrieval tasks such as image-text and video-text retrievals, audio-text retrieval is still an unexplored task. In this work, we aim to study the impact of different metric learning objectives on the audio-text retrieval task. We present an extensive evaluation of popular metric learning objectives on the AudioCaps and Clotho datasets. We demonstrate that NT-Xent loss adapted from self-supervised learning shows stable performance across different datasets and training settings, and outperforms the popular triplet-based losses. Our code is available at https://github.com/XinhaoMei/audio-text_retrieval."
   ],
   "doi": "10.21437/Interspeech.2022-11115"
  },
  "udagawa22b_interspeech": {
   "authors": [
    [
     "Takuma",
     "Udagawa"
    ],
    [
     "Masayuki",
     "Suzuki"
    ],
    [
     "Gakuto",
     "Kurata"
    ],
    [
     "Nobuyasu",
     "Itoh"
    ],
    [
     "George",
     "Saon"
    ]
   ],
   "title": "Effect and Analysis of Large-scale Language Model Rescoring on Competitive ASR Systems",
   "original": "11123",
   "page_count": 5,
   "order": 794,
   "p1": 3919,
   "pn": 3923,
   "abstract": [
    "Large-scale language models (LLMs) such as GPT-2, BERT and RoBERTa have been successfully applied to ASR N-best rescoring. However, whether or how they can benefit competitive, near state-of-the-art ASR systems remains unexplored. In this study, we incorporate LLM rescoring into one of the most competitive ASR baselines: the Conformer-Transducer model. We demonstrate that consistent improvement is achieved by the LLM's bidirectionality, pretraining, in-domain finetuning and context augmentation. Furthermore, our lexical analysis sheds light on how each of these components may be contributing to the ASR performance."
   ],
   "doi": "10.21437/Interspeech.2022-11123"
  },
  "chung22_interspeech": {
   "authors": [
    [
     "Raymond",
     "Chung"
    ],
    [
     "Brian",
     "Mak"
    ]
   ],
   "title": "Synthesizing Near Native-accented Speech for a Non-native Speaker by Imitating the Pronunciation and Prosody of a Native Speaker",
   "original": "11124",
   "page_count": 5,
   "order": 872,
   "p1": 4302,
   "pn": 4306,
   "abstract": [
    "This paper investigates how to reduce foreign accent in the synthesis of native (L1) speech for a non-native (L2) speaker. We focus on two major aspects of foreign accents: mispronunciations and improper prosody (rhythm, phonemes duration, and pauses). Firstly, to reduce mispronunciations, the mel-spectrograms generated by an L2 text-to-speech (TTS) model are fed to a pre-trained speech recognizer and the mispronunciation information is fed back to the TTS model during back-propagation to help the model learn correct native mel-spectrograms. Secondly, to imitate L1 speech prosody, a recent data augmentation (DA) technique originally proposed for speaking style transfer is applied to transfer L1 speaking style to L2 speakers. The DA technique creates additional L2 speeches when L2 speakers try to imitate L1 speeches. Automatic speech recognition on native-accented speeches synthesized from non-native speakers by the proposed method gives a lower word error rate. The speaker embeddings produced by a pre-trained speaker verifier from the original L2 speakers' speech and their synthesized speech are highly similar. Finally, subjective MOS scores on the synthesized speech show that they have good quality and reduced accentedness."
   ],
   "doi": "10.21437/Interspeech.2022-11124"
  },
  "carne22_interspeech": {
   "authors": [
    [
     "Michael",
     "Carne"
    ],
    [
     "Yuko",
     "Kinoshita"
    ],
    [
     "Shunichi",
     "Ishihara"
    ]
   ],
   "title": "High level feature fusion in forensic voice comparison",
   "original": "11127",
   "page_count": 5,
   "order": 1073,
   "p1": 5293,
   "pn": 5297,
   "abstract": [
    "Feature robustness is particularly important in forensic applications of speaker recognition, where there are often significant differences in the recording conditions between forensic samples. For this reason, high level features have previously been recommended for use in forensic systems, since they tend to be more robust to the acoustic variability introduced by recording conditions. A drawback of high level features though is their poor performance relative to low-level cepstral features. We suggest, however, it may be possible to improve the performance of high feature systems by combining acoustic and idiolectal information, and this may deliver a better trade-off with respect to robustness, interpretability and discrimination performance. This paper describes a novel approach to a likelihood ratio-based (LR) forensic voice comparison (FVC): fusing word n-grams with long-term fundamental frequency (LTF0). Preliminary experiments demonstrate some promising performance gains. We also examine how the duration of speech data impacts on this proposed system."
   ],
   "doi": "10.21437/Interspeech.2022-11127"
  },
  "fan22d_interspeech": {
   "authors": [
    [
     "Ruchao",
     "Fan"
    ],
    [
     "Abeer",
     "Alwan"
    ]
   ],
   "title": "DRAFT: A Novel Framework to Reduce Domain Shifting in Self-supervised Learning and Its Application to Children’s ASR",
   "original": "11128",
   "page_count": 5,
   "order": 992,
   "p1": 4900,
   "pn": 4904,
   "abstract": [
    "Self-supervised learning (SSL) in the pretraining stage using un-annotated speech data has been successful in low-resource automatic speech recognition (ASR) tasks. However, models trained through SSL are biased to the pretraining data which is usually different from the data used in finetuning tasks, causing a domain shifting problem, and thus resulting in limited knowledge transfer. We propose a novel framework, domain responsible adaptation and finetuning (DRAFT), to reduce domain shifting in pretrained speech models through an additional adaptation stage. In DRAFT, residual adapters (RAs) are inserted in the pretrained model to learn domain-related information with the same SSL loss as the pretraining stage. Only RA parameters are updated during the adaptation stage. DRAFT is agnostic to the type of SSL method used and is evaluated with three widely used approaches: APC, Wav2vec2.0, and HuBERT. On two child ASR tasks (OGI and MyST databases), using SSL models trained with un-annotated adult speech data (Librispeech), relative WER improvements of up to 19.7% are observed when compared to the pretrained models without adaptation. Additional experiments examined the potential of cross knowledge transfer between the two datasets and the results are promising, showing a broader usage of the proposed DRAFT framework."
   ],
   "doi": "10.21437/Interspeech.2022-11128"
  },
  "lamichhane22_interspeech": {
   "authors": [
    [
     "Bishal",
     "Lamichhane"
    ],
    [
     "Nidal",
     "Moukaddam"
    ],
    [
     "Ankit B.",
     "Patel"
    ],
    [
     "Ashutosh",
     "Sabharwal"
    ]
   ],
   "title": "Dyadic Interaction Assessment from Free-living Audio for Depression Severity Assessment",
   "original": "11129",
   "page_count": 5,
   "order": 507,
   "p1": 2493,
   "pn": 2497,
   "abstract": [
    "Psychomotor retardation in depression has been associated with speech timing changes from dyadic clinical interviews. In this work, we investigate speech timing features from free-living dyadic interactions. Apart from the possibility of continuous monitoring to complement clinical visits, a study in free-living conditions would also allow inferring sociability features such as dyadic interaction frequency implicated in depression. We adapted a speaker count estimator as a dyadic interaction detector with a specificity of 89.5% and a sensitivity of 86.1% in the DIHARD dataset. Using the detector, we obtained speech timing features from the detected dyadic interactions in multi-day audio recordings of 32 participants comprised of 13 healthy individuals, 11 individuals with depression, and 8 individuals with psychotic disorders. The dyadic interaction frequency increased with depression severity in participants with no or mild depression, indicating a potential diagnostic marker of depression onset. However, the dyadic interaction frequency decreased with increasing depression severity for participants with moderate or severe depression. In terms of speech timing features, the response time had a significant positive correlation with depression severity. Our work shows the potential of dyadic interaction analysis from audio recordings of free-living to obtain markers of depression severity."
   ],
   "doi": "10.21437/Interspeech.2022-11129"
  },
  "yoneyama22_interspeech": {
   "authors": [
    [
     "Reo",
     "Yoneyama"
    ],
    [
     "Yi-Chiao",
     "Wu"
    ],
    [
     "Tomoki",
     "Toda"
    ]
   ],
   "title": "Unified Source-Filter GAN with Harmonic-plus-Noise Source Excitation Generation",
   "original": "11130",
   "page_count": 5,
   "order": 171,
   "p1": 848,
   "pn": 852,
   "abstract": [
    "This paper introduces a unified source-filter network with a harmonic-plus-noise source excitation generation mechanism. In our previous work, we proposed unified Source-Filter GAN (uSFGAN) for developing a high-fidelity neural vocoder with flexible voice controllability using a unified source-filter neural network architecture. However, the capability of uSFGAN to model the aperiodic source excitation signal is insufficient, and there is still a gap in sound quality between the natural and generated speech. To improve the source excitation modeling and generated sound quality, a new source excitation generation network separately generating periodic and aperiodic components is proposed. The advanced adversarial training procedure of HiFiGAN is also adopted to replace that of Parallel WaveGAN used in the original uSFGAN. Both objective and subjective evaluation results show that the modified uSFGAN significantly improves the sound quality of the basic uSFGAN while maintaining the voice controllability."
   ],
   "doi": "10.21437/Interspeech.2022-11130"
  },
  "saba22_interspeech": {
   "authors": [
    [
     "Juliana N.",
     "Saba"
    ],
    [
     "John H.L.",
     "Hansen"
    ]
   ],
   "title": "Speech Modification for Intelligibility in Cochlear Implant Listeners: Individual Effects of Vowel- and Consonant-Boosting",
   "original": "11131",
   "page_count": 5,
   "order": 1109,
   "p1": 5473,
   "pn": 5477,
   "abstract": [
    "Previous research has demonstrated techniques to improve automatic speech recognition and speech-in-noise intelligibility for normal hearing (NH) and cochlear implant (CI) listeners by synthesizing Lombard Effect (LE) speech. In this study, we emulate and evaluate segment-specific modifications based on speech production characteristics observed in natural LE speech in order to improve intelligibility for CI listeners. Two speech processing approaches were designed to modify representation of vowels, consonants, and the combination using amplitude-based compression techniques in the \"electric domain”–referring to the stimulation sequence delivered to the intracochlear electrode array that corresponds to the acoustic signal. Performance with CI listeners resulted in no significant difference using consonant-boosting and consonant- and vowel-boosting strategies with better representation of mid-frequency and high-frequency content corresponding to both formant and consonant structure, respectively. Spectral smearing and decreased amplitude variation were also observed which may have negatively impacted intelligibility. Segmental perturbations using a weighted logarithmic and sigmoid compression functions in this study demonstrated the ability to improve representation of frequency content but disrupted amplitude-based cues, regardless of comparable speech intelligibility. While there are an infinite number of acoustic domain modifications characterizing LE speech, this study demonstrates a basic framework for emulating segmental differences in the electric domain."
   ],
   "doi": "10.21437/Interspeech.2022-11131"
  },
  "yoon22b_interspeech": {
   "authors": [
    [
     "Hyun-Wook",
     "Yoon"
    ],
    [
     "Ohsung",
     "Kwon"
    ],
    [
     "Hoyeon",
     "Lee"
    ],
    [
     "Ryuichi",
     "Yamamoto"
    ],
    [
     "Eunwoo",
     "Song"
    ],
    [
     "Jae-Min",
     "Kim"
    ],
    [
     "Min-Jae",
     "Hwang"
    ]
   ],
   "title": "Language Model-Based Emotion Prediction Methods for Emotional Speech Synthesis Systems",
   "original": "11133",
   "page_count": 5,
   "order": 931,
   "p1": 4596,
   "pn": 4600,
   "abstract": [
    "This paper proposes an effective emotional text-to-speech (TTS) system with a pre-trained language model (LM)-based emotion prediction method. Unlike conventional systems that require auxiliary inputs such as manually defined emotion classes, our system directly estimates emotion-related attributes from the input text. Specifically, we utilize generative pre-trained transformer (GPT)-3 to jointly predict both an emotion class and its strength in representing emotions' coarse and fine properties, respectively. Then, these attributes are combined in the emotional embedding space and used as conditional features of the TTS model for generating output speech signals. Consequently, the proposed system can produce emotional speech only from text without any auxiliary inputs. Furthermore, because the GPT-3 enables to capture emotional context among the consecutive sentences, the proposed method can effectively handle the paragraph-level generation of emotional speech."
   ],
   "doi": "10.21437/Interspeech.2022-11133"
  },
  "saraf22_interspeech": {
   "authors": [
    [
     "Amruta",
     "Saraf"
    ],
    [
     "Ganesh",
     "Sivaraman"
    ],
    [
     "Elie",
     "Khoury"
    ]
   ],
   "title": "Confidence Measure for Automatic Age Estimation From Speech",
   "original": "11135",
   "page_count": 5,
   "order": 415,
   "p1": 2033,
   "pn": 2037,
   "abstract": [
    "Age estimation from speech is a problem that has wide applications in call centers, virtual assistants and IoT devices. The estimated age is used for various system decisions related to personalization, parental control, and anomaly detection. Performance of speech based automatic age estimation systems is generally stated in the literature using the Mean Absolute Error (MAE) and Pearson Correlation Coefficient (PC). In real-world applications of these systems, MAE and PC provide little insight into the confidence of a point estimate. An MAE of 5 years on a test set provides only the average error across all estimates in the test set and hence does not provide any information about the confidence of each individual estimate. A confidence score of predicted age is essential to know the trustworthiness of the predictions made by the system. This paper formulates age estimation from speech as a label distribution learning problem to come up with a measure of the confidence related to the point estimate being within a desired range from the ground-truth. It further uses it to analyze the age estimation system under conditions of varying speech quality. We show that the proposed measure of confidence is better than a fixed error-margin."
   ],
   "doi": "10.21437/Interspeech.2022-11135"
  },
  "tam22_interspeech": {
   "authors": [
    [
     "Derek",
     "Tam"
    ],
    [
     "Surafel M.",
     "Lakew"
    ],
    [
     "Yogesh",
     "Virkar"
    ],
    [
     "Prashant",
     "Mathur"
    ],
    [
     "Marcello",
     "Federico"
    ]
   ],
   "title": "Isochrony-Aware Neural Machine Translation for Automatic Dubbing",
   "original": "11136",
   "page_count": 5,
   "order": 360,
   "p1": 1776,
   "pn": 1780,
   "abstract": [
    "We introduce the task of isochrony-aware machine translation which aims at generating translations suitable for dubbing. Dubbing of a spoken sentence requires transferring the content as well as the speech-pause structure of the source into the target language to achieve audiovisual coherence. Practically, this implies correctly projecting pauses from the source to the target and ensuring that target speech segments have roughly the same duration of the corresponding source speech segments. In this work, we propose implicit and explicit modeling approaches to integrate isochrony information into neural machine translation. Experiments on EnglishGerman/French language pairs with automatic metrics show that the simplest of the considered approaches works best. Results are confirmed by human evaluations of translations and dubbed videos."
   ],
   "doi": "10.21437/Interspeech.2022-11136"
  },
  "yamamoto22_interspeech": {
   "authors": [
    [
     "Yuya",
     "Yamamoto"
    ],
    [
     "Juhan",
     "Nam"
    ],
    [
     "Hiroko",
     "Terasawa"
    ]
   ],
   "title": "Deformable CNN and Imbalance-Aware Feature Learning for Singing Technique Classification",
   "original": "11137",
   "page_count": 5,
   "order": 564,
   "p1": 2778,
   "pn": 2782,
   "abstract": [
    "Singing techniques are used for expressive vocal performances by employing temporal fluctuations of the timbre, the pitch, and other components of the voice. Their classification is a challenging task, because of mainly two factors: 1) the fluctuations in singing techniques have a wide variety and are affected by many factors and 2) existing datasets are imbalanced. To deal with these problems, we developed a novel audio feature learning method based on deformable convolution with decoupled training of the feature extractor and the classifier using a class-weighted loss function. The experimental results show the following: 1) the deformable convolution improves the classification results, particularly when it is applied to the last two convolutional layers, and 2) both re-training the classifier and weighting the cross-entropy loss function by a smoothed inverse frequency enhance the classification performance."
   ],
   "doi": "10.21437/Interspeech.2022-11137"
  },
  "sustek22_interspeech": {
   "authors": [
    [
     "Martin",
     "Sustek"
    ],
    [
     "Samik",
     "Sadhu"
    ],
    [
     "Hynek",
     "Hermansky"
    ]
   ],
   "title": "Dealing with Unknowns in Continual Learning for End-to-end Automatic Speech Recognition",
   "original": "11139",
   "page_count": 5,
   "order": 213,
   "p1": 1046,
   "pn": 1050,
   "abstract": [
    "Learning continually from data is a task executed effortlessly by humans but remains to be of significant challenge for machines. Moreover, when encountering unknown test scenarios machines fail to generalize. We propose a mathematically motivated dynamically expanding end-to-end model of independent sequence-to-sequence components trained on different data sets that avoid catastrophically forgetting knowledge acquired from previously seen data while seamlessly integrating knowledge from new data. During inference, the likelihoods of the unknown test scenario are computed using internal model activation distributions. The inference made by each independent component is weighted by the normalized likelihood values to obtain the final decision."
   ],
   "doi": "10.21437/Interspeech.2022-11139"
  },
  "omalley22_interspeech": {
   "authors": [
    [
     "Thomas R.",
     "O'Malley"
    ],
    [
     "Arun",
     "Narayanan"
    ],
    [
     "Quan",
     "Wang"
    ]
   ],
   "title": "A universally-deployable ASR frontend for joint acoustic echo cancellation, speech enhancement, and voice separation",
   "original": "11140",
   "page_count": 5,
   "order": 776,
   "p1": 3829,
   "pn": 3833,
   "abstract": [
    "Recent work has shown that it is possible to train a single model to perform joint acoustic echo cancellation (AEC), speech enhancement, and voice separation, thereby serving as a unified frontend for robust automatic speech recognition (ASR). The joint model uses contextual information, such as a reference of the playback audio, noise context, and speaker embedding. In this work, we propose a number of novel improvements to such a model. First, we improve the architecture of the CrossAttention Conformer that is used to ingest noise context into the model. Second, we generalize the model to be able to handle varying lengths of noise context. Third, we propose Signal Dropout, a novel strategy that models missing contextual information. In the absence of one or more signals, the proposed model performs nearly as well as task-specific models trained without these signals; and when such signals are present, our system compares well against systems that require all context signals. Over the baseline, the final model retains a relative word error rate reduction of 25.0% on background speech when speaker embedding is absent, and 61.2% on AEC when device playback is absent."
   ],
   "doi": "10.21437/Interspeech.2022-11140"
  },
  "cho22c_interspeech": {
   "authors": [
    [
     "Jaejin",
     "Cho"
    ],
    [
     "Raghavendra",
     "Pappagari"
    ],
    [
     "Piotr",
     "Żelasko"
    ],
    [
     "Laureano Moro",
     "Velazquez"
    ],
    [
     "Jesus",
     "Villalba"
    ],
    [
     "Najim",
     "Dehak"
    ]
   ],
   "title": "Non-contrastive self-supervised learning of utterance-level speech representations",
   "original": "11141",
   "page_count": 5,
   "order": 817,
   "p1": 4028,
   "pn": 4032,
   "abstract": [
    "Considering the abundance of unlabeled speech data and the high labeling costs, unsupervised learning methods can be essential for better system development. One of the most successful methods is contrastive self-supervised methods, which require negative sampling: sampling alternative samples to contrast with the current sample (anchor). However, it is hard to ensure if all the negative samples belong to classes different from the anchor class without labels. This paper applies a non-contrastive self-supervised learning method on an unlabeled speech corpus to learn utterance-level embeddings. We used DIstillation with NO labels (DINO), proposed in computer vision, and adapted it to the speech domain. Unlike the contrastive methods, DINO does not require negative sampling. These embeddings were evaluated on speaker verification and emotion recognition. In speaker verification, the unsupervised DINO embedding with cosine scoring provided 4.38% EER on the VoxCeleb1 test trial. This outperforms the best contrastive self-supervised method by 40% relative in EER. An iterative pseudolabeling training pipeline, not requiring speaker labels, further improved the EER to 1.89%. In emotion recognition, the DINO embedding performed 60.87, 79.21, and 56.98% in micro-f1 score on IEMOCAP, Crema-D, and MSP-Podcast, respectively. The results imply the generality of the DINO embedding to different speech applications."
   ],
   "doi": "10.21437/Interspeech.2022-11141"
  },
  "kim22p_interspeech": {
   "authors": [
    [
     "Suyoun",
     "Kim"
    ],
    [
     "Duc",
     "Le"
    ],
    [
     "Weiyi",
     "Zheng"
    ],
    [
     "Tarun",
     "Singh"
    ],
    [
     "Abhinav",
     "Arora"
    ],
    [
     "Xiaoyu",
     "Zhai"
    ],
    [
     "Christian",
     "Fuegen"
    ],
    [
     "Ozlem",
     "Kalinli"
    ],
    [
     "Michael",
     "Seltzer"
    ]
   ],
   "title": "Evaluating User Perception of Speech Recognition System Quality with Semantic Distance Metric",
   "original": "11144",
   "page_count": 5,
   "order": 807,
   "p1": 3978,
   "pn": 3982,
   "abstract": [
    "Measuring automatic speech recognition (ASR) system quality is critical for creating user-satisfying voice-driven applications. Word Error Rate (WER) has been traditionally used to evaluate ASR system quality; however, it sometimes correlates poorly with user perception/judgement of transcription quality. This is because WER weighs every word equally and does not consider semantic correctness which has a higher impact on user perception. In this work, we propose evaluating ASR output hypotheses quality with SemDist that can measure semantic correctness by using the distance between the semantic vectors of the reference and hypothesis extracted from a pre-trained language model. Our experimental results of 71K and 36K user annotated ASR output quality show that SemDist achieves higher correlation with user perception than WER. We also show that SemDist has higher correlation with downstream Natural Language Understanding (NLU) tasks than WER."
   ],
   "doi": "10.21437/Interspeech.2022-11144"
  },
  "kaneko22_interspeech": {
   "authors": [
    [
     "Takuhiro",
     "Kaneko"
    ],
    [
     "Hirokazu",
     "Kameoka"
    ],
    [
     "Kou",
     "Tanaka"
    ],
    [
     "Shogo",
     "Seki"
    ]
   ],
   "title": "MISRNet: Lightweight Neural Vocoder Using Multi-Input Single Shared Residual Blocks",
   "original": "11152",
   "page_count": 5,
   "order": 331,
   "p1": 1631,
   "pn": 1635,
   "abstract": [
    "Neural vocoders have recently become popular in text-to-speech synthesis and voice conversion, increasing the demand for efficient neural vocoders. One successful approach is HiFi-GAN, which archives high-fidelity audio synthesis using a relatively small model. This characteristic is obtained using a generator incorporating multi-receptive field fusion (MRF) with multiple branches of residual blocks, allowing the expansion of the description capacity with few-channel convolutions. However, MRF requires the model size to increase with the number of branches. Alternatively, we propose a network called MISRNet, which incorporates a novel module called multi-input single shared residual block (MISR). MISR enlarges the description capacity by enriching the input variation using lightweight convolutions with a kernel size of 1 and, alternatively, reduces the variation of residual blocks from multiple to single. Because the model size of the input convolutions is significantly smaller than that of the residual blocks, MISR reduces the model size compared with that of MRF. Furthermore, we introduce an implementation technique for MISR, where we accelerate the processing speed by adopting tensor reshaping. We experimentally applied our ideas to lightweight variants of HiFi-GAN and iSTFTNet, making the models more lightweight with comparable speech quality and without compromising speed."
   ],
   "doi": "10.21437/Interspeech.2022-11152"
  },
  "markovic22_interspeech": {
   "authors": [
    [
     "Dejan",
     "Markovic"
    ],
    [
     "Alexandre",
     "Defossez"
    ],
    [
     "Alexander",
     "Richard"
    ]
   ],
   "title": "Implicit Neural Spatial Filtering for Multichannel Source Separation in the Waveform Domain",
   "original": "11153",
   "page_count": 5,
   "order": 366,
   "p1": 1806,
   "pn": 1810,
   "abstract": [
    "We present a single-stage casual waveform-to-waveform multichannel model that can separate moving sound sources based on their broad spatial locations in a dynamic acoustic scene. We divide the scene into two spatial regions containing, respectively, the target and the interfering sound sources. The model is trained end-to-end and performs spatial processing implicitly, without any components based on traditional processing or use of hand-crafted spatial features. We evaluate the proposed model on a real-world dataset and show that the model matches the performance of an oracle beamformer followed by a state-of-the-art single-channel enhancement network."
   ],
   "doi": "10.21437/Interspeech.2022-11153"
  },
  "zheng22e_interspeech": {
   "authors": [
    [
     "Nianzu",
     "Zheng"
    ],
    [
     "Liqun",
     "Deng"
    ],
    [
     "Wenyong",
     "Huang"
    ],
    [
     "Yu Ting",
     "Yeung"
    ],
    [
     "Baohua",
     "Xu"
    ],
    [
     "Yuanyuan",
     "Guo"
    ],
    [
     "Yasheng",
     "Wang"
    ],
    [
     "Xiao",
     "Chen"
    ],
    [
     "Xin",
     "Jiang"
    ],
    [
     "Qun",
     "Liu"
    ]
   ],
   "title": "CoCA-MDD: A Coupled Cross-Attention based Framework for Streaming Mispronunciation Detection and Diagnosis",
   "original": "11155",
   "page_count": 5,
   "order": 882,
   "p1": 4352,
   "pn": 4356,
   "abstract": [
    "Mispronunciation detection and diagnosis (MDD) is a popular research focus in computer-aided pronunciation training (CAPT) systems. End-to-end (e2e) approaches are becoming dominant in MDD. However an e2e MDD model usually requires entire speech utterances as input context, which leads to significant time latency especially for long paragraphs. We propose a streaming e2e MDD model called CoCA-MDD. We utilize conv-transformer structure to encode input speech in a streaming manner. A coupled cross-attention (CoCA) mechanism is proposed to integrate frame-level acoustic features with encoded reference linguistic features. CoCA also enables our model to perform mispronunciation classification with whole utterances. The proposed model allows system fusion between the streaming output and mispronunciation classification output for further performance enhancement. We evaluate CoCA-MDD on publicly available corpora. CoCA-MDD achieves F1 scores of 57.03% and 60.78% for streaming and fusion modes respectively on L2-ARCTIC. For phone-level pronunciation scoring, CoCA-MDD achieves 0.58 Pearson correlation coefficient (PCC) value on SpeechOcean762."
   ],
   "doi": "10.21437/Interspeech.2022-11155"
  },
  "zhang22ca_interspeech": {
   "authors": [
    [
     "Yixuan",
     "Zhang"
    ],
    [
     "Heming",
     "Wang"
    ],
    [
     "DeLiang",
     "Wang"
    ]
   ],
   "title": "Densely-connected Convolutional Recurrent Network for Fundamental Frequency Estimation in Noisy Speech",
   "original": "11156",
   "page_count": 5,
   "order": 81,
   "p1": 401,
   "pn": 405,
   "abstract": [
    "Estimating fundamental frequency (F0) from an audio signal is a necessary step in many tasks such as speech synthesis and speech analysis. Although high estimation accuracy has been achieved for clean speech, it is still challenging for F0 estimation to handle noisy speech, mainly because of the corruption of harmonic structure caused by noise. In this paper, we view F0 estimation as a multi-class classification problem and train a frequency-domain densely-connected convolutional neural network (DC-CRN) to estimate F0 from noisy speech. The proposed model significantly outperforms baseline methods in terms of detection rate. We find that using complex short-time Fourier transform (STFT) as input produces better performance compared to using magnitude STFT as input. Furthermore, we explore improving F0 estimation with speech enhancement. Although the F0 estimation model trained on clean speech performs well on enhanced speech, the distortion introduced by the speech enhancement model limits the estimation performance. We propose a cascade model which consists of two modules that optimize enhanced speech and estimated F0 in turn. Experimental results show that the cascade model brings further improvements to the DC-CRN model, especially in low signal-to-noise ratio (SNR) conditions."
   ],
   "doi": "10.21437/Interspeech.2022-11156"
  },
  "reddy22b_interspeech": {
   "authors": [
    [
     "Neha",
     "Reddy"
    ],
    [
     "Yoonjeong",
     "Lee"
    ],
    [
     "Zhaoyan",
     "Zhang"
    ],
    [
     "Dinesh K.",
     "Chhetri"
    ]
   ],
   "title": "Optimal thyroplasty implant shape and stiffness for treatment of acute unilateral vocal fold paralysis: Evidence from a canine in vivo phonation model",
   "original": "11158",
   "page_count": 5,
   "order": 463,
   "p1": 2273,
   "pn": 2277,
   "abstract": [
    "Medialization thyroplasty is a frequently used surgical treatment for insufficient glottal closure and involves placement of an implant to medialize the vocal fold. Prior studies have been unable to determine optimal implant shape and stiffness. In this study, thyroplasty implants with various medial surface shapes (rectangular, convergent, or divergent) and stiffnesses (Silastic, Gore-Tex, soft silicone of varying stiffness, or hydrogel) were assessed for optimal voice quality in an in vivo canine model of unilateral vocal fold paralysis with graded contralateral neuromuscular stimulation to mimic expected compensation seen in patients with this laryngeal pathology. Across experiments, Silastic rectangular implants consistently result in an improved voice quality metric, indicating high-quality output phonation. These findings have clinical implications for the optimization of thyroplasty implant treatment for speakers with laryngeal pathologies causing glottic insufficiency."
   ],
   "doi": "10.21437/Interspeech.2022-11158"
  },
  "nayak22_interspeech": {
   "authors": [
    [
     "Prateeth",
     "Nayak"
    ],
    [
     "Takuya",
     "Higuchi"
    ],
    [
     "Anmol",
     "Gupta"
    ],
    [
     "Shivesh",
     "Ranjan"
    ],
    [
     "Stephen",
     "Shum"
    ],
    [
     "Siddharth",
     "Sigtia"
    ],
    [
     "Erik",
     "Marchi"
    ],
    [
     "Varun",
     "Lakshminarasimhan"
    ],
    [
     "Minsik",
     "Cho"
    ],
    [
     "Saurabh",
     "Adya"
    ],
    [
     "Chandra",
     "Dhir"
    ],
    [
     "Ahmed",
     "Tewfik"
    ]
   ],
   "title": "Improving Voice Trigger Detection with Metric Learning",
   "original": "11160",
   "page_count": 5,
   "order": 384,
   "p1": 1896,
   "pn": 1900,
   "abstract": [
    "Voice trigger detection is an important task, which enables activating a voice assistant when a target user speaks a keyword phrase. A detector is typically trained on speech data independent of speaker information and used for the voice trigger detection task. However, such a speaker independent voice trigger detector typically suffers from performance degradation on speech from underrepresented groups, such as accented speakers. In this work, we propose a novel voice trigger detector that can use a small number of utterances from a target speaker to improve detection accuracy. Our proposed model employs an encoder-decoder architecture. While the encoder performs speaker independent voice trigger detection, similar to the conventional detector, the decoder is trained with metric learning and predicts a personalized embedding for each utterance. A personalized voice trigger score is then obtained as a similarity score between the embeddings of enrollment utterances and a test utterance. The personalized embedding allows adapting to target speaker's speech when computing the voice trigger score, hence improving voice trigger detection accuracy. Experimental results show that the proposed approach achieves a 38\\% relative reduction in a false rejection rate (FRR) compared to a baseline speaker independent voice trigger model."
   ],
   "doi": "10.21437/Interspeech.2022-11160"
  },
  "yang22x_interspeech": {
   "authors": [
    [
     "Muqiao",
     "Yang"
    ],
    [
     "Joseph",
     "Konan"
    ],
    [
     "David",
     "Bick"
    ],
    [
     "Anurag",
     "Kumar"
    ],
    [
     "Shinji",
     "Watanabe"
    ],
    [
     "Bhiksha",
     "Raj"
    ]
   ],
   "title": "Improving Speech Enhancement through Fine-Grained Speech Characteristics",
   "original": "11161",
   "page_count": 5,
   "order": 599,
   "p1": 2953,
   "pn": 2957,
   "abstract": [
    "While deep learning based speech enhancement systems have made rapid progress in improving the quality of speech signals, they can still produce outputs that contain artifacts and can sound unnatural. We propose a novel approach to speech enhancement aimed at improving perceptual quality and naturalness of enhanced signals by optimizing for key characteristics of speech. We first identify key acoustic parameters that have been found to correlate well with voice quality (e.g. jitter, shimmer, and spectral flux) and then propose objective functions which are aimed at reducing the difference between clean speech and enhanced speech with respect to these features. The full set of acoustic features is the extended Geneva Acoustic Parameter Set (eGeMAPS), which includes 25 different attributes associated with perception of speech. Given the non-differentiable nature of these feature computation, we first build differentiable estimators of the eGeMAPS and then use them to fine-tune existing speech enhancement systems. Our approach is generic and can be applied to any existing deep learning based enhancement systems to further improve the enhanced speech signals. Experimental results conducted on the Deep Noise Suppression (DNS) Challenge dataset shows that our approach can improve the state-of-the-art deep learning based enhancement systems."
   ],
   "doi": "10.21437/Interspeech.2022-11161"
  },
  "siriwardena22_interspeech": {
   "authors": [
    [
     "Yashish M.",
     "Siriwardena"
    ],
    [
     "Ganesh",
     "Sivaraman"
    ],
    [
     "Carol",
     "Espy-Wilson"
    ]
   ],
   "title": "Acoustic-to-articulatory Speech Inversion with Multi-task Learning",
   "original": "11164",
   "page_count": 5,
   "order": 1016,
   "p1": 5020,
   "pn": 5024,
   "abstract": [
    "Multi-task learning (MTL) frameworks have proven to be effective in diverse speech related tasks like automatic speech recognition (ASR) and speech emotion recognition. This paper proposes a MTL framework to perform acoustic-to-articulatory speech inversion by simultaneously learning an acoustic to phoneme mapping as a shared task. We use the Haskins Production Rate Comparison (HPRC) database which has both the electromagnetic articulography (EMA) data and the corresponding phonetic transcriptions. Performance of the system was measured by computing the correlation between estimated and actual tract variables (TVs) from the acoustic to articulatory speech inversion task. The proposed MTL based Bidirectional Gated Recurrent Neural Network (RNN) model learns to map the input acoustic features to nine TVs while outperforming the baseline model trained to perform only acoustic to articulatory inversion."
   ],
   "doi": "10.21437/Interspeech.2022-11164"
  },
  "qi22_interspeech": {
   "authors": [
    [
     "Heli",
     "Qi"
    ],
    [
     "Sashi",
     "Novitasari"
    ],
    [
     "Sakriani",
     "Sakti"
    ],
    [
     "Satoshi",
     "Nakamura"
    ]
   ],
   "title": "Improved Consistency Training for Semi-Supervised Sequence-to-Sequence ASR via Speech Chain Reconstruction and Self-Transcribing",
   "original": "11169",
   "page_count": 5,
   "order": 691,
   "p1": 3413,
   "pn": 3417,
   "abstract": [
    "Consistency regularization has recently been applied to semi-supervised sequence-to-sequence (S2S) automatic speech recognition (ASR). This principle encourages an ASR model to output similar predictions for the same input speech with different perturbations. The existing paradigm of semi-supervised S2S ASR utilizes SpecAugment as data augmentation and requires a static teacher model to produce pseudo transcripts for untranscribed speech. However, this paradigm fails to take full advantage of consistency regularization. First, the masking operations of SpecAugment may damage the linguistic contents of the speech, thus influencing the quality of pseudo labels. Second, S2S ASR requires both input speech and prefix tokens to make the next prediction. The static prefix tokens made by the offline teacher model cannot match dynamic pseudo labels during consistency training. In this work, we propose an improved consistency training paradigm of semi-supervised S2S ASR. We utilize speech chain reconstruction as the weak augmentation to generate high-quality pseudo labels. Moreover, we demonstrate that dynamic pseudo transcripts produced by the student ASR model benefit the consistency training. Experiments on LJSpeech and LibriSpeech corpora show that compared to supervised baselines, our improved paradigm achieves a 12.2% CER improvement in the single-speaker setting and 38.6% in the multi-speaker setting."
   ],
   "doi": "10.21437/Interspeech.2022-11169"
  },
  "lei22c_interspeech": {
   "authors": [
    [
     "Shun",
     "Lei"
    ],
    [
     "Yixuan",
     "Zhou"
    ],
    [
     "Liyang",
     "Chen"
    ],
    [
     "Jiankun",
     "Hu"
    ],
    [
     "Zhiyong",
     "Wu"
    ],
    [
     "Shiyin",
     "Kang"
    ],
    [
     "Helen",
     "Meng"
    ]
   ],
   "title": "Towards Multi-Scale Speaking Style Modelling with Hierarchical Context Information for Mandarin Speech Synthesis",
   "original": "11171",
   "page_count": 5,
   "order": 1119,
   "p1": 5523,
   "pn": 5527,
   "abstract": [
    "Previous works on expressive speech synthesis focus on modelling the mono-scale style embedding from the current sentence or context, but the multi-scale nature of speaking style in human speech is neglected. In this paper, we propose a multiscale speaking style modelling method to capture and predict multi-scale speaking style for improving the naturalness and expressiveness of synthetic speech. A multi-scale extractor is proposed to extract speaking style embeddings at three different levels from the ground-truth speech, and explicitly guide the training of a multi-scale style predictor based on hierarchical context information. Both objective and subjective evaluations on a Mandarin audiobooks dataset demonstrate that our proposed method can signiﬁcantly improve the naturalness and expressiveness of the synthesized speech."
   ],
   "doi": "10.21437/Interspeech.2022-11171"
  },
  "xie22b_interspeech": {
   "authors": [
    [
     "Jiamin",
     "Xie"
    ],
    [
     "John H.L.",
     "Hansen"
    ]
   ],
   "title": "DEFORMER: Coupling Deformed Localized Patterns with Global Context for Robust End-to-end Speech Recognition",
   "original": "11172",
   "page_count": 5,
   "order": 283,
   "p1": 1392,
   "pn": 1396,
   "abstract": [
    "Convolutional neural networks (CNN) have improved speech recognition performance greatly by exploiting localized time-frequency patterns. But these patterns are assumed to appear in symmetric and rigid kernels by the conventional CNN operation. It motivates the question: What about asymmetric kernels? In this study, we illustrate adaptive views can discover local features which couple better with attention than fixed views of the input. We replace depthwise CNNs in the Conformer architecture with a deformable counterpart, dubbed this \"Deformer\". By analyzing our best-performing model, we visualize both local receptive fields and global attention maps learned by the Deformer and show increased feature associations on the utterance level. The statistical analysis of learned kernel offsets provides an insight into the change of information in features with the network depth. Finally, replacing only half of the layers in the encoder, the Deformer improves +5.6% relative WER without a LM and +6.4% relative WER with a LM over the Conformer baseline on the WSJ eval92 set."
   ],
   "doi": "10.21437/Interspeech.2022-11172"
  },
  "bai22c_interspeech": {
   "authors": [
    [
     "Qibing",
     "Bai"
    ],
    [
     "Tom",
     "Ko"
    ],
    [
     "Yu",
     "Zhang"
    ]
   ],
   "title": "A Study of Modeling Rising Intonation in Cantonese Neural Speech Synthesis",
   "original": "11173",
   "page_count": 5,
   "order": 101,
   "p1": 501,
   "pn": 505,
   "abstract": [
    "In human speech, the attitude of a speaker cannot be fully expressed only by the textual content. It has to come along with the intonation. Declarative questions are commonly used in daily Cantonese conversations, and they are usually uttered with rising intonation. Vanilla neural text-to-speech (TTS) systems are not capable of synthesizing rising intonation for these sentences due to the loss of semantic information. Though it has become more common to complement the systems with extra language models, their performance in modeling rising intonation is not well studied. In this paper, we propose to complement the Cantonese TTS model with a BERT-based statement/question classifier. We design different training strategies and compare their performance. We conduct our experiments on a Cantonese corpus named CanTTS. Empirical results show that the separate training approach obtains the best generalization performance and feasibility."
   ],
   "doi": "10.21437/Interspeech.2022-11173"
  },
  "dumpala22_interspeech": {
   "authors": [
    [
     "Sri Harsha",
     "Dumpala"
    ],
    [
     "Chandramouli Shama",
     "Sastry"
    ],
    [
     "Rudolf",
     "Uher"
    ],
    [
     "Sageev",
     "Oore"
    ]
   ],
   "title": "On Combining Global and Localized Self-Supervised Models of Speech",
   "original": "11174",
   "page_count": 5,
   "order": 727,
   "p1": 3593,
   "pn": 3597,
   "abstract": [
    "Self supervised learning involves learning general-purpose representations that can be useful in a variety of downstream tasks. In this work, we study the application of speech-embeddings derived from popular self-supervised learning frameworks such as wav2vec-2.0 and HuBERT over four different speech classification tasks such as sentiment classification, command detection, emotion classification and depression detection. We distinguish between and discuss self-supervised training tasks that induce localized and global features of speech based on their temporal granularity: noting that self-supervised representation learning frameworks based on the masked language modeling objective – such as wav2vec-2.0 and HuBERT – induce localized embeddings, we define a self-supervised learning framework based on SimSiam for learning global features of speech. Through our evaluations, we find that these global representations are better suited for tasks such as depression detection and emotion classification while the localized embeddings of speech can be very useful in tasks such as speech-command detection; we also find that our proposed model outperforms TRILL – a popular model for learning global representations. Finally, we also propose and confirm empirically that combining the global and localized representations of speech helps obtain better performance across a range of downstream tasks than each of the individual embedding methods."
   ],
   "doi": "10.21437/Interspeech.2022-11174"
  },
  "meripo22_interspeech": {
   "authors": [
    [
     "Nimshi Venkat",
     "Meripo"
    ],
    [
     "Sandeep",
     "Konam"
    ]
   ],
   "title": "ASR Error Detection via Audio-Transcript entailment",
   "original": "11177",
   "page_count": 5,
   "order": 680,
   "p1": 3358,
   "pn": 3362,
   "abstract": [
    "Despite improved performances of the latest Automatic Speech Recognition (ASR) systems, transcription errors are still unavoidable. These errors can have a considerable impact in critical domains such as healthcare, when used to help with clinical documentation. Therefore detecting ASR errors is a critical first step in preventing further error propagation to downstream applications. To this end, we propose a novel end-to-end approach for ASR error detection using audio-transcript entailment. To the best of our knowledge, we are the first to frame this problem as an end-to-end entailment task between the audio segment and its corresponding transcript segment. Our intuition is that there should be a bidirectional entailment between audio and transcript when there is no recognition error and vice versa. The proposed model utilizes an acoustic encoder and a linguistic encoder to model the speech and transcript respectively. The encoded representations of both modalities are fused to predict the entailment. Since doctor-patient conversations are used in our experiments, a particular emphasis is placed on medical terms. Our proposed model achieves classification error rates (CER) of 26.2% on all transcription errors and 23% on medical errors specifically, leading to improvements upon baseline by 12% and 15.4%, respectively."
   ],
   "doi": "10.21437/Interspeech.2022-11177"
  },
  "li22ba_interspeech": {
   "authors": [
    [
     "Junjie",
     "Li"
    ],
    [
     "Meng",
     "Ge"
    ],
    [
     "Zexu",
     "Pan"
    ],
    [
     "Longbiao",
     "Wang"
    ],
    [
     "Jianwu",
     "Dang"
    ]
   ],
   "title": "VCSE: Time-Domain Visual-Contextual Speaker Extraction Network",
   "original": "11183",
   "page_count": 5,
   "order": 185,
   "p1": 906,
   "pn": 910,
   "abstract": [
    "Speaker extraction seeks to extract the target speech in a multi-talker scenario given an auxiliary reference. Such reference can be auditory, i.e., a pre-recorded speech, visual, i.e., lip movements, or contextual, i.e., phonetic sequence. References in different modalities provide distinct and complementary information that could be fused to form top-down attention on the target speaker. Previous studies have introduced visual and contextual modalities in a single model. In this paper, we propose a two-stage time-domain visual-contextual speaker extraction network named VCSE, which incorporates visual and self-enrolled contextual cues stage by stage to take full advantage of every modality. In the first stage, we pre-extract a target speech with visual cues and estimate the underlying phonetic sequence. In the second stage, we refine the pre-extracted target speech with the self-enrolled contextual cues. Experimental results on the real-world Lip Reading Sentences 3 (LRS3) database demonstrate that our proposed VCSE network consistently outperforms other state-of-the-art baselines."
   ],
   "doi": "10.21437/Interspeech.2022-11183"
  },
  "zurita22_interspeech": {
   "authors": [
    [
     "Alvaro Martin Iturralde",
     "Zurita"
    ],
    [
     "Meghan",
     "Clayards"
    ]
   ],
   "title": "Lexical stress in Spanish word segmentation",
   "original": "11185",
   "page_count": 5,
   "order": 378,
   "p1": 1866,
   "pn": 1870,
   "abstract": [
    "We explored the role of lexical stress in word segmentation as speech unfolds in time. We tested participants online with a Mouse Tracking listening experiment using temporarily ambiguous phrase pairs of the form \"PAlo marron” vs. \"paLOma roja”. These pairs were segmentally ambiguous in the first three syllables but differed in the location of lexical stress. Thus, use of stress cues would allow participants to disambiguate the phrases more quickly. We also manipulated the presence of lexical stress correlates in two conditions (stress natural and stress neutral) and found that lexical stress has an early impact in Spanish word segmentation that can affect how quickly and efficiently the speech signal is processed."
   ],
   "doi": "10.21437/Interspeech.2022-11185"
  },
  "faridee22_interspeech": {
   "authors": [
    [
     "Abu Zaher Md",
     "Faridee"
    ],
    [
     "Hannes",
     "Gamper"
    ]
   ],
   "title": "Predicting label distribution improves non-intrusive speech quality estimation",
   "original": "11186",
   "page_count": 5,
   "order": 82,
   "p1": 406,
   "pn": 410,
   "abstract": [
    "Deep noise suppressors (DNS) have become an attractive solution to remove background noise, reverberation, and distortions from speech and are widely used in telephony/voice applications. They are also occasionally prone to introducing artifacts and lowering the perceptual quality of the speech. Subjective listening tests that use multiple human judges to derive a mean opinion score (MOS) are a popular way to measure these models' performance. Deep neural network based non-intrusive MOS estimation models have recently emerged as a popular cost-efficient alternative to these tests. These models are trained with only the MOS labels, often discarding the secondary statistics of the opinion scores. In this paper, we investigate several ways to integrate the distribution of opinion scores (e.g. variance, histogram information) to improve the MOS estimation performance. Our model is trained on a corpus of 419K denoised samples by 320 different DNS models and model variations and evaluated on 18K test samples from DNSMOS. We show that with very minor modification of a single task MOS estimation pipeline, these freely available labels can provide up to a 0.016 RMSE and 1% SRCC improvement."
   ],
   "doi": "10.21437/Interspeech.2022-11186"
  },
  "wei22f_interspeech": {
   "authors": [
    [
     "Xiao",
     "Wei"
    ],
    [
     "Yuke",
     "Si"
    ],
    [
     "Shiquan",
     "Wang"
    ],
    [
     "Longbiao",
     "Wang"
    ],
    [
     "Jianwu",
     "Dang"
    ]
   ],
   "title": "Hierarchical Tagger with Multi-task Learning for Cross-domain Slot Filling",
   "original": "11187",
   "page_count": 5,
   "order": 663,
   "p1": 3273,
   "pn": 3277,
   "abstract": [
    "In task-oriented dialog systems, slot filling aims to identify the semantic slot type of each token in utterances. Due to the lack of sufficient supervised data in many scenarios, it is necessary to transfer knowledge by using cross-domain slot filling. Previous studies focus on building the relationships among similar slots across domains by providing additional descriptions, yet not fully utilizing prior information. In this study, we mainly make two novel improvements. First, we improve the hierarchical frameworks based on pre-trained models. For instance, we add domain descriptions to auxiliary information in the similarity layer to enhance the relationships. Second, we improve the independent fine-tuning with multi-task learning by using an auxiliary network, where the domain detection task is deliberately set up corresponding to the domain descriptions. Additionally, we also adopt an adversarial regularization to avoid over-fitting. Experimental results on SNIPS dataset show that our model significantly outperforms the best baseline by 16.11%, 11.06% and 8.77%, respectively in settings of 0-shot, 20-shot and 50-shot in terms of micro F1, which demonstrates our model has better generalization ability, especially for domain-specific slots."
   ],
   "doi": "10.21437/Interspeech.2022-11187"
  },
  "wang22da_interspeech": {
   "authors": [
    [
     "Shiquan",
     "Wang"
    ],
    [
     "Yuke",
     "Si"
    ],
    [
     "Xiao",
     "Wei"
    ],
    [
     "Longbiao",
     "Wang"
    ],
    [
     "Zhiqiang",
     "Zhuang"
    ],
    [
     "Xiaowang",
     "Zhang"
    ],
    [
     "Jianwu",
     "Dang"
    ]
   ],
   "title": "TopicKS: Topic-driven Knowledge Selection for Knowledge-grounded Dialogue Generation",
   "original": "11188",
   "page_count": 5,
   "order": 228,
   "p1": 1121,
   "pn": 1125,
   "abstract": [
    "Knowledge-grounded dialogue generation is proposed to solve the problem of general or meaningless responses in traditional end-to-end dialogue generation methods. It generally includes two sub-modules: knowledge selection and knowledge-aware generation. Most studies consider the topic information for knowledge-aware generation, while ignoring it in knowledge selection. It may cause the topic mismatch between the overall dialogue and the selected knowledge, leading to the inconsistency of the generated response and the context. Therefore, in this study, we propose a Topic-driven Knowledge Selection method (TopicKS) to exploit topic information both in knowledge selection and knowledge-aware generation. Specifically, under the guidance of topic information, TopicKS selects more accurate candidate knowledge for the current turn of dialogue based on context information and historical knowledge information. Then the decoder uses the context information and selected knowledge to generate a higher-quality response under the guidance of topic information. Experiments on the notable benchmark corpus Wizard of Wikipedia (WoW) show that our proposed method not only achieves a significant improvement in terms of selection accuracy rate on knowledge selection, but also outperforms the baseline model in terms of the quality of the generated responses."
   ],
   "doi": "10.21437/Interspeech.2022-11188"
  },
  "kumar22c_interspeech": {
   "authors": [
    [
     "Rishabh",
     "Kumar"
    ],
    [
     "Devaraja",
     "Adiga"
    ],
    [
     "Rishav",
     "Ranjan"
    ],
    [
     "Amrith",
     "Krishna"
    ],
    [
     "Ganesh",
     "Ramakrishnan"
    ],
    [
     "Pawan",
     "Goyal"
    ],
    [
     "Preethi",
     "Jyothi"
    ]
   ],
   "title": "Linguistically Informed Post-processing for ASR Error correction in Sanskrit",
   "original": "11189",
   "page_count": 5,
   "order": 467,
   "p1": 2293,
   "pn": 2297,
   "abstract": [
    "We propose an ASR system for Sanskrit, a low-resource language, that effectively combines subword tokenisation strategies and search space enrichment with linguistic information. More specifically, to address the challenges due to the high degree of out-of-vocabulary entries present in the language, we first use a subword-based language model and acoustic model to generate a search space. The search space, so obtained, is converted into a word-based search space and is further enriched with morphological and lexical information based on a shallow parser. Finally, the transitions in the search space are rescored using a supervised morphological parser proposed for Sanskrit. Our proposed approach currently reports the state-of-the-art results in Sanskrit ASR, with a 7.18 absolute point reduction in WER than the previous state-of-the-art."
   ],
   "doi": "10.21437/Interspeech.2022-11189"
  },
  "wang22ea_interspeech": {
   "authors": [
    [
     "Xingming",
     "Wang"
    ],
    [
     "Xiaoyi",
     "Qin"
    ],
    [
     "Yikang",
     "Wang"
    ],
    [
     "Yunfei",
     "Xu"
    ],
    [
     "Ming",
     "Li"
    ]
   ],
   "title": "The DKU-OPPO System for the 2022 Spoofing-Aware Speaker Verification Challenge",
   "original": "11190",
   "page_count": 5,
   "order": 891,
   "p1": 4396,
   "pn": 4400,
   "abstract": [
    "This paper describes our DKU-OPPO system for the 2022 Spoofing-Aware Speaker Verification (SASV) Challenge. First, we split the joint task into speaker verification (SV) and spoofing countermeasure (CM) these two tasks which are optimized separately. For ASV systems, four state-of-the-art methods are employed. For CM systems, we propose two methods on top of the challenge baseline to further improve the performance, namely Embedding Random Sampling Augmentation (ERSA) and One-Class Confusion Loss(OCCL). Second, we also explore whether SV embedding could help improve CM system performance. We observe a dramatic performance degradation of existing CM systems on the domain-mismatched Voxceleb2 dataset. Third, we compare different fusion strategies, including parallel score fusion and sequential cascaded systems. Compared to the 1.71% SASV-EER baseline, our submitted cascaded system obtains a 0.21% SASV-EER on the challenge official evaluation set."
   ],
   "doi": "10.21437/Interspeech.2022-11190"
  },
  "peng22d_interspeech": {
   "authors": [
    [
     "Chiang-Jen",
     "Peng"
    ],
    [
     "Yun-Ju",
     "Chan"
    ],
    [
     "Yih-Liang",
     "Shen"
    ],
    [
     "Cheng",
     "Yu"
    ],
    [
     "Yu",
     "Tsao"
    ],
    [
     "Tai-Shih",
     "Chi"
    ]
   ],
   "title": "Perceptual Characteristics Based Multi-objective Model for Speech Enhancement",
   "original": "11197",
   "page_count": 5,
   "order": 43,
   "p1": 211,
   "pn": 215,
   "abstract": [
    "Deep learning has been widely adopted for speech applications. Many studies have shown that using the multiple objective framework and learned deep features is effective for improving system performance. In this paper, we propose a perceptual characteristics based multi-objective speech enhancement (SE) algorithm that combines the conventional loss and objective losses of pitch and timbre related features. Timbre related features include frequency modulation (encoded by the pitch contour), amplitude modulation (encoded by the energy contour), and speaker identity. For the speaker identity loss, we consider the deep features derived in a speaker identification system. The proposed algorithm consists of two parts, a LSTM based SE model and CNN based multi-objective models. The objective losses are derived between speech enhanced by the SE model and clean speech and combined with the SE loss for updating the SE model. The proposed algorithm is evaluated using the corpus of Taiwan Mandarin hearing in noise test (TMHINT). Experimental results show the proposed algorithm evidently outperforms the original SE model in all objective scores, including speech quality, speech intelligibility and signal distortion."
   ],
   "doi": "10.21437/Interspeech.2022-11197"
  },
  "ogushi22_interspeech": {
   "authors": [
    [
     "Asahi",
     "Ogushi"
    ],
    [
     "Toshiki",
     "Onishi"
    ],
    [
     "Yohei",
     "Tahara"
    ],
    [
     "Ryo",
     "Ishii"
    ],
    [
     "Atsushi",
     "Fukayama"
    ],
    [
     "Takao",
     "Nakamura"
    ],
    [
     "Akihiro",
     "Miyata"
    ]
   ],
   "title": "Analysis of praising skills focusing on utterance contents",
   "original": "11200",
   "page_count": 5,
   "order": 557,
   "p1": 2743,
   "pn": 2747,
   "abstract": [
    "Praising behavior is considered an important method of communication. It is considered effective in building good relationships with others and bringing out the best in them. However, there are quite a few people who have difficulty in praising successfully.These people have difficulty judging and improving their praising skills. The reason is that they have not clarified which behaviors are important for successfully praising. In this paper, we analyze praising behavior by focusing on the content of utterances in Japanese. We construct machine learning models for estimating praising skills to analyze behaviors for successfully praising. Then, we extract features from the utterances of persons who give praise (praiser) and those who receive praise (receiver). The results showed that the best performance of our models was F1 = 0.632. We analyzed praising skills from the results of that model. An utterance of praising scene, one receiver utterance immediately preceding the utterance of praising scene, and four receiver utterances immediately following it are crucial for estimating praising skills. The cosine similarity between an utterance by a praiser and one by a receiver is also important as a feature. Thus, we confirmed that the utterances of receiver are similar to the utterances of praiser."
   ],
   "doi": "10.21437/Interspeech.2022-11200"
  },
  "morshed22_interspeech": {
   "authors": [
    [
     "Mahir",
     "Morshed"
    ],
    [
     "Mark",
     "Hasegawa-Johnson"
    ]
   ],
   "title": "Cross-lingual articulatory feature information transfer for speech recognition using recurrent progressive neural networks",
   "original": "11202",
   "page_count": 5,
   "order": 468,
   "p1": 2298,
   "pn": 2302,
   "abstract": [
    "A system for the lateral transfer of information from end-to-end neural networks recognizing articulatory feature classes to similarly structured networks recognizing phone tokens is here proposed. The system connects recurrent layers of feature detectors pre-trained on a base language to recurrent layers of a phone recognizer for a different target language, this inspired primarily by the progressive neural network scheme. Initial experiments used detectors trained on Bengali speech for four articulatory feature classes—consonant place, consonant manner, vowel height, and vowel backness—attached to phone recognizers for four other Asian languages (Javanese, Nepali, Sinhalese, and Sundanese). While these do not currently suggest consistent performance improvements across different low-resource settings for target languages, irrespective of their genealogic or phonological relatedness to Bengali, they do suggest the need for further trials with different language sets, altered data sources and data configurations, and slightly altered network setups."
   ],
   "doi": "10.21437/Interspeech.2022-11202"
  },
  "robinson22_interspeech": {
   "authors": [
    [
     "Nathaniel Romney",
     "Robinson"
    ],
    [
     "Perez",
     "Ogayo"
    ],
    [
     "Swetha R.",
     "Gangu"
    ],
    [
     "David R.",
     "Mortensen"
    ],
    [
     "Shinji",
     "Watanabe"
    ]
   ],
   "title": "When Is TTS Augmentation Through a Pivot Language Useful?",
   "original": "11203",
   "page_count": 5,
   "order": 716,
   "p1": 3538,
   "pn": 3542,
   "abstract": [
    "Developing Automatic Speech Recognition (ASR) for low-resource languages is a challenge due to the small amount of transcribed audio data. For many such languages, audio and text are available separately, but not audio with transcriptions. Using text, speech can be synthetically produced via text-to-speech (TTS) system . However, many low-resource languages do not have quality TTS systems either. We propose an alternative: produce synthetic audio by running text from the target language through a trained TTS system for a higher-resource pivot language. We investigate when and how this technique is most effective in low-resource settings. In our experiments, using several thousand synthetic TTS data pairs and duplicating authentic data to balance yields optimal results. Our findings suggest that searching over a set of candidate pivot languages can lead to marginal improvements and that, surprisingly, ASR performance can at times by harmed by increases in measured TTS quality. Application of these findings improves ASR error rates by 64.5% and 45.0% CERR respectively for two low-resource languages: Guarani and Suba."
   ],
   "doi": "10.21437/Interspeech.2022-11203"
  },
  "nallanthighal22_interspeech": {
   "authors": [
    [
     "Venkata Srikanth",
     "Nallanthighal"
    ],
    [
     "Aki",
     "Harma"
    ],
    [
     "Helmer",
     "Strik"
    ]
   ],
   "title": "COVID-19 detection based on respiratory sensing from speech",
   "original": "11209",
   "page_count": 5,
   "order": 508,
   "p1": 2498,
   "pn": 2502,
   "abstract": [
    "COVID-19 affects a person's respiratory health, which is manifested in the form of shortness of breath during speech. Recent work shows that it is possible to use deep learning techniques to sense the speaker's respiratory parameters from a speech signal directly. Thus respiratory parameters like speech breathing rate and tidal volume can be computed and compared using deep learning techniques to detect COVID-19 from speech recordings. In this paper, we compute respiratory parameters using our pre-trained deep learning-based speech breathing models and use them for detecting COVID-19 from speech. Apart from using speech breathing models, we perform acoustic features identification using a statistical approach and classification based on low-level descriptive features. Our analysis investigates the distinction of speech of a healthy person and COVID-19 affected person."
   ],
   "doi": "10.21437/Interspeech.2022-11209"
  },
  "yu22b_interspeech": {
   "authors": [
    [
     "Fan",
     "Yu"
    ],
    [
     "Zhihao",
     "Du"
    ],
    [
     "ShiLiang",
     "Zhang"
    ],
    [
     "Yuxiao",
     "Lin"
    ],
    [
     "Lei",
     "Xie"
    ]
   ],
   "title": "A Comparative Study on Speaker-attributed Automatic Speech Recognition in Multi-party Meetings",
   "original": "11210",
   "page_count": 5,
   "order": 113,
   "p1": 560,
   "pn": 564,
   "abstract": [
    "In this paper, we conduct a comparative study on speaker-attributed automatic speech recognition (SA-ASR) in the multi-party meeting scenario, a topic with increasing attention in meeting rich transcription. Specifically, three approaches are evaluated in this study. The first approach, FD-SOT, consists of a frame-level diarization model to identify speakers and a multi-talker ASR to recognize utterances. The speaker-attributed transcriptions are obtained by aligning the diarization results and recognized hypotheses. However, such an alignment strategy may suffer from erroneous timestamps due to the modular independence, severely hindering the model performance. Therefore, we propose the second approach, WD-SOT, to address alignment errors by introducing a word-level diarization model, which can get rid of such timestamp alignment dependency. To further mitigate the alignment issues, we propose the third approach, TS-ASR, which trains a target-speaker separation module and an ASR module jointly. By comparing various strategies for each SA-ASR approach, experimental results on a real meeting scenario corpus, AliMeeting, reveal that the WD-SOT approach achieves 10.7% relative reduction on averaged speaker-dependent character error rate (SD-CER), compared with the FD-SOT approach. In addition, the TS-ASR approach also outperforms the FD-SOT approach and brings 16.5% relative average SD-CER reduction."
   ],
   "doi": "10.21437/Interspeech.2022-11210"
  },
  "an22_interspeech": {
   "authors": [
    [
     "Keyu",
     "An"
    ],
    [
     "Huahuan",
     "Zheng"
    ],
    [
     "Zhijian",
     "Ou"
    ],
    [
     "Hongyu",
     "Xiang"
    ],
    [
     "Ke",
     "Ding"
    ],
    [
     "Guanglu",
     "Wan"
    ]
   ],
   "title": "CUSIDE: Chunking, Simulating Future Context and Decoding for Streaming ASR",
   "original": "11214",
   "page_count": 5,
   "order": 429,
   "p1": 2103,
   "pn": 2107,
   "abstract": [
    "History and future contextual information are known to be important for accurate acoustic modeling. However, acquiring future context brings latency for streaming ASR. In this paper, we propose a new framework - Chunking, Simulating Future Context and Decoding (CUSIDE) for streaming speech recognition. A new simulation module is introduced to recursively simulate the future contextual frames, without waiting for future context. The simulation module is jointly trained with the ASR model using a self-supervised loss; the ASR model is optimized with the usual ASR loss, e.g., CTC-CRF as used in our experiments. Experiments show that, compared to using real future frames as right context, using simulated future context can drastically reduce the latency while maintaining recognition accuracy. With CUSIDE, we obtain new state-of-the-art streaming ASR results on the AISHELL-1 dataset."
   ],
   "doi": "10.21437/Interspeech.2022-11214"
  },
  "pandey22c_interspeech": {
   "authors": [
    [
     "Ashutosh",
     "Pandey"
    ],
    [
     "Buye",
     "Xu"
    ],
    [
     "Anurag",
     "Kumar"
    ],
    [
     "Jacob",
     "Donley"
    ],
    [
     "Paul",
     "Calamia"
    ],
    [
     "DeLiang",
     "Wang"
    ]
   ],
   "title": "Time-domain Ad-hoc Array Speech Enhancement Using a Triple-path Network",
   "original": "11215",
   "page_count": 5,
   "order": 147,
   "p1": 729,
   "pn": 733,
   "abstract": [
    "Deep neural networks (DNNs) are very effective for multichannel speech enhancement with fixed array geometries. However, it is not trivial to use DNNs for ad-hoc arrays with unknown order and placement of microphones. We propose a novel triple-path network for ad-hoc array processing in the time domain. The key idea in the network design is to divide the overall processing into spatial processing and temporal processing and use self-attention for spatial processing. Using self-attention for spatial processing makes the network invariant to the order and the number of microphones. The temporal processing is done independently for all channels using a recently proposed dual-path attentive recurrent network. The proposed network is a multiple-input multiple-output architecture that can simultaneously enhance signals at all microphones. Experimental results demonstrate the excellent performance of the proposed approach. Further, we present analysis to demonstrate the effectiveness of the proposed network in utilizing multichannel information even from microphones at far locations."
   ],
   "doi": "10.21437/Interspeech.2022-11215"
  },
  "sudo22_interspeech": {
   "authors": [
    [
     "Yui",
     "Sudo"
    ],
    [
     "Shakeel",
     "Muhammad"
    ],
    [
     "Kazuhiro",
     "Nakadai"
    ],
    [
     "Jiatong",
     "Shi"
    ],
    [
     "Shinji",
     "Watanabe"
    ]
   ],
   "title": "Streaming Automatic Speech Recognition with Re-blocking Processing Based on Integrated Voice Activity Detection",
   "original": "11216",
   "page_count": 5,
   "order": 940,
   "p1": 4641,
   "pn": 4645,
   "abstract": [
    "This paper proposes streaming automatic speech recognition (ASR) with re-blocking processing based on integrated voice activity detection (VAD). End-to-end (E2E) ASR models are promising for practical ASR. One of the key issues in realizing such a system is the detection of voice segments to cope with streaming input. There are three challenges for speech segmentation in streaming applications: 1) the extra VAD module in addition to the ASR model increases the system complexity and the number of parameters, 2) inappropriate segmentation of speech for block-based streaming methods deteriorates the performance, 3) non-voice segments that are not discarded results in the increase of unnecessary computational costs. This paper proposes a model that integrates a VAD branch into a block processing-based streaming ASR system and a re-blocking technique to avoid inappropriate isolation of the utterances. Experiments show that the proposed method reduces the detection error rate (ER) by 25.8% on the AMI dataset with a less than 1% of increase in the number of parameters. Furthermore, the proposed method show 7.5% relative improvement in character error rate (CER) on the CSJ dataset with 27.3% reduction in real-time factor (RTF)."
   ],
   "doi": "10.21437/Interspeech.2022-11216"
  },
  "begus22_interspeech": {
   "authors": [
    [
     "Gasper",
     "Begus"
    ],
    [
     "Alan",
     "Zhou"
    ]
   ],
   "title": "Modeling speech recognition and synthesis simultaneously: Encoding and decoding lexical and sublexical semantic information into speech with no direct access to speech data",
   "original": "11219",
   "page_count": 5,
   "order": 1074,
   "p1": 5298,
   "pn": 5302,
   "abstract": [
    "Human speakers encode information into raw speech which is then decoded by the listeners. This complex relationship between encoding (production) and decoding (perception) is often modeled separately. Here, we test how encoding and decoding of lexical semantic information can emerge automatically from raw speech in unsupervised generative deep convolutional networks that combine the production and perception principles of speech. We introduce, to our knowledge, the most challenging objective in unsupervised lexical learning: a network that must learn unique representations for lexical items with no direct access to training data. We train several models (ciwGAN and fiwGAN [1]) and test how the networks classify acoustic lexical items in unobserved test data. Strong evidence in favor of lexical learning and a causal relationship between latent codes and meaningful sublexical units emerge. The architecture that combines the production and perception principles is thus able to learn to decode unique information from raw acoustic data without accessing real training data directly. We propose a technique to explore lexical (holistic) and sublexical (featural) learned representations in the classifier network. The results bear implications for unsupervised speech technology, as well as for unsupervised semantic modeling as language models increasingly bypass text and operate from raw acoustics."
   ],
   "doi": "10.21437/Interspeech.2022-11219"
  },
  "parikh22c_interspeech": {
   "authors": [
    [
     "Rahil",
     "Parikh"
    ],
    [
     "Gaspar",
     "Rochette"
    ],
    [
     "Carol",
     "Espy-Wilson"
    ],
    [
     "Shihab",
     "Shamma"
    ]
   ],
   "title": "An Empirical Analysis on the Vulnerabilities of End-to-End Speech Segregation Models",
   "original": "11222",
   "page_count": 5,
   "order": 1096,
   "p1": 5408,
   "pn": 5412,
   "abstract": [
    "End-to-end learning models have demonstrated a remarkable capability in performing speech segregation. Despite their wide-scope of real-world applications, little is known about the mechanisms they employ to group and consequently segregate individual speakers. Knowing that harmonicity is a critical cue for these networks to group sources, in this work, we perform a thorough investigation on ConvTasnet and DPT-Net to analyze how they perform a harmonic analysis of the input mixture. We perform ablation studies where we apply low-pass, high-pass, and band-stop filters of varying pass-bands to empirically analyze the harmonics most critical for segregation. We also investigate how these networks decide which output channel to assign to an estimated source by introducing discontinuities in synthetic mixtures. We find that end-to-end networks are highly unstable, and perform poorly when confronted with deformations which are imperceptible to humans. Replacing the encoder in these networks with a spectrogram leads to lower overall performance, but much higher stability. This work helps us to understand what information these network rely on for speech segregation, and exposes two sources of generalization-errors. It also pinpoints the encoder as the part of the network responsible for these generalization-errors, allowing for a redesign with expert knowledge or transfer learning."
   ],
   "doi": "10.21437/Interspeech.2022-11222"
  },
  "li22ca_interspeech": {
   "authors": [
    [
     "Xiang",
     "Li"
    ],
    [
     "Changhe",
     "Song"
    ],
    [
     "Xianhao",
     "Wei"
    ],
    [
     "Zhiyong",
     "Wu"
    ],
    [
     "Jia",
     "Jia"
    ],
    [
     "Helen",
     "Meng"
    ]
   ],
   "title": "Towards Cross-speaker Reading Style Transfer on Audiobook Dataset",
   "original": "11223",
   "page_count": 5,
   "order": 1120,
   "p1": 5528,
   "pn": 5532,
   "abstract": [
    "Cross-speaker style transfer aims to extract the speech style of the given reference speech, which can be reproduced in the timbre of arbitrary target speakers. Existing methods on this topic have explored utilizing utterance-level style labels to perform style transfer via either global or local scale style representations. However, audiobook datasets are typically characterized by both the local prosody and global genre, and are rarely accompanied by utterance-level style labels. Thus, properly transferring the reading style across different speakers remains a challenging task. This paper aims to introduce a chunk-wise multi-scale cross-speaker style model to capture both the global genre and the local prosody in audiobook speeches. Moreover, by disentangling speaker timbre and style with the proposed switchable adversarial classifiers, the extracted reading style is made adaptable to the timbre of different speakers. Experiment results confirm that the model manages to transfer a given reading style to new target speakers. With the support of local prosody and global genre type predictor, the potentiality of the proposed method in multi-speaker audiobook generation is further revealed."
   ],
   "doi": "10.21437/Interspeech.2022-11223"
  },
  "lian22_interspeech": {
   "authors": [
    [
     "Jiachen",
     "Lian"
    ],
    [
     "Chunlei",
     "Zhang"
    ],
    [
     "Gopala Krishna",
     "Anumanchipalli"
    ],
    [
     "Dong",
     "Yu"
    ]
   ],
   "title": "Towards Improved Zero-shot Voice Conversion with Conditional DSVAE",
   "original": "11225",
   "page_count": 5,
   "order": 528,
   "p1": 2598,
   "pn": 2602,
   "abstract": [
    "Disentangling content and speaking style information is essential for zero-shot non-parallel voice conversion (VC). Our previous study investigated a novel framework with disentangled sequential variational autoencoder (DSVAE) as the backbone for information decomposition. We have demonstrated that simultaneously disentangle content embedding and speaker embedding from one utterance is feasible for downstream tasks such as speaker verification and zero-shot VC. In this study, we continue the direction by rising one concern remained in the prior distribution of content branch of the DSVAE baseline. We find the random initialed prior distribution will force the content embedding to reduce the phonetic-structure information during the learning process, which is not a desired property. Here, we seek to achieve a better content embedding with more phonetic information preserved. We propose conditional DSVAE, a new model that enables content bias as a condition to the prior modeling and reshapes the content embedding sampled from the posterior distribution. In our experiment on the VCTK dataset, we demonstrate that content embeddings derived from the conditional DSVAE overcome the randomness and achieve a better phoneme classification accuracy compared with the DSVAE baseline. In the meanwhile, the change results in an improved performance of zero-shot VC."
   ],
   "doi": "10.21437/Interspeech.2022-11225"
  },
  "garg22_interspeech": {
   "authors": [
    [
     "Vineet",
     "Garg"
    ],
    [
     "Ognjen",
     "Rudovic"
    ],
    [
     "Pranay",
     "Dighe"
    ],
    [
     "Ahmed Hussen",
     "Abdelaziz"
    ],
    [
     "Erik",
     "Marchi"
    ],
    [
     "Saurabh",
     "Adya"
    ],
    [
     "Chandra",
     "Dhir"
    ],
    [
     "Ahmed",
     "Tewfik"
    ]
   ],
   "title": "Device-Directed Speech Detection: Regularization via Distillation for Weakly-Supervised Models",
   "original": "11228",
   "page_count": 5,
   "order": 256,
   "p1": 1258,
   "pn": 1262,
   "abstract": [
    "We address the problem of detecting speech directed to a device that does not contain a specific wake-word. Specifically, we focus on audio coming from a touch-based invocation. Mitigating virtual assistants (VAs) activation due to accidental button presses is critical for user experience. While the majority of approaches to false trigger mitigation (FTM) are designed to detect the presence of a target keyword, inferring user intent in absence of keyword is difficult. This also poses a challenge when creating the training/evaluation data for such systems due to inherent ambiguity in the user's data. To this end, we propose a novel FTM approach that uses weakly-labeled training data obtained with a newly introduced data sampling strategy. While this sampling strategy reduces data annotation efforts, the data labels are noisy as the data are not annotated manually. We use these data to train an acoustics-only model for the FTM task by regularizing its loss function via knowledge distillation from an ASR-based (LatticeRNN) model. This improves the model decisions, resulting in 66% gain in accuracy, as measured by equal-error-rate (EER), over the base acoustics-only model. We also show that the ensemble of the LatticeRNN and acoustic-distilled models brings further accuracy improvement of 20%."
   ],
   "doi": "10.21437/Interspeech.2022-11228"
  },
  "kameoka22_interspeech": {
   "authors": [
    [
     "Hirokazu",
     "Kameoka"
    ],
    [
     "Takuhiro",
     "Kaneko"
    ],
    [
     "Shogo",
     "Seki"
    ],
    [
     "Kou",
     "Tanaka"
    ]
   ],
   "title": "CAUSE: Crossmodal Action Unit Sequence Estimation from Speech",
   "original": "11232",
   "page_count": 5,
   "order": 102,
   "p1": 506,
   "pn": 510,
   "abstract": [
    "This paper proposes a task and method for estimating a sequence of facial action units (AUs) solely from speech. AUs were introduced in the facial action coding system to objectively describe facial muscle activations. Our motivation is that AUs can be useful continuous quantities for representing speaker's subtle emotional states, attitudes, and moods in a variety of applications such as expressive speech synthesis and emotional voice conversion. We hypothesize that the information about the speaker's facial muscle movements is expressed in the generated speech and can somehow be predicted from speech alone. To verify this, we devise a neural network model that predicts an AU sequence from the mel-spectrogram of input speech and train it using a large-scale audio-visual dataset consisting of many speaking face-tracks. We call our method and model ``crossmodal AU sequence estimation/estimator (CAUSE)''. We implemented several of the most basic architectures for CAUSE, and quantitatively confirmed that the fully convolutional architecture performed best. Furthermore, by combining CAUSE with an AU-conditioned image-to-image translation method, we implemented a system that animates a given still face image from speech. Using this system, we confirmed the potential usefulness of AUs as a representation of non-linguistic features via subjective evaluations."
   ],
   "doi": "10.21437/Interspeech.2022-11232"
  },
  "lian22b_interspeech": {
   "authors": [
    [
     "Jiachen",
     "Lian"
    ],
    [
     "Alan W",
     "Black"
    ],
    [
     "Louis",
     "Goldstein"
    ],
    [
     "Gopala Krishna",
     "Anumanchipalli"
    ]
   ],
   "title": "Deep Neural Convolutive Matrix Factorization for Articulatory Representation Decomposition",
   "original": "11233",
   "page_count": 5,
   "order": 949,
   "p1": 4686,
   "pn": 4690,
   "abstract": [
    "Most of the research on data-driven speech representation learning has focused on raw audios in an end-to-end manner, paying little attention to their internal phonological or gestural structure. This work, investigating the speech representations derived from articulatory kinematics signals, uses a neural implementation of convolutive sparse matrix factorization to decompose the articulatory data into interpretable gestures and gestural scores. By applying sparse constraints, the gestural scores leverage the discrete combinatorial properties of phonological gestures. Phoneme recognition experiments were additionally performed to show that gestural scores indeed code phonological information successfully. The proposed work thus makes a bridge between articulatory phonology and deep neural networks to leverage informative, intelligible, interpretable,and efficient speech representations. The code is made publicly available at \\url{https://github.com/Berkeley-Speech-Group/ema_gesture}."
   ],
   "doi": "10.21437/Interspeech.2022-11233"
  },
  "mun22_interspeech": {
   "authors": [
    [
     "Seongkyu",
     "Mun"
    ],
    [
     "Dhananjaya",
     "Gowda"
    ],
    [
     "Jihwan",
     "Lee"
    ],
    [
     "Changwoo",
     "Han"
    ],
    [
     "Dokyun",
     "Lee"
    ],
    [
     "Chanwoo",
     "Kim"
    ]
   ],
   "title": "Prototypical speaker-interference loss for target voice separation using non-parallel audio samples",
   "original": "11236",
   "page_count": 5,
   "order": 56,
   "p1": 276,
   "pn": 280,
   "abstract": [
    "In this paper, we propose a new prototypical loss function for training neural network models for target voice separation. Conventional methods use paired parallel audio samples of the target speaker with and without an interfering speaker or noise, and minimize the spectrographic mean squared error (MSE) between the clean and enhanced target speaker audio. Motivated by the use of contrastive loss in speaker recognition task, we had earlier proposed a speaker representation loss that uses representative samples from the target speaker in addition to the conventional MSE loss. In this work, we propose a prototypical speaker-interference (PSI) loss, that makes use of representative samples from the target speaker, interfering speaker as well as the interfering noise to better utilize any non-parallel data that may be available. The performance of the proposed loss function is evaluated using VoiceFilter, a popular framework for target voice separation. Experimental results show that the proposed PSI loss significantly improves the PESQ scores of the enhanced target speaker audio."
   ],
   "doi": "10.21437/Interspeech.2022-11236"
  },
  "ghosh22c_interspeech": {
   "authors": [
    [
     "Sreyan",
     "Ghosh"
    ],
    [
     "Sonal",
     "Kumar"
    ],
    [
     "Yaman",
     "Kumar"
    ],
    [
     "Rajiv",
     "Ratn Shah"
    ],
    [
     "Srinivasan",
     "Umesh"
    ]
   ],
   "title": "Span Classification with Structured Information for Disfluency Detection in Spoken Utterances",
   "original": "11242",
   "page_count": 5,
   "order": 811,
   "p1": 3998,
   "pn": 4002,
   "abstract": [
    "Existing approaches in disfluency detection focus on solving a token-level classification task for identifying and removing disfluencies in text. Moreover, most works focus on leveraging only contextual information captured by the linear sequences in text, thus ignoring the structured information in the text which is efficiently captured by dependency trees. In this paper, building on the span classification paradigm of entity recognition, we propose a novel architecture for detecting disfluencies in transcripts from spoken utterances, incorporating both contextual information through transformers and long-distance structured information captured by dependency trees, through graph convolutional networks (GCNs). Experimental results show that our proposed model achieves state-of-the-art results on the widely used English Switchboard dataset for disfluency detection and outperforms prior-art by a significant margin. We make all our codes publicly available on GitHub."
   ],
   "doi": "10.21437/Interspeech.2022-11242"
  },
  "belitz22_interspeech": {
   "authors": [
    [
     "Chelzy",
     "Belitz"
    ],
    [
     "John H.L.",
     "Hansen"
    ]
   ],
   "title": "Challenges in Metadata Creation for Massive Naturalistic Team-Based Audio Data",
   "original": "11243",
   "page_count": 5,
   "order": 1054,
   "p1": 5210,
   "pn": 5214,
   "abstract": [
    "A broad range of research fields benefit from the information extracted from naturalistic audio data. Speech research typically relies on the availability of human-generated metadata tags to comprise a set of “ground truth” labels for the development of speech processing algorithms. While the manual generation of metadata tags may be feasible on a small scale, unique problems arise when creating speech resources for massive, naturalistic audio data. This paper presents a general discussion on these challenges and highlights suggestions when creating metadata for speech resources that are intended to be useful both in speech research and in numerous other fields such as psychology, history, and audio archiving/preservation. Further, it provides an overview of how the task of creating a speech resource for various communities has been and is continuing to be approached for the massive corpus of audio from the historic NASA Apollo missions, which includes tens of thousands of hours of naturalistic, team-based audio data featuring numerous speakers across multiple points in history."
   ],
   "doi": "10.21437/Interspeech.2022-11243"
  },
  "tseng22_interspeech": {
   "authors": [
    [
     "Wei-Cheng",
     "Tseng"
    ],
    [
     "Wei-Tsung",
     "Kao"
    ],
    [
     "Hung-yi",
     "Lee"
    ]
   ],
   "title": "Membership Inference Attacks Against Self-supervised Speech Models",
   "original": "11245",
   "page_count": 5,
   "order": 1020,
   "p1": 5040,
   "pn": 5044,
   "abstract": [
    "Recently, adapting the idea of self-supervised learning (SSL) on continuous speech has started gaining attention. SSL models pre-trained on a huge amount of unlabeled audio can generate general-purpose representations that benefit a wide variety of speech processing tasks. Despite their ubiquitous deployment, however, the potential privacy risks of these models have not been well investigated. In this paper, we present the first privacy analysis on several SSL speech models using Membership Inference Attacks (MIA) under black-box access. The experiment results show that these pre-trained models are vulnerable to MIA and prone to membership information leakage with high Area Under the Curve (AUC) in both utterance-level and speaker-level. Furthermore, we also conduct several ablation studies to understand the factors that contribute to the success of MIA."
   ],
   "doi": "10.21437/Interspeech.2022-11245"
  },
  "tseng22b_interspeech": {
   "authors": [
    [
     "Wei-Cheng",
     "Tseng"
    ],
    [
     "Wei-Tsung",
     "Kao"
    ],
    [
     "Hung-yi",
     "Lee"
    ]
   ],
   "title": "DDOS: A MOS Prediction Framework utilizing Domain Adaptive Pre-training and Distribution of Opinion Scores",
   "original": "11247",
   "page_count": 5,
   "order": 920,
   "p1": 4541,
   "pn": 4545,
   "abstract": [
    "Mean opinion score (MOS) is a typical subjective evaluation metric for speech synthesis systems. Since collecting MOS is time-consuming, it would be desirable if there are accurate MOS prediction models for automatic evaluation. In this work, we propose DDOS, a novel MOS prediction model. DDOS utilizes domain-adaptive pre-training to further pre-train self-supervised learning models on synthetic speech. And a proposed module is added to model the opinion score distribution of each utterance. With the proposed components, DDOS outperforms previous works on BVCC dataset. And the zero-shot transfer result on BC2019 dataset is significantly improved. DDOS also wins second place in Interspeech 2022 VoiceMOS challenge in terms of system-level score."
   ],
   "doi": "10.21437/Interspeech.2022-11247"
  },
  "zhang22da_interspeech": {
   "authors": [
    [
     "Chao",
     "Zhang"
    ],
    [
     "Bo",
     "Li"
    ],
    [
     "Tara",
     "Sainath"
    ],
    [
     "Trevor",
     "Strohman"
    ],
    [
     "Sepand",
     "Mavandadi"
    ],
    [
     "Shuo-Yiin",
     "Chang"
    ],
    [
     "Parisa",
     "Haghani"
    ]
   ],
   "title": "Streaming End-to-End Multilingual Speech Recognition with Joint Language Identification",
   "original": "11249",
   "page_count": 5,
   "order": 653,
   "p1": 3223,
   "pn": 3227,
   "abstract": [
    "Language identification is critical for many downstream tasks in automatic speech recognition (ASR), and is beneficial to integrate into multilingual end-to-end ASR as an additional task. In this paper, we propose to modify the structure of the cascaded-encoder-based recurrent neural network transducer (RNN-T) model by integrating a per-frame language identifier (LID) predictor. RNN-T with cascaded encoders can achieve streaming ASR with low latency using first-pass decoding with no right-context, and achieve lower word error rates (WERs) using second-pass decoding with longer right-context. By leveraging such differences in the right-contexts and a streaming implementation of statistics pooling, the proposed method can achieve accurate streaming LID prediction with little extra test-time cost. Experimental results on a voice search dataset with 9 language locales shows that the proposed method achieves an average of 96.2% LID prediction accuracy and the same second-pass WER as that obtained by including oracle LID in the input."
   ],
   "doi": "10.21437/Interspeech.2022-11249"
  },
  "delcroix22_interspeech": {
   "authors": [
    [
     "Marc",
     "Delcroix"
    ],
    [
     "Keisuke",
     "Kinoshita"
    ],
    [
     "Tsubasa",
     "Ochiai"
    ],
    [
     "Katerina",
     "Zmolikova"
    ],
    [
     "Hiroshi",
     "Sato"
    ],
    [
     "Tomohiro",
     "Nakatani"
    ]
   ],
   "title": "Listen only to me! How well can target speech extraction handle false alarms?",
   "original": "11252",
   "page_count": 5,
   "order": 44,
   "p1": 216,
   "pn": 220,
   "abstract": [
    "Target speech extraction (TSE) extracts the speech of a target speaker in a mixture given auxiliary clues characterizing the speaker, such as an enrollment utterance. TSE addresses thus the challenging problem of simultaneously performing separation and speaker identification. There has been much progress in extraction performance following the recent development of neural networks for speech enhancement and separation. Most studies have focused on processing mixtures where the target speaker is actively speaking. However, the target speaker is sometimes silent in practice, i.e., inactive speaker (IS). A typical TSE system will tend to output a signal in IS cases, causing false alarms. This is a severe problem for the practical deployment of TSE systems. This paper aims at understanding better how well TSE systems can handle IS cases. We consider two approaches to deal with IS, (1) training a system to directly output zero signals or (2) detecting IS with an extra speaker verification module. We perform an extensive experimental comparison of these schemes in terms of extraction performance and IS detection using the LibriMix dataset and reveal their pros and cons."
   ],
   "doi": "10.21437/Interspeech.2022-11252"
  },
  "miao22b_interspeech": {
   "authors": [
    [
     "Chenfeng",
     "Miao"
    ],
    [
     "Ting",
     "Chen"
    ],
    [
     "Minchuan",
     "Chen"
    ],
    [
     "Jun",
     "Ma"
    ],
    [
     "Shaojun",
     "Wang"
    ],
    [
     "Jing",
     "Xiao"
    ]
   ],
   "title": "A compact transformer-based GAN vocoder",
   "original": "11254",
   "page_count": 5,
   "order": 332,
   "p1": 1636,
   "pn": 1640,
   "abstract": [
    "Recent work has shown that self-attention module in Transformer architecture is an effective way of modeling natural languages and images. In this work, we propose a novel way for audio synthesis using Self-Attention Network (SAN). To the best of our knowledge, there is no successful application of Transformer architecture or SAN in high-fidelity waveform generation tasks. The main challenge in adapting SAN to audio generation tasks lies in its quadratic growth of the computational complexity with respect to the input sequence length, making it impractical with high-resolution audio tasks. To tackle this problem, we apply dilated sliding window to vanilla SAN. This technique enables our model to have large receptive field, linear computational complexity and extremely small footprint. We experimentally show that the proposed model archives smaller model size, while producing audio samples with comparable speech quality in comparison with the best publicly available model. In particular, our small footprint model has only 0.57M parameters and can generate 22.05kHz high-fidelity audio 113 times faster than real-time on a NVIDIA V100 GPU without engineered inference kernels."
   ],
   "doi": "10.21437/Interspeech.2022-11254"
  },
  "agarwal22_interspeech": {
   "authors": [
    [
     "Shrutina",
     "Agarwal"
    ],
    [
     "Naoya",
     "Takahashi"
    ],
    [
     "Sriram",
     "Ganapathy"
    ]
   ],
   "title": "Leveraging Symmetrical Convolutional Transformer Networks for Speech to Singing Voice Style Transfer",
   "original": "11256",
   "page_count": 5,
   "order": 611,
   "p1": 3013,
   "pn": 3017,
   "abstract": [
    "In this paper, we propose a model to perform style transfer of speech to singing voice. Contrary to the previous signal processing-based methods, which require high-quality singing templates or phoneme synchronization, we explore a data-driven approach for the problem of converting natural speech to singing voice. We develop a novel neural network architecture, called SymNet, which models the alignment of the input speech with the target melody while preserving the speaker identity and naturalness. The proposed SymNet model is comprised of symmetrical stack of three types of layers - convolutional, transformer, and self-attention layers. The paper also explores novel data augmentation and generative loss annealing methods to facilitate the model training. Experiments are performed on the NUS and NHSS datasets which consist of parallel data of speech and singing voice. In these experiments, we show that the proposed SymNet model improves the objective reconstruction quality significantly over the previously published methods and baseline architectures. Further, a subjective listening test confirms the improved quality of the audio obtained using the proposed approach (absolute improvement of 0.37 in mean opinion score measure over the baseline system)."
   ],
   "doi": "10.21437/Interspeech.2022-11256"
  },
  "jun22_interspeech": {
   "authors": [
    [
     "Sun-Ah",
     "Jun"
    ],
    [
     "Maria Luisa",
     "Zubizarreta"
    ]
   ],
   "title": "Paraguayan Guarani: Tritonal pitch accent and Accentual Phrase",
   "original": "11257",
   "page_count": 5,
   "order": 1075,
   "p1": 5303,
   "pn": 5307,
   "abstract": [
    "This paper investigates the intonation system of Paraguayan Guarani in the Autosegmental-metrical (AM) framework of intonational phonology. Previous work on Guarani intonation stated that Guarani has two types of pitch accent, rising (L*+H or LH) and falling (H+L* or HL), and there is no prosodic unit between a word and an Intonational Phrase. But these findings seem to have resulted from the limitation of the data examined. When longer words/sentences and various syntactic structures are examined, it was found that Guarani has one type of pitch accent, a tritonal HLH*, and has an Accentual Phrase (AP). The tonal pattern of AP is /H HLH* Ha/, i.e., it has one pitch accent and its edges are marked by a H tone. However, because the pitch accent is tritonal, AP edge tones are realized only when there are unstressed syllables before and after the syllables carrying the tritonal pitch accent, suggesting that the function of AP boundary tone is not marking word prominence as in other AP languages. Instead, an important function of Guarani AP seems to mark specific syntactic categories and groupings. These findings are compared with other AP languages and discussed in terms of the typology of word-prominence type."
   ],
   "doi": "10.21437/Interspeech.2022-11257"
  },
  "dissanayake22_interspeech": {
   "authors": [
    [
     "Vipula",
     "Dissanayake"
    ],
    [
     "Sachith",
     "Seneviratne"
    ],
    [
     "Hussel",
     "Suriyaarachchi"
    ],
    [
     "Elliott",
     "Wen"
    ],
    [
     "Suranga",
     "Nanayakkara"
    ]
   ],
   "title": "Self-supervised Representation Fusion for Speech and Wearable Based Emotion Recognition",
   "original": "11258",
   "page_count": 5,
   "order": 728,
   "p1": 3598,
   "pn": 3602,
   "abstract": [
    "Even with modern-day advanced machine learning techniques, Speech Emotion Recognition (SER) is a challenging task. Speech signals alone might not provide enough information to build robust emotion recognition models. The widespread usage of wearable devices provides multiple signal streams containing physiological and contextual cues, which could be incredibly beneficial to improving an SER system. However, research around multimodal emotion recognition with wearable and speech signals is limited. Also, the scarcity of annotated data for such scenarios limits the applicability of deep learning techniques. This paper presents a self-supervised fusion method for speech and wearable signals and evaluates its usage in the SER context. We further discuss three different fusion techniques in the context of multimodal emotion recognition. Our evaluations show that pretraining in the fusion stage significantly impacts the downstream emotion recognition task. Our method was able to achieve F1 Scores of 82.59% (arousal), 83.05% (valence) and 72.95% (emotion categories) for K-EmoCon dataset."
   ],
   "doi": "10.21437/Interspeech.2022-11258"
  },
  "miao22c_interspeech": {
   "authors": [
    [
     "Chenfeng",
     "Miao"
    ],
    [
     "Kun",
     "Zou"
    ],
    [
     "Ziyang",
     "Zhuang"
    ],
    [
     "Tao",
     "Wei"
    ],
    [
     "Jun",
     "Ma"
    ],
    [
     "Shaojun",
     "Wang"
    ],
    [
     "Jing",
     "Xiao"
    ]
   ],
   "title": "Towards Efficiently Learning Monotonic Alignments for Attention-based End-to-End Speech Recognition",
   "original": "11259",
   "page_count": 5,
   "order": 214,
   "p1": 1051,
   "pn": 1055,
   "abstract": [
    "Inspired by EfficientTTS, a recent proposed speech synthesis model, we propose a new way to train end-to-end speech recognition models with an additional training objective, allowing the models to learn the monotonic alignments effectively and efficiently. The introduced training objective is differential, computationally cheap and most importantly, of no constraint on network structures. Thus, it is quite convenient to be incorporated into any speech recognition model. Through extensive experiments, we observed that the performance of our models significantly outperform baseline models. Specifically, our best performing model achieves WER (Word Error Rate) 3.18% on LibriSpeech test-clean benchmark and 8.41% on test-other. Comparing with a strong baseline obtained by WeNet, the proposed model gets 7.6% relative WER reduction on test-clean and 6.9% on test-other."
   ],
   "doi": "10.21437/Interspeech.2022-11259"
  },
  "chien22b_interspeech": {
   "authors": [
    [
     "Chin-Yueh",
     "Chien"
    ],
    [
     "Kuan-Yu",
     "Chen"
    ]
   ],
   "title": "A BERT-based Language Modeling Framework",
   "original": "11266",
   "page_count": 5,
   "order": 141,
   "p1": 699,
   "pn": 703,
   "abstract": [
    "Deep learning has brought considerable changes and created a new paradigm in many research areas, including computer vision, speech processing, and natural language processing. In the context of language modeling, recurrent-based language models and word embedding methods have been pivotal studies in the past decade. Recently, pre-trained language models have attracted widespread attention due to their enormous success in many challenging tasks. However, only a dearth of works concentrates on creating novel language models based on the pre-trained models. In order to bridge the research gap, we take the bidirectional encoder representations from Transformers (BERT) model as an example to explore novel uses of a pre-trained model for language modeling. More formally, this paper proposes a set of BERT-based language models, and a neural-based dynamic adaptation method is also introduced to combine these language models systematically and methodically. We conduct comprehensive studies on three datasets for the perplexity evaluation. Experiments show that the proposed framework achieves 11%, 39%, and 5% relative improvements over the baseline model for Penn Treebank, Wikitext 2, and Tedlium Release 2 corpora, respectively. Besides, when applied to rerank n-best lists from a speech recognizer, our framework also yields promising results compared with baseline systems."
   ],
   "doi": "10.21437/Interspeech.2022-11266"
  },
  "rajchetupalli22_interspeech": {
   "authors": [
    [
     "Srikanth",
     "Raj Chetupalli"
    ],
    [
     "Sriram",
     "Ganapathy"
    ]
   ],
   "title": "Speaker conditioned acoustic modeling for multi-speaker conversational ASR",
   "original": "11267",
   "page_count": 5,
   "order": 777,
   "p1": 3834,
   "pn": 3838,
   "abstract": [
    "In this paper, we propose a novel approach for the transcription of speech conversations with natural speaker overlap, from single channel speech recordings. The proposed model is a combination of a speaker diarization system and a hybrid automatic speech recognition (ASR) system. The speaker conditioned acoustic model (SCAM) in the ASR system consists of a series of embedding layers which use the speaker activity inputs from the diarization system to derive speaker specific embeddings. The output of the SCAM are speaker specific senones that are used for decoding the transcripts for each speaker in the conversation. In this work, we experiment with the automatic speaker activity decisions generated using an end-to-end speaker diarization system. A joint learning approach is also proposed where the diarization model and the ASR acoustic model are jointly optimized. The experiments are performed on the mixed-channel two speaker recordings from the Switchboard corpus of telephone conversations. In these experiments, we show that the proposed acoustic model, incorporating speaker activity decisions and joint optimization, improves significantly over the ASR system with explicit source filtering (relative improvements of 12% in word error rate (WER) over the baseline system)."
   ],
   "doi": "10.21437/Interspeech.2022-11267"
  },
  "shi22e_interspeech": {
   "authors": [
    [
     "Hao",
     "Shi"
    ],
    [
     "Longbiao",
     "Wang"
    ],
    [
     "Sheng",
     "Li"
    ],
    [
     "Jianwu",
     "Dang"
    ],
    [
     "Tatsuya",
     "Kawahara"
    ]
   ],
   "title": "Monaural Speech Enhancement Based on Spectrogram Decomposition for Convolutional Neural Network-sensitive Feature Extraction",
   "original": "11268",
   "page_count": 5,
   "order": 45,
   "p1": 221,
   "pn": 225,
   "abstract": [
    "Many state-of-the-art speech enhancement (SE) systems have recently used convolutional neural networks (CNNs) to extract multi-scale feature maps. However, CNN relies more on local texture than global shape, which is more susceptible to degraded spectrogram and may fail to capture the detailed structure of speech. Although some two-stage systems feed the first-stage enhanced and original noisy spectrograms to the second stage simultaneously, this does not guarantee sufficient guidance for the second stage since the first-stage spectrogram can not provide precise spectral details. In order to allow CNNs to perceive clear speech component boundary information, we compose feature maps with spectrograms containing evident speech components according to the mask value from the first stage. The positions corresponding to the mask greater than certain thresholds are extracted as feature maps. These feature maps make the boundary information of speech components obvious by ignoring others, thus making CNNs sensitive to input features. Experiments on the VB dataset show that with a proper decomposition numbers, the proposed method can enhance SE performance, which can provide 0.15 PESQ improvement. Besides, the proposed method is more effective for spectral detail recovery."
   ],
   "doi": "10.21437/Interspeech.2022-11268"
  },
  "jigang22_interspeech": {
   "authors": [
    [
     "Ren",
     "Jigang"
    ],
    [
     "Mao",
     "Qirong"
    ]
   ],
   "title": "DCTCN:Deep Complex Temporal Convolutional Network for Long Time Speech Enhancement",
   "original": "11269",
   "page_count": 5,
   "order": 1110,
   "p1": 5478,
   "pn": 5482,
   "abstract": [
    "Recently, with the rapid development of deep learning, the performance of Monaural speech enhancement (SE) in terms of intelligibility and speech quality has been significantly improved. In time-frequency (TF) domain, we generally use convolutional neural networks (CNN) to predict the mask from the noisy amplitude spectrum to the pure amplitude spectrum. Deep complex convolution recurrent network (DCCRN) uses the algorithm of complex numbers to process convolutional networks and long short-term memory(LSTM), and has achieved good results. However, LSTM can only model short time frames, and its performance is often not good enough when processing information on longer time frames. The single convolution kernel size of encoder-deocder also limits the ability of model to extract and restore features. In this paper, we design a new network to handle these problems, called Deep Complex Temporal Convolution (DCTCN), where temporal convolution network (TCN) using the rule of complex calculation. The Encoder and Decoder use selective kernel network (SkNet) to capture multi-scale receptive field in the encoding and decoding phase. On the TIMIT and VoiceBank+DEMAND datasets, our model obtains very competitive results in semi causal and non causal tasks compared with previous models."
   ],
   "doi": "10.21437/Interspeech.2022-11269"
  },
  "jung22c_interspeech": {
   "authors": [
    [
     "Jee-weon",
     "Jung"
    ],
    [
     "Hemlata",
     "Tak"
    ],
    [
     "Hye-jin",
     "Shim"
    ],
    [
     "Hee-Soo",
     "Heo"
    ],
    [
     "Bong-Jin",
     "Lee"
    ],
    [
     "Soo-Whan",
     "Chung"
    ],
    [
     "Ha-Jin",
     "Yu"
    ],
    [
     "Nicholas",
     "Evans"
    ],
    [
     "Tomi",
     "Kinnunen"
    ]
   ],
   "title": "SASV 2022: The First Spoofing-Aware Speaker Verification Challenge",
   "original": "11270",
   "page_count": 5,
   "order": 587,
   "p1": 2893,
   "pn": 2897,
   "abstract": [
    "The first spoofing-aware speaker verification (SASV) challenge aims to integrate research efforts in speaker verification and anti-spoofing. We extend the speaker verification scenario by introducing spoofed trials to the usual set of target and non-target trials. In contrast to the established ASVspoof challenge where the focus is upon separate, independently optimised spoofing detection and speaker verification sub-systems, SASV targets the development of integrated and jointly optimised solutions. Pre-trained spoofing detection and speaker verification models are provided as open source and are used in two baseline SASV solutions. Both models and baselines are freely available to participants and can be used to develop back-end fusion approaches or end-to-end solutions. Using the provided common evaluation protocol, 23 teams submitted SASV solutions. When assessed with target, non-target and spoofed trials, the best performing system reduces the equal error rate of a conventional speaker verification system from 23.83% to 0.13%. SASV challenge results are testament to the reliability of today's state-of-the-art approaches to spoofing detection and speaker verification."
   ],
   "doi": "10.21437/Interspeech.2022-11270"
  },
  "meng22c_interspeech": {
   "authors": [
    [
     "Yi",
     "Meng"
    ],
    [
     "Xiang",
     "Li"
    ],
    [
     "Zhiyong",
     "Wu"
    ],
    [
     "Tingtian",
     "Li"
    ],
    [
     "Zixun",
     "Sun"
    ],
    [
     "Xinyu",
     "Xiao"
    ],
    [
     "Chi",
     "Sun"
    ],
    [
     "Hui",
     "Zhan"
    ],
    [
     "Helen",
     "Meng"
    ]
   ],
   "title": "CALM: Constrastive Cross-modal Speaking Style Modeling for Expressive Text-to-Speech Synthesis",
   "original": "11275",
   "page_count": 5,
   "order": 1121,
   "p1": 5533,
   "pn": 5537,
   "abstract": [
    "To further improve the speaking styles of synthesized speeches, current text-to-speech (TTS) synthesis systems commonly employ reference speeches to stylize their outputs instead of just the input texts. These reference speeches are obtained by manual selection which is resource-consuming, or selected by semantic features. However, semantic features contain not only style-related information, but also style irrelevant information. The information irrelevant to speaking style in the text could interfere the reference audio selection and result in improper speaking styles. To improve the reference selection, we propose Contrastive Acoustic-Linguistic Module (CALM) to extract the Style-related Text Feature (STF) from the text. CALM optimizes the correlation between the speaking style embedding and the extracted STF with contrastive learning. Thus, a certain number of the most appropriate reference speeches for the input text are selected by retrieving the speeches with the top STF similarities. Then the style embeddings are weighted summarized according to their STF similarities and used to stylize the synthesized speech of TTS. Experiment results demonstrate the effectiveness of our proposed approach, with both objective evaluations and subjective evaluations on the speaking styles of the synthesized speeches outperform a baseline approach with semantic-feature-based reference selection."
   ],
   "doi": "10.21437/Interspeech.2022-11275"
  },
  "komatsu22_interspeech": {
   "authors": [
    [
     "Tatsuya",
     "Komatsu"
    ],
    [
     "Yusuke",
     "Fujita"
    ],
    [
     "Jaesong",
     "Lee"
    ],
    [
     "Lukas",
     "Lee"
    ],
    [
     "Shinji",
     "Watanabe"
    ],
    [
     "Yusuke",
     "Kida"
    ]
   ],
   "title": "Better Intermediates Improve CTC Inference",
   "original": "11276",
   "page_count": 5,
   "order": 1005,
   "p1": 4965,
   "pn": 4969,
   "abstract": [
    "This paper proposes a method for improved CTC inference with searched intermediates and multi-pass conditioning. The paper first formulates self-conditioned CTC as a probabilistic model with an intermediate prediction as a latent representation and provides a tractable conditioning framework. We then propose two new conditioning methods based on the new formulation: (1) Searched intermediate conditioning that refines intermediate predictions with beam-search, (2) Multi-pass conditioning that uses predictions of previous inference for conditioning the next inference. These new approaches enable better conditioning than the original self-conditioned CTC during inference and improve the final performance. Experiments with the LibriSpeech dataset show relative 3%/12% performance improvement at the maximum in test clean/other sets compared to the original self-conditioned CTC."
   ],
   "doi": "10.21437/Interspeech.2022-11276"
  },
  "kothapally22b_interspeech": {
   "authors": [
    [
     "Vinay",
     "Kothapally"
    ],
    [
     "John H.L.",
     "Hansen"
    ]
   ],
   "title": "Complex-Valued Time-Frequency Self-Attention for Speech Dereverberation",
   "original": "11277",
   "page_count": 5,
   "order": 517,
   "p1": 2543,
   "pn": 2547,
   "abstract": [
    "Several speech processing systems have demonstrated considerable performance improvements when deep complex neural networks (DCNN) are coupled with self-attention (SA) networks. However, the majority of DCNN-based studies on speech dereverberation that employ self-attention do not explicitly account for the inter-dependencies between real and imaginary features when computing attention. In this study, we propose a complex-valued T-F attention (TFA) module that models spectral and temporal dependencies by computing two-dimensional attention maps across time and frequency dimensions. We validate the effectiveness of our proposed complex-valued TFA module with the deep complex convolutional recurrent network (DCCRN) using the REVERB challenge corpus. Experimental findings indicate that integrating our complex-TFA module with DCCRN improves overall speech quality and performance of back-end speech applications, such as automatic speech recognition, compared to earlier approaches for self-attention."
   ],
   "doi": "10.21437/Interspeech.2022-11277"
  },
  "terashima22_interspeech": {
   "authors": [
    [
     "Ryo",
     "Terashima"
    ],
    [
     "Ryuichi",
     "Yamamoto"
    ],
    [
     "Eunwoo",
     "Song"
    ],
    [
     "Yuma",
     "Shirahata"
    ],
    [
     "Hyun-Wook",
     "Yoon"
    ],
    [
     "Jae-Min",
     "Kim"
    ],
    [
     "Kentaro",
     "Tachibana"
    ]
   ],
   "title": "Cross-Speaker Emotion Transfer for Low-Resource Text-to-Speech Using Non-Parallel Voice Conversion with Pitch-Shift Data Augmentation",
   "original": "11278",
   "page_count": 5,
   "order": 612,
   "p1": 3018,
   "pn": 3022,
   "abstract": [
    "Data augmentation via voice conversion (VC) has been successfully applied to low-resource expressive text-to-speech (TTS) when only neutral data for the target speaker are available. Although the quality of VC is crucial for this approach, it is challenging to learn a stable VC model because the amount of data is limited in low-resource scenarios, and highly expressive speech has large acoustic variety. To address this issue, we propose a novel data augmentation method that combines pitch-shifting and VC techniques. Because pitch-shift data augmentation enables the coverage of a variety of pitch dynamics, it greatly stabilizes training for both VC and TTS models, even when only 1,000 utterances of the target speaker's neutral data are available. Subjective test results showed that a FastSpeech 2-based emotional TTS system with the proposed method improved naturalness and emotional similarity compared with conventional methods."
   ],
   "doi": "10.21437/Interspeech.2022-11278"
  },
  "shen22b_interspeech": {
   "authors": [
    [
     "Peng",
     "Shen"
    ],
    [
     "Xugang",
     "Lu"
    ],
    [
     "Hisashi",
     "Kawai"
    ]
   ],
   "title": "Transducer-based language embedding for spoken language identification",
   "original": "11281",
   "page_count": 5,
   "order": 755,
   "p1": 3724,
   "pn": 3728,
   "abstract": [
    "The acoustic and linguistic features are important cues for the spoken language identification (LID) task. Recent advanced LID systems mainly use acoustic features that lack the usage of explicit linguistic feature encoding. In this paper, we propose a novel transducer-based language embedding approach for LID tasks by integrating an RNN transducer model into a language embedding framework. Benefiting from the advantages of the RNN transducer's linguistic representation capability, the proposed method can exploit both phonetically-aware acoustic features and explicit linguistic features for LID tasks. Experiments were carried out on the large-scale multilingual LibriSpeech and VoxLingua107 datasets. Experimental results showed the proposed method significantly improves the performance on LID tasks with 12% to 59% and 16% to 24% relative improvement on in-domain and cross-domain datasets, respectively."
   ],
   "doi": "10.21437/Interspeech.2022-11281"
  },
  "takashima22_interspeech": {
   "authors": [
    [
     "Yuki",
     "Takashima"
    ],
    [
     "Shota",
     "Horiguchi"
    ],
    [
     "Shinji",
     "Watanabe"
    ],
    [
     "Leibny Paola Garcia",
     "Perera"
    ],
    [
     "Yohei",
     "Kawaguchi"
    ]
   ],
   "title": "Updating Only Encoders Prevents Catastrophic Forgetting of End-to-End ASR Models",
   "original": "11282",
   "page_count": 5,
   "order": 452,
   "p1": 2218,
   "pn": 2222,
   "abstract": [
    "In this paper, we present an incremental domain adaptation technique to prevent catastrophic forgetting for an end-to-end automatic speech recognition (ASR) model. Conventional approaches require extra parameters of the same size as the model for optimization, and it is difficult to apply these approaches to end-to-end ASR models because they have a huge amount of parameters. To solve this problem, we first investigate which parts of end-to-end ASR models contribute to high accuracy in the target domain while preventing catastrophic forgetting. We conduct experiments on incremental domain adaptation from the LibriSpeech dataset to the AMI meeting corpus with two popular end-to-end ASR models and found that adapting only the linear layers of their encoders can prevent catastrophic forgetting. Then, on the basis of this finding, we develop an element-wise parameter selection focused on specific layers to further reduce the number of fine-tuning parameters. Experimental results show that our approach consistently prevents catastrophic forgetting compared to parameter selection from the whole model."
   ],
   "doi": "10.21437/Interspeech.2022-11282"
  },
  "nakagome22_interspeech": {
   "authors": [
    [
     "Yu",
     "Nakagome"
    ],
    [
     "Tatsuya",
     "Komatsu"
    ],
    [
     "Yusuke",
     "Fujita"
    ],
    [
     "Shuta",
     "Ichimura"
    ],
    [
     "Yusuke",
     "Kida"
    ]
   ],
   "title": "InterAug: Augmenting Noisy Intermediate Predictions for CTC-based ASR",
   "original": "11284",
   "page_count": 5,
   "order": 1040,
   "p1": 5140,
   "pn": 5144,
   "abstract": [
    "This paper proposes InterAug: a novel training method for CTC-based ASR using augmented intermediate representations for conditioning. The proposed method exploits the conditioning framework of self-conditioned CTC to train robust models by conditioning with ``noisy'' intermediate predictions. During the training, intermediate predictions are changed to incorrect intermediate predictions and fed into the next layer for conditioning. The subsequent layers are trained to correct the incorrect intermediate predictions with the intermediate losses. By repeating the augmentation and the correction, iterative refinements, which generally require a special decoder, can be realized only with the audio encoder. To produce noisy intermediate predictions, we also introduce new augmentation: intermediate feature space augmentation and intermediate token space augmentation that are designed to simulate typical errors. The combination of the proposed InterAug framework with new augmentation allows explicit training of the robust audio encoders. In experiments using augmentations simulating deletion, insertion, and substitution error, we confirmed that the trained model acquires robustness to each error, boosting the speech recognition performance of the strong self-conditioned CTC baseline."
   ],
   "doi": "10.21437/Interspeech.2022-11284"
  },
  "liesenfeld22_interspeech": {
   "authors": [
    [
     "Andreas",
     "Liesenfeld"
    ],
    [
     "Mark",
     "Dingemanse"
    ]
   ],
   "title": "Bottom-up discovery of structure and variation in response tokens (‘backchannels’) across diverse languages",
   "original": "11288",
   "page_count": 5,
   "order": 229,
   "p1": 1126,
   "pn": 1130,
   "abstract": [
    "Response tokens (also known as backchannels, continuers, or feedback) are a frequent feature of human interaction, where they serve to display understanding and streamline turn-taking. We propose a bottom-up method to study responsive behaviour across 16 languages (8 language families). We use sequential context and recurrence of turns formats to identify candidate response tokens in a language-agnostic way across diverse conversational corpora. We then use UMAP clustering directly on speech signals to represent structure and variation. We find that (i) written orthographic annotations underrepresent the attested variation, (ii) distinctions between formats can be gradient rather than discrete, (iii) most languages appear to make available a broad distinction between a minimal nasal format ‘mm' and a fuller ‘yeah'-like format. Charting this aspect of human interaction contributes to our understanding of interactional infrastructure across languages and can inform the design of speech technologies."
   ],
   "doi": "10.21437/Interspeech.2022-11288"
  },
  "mukherjee22_interspeech": {
   "authors": [
    [
     "Arijit",
     "Mukherjee"
    ],
    [
     "Shubham",
     "Bansal"
    ],
    [
     "Sandeepkumar",
     "Satpal"
    ],
    [
     "Rupesh",
     "Mehta"
    ]
   ],
   "title": "Text aware Emotional Text-to-speech with BERT",
   "original": "11293",
   "page_count": 5,
   "order": 932,
   "p1": 4601,
   "pn": 4605,
   "abstract": [
    "Emotional text to speech is the idea of synthesizing emotional audio via a text-to-speech model. With neural text-to-speech, sentence-level naturalness has improved a lot and is almost at par with human speech, but the current approach to emotional text-to-speech models heavily relies on the user to input the expected emotion along with the text to synthesize the desired speech. In this work, we propose a novel text-aware emotional text-to-speech system that leverages a pre-trained BERT model to get a deep representation of the emotional context from the text both during training and inference. We show that our proposed method synthesizes emotional audio with emotion depending on the emotional context of the input text. We also show that our method outperforms baseline systems in varying the emotional intensity depending on the text."
   ],
   "doi": "10.21437/Interspeech.2022-11293"
  },
  "velichko22_interspeech": {
   "authors": [
    [
     "Alena",
     "Velichko"
    ],
    [
     "Maxim",
     "Markitantov"
    ],
    [
     "Heysem",
     "Kaya"
    ],
    [
     "Alexey",
     "Karpov"
    ]
   ],
   "title": "Complex Paralinguistic Analysis of Speech: Predicting Gender, Emotions and Deception in a Hierarchical Framework",
   "original": "11294",
   "page_count": 5,
   "order": 959,
   "p1": 4735,
   "pn": 4739,
   "abstract": [
    "In this paper, we present a hierarchical framework for complex paralinguistic analysis of speech including gender, emotions and deception recognition. The main idea of the framework is built upon the research on interrelation between various paralinguistic phenomena. It uses gender information to predict emotional states, and the outcome of the emotion recognition to predict the truthfulness of the speech. We use multiple datasets (aGender, Ruslana, EmoDB and DSD) to perform within-corpus and cross-corpus experiments using various performance measures. The experimental results reveal that gender-specific models improve the effectiveness of automatic speech emotion recognition in terms of Unweighted Average Recall up to an absolute 5.7%, and the integration of emotion predictions improves the F-score of automatic deception detection compared to our baseline by an absolute 4.7%. The obtained cross-validation results of 88.4 +/- 1.5% for deception detection beat the existing state-of-the-art by an absolute 2.8%."
   ],
   "doi": "10.21437/Interspeech.2022-11294"
  },
  "chen22t_interspeech": {
   "authors": [
    [
     "Lianwu",
     "Chen"
    ],
    [
     "Xinlei",
     "Ren"
    ],
    [
     "Xu",
     "Zhang"
    ],
    [
     "Xiguang",
     "Zheng"
    ],
    [
     "Chen",
     "Zhang"
    ],
    [
     "Liang",
     "Guo"
    ],
    [
     "Bing",
     "Yu"
    ]
   ],
   "title": "Impairment Representation Learning for Speech Quality Assessment",
   "original": "11295",
   "page_count": 5,
   "order": 673,
   "p1": 3323,
   "pn": 3327,
   "abstract": [
    "Non-intrusive speech quality assessment has been a crucial task for speech processing. In recent years, methods based on deep neural network have achieved the start-of-the-art performance for non-intrusive speech quality assessment. However, the scarcity of annotated data is usually the main challenge for training robust speech quality assessment networks. In this paper, we proposed an impairment representation learning approach to pre-train the network on a large amount of simulated data without MOS annotation. Then we further fine-tune the pre-trained model for the MOS prediction task on annotated data. The experimental results show that the proposed pre-training methods can significantly improve the performance for speech quality assessment, especially when the annotated training data is limited. Besides, the proposed method significantly outperforms the baseline system of ConferencingSpeech 2022 Challenge."
   ],
   "doi": "10.21437/Interspeech.2022-11295"
  },
  "mohammadamini22_interspeech": {
   "authors": [
    [
     "Mohammad",
     "Mohammadamini"
    ],
    [
     "Driss",
     "Matrouf"
    ],
    [
     "Jean-Francois",
     "Bonastre"
    ],
    [
     "Sandipana",
     "Dowerah"
    ],
    [
     "Romain",
     "Serizel"
    ],
    [
     "Denis",
     "Jouvet"
    ]
   ],
   "title": "Barlow Twins self-supervised learning for robust speaker recognition",
   "original": "11301",
   "page_count": 5,
   "order": 818,
   "p1": 4033,
   "pn": 4037,
   "abstract": [
    "Acoustic noise is a big challenge for speaker recognition systems. The state-of-the-art speaker recognition systems are based on deep neural network speaker embeddings called x-vector extractor. A noise-robust x-vector extractor is highly demanded in speaker recognition systems. In this paper, we introduce Barlow Twins self-supervised loss function in the area of speaker recognition. Barlow Twins objective function tries to optimize two criteria: Firstly, it increases the similarity between two versions of the same signal (i.e. the clean and its augmented noisy version) to make the speaker embedding invariant to the acoustic noise. Secondly, it reduces the redundancy between dimensions of the x-vectors which improves the overall quality of speaker embeddings. In our research, Barlow Twins objective function is integrated with ResNet based speaker embedding system. In the proposed system, the Barlow Twins objective function is calculated in the embedding layer and it is optimized jointly with the speaker classifier loss function. The experimental results on Fabiole corpus show 22 % relative gain in terms of EER in the clean environments and 18% improvement in the presence of low SNR and reverberation."
   ],
   "doi": "10.21437/Interspeech.2022-11301"
  },
  "li22da_interspeech": {
   "authors": [
    [
     "Xu",
     "Li"
    ],
    [
     "Shansong",
     "Liu"
    ],
    [
     "Ying",
     "Shan"
    ]
   ],
   "title": "A Hierarchical Speaker Representation Framework for One-shot Singing Voice Conversion",
   "original": "11305",
   "page_count": 5,
   "order": 873,
   "p1": 4307,
   "pn": 4311,
   "abstract": [
    "Typically, singing voice conversion (SVC) depends on an embedding vector, extracted from either a speaker lookup table (LUT) or a speaker recognition network (SRN), to model speaker identity. However, singing contains more expressive speaker characteristics than conversational speech. It is suspected that a single embedding vector may only capture averaged and coarse-grained speaker characteristics, which is insufficient for the SVC task. To this end, this work proposes a novel hierarchical speaker representation framework for SVC, which can capture fine-grained speaker characteristics at different granularity. It consists of an up-sampling stream and three down-sampling streams. The up-sampling stream transforms the linguistic features into audio samples, while one down-sampling stream of the three operates in the reverse direction. It is expected that the temporal statistics of each down-sampling block can represent speaker characteristics at different granularity, which will be engaged in the up-sampling blocks to enhance the speaker modeling. Experiment results verify that the proposed method outperforms both the LUT and SRN based SVC systems. Moreover, the proposed system supports the one-shot SVC with only a few seconds of reference audio."
   ],
   "doi": "10.21437/Interspeech.2022-11305"
  },
  "takashima22b_interspeech": {
   "authors": [
    [
     "Akihiko",
     "Takashima"
    ],
    [
     "Ryo",
     "Masumura"
    ],
    [
     "Atsushi",
     "Ando"
    ],
    [
     "Yoshihiro",
     "Yamazaki"
    ],
    [
     "Mihiro",
     "Uchida"
    ],
    [
     "Shota",
     "Orihashi"
    ]
   ],
   "title": "Interactive Co-Learning with Cross-Modal Transformer for Audio-Visual Emotion Recognition",
   "original": "11307",
   "page_count": 5,
   "order": 960,
   "p1": 4740,
   "pn": 4744,
   "abstract": [
    "This paper proposes a novel modeling method for audio-visual emotion recognition. Since human emotions are expressed multi-modally, jointly capturing audio and visual cues is a potentially promising approach. In conventional multi-modal modeling methods, a recognition model was trained from an audio-visual paired dataset so as to only enhance audio-visual emotion recognition performance. However, it fails to estimate emotions from single-modal inputs, which indicates they are degraded by overfitting the combinations of the individual modal features. Our supposition is that the ideal form of the emotion recognition is to accurately perform both audio-visual multi-modal processing and single-modal processing with a single model. This is expected to promote utilization of individual modal knowledge for improving audio-visual emotion recognition. Therefore, our proposed method employs a cross-modal transformer model that enables different types of inputs to be handled. In addition, we introduce a novel training method named interactive co-learning; it allows the model to learn knowledge from both and either of the modals. Experiments on a multi-label emotion recognition task demonstrate the effectiveness of the proposed method."
   ],
   "doi": "10.21437/Interspeech.2022-11307"
  },
  "peng22e_interspeech": {
   "authors": [
    [
     "Junyi",
     "Peng"
    ],
    [
     "Rongzhi",
     "Gu"
    ],
    [
     "Ladislav",
     "Mošner"
    ],
    [
     "Oldrich",
     "Plchot"
    ],
    [
     "Lukas",
     "Burget"
    ],
    [
     "Jan",
     "Černocký"
    ]
   ],
   "title": "Learnable Sparse Filterbank for Speaker Verification",
   "original": "11309",
   "page_count": 5,
   "order": 1034,
   "p1": 5110,
   "pn": 5114,
   "abstract": [
    "Recently, feature extraction with learnable filters was extensively investigated with speaker verification systems, with filters learned both in time- and frequency-domains. Most of the learned schemes however end up with filters close to their initialization (e.g. Mel filterbank) or filters strongly limited by their constraints. In this paper, we propose a novel learnable sparse filterbank, named LearnSF, by exclusively optimizing the sparsity of the filterbank, that does not explicitly constrain the filters to follow pre-defined distribution. After standard pre-processing (STFT and square of the magnitude spectrum), the learnable sparse filterbank is employed, with its normalized outputs fed into a neural network predicting the speaker identity. We evaluated the performance of the proposed approach on both VoxCeleb and CNCeleb datasets. The experimental results demonstrate the effectiveness of the proposed LearnSF compared to both widely-used acoustic features and existing parameterized learnable front-ends."
   ],
   "doi": "10.21437/Interspeech.2022-11309"
  },
  "hong22_interspeech": {
   "authors": [
    [
     "Joanna",
     "Hong"
    ],
    [
     "Minsu",
     "Kim"
    ],
    [
     "Daehun",
     "Yoo"
    ],
    [
     "Yong Man",
     "Ro"
    ]
   ],
   "title": "Visual Context-driven Audio Feature Enhancement for Robust End-to-End Audio-Visual Speech Recognition",
   "original": "11311",
   "page_count": 5,
   "order": 576,
   "p1": 2838,
   "pn": 2842,
   "abstract": [
    "This paper focuses on designing a noise-robust end-to-end Audio-Visual Speech Recognition (AVSR) system. To this end, we propose Visual Context-driven Audio Feature Enhancement module (V-CAFE) to enhance the input noisy audio speech with a help of audio-visual correspondence. The proposed V-CAFE is designed to capture the transition of lip movements, namely visual context and to generate a noise reduction mask by considering the obtained visual context. Through context-dependent modeling, the ambiguity in viseme-to-phoneme mapping can be refined for mask generation. The noisy representations are masked out with the noise reduction mask resulting in enhanced audio features. The enhanced audio features are fused with the visual features and taken to an encoder-decoder model composed of Conformer and Transformer for speech recognition. We show the proposed end-to-end AVSR with the V-CAFE can further improve the noise-robustness of AVSR. The effectiveness of the proposed method is evaluated in noisy speech recognition and overlapped speech recognition experiments using the two largest audio-visual datasets, LRS2 and LRS3."
   ],
   "doi": "10.21437/Interspeech.2022-11311"
  },
  "ashihara22_interspeech": {
   "authors": [
    [
     "Takanori",
     "Ashihara"
    ],
    [
     "Takafumi",
     "Moriya"
    ],
    [
     "Kohei",
     "Matsuura"
    ],
    [
     "Tomohiro",
     "Tanaka"
    ]
   ],
   "title": "Deep versus Wide: An Analysis of Student Architectures for Task-Agnostic Knowledge Distillation of Self-Supervised Speech Models",
   "original": "11313",
   "page_count": 5,
   "order": 83,
   "p1": 411,
   "pn": 415,
   "abstract": [
    "Self-supervised learning (SSL) is seen as a very promising approach with high performance for several speech downstream tasks. Since the parameters of SSL models are generally so large that training and inference require a lot of memory and computational cost, it is desirable to produce compact SSL models without a significant performance degradation by applying compression methods such as knowledge distillation (KD). Although the KD approach is able to shrink the depth and/or width of SSL model structures, there has been little research on how varying the depth and width impacts the internal representation of the small-footprint model. This paper provides an empirical study that addresses the question. We investigate the performance on SUPERB while varying the structure and KD methods so as to keep the number of parameters constant; this allows us to analyze the contribution of the representation introduced by varying the model architecture. Experiments demonstrate that a certain depth is essential for solving content-oriented tasks (e.g. automatic speech recognition) accurately, whereas a certain width is necessary for achieving high performance on several speaker-oriented tasks (e.g. speaker identification). Based on these observations, we identify, for SUPERB, a more compressed model with better performance than previous studies."
   ],
   "doi": "10.21437/Interspeech.2022-11313"
  },
  "guillaume22_interspeech": {
   "authors": [
    [
     "Séverine",
     "Guillaume"
    ],
    [
     "Guillaume",
     "Wisniewski"
    ],
    [
     "Benjamin",
     "Galliot"
    ],
    [
     "Minh-Châu",
     "Nguyên"
    ],
    [
     "Maxime",
     "Fily"
    ],
    [
     "Guillaume",
     "Jacques"
    ],
    [
     "Alexis",
     "Michaud"
    ]
   ],
   "title": "Plugging a neural phoneme recognizer into a simple language model: a workflow for low-resource setting",
   "original": "11314",
   "page_count": 5,
   "order": 993,
   "p1": 4905,
   "pn": 4909,
   "abstract": [
    "Recently, several works have shown that fine-tuning a multilingual model of speech representation (typically XLS-R) with very small amounts of annotated data allows for the development of phonemic transcription systems of sufficient quality to help field linguists in their efforts to document the languages of the world. In this work, we explain how the quality of these systems can be improved by a very simple method, namely integrating them with a language model. Our experiments on an endangered language, Japhug (Trans-Himalayan/Tibeto-Burman), show that this appr oach can significantly reduce the WER, reaching the stage of automatic recognition of entire words."
   ],
   "doi": "10.21437/Interspeech.2022-11314"
  },
  "rouhe22_interspeech": {
   "authors": [
    [
     "Aku",
     "Rouhe"
    ],
    [
     "Anja",
     "Virkkunen"
    ],
    [
     "Juho",
     "Leinonen"
    ],
    [
     "Mikko",
     "Kurimo"
    ]
   ],
   "title": "Low Resource Comparison of Attention-based and Hybrid ASR Exploiting wav2vec 2.0",
   "original": "11318",
   "page_count": 5,
   "order": 717,
   "p1": 3543,
   "pn": 3547,
   "abstract": [
    "Low resource speech recognition can potentially benefit a lot from exploiting a pretrained model such as wav2vec 2.0. These pretrained models have learned useful representations in an unsupervised or self-supervised task, often leveraging a very large corpus of untranscribed speech. The pretrained models can then be used in various ways. In this work we compare two approaches which exploit wav2vec 2.0: an attention-based end-to-end model (AED), where the wav2vec 2.0 model is used in the model encoder, and a hybrid hidden Markov model (HMM/DNN) speech recognition system, where the wav2vec 2.0 model is used in the acoustic model. These approaches are compared in a very difficult Northern Sámi task, as well as an easier, simulated low resource task in Finnish. We find that the wav2vec 2.0 AED models can learn a working attention mechanism, but are still outperformed by wav2vec 2.0 HMM/DNN systems. Our best wav2vec 2.0 HMM/DNN recipe on 20 hours is competitive with an HMM/DNN system trained on 1600 hours."
   ],
   "doi": "10.21437/Interspeech.2022-11318"
  },
  "zhang22ea_interspeech": {
   "authors": [
    [
     "Zhao",
     "Zhang"
    ],
    [
     "Ju",
     "Zhang"
    ],
    [
     "Jianguo",
     "Wei"
    ],
    [
     "Kiyoshi",
     "Honda"
    ],
    [
     "Tatsuya",
     "Kitamura"
    ]
   ],
   "title": "Vocal-Tract Area Functions with Articulatory Reality for Tract Opening",
   "original": "11320",
   "page_count": 4,
   "order": 950,
   "p1": 4691,
   "pn": 4694,
   "abstract": [
    "Vocal-tract area function is a one-dimensional representation of the vocal tract, in which speech signals are interpreted according to their place vs. area patterns. Recent work on deriving vocal-tract area functions from volumetric vocal-tract data is successful for the main tract part, whereas the region near the tract ends lacks accuracy due to the use of a planar grid system on the wedge-shaped tract opening. This study employs a special treatment on the anterior tract part using curved grid planes with a gradual evolution of convexity, which is applied to cross-sectioning the anterior tract regions including the post-incisor cavity, inter-dental channel, and lip tube. With the method, volumetric MRI data for vowels /a/ and /i/ were processed to describe the articulatory configuration in those regions. The results revealed that the anterior tract regions are observed as identifiable tract segments with a natural-shaped final opening. Thus, our proposed area function scheme promises nearly complete descriptions of articulatory configuration together with a smooth interface for sound radiation with minor modifications."
   ],
   "doi": "10.21437/Interspeech.2022-11320"
  },
  "stoidis22_interspeech": {
   "authors": [
    [
     "Dimitrios",
     "Stoidis"
    ],
    [
     "Andrea",
     "Cavallaro"
    ]
   ],
   "title": "Generating gender-ambiguous voices for privacy-preserving speech recognition",
   "original": "11322",
   "page_count": 5,
   "order": 859,
   "p1": 4237,
   "pn": 4241,
   "abstract": [
    "Our voice encodes a uniquely identifiable pattern which can be used to infer private attributes, such as gender or identity, that an individual might wish not to reveal when using a speech recognition service. To prevent attribute inference attacks alongside speech recognition tasks, we present a generative adversarial network, GenGAN, that synthesises voices that conceal the gender or identity of a speaker. The proposed network includes a generator with a U-Net architecture that learns to fool a discriminator. We condition the generator only on gender information and use an adversarial loss between signal distortion and privacy preservation. We show that GenGAN improves the trade-off between privacy and utility compared to privacy-preserving representation learning methods that consider gender information as a sensitive attribute to protect."
   ],
   "doi": "10.21437/Interspeech.2022-11322"
  },
  "gao22f_interspeech": {
   "authors": [
    [
     "Xiyuan",
     "Gao"
    ],
    [
     "Shekhar",
     "Nayak"
    ],
    [
     "Matt",
     "Coler"
    ]
   ],
   "title": "Deep CNN-based Inductive Transfer Learning for Sarcasm Detection in Speech",
   "original": "11323",
   "page_count": 5,
   "order": 473,
   "p1": 2323,
   "pn": 2327,
   "abstract": [
    "Sarcasm is a frequently used linguistic device which is expressed in a multitude of ways, both with acoustic cues (including pitch, intonation, intensity, etc.) and visual cues (including facial expression, eye gaze, etc.). While cues used in the expression of sarcasm are well-described in the literature, there is a striking paucity of attempts to perform automatic sarcasm detection in speech. To explore this gap, we elaborate a methodology of implementing Inductive Transfer Learning (ITL) based on pre-trained Deep Convolutional Neural Networks (DCNNs) to detect sarcasm in speech. To those ends, the multimodal dataset MUStARD is used as a target dataset in this study. The two selected pre-trained DCNN models used are Xception and VGGish, which we trained on visual and audio datasets. Results show that VGGish, which is applied as a feature extractor in the experiment, performs better than Xception, which has its convolutional layers and pooling layers retrained. Both models achieve a higher F-score compared to the baseline Support Vector Machines (SVM) model by 7% and 5% in unimodal sarcasm detection in speech."
   ],
   "doi": "10.21437/Interspeech.2022-11323"
  },
  "he22d_interspeech": {
   "authors": [
    [
     "Jiaxu",
     "He"
    ],
    [
     "Cheng",
     "Gong"
    ],
    [
     "Longbiao",
     "Wang"
    ],
    [
     "Di",
     "Jin"
    ],
    [
     "Xiaobao",
     "Wang"
    ],
    [
     "Junhai",
     "Xu"
    ],
    [
     "Jianwu",
     "Dang"
    ]
   ],
   "title": "Improve emotional speech synthesis quality by learning explicit and implicit representations with semi-supervised training",
   "original": "11336",
   "page_count": 5,
   "order": 1122,
   "p1": 5538,
   "pn": 5542,
   "abstract": [
    "Due to the lack of high-quality emotional speech synthesis datasets, the naturalness and expressiveness of synthesized speech are still lacking in order to achieve human-like communication. And existing emotional speech synthesis system usually extracts emotional information only from reference audio and ignores sentiment information implicit in the text. Therefore, we propose a novel model to improve emotional speech synthesis quality by learning explicit and implicit representations with semi-supervised learning. In addition to explicit emotional representations from reference audio, we propose an implicit emotion representations learning method based on graph neural network, considering dependency relations of a sentence and text sentiment classification (TSC) task. For the lack of emotion-annotated datasets, we leverage large amounts of expressive datasets to reinforce training the proposed model with semi-supervised learning. Experiments show that the proposed method can improve the naturalness and expressiveness of synthetic speech and is better than the baseline model."
   ],
   "doi": "10.21437/Interspeech.2022-11336"
  },
  "lemercier22_interspeech": {
   "authors": [
    [
     "Jean-Marie",
     "Lemercier"
    ],
    [
     "Joachim",
     "Thiemann"
    ],
    [
     "Raphael",
     "Koning"
    ],
    [
     "Timo",
     "Gerkmann"
    ]
   ],
   "title": "Neural Network-augmented Kalman Filtering for Robust Online Speech Dereverberation in Noisy Reverberant Environments",
   "original": "11337",
   "page_count": 5,
   "order": 46,
   "p1": 226,
   "pn": 230,
   "abstract": [
    "In this paper, a neural network-augmented algorithm for noise-robust online dereverberation with a Kalman filtering variant of the weighted prediction error (WPE) method is proposed. The filter stochastic variations are predicted by a deep neural network (DNN) trained end-to-end using the filter residual error and signal characteristics. The presented framework allows for robust dereverberation on a single-channel noisy reverberant dataset similar to WHAMR!. The Kalman filtering WPE introduces distortions in the enhanced signal when predicting the filter variations from the residual error only, if the target speech power spectral density is not perfectly known and the observation is noisy. The proposed approach avoids these distortions by correcting the filter variations estimation in a data-driven way, increasing the robustness of the method to noisy scenarios. Furthermore, it yields a strong dereverberation and denoising performance compared to a DNN-supported recursive least squares variant of WPE, especially for highly noisy inputs."
   ],
   "doi": "10.21437/Interspeech.2022-11337"
  },
  "arunkumar22_interspeech": {
   "authors": [
    [
     "A",
     "Arunkumar"
    ],
    [
     "Srinivasan",
     "Umesh"
    ]
   ],
   "title": "Joint Encoder-Decoder Self-Supervised Pre-training for ASR",
   "original": "11338",
   "page_count": 5,
   "order": 692,
   "p1": 3418,
   "pn": 3422,
   "abstract": [
    "Self-supervised learning (SSL) has shown tremendous success in various speech-related downstream tasks, including Automatic Speech Recognition (ASR). The output embeddings of the SSL model are treated as powerful short-time representations of the speech signal. However, in the ASR task, the main objective is to get the correct sequence of acoustic units, characters, or byte-pair encodings (BPEs). Usually, encoder-decoder architecture works exceptionally well for a sequence-to-sequence task like ASR. Therefore, in this paper, we propose a new paradigm that exploits the power of a decoder during self-supervised learning. We use Hidden Unit BERT (HuBERT) SSL framework to compute the conventional masked prediction loss for the encoder. In addition, we have introduced a decoder in the SSL framework and proposed a target preparation strategy for the decoder. Finally, we use a multitask SSL setup wherein we jointly optimize both the encoder and decoder losses. We hypothesize that the presence of a decoder in the SSL model helps it learn an acoustic unit-based language model, which might improve the performance of an ASR downstream task. We compare our proposed SSL model with HuBERT and show up to 25% relative improvement in performance on ASR by finetuning on various LibriSpeech subsets."
   ],
   "doi": "10.21437/Interspeech.2022-11338"
  },
  "singh22b_interspeech": {
   "authors": [
    [
     "Vishwanath Pratap",
     "Singh"
    ],
    [
     "Hardik",
     "Sailor"
    ],
    [
     "Supratik",
     "Bhattacharya"
    ],
    [
     "Abhishek",
     "Pandey"
    ]
   ],
   "title": "Spectral Modification Based Data Augmentation For Improving End-to-End ASR For Children’s Speech",
   "original": "11343",
   "page_count": 5,
   "order": 651,
   "p1": 3213,
   "pn": 3217,
   "abstract": [
    "Training a robust Automatic Speech Recognition (ASR) system for children's speech recognition is a challenging task due to inherent differences in acoustic attributes of adult and child speech and scarcity of publicly available children's speech dataset. In this paper, a novel segmental spectrum warping and perturbations in formant energy are introduced, to generate a children-like speech spectrum from that of an adult's speech spectrum. Then, this modified adult spectrum is used as augmented data to improve end-to-end ASR systems for children's speech recognition. The proposed data augmentation methods give 6.5% and 6.1% relative reduction in WER on children dev and test sets respectively, compared to the vocal tract length perturbation (VTLP) baseline system trained on Librispeech 100 hours adult speech dataset. When children's speech data is added in training with Librispeech set, it gives a 3.7 % and 5.1% relative reduction in WER, compared to the VTLP baseline system."
   ],
   "doi": "10.21437/Interspeech.2022-11343"
  },
  "meng22d_interspeech": {
   "authors": [
    [
     "Kevin",
     "Meng"
    ],
    [
     "Seo-Hyun",
     "Lee"
    ],
    [
     "Farhad",
     "Goodarzy"
    ],
    [
     "Simon",
     "Vogrin"
    ],
    [
     "Mark J.",
     "Cook"
    ],
    [
     "Seong-Whan",
     "Lee"
    ],
    [
     "David B.",
     "Grayden"
    ]
   ],
   "title": "Evidence of Onset and Sustained Neural Responses to Isolated Phonemes from Intracranial Recordings in a Voice-based Cursor Control Task",
   "original": "11344",
   "page_count": 5,
   "order": 824,
   "p1": 4063,
   "pn": 4067,
   "abstract": [
    "We developed a voice-based, self-paced cursor control task to collect corresponding intracranial neural data during isolated utterances of phonemes, namely vowel, nasal and fricative sounds. Two patients implanted with intracranial depth electrodes for clinical epilepsy monitoring performed closed-loop voice-based cursor control from real-time processing of microphone input. In post-hoc data analyses, we searched for neural features that correlated with the occurrence of non-specific speech sounds or specific phonemes. In line with previous studies, we observed onset and sustained responses to speech sounds at multiple recording sites within the superior temporal gyrus. Based on differential patterns of activation in narrow frequency bands up to 200 Hz, we tracked voice activity with 91% accuracy (chance level: 50%) and classified individual utterances into one of five phonemes with 68% accuracy (chance level: 20%). We propose that our framework could be extended to additional phonemes to better characterize neurophysiological mechanisms underlying the production and perception of speech sounds in the absence of language context. In general, our findings provide supplementary evidence and information toward the development of speech brain-computer interfaces using intracranial electrodes."
   ],
   "doi": "10.21437/Interspeech.2022-11344"
  },
  "wang22fa_interspeech": {
   "authors": [
    [
     "Pengwei",
     "Wang"
    ],
    [
     "Yinpei",
     "Su"
    ],
    [
     "Xiaohuan",
     "Zhou"
    ],
    [
     "Xin",
     "Ye"
    ],
    [
     "Liangchen",
     "Wei"
    ],
    [
     "Ming",
     "Liu"
    ],
    [
     "Yuan",
     "You"
    ],
    [
     "Feijun",
     "Jiang"
    ]
   ],
   "title": "Speech2Slot: A Limited Generation Framework with Boundary Detection for Slot Filling from Speech",
   "original": "11347",
   "page_count": 5,
   "order": 558,
   "p1": 2748,
   "pn": 2752,
   "abstract": [
    "Slot filling is an essential component of Spoken Language Understanding. In contrast to conventional pipeline approaches, which extract slots from the ASR output, end-to-end approaches directly get slots from speech within a classification or generation framework. However, classification relies on predefined categories, which is not scalable, and the generative model is decoding in an open-domain space, suffering from blurred boundaries of slots in speech. To address the shortcomings of these two formulations, we propose a new encoder-decoder framework for slot filling, named Speech2Slot, leveraging a limited generation method with boundary detection. We also released a large-scale Chinese spoken slot filling dataset named Voice Navigation Dataset in Chinese (VNDC). Experiments on VNDC show that our model is markedly superior to other approaches, outperforming the state-of-the-art slot filling approach with 6.65% accuracy improvement. We make our code (https://github.com/eehover/speech2slot) publicly available for researchers to replicate and build on our work."
   ],
   "doi": "10.21437/Interspeech.2022-11347"
  },
  "wang22ga_interspeech": {
   "authors": [
    [
     "Binling",
     "Wang"
    ],
    [
     "Feng",
     "Wang"
    ],
    [
     "Wenxuan",
     "Hu"
    ],
    [
     "Qiulin",
     "Wang"
    ],
    [
     "Jing",
     "Li"
    ],
    [
     "Dong",
     "Wang"
    ],
    [
     "Lin",
     "Li"
    ],
    [
     "Qingyang",
     "Hong"
    ]
   ],
   "title": "Oriental Language Recognition (OLR) 2021: Summary and Analysis",
   "original": "11348",
   "page_count": 5,
   "order": 756,
   "p1": 3729,
   "pn": 3733,
   "abstract": [
    "The sixth oriental language recognition (OLR) challenge focuses on language identification (LID) and automatic speech recognition (ASR) within multilingual scenarios. OLR 2021 includes four tasks: (1) constrained LID, (2) unconstrained LID, (3) constrained multilingual ASR, (4) unconstrained multilingual ASR. For the LID tasks, Cavg and equal error rate (EER) are considered as primary and secondary metrics, respectively. For the ASR tasks, character error rate (CER) is set as the unique metric. In this challenge, there were 48 participating teams and about half of the teams submitted valid results. Compared to the official baseline systems, the top-ranking teams achieved great progress. Among them, the best submission of constrained LID reduced Cavg from 0.0817 to 0.0025, meanwhile, the unconstrained LID achieved a very low Cavg of 0.0039. As for ASR tasks, the CER was reduced to 13.1% in the constrained ASR and 12.6% in the unconstrained ASR, respectively. This paper describes the above four tasks, database profile, and gives the analysis of results in detail."
   ],
   "doi": "10.21437/Interspeech.2022-11348"
  },
  "chang22h_interspeech": {
   "authors": [
    [
     "Yi",
     "Chang"
    ],
    [
     "Zhao",
     "Ren"
    ],
    [
     "Thanh Tam",
     "Nguyen"
    ],
    [
     "Wolfgang",
     "Nejdl"
    ],
    [
     "Björn W.",
     "Schuller"
    ]
   ],
   "title": "Example-based Explanations with Adversarial Attacks for Respiratory Sound Analysis",
   "original": "11355",
   "page_count": 5,
   "order": 812,
   "p1": 4003,
   "pn": 4007,
   "abstract": [
    "Respiratory sound classification is an important tool for remote screening of respiratory-related diseases such as pneumonia, asthma, and COVID-19. To facilitate the interpretability of classification results, especially ones based on deep learning, many explanation methods have been proposed using prototypes. However, existing explanation techniques often assume that the data is non-biased and the prediction results can be explained by a set of prototypical examples. In this work, we develop a unified example-based explanation method for selecting both representative data (prototypes) and outliers (criticisms). In particular, we propose a novel application of adversarial attacks to generate an explanation spectrum of data instances via an iterative fast gradient sign method. Such unified explanation can avoid over-generalisation and bias by allowing human experts to assess the model mistakes case by case. We performed a wide range of quantitative and qualitative evaluations to show that our approach generates effective and understandable explanation and is robust with many deep learning models."
   ],
   "doi": "10.21437/Interspeech.2022-11355"
  },
  "xu22h_interspeech": {
   "authors": [
    [
     "MengLong",
     "Xu"
    ],
    [
     "Shengqiang",
     "Li"
    ],
    [
     "Chengdong",
     "Liang"
    ],
    [
     "Xiao-Lei",
     "Zhang"
    ]
   ],
   "title": "Multi-class AUC Optimization for Robust Small-footprint Keyword Spotting with Limited Training Data",
   "original": "11356",
   "page_count": 5,
   "order": 664,
   "p1": 3278,
   "pn": 3282,
   "abstract": [
    "Deep neural networks provide effective solutions to small-footprint keyword spotting (KWS). However, most of the KWS methods take softmax with the minimum cross-entropy as the loss function, which focuses only on maximizing the classification accuracy on the training set, without taking unseen sounds that are out of the training data into account. If training data is limited, it remains challenging to achieve robust and highly accurate KWS in real-world scenarios where the unseen sounds are frequently encountered. In this paper, we propose a new KWS method, which consists of a novel loss function, named the maximization of the area under the receiver-operating-characteristic curve (AUC), and a confidence-based decision method. The proposed KWS method not only maintains high keywords classification accuracy, but is also robust to the unseen sounds. Experimental results on the Google Speech Commands dataset v1 and v2 show that our method achieves state-of-the-art performance in terms of most evaluation metrics."
   ],
   "doi": "10.21437/Interspeech.2022-11356"
  },
  "masumura22_interspeech": {
   "authors": [
    [
     "Ryo",
     "Masumura"
    ],
    [
     "Yoshihiro",
     "Yamazaki"
    ],
    [
     "Saki",
     "Mizuno"
    ],
    [
     "Naoki",
     "Makishima"
    ],
    [
     "Mana",
     "Ihori"
    ],
    [
     "Mihiro",
     "Uchida"
    ],
    [
     "Hiroshi",
     "Sato"
    ],
    [
     "Tomohiro",
     "Tanaka"
    ],
    [
     "Akihiko",
     "Takashima"
    ],
    [
     "Satoshi",
     "Suzuki"
    ],
    [
     "Shota",
     "Orihashi"
    ],
    [
     "Takafumi",
     "Moriya"
    ],
    [
     "Nobukatsu",
     "Hojo"
    ],
    [
     "Atsushi",
     "Ando"
    ]
   ],
   "title": "End-to-End Joint Modeling of Conversation History-Dependent and Independent ASR Systems with Multi-History Training",
   "original": "11357",
   "page_count": 5,
   "order": 652,
   "p1": 3218,
   "pn": 3222,
   "abstract": [
    "This paper proposes end-to-end joint modeling of conversation history-dependent and independent automatic speech recognition (ASR) systems. Conversation histories are available in ASR systems such as meeting transcription applications but not available in those such as voice search applications. So far, these two ASR systems have been individually constructed using different models, but this is inefficient for each application. In fact, conventional conversation history-dependent ASR systems can perform both history-dependent and independent processing. However, their performance is inferior to history-independent ASR systems. This is because the model architecture and its training criterion in the conventional conversation history-dependent ASR systems are specialized in the case where conversational histories are available. To address this problem, our proposed end-to-end joint modeling method uses a crossmodal transformer-based architecture that can flexibly switch to use the conversation histories or not. In addition, we propose multi-history training that simultaneously utilizes a dataset without histories and datasets with various histories to effectively improve both types of ASR processing by introducing unified architecture. Experiments on Japanese ASR tasks demonstrate the effectiveness of the proposed method."
   ],
   "doi": "10.21437/Interspeech.2022-11357"
  },
  "zhang22fa_interspeech": {
   "authors": [
    [
     "Jisi",
     "Zhang"
    ],
    [
     "Catalin",
     "Zorila"
    ],
    [
     "Rama",
     "Doddipatla"
    ],
    [
     "Jon",
     "Barker"
    ]
   ],
   "title": "On monoaural speech enhancement for automatic recognition of real noisy speech using mixture invariant training",
   "original": "11359",
   "page_count": 5,
   "order": 215,
   "p1": 1056,
   "pn": 1060,
   "abstract": [
    "In this paper, we explore an improved framework to train a monoaural neural enhancement model for robust speech recognition. The designed training framework extends the existing mixture invariant training criterion to exploit both unpaired clean speech and real noisy data. It is found that the unpaired clean speech is crucial to improve quality of separated speech from real noisy speech. The proposed method also performs remixing of processed and unprocessed signals to alleviate the processing artifacts. Experiments on the single-channel CHiME-3 real test sets show that the proposed method improves significantly in terms of speech recognition performance over the enhancement system trained either on the mismatched simulated data in a supervised fashion or on the matched real data in an unsupervised fashion. Between 16% and 39% relative WER reduction has been achieved by the proposed system compared to the unprocessed signal using end-to-end and hybrid acoustic models without retraining on distorted data."
   ],
   "doi": "10.21437/Interspeech.2022-11359"
  },
  "das22_interspeech": {
   "authors": [
    [
     "Nilaksh",
     "Das"
    ],
    [
     "Polo",
     "Chau"
    ]
   ],
   "title": "Hear No Evil: Towards Adversarial Robustness of Automatic Speech Recognition via Multi-Task Learning",
   "original": "11361",
   "page_count": 5,
   "order": 778,
   "p1": 3839,
   "pn": 3843,
   "abstract": [
    "As automatic speech recognition (ASR) systems are now being widely deployed in the wild, the increasing threat of adversarial attacks raises serious questions about the security and reliability of using such systems. On the other hand, multi-task learning (MTL) has shown success in training models that can resist adversarial attacks in the computer vision domain. In this work, we investigate the impact of performing such multi-task learning on the adversarial robustness of ASR models in the speech domain. We conduct extensive MTL experimentation by combining semantically diverse tasks such as accent classification and ASR, and evaluate a wide range of adversarial settings. Our thorough analysis reveals that performing MTL with semantically diverse tasks consistently makes it harder for an adversarial attack to succeed. We also discuss in detail the serious pitfalls and their related remedies that have a significant impact on the robustness of MTL models. Our proposed MTL approach shows considerable absolute improvements in adversarially targeted WER ranging from 17.25 up to 59.90 compared to single-task learning baselines (attention decoder and CTC respectively). Ours is the first in-depth study that uncovers adversarial robustness gains from multi-task learning for ASR."
   ],
   "doi": "10.21437/Interspeech.2022-11361"
  },
  "tachibana22_interspeech": {
   "authors": [
    [
     "Hideyuki",
     "Tachibana"
    ],
    [
     "Muneyoshi",
     "Inahara"
    ],
    [
     "Mocho",
     "Go"
    ],
    [
     "Yotaro",
     "Katayama"
    ],
    [
     "Yotaro",
     "Watanabe"
    ]
   ],
   "title": "Diffusion Generative Vocoder for Fullband Speech Synthesis Based on Weak Third-order SDE Solver",
   "original": "11366",
   "page_count": 5,
   "order": 333,
   "p1": 1641,
   "pn": 1645,
   "abstract": [
    "Diffusion generative models, which generate data by the time-reverse dynamics of diffusion processes, have attracted much attention recently, and have already been applied in the speech domain such as speech waveform synthesis. Diffusion generative models initially had the disadvantage of slow synthesis, but many fast samplers have been proposed and this disadvantage is being overcome. The authors have also proposed an efficient sampler based on a second-order approximation derived from the Itô-Taylor series, and have achieved some success. This study further examines the possibility of incorporating third-order terms and experimentally verifies that a vocoder using this method can synthesize high-fidelity fullband (48 kHz) speech signals faster than in real time. It is also shown that the method is applicable to the extension of speech bandwidth from wideband (16 kHz) to fullband (48 kHz)."
   ],
   "doi": "10.21437/Interspeech.2022-11366"
  },
  "zheng22f_interspeech": {
   "authors": [
    [
     "Xianrui",
     "Zheng"
    ],
    [
     "Chao",
     "Zhang"
    ],
    [
     "Phil",
     "Woodland"
    ]
   ],
   "title": "Tandem Multitask Training of Speaker Diarisation and Speech Recognition for Meeting Transcription",
   "original": "11368",
   "page_count": 5,
   "order": 779,
   "p1": 3844,
   "pn": 3848,
   "abstract": [
    "Self-supervised-learning-based pre-trained models for speech data, such as Wav2Vec 2.0 (W2V2), have become the backbone of many speech tasks. In this paper, to achieve speaker diarisation and speech recognition using a single model, a tandem multitask training (TMT) method is proposed to fine-tune W2V2. For speaker diarisation, the tasks of voice activity detection (VAD) and speaker classification (SC) are required, and connectionist temporal classification (CTC) is used for ASR. The multitask framework implements VAD, SC, and ASR using an early layer, middle layer, and late layer of W2V2, which coincides with the order of segmenting the audio with VAD, clustering the segments based on speaker embeddings, and transcribing each segment with ASR. Experimental results on the augmented multi-party (AMI) dataset showed that using different W2V2 layers for VAD, SC, and ASR from the earlier to later layers for TMT not only saves computational cost, but also reduces diarisation error rates (DERs). Joint fine-tuning of VAD, SC, and ASR yielded 16%/17% relative reductions of DER with manual/automatic segmentation respectively, and consistent reductions in speaker attributed word error rate, compared to the baseline with separately fine-tuned models."
   ],
   "doi": "10.21437/Interspeech.2022-11368"
  },
  "vandermerwe22_interspeech": {
   "authors": [
    [
     "Werner",
     "van der Merwe"
    ],
    [
     "Herman",
     "Kamper"
    ],
    [
     "Johan",
     "Adam du Preez"
    ]
   ],
   "title": "A Temporal Extension of Latent Dirichlet Allocation for Unsupervised Acoustic Unit Discovery",
   "original": "11369",
   "page_count": 5,
   "order": 290,
   "p1": 1426,
   "pn": 1430,
   "abstract": [
    "Latent Dirichlet allocation (LDA) is widely used for unsupervised topic modelling on sets of documents. No temporal information is used in the model. However, there is often a relationship between the corresponding topics of consecutive tokens. In this paper, we present an extension to LDA that uses a Markov chain to model temporal information. We use this new model for acoustic unit discovery from speech. As input tokens, the model takes a discretised encoding of speech from a vector quantised (VQ) neural network with 512 codes. The goal is then to map these 512 VQ codes to 50 phone-like units (topics) in order to more closely resemble true phones. In contrast to the base LDA, which only considers how VQ codes co-occur within utterances (documents), the Markov chain LDA additionally captures how consecutive codes follow one another. This extension leads to an increase in cluster quality and phone segmentation results compared to the base LDA. Compared to a recent vector quantised neural network approach that also learns 50 units, the extended LDA model performs better in phone segmentation but worse in mutual information."
   ],
   "doi": "10.21437/Interspeech.2022-11369"
  },
  "bhanushali22_interspeech": {
   "authors": [
    [
     "Anish",
     "Bhanushali"
    ],
    [
     "Grant",
     "Bridgman"
    ],
    [
     "Deekshitha",
     "G"
    ],
    [
     "Prasanta",
     "Ghosh"
    ],
    [
     "Pratik",
     "Kumar"
    ],
    [
     "Saurabh",
     "Kumar"
    ],
    [
     "Adithya",
     "Raj Kolladath"
    ],
    [
     "Nithya",
     "Ravi"
    ],
    [
     "Aaditeshwar",
     "Seth"
    ],
    [
     "Ashish",
     "Seth"
    ],
    [
     "Abhayjeet",
     "Singh"
    ],
    [
     "Vrunda",
     "Sukhadia"
    ],
    [
     "Umesh",
     "S"
    ],
    [
     "Sathvik",
     "Udupa"
    ],
    [
     "Lodagala V. S. V. Durga",
     "Prasad"
    ]
   ],
   "title": "Gram Vaani ASR Challenge on spontaneous telephone speech recordings in regional variations of Hindi",
   "original": "11371",
   "page_count": 5,
   "order": 718,
   "p1": 3548,
   "pn": 3552,
   "abstract": [
    "This paper describes the corpus and baseline systems for the Gram Vaani Automatic Speech Recognition (ASR) challenge in regional variations of Hindi. The corpus for this challenge comprises the spontaneous telephone speech recordings collected by a social technology enterprise, Gram Vaani. The regional variations of Hindi together with spontaneity of speech, natural background and transcriptions with variable accuracy due to crowdsourcing make it a unique corpus for ASR on spontaneous telephonic speech. Around, 1108 hours of real-world spontaneous speech recordings, including 1000 hours of unlabelled training data, 100 hours of labelled training data, 5 hours of development data and 3 hours of evaluation data, have been released as a part of the challenge. The efficacy of both training and test sets are validated on different ASR systems in both traditional time-delay neural network-hidden Markov model (TDNN-HMM) frameworks and fully-neural end-to-end (E2E) setup. The word error rate (WER) and character error rate (CER) on eval set for a TDNN model trained on 100 hours of labelled data are 29.7% and 15.1%, respectively. While, in E2E setup, WER and CER on eval set for a conformer model trained on 100 hours of data are 32.9% and 19.0%, respectively."
   ],
   "doi": "10.21437/Interspeech.2022-11371"
  },
  "zeng22b_interspeech": {
   "authors": [
    [
     "Qingcheng",
     "Zeng"
    ],
    [
     "Dading",
     "Chong"
    ],
    [
     "Peilin",
     "Zhou"
    ],
    [
     "Jie",
     "Yang"
    ]
   ],
   "title": "Low-resource Accent Classification in Geographically-proximate Settings: A Forensic and Sociophonetics Perspective",
   "original": "11372",
   "page_count": 5,
   "order": 1076,
   "p1": 5308,
   "pn": 5312,
   "abstract": [
    "Accented speech recognition and accent classification are relatively under-explored research areas in speech technology. Recently, deep learning-based methods and Transformer-based pretrained models have achieved superb performances in both areas. However, most accent classification tasks focused on classifying different kinds of English accents and little attention was paid to geographically-proximate accent classification, especially under a low-resource setting where forensic speech science tasks usually confront. In this paper, we explored three main accent modelling methods combined with two different classifiers based on 105 speakers' recordings retrieved from five urban varieties in Northern England. Although speech representations generated from pretrained models generally have better performances in downstream classification, traditional methods like Mel Frequency Cepstral Coefficients (MFCCs) and formants' measurement are equipped with specific strengths. These results suggest that in forensic phonetics scenario where data are relatively scarce, a simple modelling method and classifier could be competitive with state-of-the-art pretrained speech models as feature extractors, which could enhance a sooner estimation for the accent information in practices. Besides, our findings also cross-validated a new methodology in quantifying sociophonetic changes."
   ],
   "doi": "10.21437/Interspeech.2022-11372"
  },
  "arunkumar22b_interspeech": {
   "authors": [
    [
     "A",
     "Arunkumar"
    ],
    [
     "Vrunda",
     "Nileshkumar Sukhadia"
    ],
    [
     "Srinivasan",
     "Umesh"
    ]
   ],
   "title": "Investigation of Ensemble features of Self-Supervised Pretrained Models for Automatic Speech Recognition",
   "original": "11376",
   "page_count": 5,
   "order": 1041,
   "p1": 5145,
   "pn": 5149,
   "abstract": [
    "Self-supervised learning (SSL) based models have been shown to generate powerful representations that can be used to improve the performance of downstream speech tasks. Several state-of-the-art SSL models are available, and each of these models optimizes a different loss which gives rise to the possibility of their features being complementary. This paper proposes using an ensemble of such SSL representations and models, which exploits the complementary nature of the features extracted by the various pretrained models. We hypothesize that this results in a richer feature representation and show results for the ASR downstream task. To this end, we use three SSL models that have shown excellent results on ASR tasks, namely HuBERT, Wav2Vec2.0, and WavLM. We explore the ensemble of models fine-tuned for the ASR task and the ensemble of features using the embeddings obtained from the pre-trained models for a downstream ASR task. We get a relative improvement of 10% in ASR performance over individual models and pre-trained features when using LibriSpeech(100h) and WSJ dataset for the downstream tasks."
   ],
   "doi": "10.21437/Interspeech.2022-11376"
  },
  "zhu22f_interspeech": {
   "authors": [
    [
     "Yi",
     "Zhu"
    ],
    [
     "Zexun",
     "Wang"
    ],
    [
     "Hang",
     "Liu"
    ],
    [
     "Peiying",
     "Wang"
    ],
    [
     "Mingchao",
     "Feng"
    ],
    [
     "Meng",
     "Chen"
    ],
    [
     "Xiaodong",
     "He"
    ]
   ],
   "title": "Cross-modal Transfer Learning via Multi-grained Alignment for End-to-End Spoken Language Understanding",
   "original": "11378",
   "page_count": 5,
   "order": 230,
   "p1": 1131,
   "pn": 1135,
   "abstract": [
    "End-to-end spoken language understanding (E2E-SLU) has witnessed impressive improvements through cross-modal (text-to-audio) transfer learning. However, current methods mostly focus on coarse-grained sequence-level text-to-audio knowledge transfer with simple loss, and neglecting the fine-grained temporal alignment between two modalities. In this work, we propose a novel multi-grained cross-modal transfer learning model for E2E-SLU. Specifically, we devise a cross attention module to align the tokens of text with the frame features of speech, encouraging the model to target at the salient acoustic features attended to each token during transferring the semantic information. We also leverage contrastive learning to facilitate cross-modal representation learning in sentence level. Finally, we explore various data augmentation methods to mitigate the deficiency of large amount of labelled data for the training of E2E-SLU. Extensive experiments are conducted on both English and Chinese SLU datasets to verify the effectiveness of our proposed approach. Experimental results and detailed analyses demonstrate the superiority and competitiveness of our model."
   ],
   "doi": "10.21437/Interspeech.2022-11378"
  },
  "rennie22_interspeech": {
   "authors": [
    [
     "Gordon",
     "Rennie"
    ],
    [
     "Olga",
     "Perepelkina"
    ],
    [
     "Alessandro",
     "Vinciarelli"
    ]
   ],
   "title": "Which Model is Best: Comparing Methods and Metrics for Automatic Laughter Detection in a Naturalistic Conversational Dataset",
   "original": "11379",
   "page_count": 5,
   "order": 813,
   "p1": 4008,
   "pn": 4012,
   "abstract": [
    "Laughter is a common paralinguistic vocalization that has been shown to be used for controlling the flow of a conversation, nullifying previous statements, and managing conversations on delicate topics. Already there have been concerted efforts to develop methods for automatically detecting laughter in speech. Many of these studies use artificial datasets and report their model performance using the AUC metric. This paper replicates previous work on laughter detection on those artificial datasets and then extends them by validating the methods on a larger and more naturalistic dataset made up of 60 spontaneous conversations (120 speakers and roughly 12 hours of material in total) with the best performing model achieving an AUC of 90.39\\5 +/- 1.10 (precision=13.99 +/- 4.09, recall=76.36 +/- 12.00, F1=23.06 +/- 4.99). The paper then goes on to discuss the shortcomings with the current standard comparison metric in the field of AUC and suggests alternatives which may aid in the comparison and understanding of method's effectiveness."
   ],
   "doi": "10.21437/Interspeech.2022-11379"
  },
  "fukuda22b_interspeech": {
   "authors": [
    [
     "Ryo",
     "Fukuda"
    ],
    [
     "Katsuhito",
     "Sudoh"
    ],
    [
     "Satoshi",
     "Nakamura"
    ]
   ],
   "title": "Speech Segmentation Optimization using Segmented Bilingual Speech Corpus for End-to-end Speech Translation",
   "original": "11382",
   "page_count": 5,
   "order": 25,
   "p1": 121,
   "pn": 125,
   "abstract": [
    "Speech segmentation, which splits long speech into short segments, is essential for speech translation (ST). Popular VAD tools like WebRTC VAD have generally relied on pause-based segmentation. Unfortunately, pauses in speech do not necessarily match sentence boundaries, and sentences can be connected by a very short pause that is difficult to detect by VAD. In this study, we propose a speech segmentation method using a binary classification model trained using a segmented bilingual speech corpus. We also propose a hybrid method that combines VAD and the above speech segmentation method. Experimental results revealed that the proposed method is more suitable for cascade and end-to-end ST systems than conventional segmentation methods. The hybrid approach further improved the translation performance."
   ],
   "doi": "10.21437/Interspeech.2022-11382"
  },
  "ye22b_interspeech": {
   "authors": [
    [
     "Tong",
     "Ye"
    ],
    [
     "Shijing",
     "Si"
    ],
    [
     "Jianzong",
     "Wang"
    ],
    [
     "Ning",
     "Cheng"
    ],
    [
     "Jing",
     "Xiao"
    ]
   ],
   "title": "Uncertainty Calibration for Deep Audio Classifiers",
   "original": "11384",
   "page_count": 5,
   "order": 316,
   "p1": 1556,
   "pn": 1560,
   "abstract": [
    "Although deep Neural Networks (DNNs) have achieved tremendous success in audio classification tasks, their uncertainty calibration are still under-explored. A well-calibrated model should be accurate when it is certain about its prediction and indicate high uncertainty when it is likely to be inaccurate. In this work, we investigate the uncertainty calibration for deep audio classifiers. In particular, we empirically study the performance of popular calibration methods: (i) Monte Carlo dropout, (ii) ensemble, (iii) focal loss, and (iv) spectral-normalized Gaussian process (SNGP), on audio classification datasets. To this end, we evaluate (i–iv) for the tasks of environment sound and music genre classification. Results indicate that uncalibrated deep audio classifiers may be over-confident, and SNGP performs the best and is very efficient on the two datasets of this paper."
   ],
   "doi": "10.21437/Interspeech.2022-11384"
  },
  "aroudi22_interspeech": {
   "authors": [
    [
     "Ali",
     "Aroudi"
    ],
    [
     "Stefan",
     "Uhlich"
    ],
    [
     "Marc",
     "Ferras Font"
    ]
   ],
   "title": "TRUNet: Transformer-Recurrent-U Network for Multi-channel Reverberant Sound Source Separation",
   "original": "11386",
   "page_count": 5,
   "order": 186,
   "p1": 911,
   "pn": 915,
   "abstract": [
    "In recent years, many deep learning techniques for single-channel sound source separation have been proposed using recurrent, convolutional and transformer networks. When multiple microphones are available, spatial diversity between speakers and background noise in addition to spectro-temporal diversity can be exploited by using multi-channel filters for sound source separation. Aiming at end-to-end multi-channel source separation, in this paper we propose a transformer-recurrent-U network (TRUNet), which directly estimates multi-channel filters from multi-channel input spectra. TRUNet consists of a spatial processing network with an attention mechanism across microphone channels aiming at capturing the spatial diversity, and a spectro-temporal processing network aiming at capturing spectral and temporal diversities. In addition to multi-channel filters, we also consider estimating single-channel filters from multi-channel input spectra using TRUNet. We train the network on a large reverberant dataset using a proposed combined compressed mean-squared error loss function, which further improves the sound separation performance. We evaluate the network on a realistic and challenging reverberant dataset, generated from measured room impulse responses of an actual microphone array. The experimental results on realistic reverberant sound source separation show that the proposed TRUNet outperforms state-of-the-art single-channel and multi-channel source separation methods."
   ],
   "doi": "10.21437/Interspeech.2022-11386"
  },
  "ochi22_interspeech": {
   "authors": [
    [
     "Keiko",
     "Ochi"
    ],
    [
     "Nobutaka",
     "Ono"
    ],
    [
     "Keiho",
     "Owada"
    ],
    [
     "Kuroda",
     "Miho"
    ],
    [
     "Shigeki",
     "Sagayama"
    ],
    [
     "Hidenori",
     "Yamasue"
    ]
   ],
   "title": "Use of Nods Less Synchronized with Turn-Taking and Prosody During Conversations in Adults with Autism",
   "original": "11388",
   "page_count": 5,
   "order": 231,
   "p1": 1136,
   "pn": 1140,
   "abstract": [
    "Autism spectral disorder (ASD) is a highly prevalent neurodevelopmental disorder characterized by deficits in communication and social interaction. Head-nodding, a kind of visual backchannels, is used to co-construct the conversation and is crucial to smooth social interaction. In the present study, we quantitively analyze how head-nodding relates to speech turn-taking and prosodic change in Japanese conversation. The results showed that nodding was less frequently observed in ASD participants, especially around speakers' turn transitions, whereas it was notable just before and after turn-taking in individuals with typical development (TD). Analysis using 16 sec of long-time sliding segments revealed that synchronization between nod frequency and mean vocal intensity was higher in the TD group than in the ASD group. Classification by a support vector machine (SVM) using these proposed features achieved high performance with an accuracy of 91.1% and an F-measure of 0.942. In addition, the results indicated an optimal way of nodding according to turn-ending and emphasis, which could provide standard responses for reference or feedback in social skill training for people with ASD. Furthermore, the natural timing of nodding implied by the results can also be applied to developing interactive responses in humanoid robots or computer graphic (CG) agents."
   ],
   "doi": "10.21437/Interspeech.2022-11388"
  },
  "handekabil22_interspeech": {
   "authors": [
    [
     "Selen",
     "Hande Kabil"
    ],
    [
     "Herve",
     "Bourlard"
    ]
   ],
   "title": "From Undercomplete to Sparse Overcomplete Autoencoders to Improve LF-MMI based Speech Recognition",
   "original": "11390",
   "page_count": 5,
   "order": 216,
   "p1": 1061,
   "pn": 1065,
   "abstract": [
    "Starting from a strong Lattice-Free Maximum Mutual Information (LF-MMI) baseline system, we explore different autoencoder configurations to enhance Mel-Frequency Cepstral Coefficients (MFCC) features. Autoencoders are expected to generate new MFCC features that can be used in our LF-MMI based baseline system (with or without retraining) towards speech recognition improvements. Starting from shallow undercomplete autoencoders, and their known equivalence with Principal Component Analysis (PCA), we go to deeper or sparser architectures. In the spirit of kernel-based learning methods, we explore alternatives where the autoencoder first goes overcomplete (i.e., expand the representation space) in a nonlinear way, and then we restrict the autoencoder by means of a sequent bottleneck layer. Finally, as a third solution, we use sparse overcomplete autoencoders where a sparsity constraint is imposed on the higher-dimensional encoding layer. Experimental results are provided on the Augmented Multiparty Interaction (AMI) dataset, where we show that all aforementioned architectures improve speech recognition performance."
   ],
   "doi": "10.21437/Interspeech.2022-11390"
  },
  "wu22j_interspeech": {
   "authors": [
    [
     "Xianchao",
     "Wu"
    ]
   ],
   "title": "Attention Enhanced Citrinet for Speech Recognition",
   "original": "11394",
   "page_count": 5,
   "order": 430,
   "p1": 2108,
   "pn": 2112,
   "abstract": [
    "Citrinet is an end-to-end convolutional Connectionist Temporal Classification (CTC) based automatic speech recognition (ASR) model. To capture local and global contextual information, 1D time-channel separable convolutions combined with sub-word encoding and squeeze-and-excitation (SE) are used in Citrinet, making the whole architecture to be as deep as including 23 blocks with 235 convolution layers and 46 linear layers. This pure convolutional and deep architecture makes Critrinet relatively slow at convergence. In this paper, we propose to introduce multi-head attentions together with feed-forward networks in the convolution module in Citrinet blocks while keeping the SE module and residual module unchanged. For speeding up, we remove 8 convolution layers in each attention-enhanced Citrinet block and reduce 23 blocks to 13. Experiments on the Japanese CSJ-500h and Magic-1600h dataset show that the attention-enhanced Citrinet with less layers and blocks and converges faster with lower character error rates than (1) Citrinet with 80% training time and (2) Conformer with 40% training time and 29.8% model size."
   ],
   "doi": "10.21437/Interspeech.2022-11394"
  },
  "meeus22_interspeech": {
   "authors": [
    [
     "Quentin",
     "Meeus"
    ],
    [
     "Marie",
     "Francine Moens"
    ],
    [
     "Hugo",
     "Van hamme"
    ]
   ],
   "title": "Multitask Learning for Low Resource Spoken Language Understanding",
   "original": "11401",
   "page_count": 5,
   "order": 826,
   "p1": 4073,
   "pn": 4077,
   "abstract": [
    "We explore the benefits that multitask learning offer to speech processing as we train models on dual objectives with automatic speech recognition and intent classification or sentiment classification. Our models, although being of modest size, show improvements over models trained end-to-end on intent classification. We compare different settings to find the optimal disposition of each task module compared to one another. Finally, we study the performance of the models in low-resource scenario by training the models with as few as one example per class. We show that multitask learning in these scenarios compete with a baseline model trained on text features and performs considerably better than a pipeline model. On sentiment classification, we match the performance of an end-to-end model with ten times as many parameters. We consider 4 tasks and 4 datasets in Dutch and English."
   ],
   "doi": "10.21437/Interspeech.2022-11401"
  },
  "kinoshita22_interspeech": {
   "authors": [
    [
     "Keisuke",
     "Kinoshita"
    ],
    [
     "Thilo von",
     "Neumann"
    ],
    [
     "Marc",
     "Delcroix"
    ],
    [
     "Christoph",
     "Boeddeker"
    ],
    [
     "Reinhold",
     "Haeb-Umbach"
    ]
   ],
   "title": "Utterance-by-utterance overlap-aware neural diarization with Graph-PIT",
   "original": "11408",
   "page_count": 5,
   "order": 302,
   "p1": 1486,
   "pn": 1490,
   "abstract": [
    "Recent speaker diarization studies showed that integration of end-to-end neural diarization (EEND) and clustering-based diarization is a promising approach for achieving state-of-the-art performance on various tasks. Such an approach first divides an observed signal into fixed-length segments, then performs segment-level local diarization based on an EEND module, and merges the segment-level results via clustering to form a final global diarization result. The segmentation is done to limit the number of speakers in each segment since the current EEND cannot handle many of speakers. In this paper, we argue that such an approach involving the segmentation has several issues; for example, it inevitably faces a dilemma that larger segment sizes increase both the context available for enhancing the performance and the number of speakers for the EEND module to handle. To resolve such a problem, this paper proposes a novel framework that performs diarization without segmentation. However, it can still handle challenging data containing many speakers and a significant amount of overlapping speech. To this end, we leverage a neural network training scheme called Graph-PIT proposed recently for neural source separation. Experiments with simulated active-meeting-like data and CALLHOME data show the superiority of the proposed approach over the conventional methods."
   ],
   "doi": "10.21437/Interspeech.2022-11408"
  },
  "wang22ha_interspeech": {
   "authors": [
    [
     "Jie",
     "Wang"
    ],
    [
     "Yuji",
     "Liu"
    ],
    [
     "Binling",
     "Wang"
    ],
    [
     "Yiming",
     "Zhi"
    ],
    [
     "Song",
     "Li"
    ],
    [
     "Shipeng",
     "Xia"
    ],
    [
     "Jiayang",
     "Zhang"
    ],
    [
     "Feng",
     "Tong"
    ],
    [
     "Lin",
     "Li"
    ],
    [
     "Qingyang",
     "Hong"
    ]
   ],
   "title": "Spatial-aware Speaker Diarizaiton for Multi-channel Multi-party Meeting",
   "original": "11412",
   "page_count": 5,
   "order": 303,
   "p1": 1491,
   "pn": 1495,
   "abstract": [
    "This paper describes a spatial-aware speaker diarization system for the multi-channel multi-party meeting. The diarization system obtains direction information of speaker by microphone array. Speaker-spatial embedding is generated by x-vector and s-vector derived from superdirective beamforming (SDB) which makes the embedding more robust. Specifically, we propose a novel multi-channel sequence-to-sequence neural network architecture named discriminative multi-stream neural network (DMSNet) which consists of attention superdirective beamforming (ASDB) block and Conformer encoder. The proposed ASDB is a self-adapted channel-wise block that extracts the latent spatial features of array audios by modeling interdependencies between channels. We explore DMSNet to address overlapped speech problem on multi-channel audio and achieve 93.53% accuracy on evaluation set. By performing DMSNet based overlapped speech detection (OSD) module, the diarization error rate (DER) of cluster-based diarization system decrease significantly from 13.45% to 7.64%."
   ],
   "doi": "10.21437/Interspeech.2022-11412"
  },
  "tanaka22_interspeech": {
   "authors": [
    [
     "Tomohiro",
     "Tanaka"
    ],
    [
     "Ryo",
     "Masumura"
    ],
    [
     "Hiroshi",
     "Sato"
    ],
    [
     "Mana",
     "Ihori"
    ],
    [
     "Kohei",
     "Matsuura"
    ],
    [
     "Takanori",
     "Ashihara"
    ],
    [
     "Takafumi",
     "Moriya"
    ]
   ],
   "title": "Domain Adversarial Self-Supervised Speech Representation Learning for Improving Unknown Domain Downstream Tasks",
   "original": "11414",
   "page_count": 5,
   "order": 217,
   "p1": 1066,
   "pn": 1070,
   "abstract": [
    "In this paper, we propose novel self-supervised speech representation learning method that obtains domain invariant representations by using a domain adversarial neural network. Recently, self-supervised representation learning has been actively studied in the speech field. Since self-supervised learning requires large-scale unlabeled data, we need to effectively use data collected from a variety of domains. However, existing methods cannot construct valid representations in unknown domains because they cause overfitting to the domains in the training data. To solve this problem, our proposed method constructs contextual representations that cannot identify the domains from input speech by using domain adversarial neural networks. The domain adversarial training can improve robustness for data in unknown domains because the model trained by our proposed method can construct domain invariant representations. In addition, we investigate multi-task learning of representation construction and domain classification to consider domain information. Experimental results show that our proposed method outperforms the conventional training method of wav2vec 2.0 in unknown domain downstream automatic speech recognition tasks."
   ],
   "doi": "10.21437/Interspeech.2022-11414"
  },
  "xu22i_interspeech": {
   "authors": [
    [
     "Qiang",
     "Xu"
    ],
    [
     "Tongtong",
     "Song"
    ],
    [
     "Longbiao",
     "Wang"
    ],
    [
     "Hao",
     "Shi"
    ],
    [
     "Yuqin",
     "Lin"
    ],
    [
     "Yongjie",
     "Lv"
    ],
    [
     "Meng",
     "Ge"
    ],
    [
     "Qiang",
     "Yu"
    ],
    [
     "Jianwu",
     "Dang"
    ]
   ],
   "title": "Self-Distillation Based on High-level Information Supervision for Compressing End-to-End ASR Model",
   "original": "11423",
   "page_count": 5,
   "order": 348,
   "p1": 1716,
   "pn": 1720,
   "abstract": [
    "Model compression of ASR aims to reduce the model parameters while bringing as little performance degradation as possible. Knowledge Distillation (KD) is an efficient model compression method that transfers the knowledge from a large teacher model to a smaller student model. However, most of the existing KD methods study how to fully utilize the teacher's knowledge without paying attention to the student's own knowledge. In this paper, we explore whether the high-level information of the model itself is helpful for low-level information. We first propose neighboring feature self-distillation (NFSD) approach to distill the knowledge from the adjacent deeper layer to the shallow one, which shows significant performance improvement. Therefore, we further propose attention-based feature self-distillation (AFSD) approach to exploit more high-level information. Specifically, AFSD fuses the knowledge from multiple deep layers with an attention mechanism and distills it to a shallow one. The experimental results on AISHELL-1 dataset show that 7.3% and 8.3% relative character error rate (CER) reduction can be achieved from NFSD and AFSD, respectively. In addition, our proposed two approaches can be easily combined with the general teacher-student knowledge distillation method to achieve 12.4% and 13.4% relative CER reduction compared with the baseline student model, respectively."
   ],
   "doi": "10.21437/Interspeech.2022-11423"
  },
  "moriya22_interspeech": {
   "authors": [
    [
     "Takafumi",
     "Moriya"
    ],
    [
     "Hiroshi",
     "Sato"
    ],
    [
     "Tsubasa",
     "Ochiai"
    ],
    [
     "Marc",
     "Delcroix"
    ],
    [
     "Takahiro",
     "Shinozaki"
    ]
   ],
   "title": "Streaming Target-Speaker ASR with Neural Transducer",
   "original": "11425",
   "page_count": 5,
   "order": 543,
   "p1": 2673,
   "pn": 2677,
   "abstract": [
    "Although recent advances in deep learning technology have boosted automatic speech recognition (ASR) performance in the single-talker case, it remains difficult to recognize multi-talker speech in which many voices overlap. One conventional approach to tackle this problem is to use a cascade of a speech separation or target speech extraction front-end with an ASR back-end. However, the extra computation costs of the front-end module are critical for a quick response, especially for streaming ASR. In this paper, we propose a target-speaker ASR (TS-ASR) system, which integrates implicitly the target speech extraction functionality within a streaming end-to-end (E2E) ASR system, i.e. recurrent neural network-transducer (RNNT). Our system uses a similar idea as target speech extraction, but implements it directly at the level of the encoder of RNNT. This allows to realize TS-ASR without extra computation costs for the front-end. Note that our works present two major differences between prior studies about E2E-TS-ASR, we investigate streaming models and base our study on Conformer models, whereas prior studies used RNN-based systems and only dealt with offline processing. We confirm in experiments that our TS-ASR achieves comparable recognition performance with conventional cascade system in offline setting, while reducing computation costs and allowing streaming TS-ASR."
   ],
   "doi": "10.21437/Interspeech.2022-11425"
  },
  "song22e_interspeech": {
   "authors": [
    [
     "Tongtong",
     "Song"
    ],
    [
     "Qiang",
     "Xu"
    ],
    [
     "Meng",
     "Ge"
    ],
    [
     "Longbiao",
     "Wang"
    ],
    [
     "Hao",
     "Shi"
    ],
    [
     "Yongjie",
     "Lv"
    ],
    [
     "Yuqin",
     "Lin"
    ],
    [
     "Jianwu",
     "Dang"
    ]
   ],
   "title": "Language-specific Characteristic Assistance for Code-switching Speech Recognition",
   "original": "11426",
   "page_count": 5,
   "order": 795,
   "p1": 3924,
   "pn": 3928,
   "abstract": [
    "Dual-encoder structure successfully utilizes two language-specific encoders (LSEs) for code-switching speech recognition. Because LSEs are initialized by two pre-trained language-specific models (LSMs), the dual-encoder structure can exploit sufficient monolingual data and capture the individual language attributes. However, existing methods have no language constraints on LSEs and underutilize language-specific knowledge of LSMs. In this paper, we propose a language-specific characteristic assistance (LSCA) method to mitigate the above problems. Specifically, during training, we introduce two language-specific losses as language constraints and generate corresponding language-specific targets for them. During decoding, we take the decoding abilities of LSMs into account by combining the output probabilities of two LSMs and the mixture model to obtain the final predictions. Experiments show that either the training or decoding method of LSCA can improve the model's performance. Furthermore, the best result can obtain up to 15.4% relative error reduction on the code-switching test set by combining the training and decoding methods of LSCA. Moreover, the system can process code-switching speech recognition tasks well without extra shared parameters or even retraining based on two pre-trained LSMs by using our method."
   ],
   "doi": "10.21437/Interspeech.2022-11426"
  },
  "takeuchi22_interspeech": {
   "authors": [
    [
     "Daiki",
     "Takeuchi"
    ],
    [
     "Yasunori",
     "Ohishi"
    ],
    [
     "Daisuke",
     "Niizumi"
    ],
    [
     "Noboru",
     "Harada"
    ],
    [
     "Kunio",
     "Kashino"
    ]
   ],
   "title": "Introducing Auxiliary Text Query-modifier to Content-based Audio Retrieval",
   "original": "11428",
   "page_count": 5,
   "order": 851,
   "p1": 4197,
   "pn": 4201,
   "abstract": [
    "The amount of audio data available on public websites is growing rapidly, and an efficient mechanism for accessing the desired data is necessary. We propose a content-based audio retrieval method that can retrieve a target audio that is similar to but slightly different from the query audio by introducing auxiliary textual information which describes the difference between the query and target audio. While the range of conventional content-based audio retrieval is limited to audio that is similar to the query audio, the proposed method can adjust the retrieval range by adding an embedding of the auxiliary text query-modifier to the embedding of the query sample audio in a shared latent space. To evaluate our method, we built a dataset comprising two different audio clips and the text that describes the difference. The experimental results show that the proposed method retrieves the paired audio more accurately than the baseline. We also confirmed based on visualization that the proposed method obtains the shared latent space in which the audio difference and the corresponding text are represented as similar embedding vectors."
   ],
   "doi": "10.21437/Interspeech.2022-11428"
  },
  "li22ea_interspeech": {
   "authors": [
    [
     "Nan",
     "Li"
    ],
    [
     "Xiguang",
     "Zheng"
    ],
    [
     "Chen",
     "Zhang"
    ],
    [
     "Liang",
     "Guo"
    ],
    [
     "Bing",
     "Yu"
    ]
   ],
   "title": "End-to-End Multi-Loss Training for Low Delay Packet Loss Concealment",
   "original": "11439",
   "page_count": 5,
   "order": 118,
   "p1": 585,
   "pn": 589,
   "abstract": [
    "Real-time teleconferencing has become one of the essential parts in our daily life. While packet loss during real-time data transmission is unavoidable, traditional signal processing based Packet Loss Concealment (PLC) techniques have been developed in recent decades. In recent years, deep learning based approaches have also proposed and achieved state-of-the-art PLC performance. This work presents a low-delay multi-loss based neural PLC system. The multi-loss is consisted by a signal loss, a perceptual loss and an ASR loss ensuring good speech quality and automatic speech recognition compatibility. The proposed system was ranked 1st place in INTERSPEECH 2022's Audio Deep Packet Loss Concealment Challenge."
   ],
   "doi": "10.21437/Interspeech.2022-11439"
  },
  "maekaku22_interspeech": {
   "authors": [
    [
     "Takashi",
     "Maekaku"
    ],
    [
     "Yuya",
     "Fujita"
    ],
    [
     "Yifan",
     "Peng"
    ],
    [
     "Shinji",
     "Watanabe"
    ]
   ],
   "title": "Attention Weight Smoothing Using Prior Distributions for Transformer-Based End-to-End ASR",
   "original": "11441",
   "page_count": 5,
   "order": 218,
   "p1": 1071,
   "pn": 1075,
   "abstract": [
    "Transformer-based encoder-decoder models have so far been widely used for end-to-end automatic speech recognition. However, it has been found that the self-attention weight matrix could be too peaky and biased toward the diagonal component. Such attention weight matrix contains little useful context information, which may result in poor speech recognition performance. Therefore, we propose the following two attention weight smoothing methods based on the hypothesis that an attention weight matrix whose diagonal components are not peaky can capture more context information. One is a method to linearly interpolate the attention weight using a learnable truncated prior distribution. The other uses the attention weight from a previous layer as a prior distribution given that lower-layer weights tend to be less peaky and diagonal. Experiments on LibriSpeech and Wall Street Journal show that the proposed approach achieves 2.9% and 7.9% relative improvement, respectively, over a vanilla Transformer model."
   ],
   "doi": "10.21437/Interspeech.2022-11441"
  },
  "farooq22b_interspeech": {
   "authors": [
    [
     "Muhammad Umar",
     "Farooq"
    ],
    [
     "Darshan Adiga Haniya",
     "Narayana"
    ],
    [
     "Thomas",
     "Hain"
    ]
   ],
   "title": "Non-Linear Pairwise Language Mappings for Low-Resource Multilingual Acoustic Model Fusion",
   "original": "11449",
   "page_count": 5,
   "order": 982,
   "p1": 4850,
   "pn": 4854,
   "abstract": [
    "Multilingual speech recognition has drawn significant attention as an effective way to compensate data scarcity for low-resource languages. End-to-end (e2e) modelling is preferred over conventional hybrid systems, mainly because of no lexicon requirement. However, hybrid DNN-HMMs still outperform e2e models in limited data scenarios. Furthermore, the problem of manual lexicon creation has been alleviated by publicly available trained models of grapheme-to-phoneme (G2P) and text to IPA transliteration for a lot of languages. In this paper, a novel approach of hybrid DNN-HMM acoustic models fusion is proposed in a multilingual setup for the low-resource languages. Posterior distributions from different monolingual acoustic models, against a target language speech signal, are fused together. A separate regression neural network is trained for each source-target language pair to transform posteriors from source acoustic model to the target language. These networks require very limited data as compared to the ASR training. Posterior fusion yields a relative gain of 14.65% and 6.5% when compared with multilingual and monolingual baselines respectively. Cross-lingual model fusion shows that the comparable results can be achieved without using posteriors from the language dependent ASR."
   ],
   "doi": "10.21437/Interspeech.2022-11449"
  },
  "kang22d_interspeech": {
   "authors": [
    [
     "Zuheng",
     "Kang"
    ],
    [
     "Junqing",
     "Peng"
    ],
    [
     "Jianzong",
     "Wang"
    ],
    [
     "Jing",
     "Xiao"
    ]
   ],
   "title": "SpeechEQ: Speech Emotion Recognition based on Multi-scale Unified Datasets and Multitask Learning",
   "original": "11456",
   "page_count": 5,
   "order": 961,
   "p1": 4745,
   "pn": 4749,
   "abstract": [
    "Speech emotion recognition (SER) has many challenges, but one of the main challenges is that each framework does not have a unified standard. In this paper, we propose SpeechEQ, a framework for unifying SER tasks based on a multi-scale unified metric. This metric can be trained by Multitask Learning (MTL), which includes two emotion recognition tasks of Emotion States Category (EIS) and Emotion Intensity Scale (EIS), and two auxiliary tasks of phoneme recognition and gender recognition. For this framework, we build a Mandarin SER dataset - SpeechEQ Dataset (SEQD). We conducted experiments on the public CASIA and ESD datasets in Mandarin, which exhibit that our method outperforms baseline methods by a relatively large margin, yielding 8.0% and 6.5% improvement in accuracy respectively. Additional experiments on IEMOCAP with four emotion categories (i.e., angry, happy, sad, and neutral) also show the proposed method achieves a state-of-the-art of both weighted accuracy (WA) of 78.16% and unweighted accuracy (UA) of 77.47%."
   ],
   "doi": "10.21437/Interspeech.2022-11456"
  },
  "lee22q_interspeech": {
   "authors": [
    [
     "Jin Woo",
     "Lee"
    ],
    [
     "Eungbeom",
     "Kim"
    ],
    [
     "Junghyun",
     "Koo"
    ],
    [
     "Kyogu",
     "Lee"
    ]
   ],
   "title": "Representation Selective Self-distillation and wav2vec 2.0 Feature Exploration for Spoof-aware Speaker Verification",
   "original": "11460",
   "page_count": 5,
   "order": 588,
   "p1": 2898,
   "pn": 2902,
   "abstract": [
    "Text-to-speech and voice conversion studies are constantly improving to the extent where they can produce synthetic speech almost indistinguishable from bona fide human speech. In this regard, the importance of countermeasures (CM) against synthetic voice attacks of the automatic speaker verification (ASV) systems emerges. Nonetheless, most end-to-end spoofing detection networks are black-box systems, and the answer to what is an effective representation for finding artifacts remains veiled. In this paper, we examine which feature space can effectively represent synthetic artifacts using wav2vec 2.0, and study which architecture can effectively utilize the space. Our study allows us to analyze which attribute of speech signals is advantageous for the CM systems. The proposed CM system achieved 0.31% equal error rate (EER) on ASVspoof 2019 LA evaluation set for the spoof detection task. We further propose a simple yet effective spoofing aware speaker verification (SASV) method, which takes advantage of the disentangled representations from our countermeasure system. Evaluation performed with the SASV Challenge 2022 database show 1.08\\% of SASV EER. Quantitative analysis shows that using the explored feature space of wav2vec 2.0 advantages both spoofing CM and SASV."
   ],
   "doi": "10.21437/Interspeech.2022-11460"
  },
  "zhao22l_interspeech": {
   "authors": [
    [
     "Ding",
     "Zhao"
    ],
    [
     "Zhan",
     "Zhang"
    ],
    [
     "Bin",
     "Yu"
    ],
    [
     "Yuehai",
     "Wang"
    ]
   ],
   "title": "Improve Speech Enhancement using Perception-High-Related Time-Frequency Loss",
   "original": "11471",
   "page_count": 5,
   "order": 1111,
   "p1": 5483,
   "pn": 5487,
   "abstract": [
    "Commonly used speech enhancement (SE) training losses like mean absolute error (MAE) loss and short-time Fourier transformation (STFT) loss suffer from the problem of mismatch with the speech quality, which leads to suboptimal training results. To tackle this problem, we propose a new loss named perception-high-related time-frequency (PHRTF) loss. The proposed loss modifies STFT loss by adding a trainable module named perceptual spectrum mask predictor (PSMP). This module can predict the perceptual spectrum mask (PSM) from the magnitude spectrum of enhanced and clean speech. Further, PHRTF loss multiplies the amplitude error spectrum (AES) with PSM to emphasize perception-relevant loss components to correlate highly with the speech quality. We conduct experiments on the VoiceBank-DEMAND dataset, and the results show that PHRTF loss has a significantly higher correlation with the speech quality than other losses. Meanwhile, PHRTF loss outperforms other losses and improves PESQ by 0.32 over MAE loss and 0.19 over STFT loss on the training of Wave-U-Net. We also apply PHRTF loss to a more advanced SE model, and the training result outperforms other competitive baselines."
   ],
   "doi": "10.21437/Interspeech.2022-11471"
  },
  "fuchs22_interspeech": {
   "authors": [
    [
     "Tzeviya",
     "Fuchs"
    ],
    [
     "Yedid",
     "Hoshen"
    ],
    [
     "Yossi",
     "Keshet"
    ]
   ],
   "title": "Unsupervised Word Segmentation using K Nearest Neighbors",
   "original": "11474",
   "page_count": 5,
   "order": 941,
   "p1": 4646,
   "pn": 4650,
   "abstract": [
    "In this paper, we propose an unsupervised kNN-based approach for word segmentation in speech utterances. Our method relies on self-supervised pre-trained speech representations, and compares each audio segment of a given utterance to its K nearest neighbors within the training set. Our main assumption is that a segment containing more than one word would occur less often than a segment containing a single word. Our method does not require phoneme discovery and is able to operate directly on pre-trained audio representations. This is in contrast to current methods that use a two-stage approach; first detecting the phonemes in the utterance and then detecting word-boundaries according to statistics calculated on phoneme patterns. Experiments on two datasets demonstrate improved results over previous single-stage methods and competitive results on state-of-the-art two-stage methods."
   ],
   "doi": "10.21437/Interspeech.2022-11474"
  },
  "liu22aa_interspeech": {
   "authors": [
    [
     "Yang",
     "Liu"
    ],
    [
     "Haoqin",
     "Sun"
    ],
    [
     "Wenbo",
     "Guan"
    ],
    [
     "Yuqi",
     "Xia"
    ],
    [
     "Zhen",
     "Zhao"
    ]
   ],
   "title": "Discriminative Feature Representation Based on Cascaded Attention Network with Adversarial Joint Loss for Speech Emotion Recognition",
   "original": "11480",
   "page_count": 5,
   "order": 962,
   "p1": 4750,
   "pn": 4754,
   "abstract": [
    "Accurately recognizing emotion from speech is a necessary yet challenging task due to its complexity. A common problem existing in most of the previous studies is that some of the particular emotions are severely misclassified. In this paper, we propose a novel framework integrating cascaded attention and adversarial joint loss for speech emotion recognition, aiming at discriminating the confusions by emphasizing more on the emotions which are difficult to be correctly classified. Specifically, we propose a cascaded attention network to extract effective emotional features, where spatiotemporal attention selectively locates the targeted emotional regions from the input features. In these targeted regions, the self-attention with head fusion captures the long-distance dependence of temporal features. Furthermore, an adversarial joint loss strategy is proposed to distinguish the emotional embeddings with high similarity by the generated hard triplets in an adversarial fashion. Experimental results on the benchmark dataset IEMOCAP demonstrate that our method gains an absolute improvement of 3.17% and 0.39% over state-of-the-art strategies in terms of weighted accuracy (WA) and unweighted accuracy (UA), respectively."
   ],
   "doi": "10.21437/Interspeech.2022-11480"
  },
  "hou22c_interspeech": {
   "authors": [
    [
     "Yuanbo",
     "Hou"
    ],
    [
     "Dick",
     "Botteldooren"
    ]
   ],
   "title": "Event-related data conditioning for acoustic event classification",
   "original": "11481",
   "page_count": 5,
   "order": 317,
   "p1": 1561,
   "pn": 1565,
   "abstract": [
    "Models based on diverse attention mechanisms have recently shined in tasks related to acoustic event classification (AEC). Among them, self-attention is often used in audio-only tasks to help the model recognize different acoustic events. Self-attention relies on the similarity between time frames, and uses global information from the whole segment to highlight specific features within a frame. In real life, information related to acoustic events will attenuate over time, which means the information within some frames around the event deserves more attention than distant time global information that may be unrelated to the event. This paper shows that self-attention may over-enhance certain segments of audio representations, and smooth out the boundaries between events representations and background noises. Hence, this paper proposes an event-related data conditioning (EDC) for AEC. EDC directly works on spectrograms. The idea of EDC is to adaptively select the frame-related attention range based on acoustic features, and gather the event-related local information to represent the frame. Experiments show that: 1) compared with spectrogram-based data augmentation methods and trainable feature weighting and self-attention, EDC outperforms them in both the original-size mode and the augmented mode; 2) EDC effectively gathers event-related local information and enhances boundaries between events and backgrounds, improving the performance of AEC."
   ],
   "doi": "10.21437/Interspeech.2022-11481"
  },
  "badi22_interspeech": {
   "authors": [
    [
     "Alzahra",
     "Badi"
    ],
    [
     "Chungho",
     "Park"
    ],
    [
     "Minseok",
     "Keum"
    ],
    [
     "Miguel",
     "Alba"
    ],
    [
     "Youngsuk",
     "Ryu"
    ],
    [
     "Jeongmin",
     "Bae"
    ]
   ],
   "title": "SKYE: More than a conversational AI",
   "original": "X9001",
   "page_count": 2,
   "order": 175,
   "p1": 859,
   "pn": 860,
   "abstract": [
    "This paper presents how a virtual character was created through a conversational AI system that not only has an open-domain dialog system but also takes visual and audio inputs to create an interactive and interpersonal responses to users. The virtual character has its 3D figure with a particular voice and personality for the immersive AI experience. Multiple AI modules were integrated into the system to make human-like interaction including automatic speech recognition, Text-to-speech model, intelligent open-domain dialog system, and 3D game engine."
   ]
  },
  "nguyen22e_interspeech": {
   "authors": [
    [
     "Tuan-Duy H.",
     "Nguyen"
    ],
    [
     "Duy",
     "Phung"
    ],
    [
     "Duy Tran-Cong",
     "Nguyen"
    ],
    [
     "Hieu Minh",
     "Tran"
    ],
    [
     "Manh",
     "Luong"
    ],
    [
     "Tin Duy",
     "Vo"
    ],
    [
     "Hung Hai",
     "Bui"
    ],
    [
     "Dinh",
     "Phung"
    ],
    [
     "Dat Quoc",
     "Nguyen"
    ]
   ],
   "title": "A Vietnamese-English Neural Machine Translation System",
   "original": "X9002",
   "page_count": 2,
   "order": 1123,
   "p1": 5543,
   "pn": 5544,
   "abstract": [
    "We present VinAI Translate—a system that integrates state-of the-art deep learning technologies in speech and natural language processing to translate speech and text between Vietnamese and English. Experimental results show that our system obtains a state-of-the-art performance for each translation direction, and outperforms Google Translate in both automatic and human evaluations."
   ]
  },
  "nicmanis22_interspeech": {
   "authors": [
    [
     "Davis",
     "Nicmanis"
    ],
    [
     "Askars",
     "Salimbajevs"
    ]
   ],
   "title": "Spoken Dialogue System for Call Centers with Expressive Speech Synthesis",
   "original": "X9003",
   "page_count": 2,
   "order": 1055,
   "p1": 5215,
   "pn": 5216,
   "abstract": [
    "In this paper, we present a prototype of a spoken dialogue system that integrates automatic speech recognition (ASR), natural language understanding (NLU), bot management system, and expressive text-to-speech (TTS). Such a solution can be inte grated into a call center to provide first-line support, replace older interactive voice response (IVR) systems,decrease the load on call center operators and greatly improve client experience. The prototype is primarily designed for the Latvian lan guage, however, support for other languages can be easily added by replacing language-specific components like ASR and TTS."
   ]
  },
  "xu22j_interspeech": {
   "authors": [
    [
     "Yi",
     "Xu"
    ],
    [
     "Anqi",
     "Xu"
    ],
    [
     "Daniel R.",
     "van Niekerk"
    ],
    [
     "Branislav",
     "Gerazov"
    ],
    [
     "Peter",
     "Birkholz"
    ],
    [
     "Paul Konstantin",
     "Krug"
    ],
    [
     "Santitham",
     "Prom-on"
    ],
    [
     "Lorna F.",
     "Halliday"
    ]
   ],
   "title": "Evoc-Learn — High quality simulation of early vocal learning",
   "original": "X9004",
   "page_count": 2,
   "order": 742,
   "p1": 3665,
   "pn": 3666,
   "abstract": [
    "Evoc-Learn is a system for simulating early vocal learning of  spoken language in ways that can overcome some of the major  bottlenecks in vocal learning. The system consists of VocalTractLab, a geometrical three-dimensional vocal tract  model for simulating aeroacoustics and articulatory dynamics,  a coarticulation model for controlling the temporal dynamics of  articulation, and a sensory feedback system for guiding the  learning process. We will demonstrate each component of  Evoc-Learn and show how they work together to simulate the  learning of highly intelligible speech."
   ]
  },
  "arco22_interspeech": {
   "authors": [
    [
     "Leticia",
     "Arco"
    ],
    [
     "Carlos",
     "Mosquera"
    ],
    [
     "Fabjola",
     "Braho"
    ],
    [
     "Yisel",
     "Clavel"
    ],
    [
     "Johan",
     "Loeckx"
    ]
   ],
   "title": "Evaluation of call centre conversations based on a high-level symbolic representation",
   "original": "X9005",
   "page_count": 2,
   "order": 741,
   "p1": 3663,
   "pn": 3664,
   "abstract": [
    "We present a demo that illustrates the performance of our  system to analyse and evaluate call centre conversations. Our  solution can be used at different stages of the quality feedback  loop. The high-level symbolic representation developed on the  context-based intent recognition core module allows for  detecting fine-grained reasons for quality assurance problems  and going in-depth qualitative analysis of how agents and  customers interact. We illustrate the evaluation and insights of  real-life conversations provided by a Belgian call centre.  Participants can interact with the demo by playing with call  annotation, recommendations, and diverse parameters."
   ]
  },
  "tan22c_interspeech": {
   "authors": [
    [
     "Kye Min",
     "Tan"
    ],
    [
     "Richeng",
     "Duan"
    ],
    [
     "Xin",
     "Huang"
    ],
    [
     "Bowei",
     "Zou"
    ],
    [
     "Xuan",
     "Long Do"
    ]
   ],
   "title": "A Deep Learning Platform for Language Education Research and Development",
   "original": "X9006",
   "page_count": 2,
   "order": 800,
   "p1": 3949,
   "pn": 3950,
   "abstract": [
    "We present a language education research, development, and  deployment platform that allows researchers, developers and teachers alike to easily design pipelines and collaborate on components. We also show an example use case of our platform with components such as speech evaluation, question generation, and dialog systems. To our knowledge, this is the first language education research platform which supports the integration of speech, NLP, and various custom components."
   ]
  },
  "draxler22_interspeech": {
   "authors": [
    [
     "Christoph",
     "Draxler"
    ],
    [
     "Julian",
     "Pomp"
    ]
   ],
   "title": "OCTRA – An Innovative Approach to Orthographic Transcription",
   "original": "X9007",
   "page_count": 2,
   "order": 1056,
   "p1": 5217,
   "pn": 5218,
   "abstract": [
    "Octra is an open-source editor for web-based orthographic transcription. It runs in a web browser on any platform, and operates in an online and a local mode. In the local mode, files are opened via drag & drop on the browser window, in the on line mode the transcriber logs in and selects the transcription project; the next file to transcribe is opened immediately. Octra features three different viewers: dictaphone, linear and 2D, and it may access external web services, e. g. for ASR or time-aligning the transcript and the signal. It supports various common file formats for interoperability, for import and export. \nIn Octra, a transcriber places boundaries in the signal and then transcribes these transcription units. The length of the tran scription units is determined by the personal preferences of the transcriber, and may vary with the type of recordings. "
   ]
  },
  "ingle22_interspeech": {
   "authors": [
    [
     "Digvijay",
     "Ingle"
    ],
    [
     "Ayush",
     "Kumar"
    ],
    [
     "Krishnachaitanya",
     "Gogineni"
    ],
    [
     "Jithendra",
     "Vepa"
    ]
   ],
   "title": "Real-Time Monitoring of Silences in Contact Center Conversations",
   "original": "X9008",
   "page_count": 2,
   "order": 395,
   "p1": 1951,
   "pn": 1952,
   "abstract": [
    "Contact center conversations often contain segments with hold music, automatic-recorded-messages or pure silences, where neither the customer nor the agent is speaking. We refer to these segments as Conversational Silences [1]. These silences when continued beyond an acceptable level can negatively impact im portant contact center KPIs, like average handling time, agent efficiency, etc. and may lead to poor customer experience. As a result, it becomes imperative for contact centers to identify si lences in conversations and define mechanisms to better handle them. In this paper, we propose a cascaded system consisting of an ASR engine, a silence detector block, a text classification layer and a heuristic engine to surface instances in calls where agents are missing the protocols to handle silences. This system is used to trigger alerts to agents in real time thus enabling them to course correct while being on call with the customer. More over, these instances can also be surfaced to their supervisors so as to identify agents who are frequently missing these protocols and thereby design dedicated coaching sessions."
   ]
  },
  "park22e_interspeech": {
   "authors": [
    [
     "Tae Jin",
     "Park"
    ],
    [
     "Nithin Rao",
     "Koluguri"
    ],
    [
     "Fei",
     "Jia"
    ],
    [
     "Jagadeesh",
     "Balam"
    ],
    [
     "Boris",
     "Ginsburg"
    ]
   ],
   "title": "NeMo Open Source Speaker Diarization System",
   "original": "X9009",
   "page_count": 2,
   "order": 172,
   "p1": 853,
   "pn": 854,
   "abstract": [
    "We introduce an open-source speaker diarization system which is part of the NeMo conversational AI toolkit. During the Show and Tell session, we will present an interactive system which demonstrates both online and offline speaker diarization. The audience would be able to test the speaker diarization system by recording their voice. We believe that our demo session would be an excellent opportunity to learn and experience how a speaker diarization system can be implemented for real-life applications using an open source toolkit."
   ]
  },
  "zielinski22_interspeech": {
   "authors": [
    [
     "Konrad",
     "Zieliński"
    ],
    [
     "Marek",
     "Grzelec"
    ],
    [
     "Martin",
     "Hagmüller"
    ]
   ],
   "title": "Humanizing bionic voice: interactive demonstration of aesthetic design and control factors influencing the devices assembly and waveshape engineering",
   "original": "X9010",
   "page_count": 2,
   "order": 396,
   "p1": 1953,
   "pn": 1954,
   "abstract": [
    "Electrolarynx is a speech aid providing voice source for  people who have their larynx resected. Bionic voices (BV)  extend the device capabilities by combining the biological and  technical parts in a functioning communicative system. This  interactive demonstration illustrates the challenges in material  design and sound engineering in the domain of BV which are  aimed at enriching the performance of a conversing dyad in  the social context. The factors of control and aesthetics will be  introduced by alaryngeal speaker presenting novel prototypes.  The participants will experience the concepts in a multimodal demo, try various devices and talk with a laryngectomee. "
   ]
  },
  "jin22b_interspeech": {
   "authors": [
    [
     "Yujia",
     "Jin"
    ],
    [
     "Yanlu",
     "Xie"
    ],
    [
     "Jinsong",
     "Zhang"
    ]
   ],
   "title": "A VR Interactive 3D Mandarin Pronunciation Teaching Model",
   "original": "X9011",
   "page_count": 2,
   "order": 801,
   "p1": 3951,
   "pn": 3952,
   "abstract": [
    "This paper presents a Virtual Reality interactive three dimensional Mandarin pronunciation teaching model with  higher clarity, more friendly interaction and more comfortable  user experience. Our system provides four learning modes:  initial, final, word and confusion. According to personal needs,  learners can switch the demo views, customize visualization of  3D anatomical model’s physiological structure, control play  speed of 3D animation, and change position to observe  movements and deformations of articulators. Moreover, the  system provides assistant pronunciation guidance, such as the  pronunciation method, 2D oral section animation, target zone  of critical articulators, and contrastive models of confusion  minimal pairs to help learners understand the essentials of  Mandarin pronunciation."
   ]
  },
  "ronssin22_interspeech": {
   "authors": [
    [
     "Damien",
     "Ronssin"
    ],
    [
     "Milos",
     "Cernak"
    ]
   ],
   "title": "Application for Real-time Personalized Speaker Extraction",
   "original": "X9012",
   "page_count": 2,
   "order": 397,
   "p1": 1955,
   "pn": 1956,
   "abstract": [
    "This short paper demonstrates an audio processing desktop ap plication that allows isolating in real-time the voice of a spe cific speaker from the possibly noisy audio input after a short enrollment phase. The machine learning model embedded in this application suppresses all other sounds than the target voice from the incoming audio stream, including disturbing distractor voices. In the context of a growing need for video-collaboration solutions, personalized speech enhancement enables the use of such technologies in more challenging acoustic environments, i.e., in the presence of near distractor speech. In this situation, classical speech enhancement systems typically fail as they do not filter out any speech, hence the need for personalized meth ods. The presented application is an all-in-one solution for per sonalized speech enhancement: it allows the user to enroll and then to apply the effect seamlessly for one-to-one or one-to many online meetings."
   ]
  },
  "paul22_interspeech": {
   "authors": [
    [
     "Debjyoti",
     "Paul"
    ],
    [
     "Yutong",
     "Pang"
    ],
    [
     "Szu-Jui",
     "Chen"
    ],
    [
     "Xuedong",
     "Zhang"
    ]
   ],
   "title": "Improving Data Driven Inverse Text Normalization using Data Augmentation and Machine Translation",
   "original": "X9013",
   "page_count": 2,
   "order": 1058,
   "p1": 5221,
   "pn": 5222,
   "abstract": [
    "Inverse text normalization (ITN) is used to convert the spoken form output of an automatic speech recognition (ASR) system to a written form. Traditional handcrafted ITN rules can be complex to transcribe and maintain. Meanwhile neural modeling approaches require quality large-scale spoken-written pair exam ples in the same or similar domain as the ASR system (in-domain data), to train. Both these approaches require costly and complex annotation. In this paper, we present a data augmentation tech nique with neural machine translation that effectively generates rich spoken-written pairs for high and low resource languages effectively. We empirically demonstrate that ITN models (in tar get language) trained using our data augmentation with machine translation technique can achieve similar performance as ITN models (en) trained directly with in-domain language."
   ]
  },
  "ivanko22_interspeech": {
   "authors": [
    [
     "Denis",
     "Ivanko"
    ],
    [
     "Dmitry",
     "Ryumin"
    ],
    [
     "Alexey",
     "Kashevnik"
    ],
    [
     "Alexandr",
     "Axyonov"
    ],
    [
     "Andrey",
     "Kitenko"
    ],
    [
     "Igor",
     "Lashkov"
    ],
    [
     "Alexey",
     "Karpov"
    ]
   ],
   "title": "DAVIS: Driver’s Audio-Visual Speech recognition",
   "original": "X9014",
   "page_count": 2,
   "order": 232,
   "p1": 1141,
   "pn": 1142,
   "abstract": [
    "DAVIS is a driver’s audio-visual assistive system intended to  improve accuracy and robustness of speech recognition of the  most frequent drivers’ requests in natural driving conditions.  Since speech recognition in driving condition is highly  challenging due to acoustic noises, active head turns, pose  variation, distance to recording devices, lightning conditions,  etc. We rely on multimodal information and use both automatic  lip-reading system for visual stream and ASR for audio stream processing. We have trained audio and video models on own  RUSAVIC dataset containing in-the-wild audio and video recordings of 20 drivers. The recognition application comprises  a graphical user interface and modules for audio and video  signal acquisition, analysis, and recognition. The obtained  results demonstrate rather high performance of DAVIS and also  the fundamental possibility of recognizing speech commands  by using video modality, even in such difficult natural  conditions as driving. "
   ]
  },
  "schafer22_interspeech": {
   "authors": [
    [
     "P.",
     "Schäfer"
    ],
    [
     "P. A.",
     "Pérez-Toro"
    ],
    [
     "P.",
     "Klumpp"
    ],
    [
     "J. R.",
     "Orozco-Arroyave"
    ],
    [
     "E.",
     "Nöth"
    ],
    [
     "K.",
     "Maier"
    ],
    [
     "A.",
     "Abad"
    ],
    [
     "M.",
     "Schuster"
    ],
    [
     "T.",
     "Arias-Vergara"
    ]
   ],
   "title": "CoachLea: an Android Application to Evaluate the Speech Production and Perception of Children with Hearing Loss",
   "original": "X9015",
   "page_count": 2,
   "order": 399,
   "p1": 1959,
   "pn": 1960,
   "abstract": [
    "Hearing loss can affect children’s language, speech, and social skills development. Hearing problems can result from impaired auditory feedback due to various reasons such as trauma, a clin ical condition, genetic alterations, and infections, among oth ers. Early treatment is the key to successful hearing and speech rehabilitation if the hearing loss occurs before or during spo ken language acquisition. In this work, we present CoachLea, an Android application to support the clinical evaluation and therapy of the speech production and perception of children with hearing loss. The app includes numerous daily exercises to capture speech and hearing data continuously and longitudi nally using a game-like user interface. Speech exercises include the “Snail race”, “Animal Sounds”, and “Image identification”, whereas the hearing exercise consists of a word identification game based on minimal pairs with speech-in-noise recordings. In the long term, CoachLea aims to be a tool that supports the therapy of children with hearing loss."
   ]
  },
  "vandevreken22_interspeech": {
   "authors": [
    [
     "Emelie",
     "Van De Vreken"
    ],
    [
     "Korin",
     "Richmond"
    ],
    [
     "Catherine",
     "Lai"
    ]
   ],
   "title": "Voice Puppetry with FastPitch",
   "original": "X9016",
   "page_count": 2,
   "order": 1057,
   "p1": 5219,
   "pn": 5220,
   "abstract": [
    "Affective speech synthesis is an active research area, but re cent approaches usually lack the full, fine-grained controllabil ity to produce utterances with any exact affect intended by the user. We propose a puppetry tool based on FastPitch to help model output convey any required suprasegmental meanings. Users can choose any trained FastPitch model, and which fea tures should be mimicked, making the approach fine-grained and language-independent."
   ]
  },
  "haider22_interspeech": {
   "authors": [
    [
     "Fasih",
     "Haider"
    ],
    [
     "Saturnino",
     "Luz"
    ]
   ],
   "title": "An Automated Mood Diary for Older User’s using Ambient Assisted Living Recorded Speech",
   "original": "X9017",
   "page_count": 2,
   "order": 400,
   "p1": 1961,
   "pn": 1962,
   "abstract": [
    "In this paper, we describe a system for recording of mood di aries in the context of an ambient assisted living and intelli gent coaching environment, which ensures privacy by design. The system performs affect recognition in speech features with out recording speech content in any form. We demonstrate re sults of affect recognition models tested on data collected in care-home settings during the SAAM project (Supporting Ac tive Ageing through Multimodal Coaching) using our custom designed audio collection hardware. The proposed system was trained using Bulgarian speech augmented with training data obtained from comparable mood diaries recorded in Scottish English. A degree of transfer learning of Scottish English speech to Bulgarian speech was demonstrated."
   ]
  },
  "siddarth22_interspeech": {
   "authors": [
    [
     "C",
     "Siddarth"
    ],
    [
     "Sathvik",
     "Udupa"
    ],
    [
     "Prasanta",
     "Kumar Ghosh"
    ]
   ],
   "title": "Watch Me Speak: 2D Visualization of Human Mouth during Speech",
   "original": "X9018",
   "page_count": 2,
   "order": 743,
   "p1": 3667,
   "pn": 3668,
   "abstract": [
    "We present a web interface to visualize the midsagittal plane of the human mouth during speech. Given an articulated sentence, we estimate the corresponding articulatory trajecto ries and visualize the same. This web interface provides a comprehensive view of the articulators’ trajectories and could serve as an important tool for speech training."
   ]
  },
  "lin22e_interspeech": {
   "authors": [
    [
     "Baihan",
     "Lin"
    ]
   ],
   "title": "Voice2Alliance: Automatic Speaker Diarization and Quality Assurance of Conversational Alignment",
   "original": "X9019",
   "page_count": 2,
   "order": 173,
   "p1": 855,
   "pn": 856,
   "abstract": [
    "We propose a real-time AI system to conduct sentence-level quality assurance of conversational alignment based on speakerdiarized dialogues transcribed from automatic speech recognition of continuous audio stream. This system utilizes two new interactive engine: (1) an online registration-free speaker diarization component to perform separation of speech utterances of multiple speakers in the conversations that learns from user feedback; (2) a turn-level scoring mechanism that infers the conversation quality by computing a similarity score between the deep embeddings of a user-specified scoring inventory of interest, and the current sentence that the user is speaking. These real-time scores are known to be predictive to\nsuccessful conversational outcome (such as relating to the therapeutic working alliance, which is an important indicator of clinical psychotherapy  outcome). Other than evaluating the empirical advantages of the core components on existing dataset, we demonstrate the effectiveness of this system in a web-based application at https://www.baihan.nyc/viz/Voice2Alliance/."
   ]
  },
  "bhattacharya22b_interspeech": {
   "authors": [
    [
     "Debarpan",
     "Bhattacharya"
    ],
    [
     "Debottam",
     "Dutta"
    ],
    [
     "Neeraj",
     "Kumar Sharma"
    ],
    [
     "Srikanth",
     "Raj Chetupalli"
    ],
    [
     "Pravin",
     "Mote"
    ],
    [
     "Sriram",
     "Ganapathy"
    ],
    [
     "Chandrakiran",
     "C"
    ],
    [
     "Sahiti",
     "Nori"
    ],
    [
     "Suhail",
     "K K"
    ],
    [
     "Sadhana",
     "Gonuguntla"
    ],
    [
     "Murali",
     "Alagesan"
    ]
   ],
   "title": "Coswara: A website application enabling COVID-19 screening by analysing respiratory sound samples and health symptoms",
   "original": "X9020",
   "page_count": 2,
   "order": 398,
   "p1": 1957,
   "pn": 1958,
   "abstract": [
    "The COVID-19 pandemic has accelerated research on design of alternative, quick and effective COVID-19 diagnosis approaches. In this paper, we describe the Coswara tool, a website application designed to enable COVID-19 detection by analysing respiratory sound samples and health symptoms. A user using this service can log into a website using any device connected to the internet, provide there current health symptom information and record few sound sampled corresponding to breathing, cough, and speech. Within a minute of analysis of this information on a cloud server the website tool will output a COVID-19 probability score to the user. As the COVID-19 pandemic continues to demand massive and scalable population level testing, we hypothesize that the proposed tool provides a potential solution towards this."
   ]
  },
  "kumar22d_interspeech": {
   "authors": [
    [
     "Rishabh",
     "Kumar"
    ],
    [
     "Devaraja",
     "Adiga"
    ],
    [
     "Mayank",
     "Kothari"
    ],
    [
     "Jatin",
     "Dalal"
    ],
    [
     "Ganesh",
     "Ramakrishnan"
    ],
    [
     "Preethi",
     "Jyothi"
    ]
   ],
   "title": "VAgyojaka: An Annotating and Post-Editing Tool for Automatic Speech Recognition",
   "original": "X9021",
   "page_count": 2,
   "order": 174,
   "p1": 857,
   "pn": 858,
   "abstract": [
    "Vagyojaka is an open-source post-editing and annotation tool ¯ for automatic speech recognition (ASR) that aims to reduce the human effort required to correct the ASR results. We adopt a dictionary-based lookup method to highlight the incorrect words in the ASR transcript and give suggestions by generat ing the closest valid words. For curating the speech corpus, we provide a rich list of tagset that captures various spoken audio features. Further, we conducted a user study to evaluate the ef fectiveness of our tool and observed that post-editing requires 1/3 lesser time than editing without using our tool. The user study can be found on our website 1."
   ]
  }
 },
 "sessions": [
  {
   "title": "Speech Synthesis: Toward end-to-end synthesis",
   "papers": [
    "cho22_interspeech",
    "bae22_interspeech",
    "lenglet22_interspeech",
    "ju22_interspeech",
    "lim22_interspeech"
   ]
  },
  {
   "title": "Technology for Disordered Speech",
   "papers": [
    "turrisi22_interspeech",
    "yue22_interspeech",
    "prananta22_interspeech",
    "violeta22_interspeech",
    "bhat22_interspeech",
    "hernandez22_interspeech"
   ]
  },
  {
   "title": "Neural Network Training Methods for ASR I",
   "papers": [
    "lee22b_interspeech",
    "chan22_interspeech",
    "karakasidis22_interspeech",
    "baby22_interspeech",
    "hard22_interspeech"
   ]
  },
  {
   "title": "Acoustic Phonetics and Prosody",
   "papers": [
    "song22b_interspeech",
    "ribeiro22_interspeech",
    "magistro22_interspeech",
    "oh22_interspeech",
    "bradshaw22_interspeech"
   ]
  },
  {
   "title": "Spoken Machine Translation",
   "papers": [
    "tsiamas22_interspeech",
    "zhao22g_interspeech",
    "zaidi22_interspeech",
    "fukuda22b_interspeech",
    "r22_interspeech"
   ]
  },
  {
   "title": "(Multimodal) Speech Emotion Recognition I",
   "papers": [
    "ahn22_interspeech",
    "kim22d_interspeech",
    "lee22e_interspeech",
    "triantafyllopoulos22b_interspeech",
    "prabhu22_interspeech",
    "perez22_interspeech",
    "chou22_interspeech",
    "dhamyal22_interspeech"
   ]
  },
  {
   "title": "Dereverberation, Noise Reduction, and Speaker Extraction",
   "papers": [
    "vuho22_interspeech",
    "ho22_interspeech",
    "kim22i_interspeech",
    "hung22_interspeech",
    "hwang22b_interspeech",
    "muckenhirn22_interspeech",
    "pandey22_interspeech",
    "vuong22_interspeech",
    "peng22d_interspeech",
    "delcroix22_interspeech",
    "shi22e_interspeech",
    "lemercier22_interspeech"
   ]
  },
  {
   "title": "Source Separation II",
   "papers": [
    "schmidt22_interspeech",
    "saijo22_interspeech",
    "saijo22b_interspeech",
    "bellows22_interspeech",
    "saijo22c_interspeech",
    "borsdorf22_interspeech",
    "guzik22_interspeech",
    "deadman22_interspeech",
    "boeddeker22_interspeech",
    "mun22_interspeech"
   ]
  },
  {
   "title": "Embedding and Network Architecture for Speaker Recognition",
   "papers": [
    "bousquet22_interspeech",
    "liu22f_interspeech",
    "liu22g_interspeech",
    "liu22h_interspeech",
    "ruida22_interspeech",
    "zhang22h_interspeech",
    "zhang22j_interspeech",
    "tian22b_interspeech",
    "sang22_interspeech",
    "cai22_interspeech",
    "li22l_interspeech",
    "peng22b_interspeech",
    "wei22d_interspeech"
   ]
  },
  {
   "title": "Speech Representation II",
   "papers": [
    "chen22_interspeech",
    "feinberg22_interspeech",
    "shor22_interspeech",
    "li22b_interspeech",
    "sadeghi22_interspeech",
    "zhao22h_interspeech",
    "hansen22_interspeech",
    "bergsma22_interspeech",
    "elbanna22_interspeech",
    "wang22w_interspeech",
    "yadav22_interspeech",
    "zhang22ca_interspeech",
    "faridee22_interspeech",
    "ashihara22_interspeech",
    "azeemi22_interspeech"
   ]
  },
  {
   "title": "Speech Synthesis: Linguistic Processing, Paradigms and Other Topics II",
   "papers": [
    "tae22_interspeech",
    "chen22b_interspeech",
    "borsos22_interspeech",
    "zhang22b_interspeech",
    "he22b_interspeech",
    "zhu22_interspeech",
    "mathur22_interspeech",
    "zhang22i_interspeech",
    "ni22_interspeech",
    "tran22_interspeech",
    "valentinibotinhao22_interspeech",
    "chen22j_interspeech",
    "tan22_interspeech",
    "ploujnikov22_interspeech",
    "bakhturina22_interspeech",
    "virkar22_interspeech",
    "bai22c_interspeech",
    "kameoka22_interspeech",
    "abeysinghe22_interspeech"
   ]
  },
  {
   "title": "Other Topics in Speech Recognition",
   "papers": [
    "saeed22_interspeech",
    "kanda22b_interspeech",
    "makishima22_interspeech",
    "zhang22k_interspeech",
    "jia22_interspeech",
    "yang22k_interspeech",
    "kons22_interspeech",
    "antonova22_interspeech",
    "cho22b_interspeech",
    "yu22b_interspeech"
   ]
  },
  {
   "title": "Audio Deep PLC (Packet Loss Concealment) Challenge",
   "papers": [
    "guan22_interspeech",
    "valin22_interspeech",
    "liu22s_interspeech",
    "diener22_interspeech",
    "li22ea_interspeech"
   ]
  },
  {
   "title": "Robust Speaker Recognition",
   "papers": [
    "kim22b_interspeech",
    "yang22j_interspeech",
    "wang22r_interspeech",
    "stafylakis22_interspeech",
    "luu22_interspeech",
    "kataria22_interspeech"
   ]
  },
  {
   "title": "Speech Production",
   "papers": [
    "yoshinaga22_interspeech",
    "udupa22_interspeech",
    "rakotomalala22_interspeech",
    "vanniekerk22_interspeech",
    "lee22m_interspeech",
    "nagamine22_interspeech"
   ]
  },
  {
   "title": "Speech Quality Assessment",
   "papers": [
    "manocha22b_interspeech",
    "manocha22c_interspeech",
    "kawahara22_interspeech",
    "li22o_interspeech",
    "zaiem22_interspeech",
    "mumtaz22_interspeech"
   ]
  },
  {
   "title": "Language Modeling and Lexical Modeling for ASR",
   "papers": [
    "vangysel22_interspeech",
    "dingliwal22_interspeech",
    "huang22j_interspeech",
    "breiner22_interspeech",
    "chien22b_interspeech"
   ]
  },
  {
   "title": "Challenges and Opportunities for Signal Processing and Machine Learning for Multiple Smart Devices",
   "papers": [
    "masuyama22_interspeech",
    "ciccarelli22_interspeech",
    "agaskar22_interspeech",
    "koppelmann22_interspeech",
    "nespoli22_interspeech",
    "pandey22c_interspeech"
   ]
  },
  {
   "title": "Speech Processing & Measurement",
   "papers": [
    "fietkau22_interspeech",
    "matsui22_interspeech",
    "ahn22b_interspeech",
    "coppietersdegibson22_interspeech",
    "machado22_interspeech",
    "toya22_interspeech",
    "mohapatra22_interspeech",
    "dejong22_interspeech",
    "georges22_interspeech",
    "wu22i_interspeech",
    "ashokumar22_interspeech"
   ]
  },
  {
   "title": "Speech Synthesis: Acoustic Modeling and Neural Waveform Generation I",
   "papers": [
    "kim22c_interspeech",
    "saeki22_interspeech",
    "mitsui22b_interspeech",
    "koizumi22_interspeech",
    "park22_interspeech",
    "bae22b_interspeech",
    "subramani22_interspeech",
    "lam22_interspeech",
    "nikitaras22_interspeech",
    "siuzdak22_interspeech",
    "vovk22_interspeech",
    "liu22z_interspeech",
    "yoneyama22_interspeech"
   ]
  },
  {
   "title": "Show and Tell I",
   "papers": [
    "park22e_interspeech",
    "lin22e_interspeech",
    "kumar22d_interspeech",
    "badi22_interspeech"
   ]
  },
  {
   "title": "Spatial Audio",
   "papers": [
    "munakata22_interspeech",
    "chen22e_interspeech",
    "xiong22b_interspeech",
    "wu22d_interspeech",
    "ouyang22_interspeech",
    "wang22s_interspeech",
    "yin22b_interspeech",
    "fu22c_interspeech",
    "patterson22_interspeech",
    "li22ba_interspeech",
    "aroudi22_interspeech"
   ]
  },
  {
   "title": "Single-channel Speech Enhancement II",
   "papers": [
    "ge22_interspeech",
    "chen22c_interspeech",
    "cheng22_interspeech",
    "xiong22_interspeech",
    "cao22_interspeech",
    "wei22_interspeech",
    "zhang22l_interspeech",
    "guo22c_interspeech",
    "zhang22m_interspeech",
    "huang22e_interspeech",
    "xu22f_interspeech",
    "xu22g_interspeech",
    "chen22k_interspeech",
    "yu22_interspeech",
    "jiang22b_interspeech",
    "thakker22_interspeech",
    "sato22b_interspeech"
   ]
  },
  {
   "title": "Novel Models and Training Methods for ASR II",
   "papers": [
    "mehmood22_interspeech",
    "fu22_interspeech",
    "liu22k_interspeech",
    "wei22e_interspeech",
    "ma22_interspeech",
    "audhkhasi22_interspeech",
    "weiran22_interspeech",
    "zeineldeen22_interspeech",
    "laptev22_interspeech",
    "sustek22_interspeech",
    "miao22c_interspeech",
    "zhang22fa_interspeech",
    "handekabil22_interspeech",
    "tanaka22_interspeech",
    "maekaku22_interspeech"
   ]
  },
  {
   "title": "Spoken Dialogue Systems and Multimodality",
   "papers": [
    "uchida22_interspeech",
    "wu22b_interspeech",
    "nihei22_interspeech",
    "bekal22_interspeech",
    "zhou22b_interspeech",
    "feng22c_interspeech",
    "dao22_interspeech",
    "zhou22g_interspeech",
    "sartzetaki22_interspeech",
    "wang22da_interspeech",
    "liesenfeld22_interspeech",
    "zhu22f_interspeech",
    "ochi22_interspeech"
   ]
  },
  {
   "title": "Show and Tell I(VR)",
   "papers": [
    "ivanko22_interspeech"
   ]
  },
  {
   "title": "Speech Emotion Recognition I",
   "papers": [
    "vaaras22_interspeech",
    "chen22n_interspeech",
    "su22_interspeech",
    "parry22_interspeech",
    "gudmalwar22_interspeech",
    "goncalves22_interspeech"
   ]
  },
  {
   "title": "Single-channel Speech Enhancement I",
   "papers": [
    "koizumi22b_interspeech",
    "sanada22_interspeech",
    "lee22d_interspeech",
    "shchekotov22_interspeech",
    "tal22_interspeech",
    "shin22c_interspeech"
   ]
  },
  {
   "title": "Speech Synthesis: New Applications",
   "papers": [
    "goswami22_interspeech",
    "bensimon22_interspeech",
    "fong22_interspeech",
    "huang22i_interspeech",
    "koch22_interspeech",
    "krug22_interspeech"
   ]
  },
  {
   "title": "Spoken Language Understanding I",
   "papers": [
    "lee22c_interspeech",
    "cattan22_interspeech",
    "heo22b_interspeech",
    "wang22y_interspeech",
    "raju22_interspeech",
    "garg22_interspeech"
   ]
  },
  {
   "title": "Inclusive and Fair Speech Technologies I",
   "papers": [
    "ogayo22_interspeech",
    "dheram22_interspeech",
    "chan22b_interspeech"
   ]
  },
  {
   "title": "Inclusive and Fair Speech Technologies II",
   "papers": [
    "zanonboito22_interspeech",
    "johnson22_interspeech",
    "kukk22_interspeech",
    "toussaint22_interspeech",
    "trinh22_interspeech"
   ]
  },
  {
   "title": "Phonetics I",
   "papers": [
    "kunihara22_interspeech",
    "kirchhubel22_interspeech",
    "jeon22_interspeech",
    "hwang22_interspeech",
    "alicehajic22_interspeech",
    "langheinrich22_interspeech",
    "ludusan22_interspeech",
    "delvaux22_interspeech",
    "spinu22_interspeech",
    "liao22_interspeech",
    "buech22_interspeech"
   ]
  },
  {
   "title": "Multi-, Cross-lingual and Other Topics in ASR I",
   "papers": [
    "rumberg22b_interspeech",
    "soky22_interspeech",
    "mussakhojayeva22_interspeech",
    "szalay22_interspeech",
    "scharf22_interspeech",
    "flechl22_interspeech",
    "yadavalli22_interspeech",
    "xie22b_interspeech"
   ]
  },
  {
   "title": "Zero, low-resource and multi-modal speech recognition I",
   "papers": [
    "lee22_interspeech",
    "deseyssel22_interspeech",
    "birladeanu22_interspeech",
    "kim22k_interspeech",
    "miller22_interspeech",
    "hwang22c_interspeech",
    "vandermerwe22_interspeech"
   ]
  },
  {
   "title": "Speaker Embedding and Diarization",
   "papers": [
    "zheng22_interspeech",
    "qin22_interspeech",
    "wang22j_interspeech",
    "brummer22_interspeech",
    "gu22_interspeech",
    "chen22f_interspeech",
    "he22c_interspeech",
    "yue22b_interspeech",
    "yang22r_interspeech",
    "liu22t_interspeech",
    "tanveer22_interspeech",
    "kinoshita22_interspeech",
    "wang22ha_interspeech"
   ]
  },
  {
   "title": "Acoustic Event Detection and Classification",
   "papers": [
    "liang22_interspeech",
    "liu22d_interspeech",
    "xu22c_interspeech",
    "yang22e_interspeech",
    "tripathi22_interspeech",
    "li22f_interspeech",
    "wang22i_interspeech",
    "hu22d_interspeech",
    "park22c_interspeech",
    "shao22_interspeech",
    "xin22_interspeech",
    "wang22aa_interspeech",
    "ye22b_interspeech",
    "hou22c_interspeech"
   ]
  },
  {
   "title": "Speech Synthesis: Acoustic Modeling and Neural Waveform Generation II",
   "papers": [
    "guo22_interspeech",
    "yin22_interspeech",
    "luong22_interspeech",
    "liu22c_interspeech",
    "yuan22_interspeech",
    "xu22d_interspeech",
    "du22b_interspeech",
    "mengnan22_interspeech",
    "yi22_interspeech",
    "guo22d_interspeech",
    "li22q_interspeech",
    "gorai22_interspeech",
    "kanagawa22_interspeech",
    "kaneko22_interspeech",
    "miao22b_interspeech",
    "tachibana22_interspeech"
   ]
  },
  {
   "title": "ASR: Architecture and Search",
   "papers": [
    "variani22_interspeech",
    "wu22_interspeech",
    "shi22b_interspeech",
    "zhang22g_interspeech",
    "liu22j_interspeech",
    "li22i_interspeech",
    "bai22_interspeech",
    "yang22n_interspeech",
    "wang22t_interspeech",
    "rathod22_interspeech",
    "weiran22b_interspeech",
    "lin22d_interspeech",
    "ding22b_interspeech",
    "ding22c_interspeech",
    "xu22i_interspeech"
   ]
  },
  {
   "title": "Spoken Language Processing II",
   "papers": [
    "jia22b_interspeech",
    "nguyen22_interspeech",
    "wang22f_interspeech",
    "yang22h_interspeech",
    "li22j_interspeech",
    "deng22b_interspeech",
    "tran22b_interspeech",
    "markitantov22_interspeech",
    "chien22_interspeech",
    "chen22o_interspeech",
    "liu22u_interspeech",
    "tam22_interspeech",
    "dong22b_interspeech"
   ]
  },
  {
   "title": "Source Separation I",
   "papers": [
    "pan22b_interspeech",
    "berg22_interspeech",
    "tzinis22_interspeech",
    "liu22w_interspeech",
    "markovic22_interspeech"
   ]
  },
  {
   "title": "ASR Technologies and Systems",
   "papers": [
    "nozaki22_interspeech",
    "pupier22_interspeech",
    "chang22_interspeech",
    "chang22b_interspeech",
    "lehecka22_interspeech",
    "schoburgcarrillodemira22_interspeech"
   ]
  },
  {
   "title": "Speech Perception",
   "papers": [
    "kishiyama22_interspeech",
    "perezramon22_interspeech",
    "leemann22_interspeech",
    "zhang22ba_interspeech",
    "lee22n_interspeech",
    "zurita22_interspeech"
   ]
  },
  {
   "title": "Spoken Term Detection and Voice Search",
   "papers": [
    "shin22_interspeech",
    "abdullah22_interspeech",
    "yang22l_interspeech",
    "svec22_interspeech",
    "jose22_interspeech",
    "nayak22_interspeech"
   ]
  },
  {
   "title": "Speech and Language in Health: From Remote Monitoring to Medical Conversations I",
   "papers": [
    "soltau22_interspeech",
    "wu22c_interspeech",
    "fara22_interspeech",
    "romana22_interspeech",
    "botelho22_interspeech"
   ]
  },
  {
   "title": "Speech Synthesis: Linguistic Processing, Paradigms and Other Topics I",
   "papers": [
    "chen22d_interspeech",
    "park22b_interspeech",
    "raitio22_interspeech",
    "song22d_interspeech",
    "comini22_interspeech"
   ]
  },
  {
   "title": "Show and Tell II",
   "papers": [
    "ingle22_interspeech",
    "zielinski22_interspeech",
    "ronssin22_interspeech",
    "bhattacharya22b_interspeech",
    "schafer22_interspeech",
    "haider22_interspeech"
   ]
  },
  {
   "title": "Multimodal Speech Emotion Recognition and Paralinguistics",
   "papers": [
    "xu22_interspeech",
    "fernau22_interspeech",
    "qian22_interspeech",
    "gupta22_interspeech",
    "zheng22b_interspeech",
    "wei22b_interspeech",
    "zhang22r_interspeech",
    "yang22q_interspeech",
    "tao22_interspeech",
    "tao22b_interspeech",
    "li22v_interspeech",
    "wang22z_interspeech",
    "sardhaei22_interspeech",
    "baird22_interspeech",
    "saraf22_interspeech"
   ]
  },
  {
   "title": "Neural Transducers, Streaming ASR and Novel ASR Models",
   "papers": [
    "fasoli22_interspeech",
    "sun22_interspeech",
    "hou22b_interspeech",
    "weninger22_interspeech",
    "zhou22c_interspeech",
    "gao22b_interspeech",
    "kuang22_interspeech",
    "wu22h_interspeech",
    "lee22l_interspeech",
    "mahadeokar22_interspeech",
    "li22y_interspeech",
    "albesano22_interspeech",
    "shinohara22_interspeech",
    "an22_interspeech",
    "wu22j_interspeech"
   ]
  },
  {
   "title": "Zero, Low-resource and Multi-Modal Speech Recognition II",
   "papers": [
    "xu22b_interspeech",
    "shi22_interspeech",
    "algayres22_interspeech",
    "xu22e_interspeech",
    "qin22b_interspeech"
   ]
  },
  {
   "title": "Atypical Speech Analysis and Detection",
   "papers": [
    "janbakhshi22_interspeech",
    "zhang22o_interspeech",
    "fan22b_interspeech",
    "ke22_interspeech",
    "liu22q_interspeech",
    "mallolragolta22_interspeech",
    "zhang22z_interspeech",
    "javanmardi22_interspeech",
    "chatzoudis22_interspeech",
    "zhu22d_interspeech",
    "fougeron22_interspeech"
   ]
  },
  {
   "title": "Adaptation, Transfer Learning, and Distillation for ASR",
   "papers": [
    "huang22b_interspeech",
    "lin22b_interspeech",
    "choi22d_interspeech",
    "sato22_interspeech",
    "thienpondt22_interspeech",
    "takashima22_interspeech"
   ]
  },
  {
   "title": "Speaker and Language Recognition I",
   "papers": [
    "choi22_interspeech",
    "jung22_interspeech",
    "liu22e_interspeech",
    "tzudir22_interspeech",
    "lee22o_interspeech"
   ]
  },
  {
   "title": "Pathological Speech Analysis",
   "papers": [
    "daoudi22_interspeech",
    "ardaillon22_interspeech",
    "talkar22_interspeech",
    "tran22c_interspeech",
    "brueggeman22_interspeech",
    "reddy22b_interspeech"
   ]
  },
  {
   "title": "Cross/Multi-lingual ASR",
   "papers": [
    "babu22_interspeech",
    "rugayan22_interspeech",
    "klejch22_interspeech",
    "kumar22c_interspeech",
    "morshed22_interspeech"
   ]
  },
  {
   "title": "Speaking Styles and Interaction Styles I",
   "papers": [
    "aguirre22_interspeech",
    "kawano22_interspeech",
    "shin22b_interspeech",
    "adigwe22_interspeech",
    "gao22f_interspeech"
   ]
  },
  {
   "title": "Speaking Styles and Interaction Styles II",
   "papers": [
    "mitsui22_interspeech",
    "afshan22_interspeech",
    "afshan22b_interspeech",
    "martikainen22_interspeech"
   ]
  },
  {
   "title": "Speech Synthesis: Tools, Data, and Evaluation",
   "papers": [
    "deja22_interspeech",
    "zhang22c_interspeech",
    "takamichi22_interspeech",
    "webber22_interspeech",
    "dekorte22_interspeech",
    "pandey22b_interspeech",
    "lemaguer22_interspeech",
    "meyer22c_interspeech",
    "maniati22_interspeech"
   ]
  },
  {
   "title": "Acoustic Signal Representation and Analysis II",
   "papers": [
    "kim22_interspeech",
    "rui22_interspeech",
    "rajan22_interspeech",
    "he22_interspeech",
    "bergler22_interspeech",
    "chang22d_interspeech",
    "bassan22_interspeech",
    "shirian22_interspeech",
    "singh22_interspeech",
    "baade22_interspeech"
   ]
  },
  {
   "title": "Speech and Language in Health: From Remote Monitoring to Medical Conversations II",
   "papers": [
    "bayerl22_interspeech",
    "frost22_interspeech",
    "berisha22_interspeech",
    "mirheidari22_interspeech",
    "mirheidari22b_interspeech",
    "dang22_interspeech",
    "bhattacharya22_interspeech",
    "braun22_interspeech",
    "pereztoro22_interspeech",
    "su22b_interspeech",
    "lamichhane22_interspeech",
    "nallanthighal22_interspeech"
   ]
  },
  {
   "title": "Dereverberation and Echo Cancellation",
   "papers": [
    "luo22c_interspeech",
    "cheng22b_interspeech",
    "han22c_interspeech",
    "zhang22t_interspeech",
    "zhang22v_interspeech",
    "kothapally22_interspeech",
    "helwani22_interspeech",
    "panchapagesan22_interspeech",
    "kothapally22b_interspeech"
   ]
  },
  {
   "title": "Voice Conversion and Adaptation III",
   "papers": [
    "xue22_interspeech",
    "yang22f_interspeech",
    "huang22c_interspeech",
    "lei22_interspeech",
    "wu22f_interspeech",
    "zhou22d_interspeech",
    "yang22p_interspeech",
    "nguyen22d_interspeech",
    "vanrijn22_interspeech",
    "yuan22b_interspeech",
    "lian22_interspeech",
    "du22c_interspeech"
   ]
  },
  {
   "title": "Novel Models and Training Methods for ASR III",
   "papers": [
    "meng22_interspeech",
    "du22_interspeech",
    "gong22_interspeech",
    "deng22_interspeech",
    "zhu22b_interspeech",
    "tian22_interspeech",
    "cui22b_interspeech",
    "wang22m_interspeech",
    "ren22_interspeech",
    "zhang22u_interspeech",
    "ao22_interspeech",
    "sawhney22_interspeech",
    "yang22w_interspeech",
    "moriya22_interspeech",
    "jain22_interspeech"
   ]
  },
  {
   "title": "Spoken Language Modeling and Understanding",
   "papers": [
    "sunder22_interspeech",
    "ohsugi22_interspeech",
    "dong22_interspeech",
    "avila22_interspeech",
    "gao22_interspeech",
    "wang22o_interspeech",
    "ou22_interspeech",
    "li22w_interspeech",
    "li22x_interspeech",
    "dinarelli22_interspeech",
    "kapelonis22_interspeech",
    "gao22e_interspeech",
    "ogushi22_interspeech",
    "wang22fa_interspeech"
   ]
  },
  {
   "title": "Acoustic Signal Representation and Analysis I",
   "papers": [
    "koutini22_interspeech",
    "sharma22_interspeech",
    "nam22_interspeech",
    "mostaani22_interspeech",
    "chen22p_interspeech",
    "yamamoto22_interspeech"
   ]
  },
  {
   "title": "Privacy and Security in Speech Communication",
   "papers": [
    "muller22_interspeech",
    "muller22b_interspeech",
    "pierre22_interspeech",
    "teixeira22_interspeech",
    "amid22_interspeech",
    "huang22k_interspeech"
   ]
  },
  {
   "title": "Multimodal Systems",
   "papers": [
    "taniguchi22_interspeech",
    "gabeur22_interspeech",
    "peng22c_interspeech",
    "rose22_interspeech",
    "serdyuk22_interspeech",
    "hong22_interspeech"
   ]
  },
  {
   "title": "Atypical Speech Detection",
   "papers": [
    "harvill22_interspeech",
    "priyasad22_interspeech",
    "ng22_interspeech",
    "woszczyk22_interspeech",
    "dutta22b_interspeech",
    "bayerl22b_interspeech"
   ]
  },
  {
   "title": "Spoofing-Aware Automatic Speaker Verification (SASV) I",
   "papers": [
    "choi22b_interspeech",
    "heo22_interspeech",
    "zeng22_interspeech",
    "alenin22_interspeech",
    "jung22c_interspeech",
    "lee22q_interspeech"
   ]
  },
  {
   "title": "Single-channel and multi-channel Speech Enhancement",
   "papers": [
    "westhausen22_interspeech",
    "tesch22_interspeech",
    "li22e_interspeech",
    "du22d_interspeech",
    "bartolewska22_interspeech",
    "welker22_interspeech",
    "ali22_interspeech",
    "turetzky22_interspeech",
    "fras22_interspeech",
    "deoliveira22_interspeech",
    "yang22x_interspeech"
   ]
  },
  {
   "title": "Voice Conversion and Adaptation II",
   "papers": [
    "bilinski22_interspeech",
    "sanchez22_interspeech",
    "udagawa22_interspeech",
    "proszewska22_interspeech",
    "lee22h_interspeech",
    "levkovitch22_interspeech",
    "lee22j_interspeech",
    "kakoulidis22_interspeech",
    "um22_interspeech",
    "sadekova22_interspeech",
    "kim22o_interspeech",
    "agarwal22_interspeech",
    "terashima22_interspeech"
   ]
  },
  {
   "title": "Resource-constrained ASR",
   "papers": [
    "yang22_interspeech",
    "baskar22_interspeech",
    "zhen22_interspeech",
    "kim22l_interspeech",
    "xie22_interspeech",
    "vyas22_interspeech",
    "vanderreydt22_interspeech",
    "chen22q_interspeech"
   ]
  },
  {
   "title": "Speech Production, Perception and Multimodality",
   "papers": [
    "kitamura22_interspeech",
    "tsukada22_interspeech",
    "obrien22_interspeech",
    "yang22i_interspeech",
    "arai22_interspeech",
    "chen22i_interspeech",
    "weise22b_interspeech",
    "li22t_interspeech",
    "eranovic22_interspeech",
    "zhang22aa_interspeech",
    "roy22_interspeech",
    "gibson22_interspeech",
    "antony22_interspeech",
    "kadandale22_interspeech"
   ]
  },
  {
   "title": "Multi-, Cross-lingual and Other Topics in ASR II",
   "papers": [
    "dalhouse22_interspeech",
    "fukuda22_interspeech",
    "bernhard22_interspeech",
    "pundak22_interspeech",
    "quan22_interspeech",
    "cui22_interspeech",
    "ye22_interspeech",
    "zhang22n_interspeech",
    "li22k_interspeech",
    "tian22c_interspeech",
    "pattanayak22_interspeech",
    "li22n_interspeech",
    "yang22m_interspeech",
    "schuppler22_interspeech",
    "zuo22b_interspeech",
    "sadhu22_interspeech",
    "singh22b_interspeech",
    "masumura22_interspeech",
    "zhang22da_interspeech"
   ]
  },
  {
   "title": "Spoken Language Processing III",
   "papers": [
    "zhao22d_interspeech",
    "wang22g_interspeech",
    "dinkel22_interspeech",
    "wang22n_interspeech",
    "conneau22_interspeech",
    "liu22r_interspeech",
    "cui22c_interspeech",
    "xue22d_interspeech",
    "zhou22h_interspeech",
    "wei22f_interspeech",
    "xu22h_interspeech",
    "martnek22_interspeech"
   ]
  },
  {
   "title": "Non-intrusive Objective Speech Quality Assessment (NISQA) Challenge for Online Conferencing Applications",
   "papers": [
    "liu22n_interspeech",
    "liu22o_interspeech",
    "shu22_interspeech",
    "hao22_interspeech",
    "yi22b_interspeech",
    "elhajal22_interspeech",
    "liu22v_interspeech",
    "chen22t_interspeech"
   ]
  },
  {
   "title": "Speech and Language in Health: From Remote Monitoring to Medical Conversations III",
   "papers": [
    "wang22l_interspeech",
    "wang22q_interspeech",
    "ravi22_interspeech",
    "rutowski22_interspeech",
    "ablimit22_interspeech",
    "seneviratne22_interspeech",
    "meripo22_interspeech"
   ]
  },
  {
   "title": "Speech Synthesis: Prosody Modeling",
   "papers": [
    "karlapati22_interspeech",
    "makarov22_interspeech",
    "nishimura22_interspeech",
    "seshadri22_interspeech",
    "stephenson22_interspeech",
    "omahony22_interspeech"
   ]
  },
  {
   "title": "Self-supervised, Semi-supervised, Adaptation and Data Augmentation for ASR",
   "papers": [
    "lu22_interspeech",
    "lee22i_interspeech",
    "baskar22b_interspeech",
    "wang22ba_interspeech",
    "qi22_interspeech",
    "arunkumar22_interspeech"
   ]
  },
  {
   "title": "Phonetics and Phonology",
   "papers": [
    "zellers22_interspeech",
    "noguchi22_interspeech",
    "ridouane22_interspeech",
    "hutin22_interspeech",
    "kuang22b_interspeech",
    "buech22b_interspeech"
   ]
  },
  {
   "title": "Spoken Language Understanding II",
   "papers": [
    "pelloin22_interspeech",
    "chang22c_interspeech",
    "kumar22_interspeech",
    "le22_interspeech",
    "yadav22b_interspeech",
    "arora22_interspeech"
   ]
  },
  {
   "title": "Speech Intelligibility Prediction for Hearing-Impaired Listeners I",
   "papers": [
    "close22_interspeech",
    "tu22_interspeech",
    "tu22b_interspeech",
    "robach22_interspeech",
    "cardinale22_interspeech",
    "barker22_interspeech"
   ]
  },
  {
   "title": "Low-Resource ASR Development I",
   "papers": [
    "baas22_interspeech",
    "zevallos22_interspeech",
    "fatehi22_interspeech",
    "bandarupalli22_interspeech",
    "berrebbi22_interspeech",
    "robinson22_interspeech",
    "rouhe22_interspeech",
    "bhanushali22_interspeech"
   ]
  },
  {
   "title": "Speech representation I",
   "papers": [
    "manocha22_interspeech",
    "choi22c_interspeech",
    "shrem22_interspeech",
    "eom22_interspeech",
    "batra22_interspeech",
    "chernyak22_interspeech",
    "wells22_interspeech",
    "lee22p_interspeech",
    "dumpala22_interspeech",
    "dissanayake22_interspeech",
    "peyser22_interspeech"
   ]
  },
  {
   "title": "Pathological Speech Assessment",
   "papers": [
    "quintas22_interspeech",
    "neijman22_interspeech",
    "getman22_interspeech",
    "triantafyllopoulos22_interspeech",
    "lee22k_interspeech",
    "vitormenezes22_interspeech",
    "abderrazek22_interspeech",
    "baumann22_interspeech",
    "benway22_interspeech",
    "cao22b_interspeech",
    "kothare22_interspeech"
   ]
  },
  {
   "title": "Show and Tell III",
   "papers": [
    "arco22_interspeech",
    "xu22j_interspeech",
    "siddarth22_interspeech"
   ]
  },
  {
   "title": "Speaker and Language Recognition II",
   "papers": [
    "lesnichaia22_interspeech",
    "kang22c_interspeech",
    "liang22b_interspeech",
    "lyu22_interspeech",
    "hu22b_interspeech",
    "li22m_interspeech",
    "chen22g_interspeech",
    "zhao22j_interspeech",
    "li22s_interspeech",
    "gao22c_interspeech",
    "shao22b_interspeech",
    "shen22b_interspeech",
    "wang22ga_interspeech"
   ]
  },
  {
   "title": "Speech Segmentation II",
   "papers": [
    "kang22b_interspeech",
    "ghosh22_interspeech",
    "ding22_interspeech",
    "fan22_interspeech",
    "rho22_interspeech",
    "larsen22_interspeech",
    "xiao22_interspeech",
    "zhu22e_interspeech"
   ]
  },
  {
   "title": "Robust ASR, and Far-field/Multi-talker ASR",
   "papers": [
    "kanda22_interspeech",
    "pesoparada22_interspeech",
    "novitasari22_interspeech",
    "takeda22_interspeech",
    "zhuang22_interspeech",
    "guo22b_interspeech",
    "wei22c_interspeech",
    "gao22d_interspeech",
    "wang22x_interspeech",
    "chang22g_interspeech",
    "bando22_interspeech",
    "omalley22_interspeech",
    "rajchetupalli22_interspeech",
    "das22_interspeech",
    "zheng22f_interspeech"
   ]
  },
  {
   "title": "ASR: Linguistic Components ",
   "papers": [
    "farooq22_interspeech",
    "shen22_interspeech",
    "wang22e_interspeech",
    "nie22_interspeech",
    "li22g_interspeech",
    "yang22g_interspeech",
    "pham22_interspeech",
    "thithuuyen22_interspeech",
    "futami22_interspeech",
    "zhang22x_interspeech",
    "tsunoo22_interspeech",
    "zheng22c_interspeech",
    "yang22s_interspeech",
    "fox22_interspeech",
    "udagawa22b_interspeech",
    "song22e_interspeech"
   ]
  },
  {
   "title": "Speech Intelligibility Prediction for Hearing-Impaired Listeners II",
   "papers": [
    "irino22_interspeech",
    "huckvale22_interspeech",
    "yang22u_interspeech",
    "edozezario22_interspeech"
   ]
  },
  {
   "title": "Show and Tell III(VR)",
   "papers": [
    "tan22c_interspeech",
    "jin22b_interspeech"
   ]
  },
  {
   "title": "Summarization, Entity Extraction, Evaluation and Others",
   "papers": [
    "strom22_interspeech",
    "hollands22_interspeech",
    "singla22_interspeech",
    "roux22_interspeech",
    "faria22_interspeech",
    "kim22p_interspeech"
   ]
  },
  {
   "title": "Automatic Analysis of Paralinguistics",
   "papers": [
    "yoon22_interspeech",
    "saeki22d_interspeech",
    "gent22_interspeech",
    "ghosh22c_interspeech",
    "chang22h_interspeech",
    "rennie22_interspeech"
   ]
  },
  {
   "title": "Self Supervision and Anti-Spoofing",
   "papers": [
    "dissen22_interspeech",
    "lepage22_interspeech",
    "kawa22_interspeech",
    "cho22c_interspeech",
    "mohammadamini22_interspeech"
   ]
  },
  {
   "title": "Speech Articulation & Neural Processing",
   "papers": [
    "puffay22_interspeech",
    "defino22_interspeech",
    "rana22_interspeech",
    "li22z_interspeech",
    "cordero22_interspeech",
    "meng22d_interspeech"
   ]
  },
  {
   "title": "Low Resource Spoken Language Understanding",
   "papers": [
    "mdhaffar22_interspeech",
    "meeus22_interspeech"
   ]
  },
  {
   "title": "Non-intrusive Objective Speech Quality Assessment (NISQA) Challenge for Online Conferencing Applications",
   "papers": [
    "jayesh22_interspeech",
    "tamm22_interspeech",
    "becerra22_interspeech"
   ]
  },
  {
   "title": "Novel Models and Training Methods for ASR I",
   "papers": [
    "chen22r_interspeech",
    "yang22b_interspeech",
    "ristea22_interspeech",
    "horii22_interspeech",
    "olivier22_interspeech",
    "shim22_interspeech",
    "kim22f_interspeech",
    "song22c_interspeech",
    "meyer22_interspeech",
    "bittar22_interspeech"
   ]
  },
  {
   "title": "Acoustic scene analysis",
   "papers": [
    "mei22_interspeech",
    "hou22_interspeech",
    "huang22d_interspeech",
    "lei22b_interspeech",
    "reddy22_interspeech",
    "chen22h_interspeech",
    "li22p_interspeech",
    "wang22v_interspeech",
    "sonowal22_interspeech",
    "hu22f_interspeech",
    "parikh22_interspeech",
    "takeuchi22_interspeech"
   ]
  },
  {
   "title": "Speech Coding and Privacy",
   "papers": [
    "pan22_interspeech",
    "liu22_interspeech",
    "pia22_interspeech",
    "xue22b_interspeech",
    "jiang22_interspeech",
    "liu22x_interspeech",
    "liu22y_interspeech",
    "stoidis22_interspeech"
   ]
  },
  {
   "title": "Speech Synthesis: Singing, Multimodal, Crosslingual Synthesis",
   "papers": [
    "wang22b_interspeech",
    "zhan22_interspeech",
    "zhang22e_interspeech",
    "peng22_interspeech",
    "zhuang22b_interspeech",
    "xue22c_interspeech",
    "guo22e_interspeech",
    "shi22d_interspeech",
    "liu22p_interspeech",
    "wang22u_interspeech",
    "zhou22f_interspeech",
    "manghat22_interspeech",
    "chung22_interspeech",
    "li22da_interspeech"
   ]
  },
  {
   "title": "Applications in Transcription, Education and Learning II",
   "papers": [
    "yang22c_interspeech",
    "zhang22_interspeech",
    "dutta22_interspeech",
    "zhang22p_interspeech",
    "zhang22q_interspeech",
    "fu22b_interspeech",
    "chen22l_interspeech",
    "nidadavolu22_interspeech",
    "zheng22e_interspeech"
   ]
  },
  {
   "title": "Spoofing-Aware Automatic Speaker Verification (SASV) II",
   "papers": [
    "wu22g_interspeech",
    "kang22_interspeech",
    "lin22_interspeech",
    "zhang22f_interspeech",
    "zhang22s_interspeech",
    "zhang22w_interspeech",
    "tan22b_interspeech",
    "teng22_interspeech",
    "wang22ea_interspeech"
   ]
  },
  {
   "title": "Speech Coding and Restoration",
   "papers": [
    "han22_interspeech",
    "saeki22b_interspeech",
    "byun22_interspeech",
    "kim22m_interspeech",
    "siahkoohi22_interspeech",
    "miao22_interspeech"
   ]
  },
  {
   "title": "Streaming ASR",
   "papers": [
    "radfar22_interspeech",
    "zhao22f_interspeech",
    "lee22g_interspeech",
    "do22_interspeech",
    "sklyar22_interspeech"
   ]
  },
  {
   "title": "Applications in Transcription, Education and Learning I",
   "papers": [
    "wong22_interspeech",
    "kunihara22b_interspeech",
    "suzuki22b_interspeech",
    "banno22_interspeech",
    "bai22b_interspeech",
    "yang22v_interspeech"
   ]
  },
  {
   "title": "Spoken Dialogue Systems",
   "papers": [
    "sakuma22_interspeech",
    "jabeen22_interspeech",
    "girish22_interspeech",
    "kumar22b_interspeech",
    "wallbridge22_interspeech",
    "hori22_interspeech"
   ]
  },
  {
   "title": "The VoiceMOS Challenge",
   "papers": [
    "stan22_interspeech",
    "saeki22c_interspeech",
    "nguyen22b_interspeech",
    "chinen22_interspeech",
    "huang22f_interspeech",
    "tseng22b_interspeech"
   ]
  },
  {
   "title": "Speech Synthesis: Speaking Style, Emotion and Accents I",
   "papers": [
    "ammarabbas22_interspeech",
    "nakata22_interspeech",
    "kim22g_interspeech",
    "kim22j_interspeech",
    "huang22g_interspeech",
    "finkelstein22_interspeech",
    "yoshioka22_interspeech",
    "kulkarni22_interspeech",
    "rattcliffe22_interspeech",
    "zaidi22b_interspeech",
    "yoon22b_interspeech",
    "mukherjee22_interspeech"
   ]
  },
  {
   "title": "Speech Segmentation I",
   "papers": [
    "mateju22_interspeech",
    "segal22_interspeech",
    "meneses22_interspeech",
    "kim22h_interspeech",
    "sarkar22_interspeech",
    "sharon22_interspeech",
    "mariotte22_interspeech",
    "sudo22_interspeech",
    "fuchs22_interspeech"
   ]
  },
  {
   "title": "Human Speech & Signal Processing",
   "papers": [
    "zhang22d_interspeech",
    "sun22b_interspeech",
    "sun22c_interspeech",
    "zhao22i_interspeech",
    "nabe22_interspeech",
    "liebig22_interspeech",
    "parikh22b_interspeech",
    "lian22b_interspeech",
    "zhang22ea_interspeech"
   ]
  },
  {
   "title": "Speech Emotion Recognition II",
   "papers": [
    "li22_interspeech",
    "santoso22_interspeech",
    "hu22c_interspeech",
    "baruah22_interspeech",
    "mitra22_interspeech",
    "hu22e_interspeech",
    "zhao22k_interspeech",
    "chen22m_interspeech",
    "velichko22_interspeech",
    "takashima22b_interspeech",
    "kang22d_interspeech",
    "liu22aa_interspeech",
    "audibert22_interspeech"
   ]
  },
  {
   "title": "Speaker Recognition and Anti-Spoofing",
   "papers": [
    "vaessen22_interspeech",
    "lou22_interspeech",
    "zhao22e_interspeech",
    "wang22h_interspeech",
    "han22b_interspeech",
    "shi22c_interspeech",
    "li22r_interspeech",
    "wen22_interspeech",
    "jin22_interspeech",
    "chen22s_interspeech"
   ]
  },
  {
   "title": "Miscellaneous Topics in Speech, Voice and Hearing Disorders",
   "papers": [
    "zuo22_interspeech",
    "weise22_interspeech",
    "song22_interspeech",
    "wang22k_interspeech",
    "fan22c_interspeech",
    "aminidigehsara22_interspeech",
    "cheng22c_interspeech"
   ]
  },
  {
   "title": "Low-Resource ASR Development II",
   "papers": [
    "huo22_interspeech",
    "farooq22b_interspeech",
    "zhao22c_interspeech",
    "zhong22_interspeech",
    "lonergan22_interspeech",
    "zhu22c_interspeech",
    "schraner22_interspeech",
    "patel22_interspeech",
    "li22aa_interspeech",
    "damania22_interspeech",
    "peterson22_interspeech",
    "fan22d_interspeech",
    "guillaume22_interspeech"
   ]
  },
  {
   "title": "Voice Conversion and Adaptation I",
   "papers": [
    "choi22e_interspeech",
    "yang22t_interspeech",
    "quamer22_interspeech",
    "meyer22b_interspeech",
    "kuhlmann22_interspeech",
    "klapsas22_interspeech"
   ]
  },
  {
   "title": "Search/Decoding Algorithms for ASR",
   "papers": [
    "hu22_interspeech",
    "kim22e_interspeech",
    "sriram22_interspeech",
    "kocour22_interspeech",
    "novak22_interspeech",
    "komatsu22_interspeech"
   ]
  },
  {
   "title": "Emotional Speech Production and Perception",
   "papers": [
    "gessinger22_interspeech",
    "kasun22_interspeech",
    "suzuki22_interspeech",
    "salais22_interspeech",
    "kirkland22_interspeech"
   ]
  },
  {
   "title": "Speech Analysis",
   "papers": [
    "huang22_interspeech",
    "yeh22_interspeech",
    "chang22e_interspeech",
    "lebourdais22_interspeech",
    "teytaut22_interspeech",
    "siriwardena22_interspeech"
   ]
  },
  {
   "title": "Trustworthy Speech Processing",
   "papers": [
    "maouche22_interspeech",
    "huang22h_interspeech",
    "joshi22_interspeech",
    "tseng22_interspeech",
    "shao22c_interspeech",
    "feng22_interspeech",
    "feng22b_interspeech",
    "joshi22b_interspeech"
   ]
  },
  {
   "title": "Speaker Recognition and Diarization",
   "papers": [
    "yoo22_interspeech",
    "salim22_interspeech",
    "flemotomos22_interspeech",
    "park22d_interspeech",
    "chaubey22_interspeech",
    "rybicka22_interspeech",
    "landini22_interspeech",
    "ide22_interspeech",
    "kim22n_interspeech",
    "peng22e_interspeech"
   ]
  },
  {
   "title": "Self-supervised, Semi-supervised, Adaptation and Data Augmentation for ASR II",
   "papers": [
    "sapru22_interspeech",
    "mai22_interspeech",
    "biadsy22_interspeech",
    "dieck22_interspeech",
    "zheng22d_interspeech",
    "nakagome22_interspeech",
    "arunkumar22b_interspeech"
   ]
  },
  {
   "title": "Spoken Language Processing I",
   "papers": [
    "liu22b_interspeech",
    "saito22_interspeech",
    "rumberg22_interspeech",
    "lin22c_interspeech",
    "jung22b_interspeech",
    "chang22f_interspeech",
    "nguyen22c_interspeech",
    "ghosh22b_interspeech",
    "ekstedt22_interspeech",
    "popuri22_interspeech",
    "huang22l_interspeech",
    "gharbieh22_interspeech",
    "belitz22_interspeech"
   ]
  },
  {
   "title": "Show and Tell IV",
   "papers": [
    "nicmanis22_interspeech",
    "draxler22_interspeech",
    "vandevreken22_interspeech",
    "paul22_interspeech"
   ]
  },
  {
   "title": "Phonetics II",
   "papers": [
    "wang22_interspeech",
    "luo22b_interspeech",
    "kaland22_interspeech",
    "hughes22_interspeech",
    "wang22d_interspeech",
    "lee22f_interspeech",
    "kadiri22_interspeech",
    "furukawa22_interspeech",
    "li22u_interspeech",
    "young22_interspeech",
    "tian22e_interspeech",
    "le22b_interspeech",
    "zygis22_interspeech",
    "wang22ca_interspeech",
    "carne22_interspeech",
    "begus22_interspeech",
    "jun22_interspeech",
    "zeng22b_interspeech"
   ]
  },
  {
   "title": "Source Separation III",
   "papers": [
    "luo22_interspeech",
    "zhao22_interspeech",
    "lutati22_interspeech",
    "li22d_interspeech",
    "zhao22b_interspeech",
    "yang22d_interspeech",
    "wang22c_interspeech",
    "ivry22_interspeech",
    "rixen22_interspeech",
    "lu22b_interspeech",
    "kopuklu22_interspeech",
    "liu22l_interspeech",
    "wang22p_interspeech",
    "quan22b_interspeech",
    "zhang22y_interspeech",
    "paturi22_interspeech",
    "chetupalli22_interspeech",
    "corey22_interspeech",
    "kilgour22_interspeech",
    "parikh22c_interspeech"
   ]
  },
  {
   "title": "Speech Enhancement and Intelligibility",
   "papers": [
    "li22c_interspeech",
    "iwamoto22_interspeech",
    "zhou22_interspeech",
    "nossier22_interspeech",
    "meng22b_interspeech",
    "tian22d_interspeech",
    "yang22o_interspeech",
    "chao22_interspeech",
    "geng22_interspeech",
    "lu22c_interspeech",
    "zezario22_interspeech",
    "bu22_interspeech",
    "saba22_interspeech",
    "jigang22_interspeech",
    "zhao22l_interspeech"
   ]
  },
  {
   "title": "Speech Synthesis: Speaking Style, Emotion and Accents II",
   "papers": [
    "fernandez22_interspeech",
    "liu22i_interspeech",
    "li22h_interspeech",
    "wu22e_interspeech",
    "liu22m_interspeech",
    "dai22_interspeech",
    "zhou22e_interspeech",
    "lei22c_interspeech",
    "li22ca_interspeech",
    "meng22c_interspeech",
    "he22d_interspeech"
   ]
  },
  {
   "title": "Show & Tell IV(VR)",
   "papers": [
    "nguyen22e_interspeech"
   ]
  }
 ],
 "doi": "10.21437/Interspeech.2022"
}