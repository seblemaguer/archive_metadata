{
 "title": "Interspeech 2005",
 "location": "Lisbon, Portugal",
 "startDate": "4/9/2005",
 "endDate": "8/9/2005",
 "chair": "General Chair: Isabel Trancoso",
 "conf": "Interspeech",
 "year": "2005",
 "name": "interspeech_2005",
 "series": "Interspeech",
 "SIG": "",
 "title1": "Interspeech 2005",
 "date": "4-8 September 2005",
 "booklet": "interspeech_2005.pdf",
 "papers": {
  "clark05_interspeech": {
   "authors": [
    [
     "Graeme M.",
     "Clark"
    ]
   ],
   "title": "The multiple-channel cochlear implant: interfacing electronic technology to human consciousness",
   "original": "i05_0001",
   "page_count": 4,
   "order": 1,
   "p1": "1",
   "pn": "4",
   "abstract": [
    "Fundamental research on electrical stimulation of the auditory pathways resulted in the Multiple Channel Cochlear Implant, a device which provides understanding of speech to severely-toprofoundly deaf people. The device, a miniaturized receiverstimulator with multiple electrodes fed with power and speech data through two separate aerials was first implanted in a patient in 1978 as a prototype, and since 1982, was commercially produced by Cochlear Limited, Australia. Speech processing is based on the discovery that the sensation at each electrode is \"vowel-like\". Initially, the second formant was coded as a place of stimulation, the sound pressure was coded as a current level, and the voicing frequency as a pulse rate. Further research showed that there were progressively better open-set word and sentence scores for the extraction of the first formant in addition to the second formant (the F0/F1/F2 processor), the addition of high fixed filter outputs (MULTIPEAK) and then finally 6 to 8 maximal filter outputs at low rates (SPEAK) and high rates (ACE). All the frequencies were coded on a place basis. World trials completed for the US FDA on late-deafened adults in 1985 and in 1990 on children from two years to 17 years proved that a 22-channel cochlear implant was safe and effective in enabling them to understand speech both with and without lip-reading.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-1"
  },
  "pereira05_interspeech": {
   "authors": [
    [
     "Fernando C. N.",
     "Pereira"
    ]
   ],
   "title": "Linear models for structure prediction",
   "original": "i05_0717",
   "page_count": 4,
   "order": 2,
   "p1": "717",
   "pn": "720",
   "abstract": [
    "Over the last few years, several groups have been developing models and algorithms for learning to predict the structure of complex data, sequences in particular, that extend well-known linear classification models and algorithms, such as logistic regression, the perceptron algorithm, and support vector machines. These methods combine the advantages of discriminative learning with those of probabilistic generative models like HMMs and probabilistic context-free grammars. I will introduce linear models for structure prediction and their simplest learning algorithms, and exemplify their benefits with applications to text and speech processing, including information extraction, parsing, and language modeling.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-2"
  },
  "shriberg05_interspeech": {
   "authors": [
    [
     "Elizabeth",
     "Shriberg"
    ]
   ],
   "title": "Spontaneous speech: how people really talk and why engineers should care",
   "original": "i05_1781",
   "page_count": 4,
   "order": 3,
   "p1": "1781",
   "pn": "1784",
   "abstract": [
    "Spontaneous conversation is optimized for human-human communication, but differs in some important ways from the types of speech for which human language technology is often developed. This overview describes four fundamental properties of spontaneous speech that present challenges for spoken language applications because they violate assumptions often applied in automatic processing technology.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-3"
  },
  "tam05_interspeech": {
   "authors": [
    [
     "Yik-Cheung",
     "Tam"
    ],
    [
     "Tanja",
     "Schultz"
    ]
   ],
   "title": "Dynamic language model adaptation using variational Bayes inference",
   "original": "i05_0005",
   "page_count": 4,
   "order": 4,
   "p1": "5",
   "pn": "8",
   "abstract": [
    "We propose an unsupervised dynamic language model (LM) adaptation framework using long-distance latent topic mixtures. The framework employs the Latent Dirichlet Allocation model (LDA) which models the latent topics of a document collection in an unsupervised and Bayesian fashion. In the LDA model, each word is modeled as a mixture of latent topics. Varying topics within a context can be modeled by re-sampling the mixture weights of the latent topics from a prior Dirichlet distribution. The model can be trained using the variational Bayes Expectation Maximization algorithm. During decoding, mixture weights of the latent topics are adapted dynamically using the hypotheses of previously decoded utterances. In our work, the LDA model is combined with the trigram language model using linear interpolation. We evaluated the approach on the CCTV episode of the RT04 Mandarin Broadcast News test set. Results show that the proposed approach reduces the perplexity by up to 15.4% relative and the character error rate by 4.9% relative depending on the size and setup of the training set.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-4"
  },
  "seneviratne05_interspeech": {
   "authors": [
    [
     "Vidura",
     "Seneviratne"
    ],
    [
     "Steve",
     "Young"
    ]
   ],
   "title": "The hidden vector state language model",
   "original": "i05_0009",
   "page_count": 4,
   "order": 5,
   "p1": "9",
   "pn": "12",
   "abstract": [
    "The Hidden Vector State (HVS) model extends the basic Hidden Markov Model (HMM) by encoding each state as a vector of stack states but with restricted stack operations. The model uses a right branching stack automaton to assign valid stochastic parses to a word sequence from which the language model probability can be estimated. The model is completely data driven and is able to model classes from the data that reflect the hierarchical structures found in natural language. This paper describes the design and the implementation of the HVS language model [1], focusing on the practical issues of initialisation and training using Baum-Welch re-estimation whilst accommodating a large and dynamic state space. Results of experiments conducted using the ATIS corpus [2] show that the HVS language model reduces test set perplexity compared to standard class based language models.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-5"
  },
  "mori05_interspeech": {
   "authors": [
    [
     "Shinsuke",
     "Mori"
    ],
    [
     "Gakuto",
     "Kurata"
    ]
   ],
   "title": "Class-based variable memory length Markov model",
   "original": "i05_0013",
   "page_count": 4,
   "order": 6,
   "p1": "13",
   "pn": "16",
   "abstract": [
    "In this paper, we present a class-based variable memory length Markov model and its learning algorithm. This is an extension of a variable memory length Markov model. Our model is based on a class-based probabilistic suffix tree, whose nodes have an automatically acquired word-class relation. We experimentally compared our new model with a word-based bi-gram model, a word-based tri-gram model, a class-based bi-gram model, and a word-based variable memory length Markov model. The results show that a class-based variable memory length Markov model outperforms the other models in perplexity and model size.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-6"
  },
  "gruenstein05_interspeech": {
   "authors": [
    [
     "Alexander",
     "Gruenstein"
    ],
    [
     "Chao",
     "Wang"
    ],
    [
     "Stephanie",
     "Seneff"
    ]
   ],
   "title": "Context-sensitive statistical language modeling",
   "original": "i05_0017",
   "page_count": 4,
   "order": 7,
   "p1": "17",
   "pn": "20",
   "abstract": [
    "We present context-sensitive dynamic classes - a novel mechanism for integrating contextual information from spoken dialogue into a class n-gram language model. We exploit the dialogue system's information state to populate dynamic classes, thus percolating contextual constraints to the recognizer's language model in real time. We describe a technique for training a language model incorporating context-sensitive dynamic classes which considerably reduces word error rate under several conditions. Significantly, our technique does not partition the language model based on potentially artificial dialogue state distinctions; rather, it accommodates both strong and weak expectations via dynamic manipulation of a single model.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-7"
  },
  "wang05_interspeech": {
   "authors": [
    [
     "Chao",
     "Wang"
    ],
    [
     "Stephanie",
     "Seneff"
    ],
    [
     "Grace",
     "Chung"
    ]
   ],
   "title": "Language model data filtering via user simulation and dialogue resynthesis",
   "original": "i05_0021",
   "page_count": 4,
   "order": 8,
   "p1": "21",
   "pn": "24",
   "abstract": [
    "In this paper, we address the issue of generating language model training data during the initial stages of dialogue system development. The process begins with a large set of sentence templates, automatically adapted from other application domains. We propose two methods to filter the raw data set to achieve a desired probability distribution of the semantic content, both on the sentence level and on the class level. The first method utilizes user simulation technology, which obtains the probability model via an interplay between a probabilistic user model and the dialogue system. The second method synthesizes novel dialogue interactions by modeling after a small set of dialogues produced by the developers during the course of system refinement. We evaluated our methodology by speech recognition performance on a set of 520 unseen utterances from naive users interacting with a restaurant domain dialogue system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-8"
  },
  "chien05_interspeech": {
   "authors": [
    [
     "Jen-Tzung",
     "Chien"
    ],
    [
     "Meng-Sung",
     "Wu"
    ],
    [
     "Chia-Sheng",
     "Wu"
    ]
   ],
   "title": "Bayesian learning for latent semantic analysis",
   "original": "i05_0025",
   "page_count": 4,
   "order": 9,
   "p1": "25",
   "pn": "28",
   "abstract": [
    "Probabilistic latent semantic analysis (PLSA) is a popular approach to text modeling where the semantics and statistics in documents can be effectively captured. In this paper, a novel Bayesian PLSA framework is presented. We focus on exploiting the incremental learning algorithm for solving the updating problem of new domain articles. This algorithm is developed to improve text modeling by incrementally extracting the up-to-date latent semantic information to match the changing domains at run time. The expectation-maximization (EM) algorithm is applied to resolve the quasi- Bayes (QB) estimate of PLSA parameters. The online PLSA is constructed to accomplish parameter estimation as well as hyperparameter updating. Compared to standard PLSA using maximum likelihood estimate, the proposed QB approach is capable of performing dynamic document indexing and classification. Also, we present the maximum a posteriori PLSA for corrective training. Experiments on evaluating model perplexities and classification accuracies demonstrate the superiority of using Bayesian PLSA.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-9"
  },
  "chueh05_interspeech": {
   "authors": [
    [
     "Chuang-Hua",
     "Chueh"
    ],
    [
     "To-Chang",
     "Chien"
    ],
    [
     "Jen-Tzung",
     "Chien"
    ]
   ],
   "title": "Discriminative maximum entropy language model for speech recognition",
   "original": "i05_0721",
   "page_count": 4,
   "order": 10,
   "p1": "721",
   "pn": "724",
   "abstract": [
    "This paper presents a new discriminative language model based on the whole-sentence maximum entropy (ME) framework. In the proposed discriminative ME (DME) model, we exploit an integrated linguistic and acoustic model, which properly incorporates the features from n-gram model and acoustic log likelihoods of target and competing models. Through the constrained optimization of integrated model, we estimate DME language model for speech recognition. Attractively, we illustrate the relation between DME estimation and the maximum mutual information (MMI) estimation for language modeling. It is interesting to find that using the sentence-level log likelihood ratios of competing and target sentences as the acoustic features for ME language modeling is equivalent to performing MMI discriminative language modeling. In the experiments on speech recognition, we show that DME model achieved lower word error rate compared to conventional ME model.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-10"
  },
  "bisani05_interspeech": {
   "authors": [
    [
     "Maximilian",
     "Bisani"
    ],
    [
     "Hermann",
     "Ney"
    ]
   ],
   "title": "Open vocabulary speech recognition with flat hybrid models",
   "original": "i05_0725",
   "page_count": 4,
   "order": 11,
   "p1": "725",
   "pn": "728",
   "abstract": [
    "Today's speech recognition systems are able to recognize arbitrary sentences over a large but finite vocabulary. However, many important speech recognition tasks feature an open, constantly changing vocabulary. (E.g. broadcast news transcription, translation of political debates, etc. Ideally, a system designed for such open vocabulary tasks would be able to recognize arbitrary, even previously unseen words. To some extent this can be achieved by using sub-lexical language models. We demonstrate that, by using a simple flat hybrid model, we can significantly improve a well-optimized state-of-the-art speech recognition system over a wide range of out-of-vocabulary rates.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-11"
  },
  "jeong05_interspeech": {
   "authors": [
    [
     "Minwoo",
     "Jeong"
    ],
    [
     "Jihyun",
     "Eun"
    ],
    [
     "Sangkeun",
     "Jung"
    ],
    [
     "Gary Geunbae",
     "Lee"
    ]
   ],
   "title": "An error-corrective language-model adaptation for automatic speech recognition",
   "original": "i05_0729",
   "page_count": 4,
   "order": 12,
   "p1": "729",
   "pn": "732",
   "abstract": [
    "We present a new language model adaptation framework integrated with error handling method to improve accuracy of speech recognition and performance of spoken language applications. The proposed error corrective language model adaptation approach exploits domain-specific language variations and recognition environment characteristics to provide robustness and adaptability for a spoken language system. We demonstrate some experiments of spoken dialogue tasks and empirical results which show an improvement of the accuracy for both speech recognition and spoken language understanding.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-12"
  },
  "lin05_interspeech": {
   "authors": [
    [
     "Shiuan-Sung",
     "Lin"
    ],
    [
     "François",
     "Yvon"
    ]
   ],
   "title": "Discriminative training of finite state decoding graphs",
   "original": "i05_0733",
   "page_count": 4,
   "order": 13,
   "p1": "733",
   "pn": "736",
   "abstract": [
    "Automatic Speech Recognition systems integrate three main knowledge sources: acoustic models, pronunciation dictionary and language models. In contrast to common practices, where each source is optimized independently, then combined in a finite-state search space, we investigate here a training procedure which attempts to adjust (some of) the parameters after, rather than before, combination. To this end, we adapted a discriminative training procedure originally devised for language models to the more general case of arbitrary finite-state graphs. Preliminary experiments performed on a simple name recognition task demonstrate the potential of this approach and suggest possible improvements.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-13"
  },
  "schwenk05_interspeech": {
   "authors": [
    [
     "Holger",
     "Schwenk"
    ],
    [
     "Jean-Luc",
     "Gauvain"
    ]
   ],
   "title": "Building continuous space language models for transcribing european languages",
   "original": "i05_0737",
   "page_count": 4,
   "order": 14,
   "p1": "737",
   "pn": "740",
   "abstract": [
    "Large vocabulary continuous speech recognizers for English Broadcast News achieve today word error rates below 10%. An important factor for this success is the availability of large amounts of acoustic and language modeling training data. In this paper the recognition of French Broadcast News and English and Spanish parliament speeches is addressed, tasks for which less resources are available. A neural network language model is applied that takes better advantage of the limited amount of training data. This approach performs the estimation of the probabilities in a continuous space, allowing by this means smooth interpolations. Word error reduction of up to 0.9% absolute are reported with respect to a carefully tuned backoff language model trained on the same data.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-14"
  },
  "xu05_interspeech": {
   "authors": [
    [
     "Peng",
     "Xu"
    ],
    [
     "Lidia",
     "Mangu"
    ]
   ],
   "title": "Using random forest language models in the IBM RT-04 CTS system",
   "original": "i05_0741",
   "page_count": 4,
   "order": 15,
   "p1": "741",
   "pn": "744",
   "abstract": [
    "One of the challenges in large vocabulary speech recognition is the availability of large amounts of data for training language models. In most state-of-the-art speech recognition systems, n-gram models with Kneser-Ney smoothing still prevail due to their simplicity and effectiveness. In this paper, we study the performance of a new language model, the random forest language model, in the IBM conversational telephony speech recognition system. We show that although the random forest language models are designed to deal with the data sparseness problem, they also achieve statistically significant improvements over n-gram models when the training data has over 500 million words.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-15"
  },
  "kuo05_interspeech": {
   "authors": [
    [
     "Jen-Wei",
     "Kuo"
    ],
    [
     "Berlin",
     "Chen"
    ]
   ],
   "title": "Minimum word error based discriminative training of language models",
   "original": "i05_1277",
   "page_count": 4,
   "order": 16,
   "p1": "1277",
   "pn": "1280",
   "abstract": [
    "This paper considers discriminative training of language models for large vocabulary continuous speech recognition. The minimum word error (MWE) criterion was explored to make use of the word confusion information as well as the local lexical constraints inherent in the acoustic training corpus, in conjunction with those constraints obtained from the background text corpus, for properly guiding the speech recognizer to separate the correct hypothesis from the competing ones. The underlying characteristics of the MWE-based approach were extensively investigated, and its performance was verified by comparison with the conventional maximum likelihood (ML) approaches as well. The speech recognition experiments were performed on the broadcast news collected in Taiwan.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-16"
  },
  "ghaoui05_interspeech": {
   "authors": [
    [
     "A.",
     "Ghaoui"
    ],
    [
     "François",
     "Yvon"
    ],
    [
     "C.",
     "Mokbel"
    ],
    [
     "Gérard",
     "Chollet"
    ]
   ],
   "title": "On the use of morphological constraints in n-gram statistical language model",
   "original": "i05_1281",
   "page_count": 4,
   "order": 17,
   "p1": "1281",
   "pn": "1284",
   "abstract": [
    "State of the art Speech Recognition systems use statistical language modeling and in particular N-gram models to represent the language structure. The Arabic language has a rich morphology, which motivates the introduction of morphological constraints in the language model. Class-based N-gram models have shown satisfactory results, especially for language model adaptation and training from reduced datasets. They were also proven quite effective in their use of memory space. In this paper, we investigate a new morphological class-based language model. Morphological rules are used to derive the different words in a class from their stem. As morphological analyzer, a rule-based stemming method is proposed for the Arabic language. The language model has been evaluated on a database composed of articles from Lebanese newspaper Al-Nahar for the years 1998 and 1999. In addition, a linear interpolation between the N-gram model and the morphological model is also evaluated. Preliminary experiments detailed in this paper show satisfactory results.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-17"
  },
  "siciliagarcia05_interspeech": {
   "authors": [
    [
     "Elvira I.",
     "Sicilia-Garcia"
    ],
    [
     "Ji",
     "Ming"
    ],
    [
     "F. Jack",
     "Smith"
    ]
   ],
   "title": "A posteriori multiple word-domain language model",
   "original": "i05_1285",
   "page_count": 4,
   "order": 18,
   "p1": "1285",
   "pn": "1288",
   "abstract": [
    "It is shown that the enormous improvement in the size of disk storage space in recent years can be used to build multiple worddomain statistical language models, one for each significant word of a language. Each of these word-domain language models is a precise domain model for the relevant significant word and when combined appropriately they provide a highly specific domain language model for the language following a cache, even a short cache. A Multiple Word- Domain model based on 20,000 individual word language models has been constructed and tested on a Wall Street Journal Corpus. Improvements in perplexity, between 25% and 68%, over a base-line tri-gram model have been obtained in tests.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-18"
  },
  "diegueztirado05_interspeech": {
   "authors": [
    [
     "Javier",
     "Dieguez-Tirado"
    ],
    [
     "Carmen García",
     "Mateo"
    ],
    [
     "Antonio",
     "Cardenal-Lopez"
    ]
   ],
   "title": "Effective topic-tree based language model adaptation",
   "original": "i05_1289",
   "page_count": 4,
   "order": 19,
   "p1": "1289",
   "pn": "1292",
   "abstract": [
    "We work on adaptation schemes for language modeling well suited for limited resources scenarios. In order to take advantage of available out-of-domain corpora, language model adaptation using topic mixtures was investigated. This technique has not given good practical results in the past. In this paper, we have performed several modifications to an existing tree-based approach. The tree was obtained from the background corpus by means of partitional clustering. All the nodes were exploited in the adapted model, and non-erroneous in-domain transcriptions were used as the adaptation corpus. The modified technique yielded a 14% perplexity improvement in a bilingual BN task, outperforming several nonhierarchical approaches. A strategy for an early application of the language model allowed to translate this perplexity improvement into a 4% WER reduction.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-19"
  },
  "sethy05_interspeech": {
   "authors": [
    [
     "Abhinav",
     "Sethy"
    ],
    [
     "Panayiotis G.",
     "Georgiou"
    ],
    [
     "Shrikanth",
     "Narayanan"
    ]
   ],
   "title": "Building topic specific language models from webdata using competitive models",
   "original": "i05_1293",
   "page_count": 4,
   "order": 20,
   "p1": "1293",
   "pn": "1296",
   "abstract": [
    "The ability to build topic specific language models, rapidly and with minimal human effort, is a critical need for fast deployment and portability of ASR across different domains. The World Wide Web (WWW) promises to be an excellent textual data resource for creating topic specific language models. In this paper we describe an iterative web crawling approach which uses a competitive set of adaptive models comprised of a generic topic independent background language model, a noise model representing spurious text encountered in web based data (Webdata), and a topic specific model to generate query strings using a relative entropy based approach for WWW search engines and to weight the downloaded Webdata appropriately for building topic specific language models. We demonstrate how this system can be used to rapidly build language models for a specific domain given just an initial set of example utterances and how it can address the various issues attached with Webdata. In our experiments we were able to achieve a 20% reduction in perplexity for our target medical domain. The gains in perplexity translated to a 4% improvement in ASR word error rate (absolute) corresponding to a relative gain of 14%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-20"
  },
  "troncoso05_interspeech": {
   "authors": [
    [
     "Carlos",
     "Troncoso"
    ],
    [
     "Tatsuya",
     "Kawahara"
    ]
   ],
   "title": "Trigger-based language model adaptation for automatic meeting transcription",
   "original": "i05_1297",
   "page_count": 4,
   "order": 21,
   "p1": "1297",
   "pn": "1300",
   "abstract": [
    "We present a novel trigger-based language model adaptation method oriented to the transcription of meetings. In meetings, the topic is focused and consistent throughout the whole session, therefore keywords can be correlated over long distances. The trigger-based language model is designed to capture such longdistance dependencies, but it is typically constructed from a large corpus, which is usually too general to derive task-dependent trigger pairs. In the proposed method, we make use of the initial speech recognition results to extract task-dependent trigger pairs and to estimate their statistics. Moreover, we introduce a back-off scheme that also exploits the statistics estimated from a large corpus. The proposed model reduced the test-set perplexity twice as much as the typical trigger-based language model constructed from a large corpus, and achieved a remarkable perplexity reduction of 41% over the baseline when combined with an adapted trigram language model.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-21"
  },
  "duchateau05_interspeech": {
   "authors": [
    [
     "Jacques",
     "Duchateau"
    ],
    [
     "Dong Hoon Van",
     "Uytsel"
    ],
    [
     "Hugo Van",
     "Hamme"
    ],
    [
     "Patrick",
     "Wambacq"
    ]
   ],
   "title": "Statistical language models for large vocabulary spontaneous speech recognition in dutch",
   "original": "i05_1301",
   "page_count": 4,
   "order": 22,
   "p1": "1301",
   "pn": "1304",
   "abstract": [
    "In state-of-the-art large vocabulary automatic recognition systems, a large statistical language model is used, typically an N-gram. However in order to estimate this model, a large database of sentences or texts in the same style as the recognition task is needed. For spontaneous speech one doesn't dispose of such database since it should consist of accurate thus expensive orthographic transcriptions of spoken audio.\n",
    "This paper investigates how readily available large news paper corpora can be used to improve language models for spontaneous speech recognition although both language styles differ considerably. A technique is proposed that does a perplexity based automatic selection of appropriate news paper articles and that subsequently uses these texts in the language model estimation. Recognition experiments on spontaneous broadcast speech in Dutch showed significant improvements using this technique.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-22"
  },
  "allauzen05_interspeech": {
   "authors": [
    [
     "Alexandre",
     "Allauzen"
    ],
    [
     "Jean-Luc",
     "Gauvain"
    ]
   ],
   "title": "Diachronic vocabulary adaptation for broadcast news transcription",
   "original": "i05_1305",
   "page_count": 4,
   "order": 23,
   "p1": "1305",
   "pn": "1308",
   "abstract": [
    "This article investigates the use of Internet news sources to automatically adapt the vocabulary of a French and an English broadcast news transcription system. A specific method is developed to gather training, development and test corpora from selected websites, normalizing them for further use. A vectorial vocabulary adaptation algorithm is described which interpolates word frequencies estimated on adaptation corpora to directly maximize lexical coverage on a development corpus. To test the generality of this approach, experiments were carried out simultaneously in French and in English (UK) on a daily basis for the month May 2004. In both languages, the OOV rate is reduced by more than a half.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-23"
  },
  "siivola05_interspeech": {
   "authors": [
    [
     "Vesa",
     "Siivola"
    ],
    [
     "Bryan L.",
     "Pellom"
    ]
   ],
   "title": "Growing an n-gram language model",
   "original": "i05_1309",
   "page_count": 4,
   "order": 24,
   "p1": "1309",
   "pn": "1312",
   "abstract": [
    "Traditionally, when building an n-gram model, we decide the span of the model history, collect the relevant statistics and estimate the model. The model can be pruned down to a smaller size by manipulating the statistics or the estimated model. This paper shows how an n-gram model can be built by adding suitable sets of n-grams to a unigram model until desired complexity is reached. Very high order n-grams can be used in the model, since the need for handling the full unpruned model is eliminated by the proposed technique. We compare our growing method to entropy based pruning. In Finnish speech recognition tests, the models trained by the growing method outperform the entropy pruned models of similar size.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-24"
  },
  "huning05_interspeech": {
   "authors": [
    [
     "Harald",
     "Hüning"
    ],
    [
     "Manuel",
     "Kirschner"
    ],
    [
     "Fritz",
     "Class"
    ],
    [
     "Andre",
     "Berton"
    ],
    [
     "Udo",
     "Haiber"
    ]
   ],
   "title": "Embedding grammars into statistical language models",
   "original": "i05_1313",
   "page_count": 4,
   "order": 25,
   "p1": "1313",
   "pn": "1316",
   "abstract": [
    "This work combines grammars and statistical language models for speech recognition together in the same sentence. The grammars are compiled into bigrams with word indices, which serve to distinguish different syntactic positions of the same word. For both the grammatical and statistical parts there is one common interface for obtaining a language model score for bi- or trigrams. With only a small modification to a recogniser prepared for statistical language models, this new model can be applied without using a parser or a finite-state network in the recogniser. Priority is given to the grammar, therefore the combined model is able to disallow certain word transitions. With this combined language model, one or several grammatical phrases can be embedded into longer sentences.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-25"
  },
  "broman05_interspeech": {
   "authors": [
    [
     "Simo",
     "Broman"
    ],
    [
     "Mikko",
     "Kurimo"
    ]
   ],
   "title": "Methods for combining language models in speech recognition",
   "original": "i05_1317",
   "page_count": 4,
   "order": 26,
   "p1": "1317",
   "pn": "1320",
   "abstract": [
    "Statistical language models have a vital part in contemporary speech recognition systems and a lot of language models have been presented in the literature. The best results have been achieved when different language models have been used together. Several combination methods have been presented, but few comparisons of the different methods has been done.\n",
    "In this work, three combination methods that have been used with language models are studied. In addition, a new approach based on likelihood density function estimation using histograms is presented. The methods are evaluated in speech recognition experiments and perplexity calculations. The test data consist of Finnish news articles and four language models work as the component models.\n",
    "In the perplexity experiments, all combining methods produced statistically significant improvement compared to the 4-gram model that worked as a baseline. The best result, 46% improvement to the 4-gram model, was achieved when combining three language models together by using the new bin estimation method. In the speech recognition experiments, 4% reduction to the word error and over 7% reduction to the phoneme error was achieved by unigram rescaling method.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-26"
  },
  "vaiciunas05_interspeech": {
   "authors": [
    [
     "Airenas",
     "Vaiciunas"
    ],
    [
     "Gailius",
     "Raskinis"
    ]
   ],
   "title": "Review of statistical modeling of highly inflected lithuanian using very large vocabulary",
   "original": "i05_1321",
   "page_count": 4,
   "order": 27,
   "p1": "1321",
   "pn": "1324",
   "abstract": [
    "This paper presents state of the art language modeling (LM) of Lithuanian, which is highly inflected free word order language. Perplexities and word error rates (WER) of standard n-gram, class-based, cache-based, topic mixture and morphological LMs were estimated and compared for the vocabulary of more than 1 million words. WER estimates were obtained by solving a speakerdependent ASR task where LMs were used to rescore acoustical hypothesis. LM perplexity appeared to be uncorrelated with WER. Cache-based language models resulted in the greatest perplexity improvement, while class-based language models achieved the greatest though insignificant WER improvement over the baseline 3-gram.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-27"
  },
  "gorrell05_interspeech": {
   "authors": [
    [
     "Genevieve",
     "Gorrell"
    ],
    [
     "Brandyn",
     "Webb"
    ]
   ],
   "title": "Generalized hebbian algorithm for incremental latent semantic analysis",
   "original": "i05_1325",
   "page_count": 4,
   "order": 28,
   "p1": "1325",
   "pn": "1328",
   "abstract": [
    "The Generalized Hebbian Algorithm is shown to be equivalent to Latent Semantic Analysis, and applicable to a range of LSA-style tasks. GHA is a learning algorithm which converges on an approximation of the eigen decomposition of an unseen frequency matrix given observations presented in sequence. Use of GHA allows very large datasets to be processed.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-28"
  },
  "jensson05_interspeech": {
   "authors": [
    [
     "Arnar Thor",
     "Jensson"
    ],
    [
     "Edward W. D.",
     "Whittaker"
    ],
    [
     "Koji",
     "Iwano"
    ],
    [
     "Sadaoki",
     "Furui"
    ]
   ],
   "title": "Language model adaptation for resource deficient languages using translated data",
   "original": "i05_1329",
   "page_count": 4,
   "order": 29,
   "p1": "1329",
   "pn": "1332",
   "abstract": [
    "Text corpus size is an important issue when building a language model (LM). This is a particularly important issue for languages where little data is available. This paper introduces a technique to improve a LM built using a small amount of task dependent text with the help of a machine-translated text corpus. Perplexity experiments were performed using data, machine translated (MT) from English to French on a sentence-by-sentence basis and using dictionary lookup on a word-by-word basis. Then perplexity and word error rate experiments using MT data from English to Icelandic were done on a word-by-word basis. For the latter, the baseline word error rate was 44.0%. LM interpolation reduced word error rate significantly to 39.2%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-29"
  },
  "witschel05_interspeech": {
   "authors": [
    [
     "Petra",
     "Witschel"
    ],
    [
     "Sergey",
     "Astrov"
    ],
    [
     "Gabriele",
     "Bakenecker"
    ],
    [
     "Josef G.",
     "Bauer"
    ],
    [
     "Harald",
     "Höge"
    ]
   ],
   "title": "POS-based language models for large vocabulary speech recognition on embedded systems",
   "original": "i05_1333",
   "page_count": 4,
   "order": 30,
   "p1": "1333",
   "pn": "1336",
   "abstract": [
    "Speech recognition on embedded systems requires components of low memory footprint and low computational complexity. In this paper a POS-based (part of speech based) language modeling approach is presented which decreases the number of language model parameters combined with a method for reducing memory consumptions via quantization of language model penalties. For the application of short message dictation a language model with about 10,000 words of vocabulary is generated. Using the POS-based language modeling approach the number of parameters comprises 70,058 penalties. The memory consumptions for storing those penalties are reduced about 50% using the presented coding method. Experiments show that the POS-based language model is able to reduce the WER up to 65% for n-best isolated word recognition in comparison to the case without language model. Moreover the increase of WER caused by coding of the language model penalties is not significant.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-30"
  },
  "hirst05_interspeech": {
   "authors": [
    [
     "Daniel",
     "Hirst"
    ],
    [
     "Caroline",
     "Bouzon"
    ]
   ],
   "title": "The effect of stress and boundaries on segmental duration in a corpus of authentic speech (british English)",
   "original": "i05_0029",
   "page_count": 4,
   "order": 31,
   "p1": "29",
   "pn": "32",
   "abstract": [
    "Research into the effect of stress and boundaries on segmental duration in speech has, for obvious reasons, most often been applied to carefully constructed sentences pronounced in laboratory conditions. The availability of a large labelled database of British English (Aix-Marsec) provides an opportunity to test different hypotheses concerning the factors influencing segmental duration from a corpus of authentic speech (defined as speech produced with the intent of communicating its meaning to the listener). In particular, in this paper, we look at the effect of stress and boundaries on prosodic structure in British English. Recent work has suggested that while word boundaries seem definitely to have a significant effect of the duration of segments, once the number of segments in the narrow rhythm unit is known, there is no orthogonal effect of word stress. In this study we look in particular at effects of word and intonation unit boundaries and at their possible interaction with stress and find that while intonation unit boundaries definitely affect segmental duration, no similar effect could be shown for word boundaries.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-31"
  },
  "ohsuga05_interspeech": {
   "authors": [
    [
     "Tomoko",
     "Ohsuga"
    ],
    [
     "Masafumi",
     "Nishida"
    ],
    [
     "Yasuo",
     "Horiuchi"
    ],
    [
     "Akira",
     "Ichikawa"
    ]
   ],
   "title": "Investigation of the relationship between turn-taking and prosodic features in spontaneous dialogue",
   "original": "i05_0033",
   "page_count": 4,
   "order": 32,
   "p1": "33",
   "pn": "36",
   "abstract": [
    "In this study, we investigated the relationship between turn-taking and prosody. We considered that to interact smoothly in real-time communication, speakers must show presignals to turn-taking as prosodic features before turn edges. We attempted to discriminate the turn change by the decision tree method using only prosodic features in turn-final accentual phrases that include earlier positions compared with turn-final mora. In the discrimination experiment, we used the corpus of Japanese spontaneous dialogue, and defined prosodic parameters such as F0 contour, power contour and duration. We compared the two parameter conditions for using parameters with and without the final mora of turns. From the results, the accuracy under the conditions of not using the parameters of the final mora is 80%, which is not significantly worse than the result of 83% when using all parameters. Taking into account only prosody was used, we consider this result to be reasonably good.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-32"
  },
  "watanabe05_interspeech": {
   "authors": [
    [
     "Michiko",
     "Watanabe"
    ],
    [
     "Keikichi",
     "Hirose"
    ],
    [
     "Yasuharu",
     "Den"
    ],
    [
     "Nobuaki",
     "Minematsu"
    ]
   ],
   "title": "Filled pauses as cues to the complexity of following phrases",
   "original": "i05_0037",
   "page_count": 4,
   "order": 33,
   "p1": "37",
   "pn": "40",
   "abstract": [
    "Corpus based studies of spontaneous speech showed that filled pauses tended to precede relatively long and complex constituents. We examined whether listeners made use of such a tendency in speech processing. We tested the hypothesis that when listeners heard filled pauses they tended to expect a relatively long and complex phrase to follow. In the experiment participants listened to sentences referring to both simple and compound shapes presented on a computer screen. Their task was to press a button as soon as they had identified the shape that they heard. The sentences involved two factors: complexity and fluency. As the complexity factor, a half of the sentences described compound shapes with long and complex phrases and the other half described simple shapes with short and simple phrases. As the fluency factor phrases describing a shape had a preceding filled pause, a preceding silent pause of the same length as the filled pause, or no preceding pause. The results showed that response times for the complex phrases were significantly shorter after filled or silent pauses than when there was no pause. In contrast, there was no significant difference between the three conditions for the simple phrases. The results support the hypothesis and indicate that it is the duration of filled pauses that give listeners cues to the complexity of upcoming phrases.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-33"
  },
  "schneider05_interspeech": {
   "authors": [
    [
     "Katrin",
     "Schneider"
    ],
    [
     "Bernd",
     "Möbius"
    ]
   ],
   "title": "Perceptual magnet effect in German boundary tones",
   "original": "i05_0041",
   "page_count": 4,
   "order": 34,
   "p1": "41",
   "pn": "44",
   "abstract": [
    "The experiment described in this paper tests for the perceptual magnet effect within the categories of high and low boundary tones in German, referring to question and statement, respectively. The experiment is based on previous work in which the categorical status of the two German boundary tones had been evaluated. The results found there showed that there was a discrimination ability within categories which could not be explained by the classical definition of categorical perception. The results reported in the present paper show that a perceptual magnet exists in the statement category but not in the question category.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-34"
  },
  "grimm05_interspeech": {
   "authors": [
    [
     "Angela",
     "Grimm"
    ],
    [
     "Jochen",
     "Trommer"
    ]
   ],
   "title": "Constraints on the acquisition of simplex and complex words in German",
   "original": "i05_0045",
   "page_count": 4,
   "order": 35,
   "p1": "45",
   "pn": "48",
   "abstract": [
    "It is a common assumption that prosodic restrictions on the shape of children's early productions refer to the prosodic word (cf. [1]). However, empirical research on word structure has focused almost exclusively on simplex words where the morphosyntactic and prosodic word boundaries coincide ([2], [3], [4], [5]). In this paper, we provide new evidence from the acquisition of German complex words (compounds and particle verbs) showing that the restriction to a single foot indeed holds for the prosodic word, not for the morphosyntactic word. Thus, our results corroborate the crucial function of the prosodic word in language development.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-35"
  },
  "meyer05_interspeech": {
   "authors": [
    [
     "Julien",
     "Meyer"
    ]
   ],
   "title": "Whistled speech: a natural phonetic description of languages adapted to human perception and to the acoustical environment",
   "original": "i05_0049",
   "page_count": 4,
   "order": 36,
   "p1": "49",
   "pn": "52",
   "abstract": [
    "The scientific study of the whistled speech of several languages has already provided an alternative point of view on many aspects of language. After a general overview on the phenomenon, this paper develops a comparative analysis of several whistled forms of non tonal languages which are still in use. Meanwhile, the vocalic and consonantal reductions observed in this type of whistled speech are detailed thanks to a typological approach. It sheds a new light on the main aspects of the encoding strategy thanks to results of acoustic propagation and perceptive tests. Actually, whistled languages naturally take advantage of a narrow band of frequencies to focus on key elements of the phonology. They carry an essential part of the linguistic information that the listeners are able to recognize if they have overcome a long period of learning. Therefore, they can be seen as phonetic descriptions of local languages. Such properties are enabled by whistles which are remarkably adapted to the perceptive capacities of human beings and to the natural acoustic environment.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-36"
  },
  "kim05_interspeech": {
   "authors": [
    [
     "Heejin",
     "Kim"
    ],
    [
     "Jennifer",
     "Cole"
    ]
   ],
   "title": "The stress foot as a unit of planned timing: evidence from shortening in the prosodic phrase",
   "original": "i05_2365",
   "page_count": 4,
   "order": 37,
   "p1": "2365",
   "pn": "2368",
   "abstract": [
    "This study investigates whether the stress foot is a planned timing unit in American English, by examining the durational characteristics of the foot in three different prosodic contexts - i) within an intermediate phrase, ii) across an intermediate phrase and iii) across an intonational phrase. The results show that as the number of syllables in a foot increases, the duration of the foot increases, but the mean duration of syllables is reduced. Our examination of the internal structure of the foot reveals that there is a consistent shortening of stressed syllables within an intermediate phrase. These findings indicate that the stress foot within the intermediate phrase is a timing unit where durational shortening occurs in compensation for an increase in syllable count within the foot.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-37"
  },
  "welby05_interspeech": {
   "authors": [
    [
     "Pauline",
     "Welby"
    ],
    [
     "Hélène",
     "Loevenbruck"
    ]
   ],
   "title": "Segmental \"anchorage\" and the French late rise",
   "original": "i05_2369",
   "page_count": 4,
   "order": 38,
   "p1": "2369",
   "pn": "2372",
   "abstract": [
    "We examined the tonal alignment and scaling patterns of the start and end points of the French late rise, using a rate manipulation paradigm. Our findings call into question aspects of the segmental anchoring hypothesis: the low starting point of the late rise was not stably anchored to a segmental landmark, and for some speakers, F0 excursion size varied across rates. The position of the peak of the late rise was found to vary across syllable structures. To account for the observed patterns, we propose the notion of an \"anchorage,\" that is, a region within which an intonational turning point can anchor. For the peak of the French late rise, this anchorage stretches from just before the end of the vowel of the last full syllable of the accentual phrase to the end of the phrase.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-38"
  },
  "chow05_interspeech": {
   "authors": [
    [
     "Ivan",
     "Chow"
    ]
   ],
   "title": "Prosodic cues for syntactically-motivated junctures",
   "original": "i05_2373",
   "page_count": 4,
   "order": 39,
   "p1": "2373",
   "pn": "2376",
   "abstract": [
    "A pilot study was conducted to examine the manner in which Cantonese speakers use prosody to mark syntactic junctures in speech production. Test sentences were designed to create the intended experimental conditions - each sentence-pair consists of an identical array of morphemes with exactly two interpretations according to two different syntactic structures. These structures can be clarified by conveying prosodic boundaries that coincide with junctures between major syntactic constituents. The purpose of the experiment as well as the meaning of the test sentences (with respect to their syntactic structures) were explained to the subjects. After a few practice runs, they were asked to read these sentences aloud, conveying the marked boundaries, while their voices were being recorded. Three prosodic juncture markers: pauses, pitch reset and pre-boundary lengthening were under examination. Results of the acoustic and statistical analyses of the recorded signals indicated that, pauses were the most effective way of marking junctures, followed by pitch reset. Pre-boundary lengthening was found to be infrequent; in the rare cases where it was detected, pre-boundary syllables were only slightly longer than their non-boundary counterparts. Nonetheless, vast individual differences in terms of the amplitudes and frequencies of prosodic juncture markers were observed. The present study provides acoustical data regarding the manner in which Cantonese speakers use prosody in utterance structure clarification, vis-à-vis their specific language experience. In a large-scale experiment, test sentences will be embedded in contextual paragraphs that semantically and prosodically prompt readers to convey the intended prosodic structure. This experiment is underway and is expected to yield more conclusive results.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-39"
  },
  "fale05_interspeech": {
   "authors": [
    [
     "Isabel",
     "Falé"
    ],
    [
     "Isabel",
     "Hub Faria"
    ]
   ],
   "title": "A glimpse of the time-course of intonation processing in European Portuguese",
   "original": "i05_2377",
   "page_count": 4,
   "order": 40,
   "p1": "2377",
   "pn": "2380",
   "abstract": [
    "We have investigated the phenomenon of prediction in speech processing through intonational contrasts in European Portuguese (EP) grammar.\n",
    "20 EP subjects were presented with auditory speech stimuli gated in specific locations on a sentence which they had to classify within a category. Afterwards, they also had to rate the confidence level for each of their answers.\n",
    "Declarative sentences (statements) were identified at the first stressed vowel in the 5 pairs of stimuli by more than 50% of the subjects. On the other hand, interrogatives (questions) were identified later on the sentence near its end (specifically, on the last stressed vowel). These results suggest a dual approach for EP intonational data analysis: global and local.\n",
    "The confidence level results showed that listeners need more data to be sure about their stimulus sentence type identification. EP listeners were really attentive to phonetic detail specificities of the speech signal and started to build their internal representation of the intonation contour very early in the sentence with these data. Therefore, relevant prosodic cues must be available early in the speech signal.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-40"
  },
  "wagner05_interspeech": {
   "authors": [
    [
     "Petra",
     "Wagner"
    ]
   ],
   "title": "Great expectations - introspective vs. perceptual prominence ratings and their acoustic correlates",
   "original": "i05_2381",
   "page_count": 4,
   "order": 41,
   "p1": "2381",
   "pn": "2384",
   "abstract": [
    "In order to gain knowledge about the interaction between top-down expectations of listeners concerning prosodic prominence and its acoustic correlates, two exploratory empirical studies were carried out. First, native and non-native subjects rated prominences of speech read at normal and very fast - prosodically very different - speech. Later, these ratings were compared with introspective prominence ratings of different listeners. First results indicate a major influence of the introspection on prominence ratings, especially if acoustic cues are difficult to interpret, as it is the case in very fast speech. Compared to native subjects, non-natives rely less on their introspection and more on the acoustics.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-41"
  },
  "jensen05_interspeech": {
   "authors": [
    [
     "Christian",
     "Jensen"
    ],
    [
     "John",
     "Tøndering"
    ]
   ],
   "title": "Choosing a scale for measuring perceived prominence",
   "original": "i05_2385",
   "page_count": 4,
   "order": 42,
   "p1": "2385",
   "pn": "2388",
   "abstract": [
    "Three different scales which have been used to measure perceived prominence are evaluated in a perceptual experiment. Average scores of raters using a multi-level (31-point) scale, a simple binary (2-point) scale and an intermediate 4-point scale are almost identical. The potentially finer gradation possible with the multi-level scale(s) is compensated for by having multiple listeners, which is a also a requirement for obtaining reliable data. In other words, a high number of levels is neither a sufficient nor a necessary requirement. Overall the best results were obtained using the 4-point scale, and there seems to be little justification for using a 31-point scale.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-42"
  },
  "edlund05_interspeech": {
   "authors": [
    [
     "Jens",
     "Edlund"
    ],
    [
     "David",
     "House"
    ],
    [
     "Gabriel",
     "Skantze"
    ]
   ],
   "title": "The effects of prosodic features on the interpretation of clarification ellipses",
   "original": "i05_2389",
   "page_count": 4,
   "order": 43,
   "p1": "2389",
   "pn": "2392",
   "abstract": [
    "In this paper, the effects of prosodic features on the interpretation of elliptical clarification requests in dialogue are studied. An experiment is presented where subjects were asked to listen to short human-computer dialogue fragments in Swedish, where a synthetic voice was making an elliptical clarification after a user turn. The prosodic features of the synthetic voice were systematically varied, and the subjects were asked to judge what was actually intended by the computer. The results show that an early low F0 peak signals acceptance, that a late high peak is perceived as a request for clarification of what was said, and that a mid high peak is perceived as a request for clarification of the meaning of what was said. The study can be seen as the beginnings of a tentative model for intonation of clarification ellipses in Swedish, which can be implemented and tested in spoken dialogue systems.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-43"
  },
  "jilka05_interspeech": {
   "authors": [
    [
     "Matthias",
     "Jilka"
    ]
   ],
   "title": "Exploration of different types of intonational deviations in foreign-accented and synthesized speech",
   "original": "i05_2393",
   "page_count": 4,
   "order": 44,
   "p1": "2393",
   "pn": "2396",
   "abstract": [
    "The study provides an analysis of the basic manifestations of intonational deviations in foreign-accented (American English accent in German) and synthesized speech. It takes into account the crucial influence of the used model of intonation description and makes a major distinction between individual deviations that cause the impression of foreignness or unnaturalness immediately when they occur, and others that do so only when an accumulation of several such deviations does not allow for a meaningful interpretation anymore. It is argued that this is due to the high variability allowed in prosodic contexts. A closer description of the first group of deviations includes the transfer of categories and of the phonetic realizations of categories as well as a discussion of seemingly unmotivated errors and the most likely causes of intonation errors in synthesized speech. Finally, it is shown that in the case of foreign accent the language-specific manifestations of the presented deviations combine to create a characteristic overall impression of foreignness that is recognizable independently of the segmental content of an utterance.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-44"
  },
  "broggelwirth05_interspeech": {
   "authors": [
    [
     "Jörg",
     "Bröggelwirth"
    ]
   ],
   "title": "A rhythmic-prosodic model of poetic speech",
   "original": "i05_2397",
   "page_count": 4,
   "order": 45,
   "p1": "2397",
   "pn": "2400",
   "abstract": [
    "In this paper a new approach towards the analysis of speech rhythm is presented. In the speech rhythm literature it was often discussed that rhythmic phenomena are more transparent in the metrical structure of orally produced poetry. However, up to now only a few phoneticians have worked on this special speaking style. For analyzing the rhythmic and prosodic patterns of this kind of speech, a corpus of read German poetry, including four different meters, was recorded. This study gives a first sight on durational and intonational effects in the data. A final prosodic modeling and its perceptual evaluation is currently under development.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-45"
  },
  "biersack05_interspeech": {
   "authors": [
    [
     "Sonja",
     "Biersack"
    ],
    [
     "Vera",
     "Kempe"
    ],
    [
     "Lorna",
     "Knapton"
    ]
   ],
   "title": "Fine-tuning speech registers: a comparison of the prosodic features of child-directed and foreigner-directed speech",
   "original": "i05_2401",
   "page_count": 4,
   "order": 46,
   "p1": "2401",
   "pn": "2404",
   "abstract": [
    "The present study compares prosodic features of child-directed speech (CDS) and foreigner-directed speech (FDS), to examine whether FDS is a derivative of CDS as suggested in sociolinguistic studies. Twelve female speakers completed a simple referential communication task addressed to an imaginary adult, an imaginary foreigner, and an imaginary child.\n",
    "The results showed that, compared to the adult-directed baseline (ADS), participants increased pitch range and f0 maxima when addressing a child, but not when addressing a foreigner. Furthermore, participants lowered their speech rate when addressing interlocutors with limited linguistic capacity, but did so differentially: Participants tended to lengthen pauses when addressing an imaginary foreigner, and tended to lengthen segments when addressing an imaginary child.\n",
    "These findings suggest that the prosodic features of CDS and FDS are different, and that speakers have acquired knowledge about how to fine-tune their prosodic adjustments to the specific needs of different interlocutors.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-46"
  },
  "arbisikelm05_interspeech": {
   "authors": [
    [
     "Timothy",
     "Arbisi-Kelm"
    ]
   ],
   "title": "An analysis of the intonational structure of stuttered speech",
   "original": "i05_2405",
   "page_count": 4,
   "order": 47,
   "p1": "2405",
   "pn": "2408",
   "abstract": [
    "While previous studies have successfully revealed areas vulnerable to disfluency at the word level in stuttering, identifying the specific factors responsible for this instability has proved difficult. Analyzing the effects of phrasal prosody, which governs such word-level factors as lexical stress [1], is critical in order to account for the relations between word-level and phrase-level effects, and how they affect patterns of disfluency in stuttered speech. In a story-telling task performed by two stutterers and two control subjects, it was found that stutterers' disfluencies were accompanied by more prosodic irregularities prior to the actual cause of the disfluency. In particular, changes in f0 and duration affect the realization of cues in the disfluent environment, resulting in fundamental alterations of intonational phrase structure. Anticipatory and target-realized disfluencies contribute different acoustic cues to their immediate environments, the results of which often create conflicting prominence relationships among prosodic constituents, thereby losing information key for conveying meaning.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-47"
  },
  "lintfert05_interspeech": {
   "authors": [
    [
     "Britta",
     "Lintfert"
    ],
    [
     "Wolfgang",
     "Wokurek"
    ]
   ],
   "title": "Voice quality dimensions of pitch accents",
   "original": "i05_2409",
   "page_count": 4,
   "order": 48,
   "p1": "2409",
   "pn": "2412",
   "abstract": [
    "Acoustic and electroglottographic (EGG) measurements were used to examine voice quality parameters during the production of the rising and falling pitch movements in German. The vowels /a:/ and /E/ were studied in a single-speaker speech corpus. The acoustic measurements comprised an automatic spectral analysis of the glottal parameters open quotient (OQ), glottal opening (GO), skewness of glottal pulse (SK), rate of closure (RC), amplitude of voicing (AV) and completeness of closure (CC). OQ and AV seem to be the only acoustic parameters influenced by pitch accent and not by word stress. From the electroglottographic measurements only open quotient parameters (OQI and OQII), two parameters of closing phase (SCV and SCA) and two parameters of opening phase (EOV and EOT) showed a significant difference as a function of pitch accent type.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-48"
  },
  "dohen05_interspeech": {
   "authors": [
    [
     "Marion",
     "Dohen"
    ],
    [
     "Hélène",
     "Loevenbruck"
    ]
   ],
   "title": "Audiovisual production and perception of contrastive focus in French: a multispeaker study",
   "original": "i05_2413",
   "page_count": 4,
   "order": 49,
   "p1": "2413",
   "pn": "2416",
   "abstract": [
    "This study examines the visual cues to prosodic contrastive focus in Hexagonal French and their role in visual speech perception. Two audiovisual corpora were recorded (from two male native speakers of French) consisting of sentences with a subject-verb-object (SVO) syntactic structure. Four conditions were studied: focus on each phrase (S,V,O) and broad focus. The corpora were first acoustically validated. Then lip area and jaw opening were extracted from the video. For each speaker, we identified a set of visible correlates of contrastive focus. The combined results showed that there were consistent visible articulatory correlates of contrastive focus across speakers: a) an increase in lip area and its first derivative on the focused item b) a lengthening of the focal syllables. There were also speaker-specific strategies in the amount of a) pre-focal anticipation or b) post-focal hypo-articulation.\n",
    "Visual only perception tests were then conducted to see if the identified correlates were valid cues in perception. They showed that contrastive focus was well perceived visually for both speakers. The scores were better for the first speaker who displayed greater focal hyper-articulation. We also found that presence and salience of the visual cues enhances perception.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-49"
  },
  "barkhuysen05_interspeech": {
   "authors": [
    [
     "Pashiera",
     "Barkhuysen"
    ],
    [
     "Emiel",
     "Krahmer"
    ],
    [
     "Marc",
     "Swerts"
    ]
   ],
   "title": "Predicting end of utterance in multimodal and unimodal conditions",
   "original": "i05_2417",
   "page_count": 4,
   "order": 50,
   "p1": "2417",
   "pn": "2420",
   "abstract": [
    "In this paper, we describe a series of perception studies on uniand multimodal cues to end of utterance. Stimuli were fragments taken from a recorded interview session, consisting of the parts in which speakers provided answers. The answers varied in length and were presented without the preceding question of the interviewer. The subjects had to predict when the speaker would finish his turn, based on video material and/or auditory material. The experiment consisted of 3 conditions: in one condition, the stimuli were presented as they were recorded (both audio and vision), in the two remaining conditions stimuli were presented in only the auditory or the visual channel. Results show that the audiovisual condition evoked the fastest reaction times and the visual condition the slowest. Arguably, the combination of cues from different modalities function as complementary sources and might thus improve prediction.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-50"
  },
  "tanaka05_interspeech": {
   "authors": [
    [
     "Saori",
     "Tanaka"
    ],
    [
     "Masafumi",
     "Nishida"
    ],
    [
     "Yasuo",
     "Horiuchi"
    ],
    [
     "Akira",
     "Ichikawa"
    ]
   ],
   "title": "Production of prominence in Japanese sign language",
   "original": "i05_2421",
   "page_count": 4,
   "order": 51,
   "p1": "2421",
   "pn": "2424",
   "abstract": [
    "In sign language research, technically it has been possible to investigate prominence around a unit of sign movements that realizes strong visual impression. Based on the researches of speech prominence, this study proposes techniques to delimit a sequential hand-movement into small units, and investigates the prominence by the comparisons of physical properties on each unit between emphasized signing and non-emphasized signing in Japanese Sign Language. The original data for this paper came from 3 native signers who produced 3 sentences, 5 times in 2 modes. The result of Factor Analysis showed that prominence on the lexical parts of the sign movements was most distinctive through all subjects and examples. The varieties in the transitional part of the sign movements and the longer pause insertions before the lexical part of the sign movements were also observed.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-51"
  },
  "siohan05_interspeech": {
   "authors": [
    [
     "Olivier",
     "Siohan"
    ],
    [
     "Michiel",
     "Bacchiani"
    ]
   ],
   "title": "Fast vocabulary-independent audio search using path-based graph indexing",
   "original": "i05_0053",
   "page_count": 4,
   "order": 52,
   "p1": "53",
   "pn": "56",
   "abstract": [
    "Classical audio retrieval techniques consist in transcribing audio documents using a large vocabulary speech recognition system and indexing the resulting transcripts. However, queries that are not part of the recognizer's vocabulary or have a large probability of getting misrecognized can significantly impair the performance of the retrieval system. Instead, we propose a fast vocabulary independent audio search approach that operates on phonetic lattices and is suitable for any query. However, indexing phonetic lattices so that any arbitrary phone sequence query can be processed efficiently is a challenge, as the choice of the indexing unit is unclear. We propose an inverted index structure on lattices that uses paths as indexing features. The approach is inspired by a general graph indexing method that defines an automatic procedure to select a small number of paths as indexing features, keeping the index size small while allowing fast retrieval of the lattices matching a given query. The effectiveness of the proposed approach is illustrated on broadcast news and Switchboard databases.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-52"
  },
  "makhoul05_interspeech": {
   "authors": [
    [
     "John",
     "Makhoul"
    ],
    [
     "Alex",
     "Baron"
    ],
    [
     "Ivan",
     "Bulyko"
    ],
    [
     "Long",
     "Nguyen"
    ],
    [
     "Lance",
     "Ramshaw"
    ],
    [
     "David",
     "Stallard"
    ],
    [
     "Richard",
     "Schwartz"
    ],
    [
     "Bing",
     "Xiang"
    ]
   ],
   "title": "The effects of speech recognition and punctuation on information extraction performance",
   "original": "i05_0057",
   "page_count": 4,
   "order": 53,
   "p1": "57",
   "pn": "60",
   "abstract": [
    "We report on experiments to measure the effect of speech recognition errors and automatic punctuation insertion errors on the performance of information extraction (entity and relation extraction). The outputs of several recognition systems with a range of word error rates (WER), along with punctuation insertion, were fed into a system that extracts entities and relations from the recognized text. Entity and relation value scores were measured as a function of WER and types of punctuation used. The results of the experiments showed that both entity and relation value scores degrade linearly with increasing WER, with a relative reduction in scores of about twice the WER. The information extraction modules require the inclusion of sentence boundaries, at a minimum; however, the experiments showed that the exact locations of these boundaries are not important for entity and relation extraction. In contrast, when comparing the effects of full punctuation to just automatic sentence boundary insertion, there was a loss in entity value scores of 13.5% and in relation value scores of 25%. Further, commas play a significantly greater role in entity and relation extraction than other types of punctuation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-53"
  },
  "chelba05_interspeech": {
   "authors": [
    [
     "Ciprian",
     "Chelba"
    ],
    [
     "Alex",
     "Acero"
    ]
   ],
   "title": "Indexing uncertainty for spoken document search",
   "original": "i05_0061",
   "page_count": 4,
   "order": 54,
   "p1": "61",
   "pn": "64",
   "abstract": [
    "The paper presents the Position Specific Posterior Lattice, a novel lossy representation of automatic speech recognition lattices that naturally lends itself to efficient indexing and subsequent relevance ranking of spoken documents. Albeit lossy, the PSPL lattice is much more compact than the ASR 3-gram lattice from which it is computed, at virtually no degradation in word-error-rate performance. Since new paths are introduced in the lattice, the \"oracle\" accuracy increases over the original ASR lattice. In experiments performed on a collection of lecture recordings - MIT iCampus database - the spoken document ranking accuracy was improved by 20% relative over the commonly used baseline of indexing the 1-best output from an automatic speech recognizer. The Mean Average Precision (MAP) increased from 0.53 when using 1-best output to 0.62 when using the new lattice representation. The reference used for evaluation is the output of a standard retrieval engine working on the manual transcription of the speech collection.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-54"
  },
  "akiba05_interspeech": {
   "authors": [
    [
     "Tomoyosi",
     "Akiba"
    ],
    [
     "Hiroyuki",
     "Abe"
    ]
   ],
   "title": "Exploiting passage retrieval for n-best rescoring of spoken questions",
   "original": "i05_0065",
   "page_count": 4,
   "order": 55,
   "p1": "65",
   "pn": "68",
   "abstract": [
    "Speech interfaces using LVCSR system have promise for improving the utility of Open-domain Question Answering, in which natural language questions about diversified topics are used as inputs. In this paper, we propose a method to improve both speech recognition and question answering performance by incorporating the passage retrieval, which is a component common to many QA systems, with respect to the target documents that the input question asked about. In the QA process, the passage that has the high similarity to the question has the high possibility to have the correct answer in it. Conversely, this similarity can be used to select the appropriate candidate from N-best list of speech recognition results. From language modeling perspective, this process can be seen to capture the semantic consistency of spoken question in sentence level as compared with conventional n-gram language models. We show the effectiveness of our method by means of experiments.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-55"
  },
  "kolluru05_interspeech": {
   "authors": [
    [
     "BalaKrishna",
     "Kolluru"
    ],
    [
     "Heidi",
     "Christensen"
    ],
    [
     "Yoshihiko",
     "Gotoh"
    ]
   ],
   "title": "Multi-stage compaction approach to broadcast news summarisation",
   "original": "i05_0069",
   "page_count": 4,
   "order": 56,
   "p1": "69",
   "pn": "72",
   "abstract": [
    "This paper presents a fully automatic, multi-stage compaction approach to broadcast news summarisation, targeting transcripts from automatic speech recognition (ASR) systems. It employs a network of multi-layer perceptrons to remove incorrectly transcribed words based on confidence scores, and to select significant chunks at multiple stages based on tf.idf scores and named entity frequency. The resulting summaries are assessed using a combination of cross comprehension test and a fluency test, finally compared with an automatic evaluation scheme. The experimental results show the approach can produce summaries with good information content.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-56"
  },
  "huang05_interspeech": {
   "authors": [
    [
     "Chien-Lin",
     "Huang"
    ],
    [
     "Chia-Hsin",
     "Hsieh"
    ],
    [
     "Chung-Hsien",
     "Wu"
    ]
   ],
   "title": "Audio-video summarization of TV news using speech recognition and shot change detection",
   "original": "i05_0073",
   "page_count": 4,
   "order": 57,
   "p1": "73",
   "pn": "76",
   "abstract": [
    "This paper presents an approach to audio-video summarization of TV news to provide concise information about the content while preserves the essential message of the original. In this study, anchor speech and field report videos are considered separately. First, speech signal is automatically recognized as transcripts and a confidence measure considering syntactic and semantic relations is used to estimate the reliability of words. For video skimming, RGB color histogram difference is adopted to segment video shots and evaluate the smoothness of images concatenation. As a result, the extracted anchor speech and the field report image sequence of TV news are aggregated into a summarization output. The experimental results indicate that the proposed approach effectively extracts important speech segments and gives a concise video sequence.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-57"
  },
  "taniguchi05_interspeech": {
   "authors": [
    [
     "Toru",
     "Taniguchi"
    ],
    [
     "Akishige",
     "Adachi"
    ],
    [
     "Shigeki",
     "Okawa"
    ],
    [
     "Masaaki",
     "Honda"
    ],
    [
     "Katsuhiko",
     "Shirai"
    ]
   ],
   "title": "Discrimination of speech, musical instruments and singing voices using the temporal patterns of sinusoidal segments in audio signals",
   "original": "i05_0589",
   "page_count": 4,
   "order": 58,
   "p1": "589",
   "pn": "592",
   "abstract": [
    "We developed a method for discriminating speech, musical instruments and singing voices based on sinusoidal decomposition of audio signals. Although many studies have been conducted, few have worked on the problem of the temporal overlapping of the categories of sounds. In order to cope with such problems, we used sinusoidal segments with variable lengths as the discrimination units, although most of traditional work has used fixed-length units. The discrimination is based on the temporal characteristics of the sinusoidal segments. We achieved an average discrimination rate of 71.56% in classifying sinusoidal segments in non-mixed audio data. In the time segments, the accuracy 87.9% in non-mixedcategory audio data and 66.4% in 2-mixed-category are achieved. In the comparison of the proposed and the MFCC methods, the effectiveness of temporal features and the importance of the use of both the spectral and temporal characteristics were proved.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-58"
  },
  "murray05_interspeech": {
   "authors": [
    [
     "Gabriel",
     "Murray"
    ],
    [
     "Steve",
     "Renals"
    ],
    [
     "Jean",
     "Carletta"
    ]
   ],
   "title": "Extractive summarization of meeting recordings",
   "original": "i05_0593",
   "page_count": 4,
   "order": 59,
   "p1": "593",
   "pn": "596",
   "abstract": [
    "Several approaches to automatic speech summarization are discussed below, using the ICSI Meetings corpus. We contrast feature-based approaches using prosodic and lexical features with maximal marginal relevance and latent semantic analysis approaches to summarization. While the latter two techniques are borrowed directly from the field of text summarization, featurebased approaches using prosodic information are able to utilize characteristics unique to speech data. We also investigate how the summarization results might deteriorate when carried out on ASR output as opposed to manual transcripts. All of the summaries are of an extractive variety, and are compared using the software ROUGE.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-59"
  },
  "hessen05_interspeech": {
   "authors": [
    [
     "Arjan van",
     "Hessen"
    ],
    [
     "Jaap",
     "Hinke"
    ]
   ],
   "title": "IR-based classification of customer-agent phone calls",
   "original": "i05_0597",
   "page_count": 4,
   "order": 60,
   "p1": "597",
   "pn": "600",
   "abstract": [
    "The use of menu based speech recognition in which callers get the desired information or are routed to the right department is limited by the a-priori knowledge of these menus by the callers. Automatic speech recognition has always been promoted as superior to classic DTMF-IVR (push 1 for John, 2 for . . .) because it is faster and more user friendly. Instead of listening to endless menu options and pushing the corresponding key(s), a user may just say the desired item. This works fine but the problem is that often people do not know what to say.\n",
    "Classification based on the results of mostly imperfect speech recognition of open questions may help to overcome this deadlock. In situations were the caller can explain his/her problem but does not know were to address it too, this approach may be very useful. Moreover, classification of incoming calls can be used to generate automatically management reports about what time people spoke about which subject.\n",
    "In this paper we will discuss the design and the results of the first pilot implementation of a speech recogniser in an Information Retrieval (IR) system at an assurance company. Incoming calls are recognised and matched with previous, already classified calls. The labels of these classified calls are used to calculate a label (i.e. class) for this new call.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-60"
  },
  "favre05_interspeech": {
   "authors": [
    [
     "Benoît",
     "Favre"
    ],
    [
     "Frédéric",
     "Béchet"
    ],
    [
     "Pascal",
     "Nocéra"
    ]
   ],
   "title": "Mining broadcast news data: robust information extraction from word lattices",
   "original": "i05_0601",
   "page_count": 4,
   "order": 61,
   "p1": "601",
   "pn": "604",
   "abstract": [
    "Fine-grained information extraction performance from spoken corpora is strongly correlated with the Word Error Rate (WER) of the automatic transcriptions processed. Despite the recent advances in Automatic Speech Recognition (ASR) methods, high WER transcriptions are common when dealing with unmatched conditions between the documents to process and those used to train the ASR models. Such mismatch is inevitable in the processing of large spoken archives containing documents related to a large number of time periods and topics. Moreover, from a text indexation point of view, rare events and entities are often the most interesting information to extract as well as the ones that are very likely to be poorly recognized. In order to deal with high WER transcriptions this paper proposes a robust Information Extraction method that mines the full ASR search space for specific entities thanks to a 3-steps process: firstly, adaptation of the extraction models thanks to metadata information linked to the documents to process; secondly transduction of a word lattice outputs by the ASR module into an entity lattice; thirdly a decision module that scores each entity hypothesis with different confidence scores. A first implementation of this model is proposed for the French Broadcast News Named Entity extraction task of the evaluation program ESTER.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-61"
  },
  "kurimo05_interspeech": {
   "authors": [
    [
     "Mikko",
     "Kurimo"
    ],
    [
     "Ville",
     "Turunen"
    ]
   ],
   "title": "To recover from speech recognition errors in spoken document retrieval",
   "original": "i05_0605",
   "page_count": 4,
   "order": 62,
   "p1": "605",
   "pn": "608",
   "abstract": [
    "An important difference between the retrieval of spoken and written documents is that the indexing of the speech data is usually based on automatic speech transcripts that contain recognition errors. However, there are several ways of reducing the effect of incorrect index terms in the retrieval. This paper presents retrieval experiments with unlimited vocabulary speech recognizer that utilizes a lexicon of unsupervised morpheme-like units. Based on this recognizer, three different methods are evaluated for error recovery. First, the recognized words are expanded by adding the recognized morphemes, too. Second, the words are expanded by adding the best rival morpheme candidates that were pruned away by the recognizer. Third, the queries are expanded by the potentially relevant terms found from text documents, which were retrieved from parallel text corpora by the original queries. The best results are obtained by that latter method which significantly improves the precision compared to the original queries and brings the spoken document retrieval precision to the same level as the corresponding text document retrieval.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-62"
  },
  "gonzalez05_interspeech": {
   "authors": [
    [
     "Edgar",
     "Gonzàlez"
    ],
    [
     "Jordi",
     "Turmo"
    ]
   ],
   "title": "Unsupervised clustering of spontaneous speech documents",
   "original": "i05_0609",
   "page_count": 4,
   "order": 63,
   "p1": "609",
   "pn": "612",
   "abstract": [
    "This paper presents an unsupervised method for clustering spontaneous speech documents. The approach uses a hierarchical algorithm to automatically determine the number of clusters and a starting model for a subsequent iterative algorithm. We have evaluated this method on the Switchboard corpus and compared it to a set of supervised and other unsupervised methods. The results show that our method significantly outperforms the rest of the approaches.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-63"
  },
  "yamaguchi05_interspeech": {
   "authors": [
    [
     "Masahide",
     "Yamaguchi"
    ],
    [
     "Masaru",
     "Yamashita"
    ],
    [
     "Shoichi",
     "Matsunaga"
    ]
   ],
   "title": "Spectral cross-correlation features for audio indexing of broadcast news and meetings",
   "original": "i05_0613",
   "page_count": 4,
   "order": 64,
   "p1": "613",
   "pn": "616",
   "abstract": [
    "This paper describes the effect of three new acoustic feature parameters to detect audio source segments that are based on spectral cross-correlation: spectral stability, white noise similarity, and sound spectral shape. These parameters are devised for accurate audio source detection and are used in a pre-processing module for automatic indexing of the broadcast news and the meetings. We conducted two audio source classification experiments: one with the broadcast news and the other with the meetings. The experiment with the broadcast news shows that proposed parameters make it possible to capture the audio sources more accurately than can be done with conventional parameters. Classification performance is increased by 6.6% when the proposed parameters are used. The spectral stability is proved to be the most effective among the conventional and the three proposed parameters. Regarding the experiments with the meeting corpus, we conducted speaker identification in addition to the audio source classification. First, the audio source classification procedure detects each sound source segment. Then, a speaker identification procedure finds cross-talk from other participants, and determines her/his own speech period. Speaker identification performance is increased by 2.7% when the proposed parameters are used.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-64"
  },
  "hori05_interspeech": {
   "authors": [
    [
     "Chiori",
     "Hori"
    ],
    [
     "Alex",
     "Waibel"
    ]
   ],
   "title": "Spontaneous speech consolidation for spoken language applications",
   "original": "i05_0617",
   "page_count": 4,
   "order": 65,
   "p1": "617",
   "pn": "620",
   "abstract": [
    "This paper describes the work done as a part of the International Workshop on Speech Summarization for Information Extraction and Machine Translation (IWSpS)1, on spoken language processing including summarization, machine translation and question answering on lecture speech in the Translanguage English Database (TED) corpus2. The hypotheses of lecture speech obtained by automatic speech recognition (ASR) system are ill-formed due to the spontaneity of speakers and recognition errors. The overall performance of spoken language processing components is affected by the errors introduced by the ASR system. In order to get more reliable phrases which maintain the original meaning and contribute positively to the total performance of the spoken language system, this paper proposes a consolidation framework. The consolidation approach extracts words by excluding redundant and irrelevant information and concatenating words so as to maintain the original meaning. Automatic consolidation performance is evaluated by comparing with manual consolidation by humans using a word accuracy metric. Our approach gives 58% accuracy on ASR output with 70% word accuracy.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-65"
  },
  "maskey05_interspeech": {
   "authors": [
    [
     "Sameer",
     "Maskey"
    ],
    [
     "Julia",
     "Hirschberg"
    ]
   ],
   "title": "Comparing lexical, acoustic/prosodic, structural and discourse features for speech summarization",
   "original": "i05_0621",
   "page_count": 4,
   "order": 66,
   "p1": "621",
   "pn": "624",
   "abstract": [
    "We present results of an empirical study of the usefulness of different types of features in selecting extractive summaries of news broadcasts for our Broadcast News Summarization System. We evaluate lexical, prosodic, structural and discourse features as predictors of those news segments which should be included in a summary. We show that a summarization system that uses a combination of these feature sets produces the most accurate summaries, and that a combination of acoustic/ prosodic and structural features are enough to build a good' summarizer when speech transcription is not available.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-66"
  },
  "li05_interspeech": {
   "authors": [
    [
     "Te-Hsuan",
     "Li"
    ],
    [
     "Ming-Han",
     "Lee"
    ],
    [
     "Berlin",
     "Chen"
    ],
    [
     "Lin-Shan",
     "Lee"
    ]
   ],
   "title": "Hierarchical topic organization and visual presentation of spoken documents using probabilistic latent semantic analysis (PLSA) for efficient retrieval/browsing applications",
   "original": "i05_0625",
   "page_count": 4,
   "order": 67,
   "p1": "625",
   "pn": "628",
   "abstract": [
    "The most attractive form of future network content will be multimedia including speech information, and such speech information usually carries the core concepts for the content. As a result, the spoken documents associated with the multi-media content very possibly can serve as the key for retrieval and browsing. This paper presents a new approach of hierarchical topic organization and visual presentation of spoken documents for such a purpose based on the Probabilistic Latent Semantic Analysis (PLSA). With this approach the spoken documents can be organized into a two-dimensional tree (or multi-layered map) of topic clusters, and the user can very efficiently retrieve or browse the network content or associated spoken documents. Different from the conventional document clustering approaches, with PLSA the relationships among the topic clusters and the appropriate terms as the topic labels can be very well derived. An initial prototype system with Chinese broadcast news as the example spoken documents including automatic generation of titles and summaries and retrieval/browsing functionalities is also presented. Choice of different units other than words to be used as the terms in the processing is also considered in the system based on the special structure of the Chinese language.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-67"
  },
  "zibert05_interspeech": {
   "authors": [
    [
     "Janez",
     "Zibert"
    ],
    [
     "France",
     "Mihelic"
    ],
    [
     "Jean-Pierre",
     "Martens"
    ],
    [
     "Hugo",
     "Meinedo"
    ],
    [
     "Joao",
     "Neto"
    ],
    [
     "Laura",
     "Docio"
    ],
    [
     "Carmen Garcia",
     "Mateo"
    ],
    [
     "Petr",
     "David"
    ],
    [
     "Jindrich",
     "Zdansky"
    ],
    [
     "Matus",
     "Pleva"
    ],
    [
     "Anton",
     "Cizmar"
    ],
    [
     "Andrej",
     "Zgank"
    ],
    [
     "Zdravko",
     "Kacic"
    ],
    [
     "Csaba",
     "Teleki"
    ],
    [
     "Klara",
     "Vicsi"
    ]
   ],
   "title": "The COST278 broadcast news segmentation and speaker clustering evaluation - overview, methodology, systems, results",
   "original": "i05_0629",
   "page_count": 4,
   "order": 68,
   "p1": "629",
   "pn": "632",
   "abstract": [
    "This paper describes a large scale experiment in which eight research institutions have tested their audio partitioning and labeling algorithms on the same data, a multi-lingual database of news broadcasts, using the same evaluation tools and protocols. The experiments have provide more insight in the cross-lingual robustness of the methods and they have demonstrated that by further collaborating in the domains of speaker change detection and speaker clustering it should be possible to achieve further technological progress in the near future.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-68"
  },
  "szoke05_interspeech": {
   "authors": [
    [
     "Igor",
     "Szoke"
    ],
    [
     "Petr",
     "Schwarz"
    ],
    [
     "Pavel",
     "Matejka"
    ],
    [
     "Lukas",
     "Burget"
    ],
    [
     "Martin",
     "Karafiat"
    ],
    [
     "Michal",
     "Fapso"
    ],
    [
     "Jan",
     "Cernocky"
    ]
   ],
   "title": "Comparison of keyword spotting approaches for informal continuous speech",
   "original": "i05_0633",
   "page_count": 4,
   "order": 69,
   "p1": "633",
   "pn": "636",
   "abstract": [
    "This paper describes several approaches to keyword spotting (KWS) for informal continuous speech. We compare acoustic keyword spotting, spotting in word lattices generated by large vocabulary continuous speech recognition and a hybrid approach making use of phoneme lattices generated by a phoneme recognizer. The systems are compared on carefully defined test data extracted from ICSI meeting database. The acoustic and phoneme-lattice based KWS are based on a phoneme recognizer making use of temporalpattern (TRAP) feature extraction and posterior estimation using neural nets. We show its superiority over traditional HMM/GMM systems. The advantages and drawbacks of different approaches are discussed.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-69"
  },
  "misu05_interspeech": {
   "authors": [
    [
     "Teruhisa",
     "Misu"
    ],
    [
     "Tatsuya",
     "Kawahara"
    ]
   ],
   "title": "Dialogue strategy to clarify user's queries for document retrieval system with speech interface",
   "original": "i05_0637",
   "page_count": 4,
   "order": 70,
   "p1": "637",
   "pn": "640",
   "abstract": [
    "This paper addresses a dialogue strategy to clarify and constrain the queries for document retrieval systems. In spoken dialogue interfaces, users often make utterances before the query is completely generated in their mind; thus input queries are often vague or fragmental. As a result, usually many items are matched. We propose an efficient dialogue framework, where the system dynamically selects an optimal question based on information gain (IG), which represents reduction of matched items. A set of possible questions is prepared using various knowledge sources. As a bottom-up knowledge source, we extract a list of words that can take a number of objects and potentially causes ambiguity, using a dependency structure analysis of the document texts. This is complemented by top-down knowledge sources of metadata and hand-crafted questions. An experimental evaluation showed that the method significantly improved the success rate of retrieval, and all categories of the prepared questions contributed to the improvement.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-70"
  },
  "moreau05_interspeech": {
   "authors": [
    [
     "Nicolas",
     "Moreau"
    ],
    [
     "Shan",
     "Jin"
    ],
    [
     "Thomas",
     "Sikora"
    ]
   ],
   "title": "Comparison of different phone-based spoken document retrieval methods with text and spoken queries",
   "original": "i05_0641",
   "page_count": 4,
   "order": 71,
   "p1": "641",
   "pn": "644",
   "abstract": [
    "This study compares four phone-based spoken document retrieval (SDR) approaches. In all cases, the indexing and retrieval system uses phonetic information only. The first retrieval method is based on the vector space model, using phone 3-grams as indexing terms. This approach is compared with 2 string-matching methods. A fourth method, combining the VSM approach with the slot detection step of string-matching techniques is proposed. This method is tested on a collection of short German spoken documents, using three different sets of queries: text queries, clean spoken queries and noisy spoken queries.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-71"
  },
  "black05_interspeech": {
   "authors": [
    [
     "Alan W.",
     "Black"
    ],
    [
     "Keiichi",
     "Tokuda"
    ]
   ],
   "title": "The blizzard challenge - 2005: evaluating corpus-based speech synthesis on common datasets",
   "original": "i05_0077",
   "page_count": 4,
   "order": 72,
   "p1": "77",
   "pn": "80",
   "abstract": [
    "In order to better understand different speech synthesis techniques on a common dataset, we devised a challenge that will help us better compare research techniques in building corpus-based speech synthesizers. In 2004, we released the first two 1200-utterance single-speaker databases from the CMU ARCTIC speech databases, and challenged current groups working in speech synthesis around the world to build their best voices from these databases. In January of 2005, we released two further databases and a set of 50 utterance texts from each of five genres and asked the participants to synthesize these utterances. Their resulting synthesized utterances were then presented to three groups of listeners: speech experts, volunteers, and US English-speaking undergraduates. This paper summarizes the purpose, design, and whole process of the challenge.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-72"
  },
  "sakai05_interspeech": {
   "authors": [
    [
     "Shinsuke",
     "Sakai"
    ],
    [
     "Han",
     "Shu"
    ]
   ],
   "title": "A probabilistic approach to unit selection for corpus-based speech synthesis",
   "original": "i05_0081",
   "page_count": 4,
   "order": 73,
   "p1": "81",
   "pn": "84",
   "abstract": [
    "In this paper, we present a novel statistical approach to corpusbased speech synthesis. Unit selection is directed by probabilistic models for F0 contour, duration, and spectral characteristics of the synthesis units. The F0 targets for units are modeled by statistical additive models, and duration targets are modeled by regression trees. Spectral targets for a unit is modeled by Gaussian mixtures on MFCC-based features. Goodness of concatenation of two units is modeled by conditional Gaussian models on MFCC-based features. Although the system is in its early stage of development, we implemented an English speech synthesizer with CMU Arctic corpora and confirmed the effectiveness of this new framework.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-73"
  },
  "kominek05_interspeech": {
   "authors": [
    [
     "John",
     "Kominek"
    ],
    [
     "Christina L.",
     "Bennett"
    ],
    [
     "Brian",
     "Langner"
    ],
    [
     "Arthur R.",
     "Toth"
    ]
   ],
   "title": "The blizzard challenge 2005 CMU entry - a method for improving speech synthesis systems",
   "original": "i05_0085",
   "page_count": 4,
   "order": 74,
   "p1": "85",
   "pn": "88",
   "abstract": [
    "In CMU's Blizzard Challenge 2005 entry we investigated twelve ideas for improving Festival-based unit selection voices. We tracked progress by adopting a 3-tiered strategy in which candidate ideas must pass through three stages of listening tests to warrant inclusion in the final build. This allowed us to evaluate ideas consistently without us having large human resources at our disposal, and thereby improve upon our baseline system within a short amount of time.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-74"
  },
  "bunnell05_interspeech": {
   "authors": [
    [
     "H. Timothy",
     "Bunnell"
    ],
    [
     "Chris",
     "Pennington"
    ],
    [
     "Debra",
     "Yarrington"
    ],
    [
     "John",
     "Gray"
    ]
   ],
   "title": "Automatic personal synthetic voice construction",
   "original": "i05_0089",
   "page_count": 4,
   "order": 75,
   "p1": "89",
   "pn": "92",
   "abstract": [
    "We describe techniques used for automatic personal synthetic voice creation in our laboratory. These techniques are implemented in two pieces of software. One, called InvTool, guides novice users in the process of recording a corpus of speech that is appropriate for creation of a concatenative synthetic voice. The other program, called BCC, compiles a speech corpus recorded with InvTool into a database appropriate for use with the ModelTalker TTS system. Our primary goal in this project is to develop software to support \"voice banking\" wherein individuals at risk to lose the ability to speak will be able to record their own personal synthetic voice for later use in voice output communication devices.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-75"
  },
  "zen05_interspeech": {
   "authors": [
    [
     "Heiga",
     "Zen"
    ],
    [
     "Tomoki",
     "Toda"
    ]
   ],
   "title": "An overview of nitech HMM-based speech synthesis system for blizzard challenge 2005",
   "original": "i05_0093",
   "page_count": 4,
   "order": 76,
   "p1": "93",
   "pn": "96",
   "abstract": [
    "In the present paper, hidden Markov model (HMM) based speech synthesis system developed in Nagoya Institute of Technology (Nitech-HTS) for a competition of text-to-speech synthesis systems using the same speech databases, named Blizzard Challenge 2005, is described. We show an overview of the basic HMM-based speech synthesis system and then recent developments to the latest one such as STRAIGHT-based vocoding, hidden semi-Markov model (HSMM) based acoustic modeling, and parameter generation considering global variance are illustrated. Constructed voices can synthesize speech around 0.3 xRT (real time ratio) and their footprints are less than 2 MB. The listening test results show that performances of our systems are much better than we expected.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-76"
  },
  "hamza05_interspeech": {
   "authors": [
    [
     "Wael",
     "Hamza"
    ],
    [
     "Raimo",
     "Bakis"
    ],
    [
     "Zhi Wei",
     "Shuang"
    ],
    [
     "Heiga",
     "Zen"
    ]
   ],
   "title": "On building a concatenative speech synthesis system from the blizzard challenge speech databases",
   "original": "i05_0097",
   "page_count": 4,
   "order": 77,
   "p1": "97",
   "pn": "100",
   "abstract": [
    "In this paper, we compare two methods of building a concatenative speech synthesis system from the relatively small, \"Blizzard Challenge\" speech databases. In the first method we build a system directly from the Blizzard databases using the IBM Concatenative Speech Synthesis System originally designed for very large speech databases. In the second method, a larger database is used to build the synthesis system and the output is \"morphed\" to match the speakers in the Blizzard databases. The second method outperformed the first while maintaining the identity of the Blizzard target speakers.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-77"
  },
  "clark05b_interspeech": {
   "authors": [
    [
     "Robert A. J.",
     "Clark"
    ],
    [
     "Korin",
     "Richmond"
    ],
    [
     "Simon",
     "King"
    ]
   ],
   "title": "Multisyn voices from ARCTIC data for the blizzard challenge",
   "original": "i05_0101",
   "page_count": 4,
   "order": 78,
   "p1": "101",
   "pn": "104",
   "abstract": [
    "This paper describes the process of building unit selection voices for the Festival multisyn engine using four ARCTIC datasets, as part of the Blizzard evaluation challenge. The build procedure is almost entirely automatic, with very little need for human intervention. We discuss the difference in the evaluation results for each voice and evaluate the suitability of the ARCTIC datasets for building this type of voice.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-78"
  },
  "bennett05_interspeech": {
   "authors": [
    [
     "Christina L.",
     "Bennett"
    ]
   ],
   "title": "Large scale evaluation of corpus-based synthesizers: results and lessons from the blizzard challenge 2005",
   "original": "i05_0105",
   "page_count": 4,
   "order": 79,
   "p1": "105",
   "pn": "108",
   "abstract": [
    "The Blizzard Challenge 2005 was a large scale international evaluation of various corpus-based speech synthesis systems using common datasets. Six sites from around the world, both academic and industrial, participated in this evaluation, the first ever to compare voices built by different systems using the same data. Here we describe results of the evaluation and many of the observations and lessons discovered in carrying it out.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-79"
  },
  "chen05_interspeech": {
   "authors": [
    [
     "Berlin",
     "Chen"
    ],
    [
     "Yi-Ting",
     "Chen"
    ],
    [
     "Chih-Hao",
     "Chang"
    ],
    [
     "Hung-Bin",
     "Chen"
    ]
   ],
   "title": "Speech retrieval of Mandarin broadcast news via mobile devices",
   "original": "i05_0109",
   "page_count": 4,
   "order": 80,
   "p1": "109",
   "pn": "112",
   "abstract": [
    "This paper presents a system for speech retrieval of Mandarin broadcast news. First, several data-driven and unsupervised approaches are integrated into the broadcast news transcription system to improve the speech recognition accuracy and efficiency. Then, a multi-scale indexing paradigm for broadcast news retrieval is proposed to make use of the special structural properties of the Chinese language as well as to alleviate the problems caused by the speech recognition errors. Finally, we use the PDA as the platform and Mandarin broadcast news stories collected in Taiwan as the document collection to establish a speech-based multimedia information retrieval prototype system. Very encouraging results are obtained.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-80"
  },
  "katoh05_interspeech": {
   "authors": [
    [
     "Michiaki",
     "Katoh"
    ],
    [
     "Kiyoshi",
     "Yamamoto"
    ],
    [
     "Jun",
     "Ogata"
    ],
    [
     "Takashi",
     "Yoshimura"
    ],
    [
     "Futoshi",
     "Asano"
    ],
    [
     "Hideki",
     "Asoh"
    ],
    [
     "Nobuhiko",
     "Kitawaki"
    ]
   ],
   "title": "State estimation of meetings by information fusion using Bayesian network",
   "original": "i05_0113",
   "page_count": 4,
   "order": 81,
   "p1": "113",
   "pn": "116",
   "abstract": [
    "In this paper, a method of structuring the multi-media recording of a small-sized meeting based on various information such as sound source localization, multiple-talk detection, and the detection of non-speech sound events, is proposed. The information from these detectors is fused by a Bayesian network to estimate the state of the meeting. Based on the estimated state, the recording of the meeting is structured using a XML-based description language and is visualized by a browser.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-81"
  },
  "moore05_interspeech": {
   "authors": [
    [
     "Roger K.",
     "Moore"
    ]
   ],
   "title": "Results from a survey of attendees at ASRU 1997 and 2003",
   "original": "i05_0117",
   "page_count": 4,
   "order": 82,
   "p1": "117",
   "pn": "120",
   "abstract": [
    "In 1997 the author conducted a survey at the IEEE workshop on Automatic Speech Recognition and Understanding' (ASRU) in which attendees were offered a set of twelve putative future events to which they were asked to assign a date. Six years later at ASRU'2003, the author repeated the survey with the addition of eight additional items. This paper presents the combined results from both surveys.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-82"
  },
  "haebumbach05_interspeech": {
   "authors": [
    [
     "Reinhold",
     "Haeb-Umbach"
    ],
    [
     "Basilis",
     "Kladis"
    ],
    [
     "Joerg",
     "Schmalenstroeer"
    ]
   ],
   "title": "Speech processing in the networked home environment - a view on the amigo project",
   "original": "i05_0121",
   "page_count": 4,
   "order": 83,
   "p1": "121",
   "pn": "124",
   "abstract": [
    "Full interoperability of networked devices in the home has been kind of an elusive concept for quite some years. Amigo, an Integrated Project within the EU 6-th framework program, tries to make home networking a reality by addressing two key issues: First, it brings together many major players in the domestic appliances, communications, consumer electronics and computer industry to develop a common open source middleware platform. Second, emphasis is placed on the development of intelligent user services that make the benefit of a networked home environment tangible for the end user. This paper shows how speech processing can contribute to this second goal of user-friendly, personalized, context-aware services.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-83"
  },
  "sugiyama05_interspeech": {
   "authors": [
    [
     "Masahide",
     "Sugiyama"
    ]
   ],
   "title": "Fixed distortion segmentation in efficient sound segment searching",
   "original": "i05_0125",
   "page_count": 4,
   "order": 84,
   "p1": "125",
   "pn": "128",
   "abstract": [
    "Searching query signal from stored signal is formulated as a segment searching problem where signal is converted into a sequence of feature vectors. As an efficient segment searching algorithm, a new pruning method of candidates in the segment sequence has been proposed and the effectiveness has been shown through experimental results. The proposed searching algorithm is 20 - 30 times faster than the conventional Active Searching algorithm. As the first step of the proposed method distortion based segmentation is carried out. As searching criterion is based on l1 norm, the segmentation is expected to be carried out using l1 criterion. This paper compares two segmentation methods; maximum l1 distortion segmentation and average l2 distortion segmentation. The average l2 distortion segmentation is very efficient. On the other hand, the maximum l1 distortion segmentation does not require radius information. The experimental results show that two methods have almost equal performance in segment searching when the number of segments are same.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-84"
  },
  "nwe05_interspeech": {
   "authors": [
    [
     "Tin Lay",
     "Nwe"
    ],
    [
     "Haizhou",
     "Li"
    ]
   ],
   "title": "Identifying singers of popular songs",
   "original": "i05_0129",
   "page_count": 4,
   "order": 85,
   "p1": "129",
   "pn": "132",
   "abstract": [
    "In this paper, we propose to identify the singers of popular songs using vibrato characteristics and high level musical knowledge of song structure. The proposed framework starts with a vocal detection process followed by a hypothesis test for the vocal/nonvocal verification. This method allows us to select vocal segments of high confidence for singer identification. From the selected vocal segments, the cepstral coefficients which reflect the vibrato characteristics are computed using the parabola bandpass filters spread according to the music frequency scale. The strategy in our classifier formulation is to utilize the high level musical knowledge of song structure in singer modeling. The proposed framework is validated on a database containing 84 popular songs of commercially available CD records from 12 singers. We achieve an average error rate of 17.9% in segment level identification.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-85"
  },
  "ogata05_interspeech": {
   "authors": [
    [
     "Jun",
     "Ogata"
    ],
    [
     "Masataka",
     "Goto"
    ]
   ],
   "title": "Speech repair: quick error correction just by using selection operation for speech input interfaces",
   "original": "i05_0133",
   "page_count": 4,
   "order": 86,
   "p1": "133",
   "pn": "136",
   "abstract": [
    "In this paper, we propose a novel speech input interface function, called \"Speech Repair\" in which recognition errors can be easily corrected by selecting candidates. During the speech input, this function displays not only the typical speech-recognition result but also other competitive candidates. Each word in the result is separated by line segments and accompanied by other word candidates. A user who finds a recognition error can simply select the correct word from the candidates for that temporal region. In order to overcome the difficulty of generating appropriate candidates, we adopted a confusion network that can condense a huge internal word graph of a large vocabulary continuous speech recognition (LVCSR) system. In our experiments, almost all recognition errors were corrected and the effectiveness of speech repair was confirmed.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-86"
  },
  "olszewski05_interspeech": {
   "authors": [
    [
     "Dirk",
     "Olszewski"
    ],
    [
     "Fransiskus",
     "Prasetyo"
    ],
    [
     "Klaus",
     "Linhard"
    ]
   ],
   "title": "Steerable highly directional audio beam loudspeaker",
   "original": "i05_0137",
   "page_count": 4,
   "order": 87,
   "p1": "137",
   "pn": "140",
   "abstract": [
    "This paper presents a method of steering audible sound beams generated by parametric arrays in air. A hybrid system consisting of combined electronic / mechanical phased array technique is used. Although commercially available emitter technology has been used and therefore several guidelines known from prior-art phased arrays could not be realized, sufficient beam steering performance has been reached.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-87"
  },
  "ezzaidi05_interspeech": {
   "authors": [
    [
     "Hassan",
     "Ezzaidi"
    ],
    [
     "Jean",
     "Rouat"
    ]
   ],
   "title": "Automatic music genre classification using second-order statistical measures for the prescriptive approach",
   "original": "i05_0141",
   "page_count": 4,
   "order": 88,
   "p1": "141",
   "pn": "144",
   "abstract": [
    "Several works proposed for the automatic genre musical classification are based on various combinations of parameters, exploiting different models. However, the comparison of all previous works remain impossible since they used different target taxonomies, genre definitions and databases. In this paper, the world largest music database (Real World Computing) is used. Also, different measures related to second-order statistics methods are investigated to achieve the genre classification. Various strategies are proposed for training and testing sessions such as matched conditions, mismatched conditions, long training/testing, long training and short testing. For all experiments, the section of file used in testing has never been presented during the training session. The best classifier achieved 97% and 69% performance when matched and mismatched conditions are used, respectively.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-88"
  },
  "abad05_interspeech": {
   "authors": [
    [
     "Alberto",
     "Abad"
    ],
    [
     "Dusan",
     "Macho"
    ],
    [
     "Carlos",
     "Segura"
    ],
    [
     "Javier",
     "Hernando"
    ],
    [
     "Climent",
     "Nadeu"
    ]
   ],
   "title": "Effect of head orientation on the speaker localization performance in smart-room environment",
   "original": "i05_0145",
   "page_count": 4,
   "order": 89,
   "p1": "145",
   "pn": "148",
   "abstract": [
    "Reliable measures of speaker positions are needed for computational perception of human activities taking place in a smart-room environment. In this work, we investigate the effect of talkers head orientation on the accuracy of acoustical source localization techniques and its relation with the talker directivity pattern and room reverberation. Two different representative speaker localization techniques are assessed, steered response power and a crossing lines based method, in both cases on the basis of the estimated delays between pairs of microphones with the GCC-PHAT algorithm. A small database has been collected at the UPC's smart room for evaluation. The results show how the localization error heavily depends on the head orientation, and also the fact that the space exploration based technique is much more robust to head orientation changes than the crossing lines technique, due to the way the contributions from the various microphones are combined.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-89"
  },
  "fredouille05_interspeech": {
   "authors": [
    [
     "Corinne",
     "Fredouille"
    ],
    [
     "G.",
     "Pouchoulin"
    ],
    [
     "Jean-François",
     "Bonastre"
    ],
    [
     "M.",
     "Azzarello"
    ],
    [
     "A.",
     "Giovanni"
    ],
    [
     "A.",
     "Ghio"
    ]
   ],
   "title": "Application of automatic speaker recognition techniques to pathological voice assessment (dysphonia)",
   "original": "i05_0149",
   "page_count": 4,
   "order": 90,
   "p1": "149",
   "pn": "152",
   "abstract": [
    "This paper investigates the adaptation of Automatic Speaker Recognition (ASR) techniques to the pathological voice assessment (dysphonic voices). The aim of this study is to provide a novel method, suitable for keeping track of the evolution of the patient's pathology: easy-to-use, fast, non-invasive for the patient, and affordable for the clinicians. This method will be complementary to the existing ones - the perceptual judgment and the usual objective measurement (jitter, airflows...) which remain time and human resource consuming.\n",
    "The system designed for this particular task relies on the GMMbased approach, which is the state-of-the-art for speaker recognition. It is derived from the open source ASR tools (LIA_SpkDet and ALIZE) of the LIA lab.\n",
    "Experiments conducted on a dysphonic corpus provide promising results, underlining the interest of such an approach and opening further research investigation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-90"
  },
  "chaudhari05_interspeech": {
   "authors": [
    [
     "Upendra V.",
     "Chaudhari"
    ],
    [
     "Ganesh N.",
     "Ramaswamy"
    ],
    [
     "Eddie",
     "Epstein"
    ],
    [
     "Sasha P.",
     "Caskey"
    ],
    [
     "Mohamed Kamal",
     "Omar"
    ]
   ],
   "title": "Adaptive speech analytics: system, infrastructure, and behavior",
   "original": "i05_0153",
   "page_count": 4,
   "order": 91,
   "p1": "153",
   "pn": "156",
   "abstract": [
    "This paper describes an adaptive system and infrastructure for Speech Analytics, based on the UIMA framework and consisting of a set of analysis engines (analytics) and control units, whose input is an unspecified and ever changing number of continuous streams of audio data and whose output is the detection of events consistent with a focus of analysis and/or the discovery of relationships among the outputs of the constituent analytics in the system. The central theme presented concerns the ability of the system to use the meta-data generated during the analysis to adapt both the behavior of the underlying analytics engines and the overall data flow to adjust the granularity and accuracy of the analysis in order to allow processing of increasing amounts of data with limited resources.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-91"
  },
  "forbesriley05_interspeech": {
   "authors": [
    [
     "Katherine",
     "Forbes-Riley"
    ],
    [
     "Diane J.",
     "Litman"
    ]
   ],
   "title": "Correlating student acoustic-prosodic profiles with student learning in spoken tutoring dialogues",
   "original": "i05_0157",
   "page_count": 4,
   "order": 92,
   "p1": "157",
   "pn": "160",
   "abstract": [
    "We examine correlations between student learning and student acoustic-prosodic profiles, which prior research has shown to be predictive of emotional states. We compare these correlations in two corpora of spoken tutoring dialogues: a human-human corpus and a human-computer corpus. Our results suggest that rather than relying on emotion prediction models developed via the more labor-intensive method of manually labeling emotions, adaptive strategies for our spoken dialogue tutoring system can be developed based on observed acoustic-prosodic profiles that we hypothesize to be reflective of emotion.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-92"
  },
  "litman05_interspeech": {
   "authors": [
    [
     "Diane J.",
     "Litman"
    ],
    [
     "Katherine",
     "Forbes-Riley"
    ]
   ],
   "title": "Speech recognition performance and learning in spoken dialogue tutoring",
   "original": "i05_0161",
   "page_count": 4,
   "order": 93,
   "p1": "161",
   "pn": "164",
   "abstract": [
    "Speech recognition errors have been shown to negatively correlate with user satisfaction in evaluations of task-oriented spoken dialogue systems. In the domain of tutorial dialogue systems, however, where the primary evaluation metric is student learning, there has been little investigation of whether speech recognition errors also negatively correlate with learning. In this paper we examine correlations between student learning and automatic speech recognition performance, in a corpus of dialogues collected with an intelligent tutoring spoken dialogue system. We examine numerous quantitative measures of speech recognition error, including rejection versus misrecognition errors, word versus sentence-level errors, and transcription versus semantic errors. Our results show that although many of our students experience problems with speech recognition, none of our measures negatively correlates with student learning.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-93"
  },
  "asakawa05_interspeech": {
   "authors": [
    [
     "Satoshi",
     "Asakawa"
    ],
    [
     "Nobuaki",
     "Minematsu"
    ],
    [
     "Toshiko",
     "Isei-Jaakkola"
    ],
    [
     "Keikichi",
     "Hirose"
    ]
   ],
   "title": "Structural representation of the non-native pronunciations",
   "original": "i05_0165",
   "page_count": 4,
   "order": 94,
   "p1": "165",
   "pn": "168",
   "abstract": [
    "Acoustic representation of speech provided by phonetics, spectrogram, is noisy representation in that it shows every acoustic aspect of speech. Age, gender, shape, microphone, room, line, etc. are completely irrelevant to the pronunciation assessment. However, the spectrogram is affected inevitably by these factors. Recently, a novel acoustic representation of speech was proposed, where dimensions of these non-linguistic factors can hardly be seen[1, 2]. Every acoustic substance of speech is discarded and only their interrelations are extracted to represent the pronunciation structurally. Using this method, individual learners were described as distorted phonemic structures[3] and automatic scoring of the pronunciation was investigated[3, 4]. This paper describes two new analyses using the proposed method. The first analysis is done to examine whether the method can trace the development of a student's pronunciation appropriately using only a limited amount of speech. The second one focuses on the prosodic aspect of the pronunciation, especially stressed and unstressed vowels. The former indicates that the proposed method can show history of the student's development adequately and the latter clarifies that size of the pronunciation structure is highly correlated with the pronunciation proficiency.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-94"
  },
  "chou05_interspeech": {
   "authors": [
    [
     "Fu-chiang",
     "Chou"
    ]
   ],
   "title": "Ya-ya language box - a portable device for English pronunciation training with speech recognition technologies",
   "original": "i05_0169",
   "page_count": 4,
   "order": 95,
   "p1": "169",
   "pn": "172",
   "abstract": [
    "This paper describes the application of speech recognition technologies in computer assisted pronunciation training. A portable pronunciation training device called Ya-Ya language box was completed in this research. The system provides a pronunciation training course and uses the speech recognition technologies to identify the pronunciation errors and provide diagnostic feedbacks for the users to correct their pronunciations. The system had been evaluated by forty students in elementary schools and most of them affirmed the system is helpful for their English pronunciation learning.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-95"
  },
  "ito05_interspeech": {
   "authors": [
    [
     "Akinori",
     "Ito"
    ],
    [
     "Yen-Ling",
     "Lim"
    ],
    [
     "Motoyuki",
     "Suzuki"
    ],
    [
     "Shozo",
     "Makino"
    ]
   ],
   "title": "Pronunciation error detection method based on error rule clustering using a decision tree",
   "original": "i05_0173",
   "page_count": 4,
   "order": 96,
   "p1": "173",
   "pn": "176",
   "abstract": [
    "We are developing a CALL system to train English pronunciation for Japanese native speakers. However, the precision of the error detection was not very high because the threshold for the detection was not optimum. To improve the detection accuracy, we propose a new method to optimize the thresholds of error detection. The proposed method makes several clusters of the pronunciation error rules, and the thresholds are determined for each cluster. An experiment was carried out to investigate the performance of the proposed method. As a result, about 90% of detection rate was obtained, which is a remarkable improvement from the conventional method.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-96"
  },
  "sethy05b_interspeech": {
   "authors": [
    [
     "Abhinav",
     "Sethy"
    ],
    [
     "Shrikanth",
     "Narayanan"
    ],
    [
     "Nicolaus",
     "Mote"
    ],
    [
     "W. Lewis",
     "Johnson"
    ]
   ],
   "title": "Modeling and automating detection of errors in Arabic language learner speech",
   "original": "i05_0177",
   "page_count": 4,
   "order": 97,
   "p1": "177",
   "pn": "180",
   "abstract": [
    "Human tutors, in dealing with non-native speakers, draw from their knowledge of common learner mistakes to understand learner speech and offer effective corrective advice. In this paper we present our work towards embedding some of this knowledge in the speech recognition and learner speech error detection subsystems of the Tactical Language Training System (TLTS). We discuss the implementation and effectiveness of our methodology which uses a combination of rule based and probabilistic models derived from linguistic knowledge about the target language and annotated speech to identify potential learner errors, detect them using ASR and to provide the user with corrective feedback based on error severity and factors such as the learner history. Evaluation results show that our system can provide effective feedback to the learner with good accuracy.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-97"
  },
  "zhang05_interspeech": {
   "authors": [
    [
     "Felicia",
     "Zhang"
    ],
    [
     "Michael",
     "Wagner"
    ]
   ],
   "title": "Effects of F0 feedback on the learning of Chinese tones by native speakers of English",
   "original": "i05_0181",
   "page_count": 4,
   "order": 98,
   "p1": "181",
   "pn": "184",
   "abstract": [
    "This paper reports on the effects of an interactive software tool on the learning of Chinese tones by English-L1 learners of Chinese. The software enables students to listen to model utterances by native speakers of Chinese and to record their own renditions of those utterances. Simultaneously the F0 contours of both the model utterances and the student utterances are displayed on the computer screen. The effect of the interactive software on the L2 tone production performance by a group of students is compared with that of a control group. The students who could make use of the software show a better approximation of native speakers in F0 mean and range.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-98"
  },
  "brndsted05_interspeech": {
   "authors": [
    [
     "Tom",
     "Brøndsted"
    ],
    [
     "Erik",
     "Aaskoven"
    ]
   ],
   "title": "Voice-controlled internet browsing for motor-handicapped users. design and implementation issues",
   "original": "i05_0185",
   "page_count": 4,
   "order": 99,
   "p1": "185",
   "pn": "188",
   "abstract": [
    "The public-funded project \"Indtal\" (\"Speak-it\") has succeeded in developing a Danish voice-controlled utility for internet browsing targeting motor-handicapped users having difficulties using a standard keyboard and/or a standard mouse. The system has been designed and implemented in collaboration with an advisory board of motor-handicapped (potential) end-users and underlies a number of a priori defined design criteria: learnability and memorability rather than naturalness, minimal need for maintenance after release, support for \"all\" web standards (not just HTML conforming to certain \"recommendations\"), independency of the language on the websites being browsed, etc. These criteria have lead to a primarily message-driven system interacting with an existing browser on the end users' systems.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-99"
  },
  "williams05_interspeech": {
   "authors": [
    [
     "Briony",
     "Williams"
    ],
    [
     "Delyth",
     "Prys"
    ],
    [
     "Ailbhe",
     "Ní Chasaide"
    ]
   ],
   "title": "Creating an ongoing research capability in speech technology for two minority languages: experiences from the WISPR project",
   "original": "i05_0189",
   "page_count": 4,
   "order": 100,
   "p1": "189",
   "pn": "192",
   "abstract": [
    "This paper reports on efforts to set up a research capability in speech technology for two minority languages (Welsh and Irish), where the focus is on ensuring that this capability will outlive the project that provided the initial impetus (the WISPR project). The existing situation at the start of the project is summarized, together with the challenges (setting up plant and equipment from scratch, training researchers in specialised skills). Some innovative strategies are examined: remote working by researchers with regular intensive on-site work weeks, and the use of Internet technology as a medium for collaborative work patterns. It is hoped that these experiences may be useful to researchers in other minority languages who wish to set up a similar research capability.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-100"
  },
  "vovos05_interspeech": {
   "authors": [
    [
     "A.",
     "Vovos"
    ],
    [
     "Basilis",
     "Kladis"
    ],
    [
     "Nikolaos",
     "Fakotakis"
    ]
   ],
   "title": "Speech operated smart-home control system for users with special needs",
   "original": "i05_0193",
   "page_count": 4,
   "order": 101,
   "p1": "193",
   "pn": "196",
   "abstract": [
    "This paper presents a speech operated smart-home control system with a focus on users with special needs - elderly people, people suffering from blindness or low vision and people with mobility impairments. The proposed system simplifies the use of various home appliances by providing a unified speech-controlled interface and thus improves the quality of their everyday life. The paper presents the implementation scheme and decisions taken in order to have a real-time operational system as well as real users' experimental results.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-101"
  },
  "jitsuhiro05_interspeech": {
   "authors": [
    [
     "Takatoshi",
     "Jitsuhiro"
    ],
    [
     "Shigeki",
     "Matsuda"
    ],
    [
     "Yutaka",
     "Ashikari"
    ],
    [
     "Satoshi",
     "Nakamura"
    ],
    [
     "Ikuko Eguchi",
     "Yairi"
    ],
    [
     "Seiji",
     "Igi"
    ]
   ],
   "title": "Spoken dialog system and its evaluation of geographic information system for elderly persons' mobility support",
   "original": "i05_0197",
   "page_count": 4,
   "order": 102,
   "p1": "197",
   "pn": "200",
   "abstract": [
    "We propose a speech input interface for the Mobility Support Geographic Information System (GIS). This GIS can provide optimal paths to destinations with information on barrier and barrier-free objects related to sidewalks for all pedestrians, especially elderly people and disabled people. We have created a speech input interface for the GIS, and evaluated it for elderly people. Results indicated that speech input is much easier than mouse input for them, especially for half of them who have difficulty in using PCs. We also investigated elderly peoples' behavior toward a dialog system by using Wizard of Oz. In this experiment, we applied several dialog styles, user-initiated, system-initiated, with/without the application. Experiment results show that their behavior is dependent on their own experience with computers.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-102"
  },
  "falavigna05_interspeech": {
   "authors": [
    [
     "Daniele",
     "Falavigna"
    ],
    [
     "Toni",
     "Giorgino"
    ],
    [
     "Roberto",
     "Gretter"
    ]
   ],
   "title": "A frame based spoken dialog system for home care",
   "original": "i05_0201",
   "page_count": 4,
   "order": 103,
   "p1": "201",
   "pn": "204",
   "abstract": [
    "This paper deals with the evaluation of a dialog system, developed in order to monitor the health status of hypertensive patients, taking into account their needs, preferences and the time course of her/his disease. Statistics and dialog refinements are described with reference to data acquired, from patients, actually using the system, over a controlled clinical trial that lasted 12 months and collected 541 dialogs.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-103"
  },
  "hawley05_interspeech": {
   "authors": [
    [
     "Mark S.",
     "Hawley"
    ],
    [
     "Phil",
     "Green"
    ],
    [
     "Pam",
     "Enderby"
    ],
    [
     "Stuart",
     "Cunningham"
    ],
    [
     "Roger K.",
     "Moore"
    ]
   ],
   "title": "Speech technology for e-inclusion of people with physical disabilities and disordered speech",
   "original": "i05_0445",
   "page_count": 4,
   "order": 104,
   "p1": "445",
   "pn": "448",
   "abstract": [
    "Speech technology is potentially of enormous benefit to people with physical disabilities. Applications of speech technology to e-inclusion are reviewed and described in the areas of access, control, communication and rehabilitation/therapy, with particular reference to speech technology developments for people with disordered speech. To be successful, applications should effectively take into account the needs of user groups and have the ability to adapt to the needs of individuals. This is a challenging area but effective progress can be made through multi-disciplinary research and development.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-104"
  },
  "granstrom05_interspeech": {
   "authors": [
    [
     "Björn",
     "Granström"
    ]
   ],
   "title": "Speech technology for language training and e-inclusion",
   "original": "i05_0449",
   "page_count": 4,
   "order": 105,
   "p1": "449",
   "pn": "452",
   "abstract": [
    "Efficient language learning is one of the keys to social inclusion. In this paper we present some work aiming at creating a virtual language tutor. The ambition is to create a tutor that can be engaged in many aspects of language learning from detailed pronunciation training to conversational practice. Some of the crucial components of such a system are described. An initial implementation of a stress/quantity training tutor for Swedish will be presented.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-105"
  },
  "tucker05_interspeech": {
   "authors": [
    [
     "Roger",
     "Tucker"
    ],
    [
     "Ksenia",
     "Shalonova"
    ]
   ],
   "title": "Supporting the creation of TTS for local language voice information systems",
   "original": "i05_0453",
   "page_count": 4,
   "order": 106,
   "p1": "453",
   "pn": "456",
   "abstract": [
    "We report on the Local Language Speech Technology Initiative, which is producing the TTS required for voice information systems in the developing world. We overview the whole process now the initial phases of Hindi, isiZulu, Kiswahili and Ibibio are complete, outline some applications we are targeting, and draw some lessons for the future.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-106"
  },
  "andersen05_interspeech": {
   "authors": [
    [
     "Ove",
     "Andersen"
    ],
    [
     "Christian",
     "Hjulmand"
    ]
   ],
   "title": "Access for all - a talking internet service",
   "original": "i05_0457",
   "page_count": 4,
   "order": 107,
   "p1": "457",
   "pn": "460",
   "abstract": [
    "Approximately 30% of the Danish population has severe problems in reading everyday text. In the light of the increasing amount of text available on the Internet this poses a democratic challenge to ensure \"equal access\" to information. The Talking Internet service - Access For All (AFA) - offers a free Internet-based tool for reading aloud any marked text with a synthetic voice. The only requirements are a standard equipped PC running a recent Windows OS and an Internet connection.\n",
    "Experiences gathered from running the service for more than 28 months underline the viability of the concept. There is a clear need for a free internet based Danish text-to-speech synthesizer. Furthermore, the current state of technology i.e. internet bandwidth, response time and server technology is sufficient for setting up an online automatic reading service that is used by steadily growing number of individuals and institutions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-107"
  },
  "kvale05_interspeech": {
   "authors": [
    [
     "Knut",
     "Kvale"
    ],
    [
     "Narada",
     "Warakagoda"
    ]
   ],
   "title": "A speech centric mobile multimodal service useful for dyslectics and aphasics",
   "original": "i05_0461",
   "page_count": 4,
   "order": 108,
   "p1": "461",
   "pn": "464",
   "abstract": [
    "Multimodality has the potential of benefiting non-disabled as well as disabled people. We have developed a speech-centric composite multimodal interface to a map-based information service on a mobile terminal. This interface to the service has proven useful for a severe dyslectic and an aphasic. These persons did not manage to use the ordinary public information service, neither on the web (text only) nor by calling a manual operator phone (speech only). But they fairly easily employed our multimodal interface by pointing at the map on the touch screen while uttering short commands or phrases. Although this is a limited qualitative evaluation it indicates that multimodal interfaces to information services is a step in the right direction for achieving the goal of inclusive design or design for all (DfA).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-108"
  },
  "wolfel05_interspeech": {
   "authors": [
    [
     "Matthias",
     "Wölfel"
    ]
   ],
   "title": "Frame based model order selection of spectral envelopes",
   "original": "i05_0205",
   "page_count": 4,
   "order": 109,
   "p1": "205",
   "pn": "208",
   "abstract": [
    "Spectral envelopes, using (warped or perceptual) linear prediction or minimum variance distortionless response for the underlying linear parametric model, are widely used in speech recognition systems where the frequency resolution, namely the model order (MO), of the spectrum is kept constant. Modeling different types of phonemes such as vowels or fricatives with the same frequency resolution might not lead to the best possible performance. This could be due to the fact that important parts of various phonemes lie in different frequency regions, that the fundamental frequency varies for different speakers or because of a high variance in the signal to noise ratio. To address this problem we propose to vary the MO frame by frame according to a control factor. In our case, the control factor could be either a relation of autocorrelation coefficients or the spectral entropy. Experimental results on the Translanguage English Database show an improvement by 2.4% relative in word error rate compared to the fixed MO and 4.2% relative to the traditional Mel-frequency cepstral coefficients.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-109"
  },
  "tyagi05_interspeech": {
   "authors": [
    [
     "Vivek",
     "Tyagi"
    ],
    [
     "Christian",
     "Wellekens"
    ],
    [
     "Hervé",
     "Bourlard"
    ]
   ],
   "title": "On variable-scale piecewise stationary spectral analysis of speech signals for ASR",
   "original": "i05_0209",
   "page_count": 4,
   "order": 110,
   "p1": "209",
   "pn": "212",
   "abstract": [
    "A fixed scale (typically 25ms) short time spectral analysis of speech signals, which are inherently multi-scale in nature (typically vowels last for 40-80ms while stops last for 10-20ms), is clearly sub-optimal for time-frequency resolution. Based on the usual assumption that the speech signal can be modeled by a time-varying autoregressive (AR) Gaussian process, we estimate the largest piecewise quasi-stationary speech segments, based on the likelihood that a segment was generated by the same AR process. This likelihood is estimated from the Linear Prediction (LP) residual error. Each of these quasi-stationary segments is then used as an analysis window from which spectral features are extracted. Such an approach thus results in a variable scale time spectral analysis, adaptively estimating the largest possible analysis window size such that the signal remains quasi-stationary, thus the best temporal/frequency resolution tradeoff. The speech recognition experiments on the OGI Numbers95 database, show that the proposed variable-scale piecewise stationary spectral analysis based features indeed yield improved recognition accuracy in clean conditions, compared to features based on minimum cross entropy spectrum [1] as well as those based on fixed scale spectral analysis.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-110"
  },
  "faria05_interspeech": {
   "authors": [
    [
     "Arlo",
     "Faria"
    ],
    [
     "David",
     "Gelbart"
    ]
   ],
   "title": "Efficient pitch-based estimation of VTLN warp factors",
   "original": "i05_0213",
   "page_count": 4,
   "order": 111,
   "p1": "213",
   "pn": "216",
   "abstract": [
    "To reduce inter-speaker variability, vocal tract length normalization (VTLN) is commonly used to transform acoustic features for automatic speech recognition (ASR). The warp factors used in this process are usually derived by maximum likelihood (ML) estimation, involving an exhaustive search over possible values. We describe an alternative approach: exploit the correlation between a speaker's average pitch and vocal tract length, and model the probability distribution of warp factors conditioned on pitch observations. This can be used directly for warp factor estimation, or as a smoothing prior in combination with ML estimates. Pitch-based warp factor estimation for VTLN is effective and requires relatively little memory and computation. Such an approach is well-suited for environments with constrained resources, or where pitch is already being computed for other purposes.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-111"
  },
  "zheng05_interspeech": {
   "authors": [
    [
     "Yanli",
     "Zheng"
    ],
    [
     "Richard",
     "Sproat"
    ],
    [
     "Liang",
     "Gu"
    ],
    [
     "Izhak",
     "Shafran"
    ],
    [
     "Haolang",
     "Zhou"
    ],
    [
     "Yi",
     "Su"
    ],
    [
     "Daniel",
     "Jurafsky"
    ],
    [
     "Rebecca",
     "Starr"
    ],
    [
     "Su-Youn",
     "Yoon"
    ]
   ],
   "title": "Accent detection and speech recognition for Shanghai-accented Mandarin",
   "original": "i05_0217",
   "page_count": 4,
   "order": 112,
   "p1": "217",
   "pn": "220",
   "abstract": [
    "As speech recognition systems are used in ever more applications, it is crucial for the systems to be able to deal with accented speakers. Various techniques, such as acoustic model adaptation and pronunciation adaptation, have been reported to improve the recognition of non-native or accented speech. In this paper, we propose a new approach that combines accent detection, accent discriminative acoustic features, acoustic adaptation and model selection for accented Chinese speech recognition. Experimental results show that this approach can improve the recognition of accented speech.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-112"
  },
  "barrault05_interspeech": {
   "authors": [
    [
     "Loic",
     "Barrault"
    ],
    [
     "Renato de",
     "Mori"
    ],
    [
     "Roberto",
     "Gemello"
    ],
    [
     "Franco",
     "Mana"
    ],
    [
     "Driss",
     "Matrouf"
    ]
   ],
   "title": "Variability of automatic speech recognition systems using different features",
   "original": "i05_0221",
   "page_count": 4,
   "order": 113,
   "p1": "221",
   "pn": "224",
   "abstract": [
    "The paper describes the use of two recognizers fed by different acoustic features. The first recognizer performs Multiple Resolution Analysis (MRA) while the other recognizer computes JRASTA Perceptual Linear Prediction Coefficients (JRASTAPLP). The two recognizers use the same denoising method but perform different partitions of their acoustic spaces. Experiments with the Italian and Spanish components of the AURORA3 corpus show that the two systems provide, in a significant proportion of cases, substantially different posterior probabilities for the same phoneme in the same time interval. A decision rule is proposed when two different words are hypothesized by the two recognizers. It is based on the probability that a hypothesis is correct, given the identity of the word hypotheses that are in competition. Significant word error rate (WER) reductions have been found for the CH1 proportion of the Italian and Spanish components of the AURORA3 corpus.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-113"
  },
  "lihan05_interspeech": {
   "authors": [
    [
     "Slavomir",
     "Lihan"
    ],
    [
     "Jozef",
     "Juhar"
    ],
    [
     "Anton",
     "Cizmar"
    ]
   ],
   "title": "Crosslingual and bilingual speech recognition with Slovak and Czech speechdat-e databases",
   "original": "i05_0225",
   "page_count": 4,
   "order": 114,
   "p1": "225",
   "pn": "228",
   "abstract": [
    "This paper presents the work on crosslingual and bilingual speech recognition carried out with SpeechDat databases for Czech and Slovak language. The work follows the MASPER initiative that was formed as a part of the COST 278 Action. In crosslingual experiments the expert-driven and the data-driven approaches were used for transferring monolingual source acoustic models to a target language. The results' analysis showed that the crosslingual Czech/Slovak speech recognition performance outperforms the results got in MASPER initiative for other target languages and that similarities between source and target language have a significant influence on the performance. Consecutively a bilingual Czech/Slovak recognition experiment with linked SpeechDat-CZ/SK was performed. The positive results indicate possibility to share Czech and Slovak speech databases for training bilingual and monolingual acoustic models.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-114"
  },
  "pelaezmoreno05_interspeech": {
   "authors": [
    [
     "Carmen",
     "Pelaez-Moreno"
    ],
    [
     "Qifeng",
     "Zhu"
    ],
    [
     "Barry Y.",
     "Chen"
    ],
    [
     "Nelson",
     "Morgan"
    ]
   ],
   "title": "Automatic data selection for MLP-based feature extraction for ASR",
   "original": "i05_0229",
   "page_count": 4,
   "order": 115,
   "p1": "229",
   "pn": "232",
   "abstract": [
    "The use of huge databases in ASR has become an important source of ASR system improvements in the last years. However, their use demands an increase of the computational resources necessary to train the recognizers. Several techniques have been proposed in the literature with the purpose of making a better use of these enormous databases by selecting the most 'informative' portions and thus reducing the computational burden. In this paper, we present a technique to select samples from a database that allows us to obtain similar results in MLP-based feature extraction stages by using around 60% of the data.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-115"
  },
  "kohler05_interspeech": {
   "authors": [
    [
     "Thilo W.",
     "Kohler"
    ],
    [
     "Christian",
     "Fugen"
    ],
    [
     "Sebastian",
     "Stüker"
    ],
    [
     "Alex",
     "Waibel"
    ]
   ],
   "title": "Rapid porting of ASR-systems to mobile devices",
   "original": "i05_0233",
   "page_count": 4,
   "order": 116,
   "p1": "233",
   "pn": "236",
   "abstract": [
    "Portable devices for the consumer market are becoming available in large quantities. Because of their design and use, human speech often is the input modality of choice, for example for car navigation systems or portable speech-to-speech translation devices. In this paper we describe our work in porting our existing desktop PC based speech recognition system to an off-the-shelf PDA running WindowsCE3.0. We do this in a way that our already well performing language and acoustic models can be taken over without the need of retraining them for the PDA. In order to achieve an acceptable run-time behavior we apply several optimization techniques to the preprocessing and decoding process. Among other things we introduce the newly developed early feature vector reduction. In that way the execution time of our recognition system can be reduced from initially 28x real-time to 2.6x real-time with a tolerable increase in word error rate. The size of the acoustic models is reduced to 25% of its original size.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-116"
  },
  "meinedo05_interspeech": {
   "authors": [
    [
     "Hugo",
     "Meinedo"
    ],
    [
     "Joao",
     "Neto"
    ]
   ],
   "title": "A stream-based audio segmentation, classification and clustering pre-processing system for broadcast news using ANN models",
   "original": "i05_0237",
   "page_count": 4,
   "order": 117,
   "p1": "237",
   "pn": "240",
   "abstract": [
    "This paper describes our work on the development of a low latency stream-based audio pre-processing system for broadcast news using model-based techniques. It performs speech/non-speech classification, speaker segmentation, speaker clustering, gender and background conditions classification. As a way to increase the modelling accuracy our algorithms make extensive use of Artificial Neural Networks (ANN) thus avoiding the rough assumptions normally made about the audio signal distribution. Experiments were conducted on the COST278 multilingual TV broadcast news database and compared with current state of the art algorithms using standard evaluation tools. Additionally we investigated the impact of automatic audio pre-processing system within the recognition using a large broadcast news test database for the European Portuguese. These tests show a small degradation in recognition performance when compared with hand labelled audio segmentation. Our system is part of a prototype close-captioning system that is daily processing the main news show of two Portuguese Broadcasters.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-117"
  },
  "marcheret05_interspeech": {
   "authors": [
    [
     "Etienne",
     "Marcheret"
    ],
    [
     "Karthik",
     "Visweswariah"
    ],
    [
     "Gerasimos",
     "Potamianos"
    ]
   ],
   "title": "Speech activity detection fusing acoustic phonetic and energy features",
   "original": "i05_0241",
   "page_count": 4,
   "order": 118,
   "p1": "241",
   "pn": "244",
   "abstract": [
    "With the wider deployment of automatic speech recognition (ASR) systems, the importance of robust speech activity detection has been elevated both as a means of reducing bandwidth in client/server ASR and for overall system stability from barge-in through the recognition process. In this paper we investigate a novel technique for speech activity detection, that we have found to be effective in handling non-stationary noise events without negatively impacting the recognition process. This technique is based on combining acoustic phonetic likelihood based features with energy features extracted from the signal waveform. Reported results on two speech activity detection tasks demonstrate that the proposed method outperforms techniques which rely solely on acoustic or energy features.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-118"
  },
  "tuske05_interspeech": {
   "authors": [
    [
     "Zoltan",
     "Tuske"
    ],
    [
     "Peter",
     "Mihajlik"
    ],
    [
     "Zoltan",
     "Tobler"
    ],
    [
     "Tibor",
     "Fegyo"
    ]
   ],
   "title": "Robust voice activity detection based on the entropy of noise-suppressed spectrum",
   "original": "i05_0245",
   "page_count": 4,
   "order": 119,
   "p1": "245",
   "pn": "248",
   "abstract": [
    "A novel noise robust voice activity detection approach is introduced. The novelty of the method that it uses noise suppressed spectrum of the input signal for spectral entropy calculation. As a result excellent end-pointing performance is observed based on predefined global entropy threshold and time constraints. The effect of frame dropping controlled by the proposed algorithm was investigated on the accuracy of automatic speech recognition. The experiments were performed on Hungarian publicly available noisy and normal telephony speech databases. The relative improvement due to dropping of non-speech frames was positive in all test configurations with a maximum of 29,5%. Besides, in average more than 50% of the frames were dropped.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-119"
  },
  "murase05_interspeech": {
   "authors": [
    [
     "Masamitsu",
     "Murase"
    ],
    [
     "Shunichi",
     "Yamamoto"
    ],
    [
     "Jean-Marc",
     "Valin"
    ],
    [
     "Kazuhiro",
     "Nakadai"
    ],
    [
     "Kentaro",
     "Yamada"
    ],
    [
     "Kazunori",
     "Komatani"
    ],
    [
     "Tetsuya",
     "Ogata"
    ],
    [
     "Hiroshi G.",
     "Okuno"
    ]
   ],
   "title": "Multiple moving speaker tracking by microphone array on mobile robot",
   "original": "i05_0249",
   "page_count": 4,
   "order": 120,
   "p1": "249",
   "pn": "252",
   "abstract": [
    "Real-world applications often require tracking multiple moving speakers for improving human-robot interactions and/or sound source separation. This paper presents multiple moving speaker tracking using an 8ch microphone array system installed on a mobile robot. This problem is difficult because the system does not assume that sound sources and/or the microphone array are fixed. Our solutions consist of two key ideas - time delay of arrival estimation, and multiple Kalman filters. The former localizes multiple sound sources based on beamforming in real time. Non-linear movements are tracked by using a set of Kalman filters with different history lengths in order to reduce errors in tracking multiple moving speakers under noisy and echoic environments. For quantitative evaluation of the tracking, motion references of sound sources and a mobile robot, called SIG2, were measured accurately by ultrasonic 3D tag sensors. As a result, we showed that the system tracked three simultaneous sound sources even when SIG2 moved in a room with large reverberation due to glass walls.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-120"
  },
  "deng05_interspeech": {
   "authors": [
    [
     "Li",
     "Deng"
    ],
    [
     "Dong",
     "Yu"
    ],
    [
     "Alex",
     "Acero"
    ]
   ],
   "title": "Learning statistically characterized resonance targets in a hidden trajectory model of speech coarticulation and reduction",
   "original": "i05_1097",
   "page_count": 4,
   "order": 121,
   "p1": "1097",
   "pn": "1100",
   "abstract": [
    "We report our new development of a hidden trajectory model for co-articulated, time-varying patterns of speech. The model uses bi-directional filtering of vocal tract resonance targets to jointly represent contextual variation and phonetic reduction in speech acoustics. A novel maximum-likelihood-based learning algorithm is presented that accurately estimates the distributional parameters of the resonance targets. The results of the estimates are analyzed and shown to be consistent with all the relevant acousticphonetic facts and intuitions. Phonetic recognition experiments demonstrate that the model with more rigorous target training outperforms the most recent earlier version of the model, producing 17.5% fewer errors in N-best rescoring.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-121"
  },
  "kocharov05_interspeech": {
   "authors": [
    [
     "Daniil",
     "Kocharov"
    ],
    [
     "András",
     "Zolnay"
    ],
    [
     "Ralf",
     "Schlüter"
    ],
    [
     "Hermann",
     "Ney"
    ]
   ],
   "title": "Articulatory motivated acoustic features for speech recognition",
   "original": "i05_1101",
   "page_count": 4,
   "order": 122,
   "p1": "1101",
   "pn": "1104",
   "abstract": [
    "In this paper, we consider the use of multiple acoustic features of the speech signal for continuous speech recognition. A novel articulatory motivated acoustic feature is introduced, namely the spectrum derivative feature. The new feature is tested in combination with the standard Mel Frequency Cepstral Coefficients (MFCC) and the voicedness features. Linear Discriminant Analysis is applied to find the optimal combination of different acoustic features. Experiments have been performed on small and large vocabulary tasks. Significant improvements in word error rate have been obtained by combining the MFCC feature with the articulatory motivated voicedness and spectrum derivative features: improvements of up to 25% on the small-vocabulary task and improvements of up to 4% on the large-vocabulary task relative to using MFCC alone with the same overall number of parameters in the system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-122"
  },
  "watanabe05b_interspeech": {
   "authors": [
    [
     "Shinji",
     "Watanabe"
    ],
    [
     "Atsushi",
     "Nakamura"
    ]
   ],
   "title": "Effects of Bayesian predictive classification using variational Bayesian posteriors for sparse training data in speech recognition",
   "original": "i05_1105",
   "page_count": 4,
   "order": 123,
   "p1": "1105",
   "pn": "1108",
   "abstract": [
    "We introduce a robust classification method using Bayesian predictive distribution (Bayesian predictive classification, referred to as BPC) into speech recognition. We and others have recently proposed a total Bayesian framework for speech recognition, Variational Bayesian Estimation and Clustering for speech recognition (VBEC). VBEC includes an analytical derivation of approximate posterior distributions that are essential for BPC, based on variational Bayes (VB). BPC using VB posterior distributions (VB-BPC) can mitigate the over-training effects by marginalizing output distribution. We address the sparse data problem in speech recognition, and show how VB-BPC is robust against the data sparseness, experimentally.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-123"
  },
  "tsao05_interspeech": {
   "authors": [
    [
     "Yu",
     "Tsao"
    ],
    [
     "Jinyu",
     "Li"
    ],
    [
     "Chin-Hui",
     "Lee"
    ]
   ],
   "title": "A study on separation between acoustic models and its applications",
   "original": "i05_1109",
   "page_count": 4,
   "order": 124,
   "p1": "1109",
   "pn": "1112",
   "abstract": [
    "We study separation between models of speech attributes. A good measure of separation usually serves as a key indicator of the discrimination power of these speech models because it can often be used to indirectly determine the performance of speech recognition and verification systems. In this study, we use a probabilistic distance, called generalized log likelihood ratio (GLLR), to measure the separation between a model of a target speech attribute and models of its competing attributes. We illustrate five applications to compare separations among models obtained over multiple levels of discrimination capabilities, at various degrees of acoustic definitions and resolutions, under mismatched training and testing conditions, and with different training criteria and speech parameters. We demonstrate that the well-known GLLR distance and its corresponding histograms also provide a good utility to qualitatively and quantitatively characterize the properties of trained models without performing large scale speech recognition and verification experiments.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-124"
  },
  "afify05_interspeech": {
   "authors": [
    [
     "Mohamed",
     "Afify"
    ]
   ],
   "title": "Extended baum-welch reestimation of Gaussian mixture models based on reverse Jensen inequality",
   "original": "i05_1113",
   "page_count": 4,
   "order": 125,
   "p1": "1113",
   "pn": "1116",
   "abstract": [
    "In this paper we derive the well known EBW reestimation formulae for Gaussian mixture models using the recently proposed reverse Jensen inequality. In addition to the simplicity of the derivation, it leads to closed form expressions for the D of each Gaussian in the mixture. Using some approximations, it is shown that the expressions can be reduced to the popular formula of [8] with a Gaussian dependent step size. The average of the normalized distance used in calculating the step size is empirically verified to be very close to one. Hence, the proposed formula leads to a global value very close to 3. The obtained results are validated in experiments on large vocabulary speech recognition.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-125"
  },
  "gunawardana05_interspeech": {
   "authors": [
    [
     "Asela",
     "Gunawardana"
    ],
    [
     "Milind",
     "Mahajan"
    ],
    [
     "Alex",
     "Acero"
    ],
    [
     "John C.",
     "Platt"
    ]
   ],
   "title": "Hidden conditional random fields for phone classification",
   "original": "i05_1117",
   "page_count": 4,
   "order": 126,
   "p1": "1117",
   "pn": "1120",
   "abstract": [
    "In this paper, we show the novel application of hidden conditional random fields (HCRFs) - conditional random fields with hidden state sequences - for modeling speech. Hidden state sequences are critical for modeling the non-stationarity of speech signals. We show that HCRFs can easily be trained using the simple direct optimization technique of stochastic gradient descent. We present the results on the TIMIT phone classification task and show that HCRFs outperforms comparable ML and CML/MMI trained HMMs. In fact, HCRF results on this task are the best single classifier results known to us. We note that the HCRF framework is easily extensible to recognition since it is a state and label sequence modeling technique. We also note that HCRFs have the ability to handle complex features without any change in training procedure.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-126"
  },
  "jonas05_interspeech": {
   "authors": [
    [
     "Michael",
     "Jonas"
    ],
    [
     "James G.",
     "Schmolze"
    ]
   ],
   "title": "Hierarchical clustering of mixture tying using a partially observable Markov decision process",
   "original": "i05_2953",
   "page_count": 4,
   "order": 127,
   "p1": "2953",
   "pn": "2956",
   "abstract": [
    "For over a decade, the Hidden Markov Model (HMM) has been the primary tool used for acoustic modeling in the field of speech recognition. In this paper we examine a more general approach using a Partially Observable Markov Decision Process (POMDP) to model the base phonetic unit. We introduce the concept of multiple phonetic context classes, one for each of the infinite possible contexts a phoneme can be in, and show how a POMDP can be used to represent such a model. Much the same way that tying mixtures at the state level across phonemes sharing linguistic properties is used to fill in gaps in the model space due to lack of data, the POMDP model can fill in additional gaps, in effect adding a second level of clustering driven by the data itself.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-127"
  },
  "ouellet05_interspeech": {
   "authors": [
    [
     "Pierre",
     "Ouellet"
    ],
    [
     "Gilles",
     "Boulianne"
    ],
    [
     "Patrick",
     "Kenny"
    ]
   ],
   "title": "Flavors of Gaussian warping",
   "original": "i05_2957",
   "page_count": 4,
   "order": 128,
   "p1": "2957",
   "pn": "2960",
   "abstract": [
    "In recent years, speech recognition researchers have proposed the use of Gaussian warping as a step in the computation of input speech feature parameters. This warping is intended to reduce the mismatch between the actual statistical distribution of parameters and the distribution hypothesized in the acoustic models used, i.e., the Gaussian distribution. In this paper, we compare variants of Gaussianization, including off-line and windowed (short-term) versions, which we evaluate on a corpus of Canadian Parliamentary Debates.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-128"
  },
  "keshet05_interspeech": {
   "authors": [
    [
     "Joseph",
     "Keshet"
    ],
    [
     "Shai",
     "Shalev-Shwartz"
    ],
    [
     "Yoram",
     "Singer"
    ],
    [
     "Dan",
     "Chazan"
    ]
   ],
   "title": "Phoneme alignment based on discriminative learning",
   "original": "i05_2961",
   "page_count": 4,
   "order": 129,
   "p1": "2961",
   "pn": "2964",
   "abstract": [
    "We propose a new paradigm for aligning a phoneme sequence of a speech utterance with its acoustical signal counterpart. In contrast to common HMM-based approaches, our method employs a discriminative learning procedure in which the learning phase is tightly coupled with the alignment task at hand. The alignment function we devise is based on mapping the input acoustic-symbolic representations of the speech utterance along with the target alignment into an abstract vector space. We suggest a specific mapping into the abstract vector-space which utilizes standard speech features (e.g. spectral distances) as well as confidence outputs of a framewise phoneme classifier. Building on techniques used for large margin methods for predicting whole sequences, our alignment function distills to a classifier in the abstract vectorspace which separates correct alignments from incorrect ones. We describe a simple iterative algorithm for learning the alignment function and discuss its formal properties. Experiments with the TIMIT corpus show that our method outperforms the current state-of-the-art approaches.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-129"
  },
  "leppanen05_interspeech": {
   "authors": [
    [
     "Jussi",
     "Leppänen"
    ],
    [
     "Imre",
     "Kiss"
    ]
   ],
   "title": "Comparison of low footprint acoustic modeling techniques for embedded ASR systems",
   "original": "i05_2965",
   "page_count": 4,
   "order": 130,
   "p1": "2965",
   "pn": "2968",
   "abstract": [
    "In this paper we compare the performance of speech recognition systems based on hidden Markov models (HMM) with quantized parameters (qHMMs) and subspace distribution clustering hidden Markov models (SDCHMMs). Both of these HMM types provide similar performance as continuous density HMMs, but with significantly reduced memory requirements (approximately 90% less memory was needed to store the HMM densities). The experiments show that on a small vocabulary isolated word recognition task, SDCHMMs outperform qHMMs in clean conditions. However, when noisy test data is used and adaptation is enabled qHMMs outperform the SDCHMMs. In addition, the experiments show that as low as 3-bit feature quantization can be used with both qHMMs and SDCHMMs without sacrificing recognition performance.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-130"
  },
  "suchato05_interspeech": {
   "authors": [
    [
     "Atiwong",
     "Suchato"
    ],
    [
     "Proadpran",
     "Punyabukkana"
    ]
   ],
   "title": "Factors in classification of stop consonant place of articulation",
   "original": "i05_2969",
   "page_count": 4,
   "order": 131,
   "p1": "2969",
   "pn": "2972",
   "abstract": [
    "The goal of this study is to uncover significant factors in the classification of English stop consonant place of articulation, and determine their relative importance to the classification. Factor analysis arranges a set of acoustic attributes used for the classification into factors that can be interpreted in terms of articulatory attributes. Each factor is found to be comprised of attributes explaining a particular aspect of the acoustic cues significant to place classification. For a stop in CV context, significant factors are: normalized burst amplitude', burst shape', formant frequency', and formant transition'. The first two always remain regardless of the vowel frontness. The two formant-related factors arrange differently depending on the frontness. Discriminant analysis is deployed to determine the contribution of each factor to the classification. Without vowel frontness information, the burst-related factors are found to contribute more than the formant-related factors do. However, with known frontness, the opposite is true.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-131"
  },
  "toth05_interspeech": {
   "authors": [
    [
     "Arthur R.",
     "Toth"
    ],
    [
     "Alan W.",
     "Black"
    ]
   ],
   "title": "Cross-speaker articulatory position data for phonetic feature prediction",
   "original": "i05_2973",
   "page_count": 4,
   "order": 132,
   "p1": "2973",
   "pn": "2976",
   "abstract": [
    "Through the use of a device called an Electromagnetic Articulograph, it is possible to measure the locations of a person's articulators during speech. As more of this data becomes available, one important question is how it can be used. In this paper, we demonstrate that it can improve performance for the recognition of some phonetic features. As articulatory position data is scarce, we also describe experiments that use articulatory position data from one speaker with another and provide results. These experiments use cross-speaker articulatory positions to predict phonetic features.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-132"
  },
  "povey05_interspeech": {
   "authors": [
    [
     "Daniel",
     "Povey"
    ]
   ],
   "title": "Improvements to fMPE for discriminative training of features",
   "original": "i05_2977",
   "page_count": 4,
   "order": 133,
   "p1": "2977",
   "pn": "2980",
   "abstract": [
    "fMPE is a previously introduced form of discriminative training, in which offsets to the features are obtained by training a projection from a high-dimensional feature space based on posteriors of Gaussians. This paper presents recent improvements to fMPE, including improved high-dimensional features which are easier to compute, and improvements to the training procedure. Other issues investigated include cross-testing of fMPE transforms (i.e. using acoustic models other than those with which the fMPE was trained) and the best way to train the Gaussians used to obtain the vector of posteriors.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-133"
  },
  "lei05_interspeech": {
   "authors": [
    [
     "Xin",
     "Lei"
    ],
    [
     "Mei-Yuh",
     "Hwang"
    ],
    [
     "Mari",
     "Ostendorf"
    ]
   ],
   "title": "Incorporating tone-related MLP posteriors in the feature representation for Mandarin ASR",
   "original": "i05_2981",
   "page_count": 4,
   "order": 134,
   "p1": "2981",
   "pn": "2984",
   "abstract": [
    "Tone has a crucial role in Mandarin speech in distinguishing ambiguous words. In most state-of-the-art Mandarin automatic speech recognition systems, tonal acoustic units are used and F0 features are appended to the spectral features (MFCC/PLP). However, a tone depends on the F0 contour of a time span much longer than a frame. Ideally, systems would compute the frame-level likelihood of a tone using more than the F0 and derivative values at the current frame. Inspired by the tandem approach, we propose to extract tone-related features for each frame by using longer acoustic context information in a multi-layer perceptron (MLP). The extracted tone-related posteriors are then appended to the spectral feature vector to form a new feature vector for back-end HMM systems. Results show that significant improvement can be achieved by adding these tone-related MLP posterior features in a Mandarin conversational telephone speech recognition task. In one configuration, the character error rate was reduced from 35.7% to 33.2%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-134"
  },
  "han05_interspeech": {
   "authors": [
    [
     "Yan",
     "Han"
    ],
    [
     "Johan de",
     "Veth"
    ],
    [
     "Louis",
     "Boves"
    ]
   ],
   "title": "Speech trajectory clustering for improved speech recognition",
   "original": "i05_2985",
   "page_count": 4,
   "order": 135,
   "p1": "2985",
   "pn": "2988",
   "abstract": [
    "Context dependent modelling is known to improve recognition performance for automatic speech recognition. One of the major limitations, especially of approaches based on Decision Trees, is that the questions that guide the search for effective contexts must be known in advance. However, the variation in the speech signals is caused by multiple factors, not all of which may be known during the training procedure. State tying methods, on the other hand, are strictly local, and therefore do not allow to reap the benefits of variation that spans longer length units such as syllables. In this paper, we present an approach that does not require prior knowledge and that still can find the most important variants of speech units of arbitrary length. The method is based on clustering the multi-dimensional dynamic trajectories corresponding to speech units. Thus, we define multipath model topologies based on automatically derived clusters of dynamic trajectories (Trajectory Clustering based hidden Markov models, TCHMMs). In this paper we compare the clusters obtained with Trajectory Clustering and knowledge based context dependent Head and Tail models in a Head-Body-Tail model (HBT) connected digits recognition task. Our results show that TCHMMs outperform conventional HBT models significantly.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-135"
  },
  "temko05_interspeech": {
   "authors": [
    [
     "Andrey",
     "Temko"
    ],
    [
     "Dusan",
     "Macho"
    ],
    [
     "Climent",
     "Nadeu"
    ]
   ],
   "title": "Selection of features and combination of classifiers using a fuzzy approach for acoustic event classification",
   "original": "i05_2989",
   "page_count": 4,
   "order": 136,
   "p1": "2989",
   "pn": "2992",
   "abstract": [
    "In this paper, we aim to improve the classification of human non-speech sounds produced in a meeting-room environment by using concepts and tools from the fuzzy theory. Starting with an SVM-based baseline system, firstly a reduction of the number of features with the fuzzy measure is shown. And, secondly, a noticeable improvement of the classification performance is reported by combining the outputs of two classification systems with the fuzzy integral.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-136"
  },
  "stadermann05_interspeech": {
   "authors": [
    [
     "Jan",
     "Stadermann"
    ],
    [
     "Wolfram",
     "Koska"
    ],
    [
     "Gerhard",
     "Rigoll"
    ]
   ],
   "title": "Multi-task learning strategies for a recurrent neural net in a hybrid tied-posteriors acoustic model",
   "original": "i05_2993",
   "page_count": 4,
   "order": 137,
   "p1": "2993",
   "pn": "2996",
   "abstract": [
    "An important goal of an automatic classifier is to learn the best possible generalization from given training material. One possible improvement over a standard learning algorithm is to train several related tasks in parallel. We apply the multi-task learning scheme to a recurrent neural network estimating phoneme posterior probabilities and HMM state posterior probabilities, respectively. A comparison of networks with different additional tasks within a hybrid NN/HMM acoustic model is presented. The evaluation has been performed using the WSJ0 speaker independent test set with a closed vocabulary of 5000 words and shows a significant improvement compared to a standard hybrid acoustic model if gender classification is used as additional task.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-137"
  },
  "honig05_interspeech": {
   "authors": [
    [
     "Florian",
     "Hönig"
    ],
    [
     "Georg",
     "Stemmer"
    ],
    [
     "Christian",
     "Hacker"
    ],
    [
     "Fabio",
     "Brugnara"
    ]
   ],
   "title": "Revising Perceptual Linear Prediction (PLP)",
   "original": "i05_2997",
   "page_count": 4,
   "order": 138,
   "p1": "2997",
   "pn": "3000",
   "abstract": [
    "Mel Frequency Cepstral Coefficients (MFCC) and Perceptual Linear Prediction (PLP) are the most popular acoustic features used in speech recognition. Often it depends on the task, which of the two methods leads to a better performance. In this work we develop acoustic features that combine the advantages of MFCC and PLP. Based on the observation that the techniques have many similarities, we revise the processing steps of PLP. In particular, the filter-bank, the equal-loudness pre-emphasis and the input for the linear prediction are improved. It is shown for a broadcast news transcription task and a corpus of children's speech that the new variant of PLP performs better than both MFCC and conventional PLP for a wide range of clean and noisy acoustic conditions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-138"
  },
  "pinto05_interspeech": {
   "authors": [
    [
     "Joel",
     "Pinto"
    ],
    [
     "R. N. V.",
     "Sitaram"
    ]
   ],
   "title": "Confidence measures in speech recognition based on probability distribution of likelihoods",
   "original": "i05_3001",
   "page_count": 4,
   "order": 139,
   "p1": "3001",
   "pn": "3004",
   "abstract": [
    "In this paper, we propose two confidence measures (CMs) in speech recognition: one based on acoustic likelihood and the other based on phone duration. For a decoded speech frame aligned to an HMM state, the CM based on acoustic likelihood depends on the relative position of its output likelihood value in the probability distribution of likelihood value in that particular state. The CM of whole phone is the geometric mean of CMs of all frames in it. The CM based on duration depends on the deviation of the observed duration from the expected duration of the recognized phone. The two CMs are combined using weighted geometric mean to obtain a hybrid phone CM. The hybrid CM shows significant improvement over the CM based on time normalized log-likelihood score. On TI-digits database, at 20% false acceptance rate, the normalized acoustic log-likelihood based CM has a detection rate of 83.8% while the hybrid CM has a detection rate of 92.4%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-139"
  },
  "diehl05_interspeech": {
   "authors": [
    [
     "Frank",
     "Diehl"
    ],
    [
     "Asuncion",
     "Moreno"
    ],
    [
     "Enric",
     "Monte"
    ]
   ],
   "title": "Continuous local codebook features for multi- and cross-lingual acoustic phonetic modelling",
   "original": "i05_3005",
   "page_count": 4,
   "order": 140,
   "p1": "3005",
   "pn": "3008",
   "abstract": [
    "In this paper we present a method for defining the question set for the induction of acoustic phonetic decision trees. The method is data driven resulting in a continuous feature space in contrast to the usual categorical one. We apply the features to a multi-lingual speech recognition task, outperforming consistently the standard method using IPA-based characteristics. An extension to cross-lingual applications together with first preliminary results are given too.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-140"
  },
  "miguel05_interspeech": {
   "authors": [
    [
     "Antonio",
     "Miguel"
    ],
    [
     "Eduardo",
     "Lleida"
    ],
    [
     "Richard",
     "Rose"
    ],
    [
     "Luis",
     "Buera"
    ],
    [
     "Alfonso",
     "Ortega"
    ]
   ],
   "title": "Augmented state space acoustic decoding for modeling local variability in speech",
   "original": "i05_3009",
   "page_count": 4,
   "order": 141,
   "p1": "3009",
   "pn": "3012",
   "abstract": [
    "This paper presents a decoding method for automatic speech recognition (ASR) that reduces the impact of local spectral and temporal variabilities on ASR performance. The procedure involves augmenting the standard Viterbi search for an optimum state sequence with a locally constrained search for optimum degrees of spectral warping or temporal warping applied to individual analysis frames. It is argued in the paper that this represents an efficient and effective method for compensating for local variability in speech which may have potential application to a broader array of speech transformations. The techniques are presented in the context of existing methods for frequency warping based speaker normalization and existing methods for computation of dynamic features for ASR. The modified decoding algorithms were evaluated in both clean and noisy task domains using subsets of the Aurora 2 and Aurora 3 Speech Corpora under clean and noisy conditions. It was found that, under clean conditions on the Spanish Language Subset of the Speech-Dat-Car database, the modified decoding method applied with local frequency transformations reduced word error rate (WER) by 24 percent. This was a factor of two greater reduction in WER than was obtained on the same task using the more well known frequency warping based vocal tract length normalization (VTLN) procedure.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-141"
  },
  "dimitriadis05_interspeech": {
   "authors": [
    [
     "Dimitrios",
     "Dimitriadis"
    ],
    [
     "Petros",
     "Maragos"
    ],
    [
     "Alexandros",
     "Potamianos"
    ]
   ],
   "title": "Auditory Teager energy cepstrum coefficients for robust speech recognition",
   "original": "i05_3013",
   "page_count": 4,
   "order": 142,
   "p1": "3013",
   "pn": "3016",
   "abstract": [
    "In this paper, a feature extraction algorithm for robust speech recognition is introduced. The feature extraction algorithm is motivated by the human auditory processing and the nonlinear Teager-Kaiser energy operator that estimates the true energy of the source of a resonance. The proposed features are labeled as Teager Energy Cepstrum Coefficients (TECCs). TECCs are computed by first filtering the speech signal through a dense non constant-Q Gammatone filterbank and then by estimating the \"true\" energy of the signal's source, i.e., the short-time average of the output of the Teager-Kaiser energy operator. Error analysis and speech recognition experiments show that the TECCs and the mel frequency cepstrum coefficients (MFCCs) perform similarly for clean recording conditions; while the TECCs perform significantly better than the MFCCs for noisy recognition tasks. Specifically, relative word error rate improvement of 60% over the MFCC baseline is shown for the Aurora-3 database for the high-mismatch condition. Absolute error rate improvement ranging from 5% to 20% is shown for a phone recognition task in (various types of additive) noise.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-142"
  },
  "hifny05_interspeech": {
   "authors": [
    [
     "Yasser",
     "Hifny"
    ],
    [
     "Steve",
     "Renals"
    ],
    [
     "Neil D.",
     "Lawrence"
    ]
   ],
   "title": "A hybrid Maxent/HMM based ASR system",
   "original": "i05_3017",
   "page_count": 4,
   "order": 143,
   "p1": "3017",
   "pn": "3020",
   "abstract": [
    "The aim of this work is to develop a practical framework, which extends the classical Hidden Markov Models (HMM) for continuous speech recognition based on the Maximum Entropy (MaxEnt) principle. The MaxEnt models can estimate the posterior probabilities directly as with Hybrid NN/HMM connectionist speech recognition systems. In particular, a new acoustic modelling based on discriminative MaxEnt models is formulated and is being developed to replace the generative Gaussian Mixture Models (GMM) commonly used to model acoustic variability. Initial experimental results using the TIMIT phone task are reported.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-143"
  },
  "erdogan05_interspeech": {
   "authors": [
    [
     "Hakan",
     "Erdogan"
    ]
   ],
   "title": "Regularizing linear discriminant analysis for speech recognition",
   "original": "i05_3021",
   "page_count": 4,
   "order": 144,
   "p1": "3021",
   "pn": "3024",
   "abstract": [
    "Feature extraction is an essential first step in speech recognition applications. In addition to static features extracted from each frame of speech data, it is beneficial to use dynamic features (called Δ and ΔΔ coefficients) that use information from neighboring frames. Linear Discriminant Analysis (LDA) followed by a diagonalizing maximum likelihood linear transform (MLLT) applied to spliced static MFCC features yields important performance gains as compared to MFCC+Δ+ΔΔ features in most tasks. However, since LDA is obtained using statistical averages trained on limited data, it is reasonable to regularize LDA transform computation by using prior information and experience. In this paper, we regularize LDA and heteroschedastic LDA transforms using two methods: (1) Using statistical priors for the transform in a MAP formulation (2) Using structural constraints on the transform. As prior, we use a transform that computes static+Δ+ΔΔ coefficients. Our structural constraint is in the form of a block structured LDA transform where each block acts on the same cepstral parameters across frames. The second approach suggests using new coefficients for static, first difference and second difference operators as compared to the standard ones to improve performance. We test the new algorithms on two different tasks, namely TIMIT phone recognition and AURORA2 digit sequence recognition in noise. We obtain consistent improvement in our experiments as compared to MFCC features. In addition, we obtain encouraging results in some AURORA2 tests as compared to LDA+MLLT features.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-144"
  },
  "wang05b_interspeech": {
   "authors": [
    [
     "Yadong",
     "Wang"
    ],
    [
     "Steven",
     "Greenberg"
    ],
    [
     "Jayaganesh",
     "Swaminathan"
    ],
    [
     "Ramdas",
     "Kumaresan"
    ],
    [
     "David",
     "Poeppel"
    ]
   ],
   "title": "Comprehensive modulation representation for automatic speech recognition",
   "original": "i05_3025",
   "page_count": 4,
   "order": 145,
   "p1": "3025",
   "pn": "3028",
   "abstract": [
    "We present a new feature representation for speech recognition based on both amplitude modulation spectra (AMS) and frequency modulation spectra (FMS). A comprehensive modulation spectral (CMS) approach is defined and analyzed based on a modulation model of the band-pass signal. The speech signal is processed first by a bank of specially designed auditory band-pass filters. CMS are extracted from the output of the filters as the features for automatic speech recognition (ASR). A significant improvement is demonstrated in performance on noisy speech. On the Aurora 2 task the new features result in an improvement of 23.43% relative to traditional mel-cepstrum front-end features using a 3 GMM HMM back-end. Although the improvements are relatively modest, the novelty of the method and its potential for performance enhancement warrants serious attention for future-generation ASR applications.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-145"
  },
  "fu05_interspeech": {
   "authors": [
    [
     "Qiang",
     "Fu"
    ],
    [
     "Biing-Hwang",
     "Juang"
    ]
   ],
   "title": "Segment-based phonetic class detection using minimum verification error (MVE) training",
   "original": "i05_3029",
   "page_count": 4,
   "order": 146,
   "p1": "3029",
   "pn": "3032",
   "abstract": [
    "In this paper, we investigate the performance of segment-based detectors for three taxonomic sets of acoustic-phonetic classes. Acoustic-phonetic detectors form an important processing layer for speech event decoding in the new detection-based automatic speech recognition. In this study, detectors are trained within a minimum verification error (MVE) framework which is markedly different from the conventional maximum likelihood (ML) method. Performance evaluations are conducted upon the TIMIT database by comparing detectors trained via MVE and detectors trained via maximum likelihood. Remarkable improvement in terms of detection error reduction is observed and reported. The result is a solid manifestation of the effectiveness of the discriminative training method, particularly MVE, in the detection-based speech recognition approach. These detectors, aside from being an important processing stage in an overall speech recognition system, can also be extended for applications in diagnostic information retrieval or recognition rescoring for utterance verification.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-146"
  },
  "liu05_interspeech": {
   "authors": [
    [
     "Yi",
     "Liu"
    ],
    [
     "Pascale",
     "Fung"
    ]
   ],
   "title": "Acoustic and phonetic confusions in accented speech recognition",
   "original": "i05_3033",
   "page_count": 4,
   "order": 147,
   "p1": "3033",
   "pn": "3036",
   "abstract": [
    "Accented speech recognition is more challenging than standard speech recognition due to the effects of phonetic and acoustic confusions. Phonetic confusion in accented speech occurs when an expected phone is pronounced as a different one, which leads to erroneous recognition. Acoustic confusion occurs when the pronounced phone is found to lie acoustically between two baseform models and can be equally recognized as either one. We propose that it is necessary to analyze and model these confusions separately in order to improve accented speech recognition without degrading standard speech recognition. We propose using likelihood ratio test to measure phonetic confusion, and asymmetric acoustic distance to measure acoustic confusion. Only accent-specific phonetic units with low acoustic confusion are used in an augmented pronunciation dictionary, while phonetic models with high acoustic confusion are reconstructed using decision tree merging. Experimental results show that our approach is effective and superior to methods modeling phonetic confusion or acoustic confusion alone in accented speech, with a significant 5.7% absolute WER reduction, without degrading standard speech recognition.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-147"
  },
  "munich05_interspeech": {
   "authors": [
    [
     "Mario E.",
     "Munich"
    ],
    [
     "Qiguang",
     "Lin"
    ]
   ],
   "title": "Auditory image model features for automatic speech recognition",
   "original": "i05_3037",
   "page_count": 4,
   "order": 148,
   "p1": "3037",
   "pn": "3040",
   "abstract": [
    "Conventional speech recognition engines extract Mel Frequency Cepstral Coefficients (MFCC) features from incoming speech. This paper presents a novel approach for feature extraction in which speech is processed according to the Auditory Image Model, a model of human psychoacoustics. We fist describe the proposed front-end, then we present recognition results obtained with the TIMIT database. Comparing with previously published results on the same task, the new approach achieves a 10% improvement in recognition accuracy.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-148"
  },
  "heracleous05_interspeech": {
   "authors": [
    [
     "Panikos",
     "Heracleous"
    ],
    [
     "Tomomi",
     "Kaino"
    ],
    [
     "Hiroshi",
     "Saruwatari"
    ],
    [
     "Kiyohiro",
     "Shikano"
    ]
   ],
   "title": "Applications of NAM microphones in speech recognition for privacy in human-machine communication",
   "original": "i05_3041",
   "page_count": 4,
   "order": 149,
   "p1": "3041",
   "pn": "3044",
   "abstract": [
    "In this paper, we present the use of stethoscope and silicon NAM microphones in automatic speech recognition. NAM microphones are special acoustic sensors, which are attached behind the talker's ear and can capture not only normal (audible) speech, but also very quietly uttered speech (non-audible murmur). As a result, NAM microphones can be applied in automatic speech recognition systems when privacy is desired. Previously, we presented speech recognition experiments for non-audible murmur captured by a stethoscope microphone. In this paper, we also present recognition results using a more advanced NAM microphone, the so-called silicon NAM microphone. Using adaptation techniques and a small amount of training data, we achieved a 93.9% word accuracy for non-audible murmur recognition. We also report experimental results in noisy environments showing the effectiveness of using a NAM microphone in noisy environments. In addition to a dictation task, we also present a keyword spotting experiment based on non-audible murmur.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-149"
  },
  "frankel05_interspeech": {
   "authors": [
    [
     "Joe",
     "Frankel"
    ],
    [
     "Simon",
     "King"
    ]
   ],
   "title": "A hybrid ANN/DBN approach to articulatory feature recognition",
   "original": "i05_3045",
   "page_count": 4,
   "order": 150,
   "p1": "3045",
   "pn": "3048",
   "abstract": [
    "Artificial neural networks (ANN) have proven to be well suited to the task of articulatory feature (AF) recognition. However, one drawback with an ANN approach is that features are assumed to be statistically independent. We address this by using ANNs to provide virtual evidence to a dynamic Bayesian network (DBN). This gives a hybrid ANN/DBN model and allows modelling of inter-feature dependencies. We demonstrate significant increases in AF recognition accuracy from modelling dependencies between features, and present the results of embedded training experiments in which a set of asynchronous feature changes are learned. Furthermore, we report on the application of a Viterbi training scheme in which we alternate between realigning the AF training labels and retraining the ANNs.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-150"
  },
  "zhang05b_interspeech": {
   "authors": [
    [
     "Yaxin",
     "Zhang"
    ],
    [
     "Bian",
     "Wu"
    ],
    [
     "Xiaolin",
     "Ren"
    ],
    [
     "Xin",
     "He"
    ]
   ],
   "title": "A speaker biased SI recognizer for embedded mobile applications",
   "original": "i05_0253",
   "page_count": 4,
   "order": 151,
   "p1": "253",
   "pn": "256",
   "abstract": [
    "Non-native and accent speakers often face problems when using a speaker-independent (SI) speech recognition system. Speaker adaptation has been a solution to make SI recognizer work better for individuals. Targeting embedded implementation and applications in fast changing mobile environments, we in this paper proposed a supervised speaker adaptation (SA) solution with low system resource consumption, minimized disturbance to the data structure of SI recognizer, and superior adaptation performance. Adapted by UK speakers on a digit recognition task, the US English speech recognizer produced 65.9% digit error reduction. Other advantages of the proposed SA method include the multi-speaker adaptation, the fast adaptation, and the little changed speaker independency after adaptation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-151"
  },
  "bakker05_interspeech": {
   "authors": [
    [
     "Bart",
     "Bakker"
    ],
    [
     "Carsten",
     "Meyer"
    ],
    [
     "Xavier",
     "Aubert"
    ]
   ],
   "title": "Fast unsupervised speaker adaptation through a discriminative eigen-MLLR algorithm",
   "original": "i05_0257",
   "page_count": 4,
   "order": 152,
   "p1": "257",
   "pn": "260",
   "abstract": [
    "We present a new method for unsupervised, fast speaker adaptation that combines the Eigen-MLLR transform approach with discriminative MLLR. We thereby aim to profit both from the performance improvements that are generally provided by a discriminative approach, and from the reliability that Eigen-MLLR has demonstrated in fast adaptation scenarios. We present first evaluation results on the Spoke 4 subset of the 1994 Wall Street Journal (WSJ) database. Our results show that, in fast enrollment scenarios, discriminative Eigen-MLLR allows for clear improvements both over non-discriminative Eigen-MLLR and over discriminative MLLR. We further introduce a method to estimate the weight parameters of Eigen-MLLR discriminatively, and show that this allows for further improvements on the considered data sets.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-152"
  },
  "hu05_interspeech": {
   "authors": [
    [
     "Rusheng",
     "Hu"
    ],
    [
     "Jian",
     "Xue"
    ],
    [
     "Yunxin",
     "Zhao"
    ]
   ],
   "title": "Incremental largest margin linear regression and MAP adaptation for speech separation in telemedicine applications",
   "original": "i05_0261",
   "page_count": 4,
   "order": 153,
   "p1": "261",
   "pn": "264",
   "abstract": [
    "In this paper, a novel technique of online incremental speaker adaptation for speech stream separation in telemedicine is proposed. An unsupervised discriminative linear regression technique is developed based on the principle of maximizing the class separation margin to transform model mean. This adaptation approach is called largest margin linear regression (LMLR). Online incremental LMLR and MAP are performed on Gaussian mixture density based speaker models. A discounted sequential learning technique is proposed for LMLR to reduce effect of unreliable initial models on unsupervised speaker model adaptation, and the adapted models from LMLR and MAP are combined for improving accuracy of speech segment labeling as doctor or patient. Experimental results on telemedicine data show that LMLR is superior to MLLR and combining LMLR and MAP during online model adaptation is highly effective. The proposed new technique significantly improved performance of our earlier system of speech stream separation, leading to nearly perfectly separated speech streams when judged by human listeners.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-153"
  },
  "garau05_interspeech": {
   "authors": [
    [
     "Giulia",
     "Garau"
    ],
    [
     "Steve",
     "Renals"
    ],
    [
     "Thomas",
     "Hain"
    ]
   ],
   "title": "Applying vocal tract length normalization to meeting recordings",
   "original": "i05_0265",
   "page_count": 4,
   "order": 154,
   "p1": "265",
   "pn": "268",
   "abstract": [
    "Vocal Tract Length Normalisation (VTLN) is a commonly used technique to normalise for inter-speaker variability. It is based on the speaker-specific warping of the frequency axis, parameterised by a scalar warp factor. This factor is typically estimated using maximum likelihood. We discuss how VTLN may be applied to multiparty conversations, reporting a substantial decrease in word error rate in experiments using the ICSI meetings corpus. We investigate the behaviour of the VTLN warping factor and show that a stable estimate is not obtained. Instead it appears to be influenced by the context of the meeting, in particular the current conversational partner. These results are consistent with predictions made by the psycholinguistic interactive alignment account of dialogue, when applied at the acoustic and phonological levels.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-154"
  },
  "umesh05_interspeech": {
   "authors": [
    [
     "S.",
     "Umesh"
    ],
    [
     "András",
     "Zolnay"
    ],
    [
     "Hermann",
     "Ney"
    ]
   ],
   "title": "Implementing frequency-warping and VTLN through linear transformation of conventional MFCC",
   "original": "i05_0269",
   "page_count": 4,
   "order": 155,
   "p1": "269",
   "pn": "272",
   "abstract": [
    "In this paper, we show that frequency-warping (including VTLN) can be implemented through linear transformation of conventional MFCC. Unlike the Pitz-Ney [1] continuous domain approach, we directly determine the relation between frequency-warping and the linear-transformation in the discrete-domain. The advantage of such an approach is that it can be applied to any frequency-warping and is not limited to cases where an analytical closed-form solution can be found. The proposed method exploits the bandlimited interpolation idea (in the frequency-domain) to do the necessary frequency-warping and yields exact results as long as the cepstral coefficients are quefrency limited. This idea of quefrency limitedness shows the importance of the filter-bank smoothing of the spectra which has been ignored in [1, 2]. Furthermore, unlike [1], since we operate in the discrete domain, we can also apply the usual discrete-cosine transform (i.e. DCT-II) on the logarithm of the filter-bank output to get conventional MFCC features. Therefore, using our proposed method, we can linearly transform conventional MFCC cepstra to do VTLN and we do not require any recomputation of the warped-features. We provide experimental results in support of this approach.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-155"
  },
  "cui05_interspeech": {
   "authors": [
    [
     "Xiaodong",
     "Cui"
    ],
    [
     "Abeer",
     "Alwan"
    ]
   ],
   "title": "MLLR-like speaker adaptation based on linearization of VTLN with MFCC features",
   "original": "i05_0273",
   "page_count": 4,
   "order": 156,
   "p1": "273",
   "pn": "276",
   "abstract": [
    "In this paper, an MLLR-like adaptation approach is proposed whereby the transformation of the means is performed deterministically based on linearization of VTLN. Biases and adaptation of the variances are estimated statistically by the EM algorithm. In the discrete frequency domain, we show that under certain approximations, frequency warping with Mel-filterbank-based MFCCs equals a linear transformation in the cepstral domain. Utilizing the deduced linear relationship, the transformation matrix is generated by formant-like peak alignment. Experimental results using children's speech show improvements over traditional MLLR and VTLN. The improvements occur even with limited amounts of adaptation data.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-156"
  },
  "raut05_interspeech": {
   "authors": [
    [
     "Chandra Kant",
     "Raut"
    ],
    [
     "Takuya",
     "Nishimoto"
    ],
    [
     "Shigeki",
     "Sagayama"
    ]
   ],
   "title": "Model adaptation by state splitting of HMM for long reverberation",
   "original": "i05_0277",
   "page_count": 4,
   "order": 157,
   "p1": "277",
   "pn": "280",
   "abstract": [
    "In environment with considerably long reverberation time, each frame of speech is affected by reflected energy components from the preceding frames. Therefore to adapt model parameters of a state, it becomes necessary to consider these frames, and compute their contributions to current state. However, these clean speech frames preceding to a state of HMM are not known during adaptation of the models. This paper describes a method to estimate the preceding frames for a state in HMM, by splitting the state into a number of substates. The estimated sequence of frames can then be used to find reflected energy component for the state and compensate its parameters. The effectiveness of the method was confirmed by the experimental results on an isolated-word recognition task.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-157"
  },
  "liu05b_interspeech": {
   "authors": [
    [
     "Daben",
     "Liu"
    ],
    [
     "Daniel",
     "Kiecza"
    ],
    [
     "Amit",
     "Srivastava"
    ],
    [
     "Francis",
     "Kubala"
    ]
   ],
   "title": "Online speaker adaptation and tracking for real-time speech recognition",
   "original": "i05_0281",
   "page_count": 4,
   "order": 158,
   "p1": "281",
   "pn": "284",
   "abstract": [
    "This paper describes a low-latency online speaker adaptation framework. The main objective is to apply fast speaker adaptation to a real-time (RT) large vocabulary continuous speech recognition (LVCSR) engine. In this framework, speaker adaptation is performed on speaker turns generated by online speaker change detection and speaker clustering. To maximize long-term system performance, the adaptation statistics for known speakers are updated continuously while new speakers are actively discovered. In contrast to existing approaches, a re-decode of an utterance after adaptation is eliminated from the process. We demonstrate that the framework can be easily incorporated into every pass of a multi-pass decoder. We applied the framework to the BBN Audio Indexer which is a real-time end-to-end audio indexing system that runs at around 0.6xRT. The result is an 8%-12% relative word-error-rate reduction on broadcast news benchmark tests for English, Chinese, and Arabic, with less than 0.1xRT cost in real-time throughput.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-158"
  },
  "nishida05_interspeech": {
   "authors": [
    [
     "Masafumi",
     "Nishida"
    ],
    [
     "Yasuo",
     "Horiuchi"
    ],
    [
     "Akira",
     "Ichikawa"
    ]
   ],
   "title": "Automatic speech recognition based on adaptation and clustering using temporal-difference learning",
   "original": "i05_0285",
   "page_count": 4,
   "order": 159,
   "p1": "285",
   "pn": "288",
   "abstract": [
    "This paper describes a novel approach based on online unsupervised adaptation and clustering using temporal-difference (TD) learning. Temporal-difference learning is a reinforcement learning technique and is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives when interacting with a complex, uncertain environment. The adaptation progresses based on rewards that represent correctness of outputs. The adapted models gradually accumulate and cluster with the environmental conditions and can immediately adapt by selecting the optimal model from the clusters. We conducted speech recognition experiments by a connected digit recognition in noisy environments including the variation of speakers and noises. The results verify that the proposed method has a higher recognition performance than the conventional adaptation method.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-159"
  },
  "ye05_interspeech": {
   "authors": [
    [
     "Hui",
     "Ye"
    ],
    [
     "Steve",
     "Young"
    ]
   ],
   "title": "Improving the speech recognition performance of beginners in spoken conversational interaction for language learning",
   "original": "i05_0289",
   "page_count": 4,
   "order": 160,
   "p1": "289",
   "pn": "292",
   "abstract": [
    "The provision of automatic systems that can provide conversational practice for beginners would make a valuable addition to existing aids for foreign language teaching. To achieve this goal, the SCILL (Spoken Conversational Interaction for Language Learning) project is developing a spoken dialogue system that is capable of maintaining interactive dialogues with non-native students in the target language. However, the effective realisation of the intelligent language understanding and dialogue management needed for such a system, requires robust recognition of poorly articulated non-native speech. This paper studies several popular techniques for robust acoustic modelling including HLDA,MAP and CMLLR on non-native speech data within a specific dialogue domain. In addition, a novel approach for using cross language speech data to adapt the acoustic models is described and shown to be useful when very limited non-native adaptation data is available. The experimental results provide a clear story of how to improve recognition performance on non-native speech for a specific task, and this will be of interest more generally for those developing multi-lingual spoken dialogue systems.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-160"
  },
  "gomez05_interspeech": {
   "authors": [
    [
     "Randy",
     "Gomez"
    ],
    [
     "Akinobu",
     "Lee"
    ],
    [
     "Hiroshi",
     "Saruwatari"
    ],
    [
     "Kiyohiro",
     "Shikano"
    ]
   ],
   "title": "Rapid unsupervised speaker adaptation based on multi-template HMM sufficient statistics in noisy environments",
   "original": "i05_0293",
   "page_count": 4,
   "order": 161,
   "p1": "293",
   "pn": "296",
   "abstract": [
    "This paper describes a multi-template unsupervised speaker adaptation based on HMM-Sufficient Statistics. Multiple classdependent models based on gender and age are used to push up the adaptation performance while keeping adaptation time within few seconds with just one arbitrary utterance. Adaptation begins with the estimation of speaker's class from the N-best neighbor speakers using Gaussian Mixture Models (GMM) on the way of speaker selection. The corresponding template model is adopted as a base model. The adapted model is rapidly constructed using the selected HMM-Sufficient Statistics. Experiments in noisy environment conditions with 20dB SNR office, crowd, booth, and car noise are performed. The proposed multi-template method achieved 89.5% word correct rate compared with 88.0% of the conventional single-template method, while the baseline recognition rate without adaptation is 85.7%. Moreover, experiments using Vocal Tract Length Normalization (VTLN) and supervised Maximum Likelihood Linear Regression (MLLR) are also compared.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-161"
  },
  "choi05_interspeech": {
   "authors": [
    [
     "Dong-jin",
     "Choi"
    ],
    [
     "Yung-Hwan",
     "Oh"
    ]
   ],
   "title": "Rapid speaker adaptation for continuous speech recognition using merging eigenvoices",
   "original": "i05_0297",
   "page_count": 4,
   "order": 162,
   "p1": "297",
   "pn": "300",
   "abstract": [
    "Speaker adaptation in eigenvoice space is a popular method for rapid speaker adaptation. To improve the performance of the method and to obtain stabilized results, the number of speakerdependent models should be increased and a greater number of eigenvoices should be re-estimated. However, the huge computation time required to find eigenvoices makes these solutions difficult, especially in a continuous speech recognition system. This paper describes a method to reduce computation time by estimating eigenvoices only for supplementary speaker-dependent models and merging them with the used eigenvoices. Experimental results show that the computation time is reduced by 73.7% while the performance is almost the same when the numbers of speaker-dependent models in two sets to be merged are the same.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-162"
  },
  "visweswariah05_interspeech": {
   "authors": [
    [
     "Karthik",
     "Visweswariah"
    ],
    [
     "Peder",
     "Olsen"
    ]
   ],
   "title": "Feature adaptation using projection of Gaussian posteriors",
   "original": "i05_1785",
   "page_count": 4,
   "order": 163,
   "p1": "1785",
   "pn": "1788",
   "abstract": [
    "In this paper we consider the use of non-linear methods for feature adaptation to reduce the mismatch between test and training conditions. The non-linearity is introduced by using the posteriors of a set of Gaussians to adapt the original features. Parameters are estimated to maximize the likelihood of the test data. The modeling framework used is based on the fMPE models [1]. We observe significant gains (17% relative) on a test data base recorded in a car. We also see significant gains on top of FMLLR (38% relative over the baseline and 8.5% relative over FMLLR).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-163"
  },
  "li05b_interspeech": {
   "authors": [
    [
     "Xiao",
     "Li"
    ],
    [
     "Jeff",
     "Bilmes"
    ],
    [
     "Jonathan",
     "Malkin"
    ]
   ],
   "title": "Maximum margin learning and adaptation of MLP classifiers",
   "original": "i05_1789",
   "page_count": 4,
   "order": 164,
   "p1": "1789",
   "pn": "1792",
   "abstract": [
    "Conventional MLP classifiers used in phonetic recognition and speech recognition may encounter local minima during training, and they often lack an intuitive and flexible adaptation approach. This paper presents a hybrid MLP-SVM classifier and its associated adaptation strategy, where the last layer of a conventional MLP is learned and adapted in the maximum separation margin sense. This structure also provides a support vector based adaptation mechanism which better interpolates between a speaker-independent model and speaker-dependent adaptation data. Preliminary experiments on vowel classification have shown promising results for both MLP learning and adaptation problems.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-164"
  },
  "mandal05_interspeech": {
   "authors": [
    [
     "Arindam",
     "Mandal"
    ],
    [
     "Mari",
     "Ostendorf"
    ],
    [
     "Andreas",
     "Stolcke"
    ]
   ],
   "title": "Leveraging speaker-dependent variation of adaptation",
   "original": "i05_1793",
   "page_count": 4,
   "order": 165,
   "p1": "1793",
   "pn": "1796",
   "abstract": [
    "This work introduces an automatic procedure for determining the size of regression class trees for individual speakers using an ensemble of speaker-level features to control the number of transformations, if any, that should be estimated by maximum likelihood linear regression. Experiments with a state-of-the-art speech recognition system that uses this procedure show improvements in word error rate for conversational telephone speech.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-165"
  },
  "hsiao05_interspeech": {
   "authors": [
    [
     "Roger",
     "Hsiao"
    ],
    [
     "Brian",
     "Mak"
    ]
   ],
   "title": "A comparative study of two kernel eigenspace-based speaker adaptation methods on large vocabulary continuous speech recognition",
   "original": "i05_1797",
   "page_count": 4,
   "order": 166,
   "p1": "1797",
   "pn": "1800",
   "abstract": [
    "Eigenvoice (EV) speaker adaptation has been shown effective for fast speaker adaptation when the amount of adaptation data is scarce. In the past two years, we have been investigating the application of kernel methods to improve EV speaker adaptation by exploiting possible nonlinearity in the speaker space, and two methods were proposed: embedded kernel eigenvoice (eKEV) and kernel eigenspace-based MLLR (KEMLLR). In both methods, kernel PCA is used to derive eigenvoices in the kernel-induced high-dimensional feature space, and they differ mainly in the representation of the speaker models. Both had been shown to outperform all other common adaptation methods when the amount of adaptation data is less than 10s. However, in the past, only small vocabulary speech recognition tasks were tried since we were not familiar with the behaviour of these kernelized methods. As we gain more experience, we are now ready to tackle larger vocabularies. In this paper, we show that both methods continue to outperform MAP, and MLLR when only 5s or 10s of adaptation data are available on the WSJ0 5K-vocabulary task. Compared with the speaker-independent model, the two methods reduce recognition word error rate by 13.4%-21.1%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-166"
  },
  "wang05c_interspeech": {
   "authors": [
    [
     "Xuechuan",
     "Wang"
    ],
    [
     "Douglas",
     "O'Shaughnessy"
    ]
   ],
   "title": "Environmental compensation using ASR model adaptation by a Bayesian parametric representation method",
   "original": "i05_1801",
   "page_count": 4,
   "order": 167,
   "p1": "1801",
   "pn": "1804",
   "abstract": [
    "The mismatch between system training and operating conditions can seriously deteriorate the performance of ASR systems. The maximum a posteriori (MAP) estimation is used for the adaptation of HMM-based multivariate Gaussian mixture models (GMMs). In this paper, we propose an environment independent ASR model parameter adaptation approach based on Bayesian parametric representation (BPR). Compared to the MAP method, the BPR adaptation method has better performance with limited adaptation data. The performances of the two methods are investigated in the experiments designed on the AURORA 2 noisy speech database.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-167"
  },
  "luo05_interspeech": {
   "authors": [
    [
     "Jun",
     "Luo"
    ],
    [
     "Zhijian",
     "Ou"
    ],
    [
     "Zuoying",
     "Wang"
    ]
   ],
   "title": "Discriminative speaker adaptation with eigenvoices",
   "original": "i05_1805",
   "page_count": 4,
   "order": 168,
   "p1": "1805",
   "pn": "1808",
   "abstract": [
    "Eigenvoice is an effective speaker adaptation approach and capable of balancing the performance and the requirement for a large amount of adaptation data. However, the conventional Maximum Likelihood Eigen-Decomposition (MLED) method in eigenvoice adaptation is based on Maximum Likelihood (ML) criterion and suffers from the unrealistic assumption made by HMM on speech process, so alternative schemes may be more effective to improve the performance. In this paper, we propose a new discriminative adaptation algorithm called Maximum Mutual Information Eigen- Decomposition (MMIED) in which the mutual information between the training word sequences and the observation sequences is maximized. By the use of word lattice, the competing word hypotheses are taken into account to make the estimation more discriminative. MLED, MMIED and Maximum a Posteriori Eigen-Decomposition (MAPED) which is based on Maximum a Posteriori (MAP) criterion were all experimented to give a comprehensive comparison. Results showed that MMIED outperformed both MLED and MAPED.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-168"
  },
  "liu05c_interspeech": {
   "authors": [
    [
     "Jian",
     "Liu"
    ],
    [
     "Thomas Fang",
     "Zheng"
    ],
    [
     "Jing",
     "Deng"
    ],
    [
     "Wenhu",
     "Wu"
    ]
   ],
   "title": "Real-time pitch tracking based on combined SMDSF",
   "original": "i05_0301",
   "page_count": 4,
   "order": 169,
   "p1": "301",
   "pn": "304",
   "abstract": [
    "This paper presents a novel pitch tracking method in the time domain. Based on the difference function as used in YIN - referred to as the sum magnitude difference square function (SMDSF) thereinafter - we propose two modified types of SMDSFs, with several methods presented to calculate these SMDSFs efficiently and without bias by using the FFT algorithm. In pitch estimation, every type of SMDSF has its own estimation error characteristics. By analyzing these characteristics, we define a new function which combines the foresaid two types of SMDSFs to prevent estimation errors. A new, relatively accurate, and real-time pitch tracking algorithm is then proposed which does not need any extra preprocessing and post-processing. Experimental results show that this proposed algorithm can achieve remarkably good performance for pitch tracking.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-169"
  },
  "banhalmi05_interspeech": {
   "authors": [
    [
     "András",
     "Bánhalmi"
    ],
    [
     "Kornél",
     "Kovács"
    ],
    [
     "András",
     "Kocsor"
    ],
    [
     "László",
     "Tóth"
    ]
   ],
   "title": "Fundamental frequency estimation by least-squares harmonic model fitting",
   "original": "i05_0305",
   "page_count": 4,
   "order": 170,
   "p1": "305",
   "pn": "308",
   "abstract": [
    "This paper proposes a pitch estimation algorithm that is based on optimal harmonic model fitting. The algorithm operates directly on the time-domain signal and has a relatively simple mathematical background. To increase its efficiency and accuracy, the algorithm is applied in combination with an autocorrelation-based initialization phase. For testing purposes we compare its performance on pitch-annotated corpora with several conventional time-domain pitch estimation algorithms, and also with a recently proposed one. The results show that even the autocorrelation-based first phase significantly outperforms the traditional methods, and also slightly the recently proposed yin algorithm. After applying the second phase - the harmonic approximation step - the amount of errors can be further reduced by about 20% relative to the error obtained in the first phase.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-170"
  },
  "lee05_interspeech": {
   "authors": [
    [
     "S. W.",
     "Lee"
    ],
    [
     "Frank K.",
     "Soong"
    ],
    [
     "P. C.",
     "Ching"
    ]
   ],
   "title": "Harmonic filtering for joint estimation of pitch and voiced source with single-microphone input",
   "original": "i05_0309",
   "page_count": 4,
   "order": 171,
   "p1": "309",
   "pn": "312",
   "abstract": [
    "Standard correlation based methods are not effective in estimating pitch tracks of multiple speech sources from a single-microphone input In this paper, an adaptive harmonic filtering is proposed to jointly estimate the source signals and their corresponding fundamental frequencies. By exploiting the harmonic structure of voiced speech, pitch information of one source is extracted from the pitch prediction filter and the output residual becomes the estimate of the other source. The procedure is iterated successively with a summation constraint. From the evolution of pitch prediction filter, it is shown that the iterative harmonic filtering with the summation constraint is effective to separate multiple pitch tracks into individual ones.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-171"
  },
  "kepesi05_interspeech": {
   "authors": [
    [
     "Marián",
     "Képesi"
    ],
    [
     "Luis",
     "Weruaga"
    ]
   ],
   "title": "High-resolution noise-robust spectral-based pitch estimation",
   "original": "i05_0313",
   "page_count": 4,
   "order": 172,
   "p1": "313",
   "pn": "316",
   "abstract": [
    "This paper introduces a new spectral representation-based pitch estimation method. Since pitch is never stationary during real conversations, but often undergoes changes because of intonation, the spectral representation is derived from the Short-time Harmonic Chirp Transform. This lets our technique to perform very well in noisy conditions, and to extract pitch values with high confidence, even from segments with strong intonations. The paper discusses a new way of segment-vice pitch extraction and does not deal with continuous pitch tracking, which is a topic of our future work. However, the performance of the proposed method is demonstrated on real recordings and the noise-dependency of its accuracy is numerically analyzed.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-172"
  },
  "hosom05_interspeech": {
   "authors": [
    [
     "John-Paul",
     "Hosom"
    ]
   ],
   "title": "F0 estimation for adult and children's speech",
   "original": "i05_0317",
   "page_count": 4,
   "order": 173,
   "p1": "317",
   "pn": "320",
   "abstract": [
    "While there are numerous methods for estimating the fundamental frequency (F0) of speech, existing methods often suffer from pitch doubling or halving errors. Heuristics can be added to constrain the range of allowable F0 values, but it is still difficult to appropriately set the algorithm parameters if one does not know in advance the speaker's age or gender. The proposed method is distinct from most other F0-estimation algorithms in that it does not use autocorrelation, cepstral, or pattern-recognition techniques. Instead, information from 32 band-pass filters is combined at every frame, a Viterbi search provides an initial F0-contour estimate, and this estimate is then refined based on intensity discrimination of the speech signal. Despite the use of a large number of filters (which provide complementary information and hence robustness), the implementation works in less than real-time on a 2.4 GHz processor without optimization for processing speed. Results are presented for two corpora, one corpus of an adult male and one of children of different ages. For the first corpus, average absolute error is 4.10 Hz (percent error of 4.15%); for the second corpus, average absolute error is 7.74 Hz (percent error of 3.38%).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-173"
  },
  "milner05_interspeech": {
   "authors": [
    [
     "Ben",
     "Milner"
    ],
    [
     "Xu",
     "Shao"
    ],
    [
     "Jonathan",
     "Darch"
    ]
   ],
   "title": "Fundamental frequency and voicing prediction from MFCCs for speech reconstruction from unconstrained speech",
   "original": "i05_0321",
   "page_count": 4,
   "order": 174,
   "p1": "321",
   "pn": "324",
   "abstract": [
    "This work proposes a method to predict the fundamental frequency and voicing of a frame of speech from its MFCC representation. This has particular use in distributed speech recognition systems where the ability to predict fundamental frequency and voicing allows a time-domain speech signal to be reconstructed solely from the MFCC vectors. Prediction is achieved by modeling the joint density of MFCCs and fundamental frequency with a combined hidden Markov model-Gaussian mixture model (HMM-GMM) framework. Prediction results are presented on unconstrained speech using both a speaker-dependent database and a speaker-independent database. Spectrogram comparisons of the reconstructed and original speech are also made. The results show for the speaker-dependent task a percentage fundamental frequency prediction error of 3.1% is made while for the speaker-independent task this rises to 8.3%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-174"
  },
  "barbot05_interspeech": {
   "authors": [
    [
     "N.",
     "Barbot"
    ],
    [
     "Olivier",
     "Boëffard"
    ],
    [
     "D.",
     "Lolive"
    ]
   ],
   "title": "F0 stylisation with a free-knot b-spline model and simulated-annealing optimization",
   "original": "i05_0325",
   "page_count": 4,
   "order": 175,
   "p1": "325",
   "pn": "328",
   "abstract": [
    "This article describes a F0 curve estimation scheme based on a B-spline model. We compare this model with more classical spline representation. The free parameters of both models are the number of knots and their location. An optimal location is proposed using a simulated annealing strategy. Experiments on real F0 curves confirm the adequacy and good performance of the B-spline approach, estimated via the least-square error criterion.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-175"
  },
  "drepper05_interspeech": {
   "authors": [
    [
     "F. R.",
     "Drepper"
    ]
   ],
   "title": "Voiced excitation as entrained primary response of a reconstructed glottal master oscillator",
   "original": "i05_0329",
   "page_count": 4,
   "order": 176,
   "p1": "329",
   "pn": "332",
   "abstract": [
    "A time scale separation of voiced speech signals is introduced, which avoids the assumption of a frequency gap between the acoustic response and the prosodic drive. The non-stationary drive is extracted selfconsistently from a voice specific subband decomposition of the speech signal. When the band limited prosodic drive is used as fundamental drive of a two-level drive-response model, the voiced excitation can be reconstructed as a trajectory on a generalized synchronization manifold, which is suited to serve as cue for phoneme recognition and as fingerprint for speaker recognition.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-176"
  },
  "vincent05_interspeech": {
   "authors": [
    [
     "Damien",
     "Vincent"
    ],
    [
     "Olivier",
     "Rosec"
    ],
    [
     "Thierry",
     "Chonavel"
    ]
   ],
   "title": "Estimation of LF glottal source parameters based on an ARX model",
   "original": "i05_0333",
   "page_count": 4,
   "order": 177,
   "p1": "333",
   "pn": "336",
   "abstract": [
    "We propose a method to estimate the glottal flow based on the ARX model of speech production and on the LF model of glottal flow. This method splits the analysis in two stages: a low frequency analysis to estimate the glottal source parameters which have mainly a low pass effect and a second step to refine the parameters which have also a high pass effect. Along with this new analysis scheme, we introduce a new algorithm to efficiently minimize the nonlinear function resulting from the least square criterion applied to the ARX model. Results on synthetic and natural speech signals prove the effectiveness of the proposed method.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-177"
  },
  "alsteris05_interspeech": {
   "authors": [
    [
     "Leigh D.",
     "Alsteris"
    ],
    [
     "Kuldip K.",
     "Paliwal"
    ]
   ],
   "title": "Some experiments on iterative reconstruction of speech from STFT phase and magnitude spectra",
   "original": "i05_0337",
   "page_count": 4,
   "order": 178,
   "p1": "337",
   "pn": "340",
   "abstract": [
    "In our earlier work, we have measured human intelligibility of stimuli reconstructed either from the short-time magnitude spectra or short-time phase spectra of a speech signal. We demonstrated that, even for small analysis window durations of 20-40 ms (of relevance to automatic speech recognition), the short-time phase spectrum can contribute to speech intelligibility as much as the short-time magnitude spectrum. Reconstruction was performed by overlap-addition of modified short-time segments, where each segment had either the magnitude or the phase of the corresponding original speech segment. In this paper, we employ an iterative framework for signal reconstruction. With this framework, we see that a signal can be reconstructed to within a scale factor when only phase is known, while this is not the case for magnitude. The magnitude must be accompanied by sign information (i.e., one bit of phase information) for unique reconstruction. In the absence of all magnitude information, we explore how much phase information is required for intelligible signal reconstruction. We observe that (i) intelligible signal reconstruction (albeit noisy) is possible from knowledge of only the phase sign information, and (ii) when both time and frequency derivatives of phase are known, adequate information is available for intelligible signal reconstruction. In the absence of either derivative, an unintelligible signal results.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-178"
  },
  "muralishankar05_interspeech": {
   "authors": [
    [
     "R.",
     "Muralishankar"
    ],
    [
     "Abhijeet",
     "Sangwan"
    ],
    [
     "Douglas",
     "O'Shaughnessy"
    ]
   ],
   "title": "Statistical properties of the warped discrete cosine transform cepstrum compared with MFCC",
   "original": "i05_0341",
   "page_count": 4,
   "order": 179,
   "p1": "341",
   "pn": "344",
   "abstract": [
    "In this paper, we continue our investigation of the warped discrete cosine transform cepstrum (WDCTC), which was earlier introduced as a new speech processing feature [1]. Here, we study the statistical properties of the WDCTC and compare them with the mel-frequency cepstral coefficients (MFCC).We report some interesting properties of the WDCTC when compared to the MFCC: its statistical distribution is more Gaussian-like with lower variance, it obtains better vowel cluster separability, it forms tighter vowel clusters and generates better codebooks. Further, we employ the WDCTC and MFCC features in a 5-vowel recognition task using Vector Quantization (VQ) and 1-Nearest Neighbour (1-NN) as classifiers. In our experiments, the WDCTC consistently outperforms the MFCC.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-179"
  },
  "ferreira05_interspeech": {
   "authors": [
    [
     "Aníbal J. S.",
     "Ferreira"
    ]
   ],
   "title": "New signal features for robust identification of isolated vowels",
   "original": "i05_0345",
   "page_count": 4,
   "order": 180,
   "p1": "345",
   "pn": "348",
   "abstract": [
    "Current signal processing techniques do not match the astonishing ability of the Human Auditory System in recognizing isolated vowels, particularly in the case of female or child speech. As didactic and clinical interactive applications are needed using sound as the main medium of interaction, new signal features must be used that capture important perceptual cues more effectively than popular features such as formants. In this paper we propose the new concept of Perceptual Spectral Cluster (PSC) and describe its implementation. Test results are presented for child and adult speech, and indicate that features elicited by the PSC concept permit reliable and robust identification of vowels, even at high pitches.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-180"
  },
  "pincas05_interspeech": {
   "authors": [
    [
     "Jonathan",
     "Pincas"
    ],
    [
     "Philip J. B.",
     "Jackson"
    ]
   ],
   "title": "Amplitude modulation of frication noise by voicing saturates",
   "original": "i05_0349",
   "page_count": 4,
   "order": 181,
   "p1": "349",
   "pn": "352",
   "abstract": [
    "The two distinct sound sources comprising voiced frication, voicing and frication, interact. One effect is that the periodic source at the glottis modulates the amplitude of the frication source originating in the vocal tract above the constriction. Voicing strength and modulation depth for frication noise were measured for sustained English voiced fricatives using high-pass filtering, spectral analysis in the modulation (envelope) domain, and a variable pitch compensation procedure. Results show a positive relationship between strength of the glottal source and modulation depth at voicing strengths below 66 dB SPL, at which point the modulation index was approximately 0.5 and saturation occurred. The alveolar [z] was found to be more modulated than other fricatives.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-181"
  },
  "hecht05_interspeech": {
   "authors": [
    [
     "Ron M.",
     "Hecht"
    ],
    [
     "Naftali",
     "Tishby"
    ]
   ],
   "title": "Extraction of relevant speech features using the information bottleneck method",
   "original": "i05_0353",
   "page_count": 4,
   "order": 182,
   "p1": "353",
   "pn": "356",
   "abstract": [
    "We propose a novel approach to the design of efficient representations of speech for various recognition tasks. Using a principled information theoretic framework - the Information Bottleneck method - which enables quantization that preserves relevant information, we demonstrate that significantly smaller representations of the signal can be obtained that still capture most of the relevant information about phonemes or speakers. The significant implications for building more efficient speech and speaker recognition systems are discussed.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-182"
  },
  "firouzmand05_interspeech": {
   "authors": [
    [
     "Mohammad",
     "Firouzmand"
    ],
    [
     "Laurent",
     "Girin"
    ],
    [
     "Sylvain",
     "Marchand"
    ]
   ],
   "title": "Comparing several models for perceptual long-term modeling of amplitude and phase trajectories of sinusoidal speech",
   "original": "i05_0357",
   "page_count": 4,
   "order": 183,
   "p1": "357",
   "pn": "360",
   "abstract": [
    "The so-called Long-Term (LT) modeling of sinusoidal parameters, proposed in previous papers, consists in modeling the entire timetrajectory of amplitude and phase parameters over large sections of voiced speech, differing from usual Short- Term models, which are defined on a frame-by-frame basis. In the present paper, we focus on a specific novel contribution to this general framework: the comparison of four different Long- Term models, namely a polynomial model, a model based on discrete cosine functions, and combinations of discrete cosine with sine functions or polynomials. Their performances are compared in terms of synthesis signal quality, data compression and modeling accuracy, and the interest of the presented study for speech coding is shown.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-183"
  },
  "hermansky05_interspeech": {
   "authors": [
    [
     "Hynek",
     "Hermansky"
    ],
    [
     "Petr",
     "Fousek"
    ]
   ],
   "title": "Multi-resolution RASTA filtering for TANDEM-based ASR",
   "original": "i05_0361",
   "page_count": 4,
   "order": 184,
   "p1": "361",
   "pn": "364",
   "abstract": [
    "New speech representation based on multiple filtering of temporal trajectories of speech energies in frequency sub-bands is proposed and tested. The technique extends earlier works on delta features and RASTA filtering by processing temporal trajectories by a bank of band-pass filters with varying resolutions. In initial tests on OGI Digits database the technique yields about 30% relative improvement in word error rate over the conventional PLP features. Since the applied filters have zero-mean impulse responses, the technique is inherently robust to linear distortions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-184"
  },
  "jeon05_interspeech": {
   "authors": [
    [
     "Woojay",
     "Jeon"
    ],
    [
     "Biing-Hwang",
     "Juang"
    ]
   ],
   "title": "A category-dependent feature selection method for speech signals",
   "original": "i05_0365",
   "page_count": 4,
   "order": 185,
   "p1": "365",
   "pn": "368",
   "abstract": [
    "We present a novel method of dimension reduction and feature selection that makes use of category-dependent regions in highdimensional data. Our method is inspired by phoneme-dependent, noise-robust low-variance regions observed in the cortical response, and introduces the notion of category-dependence in a two-step dimension reduction process that draws on the fundamental principles of Fisher Linear Discriminant Analysis. As a method of applying these features in an actual pattern classification task, we construct a system of multiple speech recognizers that are combined by a Bayesian decision rule under some simplifying assumptions. The results show a significant increase in recognition rate for low signal-to-noise ratios compared with previous methods, providing motivation for further study on hierarchical, category-dependent recognition and detection.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-185"
  },
  "kristjansson05_interspeech": {
   "authors": [
    [
     "Trausti",
     "Kristjansson"
    ],
    [
     "Sabine",
     "Deligne"
    ],
    [
     "Peder",
     "Olsen"
    ]
   ],
   "title": "Voicing features for robust speech detection",
   "original": "i05_0369",
   "page_count": 4,
   "order": 186,
   "p1": "369",
   "pn": "372",
   "abstract": [
    "Accurate speech activity detection is a challenging problem in the car environment where high background noise and high amplitude transient sounds are common. We investigate a number of features that are designed for capturing the harmonic structure of speech. We evaluate separately three important characteristics of these features: 1) discriminative power 2) robustness to greatly varying SNR and channel characteristics and 3) performance when used in conjunction with MFCC features. We propose a new features, the Windowed Autocorrelation Lag Energy (WALE) which has desirable properties.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-186"
  },
  "gomez05b_interspeech": {
   "authors": [
    [
     "Pedro",
     "Gomez"
    ],
    [
     "Francisco",
     "Diaz"
    ],
    [
     "Agustin",
     "Alvarez"
    ],
    [
     "Rafael",
     "Martinez"
    ],
    [
     "Victoria",
     "Rodellar"
    ],
    [
     "Roberto",
     "Fernandez-Baillo"
    ],
    [
     "Alberto",
     "Nieto"
    ],
    [
     "Francisco J.",
     "Fernandez"
    ]
   ],
   "title": "PCA of perturbation parameters in voice pathology detection",
   "original": "i05_0645",
   "page_count": 4,
   "order": 187,
   "p1": "645",
   "pn": "648",
   "abstract": [
    "Voice pathology detection and classification is a special research field in voice and speech processing for its deep social impact [8]. Historically in the development of new tools for pathology detection, different sets of distortion parameters have been defined, on one hand those estimating perturbations of certain acoustic voice features such as the pitch or energy, others estimating the dispersion of the spectral density of voice. Parameters based on the estimation of the residual of the glottal source related with the mucosal wave are of special interest among these last ones, as it may be shown these to be related with the biomechanical behavior of the vocal cords [2][3]. Therefore the amount of available parameters for pathology detection and classification is rather high. Although not all of them may have the same relevance depending on the specific objective to be covered. The present work is aimed to stress the relevance of different parameters for voice pathology detection using pruning techniques based on Principal Component Analysis. Specific experiments are used to stress the relevance of the parameters in differentiating normophonic and pathologic cases. Possible applications of the method to classify among pathologies could be derived from this study.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-187"
  },
  "sarkar05_interspeech": {
   "authors": [
    [
     "Anindya",
     "Sarkar"
    ],
    [
     "T. V.",
     "Sreenivas"
    ]
   ],
   "title": "Dynamic programming based segmentation approach to LSF matrix reconstruction",
   "original": "i05_0649",
   "page_count": 4,
   "order": 188,
   "p1": "649",
   "pn": "652",
   "abstract": [
    "We propose a methodology of speech segmentation in which the LSF feature vector matrix of a segment is reconstructed optimally using a set of parametric/non-parametric functions. We have explored approximations using basis functions or polynomials. We have analyzed the performance of these methods w.r.t. phoneme segmentation (on 100 TIMIT sentences) and reconstruction error based on spectral distortion (SD) measure. We study how amenable these methods are to quantization and their suitability for speech coding. We also estimate the optimum number of segments depending on the reconstruction performance achieved using that many number of segments and the tolerance limit set on the spectral distortion error.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-188"
  },
  "nagarajan05_interspeech": {
   "authors": [
    [
     "T.",
     "Nagarajan"
    ],
    [
     "Douglas",
     "O'Shaughnessy"
    ]
   ],
   "title": "Explicit segmentation of speech based on frequency-domain AR modeling",
   "original": "i05_0653",
   "page_count": 4,
   "order": 189,
   "p1": "653",
   "pn": "656",
   "abstract": [
    "In the development of a syllable-centric Automatic Speech Recognition (ASR) system, segmentation of the speech signal into syllabic units is an important stage. In [1], an implicit algorithm is presented for segmenting the continuous speech signal into syllable-like units, in which the orthographic transcription is not used. In the present study, a new explicit segmentation algorithm is proposed and analyzed that uses the orthographic transcription of the given continuous speech signal. The advantage of using the transcription during segmentation is that the number of syllable segments present in the speech signal can be known a priori. Although the short-term energy (STE) function contains useful information about syllable segment boundaries, it cannot be directly used to perform segmentation due to significant local energy fluctuations. In the present work, an Auto-Regressive model-based algorithm is presented which essentially smooths the STE function using the knowledge of the number of syllable segments required/present in the given speech signal. Experiments carried out on the TIMIT speech corpus show that the error in segmentation is at most 40 ms for 87.84% of the syllable segments.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-189"
  },
  "motlicek05_interspeech": {
   "authors": [
    [
     "Petr",
     "Motlícek"
    ],
    [
     "Lukás",
     "Burget"
    ],
    [
     "Jan",
     "Cernocký"
    ]
   ],
   "title": "Non-parametric speaker turn segmentation of meeting data",
   "original": "i05_0657",
   "page_count": 4,
   "order": 190,
   "p1": "657",
   "pn": "660",
   "abstract": [
    "An extension of conventional speaker segmentation framework is presented for a scenario in which a number of microphones record the activity of speakers present at a meeting (one microphone per speaker). Although each microphone can receive speech from both the participant wearing the microphone (local speech) and other participants (cross-talk), the recorded audio can be broadly classified in three ways: local speech, cross-talk, and silence. This paper proposes a technique which takes into account cross-correlations, values of its maxima, and energy differences as features to identify and segment speaker turns. In particular, we have used classical cross-correlation functions, time smoothing and in part temporal constraints to sharpen and disambiguate timing differences between microphone channels that may be dominated by noise and reverberation. Experimental results show that proposed technique can be successively used for speaker segmentation of data collected from a number of different setups.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-190"
  },
  "korhonen05_interspeech": {
   "authors": [
    [
     "Petri",
     "Korhonen"
    ],
    [
     "Unto K.",
     "Laine"
    ]
   ],
   "title": "Unsupervised segmentation of continuous speech using vector autoregressive time-frequency modeling errors",
   "original": "i05_0661",
   "page_count": 4,
   "order": 191,
   "p1": "661",
   "pn": "664",
   "abstract": [
    "A vector autoregressive (VAR) model is used in the auditory time-frequency domain to predict spectral changes. Forward and backward prediction errors increases at the phone boundaries. These error signals are then used to study and detect the boundaries of the largest changes allowing the most reliable automatic segmentation. Using a fully unsupervised method yields segments consisting of a variable number of phones. The quality of performance of this method was tested with a set of 150 Finnish sentences pronounced by one female and two male speakers. The performance for English was tested using the TIMIT core test set. The boundaries between stops and vowels, in particular, are detected with high probability and precision.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-191"
  },
  "vijayalakshmi05_interspeech": {
   "authors": [
    [
     "P.",
     "Vijayalakshmi"
    ],
    [
     "M.",
     "RamasubbaReddy"
    ]
   ],
   "title": "The analysis on band-limited hypernasal speech using group delay based formant extraction technique",
   "original": "i05_0665",
   "page_count": 4,
   "order": 192,
   "p1": "665",
   "pn": "668",
   "abstract": [
    "Speakers with defective velopharyngeal mechanism, produce speech with inappropriate nasal resonances across vowel sounds. The acoustic analysis on hypernasal speech and nasalized vowels of normal speech shows that there is an additional frequency introduced in the low frequency region close to the first formant frequency [1]. The conventional formant extraction techniques may fail to resolve closely spaced formants. In this paper, an attempt is made to use the group delay based algorithm [2] for the extraction of formant frequencies from hypernasal speech. Preliminary experiments on synthetic signal with closely spaced formants show that the formants are better resolved in group delay spectrum when compared to conventional methods. But when formants are too close with wider bandwidths, the group delay algorithm also fails to resolve prominently. This is primarily because of the influence of the other resonances in the signal. To extract the additional frequency close to the first formant, the speech signal is low-pass filtered and the formants are extracted using group delay function. Following the satisfactory results on synthetic signal, the above technique is used to extract formants from phonations /a/, /i/, and /u/ uttered by 15 speakers with cleft palate who are expected to produce hypernasal speech. Invariably in all the tests, an additional nasal resonance around 250 Hz and first formant frequency of vowels are resolved properly.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-192"
  },
  "zdansky05_interspeech": {
   "authors": [
    [
     "Jindrich",
     "Zdánský"
    ],
    [
     "Jan",
     "Nouza"
    ]
   ],
   "title": "Detection of acoustic change-points in audio records via global BIC maximization and dynamic programming",
   "original": "i05_0669",
   "page_count": 4,
   "order": 193,
   "p1": "669",
   "pn": "672",
   "abstract": [
    "In this paper we propose a novel method for the detection of relevant changes in continuous acoustic stream. The aim is to identify the optimal number and position of the change-points that split the signal into shorter, more or less homogeneous sections. First we describe the theory we used to derive the segmentation algorithm. Then we show how this algorithm can be implemented efficiently. Evaluation is done on broadcast news data with the goal to segment it into parts belonging to different speakers. In simulated tests with artificially mixed utterances the algorithm identified 97.1% of all speaker changes with precision of 96.5%. In tests done with 30 hours of real broadcast news (in 9 languages) the average recall was 80% and precision 72.3%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-193"
  },
  "molla05_interspeech": {
   "authors": [
    [
     "Md. Khademul Islam",
     "Molla"
    ],
    [
     "Keikichi",
     "Hirose"
    ],
    [
     "Nobuaki",
     "Minematsu"
    ]
   ],
   "title": "Multi-band approach of audio source discrimination with empirical mode decomposition",
   "original": "i05_0673",
   "page_count": 4,
   "order": 194,
   "p1": "673",
   "pn": "676",
   "abstract": [
    "This paper presents a content-based approach of audio source indexing without any prior knowledge about the sources. The empirical mode decomposition (EMD) scheme, capable of decomposing nonlinear and non-stationary signals into some bases, is employed to implement the sub-band approach of the audio discrimination technique. The feature vectors are derived from each of selected sub-bands of the target frame. Linear predictive cepstrum coefficient (LPCC) is used as the main feature vector and Kullback-Leibler divergence (KLd) is performed as the scoring function to measure the similarity of the feature vectors. The higher order statistics (HOS) is employed to compute the LPCC. The use of HOS makes LPCC less affected by Gaussian noise. The experimental results show that the sub-band approach produces better discrimination efficiency than that of the full-band technique. This discrimination method is also suitable to solve the source permutation ambiguity in separation of multiple and concurrent moving sources from the mixture(s).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-194"
  },
  "tsuzaki05_interspeech": {
   "authors": [
    [
     "Minoru",
     "Tsuzaki"
    ],
    [
     "Satomi",
     "Tanaka"
    ],
    [
     "Hiroaki",
     "Kato"
    ],
    [
     "Yoshinori",
     "Sagisaka"
    ]
   ],
   "title": "Application of auditory image model for speech event detection",
   "original": "i05_0677",
   "page_count": 4,
   "order": 195,
   "p1": "677",
   "pn": "680",
   "abstract": [
    "To provide an appropriate model for perception of temporal structures of speech, we applied a comprehensive computational model of the human auditory peripherals to detect changes in speech signals that potentially indicate arrivals of new events. In each tonotopic sub-band, an increase in the activation level was taken into account for the plausibility of a new event, while a decrease was ignored. The total contour obtained by integrating the sub-band information exhibited sharp peaks and dips compared to the loudness contour. A quantitative evaluation to estimate the speaking rate of natural speech also demonstrated that the event-plausibility model performs better than the loudness model.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-195"
  },
  "arias05_interspeech": {
   "authors": [
    [
     "José Anibal",
     "Arias"
    ]
   ],
   "title": "Unsupervised identification of speech segments using kernel methods for clustering",
   "original": "i05_0681",
   "page_count": 4,
   "order": 196,
   "p1": "681",
   "pn": "684",
   "abstract": [
    "Kernel k-means and spectral clustering have been used to separate input space clusters by means of non-linear mappings. In this paper we adapt and extend these methods to identify constitutive units of speech: consonants, vowels and silences. The discover of this structure is very useful for prosody-based systems of automatic language identification or language disorders detection. In order to find stable speech segments, infra-phonetic segmentation is performed using the divergence forward-backward algorithm. Our test corpus is a six-languages subset of OGI_MLTS corpus. We present better classification results than traditional approaches as well as faster processing times.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-196"
  },
  "evangelopoulos05_interspeech": {
   "authors": [
    [
     "Georgios",
     "Evangelopoulos"
    ],
    [
     "Petros",
     "Maragos"
    ]
   ],
   "title": "Speech event detection using multiband modulation energy",
   "original": "i05_0685",
   "page_count": 4,
   "order": 197,
   "p1": "685",
   "pn": "688",
   "abstract": [
    "The need for efficient, sophisticated features for speech event detection is inherent in state of the art processing, enhancement and recognition systems. We explore ideas and techniques from non-linear speech modeling and analysis, like modulations and multiband filtering and propose new energy and spectral content features derived through filtering in multiple frequency bands and tracking dominant modulation energy in terms of the Teager- Kaiser Energy of separate AM-FM components. We present a detection-theoretic motivation and incorporate them in two detection schemes namely word boundary and voice activity detection. The modulation approach demonstrated noisy speech endpoint detection accuracy, reaching ¡«40% error reduction on NTIMIT. In a voice activity scheme, improvement in overall misclassification error of a high hit-rate detector reached 7.5% on Aurora 2 and 9.5% on Aurora 3 databases.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-197"
  },
  "kominek05b_interspeech": {
   "authors": [
    [
     "John",
     "Kominek"
    ],
    [
     "Alan W.",
     "Black"
    ]
   ],
   "title": "Measuring unsupervised acoustic clustering through phoneme pair merge-and-split tests",
   "original": "i05_0689",
   "page_count": 4,
   "order": 198,
   "p1": "689",
   "pn": "692",
   "abstract": [
    "Subphonetic discovery through segmental clustering is a central step in building a corpus-based synthesizer. To help decide what clustering algorithm to use we employed merge-and-split tests on English fricatives. Compared to reference of 2%, Gaussian EM achieved a misclassification rate of 6%, K-means 10%, while predictive CART trees performed poorly.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-198"
  },
  "valente05_interspeech": {
   "authors": [
    [
     "Fabio",
     "Valente"
    ],
    [
     "Christian",
     "Wellekens"
    ]
   ],
   "title": "Variational Bayesian speaker change detection",
   "original": "i05_0693",
   "page_count": 4,
   "order": 199,
   "p1": "693",
   "pn": "696",
   "abstract": [
    "In this paper we study the use of Variational Bayesian (VB) methods for speaker change detection and we compare results with the classical BIC solution. VB methods are approximated learning algorithms for fully bayesian inference that cannot be achieved in an exact form. They embed in the objective function (also known as free energy) a term that penalizes more complex models. Experiments are run on the Hub4 1996 evaluation data set and show that the VB outperforms the BIC of almost 7%. Anyway as long as the decision must be taken on a limited amount of data the VB based method must be tuned as the BIC based method in order to produce reasonable results.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-199"
  },
  "borys05_interspeech": {
   "authors": [
    [
     "Sarah",
     "Borys"
    ],
    [
     "Mark",
     "Hasegawa-Johnson"
    ]
   ],
   "title": "Distinctive feature based SVM discriminant features for improvements to phone recognition on telephone band speech",
   "original": "i05_0697",
   "page_count": 4,
   "order": 200,
   "p1": "697",
   "pn": "700",
   "abstract": [
    "Support vector machines (SVM's) can be trained to classify manner transitions between phones and to identify the place of articulation of any given phone with high accuracy. The discriminant outputs of these SVM's can be used as input features for a standard ASR system. There is a significant improvement in correctness and accuracy using these SVM discriminant features when compared to an MFCC based recognizer of equal parameters.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-200"
  },
  "vijayalakshmi05b_interspeech": {
   "authors": [
    [
     "P.",
     "Vijayalakshmi"
    ],
    [
     "M.",
     "RamasubbaReddy"
    ]
   ],
   "title": "Detection of hypernasality using statistical pattern classifiers",
   "original": "i05_0701",
   "page_count": 4,
   "order": 201,
   "p1": "701",
   "pn": "704",
   "abstract": [
    "Speakers with velopharyngeal incompetence produce hypernasal speech across voiced elements. Acoustical study [1] on hypernasal speech and nasalized vowels of normal speakers revealed the fact that there is an additional formant frequency introduced in the low-frequency region, close to the first formant of the phonations /a/, /i/, and /u/. Based on this observation, in the current study, the focus is given to the low-frequency region alone, by low-pass filtering the speech signal. From each frame of the given speech signal, a three dimensional feature vector, which comprises of the locations of first two highest frequency peaks in the group delay spectrum and the ratio of the group delay of these frequencies, is extracted. An Accumulated Minimum distance classifier and a Maximum likelihood classifier are trained for each of the phonations separately, and tested to make a decision between normal and hypernasal speakers. For the current study, phonations /a/, /i/, and /u/ uttered by 45 speakers with cleft palate who are expected to produce hypernasal speech, and phonations of 26 normal speakers are considered. Results show that the presence of hypernasality in speech can be detected with 85% of accuracy using the statistical classifiers which use the proposed three dimensional feature vector.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-201"
  },
  "weruaga05_interspeech": {
   "authors": [
    [
     "Luis",
     "Weruaga"
    ],
    [
     "Marián",
     "Képesi"
    ]
   ],
   "title": "Self-organizing chirp-sensitive artificial auditory cortical model",
   "original": "i05_0705",
   "page_count": 4,
   "order": 202,
   "p1": "705",
   "pn": "708",
   "abstract": [
    "This paper presents a novel signal-processing-based artificial model of the auditory mechanism. This work is inspired by the psychocortical fact that the biological cochlea is very sensitive to frequency-varying tones, or chirps. The method uses a novel combination of several (at least three) Harmonic-Chirp transform instances, that project the time-frequency energy on different views; all projections are data-mined by self-organizing unsupervised layers of Radial Basis Functions, this process being driven by the Expectation Maximization algorithm. The mechanism shows biological parallel, such as intrinsic chirp sensitivity and response to the logarithm of the stimulus energy. The proposed model is validated with several mammal sounds, such as human speech, bat echo, and several aquatic mammals.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-202"
  },
  "karabetsos05_interspeech": {
   "authors": [
    [
     "Sotiris",
     "Karabetsos"
    ],
    [
     "Pirros",
     "Tsiakoulis"
    ],
    [
     "Stavroula-Evita",
     "Fotinea"
    ],
    [
     "Ioannis",
     "Dologlou"
    ]
   ],
   "title": "On the use of a decimative spectral estimation method based on eigenanalysis and SVD for formant and bandwidth tracking of speech signals",
   "original": "i05_0709",
   "page_count": 4,
   "order": 203,
   "p1": "709",
   "pn": "712",
   "abstract": [
    "In this paper, a Decimative Spectral estimation method based on Eigenanalysis and SVD (Singular Value Decomposition) is presented and applied to speech signals in order to estimate Formant/Bandwidth values. The underlying model decomposes a signal into complex damped sinusoids. The algorithm is applied not only on speech samples but on a small amount of the autocorrelation coefficients of a speech frame as well, for finer estimation. Correct estimation of Formant/Bandwidth values depend on the model order thus, the requested number of poles. Overall, experimentation results indicate that the proposed methodology successfully estimates formant trajectories and their respective bandwidths.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-203"
  },
  "ivanov05_interspeech": {
   "authors": [
    [
     "Alexei V.",
     "Ivanov"
    ],
    [
     "Marek",
     "Parfieniuk"
    ],
    [
     "Alexander A.",
     "Petrovsky"
    ]
   ],
   "title": "Frequency-domain auditory suppression modelling (FASM) - a WDFT-based anthropomorphic noise-robust feature extraction algorithm for speech recognition",
   "original": "i05_0713",
   "page_count": 4,
   "order": 204,
   "p1": "713",
   "pn": "716",
   "abstract": [
    "This paper presents a physiologically inspired feature extraction algorithm for employment within the speech recognition engines, which are supposed to remain effective in noisy environments. Essentially, the algorithm simulates a key property of the \"active cochlea\" models - a signal dependent variable gain over the frequency range. In order to drastically reduce computational complexity of the algorithm in comparison to the original time domain \"active cochlea\" models, it is implemented in the frequency domain with the help of a warped discrete Fourier transformation (WDFT). The essence of FASM technique is that in the presence of the noise, higher frequency channels get more attenuation if there are \"enough\" signal components in the lower, less susceptible to the noise influence, part of the spectrum. As it is confirmed by the performed measurements FASM algorithm allows to boost feature invariance to noise while keeping feature informativeness at the acceptable level.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-204"
  },
  "gianfelici05_interspeech": {
   "authors": [
    [
     "Francesco",
     "Gianfelici"
    ],
    [
     "Giorgio",
     "Biagetti"
    ],
    [
     "Paolo",
     "Crippa"
    ],
    [
     "Claudio",
     "Turchetti"
    ]
   ],
   "title": "Asymptotically exact AM-FM decomposition based on iterated hilbert transform",
   "original": "i05_1121",
   "page_count": 4,
   "order": 205,
   "p1": "1121",
   "pn": "1124",
   "abstract": [
    "This paper presents a multicomponent sinusoidal model of speech signals, obtained through a rigorous mathematical formulation that ensures an asymptotically exact reconstruction of these nonstationary signals, despite the presence of transients, voiced segments, or unvoiced segments. This result has been obtained by means of the iterated use of the Hilbert transform, and the convergence properties of the proposed method have been both analytically investigated and empirically tested. Finally, an adaptive segmentation algorithm used to accurately compute instantaneous frequencies from unwrapped phases, suited to complete the proposed AM-FM model, is presented.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-205"
  },
  "katsamanis05_interspeech": {
   "authors": [
    [
     "Athanassios",
     "Katsamanis"
    ],
    [
     "Petros",
     "Maragos"
    ]
   ],
   "title": "Advances in statistical estimation and tracking of AM-FM speech components",
   "original": "i05_1125",
   "page_count": 4,
   "order": 206,
   "p1": "1125",
   "pn": "1128",
   "abstract": [
    "In this paper we present of a statistical framework to demodulate speech resonances, which are modeled as AM-FM signals. The first approach utilizes bandpass filtering and a standard demodulation algorithm which regularizes instantaneous amplitude and frequency estimates. The second employs particle filtering techniques to allow temporal variations of the parameters that are connected with spectral characteristics of the analyzed signal. Results are presented on both synthetic and real speech signals and improved performance is demonstrated. Both approaches appear to cope quite satisfactorily with the nonstationarity of speech signals.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-206"
  },
  "darch05_interspeech": {
   "authors": [
    [
     "Jonathan",
     "Darch"
    ],
    [
     "Ben",
     "Milner"
    ],
    [
     "Saeed",
     "Vaseghi"
    ]
   ],
   "title": "Formant frequency prediction from MFCC vectors in noisy environments",
   "original": "i05_1129",
   "page_count": 4,
   "order": 207,
   "p1": "1129",
   "pn": "1132",
   "abstract": [
    "This paper proposes a method of predicting the formant frequencies of a frame of speech from its mel-frequency cepstral coefficient (MFCC) representation. Prediction is achieved through the creation of a Gaussian mixture model (GMM) which models the joint density of formant frequencies and MFCCs. Using this GMM and an input MFCC vector, a maximum a posteriori (MAP) prediction of the formant frequencies is generated. Formant prediction accuracy is evaluated on both a constrained vocabulary connected digits database and on a 5000 word large vocabulary database. Experiments first examine the accuracy of formant frequency prediction as the number of clusters in the GMM is varied with a best formant frequency prediction error of 3.72% being obtained. Secondly the effect of noise on formant prediction accuracy is examined. A fall in accuracy is observed with reducing signal-to-noise ratios, but by using a GMM matched to the noise conditions formant prediction accuracy is significantly improved.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-207"
  },
  "prasanna05_interspeech": {
   "authors": [
    [
     "S. R. Mahadeva",
     "Prasanna"
    ],
    [
     "B.",
     "Yegnanarayana"
    ]
   ],
   "title": "Detection of vowel onset point events using excitation information",
   "original": "i05_1133",
   "page_count": 4,
   "order": 208,
   "p1": "1133",
   "pn": "1136",
   "abstract": [
    "This paper proposes a method for the detection of Vowel Onset Point (VOP) events in speech using excitation information. VOP event is defined as the instant at which the onset of vowel takes place. For syllable-like units such as Consonant Vowel (CV) type, VOP event is the instant at which the consonant ends and the vowel begins. The speech signal is processed by the Linear Prediction (LP) analysis to extract the LP residual. The LP residual mostly contains the excitation information. The Hilbert envelope of the LP residual is derived using the analytic signal concept. A method is developed for detecting the VOP events using the Hilbert envelope of the LP residual and a modulated Gaussian window function. The performance of the proposed method is evaluated using reference VOP markings. The performance of the proposed method is also compared with the existing methods based on the vocal tract system features. The comparison shows that the excitation source also contains significant information about the VOP events.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-208"
  },
  "cabral05_interspeech": {
   "authors": [
    [
     "João P.",
     "Cabral"
    ],
    [
     "Luís C.",
     "Oliveira"
    ]
   ],
   "title": "Pitch-synchronous time-scaling for prosodic and voice quality transformations",
   "original": "i05_1137",
   "page_count": 4,
   "order": 209,
   "p1": "1137",
   "pn": "1140",
   "abstract": [
    "Current time-domain pitch modification techniques have well known limitations for large variations of the original fundamental frequency. This paper proposes a technique for changing the pitch and duration of a speech signal based on time-scaling the linear prediction (LP) residual. The resulting speech signal achieves better quality than the traditional LP-PSOLA method for large fundamental frequency modifications. By using non-uniform time-scaling, this technique can also change the shape of the LP residual for each pitch period. This way we can simulate changes of the most relevant glottal source parameters like the open quotient, the spectral tilt and the asymmetry coefficient. Careful adjustments of these source parameters allows the transformation of the original speech signal so that it is perceived as if it was uttered with a different voice quality or emotion.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-209"
  },
  "ohishi05_interspeech": {
   "authors": [
    [
     "Yasunori",
     "Ohishi"
    ],
    [
     "Masataka",
     "Goto"
    ],
    [
     "Katunobu",
     "Itou"
    ],
    [
     "Kazuya",
     "Takeda"
    ]
   ],
   "title": "Discrimination between singing and speaking voices",
   "original": "i05_1141",
   "page_count": 4,
   "order": 210,
   "p1": "1141",
   "pn": "1144",
   "abstract": [
    "Discriminating between singing and speaking voices by using the local and global characteristics of voice signals is discussed. From the results of subjective experiments, we show that human beings can discriminate singing and speaking voices with more than 70% and 95% accuracy from 300 ms and one second long signals, respectively. From the subjective experiment results, assuming that different features are effective for short-term and long-term signals, we designed two measures using a spectral envelope (MFCC) and the fundamental frequency (F0, perceived as pitch) contour. Experimental results show that the F0 measure performs better than the spectral envelope measure when the input voice signals are longer than one second. Particularly, it can discriminate singing and speaking voices with more than 80% accuracy with two-second signals. On the other hand, when the input signals are shorter than one second, the spectral envelope measure performs better than the F0 measure. Finally, by simply combining the two measures, more than 90% accuracy is obtained for two-second signals.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-210"
  },
  "pettersen05_interspeech": {
   "authors": [
    [
     "Svein G.",
     "Pettersen"
    ],
    [
     "Magne H.",
     "Johnsen"
    ],
    [
     "Tor A.",
     "Myrvoll"
    ]
   ],
   "title": "Joint Bayesian predictive classification and parallel model combination for robust speech recognition",
   "original": "i05_0373",
   "page_count": 4,
   "order": 211,
   "p1": "373",
   "pn": "376",
   "abstract": [
    "In this paper we present an approach that makes use of both Bayesian predictive classification (BPC) and parallel model combination (PMC) to achieve increased robustness towards noise. PMC provides a method for finding parameter estimates for speech corrupted by noise, while BPC is a method that compensates for uncertainty of parameter estimates. Thus, these methods can be combined in order to obtain knowledge about the mismatch situation and simultaneously account for uncertainty in this knowledge. We apply this technique in an unsupervised approach on the Aurora2 database and show that good performance is obtained.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-211"
  },
  "yared05_interspeech": {
   "authors": [
    [
     "Glauco F. G.",
     "Yared"
    ],
    [
     "Fábio",
     "Violaro"
    ],
    [
     "Lívio C.",
     "Sousa"
    ]
   ],
   "title": "Gaussian elimination algorithm for HMM complexity reduction in continuous speech recognition systems",
   "original": "i05_0377",
   "page_count": 4,
   "order": 212,
   "p1": "377",
   "pn": "380",
   "abstract": [
    "Nowadays, HMM-based speech recognition systems are used in many real time processing applications, from cell phones to automobile automation. In this context, one important aspect to be considered is the HMM model size, which directly determines the computational load. So, in order to make the system practical, it is interesting to optimize the HMM model size constrained to a minimum acceptable recognition performance. Furthermore, topology optimization is also important for reliable parameter estimation. Previous works in this area have used likelihood measures in order to obtain models with a better compromise between acoustic resolution and robustness. This work presents a new approach based on a Gaussian Importance Measure (GIM) used in the Gaussian Elimination Algorithm (GEA) for determining the more suitable HMM complexity. The results are compared to the classical Bayesian Information Criterion.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-212"
  },
  "buera05_interspeech": {
   "authors": [
    [
     "Luis",
     "Buera"
    ],
    [
     "Eduardo",
     "Lleida"
    ],
    [
     "Antonio",
     "Miguel"
    ],
    [
     "Alfonso",
     "Ortega"
    ]
   ],
   "title": "Robust speech recognition in cars using phoneme dependent multi-environment linear normalization",
   "original": "i05_0381",
   "page_count": 4,
   "order": 213,
   "p1": "381",
   "pn": "384",
   "abstract": [
    "In this paper a Phoneme-Dependent Multi-Environment Models based LInear feature Normalization, PD-MEMLIN, is presented. The target of this algorithm is to learn the difference between clean and noisy feature vectors associated to a pair of gaussians of the same phoneme (one for a clean model, and the other one for a noisy model), for each basic defined environment. These differences are estimated in a previous training process with stereo data. In order to compensate some of the problems of the independence assumption of the feature vectors components and the mismatch error between perfect and proposed transformations, two approaches have been proposed too: a multi-environment rotation transformation algorithm, and the use of transformed space acoustic models. Some experiments with SpeechDat Car database were carried out in order to study the behavior of the proposed techniques in a real acoustic environment. The experimental results show an average improvement of more than 77% using PD-MEMLIN, and more than 85% using transformed space acoustic models and multi-environment rotation transformation, concerning the baseline.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-213"
  },
  "chen05b_interspeech": {
   "authors": [
    [
     "Yi",
     "Chen"
    ],
    [
     "Lin-Shan",
     "Lee"
    ]
   ],
   "title": "Energy-based frame selection for reliable feature normalization and transformation in robust speech recognition",
   "original": "i05_0385",
   "page_count": 4,
   "order": 214,
   "p1": "385",
   "pn": "388",
   "abstract": [
    "In this work, we propose a frame selection scheme based on the smoothed instantaneous energy of samples, local order statistics for them, and average of a binary energy indicator over the frame to measure the reliability of frames. By selection of reliable frames, a four-stage feature normalization and transformation process is further proposed: mean normalization, variance normalization, first-stage principal component analysis, and multi-eigenvector temporal filtering. Extensive experiments verified that the performance of each individual stage can be significantly improved by the proposed frame selection scheme, and the overall performance can also be improved stage by stage for all types of noise and all SNR values defined in AURORA 2.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-214"
  },
  "nakajima05_interspeech": {
   "authors": [
    [
     "Yoshitaka",
     "Nakajima"
    ],
    [
     "Hideki",
     "Kashioka"
    ],
    [
     "Kiyohiro",
     "Shikano"
    ],
    [
     "Nick",
     "Campbell"
    ]
   ],
   "title": "Remodeling of the sensor for non-audible murmur (NAM)",
   "original": "i05_0389",
   "page_count": 4,
   "order": 215,
   "p1": "389",
   "pn": "392",
   "abstract": [
    "We developed the next generation of skin-attachment sensors for sampling NAM (Non-Audible Murmur) signals by using soft silicone, which has an acoustic impedance close to that of human flesh, as the prime medium of vibration. With new NAM microphones we could sample expanded target voice signal, suppressing air conduction noise signal to low by the experiment of synchronous stereo sampling of air and flesh conduction voices at the same gain. The bandwidth of the soft silicone type NAM microphone has improved and we obtain a much higher accuracy of both NAM and BTOS (Body Transmitted Ordinary Speech) recognition compared with the stethoscopic type. Aural comprehension test showed that accuracy of catching sentences of NAM and BTOS has improved with soft silicon type NAM microphone almost as high as that of air conduction voices. However, the extremely low accuracy of meaningless words is a problem to be solved for developing a \"Non-Voice Phone\".\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-215"
  },
  "subramanya05_interspeech": {
   "authors": [
    [
     "Amarnag",
     "Subramanya"
    ],
    [
     "Jeff",
     "Bilmes"
    ],
    [
     "Chia-Ping",
     "Chen"
    ]
   ],
   "title": "Focused word segmentation for ASR",
   "original": "i05_0393",
   "page_count": 4,
   "order": 216,
   "p1": "393",
   "pn": "396",
   "abstract": [
    "We propose a new set of features based on the temporal statistics of the spectral entropy of speech. We show why these features make good inputs for a speech detector. Moreover, we propose a back-end that uses the evidence from the above features in a focused' manner. Subsequently, by means of recognition experiments we show that using the above back-end leads to significant performance improvements, but merely appending the features to the standard feature vector does not improve performance. We also report a 10% average improvement in word error rate over our baseline for the highly mis-matched case in the Aurora3.0 corpus.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-216"
  },
  "haebumbach05b_interspeech": {
   "authors": [
    [
     "Reinhold",
     "Haeb-Umbach"
    ],
    [
     "Joerg",
     "Schmalenstroeer"
    ]
   ],
   "title": "A comparison of particle filtering variants for speech feature enhancement",
   "original": "i05_0913",
   "page_count": 4,
   "order": 217,
   "p1": "913",
   "pn": "916",
   "abstract": [
    "This paper compares several particle filtering variants for speech feature enhancement in non-stationary noise environments. By analyzing the random processes of clean speech, noise and noisy speech, appropriate proposal densities are derived. The performances of the resulting particle filters, i.e. modified Sampling-Importance-Resampling (mod-SIR), auxiliary SIR and likelihood particle filter, are compared in terms of word accuracy achieved by the subsequent speech recognizer on the AURORA 2 database. It turns out that for the noises found in this database, noise compensation techniques that assume stationary noise work equally well.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-217"
  },
  "potamitis05_interspeech": {
   "authors": [
    [
     "Ilyas",
     "Potamitis"
    ],
    [
     "Nikolaos",
     "Fakotakis"
    ]
   ],
   "title": "Enhancement of mel log-power spectrum of speech using particle filtering",
   "original": "i05_0917",
   "page_count": 4,
   "order": 218,
   "p1": "917",
   "pn": "920",
   "abstract": [
    "The subject of this work is a statistical feature enhancement technique for robust speech recognition applied to the log-power domain after the application of the Mel filterbank. The proposed approach makes use of a state space formulation that involves a random walk model for the evolution of the underlying clean features and a non-linear observation model that connects the noisy features with noise and clean speech. The novelty of the proposed approach is that a) both observation and state noise are shown to be heavy-tailed and are subsequently modelled using a mixture of Gaussians, b) a sequential Monte Carlo filter is employed to approximate the posterior probability of clean speech thus avoiding linearization of the non-linear observation model as in the case of algorithms that perform iterative approximations. The efficiency of the approach is illustrated when additive white Gaussian (AWGN) or babble noise is present in low signal-to-noise ratios (SNR).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-218"
  },
  "shozakai05_interspeech": {
   "authors": [
    [
     "Makoto",
     "Shozakai"
    ],
    [
     "Goshu",
     "Nagino"
    ]
   ],
   "title": "Improving robustness of speech recognition performance to aggregate of noises by two-dimensional visualization",
   "original": "i05_0921",
   "page_count": 4,
   "order": 219,
   "p1": "921",
   "pn": "924",
   "abstract": [
    "This paper proposes a new methodology to improve robustness of recognition performance to aggregate of noises by two-dimensional visualization technique. At first, an aggregate of noises existing in adverse environments are collected as much as possible. Then, hidden Markov model (HMM) for each collected noise is trained. Aggregate of the trained HMMs are visualized into two-dimensional map by the statistical multidimensional scaling technique named as COSMOS method. The noises corresponding to the HMMs located in periphery of the map are overlaid to clean speech used for training HMMs of acoustic models. It is revealed that this new methodology significantly reduces recognition error rate by around 60% to non-stationary noises overlaid in the voice interval of word.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-219"
  },
  "lim05_interspeech": {
   "authors": [
    [
     "Woohyung",
     "Lim"
    ],
    [
     "Bong Kyoung",
     "Kim"
    ],
    [
     "Nam Soo",
     "Kim"
    ]
   ],
   "title": "Feature compensation based on switching linear dynamic model and soft decision",
   "original": "i05_0925",
   "page_count": 4,
   "order": 220,
   "p1": "925",
   "pn": "928",
   "abstract": [
    "In this paper, we present a new approach to feature compensation for robust speech recognition in noisy environments. We employ the switching linear dynamic model (SLDM) as a parametric model for the clean speech distribution, which enables us to utilize temporal correlations in speech signals. Both the background noise and clean speech components are simultaneously estimated by means of the interacting multiple model (IMM) algorithm. Moreover, we combine the SLDM algorithm with the spectral subtraction (SS) approach based on a soft decision. Performance of the presented compensation technique is evaluated through the experiments on AURORA 2 database.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-220"
  },
  "huang05b_interspeech": {
   "authors": [
    [
     "Shilei",
     "Huang"
    ],
    [
     "Xiang",
     "Xie"
    ],
    [
     "Jingming",
     "Kuang"
    ]
   ],
   "title": "Using output probability distribution for improving speech recognition in adverse environment",
   "original": "i05_0929",
   "page_count": 4,
   "order": 221,
   "p1": "929",
   "pn": "932",
   "abstract": [
    "This paper proposed a method to improve the accuracy of small vocabulary isolated word speaker-independent speech recognition in adverse environment. The proposed approach is implemented by using Output Probability Distributions (OPDs) and Support Vector Machine (SVM). OPDs improve the system performance by modeling inter-word relationships; then SVM classifiers are used to discriminate the difference between OPD models. The system was tested using isolated Mandarin digits database, corrupted with the NOISEX-92 database. The experiments have achieved good result in noise conditions, the WER dropped about 30% on average when compared to the HMM recognizer.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-221"
  },
  "choi05b_interspeech": {
   "authors": [
    [
     "Eric H. C.",
     "Choi"
    ]
   ],
   "title": "A generalized framework for compensation of mel-filterbank outputs in feature extraction for robust ASR",
   "original": "i05_0933",
   "page_count": 4,
   "order": 222,
   "p1": "933",
   "pn": "936",
   "abstract": [
    "This paper describes a novel and efficient noise-robust front-end that utilizes a set of Mel-filterbank output compensation methods, together with cumulative distribution mapping of cepstral coefficients, for noisy speech recognition. The proposed compensation framework includes the use of noise spectral subtraction, spectral flooring and log Mel-filterbank output weighting. Recognition experiments on the Aurora II connected digit database have revealed that the proposed front-end achieves an average digit recognition accuracy of 83.46% for a model set trained from clean data. Compared with the recognition results obtained by using the ETSI standard Mel-cepstral front-end, these results represent a relative error reduction of around 58%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-222"
  },
  "tolba05_interspeech": {
   "authors": [
    [
     "Hesham",
     "Tolba"
    ],
    [
     "Zili",
     "Li"
    ],
    [
     "Douglas",
     "O'Shaughnessy"
    ]
   ],
   "title": "Robust automatic speech recognition using a perceptually-based optimal spectral amplitude estimator speech enhancement algorithm in various low-SNR environments",
   "original": "i05_0937",
   "page_count": 4,
   "order": 223,
   "p1": "937",
   "pn": "940",
   "abstract": [
    "This paper addresses the problem of noise robustness of automatic speech recognition (ASR) systems in various noisy environments using a Minimum Mean-Square Error Short-Time Spectral Amplitude Estimator (MMSE-STSA). This was accomplished by the integration of a Perceptual Weighting Filter (PWF) with the MMSESTSA algorithm in order to improve the preprocessing speech enhancement performance. The proposed PWF-based STSA algorithm is integrated in the front-end of an ASR system in order to evaluate its robustness in severe interfering noisy environments. Experiments were conducted using a noisy version of speech signals extracted from the TIMIT database. The Hidden Markov model Toolkit (HTK) was used throughout our experiments. Results show that the proposed approach when included in the front-end of an HTK-based ASR system, outperforms that of the conventional recognition process in interfering noisy environments for a wide range of SNRs down to -4 dB.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-223"
  },
  "so05_interspeech": {
   "authors": [
    [
     "Stephen",
     "So"
    ],
    [
     "Kuldip K.",
     "Paliwal"
    ]
   ],
   "title": "Improved noise-robustness in distributed speech recognition via perceptually-weighted vector quantisation of filterbank energies",
   "original": "i05_0941",
   "page_count": 4,
   "order": 224,
   "p1": "941",
   "pn": "944",
   "abstract": [
    "In this paper, we examine a coding scheme for quantising feature vectors in a distributed speech recognition environment that is more robust to noise. It consists of a vector quantiser that operates on the logarithmic filterbank energies (LFBEs). Through the use of a perceptually-weighted Euclidean distance measure, which emphasises the LFBEs that represent the spectral peaks, the vector quantiser codebook provides a priori knowledge of the spectral characteristics of clean speech and is used to quantise features from noise-corrupted speech. Our comparative results from the ETSI Aurora-2 recognition task show that the perceptually-weighted vector quantisation of LFBEs achieves higher recognition accuracies for noisy speech than the unweighted vector quantisation, memoryless and multi-frame GMM-based block quantisation and scalar quantisation of Mel frequency-warped cepstral coefficients.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-224"
  },
  "nasersharif05_interspeech": {
   "authors": [
    [
     "Babak",
     "Nasersharif"
    ],
    [
     "Ahmad",
     "Akbari"
    ]
   ],
   "title": "Sub-band weighted projection measure for robust sub-band speech recognition",
   "original": "i05_0945",
   "page_count": 4,
   "order": 225,
   "p1": "945",
   "pn": "948",
   "abstract": [
    "In recent years, sub-band speech recognition has been found useful in robust speech recognition, especially for speech signals contaminated by band-limited noise. In sub-band speech recognition, full band speech is divided into several frequency sub-bands and then sub-band feature vectors or their generated likelihoods by corresponding sub-band recognizers are combined to give the result of recognition task. In this paper, we concatenate sub-band feature vectors, where we extract phase autocorrelation (PAC) MFCC, as noise robust features, from each sub-band. Furthermore, we extend a model adaptation method, named sub-band weighted projection measure (SWPM), to adapt HMM Gaussian mean vectors to concatenated sub-band feature vectors in noisy conditions. The experimental results indicate that the proposed method significantly improves the sub-band speech recognition system performance in presence of additive noise.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-225"
  },
  "deng05b_interspeech": {
   "authors": [
    [
     "Jianping",
     "Deng"
    ],
    [
     "Martin",
     "Bouchard"
    ],
    [
     "Tet Hin",
     "Yeap"
    ]
   ],
   "title": "Noise compensation using interacting multiple kalman filters",
   "original": "i05_0949",
   "page_count": 4,
   "order": 226,
   "p1": "949",
   "pn": "952",
   "abstract": [
    "This paper presents an approach to compensate the effects of noise with an Interacting Multiple Model algorithm using Unscented Kalman Filters (IMM-UKF) in log-spectral domain. The performance of this approach is studied experimentally on a continuous digits recognition task with additive noise conditions and compared with results previously obtained by the implementation of the Interacting Multiple Model algorithm using Extended Kalman Filters (IMM-EKF) in log-spectral domain. Simulation results show that a better performance in terms of word recognition rates can be obtained with the suggested approach.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-226"
  },
  "stouten05_interspeech": {
   "authors": [
    [
     "Veronique",
     "Stouten"
    ],
    [
     "Hugo Van",
     "Hamme"
    ],
    [
     "Patrick",
     "Wambacq"
    ]
   ],
   "title": "Kalman and unscented kalman filter feature enhancement for noise robust ASR",
   "original": "i05_0953",
   "page_count": 4,
   "order": 227,
   "p1": "953",
   "pn": "956",
   "abstract": [
    "Model-based feature enhancement is an ASR front-end technique to increase the robustness of the recogniser in noisy environments. However, its MMSE-estimates of the clean speech feature vectors are based only on the static components at the current frame. In this paper, we show how the Kalman filter framework can be seen as a natural extension that incorporates both the current and the previous frames in the enhancement process. Because multiple Kalman filters are run in parallel, the global clean speech estimate is given by a weighted linear combination of the individual MMSE-estimates. Also, the unscented transformation is considered to avoid the linearisation of the cepstral domain observation equation. We present experimental results on the Aurora2 database for both the multi-modal Kalman and the unscented Kalman filter feature enhancement.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-227"
  },
  "wan05_interspeech": {
   "authors": [
    [
     "Chia-yu",
     "Wan"
    ],
    [
     "Lin-Shan",
     "Lee"
    ]
   ],
   "title": "Histogram-based quantization (HQ) for robust and scalable distributed speech recognition",
   "original": "i05_0957",
   "page_count": 4,
   "order": 228,
   "p1": "957",
   "pn": "960",
   "abstract": [
    "The performance of conventional distance-based vector quantization (VQ) for distributed speech recognition (DSR) is inevitably degraded by the environmental noise and quantization distortion. The pre-trained codebook is less scalable and may not be matched with the testing speech. A new concept of Histogram-based Quantization (HQ) is proposed in this paper, in which the quantization levels are dynamically defined by the histogram or order statistics of a segment of local most recent past samples of the parameter to be quantized. All problems with a pre-trained codebook is automatically solved because the pre-trained codebook is not used at all. The computation requirement is low because no distance measure is needed. The approach is robust because most disturbances (including very non-stationary types) can be absorbed by the dynamic histogram. Extensive experiments with AURORA 2.0 testing environment indicated the new approach is highly robust and scalable, suitable for future personalized and context-aware DSR environment.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-228"
  },
  "chung05_interspeech": {
   "authors": [
    [
     "Yong-Joo",
     "Chung"
    ]
   ],
   "title": "A data-driven approach for the model parameter compensation in noisy speech recognition",
   "original": "i05_0961",
   "page_count": 4,
   "order": 229,
   "p1": "961",
   "pn": "964",
   "abstract": [
    "In this paper, a data-driven approach that compensates the HMM parameters for the noisy speech recognition is proposed. The various statistical information necessary for the HMM parameter compensation is estimated during the HMM training procedure by using the expectation-maximization algorithm. Instead of assuming some statistical approximations for the model combination in the conventional methods such as the PMC, the estimated statistical information is directly combined with the clean speech HMM parameters to produce the compensated parameters. The proposed method has shown improved results compared with the PMC in the isolated Korean word noisy speech recognition experiments.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-229"
  },
  "kobashikawa05_interspeech": {
   "authors": [
    [
     "Satoshi",
     "Kobashikawa"
    ],
    [
     "Satoshi",
     "Takahashi"
    ],
    [
     "Yoshikazu",
     "Yamaguchi"
    ],
    [
     "Atsunori",
     "Ogawa"
    ]
   ],
   "title": "Rapid response and robust speech recognition by preliminary model adaptation for additive and convolutional noise",
   "original": "i05_0965",
   "page_count": 4,
   "order": 230,
   "p1": "965",
   "pn": "968",
   "abstract": [
    "Users require speech recognition systems that offer rapid response and robustness (high accuracy). Speech recognition accuracy suffers from additive noise, imposed by ambient noise, and convolutional noise, created by space transfer characteristics. Existing model adaptation techniques achieve robustness by using HMM-composition and CMN (cepstral mean normalization). Since they need the additive noise sample as well as the user speech sample to generate the models required, they can not achieve rapid response. The proposed technique generates noise adapted models in a preliminary step, and then normalizes the models' parameters using just the additive noise observed by the system. In our technique, after the user's speech sample is captured, only CMN need be performed to start recognition processing, so its response is rapid. Another innovation is the creation of several HMMs to cover the wide S/N range expected in real applications; it raises accuracy and response speed. Simulations conducted using artificial speech samples generated to represent 7 S/N values show that an extended version of the proposed technique holds the reduction in average recognition error to 17.6% compared to the basic HMM composition method.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-230"
  },
  "prasad05_interspeech": {
   "authors": [
    [
     "Saurabh",
     "Prasad"
    ],
    [
     "Stephen A.",
     "Zahorian"
    ]
   ],
   "title": "Nonlinear and linear transformations of speech features to compensate for channel and noise effects",
   "original": "i05_0969",
   "page_count": 4,
   "order": 231,
   "p1": "969",
   "pn": "972",
   "abstract": [
    "Automatic speech recognizers perform poorly when training and test data are systematically different in terms of noise and channel characteristics. One manifestation of such differences is variations in the probability density functions (pdfs) between training and test features. Consequently, both automatic speech recognition and automatic speaker identification may be severely degraded. Previous attempts to minimize this problem include Cepstral Mean and Variance Normalization and transforming all speech features to a univariate Gaussian pdf. In this paper, we present a quantile based Cumulative Density Function (CDF) matching technique for data drawn from different distributions. This method can be used to compensate for the systematic marginal (i.e. each feature individually) differences between training and test features. We further propose a linear covariance normalization technique to compensate for differences in covariance properties between training and test data. Experimental results are given that illustrate these techniques for speech recognition and automatic speaker identification.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-231"
  },
  "suzuki05_interspeech": {
   "authors": [
    [
     "Motoyuki",
     "Suzuki"
    ],
    [
     "Yusuke",
     "Kato"
    ],
    [
     "Akinori",
     "Ito"
    ],
    [
     "Shozo",
     "Makino"
    ]
   ],
   "title": "Construction method of acoustic models dealing with various background noises based on combination of HMMs",
   "original": "i05_0973",
   "page_count": 4,
   "order": 232,
   "p1": "973",
   "pn": "976",
   "abstract": [
    "Background noise is one of the biggest problem for speech recognition systems in real environments. In order to achieve high recognition performance for corrupted speech, we proposed a new construction method of HMMs dealing with various kinds of background noise. At first, each HMM dealing with a single noise is trained for each background noise, and then all Gaussian components of those HMMs are combined into a \"multi-mixture HMM\". From the experimental results, the multi-mixture HMM gave the highest recognition performance for any kind of noise and any variation of SNR.\n",
    "Although the multi-mixture HMMs has high performance, it has a huge number of Gaussian components that makes the speech recognition slower. In order to solve the problem, we also proposed a reduction method of Gaussian components. It can decrease the number of Gaussian components with slight deterioration of recognition performance.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-232"
  },
  "xu05b_interspeech": {
   "authors": [
    [
     "Haitian",
     "Xu"
    ],
    [
     "Zheng-Hua",
     "Tan"
    ],
    [
     "Paul",
     "Dalsgaard"
    ],
    [
     "Børge",
     "Lindberg"
    ]
   ],
   "title": "Robust speech recognition based on noise and SNR classification - a multiple-model framework",
   "original": "i05_0977",
   "page_count": 4,
   "order": 233,
   "p1": "977",
   "pn": "980",
   "abstract": [
    "This paper presents a multiple-model framework for noise-robust speech recognition. In this framework, multiple HMM model sets are trained - each identified by a noise type and a specific Signal-to-Noise Ratio (SNR) value. This, however, does not increase the computational complexity of the recognition process since only one model set is selected according to the noise classification and SNR estimation. The optimal number of model sets is first identified on the basis of the Aurora 2 database. With only three model sets for each noise type, the framework shows superior performance to Multi-style TRaining (MTR) when testing on known noise types but lower performance on unknown noise types. To overcome this drawback, a modified Jacobian method is proposed to adapt the selected HMM models to the test environment. Furthermore, given the fact that MTR often gives relatively stable performance for unknown noise types, a combined technique is applied in which interpolation between the MTR and the adapted models is performed. This combined technique gives more than 24% performance improvement as compared to MTR.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-233"
  },
  "song05_interspeech": {
   "authors": [
    [
     "Hwa Jeon",
     "Song"
    ],
    [
     "Hyung Soon",
     "Kim"
    ]
   ],
   "title": "Eigen-environment based noise compensation method for robust speech recognition",
   "original": "i05_0981",
   "page_count": 4,
   "order": 234,
   "p1": "981",
   "pn": "984",
   "abstract": [
    "In this paper, we propose a new noise compensation method based on the eigenvoice framework in feature space to reduce the mismatch between training and testing environments. In this method, the difference between clean and noisy environments is represented by the linear combination of K eigenvectors that represent the variation among environments. Since how to construct the noisy models is crucial for the performance of the proposed method, we introduce two methods for constructing noisy models : one based on MAP adaptation method and the other using stereo DB. In experiments using Aurora 2 DB, we obtained 44.9% relative improvement with eigen-environment method in comparison with baseline system. Especially, in clean condition training mode, our proposed method yielded 67.4% relative improvement.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-234"
  },
  "graciarena05_interspeech": {
   "authors": [
    [
     "Martin",
     "Graciarena"
    ],
    [
     "Horacio",
     "Franco"
    ],
    [
     "Greg",
     "Myers"
    ],
    [
     "Victor",
     "Abrash"
    ]
   ],
   "title": "Robust feature compensation in nonstationary and multiple noise environments",
   "original": "i05_0985",
   "page_count": 4,
   "order": 235,
   "p1": "985",
   "pn": "988",
   "abstract": [
    "The probabilistic optimum filtering (POF) algorithm is a piece wise linear transformation of the noisy speech feature space into the clean speech feature space. In this work we extend the POF algorithm to allow a more accurate way to select noisy-to-clean feature mappings, by allowing different combinations of speech and noise to have combination-specific mappings selected depending on the observation. This is especially important in nonstationary environments, where different noise segments will result in different observations in the noisy feature space. Experimental results using stationary and nonstationary noises show the effectiveness of the proposed technique compared to the old approach. We also explored the use of the extended POF method to train a map with multiple noises in order to gain generalization over different noise types and be able to tackle unknown noise environments. tackle unknown noise environments.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-235"
  },
  "droppo05_interspeech": {
   "authors": [
    [
     "Jasha",
     "Droppo"
    ],
    [
     "Alex",
     "Acero"
    ]
   ],
   "title": "Maximum mutual information SPLICE transform for seen and unseen conditions",
   "original": "i05_0989",
   "page_count": 4,
   "order": 236,
   "p1": "989",
   "pn": "992",
   "abstract": [
    "SPLICE is a front-end technique for automatic speech recognition systems. It is a non-linear feature space transformation meant to increase recognition accuracy. Our previous work has shown how to train SPLICE to perform speech feature enhancement. This paper evaluates a maximum mutual information (MMI) based discriminative training method for SPLICE. Discriminative techniques tend to excel when the training and testing data are similar, and to degrade performance significantly otherwise. This paper explores both cases in detail using the Aurora 2 corpus. The overall recognition accuracy of the MMI-SPLICE system is slightly better than the Advanced Front End standard from ETSI, and much better than previous SPLICE training algorithms. Most notably, it achieves this without explicitly resorting to the standard techniques of environment modeling, noise modeling or spectral subtraction.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-236"
  },
  "kruger05_interspeech": {
   "authors": [
    [
     "Sven E.",
     "Krüger"
    ],
    [
     "Martin",
     "Schafföner"
    ],
    [
     "Marcel",
     "Katz"
    ],
    [
     "Edin",
     "Andelic"
    ],
    [
     "Andreas",
     "Wendemuth"
    ]
   ],
   "title": "Speech recognition with support vector machines in a hybrid system",
   "original": "i05_0993",
   "page_count": 4,
   "order": 237,
   "p1": "993",
   "pn": "996",
   "abstract": [
    "While the temporal dynamics of speech can be represented very efficiently by Hidden Markov Models (HMMs), the classification of speech into single speech units (phonemes) is usually done with Gaussian mixture models which do not discriminate well. Here, we use Support Vector Machines (SVMs) for classification by integrating this method in a HMM-based speech recognition system. In this hybrid SVM/HMM system we translate the outputs of the SVM classifiers into conditional probabilities and use them as emission probabilities in a HMM-based decoder. SVMs are very appealing due to their association with statistical learning theory. They have already shown very good classification results in other fields of pattern recognition. We train and test our hybrid system on the DARPA Resource Management (RM1) corpus. Our results show better performance than HMM-based decoder using Gaussian mixtures.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-237"
  },
  "barreaud05_interspeech": {
   "authors": [
    [
     "Vincent",
     "Barreaud"
    ],
    [
     "Douglas",
     "O'Shaughnessy"
    ],
    [
     "Jean-Guy",
     "Dahan"
    ]
   ],
   "title": "Experiments on speaker profile portability",
   "original": "i05_0997",
   "page_count": 4,
   "order": 238,
   "p1": "997",
   "pn": "1000",
   "abstract": [
    "This paper addresses the problem of speaker characterization in the speaker-dependent speech recognition problem. Speaker Adaptation and Normalization techniques are designed to reduce the mismatch introduced by inter-speaker variability. Yet there is another source of mismatch introduced by intra-speaker variability. Indeed, the speaking style of a speaker depends on the nature of the speech uttered. The framework of this paper is speakerdependent isolated-word recognition on an embedded engine. The limited computational and memory loads of this engine reduce the possible techniques for normalization. The proposed solution uses a speaker profile trained on dictation data and exported to the embedded engine. In this framework we study the portability of a task-dependent speaker profile from dictation task to command task. Experiments have been conducted on a Scansoft 255 speakers database. We show that the portability results in a loss of efficiency due to the nature of the considered tasks.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-238"
  },
  "colibro05_interspeech": {
   "authors": [
    [
     "Daniele",
     "Colibro"
    ],
    [
     "Luciano",
     "Fissore"
    ],
    [
     "Claudio",
     "Vair"
    ],
    [
     "Emanuele",
     "Dalmasso"
    ],
    [
     "Pietro",
     "Laface"
    ]
   ],
   "title": "A confidence measure invariant to language and grammar",
   "original": "i05_1001",
   "page_count": 4,
   "order": 239,
   "p1": "1001",
   "pn": "1004",
   "abstract": [
    "Confidence measures are necessary in all voiced activated applications to decide whether a recognized word, or a sentence, should be accepted or rejected.\n",
    "A confidence measure should not only be reliable, but possibly application independent, i.e. its dynamic range should be uniform for different languages, grammars, and vocabularies. This is an important practical issue because it allows the application developers to use the same value of the threshold for different applications and to expect comparable rejection rates. This eases their task at least in the first phase of application development.\n",
    "In this paper, we introduce a confidence measure that has these properties. It allows eliminating the cumbersome experimental procedure necessary to tune individually the rejection threshold for every developed recognition object.\n",
    "We present the results of a set of experiments that demonstrate the \"normalization\" quality of our confidence measure for six different grammars in different languages.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-239"
  },
  "schutte05_interspeech": {
   "authors": [
    [
     "Ken",
     "Schutte"
    ],
    [
     "James",
     "Glass"
    ]
   ],
   "title": "Robust detection of sonorant landmarks",
   "original": "i05_1005",
   "page_count": 4,
   "order": 240,
   "p1": "1005",
   "pn": "1008",
   "abstract": [
    "A sonorant detection scheme using Mel-frequency cepstral coefficients and support vector machines (SVMs) is presented and tested in a variety of noise conditions. Adapting the classifier threshold using an estimate of the noise level is used to bias the classifier to effectively compensate for mismatched training and testing conditions. The adaptive threshold classifier achieves low frame error rates using only clean training data without requiring specifically designed features or learning algorithms. The frame-by-frame SVM output is analyzed over longer time periods to uncover temporal modulations related to syllable structure which may aid in landmark-based speech recognition and speech detection. Appropriate filtering of this signal leads to a representation which is stable over a wide range of noise conditions. Using the smoothed output for landmark detection results in a high precision rate, enabling confident pruning of the search-space used by landmark-based speech recognizers.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-240"
  },
  "ma05_interspeech": {
   "authors": [
    [
     "Ning",
     "Ma"
    ],
    [
     "Phil",
     "Green"
    ]
   ],
   "title": "Context-dependent word duration modelling for robust speech recognition",
   "original": "i05_2609",
   "page_count": 4,
   "order": 241,
   "p1": "2609",
   "pn": "2612",
   "abstract": [
    "Conventional hidden Markov models (HMMs) have weak duration constraints. This may cause the decoder to produce word matches with unrealistic durations in noisy situations. This paper describes techniques for modelling context-dependent word duration cues and incorporating them directly in a multi-stack decoding algorithm. The proposed model is capable of penalising duration constraints of a word depending on its context. Experiments on connected digit recognition show that the new system can significantly improve recognition performance at different noise levels.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-241"
  },
  "epps05_interspeech": {
   "authors": [
    [
     "Julien",
     "Epps"
    ],
    [
     "Eric H. C.",
     "Choi"
    ]
   ],
   "title": "An energy search approach to variable frame rate front-end processing for robust ASR",
   "original": "i05_2613",
   "page_count": 4,
   "order": 242,
   "p1": "2613",
   "pn": "2616",
   "abstract": [
    "Extensive research has been devoted to robustness in the presence of various types and degrees of environmental noise over the past several years, however this remains one of the main problems facing automatic speech recognition systems. This paper describes a new variable frame rate analysis technique, based upon searching a predefined lookahead interval for the next frame position that maximizes the first-order difference of the log energy (ΔE) between the consecutive frames. The application of this novel technique to noise-robust ASR front-end processing is also reported. In comparison with existing variable frame rate methods in the literature, the proposed energy search approach is simpler and achieves similar recognition accuracy improvements at lower complexity. Experimental work on the Aurora II connected digits database reveals that the proposed front-end, together with cumulative distribution mapping, achieves average digit recognition accuracies of 78.32% for a model set trained from clean data and 89.95% for a model set trained from data with multiple noise conditions, representing 6.1% and 2.3% reductions in word error rates respectively over a cumulative distribution mapping baseline.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-242"
  },
  "gemello05_interspeech": {
   "authors": [
    [
     "Roberto",
     "Gemello"
    ],
    [
     "Franco",
     "Mana"
    ],
    [
     "Renato de",
     "Mori"
    ]
   ],
   "title": "Non-linear estimation of voice activity to improve automatic recognition of noisy speech",
   "original": "i05_2617",
   "page_count": 4,
   "order": 243,
   "p1": "2617",
   "pn": "2620",
   "abstract": [
    "Feed-forward multi-layer perceptrons (MLP) and recurrent neural networks (RNN) fed with different sets of acoustic features are proposed for computing the presence and absence of speech in continuous speech signal in presence of various levels of background noise. Detailed performance evaluations on voice activity detection (VAD) are reported using the Aurora2, Aurora3 and TIMIT corpora. It is shown that the best results are obtained with an RNN fed by the acoustic features used for automatic speech recognition (ASR) augmented by specific features. Detailed evaluations are also proposed for ASR using Aurora2 and the German, Italian and Spanish portions of the test set of the Aurora3 corpus. The highest word error rate (WER) reduction (16.9%) is obtained when the only-noise presence probability is used to modify the phone posterior probabilities used for speech decoding.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-243"
  },
  "kida05_interspeech": {
   "authors": [
    [
     "Yusuke",
     "Kida"
    ],
    [
     "Tatsuya",
     "Kawahara"
    ]
   ],
   "title": "Voice activity detection based on optimally weighted combination of multiple features",
   "original": "i05_2621",
   "page_count": 4,
   "order": 244,
   "p1": "2621",
   "pn": "2624",
   "abstract": [
    "This paper presents a voice activity detection (VAD) scheme that is robust against noise, based on an optimally weighted combination of features. The scheme uses a weighted combination of four conventional VAD features: amplitude level, zero crossing rate, spectral information, and Gaussian mixture model likelihood. This combination in effect selects the optimal method depending on the noise condition. The weights for the combination are updated using minimum classification error (MCE) training. An experimental evaluation under three types of noisy environment demonstrated the noise robustness of our proposed method. Adapting the feature weights was shown to enhance the detection ability and to be possible using ten or fewer training utterances.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-244"
  },
  "ding05_interspeech": {
   "authors": [
    [
     "Pei",
     "Ding"
    ]
   ],
   "title": "Soft decision strategy and adaptive compensation for robust speech recognition against impulsive noise",
   "original": "i05_2625",
   "page_count": 4,
   "order": 245,
   "p1": "2625",
   "pn": "2628",
   "abstract": [
    "This paper presents research on robust automatic speech recognition (ASR) in the presence of impulsive noise, which is usually caused by transmission errors or packet loss in network-based delivery of speech signals. A soft decision strategy is proposed by analyzing the degraded observation probabilities caused by impulsive noise. Based on the soft decision results, two compensation methods are developed. The first aims at suppressing the unreliable likelihood scores by flooring the observation probabilities (FOP) on sensitive feature components with an adaptive threshold. The second focuses on the recovery of corrupted features and the unreliability of reconstructed data can be further compensated by the flooring method. Evaluation results on the Aurora connected digits database show that the proposed methods significantly improve the recognition robustness against impulsive noise. For example at the occurrence rate of 50% in simulated impulsive noise environment, the accuracy is increased from 42.74% of the baseline to 85.35%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-245"
  },
  "morales05_interspeech": {
   "authors": [
    [
     "Nicolás",
     "Morales"
    ],
    [
     "Doroteo",
     "Torre Toledano"
    ],
    [
     "John H. L.",
     "Hansen"
    ],
    [
     "José",
     "Colás"
    ],
    [
     "Javier",
     "Garrido"
    ]
   ],
   "title": "Statistical class-based MFCC enhancement of filtered and band-limited speech for robust ASR",
   "original": "i05_2629",
   "page_count": 4,
   "order": 246,
   "p1": "2629",
   "pn": "2632",
   "abstract": [
    "In this paper we address the problem of bandwidth extension from the point of view of ASR. We show that an HMM-based recognition engine trained with full-bandwidth data can successfully perform ASR on limited-bandwidth test data by means of a simple correction scheme over the input feature vectors. In particular we show that results obtained using full-bandwidth HMMs and corrected feature vectors can be comparable to, or even outperform results obtained using limited-bandwidth-trained HMMs. Both results are inferior to those obtained with full-bandwidth HMMs and test data. These results suggest that the effect of channel mismatch on recognition accuracy can be partially compensated with a feature correction scheme, while the loss of information inherent to a limited-bandwidth cannot be compensated.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-246"
  },
  "misra05_interspeech": {
   "authors": [
    [
     "Hemant",
     "Misra"
    ],
    [
     "Hervé",
     "Bourlard"
    ]
   ],
   "title": "Spectral entropy feature in full-combination multi-stream for robust ASR",
   "original": "i05_2633",
   "page_count": 4,
   "order": 247,
   "p1": "2633",
   "pn": "2636",
   "abstract": [
    "In a recent paper, we reported promising automatic speech recognition results obtained by appending spectral entropy features to PLP features. In the present paper, spectral entropy features are used along with PLP features in multi-stream framework. In our multi-stream hidden Markov model/artificial neural network system, we train a separate multi-layered perceptron (MLP) for PLP features, spectral entropy features and both the features combined by concatenation. The output posteriors from these three MLPs are combined with weights inversely proportional to the output entropies of the respective MLPs. On the Numbers95 database, this approach yields a considerable improvement both under clean and noisy conditions as compared to simply appending the features. Further, in multi-stream Tandem system, we apply the same inverse entropy weighting to combine the outputs of the MLPs before the softmax non-linearity. Feeding the combined outputs after decorrelation to the standard hidden Markov model/Gaussian mixture model system gives a 9.2% relative error reduction as compared to the baseline.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-247"
  },
  "kim05b_interspeech": {
   "authors": [
    [
     "Wooil",
     "Kim"
    ],
    [
     "Richard M.",
     "Stern"
    ],
    [
     "Hanseok",
     "Ko"
    ]
   ],
   "title": "Environment-independent mask estimation for missing-feature reconstruction",
   "original": "i05_2637",
   "page_count": 4,
   "order": 248,
   "p1": "2637",
   "pn": "2640",
   "abstract": [
    "In this paper, we propose an effective mask-estimation method for missing-feature reconstruction in order to achieve robust speech recognition in unknown noise environments. In previous work, it was found that training a model for mask estimation on speech corrupted by white noise did not provide environment-independent recognition accuracy. In this paper we describe a training method based on bands of colored noise that is more effective in reflecting spectral variations across neighboring frames and subbands. We also achieved further improvement in recognition accuracy by reconsidering frames that appeared to be unvoiced in the initial pitch analysis. Performance is evaluated using the Aurora 2.0 database in the presence of various types of noise maskers. Experimental results indicate that the proposed methods are effective in estimating masks for missing-feature reconstruction while remaining more independent of the noise conditions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-248"
  },
  "coy05_interspeech": {
   "authors": [
    [
     "André",
     "Coy"
    ],
    [
     "Jon",
     "Barker"
    ]
   ],
   "title": "Soft harmonic masks for recognising speech in the presence of a competing speaker",
   "original": "i05_2641",
   "page_count": 4,
   "order": 249,
   "p1": "2641",
   "pn": "2644",
   "abstract": [
    "The paper addresses the problem of recognising speech in the presence of a competing speaker. It uses a two stage Speech Fragment Decoding' system. The system works by first segmenting a spectro-temporal representation of the mixture into a number of fragments, such that each fragment is dominated by a single source. An ASR search is then extended to find the combination of speech model sequence and fragment subset that best fits a set of clean speech models. This paper extends previous work by combining Speech Fragment Decoding' with soft missing data techniques to better handle spectro-temporal regions that cannot be confidently ascribed to either foreground or background. Recognition experiments are performed on a connected digit task using 0 db mixtures of simultaneous mixed-gender speakers. The incorporation of soft decisions leads to an increase in system performance from 66.9% to 72.2%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-249"
  },
  "szymanski05_interspeech": {
   "authors": [
    [
     "Lech",
     "Szymanski"
    ],
    [
     "Martin",
     "Bouchard"
    ]
   ],
   "title": "Comb filter decomposition for robust ASR",
   "original": "i05_2645",
   "page_count": 4,
   "order": 250,
   "p1": "2645",
   "pn": "2648",
   "abstract": [
    "The harmonic structure of the voiced speech is an effective way of conveying information in a way that is robust to white Gaussian additive noise. In this paper we propose Comb Filter Decomposition (CFD), a new method for approximating the magnitude of the speech spectrum in terms of its harmonics, which first leads to a new interpretation of the normalized autocorrelation function. Then we introduce some feature extraction methods based on CFD and on standard autocorrelation, that emphasize the harmonic peaks of the speech spectrum. The results show an improved ASR performance under noisy conditions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-250"
  },
  "heracleous05b_interspeech": {
   "authors": [
    [
     "Panikos",
     "Heracleous"
    ],
    [
     "Tomomi",
     "Kaino"
    ],
    [
     "Hiroshi",
     "Saruwatari"
    ],
    [
     "Kiyohiro",
     "Shikano"
    ]
   ],
   "title": "Investigating the role of the Lombard reflex in non-audible murmur (NAM) recognition",
   "original": "i05_2649",
   "page_count": 4,
   "order": 251,
   "p1": "2649",
   "pn": "2652",
   "abstract": [
    "In this paper, we report non-audible murmur (NAM) recognition results in noisy environments and investigate the effect of the Lombard reflex on non-audible murmur recognition. Non-Audible murmur is speech uttered very quietly and captured through body tissue by a special acoustic sensor (e.g., NAM microphone). A system based on non-audible murmur recognition can be applied in cases when privacy is preferable in human-machine communication. Moreover, due to direct body-transmission, the environmental noises do not affect the performance markedly. Previously, we reported non-audible murmur automatic recognition in a clean environment with very promising results. We also carried out experiments using clean models and simulated noisy data, showing that the performance did not change significantly. Using, however, real noisy test data, the performance decreased markedly. To investigate this problem, we studied the Lombard reflex and conducted non-audible murmur recognition experiments using Lombard data. Results show, that Lombard reflex affects non-audible murmur recognition.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-251"
  },
  "ruzanski05_interspeech": {
   "authors": [
    [
     "Evan",
     "Ruzanski"
    ],
    [
     "John H. L.",
     "Hansen"
    ],
    [
     "Don",
     "Finan"
    ],
    [
     "James",
     "Meyerhoff"
    ],
    [
     "William",
     "Norris"
    ],
    [
     "Terry",
     "Wollert"
    ]
   ],
   "title": "Improved \"TEO\" feature-based automatic stress detection using physiological and acoustic speech sensors",
   "original": "i05_2653",
   "page_count": 4,
   "order": 252,
   "p1": "2653",
   "pn": "2656",
   "abstract": [
    "The acoustic pressure microphone has served as the primary instrument for collecting speech data for automatic speech recognition systems. The acoustic microphone suffers from limitations, such as sensitivity to background noise and relatively far proximity to speech production organs. Alternative speech collection sensors may serve to enhance the effectiveness of automatic speech recognition systems. In this study, we first consider an experimental evaluation of the TEO-CB-AutoEnv feature in an actual law enforcement training scenario. We consider feature relation to stress level assessment over time. Next, we explore the use of the physiological microphone, a gel-based device placed next to the vocal folds on the outside of the throat used to measure vibrations of the vocal tract and minimize background noise, as we investigate the effectiveness of a TEO-CB-AutoEnv-based automatic stress recognition system. We employ both acoustic and physiological sensors as stand-alone speech data collection devices as well as consider both sensors concurrently. For the latter, we devise a weighted composite decision scheme using both the acoustic and physiological microphone data that yields relative average error rate reductions of 32% and 6% versus sole employment of acoustic and physiological microphone data, respectively, in a realistic stressful environment.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-252"
  },
  "kobayakawa05_interspeech": {
   "authors": [
    [
     "Takeshi S.",
     "Kobayakawa"
    ]
   ],
   "title": "Spectral subtraction using elliptic integral for multiplication factor",
   "original": "i05_2657",
   "page_count": 4,
   "order": 253,
   "p1": "2657",
   "pn": "2660",
   "abstract": [
    "Spectral subtraction(SS) has been used for speech recognition in noisy environments. The maximum likelihood of noise mixed speech is analytically studied, which leads to the reformalization of SS. - Not only the subtraction factor used in conventional SS, but also a novel multiplication factor with elliptic integrals is used. This new SS method can be used alone or together with the conventional SS. The speech recognition experiments use a principal microphone and a reference microphone. The experiments suggest the effectiveness of the mixed method.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-253"
  },
  "wang05d_interspeech": {
   "authors": [
    [
     "Longbiao",
     "Wang"
    ],
    [
     "Norihide",
     "Kitaoka"
    ],
    [
     "Seiichi",
     "Nakagawa"
    ]
   ],
   "title": "Robust distant speech recognition based on position dependent CMN using a novel multiple microphone processing technique",
   "original": "i05_2661",
   "page_count": 4,
   "order": 254,
   "p1": "2661",
   "pn": "2664",
   "abstract": [
    "In a distant environment, channel distortion may drastically degrade speech recognition performances. In this paper, we propose a robust multiple microphone speech processing approach based on position dependent Cepstral Mean Normalization (CMN). In the training stage, the system measures the transmission characteristics according to the speaker positions from some grid points in the room and estimated the compensation parameters a priori. In the recognition stage, the system estimates the speaker position and adopts the estimated compensation parameters corresponding to the estimated position, and then the system applies the CMN to the speech and performs speech recognition for each microphone. Finally, the maximum vote or the maximum summation likelihood of whole channels (that is, multiple microphones) is used to obtain the final result. In our proposed method, we use utterances emitted from a loudspeaker located at various positions to estimate compensation parameters for a convenient sake, and we also compensate the mismatch between the cepstral means of utterances spoken by human and those emitted from the loudspeaker. Our experiments showed that the proposed method improved the performances of speech recognition system in a distant environment efficiently and it could also compensate the mismatch between voices from human and loudspeaker well.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-254"
  },
  "tanaka05b_interspeech": {
   "authors": [
    [
     "H.",
     "Tanaka"
    ],
    [
     "H.",
     "Fujimura"
    ],
    [
     "C.",
     "Miyajima"
    ],
    [
     "T.",
     "Nishino"
    ],
    [
     "Katunobu",
     "Itou"
    ],
    [
     "Kazuya",
     "Takeda"
    ]
   ],
   "title": "Data collection and evaluation of speech recognition for motorbike riders",
   "original": "i05_2665",
   "page_count": 4,
   "order": 255,
   "p1": "2665",
   "pn": "2668",
   "abstract": [
    "Speech recognition should be as an eyes-free and hands-free interface. To realise this technology, we need to clarify acoustics in a helmet and determine how much high-level riding noise affects captured speech data. This paper describes the acoustics in a helmet and transfer functions of the microphone position. We constructed a data collection system and collected the speech data of motorbike riders on city roads and express highways. Speech recognition experiments were conducted and we obtained a recognition rate high of 83.1%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-255"
  },
  "alvarez05_interspeech": {
   "authors": [
    [
     "Agustín",
     "Álvarez"
    ],
    [
     "Pedro",
     "Gómez"
    ],
    [
     "V.",
     "Nieto"
    ],
    [
     "Rafael",
     "Martínez"
    ],
    [
     "Victoria",
     "Rodellar"
    ]
   ],
   "title": "Application of a first-order differential microphone for efficient voice activity detection in a car platform",
   "original": "i05_2669",
   "page_count": 4,
   "order": 256,
   "p1": "2669",
   "pn": "2672",
   "abstract": [
    "Handsfree interfaces provide a nice solution to add-on devices in car platforms. However, the amount of acoustic disturbances existing in automotive environments usually prevents satisfactory results. In most of the cases, noise reduction techniques involving a voice activity detector (VAD) are required. Through this paper, a robust microphone array processing technique for speech detection under the influence of noise and reverberation in an automobile environment is proposed. This method applies a simple two-microphone First Order Differential Microphone in order to estimate the power spectral density of the background perturbations embedded in speech signals. Afterwards, specialized order-statistics filters (OSFs) are applied in order to obtain a consistent speech/non speech decision. The paper also includes a performance evaluation of the algorithm using Aurora3 database recordings. According to our simulation results, the proposed algorithm shows a significantly better performance than standard VADs such as G.729B or ARM and, a slight advantage over other reported methods.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-256"
  },
  "setiawan05_interspeech": {
   "authors": [
    [
     "Panji",
     "Setiawan"
    ],
    [
     "Suhadi",
     "Suhadi"
    ],
    [
     "Tim",
     "Fingscheidt"
    ],
    [
     "Sorel",
     "Stan"
    ]
   ],
   "title": "Robust speech recognition for mobile devices in car noise",
   "original": "i05_2673",
   "page_count": 4,
   "order": 257,
   "p1": "2673",
   "pn": "2676",
   "abstract": [
    "Automatic speech recognition in mobile devices has to cope with varying acoustical background noises in potentially low SNR situations. Its performance in car noise environments is of our particular interest. We put focus on noise reduction techniques as applicable for speech enhancement to ensure the accuracy of the speech recognition process. We report on word recognition rate as well as on word accuracy, the latter also being a performance measure in the absence of speech (i.e. only background noise) cases.\n",
    "As a classical technique, we first investigate Wiener filtering using a voice-activity-driven noise power spectral density (psd) estimation. Then we perform a comparison with the more advanced recursive least-squares (RLS) weighting rule for speech enhancement, as well as with the use of minimum statistics as noise psd estimation. Mel based root-cepstral coefficients has been taken as an alternative to the conventional Mel-frequency cepstral coefficients (MFCCs). The a-priori SNR based Wiener filtering with the minimum statistics and Mel based root-cepstral coefficients achieves 33.16% relative improvement in word accuracy over the classical technique.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-257"
  },
  "mihajlik05_interspeech": {
   "authors": [
    [
     "Péter",
     "Mihajlik"
    ],
    [
     "Zoltán",
     "Tobler"
    ],
    [
     "Zoltán",
     "Tüske"
    ],
    [
     "Géza",
     "Gordos"
    ]
   ],
   "title": "Evaluation and optimization of noise robust front-end technologies for the automatic recognition of Hungarian telephone speech",
   "original": "i05_2677",
   "page_count": 4,
   "order": 258,
   "p1": "2677",
   "pn": "2680",
   "abstract": [
    "In this paper a variety of front-end configurations are evaluated on Hungarian telephone speech databases. Our aim was to measure directly the efficiency of the front-ends on real noisy and normal speech data. As a baseline the ETSI ADSR standard front-end is used. Some simplification on the standard is introduced resulting in better performance on our databases than the original front-end in terms of both speed and recognition rate. Besides, another recently proposed feature extraction approach is also investigated. Finally the effect of the novel voice activity detection approach is evaluated. The best front-end configuration augmented with this voice activity detector outperformed significantly the baseline in each recognition test and by 24,7% relative in average.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-258"
  },
  "chen05c_interspeech": {
   "authors": [
    [
     "Gang",
     "Chen"
    ],
    [
     "Douglas",
     "O'Shaughnessy"
    ],
    [
     "Hesham",
     "Tolba"
    ]
   ],
   "title": "A performance investigation of noisy voice recognition over IP telephony networks",
   "original": "i05_2681",
   "page_count": 4,
   "order": 259,
   "p1": "2681",
   "pn": "2684",
   "abstract": [
    "A performance analysis of noisy automatic speech recognition (ASR) based on Internet protocol (IP) telephony networks is presented in this paper. The present public telephone voice communication networks, which utilize digital technology via circuit switching, provide satisfactory quality service. In contrast, the Internet telephony switching network, which is a packet-switched network, does not provide a guarantee of quality service at all, since such networks have packet losses. Moreover, the performance of ASR systems deteriorates when such ASR systems are used in adverse environments. For measuring the influence of missing speech packets on the ASR system performance, we use a Soekris net 4501 IP simulator in order to control packet loss rate. Also, to examine how additive acoustic noise influences the speech recognition performance, eleven different types of noise sources are chosen for use in our experiments. In these experiments, the results show that the level of packet loss has a detrimental impact on the speech recognition performance. Furthermore, the speech recognition rate also degrades as the level of additive noise increases under the different kinds of noise conditions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-259"
  },
  "ito05b_interspeech": {
   "authors": [
    [
     "Akinori",
     "Ito"
    ],
    [
     "Takashi",
     "Kanayama"
    ],
    [
     "Motoyuki",
     "Suzuki"
    ],
    [
     "Shozo",
     "Makino"
    ]
   ],
   "title": "Internal noise suppression for speech recognition by small robots",
   "original": "i05_2685",
   "page_count": 4,
   "order": 260,
   "p1": "2685",
   "pn": "2688",
   "abstract": [
    "Speech recognition by a small robot is difficult because the robot makes noise itself. In this paper, two new methods are proposed that suppresses internal noise of the small robots. These methods are based on spectral subtraction (SS). The difference of the proposed methods from the original SS is that the proposed methods use the estimated noise spectrum dependent on the motion of the robot. One method, called MDSS, prepares the noise spectrums for all motions. Another method, called NPSS, predicts the noise spectrum from angular velocities of all joints of the robot using a neural network. From the results of the comparison between the original SS and the proposed methods, the proposed methods outperformed the conventional SS. The NPSS worked well even when the noise of the motion was unstable, while the MDSS method gave good result when the noise in one motion was stable.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-260"
  },
  "kraft05_interspeech": {
   "authors": [
    [
     "Florian",
     "Kraft"
    ],
    [
     "Robert",
     "Malkin"
    ],
    [
     "Thomas",
     "Schaaf"
    ],
    [
     "Alex",
     "Waibel"
    ]
   ],
   "title": "Temporal ICA for classification of acoustic events i a kitchen environment",
   "original": "i05_2689",
   "page_count": 4,
   "order": 261,
   "p1": "2689",
   "pn": "2692",
   "abstract": [
    "We describe a feature extraction method for general audio modeling using a temporal extension of Independent Component Analysis (ICA) and demonstrate its utility in the context of a sound classification task in a kitchen environment. Our approach accounts for temporal dependencies over multiple analysis frames much like the standard audio modeling technique of adding first and second temporal derivatives to the feature set. Using a real-world dataset of kitchen sounds, we show that our approach outperforms a canonical version of this standard front end, the mel-frequency cepstral coefficients (MFCCs), which has found successful application in automatic speech recognition tasks.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-261"
  },
  "krebber05_interspeech": {
   "authors": [
    [
     "Jan Felix",
     "Krebber"
    ]
   ],
   "title": "hello - is anybody at home? - about the minimum word accuracy of a smart home spoken dialogue system",
   "original": "i05_2693",
   "page_count": 4,
   "order": 262,
   "p1": "2693",
   "pn": "2696",
   "abstract": [
    "This paper presents the assessment of the minimum word accuracy of an automatic speech recognizer within a spoken dialogue system, which is required to reach acceptable quality. The spoken dialogue system has been developed in the context of the European INSPIRE project [3]. It is used to control a smart home environment. The assessment of the minimum word accuracy is done within the assessment of all modules of the INSPIRE system. The outcome of this assessment will act as a benchmark for the developers of other modules, mainly of the speech preprocessing (microphone array with beam forming and denoising) and for the engineers and phoneticians tuning the automatic speech recognizer. The article describes the test and the outcome of the estimation of the minimum word accuracy necessary to use INSPIRE in an acceptable way.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-262"
  },
  "hirsch05_interspeech": {
   "authors": [
    [
     "H. Gunter",
     "Hirsch"
    ],
    [
     "Harald",
     "Finster"
    ]
   ],
   "title": "The simulation of realistic acoustic input scenarios for speech recognition systems",
   "original": "i05_2697",
   "page_count": 4,
   "order": 263,
   "p1": "2697",
   "pn": "2700",
   "abstract": [
    "A tool for simulating the acoustic conditions during the speech input to a recognition system and the transmission in telephone networks is presented in this paper. The simulation covers the hands-free speech input in rooms and the existence of noise in the background. Furthermore the presence of telephone frequency characteristics can be simulated. Finally the transmission in a cellular telephone system like GSM or UMTS is covered including the encoding and decoding of speech and the transmission over the erroneous radio channel.\n",
    "The tool has been realized by integrating functions from the ITU software library for implementing telephone frequency characteristics and the estimation of the speech level as well as software modules from ETSI and 3GPP for the AMR encoding and decoding of speech.\n",
    "A Web interface has been designed to experience the simulation tool with acoustic examples.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-263"
  },
  "walsh05_interspeech": {
   "authors": [
    [
     "Michael",
     "Walsh"
    ],
    [
     "Gregory M. P.",
     "O'Hare"
    ],
    [
     "Julie",
     "Carson-Berndsen"
    ]
   ],
   "title": "An agent-based framework for speech investigation",
   "original": "i05_2701",
   "page_count": 4,
   "order": 264,
   "p1": "2701",
   "pn": "2704",
   "abstract": [
    "This paper presents a novel agent-based framework for investigating speech recognition which combines statistical data and explicit phonological knowledge in order to explore strategies aimed at augmenting the performance of automatic speech recognition (ASR) systems. This line of research is motivated by a desire to provide solutions to some of the more notable problems encountered, including in particular the problematic phenomena of coarticulation, underspecified input, and out-of-vocabulary items. This research also seeks to promote the use of deliberative reasoning agents in the speech and natural language processing arenas.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-264"
  },
  "liao05_interspeech": {
   "authors": [
    [
     "H.",
     "Liao"
    ],
    [
     "M. J. F.",
     "Gales"
    ]
   ],
   "title": "Joint uncertainty decoding for noise robust speech recognition",
   "original": "i05_3129",
   "page_count": 4,
   "order": 265,
   "p1": "3129",
   "pn": "3132",
   "abstract": [
    "Background noise can have a significant impact on the performance of speech recognition systems. A range of fast feature-space and model-based schemes have been investigated to increase robustness. Model-based approaches typically achieve lower error rates, but at an increased computational load compared to feature-based approaches. This makes their use in many situations impractical. The uncertainty decoding framework can be considered an elegant compromise between the two. Here, the uncertainty of features is propagated to the recogniser in a mathematically consistent fashion. The complexity of the model used to determine the uncertainty may be decoupled from the recognition model itself, allowing flexibility in the computational load. This paper describes a new approach within this framework, Joint uncertainty decoding. This approach is compared with the uncertainty decoding version of SPLICE, standard SPLICE, and a new form of front-end CMLLR. These are evaluated on a medium vocabulary speech recognition task with artificially added noise.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-265"
  },
  "vanhoucke05_interspeech": {
   "authors": [
    [
     "Vincent",
     "Vanhoucke"
    ]
   ],
   "title": "Confidence scoring and rejection using multi-pass speech recognition",
   "original": "i05_3133",
   "page_count": 4,
   "order": 266,
   "p1": "3133",
   "pn": "3136",
   "abstract": [
    "This paper presents a computationally efficient method for using multiple speech recognizers in a multi-pass framework to improve the rejection performance of an automatic speech recognition system. A set of criteria is proposed, which determine at run time when rescoring using a second pass is expected to improve the rejection performance. The second pass result is used along with a set of features derived from the first pass to compute a combined confidence score. The feature combination is optimized globally based on training data. The combined system significantly outperforms a simple two-pass system at little more computational cost than comparable one-pass and two-pass systems.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-266"
  },
  "lee05b_interspeech": {
   "authors": [
    [
     "Cheng-Lung",
     "Lee"
    ],
    [
     "Wen-Whei",
     "Chang"
    ]
   ],
   "title": "Memory-enhanced MMSE-based channel error mitigation for distributed speech recognition",
   "original": "i05_3137",
   "page_count": 4,
   "order": 267,
   "p1": "3137",
   "pn": "3140",
   "abstract": [
    "A new approach to sequence minimum mean-squared error (SMMSE) decoding for vector quantization over channels with memory is presented. The decoder is based on the Gilbert channel model that allows the exploitation of intra-vector correlation of bit error sequences. We apply the memory-enhanced SMMSE decoding algorithm to channel error mitigation in distributed speech recognition. Experiments on Mandarin digit string recognition task indicate that with the aid of Gilbert channel characterization, the proposed scheme obtains better performance than the ETSI mitigation algorithm under GSM channel conditions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-267"
  },
  "fukuda05_interspeech": {
   "authors": [
    [
     "Takashi",
     "Fukuda"
    ],
    [
     "Muhammad",
     "Ghulam"
    ],
    [
     "Tsuneo",
     "Nitta"
    ]
   ],
   "title": "Designing multiple distinctive phonetic feature extractors for canonicalization by using clustering technique",
   "original": "i05_3141",
   "page_count": 4,
   "order": 268,
   "p1": "3141",
   "pn": "3144",
   "abstract": [
    "Acoustic models of an HMM-based classifier include various types of hidden factors such as speaker-specific characteristics and acoustic environments. If there exist a canonicalization process that represses the decrease of differences in acoustic-likelihood among categories resulted from hidden factors, a robust ASR system can be realized. We have previously proposed the canonicalization process of feature-parameters composed of three distinctive phonetic feature (DPF) extractors focused on a gender factor. This paper describes an attempt to design multiple DPF extractors corresponding to unspecific hidden factors, as well as to introduce a noise suppressor that is targeted for the canonicalization of a noise factor. In an experiment on Japanese version AURORA2 database (AURORA2-J), the proposed system achieved significant improvements when combining the canonicalization process with the noise reduction technique based on a two-stage Wiener filter.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-268"
  },
  "kinoshita05_interspeech": {
   "authors": [
    [
     "Keisuke",
     "Kinoshita"
    ],
    [
     "Tomohiro",
     "Nakatani"
    ],
    [
     "Masato",
     "Miyoshi"
    ]
   ],
   "title": "Efficient blind dereverberation framework for automatic speech recognition",
   "original": "i05_3145",
   "page_count": 4,
   "order": 269,
   "p1": "3145",
   "pn": "3148",
   "abstract": [
    "A speech signal captured by a distant microphone is generally smeared by reverberation, which severely degrades Automatic Speech Recognition (ASR) performance. In this paper, we propose a novel and practical single channel dereverberation scheme, which utilizes the relationship between speech and reverberation. A dereverberation filter derived by the proposed method is capable of efficiently suppressing late reflections, which constitute a major cause of ASR performance degradation. The proposed algorithm can achieve effective dereverberation with a more reasonable computational complexity than conventional methods. Experimental results reveal a substantial improvement in ASR performance even in severely reverberant environments.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-269"
  },
  "wolfel05b_interspeech": {
   "authors": [
    [
     "Matthias",
     "Wölfel"
    ],
    [
     "John",
     "McDonough"
    ]
   ],
   "title": "Combining multi-source far distance speech recognition strategies: beamforming, blind channel and confusion network combination",
   "original": "i05_3149",
   "page_count": 4,
   "order": 270,
   "p1": "3149",
   "pn": "3152",
   "abstract": [
    "Interest within the automatic speech recognition (ASR) research community has recently focused on the recognition of speech captured with a microphone located in the medium field, rather than being mounted on a headset and positioned next to the speaker's mouth. The capacity to recognize such speech is a primary requirement in making ASR a viable modality for so-called ubiquitous computing. This is a natural application for multiple microphones whose signals can be combined in different ways: On the signal side, combination can be accomplished by beamforming techniques using a microphone array or by blind source separation. On the word hypothesis side, combination can be achieved through confusion network combination. In this work, we compare the effectiveness of the several combination techniques, and compare their performance to that achieved with a close talking microphone.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-270"
  },
  "alexander05_interspeech": {
   "authors": [
    [
     "Jennifer A.",
     "Alexander"
    ],
    [
     "Patrick C. M.",
     "Wong"
    ],
    [
     "Ann R.",
     "Bradlow"
    ]
   ],
   "title": "Lexical tone perception in musicians and non-musicians",
   "original": "i05_0397",
   "page_count": 4,
   "order": 271,
   "p1": "397",
   "pn": "400",
   "abstract": [
    "It has been suggested that music and speech maintain entirely dissociable mental processing systems. The current study, however, provides evidence that there is an overlap in the processing of certain shared aspects of the two. This study focuses on fundamental frequency (pitch), which is an essential component of melodic units in music and lexical and/or intonational units in speech. We hypothesize that extensive experience with the processing of musical pitch can transfer to a lexical pitch-processing domain. To that end, we asked nine English-speaking musicians and nine English-speaking non-musicians to identify and discriminate the four lexical tones of Mandarin Chinese. The subjects performed significantly differently on both tasks; the musicians identified the tones with 89% accuracy and discriminated them with 87% accuracy, while the non-musicians identified them with only 69% accuracy and discriminated them with 71% accuracy. These results provide counter-evidence to the theory of dissociation between music and speech processing.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-271"
  },
  "ma05b_interspeech": {
   "authors": [
    [
     "Joan K.-Y.",
     "Ma"
    ],
    [
     "Valter",
     "Ciocca"
    ],
    [
     "Tara",
     "Whitehill"
    ]
   ],
   "title": "Contextual effect on perception of lexical tones in Cantonese",
   "original": "i05_0401",
   "page_count": 4,
   "order": 272,
   "p1": "401",
   "pn": "404",
   "abstract": [
    "The present study investigated the role of tonal context (extrinsic information) in the perception of Cantonese lexical tones. Target tones at three separate positions (initial, medial and final position) were recorded by two speakers (one male and one female). These sentences were edited and presented in three conditions: original carrier (target within the original context), isolation (target without context) and neutral carrier (target word as appended at the final apposition within a new carrier). Nine female listeners were asked to identify the tones by matching targets with Chinese characters. Perceptual data showed that tones presented within the original carrier were more accurately perceived than targets presented in isolation, showing the importance of extrinsic information in the perception of lexical tones. In the neutral carrier condition, tones of the final position showed perceptual accuracy significantly above targets of the initial and medial positions. The perceptual error patterns suggested that listeners placed more emphasis on the immediate context preceding the target in tone identification. When tones were presented without an extrinsic context, the proportion of errors for each tone differed. Most of the errors involved misidentifying targets as tones of same F0 contour but different level. The results showed that the importance of extrinsic information on the perception of lexical tones was mainly on identification of F0 level while the intrinsic acoustic properties of the tone helped in identifying the F0 contour.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-272"
  },
  "mixdorff05_interspeech": {
   "authors": [
    [
     "Hansjörg",
     "Mixdorff"
    ],
    [
     "Yu",
     "Hu"
    ],
    [
     "Denis",
     "Burnham"
    ]
   ],
   "title": "Visual cues in Mandarin tone perception",
   "original": "i05_0405",
   "page_count": 4,
   "order": 273,
   "p1": "405",
   "pn": "408",
   "abstract": [
    "This paper presents results concerning the exploitation of visual cues in the perception of Mandarin tones. The lower part of a female speaker's face was recorded on digital video as she uttered 25 sets of syllabic tokens covering the four different tones of Mandarin. Then in a perception study the audio sound track alone, as well an audio plus video condition were presented to native Mandarin speakers who were required to decide which tone they perceived. Audio was presented in various conditions: clear, babble-noise masked at different SNR levels, as well as devoiced and amplitude-modulated noise conditions using LPC resynthesis. In the devoiced and the clear audio conditions, there is little augmentation of audio alone due to the addition of video. However, the addition of visual information did significantly improve perception in the babble-noise masked condition, and this effect increased with decreasing SNR. This outcome suggests that the improvement in noise-masked conditions is not due to additional information in the video per se, but rather to an effect of early integration of acoustic and visual cues facilitating auditory-visual speech perception.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-273"
  },
  "mixdorff05b_interspeech": {
   "authors": [
    [
     "Hansjörg",
     "Mixdorff"
    ],
    [
     "Yu",
     "Hu"
    ]
   ],
   "title": "Cross-language perception of word stress",
   "original": "i05_0409",
   "page_count": 4,
   "order": 274,
   "p1": "409",
   "pn": "412",
   "abstract": [
    "This paper presents a study of the perception of Mandarin disyllabic words by native speakers of German. It examines how speakers of an accent language perceive word stress in words from a tone language. A corpus of 15 sets of words with all possible combinations of the four tones of Mandarin was recorded by a professional speaker. In addition monotonized versions of the words were created. In a forced-choice listening experiment native speakers of German were asked to assess whether they perceived the word stress on the first or second syllable. Results include, inter alia, that words with two high tones, as well as the monotonized stimuli were predominantly perceived as carrying the word stress on the first syllable. Words with a falling tone on the second syllable were mostly classified as carrying stress on the second syllable, with the combination of low and falling tone yielding the highest score. Many combinations of tones, however, could not be identified as any of the two kinds. This suggests that though some tonal configurations in Mandarin are similar to German two-syllable word accent patterns and can be associated with the latter, others might be rather interpreted as pertaining to two mono-syllabic words, both of which are stressed.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-274"
  },
  "cutler05_interspeech": {
   "authors": [
    [
     "Anne",
     "Cutler"
    ]
   ],
   "title": "The lexical statistics of word recognition problems caused by L2 phonetic confusion",
   "original": "i05_0413",
   "page_count": 4,
   "order": 275,
   "p1": "413",
   "pn": "416",
   "abstract": [
    "Phonemic confusions in L2 listening lead to three types of problem at the lexical level: inability to distinguish minimal pairs (e.g. write, light), spurious activation of embedded words (e.g. write in delighted) and delay in resolution of ambiguity (e.g. distinction between register and legislate at the sixth instead of the first phoneme) The statistics of each of these, computed from a 70,000+ word English lexicon backed by frequency statistics from a 17.9 million word corpus, establish that each causes substantial added difficulty for L2 listeners.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-275"
  },
  "huang05c_interspeech": {
   "authors": [
    [
     "Chun-Fang",
     "Huang"
    ],
    [
     "Masato",
     "Akagi"
    ]
   ],
   "title": "A multi-layer fuzzy logical model for emotional speech perception",
   "original": "i05_0417",
   "page_count": 4,
   "order": 276,
   "p1": "417",
   "pn": "420",
   "abstract": [
    "This paper proposes a three-layer model for the perception of emotional speech. Much of the earlier work has focused on the relationship between emotional speech and acoustic features that is characterized by statistics. The problem is that there lacks a model which takes the aspects of vagueness and human perception into consideration. In the model proposed here, five categories of emotion constitute the topmost layer of the model. Primitive features, which are the linguistic form of adjectives, constitute the middle layer, which is used to describe the human perceptual aspect. The bottommost layer is acoustic features. Three experiments followed by MDS analysis revealed suitable primitive features. And then, fuzzy inference systems were built to map the vagueness nature of the relationship between emotions and the primitive features. Acoustic features including F0, time duration, power envelope, and power spectrum were analyzed; subsequent regression analysis revealed correlation between the primitive features and the acoustic features. The experimental results and the resulting fuzzy inference systems show a significant relationship between emotions and the primitive features. The results of the analysis also show some acoustic features that have positive or negative correlation with primitive features.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-276"
  },
  "tran05_interspeech": {
   "authors": [
    [
     "Do Dat",
     "Tran"
    ],
    [
     "Eric",
     "Castelli"
    ],
    [
     "Jean-François",
     "Serignat"
    ],
    [
     "Van Loan",
     "Trinh"
    ],
    [
     "Xuan Hung",
     "Le"
    ]
   ],
   "title": "Influence of F0 on Vietnamese syllable perception",
   "original": "i05_1697",
   "page_count": 4,
   "order": 277,
   "p1": "1697",
   "pn": "1700",
   "abstract": [
    "Understanding and managing tonal characteristics of Vietnamese language is one of the most difficult aspects in Vietnamese speech processing. However, at present, there is no common agreement about the influence of fundamental frequency (F0) on the perception of Vietnamese syllables. Thus, instead of analyzing the F0 of a limited number of Vietnamese syllables like other methods found in literature, this paper will present a new methodology based on, first the synthesis of arbitrary syllables, and secondly perception tests. This approach permits us to understand and define more precisely the role of F0 in the characterisation of Vietnamese tones.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-277"
  },
  "schwanhauer05_interspeech": {
   "authors": [
    [
     "Barbara",
     "Schwanhäußer"
    ],
    [
     "Denis",
     "Burnham"
    ]
   ],
   "title": "Lexical tone and pitch perception in tone and non-tone language speakers",
   "original": "i05_1701",
   "page_count": 4,
   "order": 278,
   "p1": "1701",
   "pn": "1704",
   "abstract": [
    "Past research on categorical perception of lexical tone has produced contradictory results. In Experiment 1 tonal (Mandarin, Vietnamese) and non-tonal (Australian) adults were tested for identification and discrimination on speech and non-speech (sine-wave) tone continua. Tonal language speakers' category boundaries and discrimination peaks were near the middle of the asymmetric continuum, whereas non-tonal speakers used an acoustically flat stimulus as a reference, indicating that tone space is linguistically oriented in tonal, and acoustically oriented in non-tonal language speakers. In Experiment 2, three tonal-language (Thai) groups (musicians, perfect pitch musicians, and non-musicians) were tested on two new continua represented as speech or sine-wave tones. Identification boundaries were in the middle of the continuum for most participants. In discrimination, the flat stimulus was used as a perceptual anchor, and this was independent of musical background, indicating that the musical Thai participants use the same mid-continuum strategy as the Mandarin and Vietnamese speakers in identification, but the flat no-contour strategy in discrimination. Hence, perception depends on the type of task in Thai speakers: it is linguistic in identification, but acoustic in discrimination.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-278"
  },
  "fale05b_interspeech": {
   "authors": [
    [
     "Isabel",
     "Falé"
    ],
    [
     "Isabel",
     "Hub Faria"
    ]
   ],
   "title": "Intonational contrasts in EP: a categorical perception approach",
   "original": "i05_1705",
   "page_count": 4,
   "order": 279,
   "p1": "1705",
   "pn": "1708",
   "abstract": [
    "European Portuguese (EP) intonational contrast between statement and question contours was tested on a Categorical Perception based paradigm. From a natural sentence produced by a male speaker, a 15-step continuum was created, from declarative to question contour, through acoustic manipulation (PSOLA) and submitted to 20 EP listeners that performed two tasks: an identification and a discrimination task.\n",
    "For identification test, subjects had to categorize each presented stimulus. In addition to response data, reaction times of the identification task were also collected. For the discrimination test, subjects were presented with an AX discrimination task and had to decide whether the stimuli in each pair were equal or different. Experimental design and procedures were developed with E-Prime.\n",
    "Identification results confirmed that the contrast is indeed categorical. However, identification reaction-time measurements point to continuous rather than categorical perception. The absence of a consistent peak of discrimination in the cross-over between categories supports the continuous perception view.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-279"
  },
  "braun05_interspeech": {
   "authors": [
    [
     "Bettina",
     "Braun"
    ],
    [
     "Andrea",
     "Weber"
    ],
    [
     "Matthew",
     "Crocker"
    ]
   ],
   "title": "Does narrow focus activate alternative referents?",
   "original": "i05_1709",
   "page_count": 4,
   "order": 280,
   "p1": "1709",
   "pn": "1712",
   "abstract": [
    "Narrow focus refers to accent placement that forces one interpretation of a sentence, which is then often perceived contrastively. Narrow focus is formalised in terms of alternative sets, i.e. contextually or situationally salient alternatives. In this paper, we investigate whether this model is valid also in human utterance processing. We present an eye-tracking experiment to study listeners' expectations (i.e. eye-movements) with respect to upcoming referents. Some of the objects contrast in colour with objects that were previously referred to, others do not; the objects are referred to with either a narrow focus on the colour adjective or with broad focus on the noun. Results show that narrow focus on the adjective increases early fixations to contrastive referents. Narrow focus hence activates alternative referents in human utterance processing.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-280"
  },
  "aikawa05_interspeech": {
   "authors": [
    [
     "Kiyoaki",
     "Aikawa"
    ],
    [
     "Hayato",
     "Hashimoto"
    ]
   ],
   "title": "Audiovisual interaction on the perception of frequency glide of linear sweep tones",
   "original": "i05_1713",
   "page_count": 4,
   "order": 281,
   "p1": "1713",
   "pn": "1716",
   "abstract": [
    "Audiovisual interaction was found on the perception of frequency glide of linear sweep tones. The auditory and visual stimuli were designed to simulate the formant trajectories of the syllables that induced the McGurk effect. Linear sweep tones were used for the auditory stimuli. For the visual stimuli, moving spheres were used. The frequency glide was differently perceived depending on the sphere motion. The experimental results showed that the identification of linear frequency glide was significantly different between with and without the motion of the visual stimuli. An interesting result was that frequency-glide was perceived for a steady tone presented with the enlarging sphere. Enlarging and shrinking motion of the sphere had strongest effect to enhance the perception of frequency glide. However, linear movement of the sphere did not show clear audiovisual interaction.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-281"
  },
  "omata05_interspeech": {
   "authors": [
    [
     "Kei",
     "Omata"
    ],
    [
     "Ken",
     "Mogi"
    ]
   ],
   "title": "Audiovisual integration in dichotic listening",
   "original": "i05_1717",
   "page_count": 4,
   "order": 282,
   "p1": "1717",
   "pn": "1720",
   "abstract": [
    "The presentation of visual stimuli of moving mouth is known to affect the phoneme perception of the auditory stimuli during natural speech, and provides important insights into the scientific and technological study of speech. In order to investigate the laterality of the audio-visual integration we studied the relationship between the McGurk effect (McGurk and MacDonald 1976) and Right Ear Advantage (REA) in dichotic listening (DL). When visual /ga/ was presented with the dichotic auditory stimuli, both the McGurk effect and REA was observed. On the other hand, when visual /ba/ was paired with the dichotic auditory stimuli, the visual /ba/ strongly affected the proportion of the phoneme perceived. In addition, the phoneme perception in the DL was influenced by the auditory syllables fed into the right ear more than those fed into the left ear. These results suggest that the cortical processes that underlie the McGurk effect is consistent with, and probably shares the same process with, those that lead to REA.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-282"
  },
  "svanfeldt05_interspeech": {
   "authors": [
    [
     "Gunilla",
     "Svanfeldt"
    ],
    [
     "Dirk",
     "Olszewski"
    ]
   ],
   "title": "Perception experiment combining a parametric loudspeaker and a synthetic talking head",
   "original": "i05_1721",
   "page_count": 4,
   "order": 283,
   "p1": "1721",
   "pn": "1724",
   "abstract": [
    "By combining the technologies of targeted audio and talking heads, a perception experiment was performed. Unvoiced consonants in a vowel context produced using speech synthesis were to be identified. It was found that the talking head could eliminate some of the confusions between consonants that occurred when the face was not present. The study also gave the possibility to analyse distortions of the speech signal due to the targeted audio device.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-283"
  },
  "mayo05_interspeech": {
   "authors": [
    [
     "Catherine",
     "Mayo"
    ],
    [
     "Robert A. J.",
     "Clark"
    ],
    [
     "Simon",
     "King"
    ]
   ],
   "title": "Multidimensional scaling of listener responses to synthetic speech",
   "original": "i05_1725",
   "page_count": 4,
   "order": 284,
   "p1": "1725",
   "pn": "1728",
   "abstract": [
    "The move to unit-selection in speech synthesis has resulted in system improvements being made at subtle sub- and supra-segmental levels. Human perceptual evaluation of such subtle improvements requires a highly sophisticated level of perceptual attention to specific acoustic characteristics or cues. However, it is not well understood what acoustic cues listeners attend to by default when asked to evaluate synthetic speech. It may, therefore, be potentially quite difficult to design an evaluation method that allows listeners to concentrate on only one dimension of the signal, while ignoring others that are perceptually more important to them.\n",
    "This paper describes a pilot study which aims to evaluate multidimensional scaling (MDS) as a possible method of determining what acoustic characteristics of synthetic speech influence listeners' judgements of the naturalness of the speech. Using distance measures (either real or perceived distances), MDS techniques represent stimuli as points in n-dimensional space. The space is configured so that similar stimuli are close together, while different stimuli are farther apart. Additionally, the dimensions of the space correspond to characteristics of the stimuli which influenced the perceived distances.\n",
    "Our results indicate that MDS techniques should be a useful tool in understanding the complex psychoacoustic processes that listeners undergo when evaluating synthetic speech. This method has allowed us to identify a number of cues that appear to be particularly perceptually salient to listeners evaluating synthetic speech naturalness, namely prosodic cues (in terms of duration and/or intonation) and segmental or unit level cues (in terms of appropriateness of units, or number of units).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-284"
  },
  "terasawa05_interspeech": {
   "authors": [
    [
     "Hiroko",
     "Terasawa"
    ],
    [
     "Malcolm",
     "Slaney"
    ],
    [
     "Jonathan",
     "Berger"
    ]
   ],
   "title": "A timbre space for speech",
   "original": "i05_1729",
   "page_count": 4,
   "order": 285,
   "p1": "1729",
   "pn": "1732",
   "abstract": [
    "We describe a perceptual space for timbre, define an objective metric that takes into account perceptual orthogonality and measure the quality of timbre interpolation. We discuss two timbre representations and measure perceptual judgments. We determine that a timbre space based on Mel-frequency cepstral coefficients (MFCC) is a good model for perceptual timbre space.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-285"
  },
  "kacha05_interspeech": {
   "authors": [
    [
     "A.",
     "Kacha"
    ],
    [
     "Francis",
     "Grenez"
    ],
    [
     "Jean",
     "Schoentgen"
    ]
   ],
   "title": "Voice quality assessment by means of comparative judgments of speech tokens",
   "original": "i05_1733",
   "page_count": 4,
   "order": 286,
   "p1": "1733",
   "pn": "1736",
   "abstract": [
    "The presentation concerns a perceptual assessment of disordered speech, which rests on comparative judgments. Pairs of stimuli are presented randomly to listeners who are asked to label the most degraded sample. At the end of a listening session, speech signals are assigned overall scores on the base of individual comparative judgments. To contrast the perceptual assessment of voice disorders via comparative judgments with a conventional method, two groups of listeners have taken part in the experiment. A first group has been comprised of six naive listeners, i.e., listeners without any training in speech therapy or laryngology. A second group of listeners has been comprised of three speech therapists. They rated from 0 to 3 the grade G (of the GRBAS scale) of each sound sample by means of an ordinal scale. Results show that comparative judgments give rise to high intra-judge and inter-judge agreements for sustained vowel [a] as well as for sentences even though the listeners had no prior experience in rating disordered voices.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-286"
  },
  "irino05_interspeech": {
   "authors": [
    [
     "Toshio",
     "Irino"
    ],
    [
     "Satoru",
     "Satou"
    ],
    [
     "Shunsuke",
     "Nomura"
    ],
    [
     "Hideki",
     "Banno"
    ],
    [
     "Hideki",
     "Kawahara"
    ]
   ],
   "title": "Speech intelligibility derived from time-frequency and source smearing",
   "original": "i05_1737",
   "page_count": 4,
   "order": 287,
   "p1": "1737",
   "pn": "1740",
   "abstract": [
    "We investigated speech intelligibility of four-mora word sounds degraded with a system based on a high quality vocoder, STRAIGHT, and warped-DCT. This system enables us to independently manipulate essential speech parameters for vocal tract filtering and glottal excitation. We report perceptual effects of: 1) temporal smearing' or reduced temporal modulation; 2) time-frequency smearing' or reduced resolution in both temporal modulation and spectral peak; and 3) source smearing' or reduced resolution of glottal pulses. By analyzing intelligibility scores from the various experiments, we quantitatively confirmed that there are linguistic dependencies of phonemes and morae within words.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-287"
  },
  "hayashi05_interspeech": {
   "authors": [
    [
     "Nahoko",
     "Hayashi"
    ],
    [
     "Takayuki",
     "Arai"
    ],
    [
     "Nao",
     "Hodoshima"
    ],
    [
     "Yusuke",
     "Miyauchi"
    ],
    [
     "Kiyohiro",
     "Kurisu"
    ]
   ],
   "title": "Steady-state pre-processing for improving speech intelligibility in reverberant environments: evaluation in a hall with an electrical reverberator",
   "original": "i05_1741",
   "page_count": 4,
   "order": 288,
   "p1": "1741",
   "pn": "1744",
   "abstract": [
    "To improve speech intelligibility in reverberant environments, Arai et al. proposed the methods of \"steady-state suppression\" (Arai, 2002) and \"steady-state zero-padding\" (Arai, 2005) as a pre-processing method. We conducted two perceptual experiments to evaluate and compare these methods in a hall with an electrical reverberator. The advantage of using this electrical reverberator is that the same subjects are able to participate in the experiments with many different reverberant conditions (reverberation time was varied from 2.6-3.3s, in this study). As results, steady-state suppression showed the floor effect under these reverberant conditions, whereas steady-state zero-padding yielded significant improvements in terms of the speech intelligibility in the same reverberant conditions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-288"
  },
  "wong05_interspeech": {
   "authors": [
    [
     "Patrick C.M.",
     "Wong"
    ],
    [
     "Kiara M.",
     "Lee"
    ],
    [
     "Todd B.",
     "Parrish"
    ]
   ],
   "title": "Neural bases of listening to speech in noise",
   "original": "i05_1745",
   "page_count": 4,
   "order": 289,
   "p1": "1745",
   "pn": "1748",
   "abstract": [
    "Ubiquitous speech processing involves listening to speech in ecological environments where noise is often present. The current study investigates the neural mechanisms involved in perceiving speech in noise using a sparse sampling fMRI method. Subjects were asked to match auditorily presented words with picture choices. The auditory stimuli were either presented in quiet or embedded in multi-talker babble noise without the presence of scanner noise in either condition. Behaviorally, it was found that subjects were slower and less accurate in identifying words presented in noise. Comparison of hemodynamic responses associated with listening to the two types of stimuli revealed increased activation in left prefrontal, inferior frontal, anterior insular, and superior temporal regions when subjects listened to speech in noise. These results confirm the importance of the lateral auditory cortex in complex auditory processing and suggest that the prefrontal cortex is likely to be prominently engaged in subvocal rehearsal when noise is affecting the integrity of the speech signal.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-289"
  },
  "jongmans05_interspeech": {
   "authors": [
    [
     "P.",
     "Jongmans"
    ],
    [
     "F. J. M.",
     "Hilgers"
    ],
    [
     "Louis C. W.",
     "Pols"
    ],
    [
     "C. J. van",
     "As-Brooks"
    ]
   ],
   "title": "The intelligibility of tracheoesophageal speech: first results",
   "original": "i05_1749",
   "page_count": 4,
   "order": 290,
   "p1": "1749",
   "pn": "1752",
   "abstract": [
    "A total laryngectomy changes the anatomy and physiology of the vocal tract, with a most noticeable effect on speech. By applying a voice prosthesis, enabling the patient to use tracheoesophageal (TE) speech, speech is of better quality than with esophageal or electrolarynx speech, but still very deviant from laryngeal speech. Most studies on TE speech have focused on voice quality. For the understanding of the physiology of the neoglottis and for improving the results of rehabilitation, it is important to study intelligibility as well. This paper will discuss the first results of a study on TE speech intelligibility.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-290"
  },
  "brown05_interspeech": {
   "authors": [
    [
     "Guy J.",
     "Brown"
    ],
    [
     "Kalle J.",
     "Palomäki"
    ]
   ],
   "title": "A computational model of the speech reception threshold for laterally separated speech and noise",
   "original": "i05_1753",
   "page_count": 4,
   "order": 291,
   "p1": "1753",
   "pn": "1756",
   "abstract": [
    "Recent psychophysical studies suggest that human listeners do not segregate concurrent sounds by grouping frequency regions that have a common interaural time difference (ITD). However, such an approach is adopted by most computational auditory scene analysis (CASA) systems that use binaural cues. Here, we propose a CASA system that separates a target speech signal from a noise interferer, but does not require the ITD of the two sources to be consistent across frequency. We compare the CASA system with human performance on the same task, in which the speech reception threshold (SRT) is measured for speech and noise stimuli which have consistent or inconsistent ITDs in different frequency bands. The CASA system is shown to be in qualitative agreement with human performance.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-291"
  },
  "janse05_interspeech": {
   "authors": [
    [
     "Esther",
     "Janse"
    ]
   ],
   "title": "Lexical inhibition effects in time-compressed speech",
   "original": "i05_1757",
   "page_count": 4,
   "order": 292,
   "p1": "1757",
   "pn": "1760",
   "abstract": [
    "A recent study claimed that inhibitory processes in spoken language processing are relatively slow and may be strongly reduced in listening conditions that require extra processing resources. Time compression of a sentence context did not reduce facilitation of a congruent word target, but inhibition of word targets that were incongruent with the preceding sentence context was strongly reduced. The central question in this paper is whether lexical competition/inhibition effects due to word-initial overlap between primes and targets will be similarly affected by moderate time compression. The alternative option is that inhibition is not affected by time compression because lexical inhibition processes are qualitatively different from sentence-priming expectancy-based processes. The results of a prime-target experiment showed evidence for the latter option. Inhibition due to lateral competition seems to be a fast process that is not affected by increased processing load.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-292"
  },
  "jacquier05_interspeech": {
   "authors": [
    [
     "Caroline",
     "Jacquier"
    ],
    [
     "Fanny",
     "Meunier"
    ]
   ],
   "title": "Perception of time-compressed rapid acoustic cues in French CV syllables",
   "original": "i05_1761",
   "page_count": 4,
   "order": 293,
   "p1": "1761",
   "pn": "1764",
   "abstract": [
    "The cognitive use of the phonetic and acoustic features still needs to be specified for speech comprehension. Several studies have established that children with language impairments like dyslexia exhibit deficits in perceiving rapid speech sounds. Our study explored the temporal encoding of acoustic cues during natural speech perception. We focused on two short attributes of speech: Voice Onset Time (VOT) and formant transitions. Bisyllabic CVCVs were constructed using stop consonants (/b/, /d/, /p/ and /t/) and vowels (/a/ and /i/). Normal hearing subjects had to identify the stimuli time-compressed acoustic cues simultaneously (Experiment 1) and separately on each cue (Experiments 2 and 3). Our results showed a non additivity of the acoustic cues and demonstrated that the VOT is a greater temporal cue than the formant transition. In addition, the redundancy of those ones is used to restore the degraded speech signal. However, both acoustic cues are needed to realize a fine perception of speech.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-293"
  },
  "grataloup05_interspeech": {
   "authors": [
    [
     "C.",
     "Grataloup"
    ],
    [
     "M.",
     "Hoen"
    ],
    [
     "F.",
     "Pellegrino"
    ],
    [
     "E.",
     "Veuillet"
    ],
    [
     "L.",
     "Collet"
    ],
    [
     "Fanny",
     "Meunier"
    ]
   ],
   "title": "Reversed speech comprehension depends on the auditory efferent system functionality",
   "original": "i05_1765",
   "page_count": 4,
   "order": 294,
   "p1": "1765",
   "pn": "1768",
   "abstract": [
    "In the present study we explore the implication of high and low level mechanisms in degraded (time-reversed) speech comprehension in normal hearing subjects. In experiment 1 we compared the loss of intelligibility due to the increasing size of reversion windows in both words and pseudowords. Results showed that words are generally reconstructed better than pseudowords, suggesting the existence of a lexical benefit in degraded speech restoration. Moreover, there was greater variability between individuals when reconstructing pseudowords than words. In experiment 2, we demonstrated that this interindividual variability correlated with the subjects' medial olivocochlear bundle functionality, as measured by contralateral suppression of otoacoustic emissions (OAEs). Together these experiments highlight the importance of low-level auditory mechanisms in degraded speech restoration. Moreover they put forward the existence of major interindividual variability in the capacity to reconstruct degraded speech, which correlates with the physiological properties of the auditory system (low-level property). In addition, our results also suggest the existence of multiple higher-level strategies that can compensate on-line for the lack of information caused by speech degradation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-294"
  },
  "tokuma05_interspeech": {
   "authors": [
    [
     "Won",
     "Tokuma"
    ],
    [
     "Shinichi",
     "Tokuma"
    ]
   ],
   "title": "Perceptual space of English fricatives for Japanese learners",
   "original": "i05_1769",
   "page_count": 4,
   "order": 295,
   "p1": "1769",
   "pn": "1772",
   "abstract": [
    "This study investigates the relationship between perceptual and physical spatial configurations of English fricatives judged by Japanese learners. Multidimensional scaling analyses were used to obtain spatial representations from similarity judgements and spectral distance analysis. The 2-dimensional perceptual solution did not correspond well to its physical solution. For the physical space, the two dimensions could be interpreted as the phonetic feature properties of sibilance' and place' of articulation for fricatives. For the perceptual space, the sibilance property was maintained while the place property was not. In terms of their spectral properties, overall spectral shape was a salient acoustic cue for L2 learners but individual spectral peak frequencies seem to be language specific. The results imply that L2 perception was not based on the acoustic signals, but largely influenced by the learners' native language phonological structure.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-295"
  },
  "vasilescu05_interspeech": {
   "authors": [
    [
     "Ioana",
     "Vasilescu"
    ],
    [
     "Maria",
     "Candea"
    ],
    [
     "Martine",
     "Adda-Decker"
    ]
   ],
   "title": "Perceptual salience of language-specific acoustic differences in autonomous fillers across eight languages",
   "original": "i05_1773",
   "page_count": 4,
   "order": 296,
   "p1": "1773",
   "pn": "1776",
   "abstract": [
    "Are acoustic differences in autonomous fillers salient for the human perception ? Acoustic measurements have been carried out on autonomous fillers from eight languages (Arabic, Mandarin Chinese, French, German, Italian, European Portuguese, American English and Latin American Spanish). They exhibit timbre differences of the support vowel of autonomous fillers across languages. In order to evaluate their salience for human perception, two discrimination experiments have been conducted, French/L2 and Portuguese/L2. These experiments test the capacity to discriminate languages by listening to isolated autonomous fillers without any lexical support and without any context. As listeners have been native French speakers, the Portuguese/L2 experiments aim at evaluating a potential mother tongue bias. Results show that the perceptual discrimination performance depends on the language pair identity and they are consistent with the acoustic measures. The present study aims at providing some insight for acoustic filler models used in automatic speech processing in a multilingual context.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-296"
  },
  "pell05_interspeech": {
   "authors": [
    [
     "Marc D.",
     "Pell"
    ]
   ],
   "title": "Effects of cortical and subcortical brain damage on the processing of emotional prosody",
   "original": "i05_1777",
   "page_count": 4,
   "order": 297,
   "p1": "1777",
   "pn": "1780",
   "abstract": [
    "Cortical and subcortical contributions to the processing of emotional speech prosody were evaluated by testing adults with single focal lesions involving the right hemisphere (n=9), adults with basal ganglia damage in idiopathic Parkinson's disease (n=21), and healthy aging adults (n=33). Participants listened to semantically-anomalous utterances in two conditions (identification, rating) which assessed their recognition of five prosodic emotions. Findings confirmed that both right hemisphere and basal ganglia pathology were associated with impaired comprehension of prosody, although possibly for distinct reasons: right hemisphere compromise produced a more pervasive insensitivity to emotive features of prosodic stimuli, whereas basal ganglia disease produced a milder and more quantitative impairment on these tasks. The implications of these findings for differentiating cortical and subcortical mechanisms involved in prosody processing are considered.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-297"
  },
  "lane05_interspeech": {
   "authors": [
    [
     "Ian R.",
     "Lane"
    ],
    [
     "Tatsuya",
     "Kawahara"
    ]
   ],
   "title": "Utterance verification incorporating in-domain confidence and discourse coherence measures",
   "original": "i05_0421",
   "page_count": 4,
   "order": 298,
   "p1": "421",
   "pn": "424",
   "abstract": [
    "Conventional confidence measures for assessing the reliability of ASR output are typically derived from \"low-level\" information which is obtained during speech recognition decoding. In contract to these approaches, we propose a novel utterance verification scheme which incorporates confidence measures derived from \"high-level\" knowledge sources. Specifically, we investigate two measures: in-domain confidence, the degree of match between the input utterance and the application domain of the back-end system, and discourse coherence, the consistency between consecutive utterances in a dialogue session. A joint verification confidence is generated by combining these two measures with an orthodox measure based on GPP (generalized posterior probability). The proposed verification scheme was evaluated on spontaneous dialogue via the ATR speech-to-speech translation system. The two proposed measures were effective in improving verification accuracy.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-298"
  },
  "boulis05_interspeech": {
   "authors": [
    [
     "Constantinos",
     "Boulis"
    ],
    [
     "Mari",
     "Ostendorf"
    ]
   ],
   "title": "Using symbolic prominence to help design feature subsets for topic classification and clustering of natural human-human conversations",
   "original": "i05_0425",
   "page_count": 4,
   "order": 299,
   "p1": "425",
   "pn": "428",
   "abstract": [
    "In this work, we use the output of a symbolic prominence classifier rather than acoustic cues of prominence, to improve the tasks of clustering and classification of spontaneous conversations to topics. In our experiments, we combine the output of a prominence classifier with lexical feature selection and combination methods to build improved feature subsets. Evaluated for the task of topic classification on a subset of Switchboard-I, the combination method offered a 11% relative reduction of classification error compared to using lexical-only feature selection methods; similar gains are reported for clustering.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-299"
  },
  "sudoh05_interspeech": {
   "authors": [
    [
     "Katsuhito",
     "Sudoh"
    ],
    [
     "Hajime",
     "Tsukada"
    ]
   ],
   "title": "Tightly integrated spoken language understanding using word-to-concept translation",
   "original": "i05_0429",
   "page_count": 4,
   "order": 300,
   "p1": "429",
   "pn": "432",
   "abstract": [
    "This paper discusses an integrated spoken language understanding method using a statistical translation model from words to semantic concepts. The translation model is an N-gram-based model that can easily be integrated with speech recognition. It can be trained using annotated corpora where only sentence-level alignments between word sequences and concept sets are available, by automatic alignment based on cooccurrence between words and concepts. It can reduce the effort for explicitly aligning words to the corresponding concept. The method determines the confidence of understanding hypotheses for rejection in a similar manner to word-posterior-based confidence scoring in speech recognition. Experimental results show the advantages of integration over a cascaded method of speech recognition and word-to-concept translation in spoken language understanding with confidence-based rejection.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-300"
  },
  "sarikaya05_interspeech": {
   "authors": [
    [
     "Ruhi",
     "Sarikaya"
    ],
    [
     "Hong-Kwang Jeff",
     "Kuo"
    ],
    [
     "Vaibhava",
     "Goel"
    ],
    [
     "Yuqing",
     "Gao"
    ]
   ],
   "title": "Exploiting unlabeled data using multiple classifiers for improved natural language call-routing",
   "original": "i05_0433",
   "page_count": 4,
   "order": 301,
   "p1": "433",
   "pn": "436",
   "abstract": [
    "This paper presents an unsupervised method that uses limited amount of labeled data and a large pool of unlabeled data to improve natural language call routing performance. The method uses multiple classifiers to select a subset of the unlabeled data to augment limited labeled data. We evaluated four widely used text classification algorithms; Naive Bayes Classification (NBC), Support Vector machines (SVM), Boosting and Maximum Entropy (MaxEnt). The NBC method is found to be poorest performer compared to other three classification methods. Combining SVM, Boosting and MaxEnt resulted in significant improvements in call classification accuracy compared to any single classifier performance across varying amounts of labeled data.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-301"
  },
  "kuo05b_interspeech": {
   "authors": [
    [
     "Hong-Kwang Jeff",
     "Kuo"
    ],
    [
     "Vaibhava",
     "Goel"
    ]
   ],
   "title": "Active learning with minimum expected error for spoken language understanding",
   "original": "i05_0437",
   "page_count": 4,
   "order": 302,
   "p1": "437",
   "pn": "440",
   "abstract": [
    "Active learning is a strategy to minimize the annotation effort required to train statistical models, such as a statistical classifier used for natural language call routing or user intent classification. Most variants of active learning are \"certainty-based;\" they typically select, for human labeling, samples that are most likely to be mis-classified by automatic procedures. This approach, while selecting informative samples, completely disregards any interaction between \"similar\" samples - something that has recently been factored into active learning procedures to further reduce the labeling effort. In this paper we present a procedure, motivated by a recently proposed minimum expected error criterion for active learning, that also attempts to exploit the similarity between samples in an effort to maximize the gains that can be obtained from labeling a given number of samples. We evaluated the proposed algorithm on two natural language call routing tasks. On both the tasks a significant gain (up to 5% absolute for systems with over 80% accuracy) over baseline active learning was seen at small sample sizes. The gain, however, diminished with increasing sample sizes and no significant label saving was observed in achieving the maximum accuracy levels.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-302"
  },
  "thomae05_interspeech": {
   "authors": [
    [
     "Matthias",
     "Thomae"
    ],
    [
     "Tibor",
     "Fabian"
    ],
    [
     "Robert",
     "Lieb"
    ],
    [
     "Günther",
     "Ruske"
    ]
   ],
   "title": "Lexical out-of-vocabulary models for one-stage speech interpretation",
   "original": "i05_0441",
   "page_count": 4,
   "order": 303,
   "p1": "441",
   "pn": "444",
   "abstract": [
    "We present an approach to explicit, statistical, lexical-level out-ofvocabulary (OOV) word modeling for direct integration into the search space of a one-stage speech interpretation system. For this purpose, a generic pronunciation model for unknown words is derived from large pronunciation lexica and, optionally, word frequency knowledge. Known statistical language modeling (LM) methods are utilized to estimate different phoneme LM and apply different smoothing techniques. The resulting OOV word models are integrated with the hierarchical language model of our uniform modeling framework by declaring semantically irrelevant parts of the training utterances as unknown. Experiments were conducted with two different OOV training lexica on an airport information dialogue application, evaluating the results with both in-vocabulary (IV) and OOV-related metrics. Results for various OOV model configurations are presented, showing that OOV detection rates of 60-70% can be achieved with 1-2% falsely accepted IV words, simultaneously improving accuracy on the semantic representation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-303"
  },
  "thomae05b_interspeech": {
   "authors": [
    [
     "Matthias",
     "Thomae"
    ],
    [
     "Tibor",
     "Fabian"
    ],
    [
     "Robert",
     "Lieb"
    ],
    [
     "Günther",
     "Ruske"
    ]
   ],
   "title": "Hierarchical language models for one-stage speech interpretation",
   "original": "i05_3425",
   "page_count": 4,
   "order": 304,
   "p1": "3425",
   "pn": "3428",
   "abstract": [
    "This paper presents a robust semantic model for one-stage interpretation of natural speech. Our semantic analysis uses no explicit syntactic and morphologic knowledge, which seems sufficient for narrow application domains. In contrast to previous approaches, our semantic model is embedded in a uniform, hierarchical, stochastic modeling framework together with acoustic-phonetic and lexical knowledge, and semantic representations are computed directly from acoustic observations through a one-stage decoding process. The decoder produces a hierarchical (tree-) structure of words and semantic category symbols by use of the so-called hierarchical language model (HLM). We discuss generation of HLM by mixing rule-based and data-driven language model (LM) generation techniques, namely weighted regular expressions, n-grams and exact LM. Different HLM configurations with varying discounting techniques, n-gram orders and scaling factors are examined. Experiments were conducted with an airport information dialogue application. The evaluation results are based on HLM perplexity and our previously published semantic tree accuracy.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-304"
  },
  "wang05e_interspeech": {
   "authors": [
    [
     "Nick J. C.",
     "Wang"
    ]
   ],
   "title": "Spoken language understanding using layered n-gram modeling",
   "original": "i05_3429",
   "page_count": 4,
   "order": 305,
   "p1": "3429",
   "pn": "3432",
   "abstract": [
    "This paper presents an approach which integrates layer concept information into the trigram language model in order to improve the understanding accuracy for spoken dialogue systems and to improve the portability of the language modeling materials among different narrow-domain applications. With this approach, both the recognition accuracy and out-of-grammar problem can be largely improved, and the concept error rate is therefore reduced. In the experiments, using real-world air-ticket information spoken dialogue system for Mandarin Chinese, the relative concept error rate reductions from 20% to 30% are observed among systems given different sizes of language model training data. Furthermore, the layered N-gram modeling approach provides an efficient way of using existing chunkphrase corpora to build a new application, so as to improve the portability of the language modeling materials. Our experiment shows that the use of time chunk-phrases from a similar domain can achieve about 90% of the concept errorrate reduction compared to that of the in-domain collected training data. It shows an initial N-gram model might be established rapidly with the help of a library of chunk-phrase corpora before exhaustively collecting and transcribing application-specific dialogue utterances.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-305"
  },
  "surdeanu05_interspeech": {
   "authors": [
    [
     "Mihai",
     "Surdeanu"
    ],
    [
     "Jordi",
     "Turmo"
    ],
    [
     "Eli",
     "Comelles"
    ]
   ],
   "title": "Named entity recognition from spontaneous open-domain speech",
   "original": "i05_3433",
   "page_count": 4,
   "order": 306,
   "p1": "3433",
   "pn": "3436",
   "abstract": [
    "This paper presents an analysis of named entity recognition and classification in spontaneous speech transcripts. We annotated a significant fraction of the Switchboard corpus with six named entity classes and investigated a battery of machine learning models that include lexical, syntactic, and semantic attributes. The best recognition and classification model obtains promising results, approaching within 5% a system evaluated on clean textual data.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-306"
  },
  "zitouni05_interspeech": {
   "authors": [
    [
     "Imed",
     "Zitouni"
    ],
    [
     "Hui",
     "Jiang"
    ],
    [
     "Qiru",
     "Zhou"
    ]
   ],
   "title": "Discriminative training and support vector machine for natural language call routing",
   "original": "i05_3437",
   "page_count": 4,
   "order": 307,
   "p1": "3437",
   "pn": "3440",
   "abstract": [
    "In natural language call routing, callers are routed to desired departments based on natural spoken responses to an open-ended \"How may I direct your call?\" prompt. Natural language call classification can be performed using support vector machines (SVMs) or the popular vector-based model used in information retrieval. We recently demonstrate how discriminative training is powerful to improve any parameterized vector-based classifier to achieve minimum classification error. Discriminative training minimizes the classification error by increasing the score separation of the correct from competing documents. It makes the classifier robust to feature selection, enabling fully automated training without the injection of human expert knowledge. Support vector machines received also a lot of attention in the machine learning community. They have often achieved better performance than customized neuronal network and state-of-the-art baseline classifiers. We investigate in this paper the classification power of SVMs and discriminative training approaches on natural language call routing. Experiments are reported for a banking call routing and for Switchboard topic identification task. Results show that the application of discriminative training on vector-based model outperforms SVMs by 7% on spoken data.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-307"
  },
  "eun05_interspeech": {
   "authors": [
    [
     "Jihyun",
     "Eun"
    ],
    [
     "Minwoo",
     "Jeong"
    ],
    [
     "Gary Geunbae",
     "Lee"
    ]
   ],
   "title": "A multiple classifier-based concept-spotting approach for robust spoken language understanding",
   "original": "i05_3441",
   "page_count": 4,
   "order": 308,
   "p1": "3441",
   "pn": "3444",
   "abstract": [
    "In this paper, we present a concept spotting approach using manifold machine learning techniques for robust spoken language understanding. The goal of this approach is to find proper values for pre-defined slots of given meaning representation. Especially we propose a voting-based selection using multiple classifiers for robust spoken language understanding. This approach proposes no full level of language understanding but partial understanding because the method is only interested in the pre-defined meaning representation slots. In spite of this partial understanding, we can acquire necessary information to make interesting applications from the slot values because the slots are properly designed for specific domain-oriented understanding tasks. In several experimental results, the SLU (Spoken Language Understanding) performance degradation of spoken inputs compared with textual inputs are only F-measure 10.72, 11.43 and 11.51 for speech act, main goal and component slot extraction task respectively although the WER of spoken inputs is as high as 18.71%. That is, the evaluation results show that our concept spotting approach for SLU system is especially robust for spoken language input which has large recognition errors.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-308"
  },
  "lieb05_interspeech": {
   "authors": [
    [
     "Robert",
     "Lieb"
    ],
    [
     "Matthias",
     "Thomae"
    ],
    [
     "Günther",
     "Ruske"
    ],
    [
     "Daniel",
     "Bobbert"
    ],
    [
     "Frank",
     "Althoff"
    ]
   ],
   "title": "A flexible and integrated interface between speech recognition, speech interpretation and dialog management",
   "original": "i05_3445",
   "page_count": 4,
   "order": 309,
   "p1": "3445",
   "pn": "3448",
   "abstract": [
    "This paper presents an integrated interface between speech recognition, speech interpretation and dialog control intended for spoken dialog systems coping with natural speech input. During the system design phase the interface co-ordinates corpus acquisition and annotation, grammar development and the construction of stochastic hierarchical language models. During system runtime, it links together speech recognition and interpretation by efficient one-stage decoding of semantic trees, from which semantic content can easily be extracted. To gain robustness, the interface provides a way to interpret semantic confidences estimated during the decoding process. Furthermore, the dialog control can manage dynamic vocabulary and language model parts depending on the dialog context. The suggested interface helps the developer to build up and maintain the speech understanding part of a spoken dialog system in a consistent and flexible way. In addition, the reported experimental results show that information extraction performance can be increased by the presented methods.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-309"
  },
  "ohno05_interspeech": {
   "authors": [
    [
     "Tomohiro",
     "Ohno"
    ],
    [
     "Shigeki",
     "Matsubara"
    ],
    [
     "Hideki",
     "Kashioka"
    ],
    [
     "Naoto",
     "Kato"
    ],
    [
     "Yasuyoshi",
     "Inagaki"
    ]
   ],
   "title": "Incremental dependency parsing of Japanese spoken monologue based on clause boundaries",
   "original": "i05_3449",
   "page_count": 4,
   "order": 310,
   "p1": "3449",
   "pn": "3452",
   "abstract": [
    "In applications of spoken monologue processing such as simultaneous machine interpretation and real-time captions generation, incremental language parsing is strongly required. This paper proposes a technique for incremental dependency parsing of Japanese spoken monologue on a clause-by-clause basis. The technique identifies the clauses based on clause boundaries analysis, analyzes the dependency structures of them, and tries to decide the dependency relations with another clauses, simultaneously with the monologue speech input. The dependency relations are generated at the stage before the input of the entire monologue, and therefore, our technique can be used for language parsing in simultaneous Japanese speech understanding. An experiment using Japanese monologues has shown that our technique had the same degree of the performance as the usual dependency parsing for monologue sentences.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-310"
  },
  "sako05_interspeech": {
   "authors": [
    [
     "Atsushi",
     "Sako"
    ],
    [
     "Tetsuya",
     "Takiguchi"
    ],
    [
     "Yasuo",
     "Ariki"
    ]
   ],
   "title": "Situation based speech recognition for structuring baseball live games",
   "original": "i05_3453",
   "page_count": 4,
   "order": 311,
   "p1": "3453",
   "pn": "3456",
   "abstract": [
    "It is a difficult problem to recognize baseball live speech because the speech is rather fast, noisy, emotional and disfluent due to rephrasing, repetition, mistake and grammatical deviation caused by spontaneous speaking style. To solve these problems, we have been studied the speech recognition method incorporating the baseball game task-dependent knowledge as well as an announcer's emotion in commentary speech [1]. In addition, in this paper, we propose the situation prediction model based on word co-occurrence. Owing to these proposed models, speech recognition errors are effectively prevented. This method is formalized in the framework of probability theory and implemented in the conventional speech decoding (Viterbi) algorithm. The experimental results showed that the proposed approach improved the structuring and segmentation accuracy as well as keywords accuracy.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-311"
  },
  "bonneaumaynard05_interspeech": {
   "authors": [
    [
     "H.",
     "Bonneau-Maynard"
    ],
    [
     "Sophie",
     "Rosset"
    ],
    [
     "C.",
     "Ayache"
    ],
    [
     "A.",
     "Kuhn"
    ],
    [
     "Djamel",
     "Mostefa"
    ]
   ],
   "title": "Semantic annotation of the French media dialog corpus",
   "original": "i05_3457",
   "page_count": 4,
   "order": 312,
   "p1": "3457",
   "pn": "3460",
   "abstract": [
    "The French Technolangue MEDIA-EVALDA project aims to evaluate spoken understanding approaches. This paper describes the semantic annotation scheme of a common dialog corpus which will be used for developing and evaluating spoken understanding models and for linguistic studies. A common semantic representation has been formalized and agreed upon by the consortium. Each utterance is divided into semantic segments and each segment is annotated with a 5-tuplet containing the mode, attribute name representing the underlying concept, normalized form of the attribute, list of related segments, and an optional comment about the annotation. Periodic inter-annotator agreement studies demonstrate that the annotation are of good quality, with an agreement of almost 90% on mode and attribute identification. An analysis of the semantic content of 12292 annotated client utterances shows that only 14.1% of the observed attributes are domain-dependent and that the semantic dictionary ensures a good coverage of the task.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-312"
  },
  "engel05_interspeech": {
   "authors": [
    [
     "Ralf",
     "Engel"
    ]
   ],
   "title": "Robust and efficient semantic parsing of free word order languages in spoken dialogue systems",
   "original": "i05_3461",
   "page_count": 4,
   "order": 313,
   "p1": "3461",
   "pn": "3464",
   "abstract": [
    "This paper presents a semantic parser for spoken dialogue systems. The parser is designed especially for the analysis of free word order languages by providing a feature called order-independent matching. We describe how this feature allows writing of rules for free word order languages in an elegant way (using German as example language) and how it increases the robustness against speech recognition errors. As order-independent matching makes efficient parsing more difficult, we present a new parsing approach which provides efficient processing for rule bases that are, according to our experience, typical for spoken dialogue systems. The key feature of the parsing approach is a fixed application order of the rules to prune irrelevant results. A preliminary evaluation of the parser shows that this approach works very well in real-world dialogue systems.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-313"
  },
  "kobus05_interspeech": {
   "authors": [
    [
     "Catherine",
     "Kobus"
    ],
    [
     "Géraldine",
     "Damnati"
    ],
    [
     "Lionel",
     "Delphin-Poulat"
    ],
    [
     "Renato de",
     "Mori"
    ]
   ],
   "title": "Conceptual language model design for spoken language understanding",
   "original": "i05_3465",
   "page_count": 4,
   "order": 314,
   "p1": "3465",
   "pn": "3468",
   "abstract": [
    "This paper describes the construction and use of language models specific to semantic concepts. A tolerance mechanism is introduced in order to design conceptual constituents that cover longer word patterns while remaining robust to spontaneous speech disfluencies and recognition errors. These models are obtained from sentence patterns which grow from essential information to longer sequences of words. It is shown that these augmented conceptual constituents provide reductions in concept error rates on a spoken dialog application task.\n",
    "The composition of semantic constituents into semantic structures is discussed together with the introduction of language models for semantic composition. Experiments are described which show the advantage of using these language models to take into account structured semantic information.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-314"
  },
  "seabralopes05_interspeech": {
   "authors": [
    [
     "Luís",
     "Seabra Lopes"
    ],
    [
     "António J. S.",
     "Teixeira"
    ],
    [
     "Marcelo",
     "Quinderé"
    ],
    [
     "Mário",
     "Rodrigues"
    ]
   ],
   "title": "From robust spoken language understanding to knowledge acquisition and management",
   "original": "i05_3469",
   "page_count": 4,
   "order": 315,
   "p1": "3469",
   "pn": "3472",
   "abstract": [
    "The recent evolution of Carl, an intelligent mobile robot, is presented. The paper focuses on robust spoken language understanding (SLU) and on knowledge representation and reasoning (KRR). Robustness in SLU is achieved through the combination of deep and shallow parsing, tolerating non-grammatical utterances. The KRR module supports the integration of information coming from different interlocutors and is capable of handling contradictory facts. The knowledge representation language is based on semantic networks. Question answering is based on deductive as well as inductive inference. A preliminary evaluation of the efficiency of the SLU/KRR system, for the purpose of knowledge acquisition, is presented.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-315"
  },
  "wu05_interspeech": {
   "authors": [
    [
     "Cheng",
     "Wu"
    ],
    [
     "Xiang",
     "Li"
    ],
    [
     "Hong-Kwang Jeff",
     "Kuo"
    ],
    [
     "E. E.",
     "Jan"
    ],
    [
     "Vaibhava",
     "Goel"
    ],
    [
     "David",
     "Lubensky"
    ]
   ],
   "title": "Improving end-to-end performance of call classification through data confusion reduction and model tolerance enhancement",
   "original": "i05_3473",
   "page_count": 4,
   "order": 316,
   "p1": "3473",
   "pn": "3476",
   "abstract": [
    "Two major challenges in the rapid deployment of automated natural language call routing system are the minimization of the manual effort in tagging data and reducing the impact of speech recognition errors on the call classification. In this paper we explore some novel approaches which target at these two challenges. One of our approaches enriches the training set with additional speech recognition hypotheses, automatically splits the neighboring data with its original classes, retags the training data based on a similarity measure, and elects the final result among multiple classifiers which are trained from the split data; the other approach incorporates the acoustical confusable information into call classifier to reduce the impact of the speech recognition error on the call classification accuracy. The experimental results show that our new approaches can reduce the classification error rate of an automated natural language call routing system by relative 10% in the end to end performance, using the live data collected from an enterprise call center.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-316"
  },
  "campbell05_interspeech": {
   "authors": [
    [
     "Nick",
     "Campbell"
    ],
    [
     "Hideki",
     "Kashioka"
    ],
    [
     "Ryo",
     "Ohara"
    ]
   ],
   "title": "No laughing matter",
   "original": "i05_0465",
   "page_count": 4,
   "order": 317,
   "p1": "465",
   "pn": "468",
   "abstract": [
    "Laughter matters! From an analysis of a very large corpus of naturally-occurring conversational speech we have confirmed that approximately one in ten utterances contains laughter. From among these laughing utterances, we were able to distinguish four types of laughter according to what each revealed about the speaker's affective state, and we were able to recognise these different types automatically, by use of Hidden Markov Models trained on laugh segments, with a success rate of greater than 75%. The paper also presents a speech synthesis interface that enables the control of such emotional expression for use in an interactive conversation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-317"
  },
  "blouin05_interspeech": {
   "authors": [
    [
     "C.",
     "Blouin"
    ],
    [
     "V.",
     "Maffiolo"
    ]
   ],
   "title": "A study on the automatic detection and characterization of emotion in a voice service context",
   "original": "i05_0469",
   "page_count": 4,
   "order": 318,
   "p1": "469",
   "pn": "472",
   "abstract": [
    "This paper presents a study on the automatic characterization of negative emotion in a context of voice service, based on the calculation of acoustic parameters from the speech signal and a LDA classifier. The novelty of this study lies in the application-specific constraints (type of dialog, ASR connection) imposed on the emotion characterization system, and in the solutions proposed to this problem (notably by taking into account the speaker gender and the type of word). Promising results are achieved with acoustic models that are simultaneously gender-specific and words-specific.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-318"
  },
  "fernandez05_interspeech": {
   "authors": [
    [
     "Raul",
     "Fernandez"
    ],
    [
     "Rosalind W.",
     "Picard"
    ]
   ],
   "title": "Classical and novel discriminant features for affect recognition from speech",
   "original": "i05_0473",
   "page_count": 4,
   "order": 319,
   "p1": "473",
   "pn": "476",
   "abstract": [
    "This paper investigates the performance and relevance of a set of acoustic features for the task of automatic recognition of affect from speech using machine learning techniques. Eighty seven novel and classical features related to loudness, intonation, and voice quality, are examined. Using feature selection, the results yield a performance level of 49.4% recognition rate (compared to a human performance rate of 60.4% and a chance level of 20%), while the relevance results show that the more exploratory and novel subset of these features outrank the more classical features in the recognition task.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-319"
  },
  "cichosz05_interspeech": {
   "authors": [
    [
     "Jaroslaw",
     "Cichosz"
    ],
    [
     "Krzysztof",
     "Slot"
    ]
   ],
   "title": "Low-dimensional feature space derivation for emotion recognition",
   "original": "i05_0477",
   "page_count": 4,
   "order": 320,
   "p1": "477",
   "pn": "480",
   "abstract": [
    "An objective of the paper was to determine a set of low-dimensional feature spaces that provide high emotion recognition rates. Candidates for target feature spaces were randomly drawn from a broad pool of speech signal parameters that comprised both commonly used characteristics and newly introduced features. As a result, several four-dimensional feature spaces that provide the highest emotion classification rates (68%) on Polish language database, which we used in experiments, were identified.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-320"
  },
  "ishi05_interspeech": {
   "authors": [
    [
     "Carlos Toshinori",
     "Ishi"
    ],
    [
     "Hiroshi",
     "Ishiguro"
    ],
    [
     "Norihiro",
     "Hagita"
    ]
   ],
   "title": "Proposal of acoustic measures for automatic detection of vocal fry",
   "original": "i05_0481",
   "page_count": 4,
   "order": 321,
   "p1": "481",
   "pn": "484",
   "abstract": [
    "Vocal fry is a voice quality that often appears in relaxed voices indicating low tension, or in pressed voices expressing attitudes/ feelings of surprise, admiration and suffering. We propose a set of acoustic measures for automatically detecting vocal fry segments in speech utterances. In order to deal with vocal fry utterances with very low fundamental frequencies, where classic short-term analysis methods become problematic, a glottal pulse synchronized method is proposed. The acoustic measures are based on power, periodicity and similarity properties of vocal fry signals. The basic idea is to scan for power peaks in a \"very short-term\" power contour, check for periodicity properties and evaluate a similarity measure between power peaks for deciding the possibility of vocal fry pulses. Sub-harmonic properties are also taken into account in the periodicity analysis. Evaluation of the proposed measures in automatic detection resulted in 73.3% correct detection, with an insertion error rate of 3.9%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-321"
  },
  "truong05_interspeech": {
   "authors": [
    [
     "Khiet P.",
     "Truong"
    ],
    [
     "David A. van",
     "Leeuwen"
    ]
   ],
   "title": "Automatic detection of laughter",
   "original": "i05_0485",
   "page_count": 4,
   "order": 322,
   "p1": "485",
   "pn": "488",
   "abstract": [
    "In the context of detecting paralinguistic events' with the aim to make classification of the speaker's emotional state possible, a detector was developed for one of the most obvious paralinguistic events', namely laughter. Gaussian Mixture Models were trained with Perceptual Linear Prediction features, pitch&energy, pitch&voicing and modulation spectrum features to model laughter and speech. Data from the ICSI Meeting Corpus and the Dutch CGN corpus were used for our classification experiments. The results showed that Gaussian Mixture Models trained with Perceptual Linear Prediction features performed best with Equal Error Rates ranging from 7.1%-20.0%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-322"
  },
  "batliner05_interspeech": {
   "authors": [
    [
     "Anton",
     "Batliner"
    ],
    [
     "Stefan",
     "Steidl"
    ],
    [
     "Christian",
     "Hacker"
    ],
    [
     "Elmar",
     "Nöth"
    ],
    [
     "Heinrich",
     "Niemann"
    ]
   ],
   "title": "Tales of tuning - prototyping for automatic classification of emotional user states",
   "original": "i05_0489",
   "page_count": 4,
   "order": 323,
   "p1": "489",
   "pn": "492",
   "abstract": [
    "Classification performance for emotional user states found in the few realistic, spontaneous databases available is as yet not very high. We present a database with emotional children's speech in a human-robot scenario. Baseline classification performance for seven classes is 44.5%, for four classes 59.2%. We discuss possible strategies for tuning, e.g., using only prototypes (based on annotation correspondence or classification scores), or taking into account requirements and feasibility in possible applications (weighting of false alarms or speaker-specific overall frequencies).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-323"
  },
  "luengo05_interspeech": {
   "authors": [
    [
     "Iker",
     "Luengo"
    ],
    [
     "Eva",
     "Navas"
    ],
    [
     "Inmaculada",
     "Hernáez"
    ],
    [
     "Jon",
     "Sánchez"
    ]
   ],
   "title": "Automatic emotion recognition using prosodic parameters",
   "original": "i05_0493",
   "page_count": 4,
   "order": 324,
   "p1": "493",
   "pn": "496",
   "abstract": [
    "This paper presents the experiments made to automatically identify emotion in an emotional speech database for Basque. Three different classifiers have been built: one using spectral features and GMM, other with prosodic features and SVM and the last one with prosodic features and GMM. 86 prosodic features were calculated and then an algorithm to select the most relevant ones was applied. The first classifier gives the best result with a 98.4% accuracy when using 512 mixtures, but the classifier built with the best 6 prosodic features achieves an accuracy of 92.3% in spite of its simplicity, showing that prosodic information is very useful to identify emotions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-324"
  },
  "lee05c_interspeech": {
   "authors": [
    [
     "Sungbok",
     "Lee"
    ],
    [
     "Serdar",
     "Yildirim"
    ],
    [
     "Abe",
     "Kazemzadeh"
    ],
    [
     "Shrikanth",
     "Narayanan"
    ]
   ],
   "title": "An articulatory study of emotional speech production",
   "original": "i05_0497",
   "page_count": 4,
   "order": 325,
   "p1": "497",
   "pn": "500",
   "abstract": [
    "Few studies exist on the topic of emotion encoding in speech in the articulatory domain. In this report, we collect and analyze the movement data of the tongue tip, the jaw and the lower lip, along with speech, and investigate differences in speech articulation among four emotion types; neutral, anger, sadness and happiness. The effectiveness of the articulatory parameters in emotion classification was also investigated. It was observed that the tongue tip, jaw and lip positioning become more advanced when emotionally charged. This tendency was especially prominent for the tongue tip and jaw movements associated with sad speech. Angry speech was characterized by greater ranges of displacement and velocity, while it was opposite for sad speech. Happy speech was comparable in articulation to the neutral speech, but showed the widest range of pitch variation. It, however, remains to be seen if there is a trade-off between articulatory activity and voicing activity in emotional speech production. Multiple discriminant analysis showed that emotion is better classified in the articulatory domain. One probable reason is that the independency in the manipulation of each articulator may provide more degrees of freedom and less overlap in the articulatory parameter space. Vowel /IY/ was less responsive to emotional changes when compared to other peripheral vowels. This illustrates that the articulatory configuration associated with a vowel determines the effect of emotion on that vowel in the acoustic domain. Effects of emotions on each articulatory parameter were fairly systematic across vowels. It is not clear yet if that is a general tendency in emotional speech production or just a speaker-dependent characteristic.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-325"
  },
  "hofer05_interspeech": {
   "authors": [
    [
     "Gregor O.",
     "Hofer"
    ],
    [
     "Korin",
     "Richmond"
    ],
    [
     "Robert A. J.",
     "Clark"
    ]
   ],
   "title": "Informed blending of databases for emotional speech synthesis",
   "original": "i05_0501",
   "page_count": 4,
   "order": 326,
   "p1": "501",
   "pn": "504",
   "abstract": [
    "The goal of this project was to build a unit selection voice that could portray emotions with varying intensities. A suitable definition of an emotion was developed along with a descriptive framework that supported the work carried out. A single speaker was recorded portraying happy and angry speaking styles. Additionally a neutral database was also recorded. A target cost function was implemented that chose units according to emotion mark-up in the database. The Dictionary of Affect supported the emotional target cost function by providing an emotion rating for words in the target utterance. If a word was particularly emotional', units from that emotion were favoured. In addition intensity could be varied which resulted in a bias to select a greater number emotional units. A perceptual evaluation was carried out and subjects were able to recognise reliably emotions with varying amounts of emotional units present in the target utterance.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-326"
  },
  "tesser05_interspeech": {
   "authors": [
    [
     "Fabio",
     "Tesser"
    ],
    [
     "Piero",
     "Cosi"
    ],
    [
     "Carlo",
     "Drioli"
    ],
    [
     "Graziano",
     "Tisato"
    ]
   ],
   "title": "Emotional FESTIVAL-MBROLA TTS synthesis",
   "original": "i05_0505",
   "page_count": 4,
   "order": 327,
   "p1": "505",
   "pn": "508",
   "abstract": [
    "The topic of this work is an extension of our previous research on the development of a general data-driven procedure for creating a neutral \"narrative-style\" prosodic module for the Italian FESTIVAL Text-To-Speech (TTS) synthesizer, and it is focused on investigating and implementing new strategies for building a new emotional FESTIVAL TTS. The new emotional prosodic modules, similarly to the neutral case, are still based on the \"Classification And Regression Tree\" (CART) theory. The extension to the emotional speech synthesis is obtained using a differential approach: the emotional prosodic modules learn the differences between the neutral (without emotions) and the emotional prosodic data. Moreover, due to the fact that Voice Quality (VQ) is known to play an important role in emotive speech, a rule-based FESTIVAL-MBROLA VQ-modification module, for control of temporal and spectral characteristics of the synthesis, has also been implemented. Even if emotional synthesis still remains an attractive open issue, our preliminary evaluation results underline the effectiveness of the proposed solution.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-327"
  },
  "burkhardt05_interspeech": {
   "authors": [
    [
     "Felix",
     "Burkhardt"
    ]
   ],
   "title": "Emofilt: the simulation of emotional speech by prosody-transformation",
   "original": "i05_0509",
   "page_count": 4,
   "order": 328,
   "p1": "509",
   "pn": "512",
   "abstract": [
    "Emofilt is a software program intended to simulate emotional arousal with speech synthesis based on the free-for-noncommercial-use MBROLA synthesis engine. It acts as a transformer between the phonetisation and the speech-generation component. Originally developed at the Technical University of Berlin it was recently revived as an open-source project written in Java (http://emofilt.sourceforge.net). Emofilt's language-dependent modules are controlled by external XML-files and it is as multilingual as MBROLA which currently supports 35 languages. It might be used for research, teaching or to implement applications that include the simulation of emotional speech.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-328"
  },
  "rosenberg05_interspeech": {
   "authors": [
    [
     "Andrew",
     "Rosenberg"
    ],
    [
     "Julia",
     "Hirschberg"
    ]
   ],
   "title": "Acoustic/prosodic and lexical correlates of charismatic speech",
   "original": "i05_0513",
   "page_count": 4,
   "order": 329,
   "p1": "513",
   "pn": "516",
   "abstract": [
    "Charisma, the ability to command authority on the basis of personal qualities, is more difficult to define than to identify. How do charismatic leaders such as Fidel Castro or Pope John Paul II attract and retain their followers? We present results of an analysis of subjective ratings of charisma from a corpus of American political speech. We identify the associations between charisma ratings and ratings of other personal attributes. We also examine acoustic/prosodic and lexical features of this speech and correlate these with charisma ratings.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-329"
  },
  "greenberg05_interspeech": {
   "authors": [
    [
     "Yoko",
     "Greenberg"
    ],
    [
     "Minoru",
     "Tsuzaki"
    ],
    [
     "Hiroaki",
     "Kato"
    ],
    [
     "Yoshinori",
     "Sagisaka"
    ]
   ],
   "title": "Communicative speech synthesis using constituent word attributes",
   "original": "i05_0517",
   "page_count": 4,
   "order": 330,
   "p1": "517",
   "pn": "520",
   "abstract": [
    "Aiming at F0 control for communicative speech synthesis, the relationship between word attributes and F0 characteristics was analyzed. By analyzing one-phrase utterances in conversational situations, we studied correlations between word attributes defined by their impressions and prosodic control characteristics. For word attribute description, we used three dimensions in perceptual impressions, confident-doubtful, allowable-unacceptable and positive-negative obtained from our previous studies on one syllable utterances of \"n\". The result showed that F0 height, F0 dynamic patterns and duration could be consistently controlled by the word attributes. The positive-(negative) can be controlled by F0 height, while confident-doubtful, allowable-unacceptable were reflected in F0 dynamic patterns and duration. These results confirmed the usefulness of word attributes in communicative speech synthesis.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-330"
  },
  "braun05b_interspeech": {
   "authors": [
    [
     "Angelika",
     "Braun"
    ],
    [
     "Matthias",
     "Katerbow"
    ]
   ],
   "title": "Emotions in dubbed speech: an intercultural approach with respect to F0",
   "original": "i05_0521",
   "page_count": 4,
   "order": 331,
   "p1": "521",
   "pn": "524",
   "abstract": [
    "A comparative intercultural study on the so-called basic emotions anger, joy, fear, and sadness as well as neutral utterances was carried out based on samples of dubbed speech. The languages studied were the American English original of a popular TV series (Ally McBeal) as well as its German and Japanese dubbings. The production by the main male and female characters in all three languages as well as the perception by American, German and Japanese listener groups were examined. The present contribution focuses on results on the production and perception side of F0 and related parameters. The principal findings indicate that there are major cultural differences and also gender differences in encoding and decoding the emotional content of the utterances studied. Differences were found to be larger between linguistically and culturally less related languages than between the more closely related ones.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-331"
  },
  "audibert05_interspeech": {
   "authors": [
    [
     "Nicolas",
     "Audibert"
    ],
    [
     "Véronique",
     "Aubergé"
    ],
    [
     "Albert",
     "Rilliard"
    ]
   ],
   "title": "The prosodic dimensions of emotion in speech: the relative weights of parameters",
   "original": "i05_0525",
   "page_count": 4,
   "order": 332,
   "p1": "525",
   "pn": "528",
   "abstract": [
    "The emotional prosody is multi-dimensional. A debated question is to understand if some parameters are more specialized to convey some emotion dimensions [13]. Selected stimuli, mixed issued of authentic and acted French corpus, expressing anxiety, disappointment, disgust, disquiet, joy, resignation, satisfaction and sadness were used to synthesize artefactual stimuli integrating the emotional contour of each prosodic parameter separately. These stimuli were evaluated on a perception experiment. Results indicate that (1) no parameter alone is able to carry the whole emotion information, (2) F0 contours (not only the global F0 value) reveal to bring more information on positive expressions, (3) voice quality and duration convey more information on negative expressions, and (4) the intensity contours do not bring any significant information when used alone.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-332"
  },
  "schotz05_interspeech": {
   "authors": [
    [
     "Susanne",
     "Schötz"
    ]
   ],
   "title": "Stimulus duration and type in perception of female and male speaker age",
   "original": "i05_0529",
   "page_count": 4,
   "order": 333,
   "p1": "529",
   "pn": "532",
   "abstract": [
    "In a small perception study, our ability to estimate speaker age from speech samples was investigated with respect to stimulus duration, stimulus type and speaker gender. Four separate listening tests were carried out with four different sets of stimuli: 10 and 3 seconds of spontaneous speech, one isolated word, and 6 concatenated isolated words, all produced by the same 24 speakers. The results showed that the listeners' judgements were about twice as accurate compared to a baseline estimator, and that both stimulus duration and type affected the judgements. It was also found that stimulus duration influenced the listeners judgements of female speakers somewhat more, while stimulus type affected the age judgments of male speakers more, indicating that listeners may use different strategies when judging female and male speaker age.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-333"
  },
  "alm05_interspeech": {
   "authors": [
    [
     "Cecilia Ovesdotter",
     "Alm"
    ],
    [
     "Richard",
     "Sproat"
    ]
   ],
   "title": "Perceptions of emotions in expressive storytelling",
   "original": "i05_0533",
   "page_count": 4,
   "order": 334,
   "p1": "533",
   "pn": "536",
   "abstract": [
    "Whereas experimental studies on emotional speech often control for neutral semantics, speech in naturalistic speech corpora is characterized by contextual cues and non-neutral semantic content. Moreover, the target emotion of an utterance is generally unknown and must be inferred by the listener. Within the context of having child-directed expressive text-to-speech synthesis as goal, we describe a perceptual study based on an expressive spoken corpus of children's stories with unknown emotional targets, and report on interannotator agreement in a forced-choice discrimination task. Moreover, a threshold of high agreement was used to establish subsets of confident exemplar utterances for emotional classes, comprising 35% of the initial corpus. The exemplars were clustered based on the differences from the default mean neutral for 11 global acoustic features, yielding clusters cutting across emotion boundaries, some of which reflected arousal levels, with the neutral exemplars showing particularly complex distributions. Moreover, the mean features for four emotional exemplar categories were contrasted against the default, finding both expected and contradictory tendencies, compared to previous reports. The results indicate that semantic and prosodic cues collaborate to express and reinforce emotional contents, while emotional sequencing seems likely to be another factor which contributes to emotional perception in this domain.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-334"
  },
  "kawahara05_interspeech": {
   "authors": [
    [
     "Hideki",
     "Kawahara"
    ],
    [
     "Alain de",
     "Cheveigné"
    ],
    [
     "Hideki",
     "Banno"
    ],
    [
     "Toru",
     "Takahashi"
    ],
    [
     "Toshio",
     "Irino"
    ]
   ],
   "title": "Nearly defect-free F0 trajectory extraction for expressive speech modifications based on STRAIGHT",
   "original": "i05_0537",
   "page_count": 4,
   "order": 335,
   "p1": "537",
   "pn": "540",
   "abstract": [
    "A new method for source information extraction is proposed. The aim of the method is to provide optimal source information for the very high quality speech manipulation system STRAIGHT. The method is based on both time interval and frequency cues, and it provides fundamental frequency and periodicity information within each frequency band, to allow mixed mode excitation. The method is designed to minimize perceptual disturbance due to errors in source information extraction. A preliminary evaluation using a database of simultaneously recorded EGG and speech signals yielded very low gross error rates (0.029% for females and 0.14% for males). In addition, the method is designed so as to minimize the perceptual disturbance caused by any such gross error.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-335"
  },
  "yonezawa05_interspeech": {
   "authors": [
    [
     "Tomoko",
     "Yonezawa"
    ],
    [
     "Noriko",
     "Suzuki"
    ],
    [
     "Kenji",
     "Mase"
    ],
    [
     "Kiyoshi",
     "Kogure"
    ]
   ],
   "title": "Gradually changing expression of singing voice based on morphing",
   "original": "i05_0541",
   "page_count": 4,
   "order": 336,
   "p1": "541",
   "pn": "544",
   "abstract": [
    "We have developed a method for synthesizing a singing voice by gradually changing the musical expression based on speech morphing. This paper shows the advantages of this method in comparison with the approach of binary discrete transformation between two expressions, confirmed by statistical analyses of perception tests. In order to synthesize different expressional strengths of a singing voice, a \"normal\" (without expression) voice of a particular singer is used as the base of morphing, and three different expressions, \"dark,\" \"whispery\" and \"wet,\" are used as the target. Through our experiments, we confirmed i) the proposed morphing algorithm effectively interpolates the expressional strength of a singing voice, ii) an approximate equation of the perceptual sense can be used to calculate the morph ratio at a perceptually linear interval, and iii) our gradual transformation method can generate a natural singing voice from the interpolation of two different expressions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-336"
  },
  "hetherington05_interspeech": {
   "authors": [
    [
     "I. Lee",
     "Hetherington"
    ]
   ],
   "title": "A multi-pass, dynamic-vocabulary approach to real-time, large-vocabulary speech recognition",
   "original": "i05_0545",
   "page_count": 4,
   "order": 337,
   "p1": "545",
   "pn": "548",
   "abstract": [
    "We present a multi-pass approach to real-time, large-vocabulary speech recognition in which we dynamically manipulate the vocabulary between passes. For recognition tasks where subsets of the vocabulary can be triggered by the occurrences of other words or phrases, a combination of unknown word modelling and vocabulary refinement can be utilized to attack large-vocabulary tasks with relatively small active vocabularies. We evaluate this approach within the JUPITER weather information domain by enabling recognition of all 30,000 city-state pairs within the USA. By maximally precompiling the static and dynamic portions of our search space using finite-state transducers (FSTs), we splice dynamic-vocabulary components on-demand during decoding with negligible speed impact while enforcing cross-word context-dependent constraints. We find that a dynamic-vocabulary system can compete quite favorably with a single-pass, large-vocabulary system. For even larger vocabularies (e.g., street addresses), static compilation may be infeasible, making a dynamic-vocabulary approach necessary.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-337"
  },
  "saon05_interspeech": {
   "authors": [
    [
     "George",
     "Saon"
    ],
    [
     "Daniel",
     "Povey"
    ],
    [
     "Geoffrey",
     "Zweig"
    ]
   ],
   "title": "Anatomy of an extremely fast LVCSR decoder",
   "original": "i05_0549",
   "page_count": 4,
   "order": 338,
   "p1": "549",
   "pn": "552",
   "abstract": [
    "We report in detail the decoding strategy that we used for the past two Darpa Rich Transcription evaluations (RT'03 and RT'04) which is based on finite state automata (FSA). We discuss the format of the static decoding graphs, the particulars of our Viterbi implementation, the lattice generation and the likelihood evaluation. This paper is intended to familiarize the reader with some of the design issues encountered when building an FSA decoder. Experimental results are given on the EARS database (English conversational telephone speech) with emphasis on our faster than real-time system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-338"
  },
  "yu05_interspeech": {
   "authors": [
    [
     "Dong",
     "Yu"
    ],
    [
     "Li",
     "Deng"
    ],
    [
     "Alex",
     "Acero"
    ]
   ],
   "title": "Evaluation of a long-contextual-Span hidden trajectory model and phonetic recognizer using a* lattice search",
   "original": "i05_0553",
   "page_count": 4,
   "order": 339,
   "p1": "553",
   "pn": "556",
   "abstract": [
    "A long-contextual-span Hidden Trajectory Model (HTM) developed recently captures underlying dynamic structure of speech coarticulation and reduction using a highly compact set of contextindependent parameters. However, the long-span nature of the HTM makes it difficult to develop efficient search algorithms for its full evaluation. In this paper, we describe our initial effort in meeting this challenge. The basic search algorithm is timeasynchronous A.. Given the structural complexity of the long-span HTM, special considerations are needed to take into account the fact that the HTM score for each frame depends on the model parameters associated with a variable number of adjacent phones. Specifically, we present details on how the nodes and links in the lattices are expanded via look-ahead, how the A. heuristics are estimated, and what pruning strategies are applied to speed up the search. The experiments on TIMIT phonetic recognition show the capability of our newly developed lattice search algorithm in evaluating billions of hypotheses based on long-span HTM scores. The results significantly extend our earlier work from N-best rescoring to A. search over lattices.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-339"
  },
  "hori05b_interspeech": {
   "authors": [
    [
     "Takaaki",
     "Hori"
    ],
    [
     "Atsushi",
     "Nakamura"
    ]
   ],
   "title": "Generalized fast on-the-fly composition algorithm for WFST-based speech recognition",
   "original": "i05_0557",
   "page_count": 4,
   "order": 340,
   "p1": "557",
   "pn": "560",
   "abstract": [
    "This paper describes a Generalized Fast On-the-fly Composition (GFOC) algorithm for Weighted Finite-State Transducers (WFSTs) in speech recognition. We already proposed the original version of GFOC, which yields fast and memory-efficient decoding using two WFSTs. GFOC enables fast on-the-fly composition of three or more WFSTs during decoding. In many cases, it is actually difficult or impossible to organize an entire transduction process of speech recognition using only one or two WFST(s) since some types of models considerably enlarge after written in WFST form. For example, a class n-gram model often results in a large WFST which is several times larger than a word n-gram model for the same vocabulary. GFOC makes it possible to use such a model after decomposing it into small multiple WFSTs. In a spontaneous speech transcription task, we evaluated the size of WFSTs, decoding speed, and word accuracy of several decoding approaches. The results show that GFOC with three or more WFSTs is an efficient algorithm when using a class-based language model.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-340"
  },
  "nanjo05_interspeech": {
   "authors": [
    [
     "Hiroaki",
     "Nanjo"
    ],
    [
     "Teruhisa",
     "Misu"
    ],
    [
     "Tatsuya",
     "Kawahara"
    ]
   ],
   "title": "Minimum Bayes-risk decoding considering word significance for information retrieval system",
   "original": "i05_0561",
   "page_count": 4,
   "order": 341,
   "p1": "561",
   "pn": "564",
   "abstract": [
    "The paper addresses a new evaluation measure of automatic speech recognition (ASR) and a decoding strategy oriented for speech-based information retrieval (IR). Although word error rate (WER), which treats all words in a uniform manner, has been widely used as an evaluation measure of ASR, significance of words are different in speech understanding or IR. In this paper, we define a new ASR evaluation measure, namely, weighted word error rate (WWER) that gives a weight on errors from a viewpoint of IR. Then, we formulate a decoding method to minimize WWER based on Minimum Bayes-Risk (MBR) framework, and show that the decoding method improves WWER and IR accuracy.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-341"
  },
  "chan05_interspeech": {
   "authors": [
    [
     "Arthur",
     "Chan"
    ],
    [
     "Mosur",
     "Ravishankar"
    ],
    [
     "Alexander I.",
     "Rudnicky"
    ]
   ],
   "title": "On improvements to CI-based GMM selection",
   "original": "i05_0565",
   "page_count": 4,
   "order": 342,
   "p1": "565",
   "pn": "568",
   "abstract": [
    "Gaussian Mixture Model (GMM) computation is known to be one of the most computation-intensive components in speech decoding. In our previous work, context-independent model based GMM selection (CIGMMS) was found to be an effective way to reduce the cost of GMM computation without significant loss in recognition accuracy. In this work, we propose three methods to further improve the performance of CIGMMS. Each method brings an additional 5.10% relative speed improvement, with a cumulative improvement up to 37% on some tasks. Detailed analysis and experimental results on three corpora are presented.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-342"
  },
  "massonie05_interspeech": {
   "authors": [
    [
     "Dominique",
     "Massonie"
    ],
    [
     "Pascal",
     "Nocera"
    ],
    [
     "Georges",
     "Linares"
    ]
   ],
   "title": "Scalable language model look-ahead for LVCSR",
   "original": "i05_0569",
   "page_count": 4,
   "order": 343,
   "p1": "569",
   "pn": "572",
   "abstract": [
    "In this paper a new computation and approximation scheme for Language Model Look-Ahead (LMLA) is introduced. The main benefit of LMLA is sharper pruning of the search space during the LVCSR decoding process. However LMLA comes with its own cost and is known to scale badly with both LM n-gram order and LM size. The proposed method tackles this problem with a divide and conquer approach which enables faster computation without additional WER cost. The obtained results allowed our system to participate in the real-time task of the ESTER Broadcast News transcription evaluation campaign for French.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-343"
  },
  "novak05_interspeech": {
   "authors": [
    [
     "Miroslav",
     "Novak"
    ]
   ],
   "title": "Memory efficient approximative lattice generation for grammar based decoding",
   "original": "i05_0573",
   "page_count": 4,
   "order": 344,
   "p1": "573",
   "pn": "576",
   "abstract": [
    "ASR decoders are often required to produce a word lattice in addition to the best scoring path. While exact lattice generation is expensive (in the context of a Viterbi decoder), approximative algorithms are available to produce high quality lattices at much lower cost. Ideally, we would like to have an algorithm which does not require additional resources (either memory of CPU) in comparison to the best-path only decoder. We will present a lattice generation technique which uses a relatively strong approximation in comparison to other published techniques but requires little memory overhead (in comparison to a decoder optimized for best path only). We will show that the technique is suitable for tasks where a grammar is used as language model with little impact on the lattice quality (evaluated as n-best coverage).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-344"
  },
  "ahn05_interspeech": {
   "authors": [
    [
     "Dong-Hoon",
     "Ahn"
    ],
    [
     "Su-Byeong",
     "Oh"
    ],
    [
     "Minhwa",
     "Chung"
    ]
   ],
   "title": "Improved semi-dynamic network decoding using WFSTs",
   "original": "i05_0577",
   "page_count": 4,
   "order": 345,
   "p1": "577",
   "pn": "580",
   "abstract": [
    "In this paper, we present an improved semi-dynamic network decoding strategy by incorporating weighted finite-state transducer (WFST)-based search network. In our approach, a static search network is first optimized by applying WFST algorithms (determinization and minimization) to the composition of a lexicon and a language model. Then the WFST is partitioned into a set of subnetworks according to language model (LM) histories, and transformed into a subnetwork-based search network with exploiting structural differences where a WFST is a Mealy machine and our representation is essentially a Moore machine. This new strategy, which is opposite to our previous approach where each subnetwork depending on a LM history is first constructed and aggregates into a complete network, can let any static network compatible to WFST enjoy the run-time efficiency from the subnetwork-caching operation as well as the static efficiency from the WFST algorithms. The experimental results using Korean read speech dictation task are presented to show its efficiency.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-345"
  },
  "pylkkonen05_interspeech": {
   "authors": [
    [
     "Janne",
     "Pylkkönen"
    ]
   ],
   "title": "New pruning criteria for efficient decoding",
   "original": "i05_0581",
   "page_count": 4,
   "order": 346,
   "p1": "581",
   "pn": "584",
   "abstract": [
    "In large vocabulary continuous speech recognizers the search space needs to be constrained efficiently to make the recognition task feasible. Beam pruning and restricting the number of active paths are the most widely applied techniques for this. In this paper, we present three additional pruning criteria, which can be used to further limit the search space. These new criteria take into account the state of the search space, which enables tighter pruning. In the speech recognition experiments, the new pruning criteria were shown to reduce the search space up to 50% without affecting the search accuracy. We also present a method for optimizing the threshold parameters of the pruning criteria for the selected level of recognition accuracy. With this method even a large number of different pruning thresholds can be determined with little effort.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-346"
  },
  "fabian05_interspeech": {
   "authors": [
    [
     "Tibor",
     "Fabian"
    ],
    [
     "Robert",
     "Lieb"
    ],
    [
     "Günther",
     "Ruske"
    ],
    [
     "Matthias",
     "Thomae"
    ]
   ],
   "title": "A confidence-guided dynamic pruning approach - utilization of confidence measurement in speech recognition",
   "original": "i05_0585",
   "page_count": 4,
   "order": 347,
   "p1": "585",
   "pn": "588",
   "abstract": [
    "Improved efficiency of pruning accelerates the search process and leads to a more time efficient speech recognition system. The goal of this work was to develop a new pruning technique which optimizes the well known probability-based pruning (beam width) by utilization of confidence measurement. We use normalized hypotheses scores to guide the beam width of the pruning process dynamically frame per frame during the whole utterance. Compared with classical pruning techniques like fixed beam pruning and histogram rank pruning we achieved significantly better results concerning the time consumption of the recognizer. The speed of the recognition process could be accelerated up to 14 times with a slight degradation in recognition accuracy.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-347"
  },
  "heeren05_interspeech": {
   "authors": [
    [
     "Willemijn",
     "Heeren"
    ]
   ],
   "title": "Perceptual development of the duration cue in dutch /a-a:/",
   "original": "i05_0745",
   "page_count": 4,
   "order": 348,
   "p1": "745",
   "pn": "748",
   "abstract": [
    "The use of duration as a perceptual cue in Dutch /A-a:/ is studied in children aged 5 and 7, and adults. Usually, Dutch listeners use both vowel duration and spectral composition as perceptual cues for /A/ and /a:/, but they can use duration alone. Despite the vowel contrast's salience, five-year-olds do not yet use duration as adults: half of them did not use duration at all. The seven-year-olds did not differ from the adults, implying that the use of duration as a single cue is acquired before age 7, but mostly after age 5.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-348"
  },
  "you05_interspeech": {
   "authors": [
    [
     "Hong",
     "You"
    ],
    [
     "Abeer",
     "Alwan"
    ],
    [
     "Abe",
     "Kazemzadeh"
    ],
    [
     "Shrikanth",
     "Narayanan"
    ]
   ],
   "title": "Pronunciation variations of Spanish-accented English spoken by young children",
   "original": "i05_0749",
   "page_count": 4,
   "order": 349,
   "p1": "749",
   "pn": "752",
   "abstract": [
    "When learning to speak English, non-native speakers may pronounce some English phonemes differently from native speakers. These pronunciation variations can degrade an automatic speech recognition system's performance on accented English. This paper is a first attempt to find common pronunciation variations in Spanish-accented English as spoken by young children. The analysis of pronunciation variation is performed using dynamic programming-based transcription alignment on 4500 words spoken by children 5-7 years old whose first language is Spanish. The findings are then compared with linguistic hypotheses.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-349"
  },
  "heeren05b_interspeech": {
   "authors": [
    [
     "Willemijn",
     "Heeren"
    ]
   ],
   "title": "L2 development of quantity perception: dutch listeners learning Finnish /t-t:/",
   "original": "i05_0753",
   "page_count": 4,
   "order": 350,
   "p1": "753",
   "pn": "756",
   "abstract": [
    "The perceptual development of Dutch listeners learning to perceive the Finnish quantity contrast /t-t:/ was studied. It is shown that short laboratory training is (i) sufficient to change identification of relevant speech sounds, but (ii) insufficient to substantially change perceptual sensitivity along the phoneme continuum. Furthermore, L2 learners need much more relevant language experience, but can obtain native-like sensitivity to the /t-t:/ contrast.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-350"
  },
  "zmarich05_interspeech": {
   "authors": [
    [
     "Claudio",
     "Zmarich"
    ],
    [
     "Serena",
     "Bonifacio"
    ]
   ],
   "title": "Phonetic inventories in Italian children aged 18-27 months: a longitudinal study",
   "original": "i05_0757",
   "page_count": 4,
   "order": 351,
   "p1": "757",
   "pn": "760",
   "abstract": [
    "The phonetic inventories of 13 Italian children at 18, 21, 24 and 27 months of age are presented. The children were tape-recorded during a play interaction with the mother, and their vocalizations were phonetically transcribed. The individual phonetic inventories were calculated by applying the criteria put forward by Stoel- Gammon [1]: a consonantal singleton or cluster was attested only if word-initially or word-medially present in two different words at least, in a individual vocabulary of 10 to 50 different word types. Variable forms of the same word type were considered only twice. The collective phonetic inventories were calculated for each month on the bases of the number of subjects producing every phone. Further, the consonants at the onset or coda of the syllables constituting the selected words were classified for place and manner of articulations and their frequencies were calculated. Results for the consonantal structures in words and syllables produced by children have been compared to results for consonantal structures of adult words and syllables attempted by the children of 27 months.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-351"
  },
  "hirano05_interspeech": {
   "authors": [
    [
     "Hiroko",
     "Hirano"
    ],
    [
     "Goh",
     "Kawai"
    ]
   ],
   "title": "Pitch patterns of intonational phrases and intonational phrase groups in native and non-native speech",
   "original": "i05_0761",
   "page_count": 4,
   "order": 352,
   "p1": "761",
   "pn": "764",
   "abstract": [
    "We examined pitch patterns within and across intonational phrases of Japanese read aloud by native and non-native (Mandarin Chinese) speakers. Japanese speakers change pitch ranges for each intonational phrase. The relative pitch ranges of neighboring intonational phrases indicate which intonational phrase belongs to which intonational phrase group. Chinese speakers are unable to acoustically convey intonational phrase groups because their intonational phrases have limited and inflexible pitch ranges. Our results might form the basis for developing automated pronunciation learning systems that assist learners in acquiring intonation contours spanning intonation phrases.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-352"
  },
  "hincks05_interspeech": {
   "authors": [
    [
     "Rebecca",
     "Hincks"
    ]
   ],
   "title": "Measuring liveliness in presentation speech",
   "original": "i05_0765",
   "page_count": 4,
   "order": 353,
   "p1": "765",
   "pn": "768",
   "abstract": [
    "This paper proposes that speech analysis be used to quantify prosodic variables in presentation speech, and reports the results of a perception test of speaker liveliness. The test material was taken from a corpus of oral presentations made by 18 Swedish native students of Technical English. Liveliness ratings from a panel of eight judges correlated strongly with normalized standard deviation of F0 and, for female speakers, with mean length of runs, which is the number of syllables between pauses of >250 ms. An application of these findings would be in the development of a feedback mechanism for the prosody of public speaking.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-353"
  },
  "amano05_interspeech": {
   "authors": [
    [
     "Shigeaki",
     "Amano"
    ]
   ],
   "title": "Developmental change of phoneme duration in a Japanese infant and mother",
   "original": "i05_2217",
   "page_count": 4,
   "order": 354,
   "p1": "2217",
   "pn": "2220",
   "abstract": [
    "To investigate the early stage of speech development, the phoneme duration in an infant's utterances and in a mother's infant-directed speech (IDS) was analyzed for an infant between the ages of 4 and 45 months. In infant utterances, the durations of vowels and special moras gradually decrease as a function of month in entire period, whereas consonants such as fricatives, plosives, and semivowels rapidly decrease before the onset of two-word utterances. Almost all the phonemes in infant utterances except for plosives are longer than those in adult-directed speech (ADS) in entire period. In mothers' IDS, vowels and long vowels gradually decrease in duration as a function of month, but most consonants do not. Phonemes in IDS except for fricatives and plosives are longer than those in ADS. These results indicate that, in terms of duration, developmental change is different for different phonemes in infant utterances and mothers' IDS.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-354"
  },
  "jia05_interspeech": {
   "authors": [
    [
     "Haiping",
     "Jia"
    ],
    [
     "Hiroki",
     "Mori"
    ],
    [
     "Hideki",
     "Kasuya"
    ]
   ],
   "title": "Mora timing organization in producing contrastive geminate/single consonants and long/short vowels by native and non-native speakers of Japanese: effects of speaking rate",
   "original": "i05_2221",
   "page_count": 4,
   "order": 355,
   "p1": "2221",
   "pn": "2224",
   "abstract": [
    "The mora timing organization for producing contrastive geminate/ single consonants and long/short vowels is a difficult task for non-native speakers of Japanese. This paper investigates characteristics of the segment durations involved in the production of contrastive phonemes by native Japanese speakers and Chinese-speaking subjects, when the speaking rate is changed. From the statistical regression analysis of the measured segment durations of the utterances for a wide range of speaking rates, it was found that: 1) Native Japanese speakers exhibited a clear linear relationship between the segment duration and the average mora duration with few standard errors and individualities, whereas for Chinese-speaking subjects there were large standard errors and individualities though they maneuvered the timing skillfully at a regular speaking rate, 2) it was difficult for Chinese speakers to control the timing when the speaking rate varied, and 3) the normalized standard error in the regression model was a good index of the degree of achievement of the contrastive mora timing organization.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-355"
  },
  "wang05f_interspeech": {
   "authors": [
    [
     "Hongyan",
     "Wang"
    ],
    [
     "Vincent J. van",
     "Heuven"
    ]
   ],
   "title": "Mutual intelligibility of american, Chinese and dutch-accented speakers of English",
   "original": "i05_2225",
   "page_count": 4,
   "order": 356,
   "p1": "2225",
   "pn": "2228",
   "abstract": [
    "This paper presents the results of a comprehensive study of the mutual intelligibility of Chinese, Dutch (both foreign-language learners) and American (native language) speakers of English. Intelligibility is tested at the level of the segment, word and sentence, after careful selection of representative speakers from the three language backgrounds. The results show that production and perception skills are generally correlated at all levels, that both speakers and listeners are more successful in the order Chinese < Dutch < American. Against this back-ground, however, intelligibility is unexpectedly good when speakers and listeners share the same mother tongue.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-356"
  },
  "henrichsen05_interspeech": {
   "authors": [
    [
     "Peter Juel",
     "Henrichsen"
    ]
   ],
   "title": "Deriving a bi-lingual dictionary from raw transcription data",
   "original": "i05_2229",
   "page_count": 4,
   "order": 357,
   "p1": "2229",
   "pn": "2232",
   "abstract": [
    "We present a bigram-based method for deriving bi-lingual dictionary entries from two corpora of spontaneous speech (as represented in transcriptions). In contrast to e.g. [1], our method does not require translated or otherwise aligned texts; the corpora representing the source and target languages may be unrelated wrt. size, vocabulary richness, frequency distribution, and activity type. Examples are given using Danish and Swedish transcription data (and hints of English). We conclude with a discussion of the use of corpus-driven methods in language preservation and literation projects.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-357"
  },
  "ohta05_interspeech": {
   "authors": [
    [
     "Kei",
     "Ohta"
    ],
    [
     "Seiichi",
     "Nakagawa"
    ]
   ],
   "title": "A statistical method of evaluating pronunciation proficiency for Japanese words",
   "original": "i05_2233",
   "page_count": 4,
   "order": 358,
   "p1": "2233",
   "pn": "2236",
   "abstract": [
    "In this paper, we propose a statistical method of evaluating the pronunciation proficiency of Japanese words. We analyze statistically the utterances to note a combination that has a high correlation between a Japanese teacher's score and certain acoustic features. We found that the syllable recognition rates (accuracy) was the best measure of pronunciation proficiency. The effective measure which was highly correlated with Japanese teacher's score was the combination of the posteriori probability, the substitution/accuracy rates and the standard deviation of mora lengths. We obtained a correlation coefficient of 0.712 with closed data and 0.591 with open data for speaker at the five words set level, respectively. The coefficient was near the correlation between humans' scores, 0.600.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-358"
  },
  "campbell05b_interspeech": {
   "authors": [
    [
     "Nick",
     "Campbell"
    ]
   ],
   "title": "Non-verbal speech processing for a communicative agent",
   "original": "i05_0769",
   "page_count": 4,
   "order": 359,
   "p1": "769",
   "pn": "772",
   "abstract": [
    "A believable life-like agent must appear to understand what is said to it, or what is being said around it, even if this is not actually the case. It must be able to follow a conversation and to understand what is happening in a discourse, even though the verbal content of the dialogue may be too complex (or too noisy) to be recognised. This paper describes the platform design and details the description language being used for the SCOPE' non-verbal speech-processing research currently being carried out at ATR for such a communicative robotic agent.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-359"
  },
  "wrigley05_interspeech": {
   "authors": [
    [
     "Stuart N.",
     "Wrigley"
    ],
    [
     "Guy J.",
     "Brown"
    ]
   ],
   "title": "Physiologically motivated audio-visual localisation and tracking",
   "original": "i05_0773",
   "page_count": 4,
   "order": 360,
   "p1": "773",
   "pn": "776",
   "abstract": [
    "An audio-visual localisation and tracking system for meeting scenarios is presented which draws its inspiration from neurobiological processing. Meetings are recorded by a KEMAR binaural manikin and a single camera placed directly above the manikin. Source localisation from the binaural audio and face, object and motion locations from the video frames are used as input to two linked neural oscillator networks. The strength of the connections between the two networks determines the mapping between activity at a particular audio azimuth and activity at a particular visual frame column. A Hebbian learning rule is used to establish the connection strengths. The combined network segments the video and audio features and then produces audio-visual groupings on the basis of common spatial location. The audio-visual groupings are tracked through time using a mechanism based upon that of the human oculomotor system which incorporates smooth pursuit and saccadic movement.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-360"
  },
  "huang05d_interspeech": {
   "authors": [
    [
     "Jing",
     "Huang"
    ],
    [
     "Daniel",
     "Povey"
    ]
   ],
   "title": "Discriminatively trained features using fMPE for multi-stream audio-visual speech recognition",
   "original": "i05_0777",
   "page_count": 4,
   "order": 361,
   "p1": "777",
   "pn": "780",
   "abstract": [
    "fMPE is a recently introduced discriminative training technique that uses the Minimum Phone Error (MPE) discriminative criterion to train a feature-level transformation. In this paper we investigate fMPE trained audio/visual features for multi-stream HMM-based audio-visual speech recognition. A flexible, layer-based implementation of fMPE allows us to combine the visual information with the audio stream using the discriminative training process, and dispense with the multiple stream approach. Experiments are reported on the IBM infrared headset audio-visual database. On average of 20-speaker 1 hour speaker independent test data, the fMPE trained acoustic features achieve 33% relative gain. Adding video layers on top of audio layers gives additional 10% gain over fMPE trained features from the audio stream alone. The fMPE trained visual features achieve 14% relative gain, while the decision fusion of audio/visual streams with fMPE trained features achieves 29% relative gain. However, fMPE trained models do not improve over the original models on the mismatched noisy test data.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-361"
  },
  "tisato05_interspeech": {
   "authors": [
    [
     "Graziano",
     "Tisato"
    ],
    [
     "Piero",
     "Cosi"
    ],
    [
     "Carlo",
     "Drioli"
    ],
    [
     "Fabio",
     "Tesser"
    ]
   ],
   "title": "INTERFACE: a new tool for building emotive/expressive talking heads",
   "original": "i05_0781",
   "page_count": 4,
   "order": 362,
   "p1": "781",
   "pn": "784",
   "abstract": [
    "In order to speed-up the procedure for building an emotive/ expressive talking head such as LUCIA, an integrated software called INTERFACE was designed and implemented in Matlab. INTERFACE simplifies and automates many of the operations needed for that purpose. A set of processing tools, focusing mainly on dynamic articulatory data physically extracted by an automatic optotracking 3D movement analyzer, was implemented in order to build up the animation engine, that is based on the Cohen-Massaro coarticulation model, and also to create the correct WAV and FAP files needed for the animation. LUCIA, our animated MPEG-4 talking face, in fact, can copy a real human by reproducing the movements of some markers positioned on his face and recorded by an optoelectronic device, or can be directly driven by an emotional XML tagged input text, thus realizing a true audio visual emotive/expressive synthesis. LUCIA's voice is based on an Italian version of FESTIVAL - MBROLA packages, modified for expressive/emotive synthesis by means of an appropriate APML/VSML tagged language.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-362"
  },
  "ejarque05_interspeech": {
   "authors": [
    [
     "P.",
     "Ejarque"
    ],
    [
     "Javier",
     "Hernando"
    ]
   ],
   "title": "Variance reduction by using separate genuine- impostor statistics in multimodal biometrics",
   "original": "i05_0785",
   "page_count": 4,
   "order": 363,
   "p1": "785",
   "pn": "788",
   "abstract": [
    "In this paper, we propose some novel normalization and fusion techniques for biometric matching score level fusion in person verification. While conventional matching score level fusion methods use global score statistics, we consider in this work both genuine and impostor statistics separately. Performing a joint mean normalization of the separate monomodal scores, multimodal scores with less separate variance than the monomodal ones are obtained. Furthermore, a weighting method has been designed in order to minimize the variance sum of the separate multimodal statistics. This method obtains a minor sum of genuine and impostor variances for the multimodal biometric than that of the monomodal ones. The results obtained in speech and face scores fusion upon POLYCOST and XM2VTS databases show that the proposed normalization and fusion techniques provide better results than the conventional methods.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-363"
  },
  "schubert05_interspeech": {
   "authors": [
    [
     "Volker",
     "Schubert"
    ],
    [
     "Stefan W.",
     "Hamerich"
    ]
   ],
   "title": "The dialog application metalanguage GDialogXML",
   "original": "i05_0789",
   "page_count": 4,
   "order": 364,
   "p1": "789",
   "pn": "792",
   "abstract": [
    "In this paper we present GDialogXML, which is a new powerful modeling language for multi-modal dialog applications. In contrast to other dialog description languages (e.g. VoiceXML) GDialogXML focuses on the complete development process, covering data models, dialog flow, interaction models, and many more. It even allows for the modality-independent modeling of dialog applications. Beside some innovative features like the modality refinement procedure, a main achievement of GDialogXML is its integrating architecture.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-364"
  },
  "kumaran05_interspeech": {
   "authors": [
    [
     "Raghunandan S.",
     "Kumaran"
    ],
    [
     "Karthik",
     "Narayanan"
    ],
    [
     "John N.",
     "Gowdy"
    ]
   ],
   "title": "Myoelectric signals for multimodal speech recognition",
   "original": "i05_1189",
   "page_count": 4,
   "order": 365,
   "p1": "1189",
   "pn": "1192",
   "abstract": [
    "A Coupled Hidden Markov Model (CHMM) is proposed in this paper to perform multimodal speech recognition using myoelectric signals (MES) from the muscles of vocal articulation. MES signals are immune to noise, and words that are acoustically similar manifest distinctly in MES. Hence, they would effectively complement the acoustic data in a multimodal speech recognition system. Research in Audio-Visual Speech Recognition has shown that CHMMs model the asynchrony between different data streams effectively. Hence, we propose CHMM for multimodal speech recognition using audio and MES as the two data streams. Our experiments indicate that the multimodal CHMM system significantly outperforms the audio only system at different SNRs. We have also provided a comparison between different features for MES and have found that wavelet features provide the best results.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-365"
  },
  "daubias05_interspeech": {
   "authors": [
    [
     "Philippe",
     "Daubias"
    ]
   ],
   "title": "Is color information really useful for lip-reading ? (or what is lost when color is not used)",
   "original": "i05_1193",
   "page_count": 4,
   "order": 366,
   "p1": "1193",
   "pn": "1196",
   "abstract": [
    "In this paper, we report on experiments aiming at evaluating quantitatively the amount of information carried by color and luminance (gray level images) for automatic lip-reading. More precisely, we focus on the lip location problem: we trained Artificial Neural Networks (ANN) classifiers which were proven to be effective for the lip, skin and inner-mouth classification task, with both color and gray scaled image blocks extracted from the same images. Experiments were conducted with 6 subjects (1 female, 5 males, some with little facial hair) taken from the freely available LIUM-AVS database. A few different ANN architectures were tested, and in all cases, the use of color information enabled an important classification error reduction. Considering all the images blocks from the lip region that were available, the classification error was reduced from 30% for gray-level to 5% using color.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-366"
  },
  "shdaifat05_interspeech": {
   "authors": [
    [
     "I.",
     "Shdaifat"
    ],
    [
     "R.-R.",
     "Grigat"
    ]
   ],
   "title": "A system for audio-visual speech recognition",
   "original": "i05_1197",
   "page_count": 4,
   "order": 367,
   "p1": "1197",
   "pn": "1200",
   "abstract": [
    "In this work, a system of audio visual speech recognition will be presented. A new hybrid visual feature combination, which is suitable for audio -visual speech recognition was implemented. The features comprise both the shape and the appearance of lips, the dimensional reduction is applied using discrete cosine transform (DCT). A large visual speech database of the German language has been assembled, the German Audio-Visual Database (GAVD). The conducted experiments using only visual features resulted in a high recognition accuracy and improved the audio-visual speech recognition drastically.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-367"
  },
  "kitaoka05_interspeech": {
   "authors": [
    [
     "Norihide",
     "Kitaoka"
    ],
    [
     "Hironori",
     "Oshikawa"
    ],
    [
     "Seiichi",
     "Nakagawa"
    ]
   ],
   "title": "Multimodal interface for organization name input based on combination of isolated word recognition and continuous base-word recognition",
   "original": "i05_1201",
   "page_count": 4,
   "order": 368,
   "p1": "1201",
   "pn": "1204",
   "abstract": [
    "We investigate a multimodal interface for organization name input to forms in WWW. The user first utters an organization name in an open vocabulary domain to the system. The system recognizes it with a combination method of isolated word recognition and continuous \"base-word\" recognition. Word candidates and a base-word lattice obtained by this recognition procedure are displayed on a touch panel. Then the user chooses a sequence or base-words from the candidates by pen touch to construct the desired name and the name is sent to the client. This recognition method performs well in the organization name recognition task, in which a very large vocabulary size and its successive update is needed when using isolated word recognition. Our interface design also elevates users' input ability of organization names.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-368"
  },
  "matsusaka05_interspeech": {
   "authors": [
    [
     "Yosuke",
     "Matsusaka"
    ]
   ],
   "title": "Recognition of (3) party conversation using prosody and gaze",
   "original": "i05_1205",
   "page_count": 4,
   "order": 369,
   "p1": "1205",
   "pn": "1208",
   "abstract": [
    "We have developed a recognition system that can understand the multi-party conversation from combined information of prosody and gaze. In multi-party conversation, the conversation becomes complex because many overlaps and interrupts are generated by side participants. And thus becomes difficult to keep track the main thread of the conversation. Gaze works as a strong clue to both clarify and perceive \"whose talking to whom\" and \"whose listening to whom\", and can be used to improve the understanding of the conversational situation. We have analyzed the gaze behavior in conversational situations based on actual human-to-human conversation recoding, and created a computational model to recognize the main thread of the conversation. The performance has improved up to 20 point compared to the condition that only used prosody.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-369"
  },
  "li05c_interspeech": {
   "authors": [
    [
     "Dongdong",
     "Li"
    ],
    [
     "Yingchun",
     "Yang"
    ],
    [
     "Zhaohui",
     "Wu"
    ]
   ],
   "title": "Combining voiceprint and face biometrics for speaker identification using SDWS",
   "original": "i05_1209",
   "page_count": 4,
   "order": 370,
   "p1": "1209",
   "pn": "1212",
   "abstract": [
    "The biometric system that uses multiple biometric traits promises higher identification accuracy than identification in either individual domain. To reach this goal, special attention should be paid to the strategies for combining voiceprint and face experts. We propose an improved weighted sum rule based on the scores difference (SDWS) between the genuine speaker class and the mistaken speaker class labeled by each classifier, and demonstrate that the performance of multi-biometric system can be further improved by SDWS. The tests were conducted on a multi-modal database with 54 users We compare our approach with other existing methods and show that SDWS improved performance by about 7.8-13.3%, much better than the others.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-370"
  },
  "cooke05_interspeech": {
   "authors": [
    [
     "Neil",
     "Cooke"
    ],
    [
     "Martin",
     "Russell"
    ]
   ],
   "title": "Using the focus of visual attention to improve spontaneous speech recognition",
   "original": "i05_1213",
   "page_count": 4,
   "order": 371,
   "p1": "1213",
   "pn": "1216",
   "abstract": [
    "We investigate recognition of spontaneous speech using the focus of visual attention as a secondary cue to speech. In our experiment we collected a corpus of eye and speech data where one participant describes a geographical map to another while having their eye movements tracked. Using this corpus we characterise the coupling between eye movement and speech. Speech recognition results are presented to demonstrate proof of concept for development of a bimodal ASR using focus of visual attention to drive a dynamic language model. Marginal improvement in WER is observed.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-371"
  },
  "gurbuz05_interspeech": {
   "authors": [
    [
     "Sabri",
     "Gurbuz"
    ]
   ],
   "title": "Real-time outer lip contour tracking for HCI applications",
   "original": "i05_1217",
   "page_count": 4,
   "order": 372,
   "p1": "1217",
   "pn": "1220",
   "abstract": [
    "A new method for tracking outer lip contours of individuals in real world conditions is presented. For an arbitrary speaker, lip color properties are learned for the Bayes decision from the current image frame using the nose tip location as a reference point. Estimated outer contour data is fit to an ellipsoid for further eliminating the effect of outliers in the contour. The algorithm, which is posed as a real-time solution to lip contour tracking in real world conditions, is made efficient by the use of an online learning method. Demonstrations of lip contour tracking and its application to a mouth movement imitation are presented.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-372"
  },
  "huang05e_interspeech": {
   "authors": [
    [
     "Jing",
     "Huang"
    ],
    [
     "Karthik",
     "Visweswariah"
    ]
   ],
   "title": "Improving lip-reading with feature space transforms for multi-stream audio-visual speech recognition",
   "original": "i05_1221",
   "page_count": 4,
   "order": 373,
   "p1": "1221",
   "pn": "1224",
   "abstract": [
    "In this paper we investigate feature space transforms to improve lip-reading performance for multi-stream HMM based audio-visual speech recognition (AVSR). The feature space transforms include non-linear Gaussianization transform and feature space maximum likelihood linear regression (fMLLR). We apply Gaussianization at the various stages of visual front-end. The results show that Gaussianizing the final visual features achieves the best performance: 8% gain on lip-reading and 14% gain on AVSR. We also compare performance of speaker-based Gaussianization and global Gaussianization. Without fMLLR adaptation, speaker-based Gaussianization improves more on lip-reading and multi-stream AVSR performance. However, with fMLLR adaptation, global Gaussianization shows better results, and achieves 18% over baseline fMLLR adaptation for AVSR.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-373"
  },
  "mixdorff05c_interspeech": {
   "authors": [
    [
     "Hansjörg",
     "Mixdorff"
    ],
    [
     "Denis",
     "Burnham"
    ],
    [
     "Guillaume",
     "Vignali"
    ],
    [
     "Patavee",
     "Charnvivit"
    ]
   ],
   "title": "Are there facial correlates of Thai syllabic tones?",
   "original": "i05_1225",
   "page_count": 4,
   "order": 374,
   "p1": "1225",
   "pn": "1228",
   "abstract": [
    "This paper deals with the influence of tones on syllabic articulation in Thai. Motion capturing of 24 facial points in the face of a female speaker was performed using an Optotrak system as she uttered 24 sets of syllabic tokens covering the five different tones of Thai 12 times each. After rigid and non-rigid-movements had been separated, a PCA was conducted on the non-rigid data. In order to determine the influence of the tones on the facial movement the first PC reflecting the jaw opening was analyzed by aligning the derivatives of the first PCs with respect to the point of maximum velocity and averaging over all tokens of a syllable/tone combination. Analysis showed great similarities in the shapes of the resulting mean velocity contours. In some syllable sets, however, certain tones exhibited specific temporal alignments that were strongly correlated with the underlying syllable duration. This outcome suggests that certain syllable/tone combinations require a specific temporal alignment of articulatory and tonal gestures, though a consistent physiological explanation remains yet to be found.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-374"
  },
  "seymour05_interspeech": {
   "authors": [
    [
     "Rowan",
     "Seymour"
    ],
    [
     "Ji",
     "Ming"
    ],
    [
     "Darryl",
     "Stewart"
    ]
   ],
   "title": "A new posterior based audio-visual integration method for robust speech recognition",
   "original": "i05_1229",
   "page_count": 4,
   "order": 375,
   "p1": "1229",
   "pn": "1232",
   "abstract": [
    "We describe the development of a multistream HMM based audiovisual speech recognition (AVSR) system and a new method for integrating the audio and visual streams using frame level posterior probabilities. This is compared to the standard feature concatenation and weighted product methods in speaker-dependent tests using our own multimodal database, by examining speech recognition robustness to corruption in either stream. For corruption in the audio stream we use additive noise at different SNR levels, and for corruption in the video stream we use MPEG4 compression at different bitrates as well as image blurring using Gaussian filters. We provide very promising results which demonstrate the robustness of the new method.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-375"
  },
  "beskow05_interspeech": {
   "authors": [
    [
     "Jonas",
     "Beskow"
    ],
    [
     "Mikael",
     "Nordenberg"
    ]
   ],
   "title": "Data-driven synthesis of expressive visual speech using an MPEG-4 talking head",
   "original": "i05_0793",
   "page_count": 4,
   "order": 376,
   "p1": "793",
   "pn": "796",
   "abstract": [
    "This paper describes initial experiments with synthesis of visual speech articulation for different emotions, using a newly developed MPEG-4 compatible talking head. The basic problem with combining speech and emotion in a talking head is to handle the interaction between emotional expression and articulation in the orofacial region. Rather than trying to model speech and emotion as two separate properties, the strategy taken here is to incorporate emotional expression in the articulation from the beginning. We use a data-driven approach, training the system to recreate the expressive articulation produced by an actor while portraying different emotions. Each emotion is modelled separately using principal component analysis and a parametric coarticulation model. The results so far are encouraging but more work is needed to improve naturalness and accuracy of the synthesized speech.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-376"
  },
  "turk05_interspeech": {
   "authors": [
    [
     "Oytun",
     "Turk"
    ],
    [
     "Marc",
     "Schröder"
    ],
    [
     "Baris",
     "Bozkurt"
    ],
    [
     "Levent M.",
     "Arslan"
    ]
   ],
   "title": "Voice quality interpolation for emotional text-to-speech synthesis",
   "original": "i05_0797",
   "page_count": 4,
   "order": 377,
   "p1": "797",
   "pn": "800",
   "abstract": [
    "Synthesizing desired emotions using concatenative algorithms relies on collection of large databases. This paper focuses on the development and assessment of a simple algorithm to interpolate the intended vocal effort in existing databases in order to create new databases with intermediate levels of vocal effort. Three diphone databases in German with soft, modal, and loud voice qualities are processed with a spectral interpolation algorithm. A listening test is performed to evaluate the intended vocal effort in the original databases as well as the interpolated ones. The results show that the interpolation algorithm can create the intended intermediate levels of vocal effort given the original databases independent of the language background of the subjects.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-377"
  },
  "bulut05_interspeech": {
   "authors": [
    [
     "Murtaza",
     "Bulut"
    ],
    [
     "Carlos",
     "Busso"
    ],
    [
     "Serdar",
     "Yildirim"
    ],
    [
     "Abe",
     "Kazemzadeh"
    ],
    [
     "Chul Min",
     "Lee"
    ],
    [
     "Sungbok",
     "Lee"
    ],
    [
     "Shrikanth",
     "Narayanan"
    ]
   ],
   "title": "Investigating the role of phoneme-level modifications in emotional speech resynthesis",
   "original": "i05_0801",
   "page_count": 4,
   "order": 378,
   "p1": "801",
   "pn": "804",
   "abstract": [
    "Recent studies in our lab show that emotions in speech are manifested as, besides supra-segmental trends, distinct variations in phoneme-level prosodic and spectral parameters. In this paper, we further investigate the significance of this finding in the context of emotional speech synthesis. Specifically, we study phonemelevel signal property manipulation in transforming the emotional information conveyed in a speech utterance. We analyze the effect of individual and combined modifications of F0, duration, energy and spectrum using data recorded by a professional actress with happy, angry, sad and neutral expressiveness. We use content matched source-target pairs and apply TDPSOLA for prosody and LPC for spectrum modifications by directly extracting the required parameters from the target speech. Listening tests conducted with 10 naive raters show that modi- fication of prosody and spectral envelope parameters by themselves is not sufficient. However, when applied together, modifying spectrum and prosody at the phone level gives successful results for most emotion pairs, except conversion to happy targets. We also observe that at the phoneme level, spectral envelope modifications are more effective than local prosodic modifications; and that, duration modifications are more effective than pitch modifications. The results confirm our hypothesis that phoneme level modifications can be used to fine tune the ensuing suprasegmental-parameter-based modifications to improve the overall quality of synthesized emotions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-378"
  },
  "schuller05_interspeech": {
   "authors": [
    [
     "Björn",
     "Schuller"
    ],
    [
     "Ronald",
     "Müller"
    ],
    [
     "Manfred",
     "Lang"
    ],
    [
     "Gerhard",
     "Rigoll"
    ]
   ],
   "title": "Speaker independent emotion recognition by early fusion of acoustic and linguistic features within ensembles",
   "original": "i05_0805",
   "page_count": 4,
   "order": 379,
   "p1": "805",
   "pn": "808",
   "abstract": [
    "Herein we present a comparison of novel concepts for a robust fusion of prosodic and verbal cues in speech emotion recognition. Thereby 276 acoustic features are extracted out of a spoken phrase. For linguistic content analysis we use the Bag-of-Words text representation. This allows for integration of acoustic and linguistic features within one vector prior to a final classification. Extensive feature selection by filter- and wrapper based methods is fulfilled. Likewise optimal sets via SVM-SFFS and single feature relevance by information gain ratio calculation are presented. Overall classification is realised by diverse ensemble approaches. Among base classifiers Kernel Machines, Decision Trees, Bayesian classifiers, and memory-based learners are found. Acoustics only tests ran on a database comprising 39 speakers for speaker independent accuracy analysis. Additionally the public Berlin Emotional Speech database is used. A further database of 4,221 movie related phrases forms the basis of acoustic and linguistic information analysis evaluation. Overall remarkable performance in the discrimination of seven discrete emotions could be observed.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-379"
  },
  "kim05c_interspeech": {
   "authors": [
    [
     "Jonghwa",
     "Kim"
    ],
    [
     "Elisabeth",
     "André"
    ],
    [
     "Matthias",
     "Rehm"
    ],
    [
     "Thurid",
     "Vogt"
    ],
    [
     "Johannes",
     "Wagner"
    ]
   ],
   "title": "Integrating information from speech and physiological signals to achieve emotional sensitivity",
   "original": "i05_0809",
   "page_count": 4,
   "order": 380,
   "p1": "809",
   "pn": "812",
   "abstract": [
    "Recently, there has been a significant amount of work on the recognition of emotions from speech and biosignals. Most approaches to emotion recognition so far concentrate on a single modality and do not take advantage of the fact that an integrated multimodal analysis may help to resolve ambiguities and compensate for errors. In this paper, we describe various methods for fusing physiological and voice data at the feature-level and the decision-level as well as a hybrid integration scheme. The results of the integrated recognition approach are then compared with the individual recognition results from each modality.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-380"
  },
  "douglascowie05_interspeech": {
   "authors": [
    [
     "Ellen",
     "Douglas-Cowie"
    ],
    [
     "Laurence",
     "Devillers"
    ],
    [
     "Jean-Claude",
     "Martin"
    ],
    [
     "Roddy",
     "Cowie"
    ],
    [
     "Suzie",
     "Savvidou"
    ],
    [
     "Sarkis",
     "Abrilian"
    ],
    [
     "Cate",
     "Cox"
    ]
   ],
   "title": "Multimodal databases of everyday emotion: facing up to complexity",
   "original": "i05_0813",
   "page_count": 4,
   "order": 381,
   "p1": "813",
   "pn": "816",
   "abstract": [
    "In everyday life, speech is part of a multichannel system involved in conveying emotion. Understanding how it operates in that context requires suitable data, consisting of multimodal records of emotion drawn from everyday life. This paper reflects the experience of two teams active in collecting and labelling data of this type. It sets out the core reasons for pursuing a multimodal approach, reviews issues and problems for developing relevant databases, and indicates how we can move forward both in terms of data collection and approaches to labelling.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-381"
  },
  "torres05_interspeech": {
   "authors": [
    [
     "Francisco",
     "Torres"
    ],
    [
     "Emilio",
     "Sanchis"
    ],
    [
     "Encarna",
     "Segarra"
    ]
   ],
   "title": "Learning of stochastic dialog models through a dialog simulation technique",
   "original": "i05_0817",
   "page_count": 4,
   "order": 382,
   "p1": "817",
   "pn": "820",
   "abstract": [
    "We present an approach for the learning of stochastic dialog models using a technique of automatic generation of dialogs. We have applied it to achieve a better performance in our dialog system, which answers telephone queries about train timetables in Spanish. Besides interacting with real users, the stochastic dialog manager can now interact with other module, in the role of the user, developing a large number of dialogs at a very low cost. From this interaction, the dialog manager is able to dynamically adapt its stochastic model, adding new transitions or modifying their probabilities, when a simulation ends satisfactorily. We expect that the modified model provides the dialog manager with a better strategy for answering real users than the strategy given by the initial model estimated from real dialogs.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-382"
  },
  "black05b_interspeech": {
   "authors": [
    [
     "Lesley-Ann",
     "Black"
    ],
    [
     "Michael",
     "McTear"
    ],
    [
     "Norman",
     "Black"
    ],
    [
     "Roy",
     "Harper"
    ],
    [
     "Michelle",
     "Lemon"
    ]
   ],
   "title": "Evaluating the DI@l-log system on a cohort of elderly, diabetic patients: results from a preliminary study",
   "original": "i05_0821",
   "page_count": 4,
   "order": 383,
   "p1": "821",
   "pn": "824",
   "abstract": [
    "DI@L-log is an automated medical spoken dialogue system designed to enable patients to regularly communicate health data to the point-of-care over the telephone. In order to investigate the performance of the system, a preliminary evaluation was conducted using 5 novice hypertensive diabetic patients from the Ulster Hospital in Northern Ireland. The purpose of the study was to assess several factors which need to be addressed when designing spoken dialogue systems for elderly, disabled users. We examined the performance of the system, the interaction preferences of the user and their usage patterns, the level of user satisfaction, and the impact of the system on the patient's health over time. A revised version of the system, modified to take account of findings from this study, is currently undergoing testing with a larger group of subjects.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-383"
  },
  "kral05_interspeech": {
   "authors": [
    [
     "Pavel",
     "Král"
    ],
    [
     "Christophe",
     "Cerisara"
    ],
    [
     "Jana",
     "Klecková"
    ]
   ],
   "title": "Combination of classifiers for automatic recognition of dialog acts",
   "original": "i05_0825",
   "page_count": 4,
   "order": 384,
   "p1": "825",
   "pn": "828",
   "abstract": [
    "This paper deals with automatic dialog acts (DAs) recognition in Czech. The dialog acts are sentence-level labels that represent different states of a dialogue, depending on the application. Our work focuses on two applications: a multimodal reservation system and an animated talking head for hearing-impaired people. In that context, we consider the following DAs: statements, orders, yes/no questions and other questions. We propose to use both lexical and prosodic information for DAs recognition. The main goal of this paper is to compare different methods to combine the results of both classifiers. On a Czech corpus simulating a reservation of train tickets, the lexical information only gives about 92% of classification accuracy, while prosody gives only about 45% of accuracy. When both classifiers are combined with a multilayer perceptron, the lowest (lexical) word error rate further decreases by 26%. We show that this improvement is close to the optimal one, given the correlation of the lexical and prosodic features. The other combination schemes do not outperform the lexical-only results.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-384"
  },
  "wu05b_interspeech": {
   "authors": [
    [
     "Xiaojun",
     "Wu"
    ],
    [
     "Thomas Fang",
     "Zheng"
    ],
    [
     "Michael",
     "Brasser"
    ],
    [
     "Zhanjiang",
     "Song"
    ]
   ],
   "title": "Rapidly developing spoken Chinese dialogue systems with the d-ear SDS SDK",
   "original": "i05_0829",
   "page_count": 4,
   "order": 385,
   "p1": "829",
   "pn": "832",
   "abstract": [
    "Developing a spoken dialog system is typically time-consuming, and must often be accomplished using difficult-to-learn professional technologies. Most existing toolkits use statistical semantic parsers and model a dialogue interaction as a finite-state network. However, for developing flexible spoken Chinese dialogue systems, these toolkits have several problems. A new toolkit named the d-Ear SDS SDK is introduced here. The SDK suggests a multi-session dialogue system framework with a powerful semantic parser specially designed for spoken Chinese understanding, and a powerful dialogue manager providing non-finite-state dialogue control. To set up a new dialogue system, the developer can customize all the system modules with domain-specific information and operations, using the d-Ear SDS Studio to save time. Using the SDK, we have built several dialogue systems with excellent performance in a very short time.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-385"
  },
  "oria05_interspeech": {
   "authors": [
    [
     "Daniela",
     "Oria"
    ],
    [
     "Akos",
     "Vetek"
    ]
   ],
   "title": "Robust algorithms and interaction strategies for voice spelling",
   "original": "i05_0833",
   "page_count": 4,
   "order": 386,
   "p1": "833",
   "pn": "836",
   "abstract": [
    "This paper describes our experiments to overcome the problem of inputting unknown words in a voice-only application by spelling. As the paper demonstrates, this is achieved by post-processing the speech recognition result to attempt to recover from possible recognition errors that occurred during the spelling phase. Since the post-processing does not involve any additional user input, it not only increases the robustness of voice spelling, but it also reduces the cognitive burden put on users and increases the overall usability of systems that require spelling by voice.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-386"
  },
  "toptsis05_interspeech": {
   "authors": [
    [
     "Ioannis",
     "Toptsis"
    ],
    [
     "Axel",
     "Haasch"
    ],
    [
     "Sonja",
     "Hüwel"
    ],
    [
     "Jannik",
     "Fritsch"
    ],
    [
     "Gernot A.",
     "Fink"
    ]
   ],
   "title": "Modality integration and dialog management for a robotic assistant",
   "original": "i05_0837",
   "page_count": 4,
   "order": 387,
   "p1": "837",
   "pn": "840",
   "abstract": [
    "The communication with robotic assistants or companions is a challenging new domain for the use of dialog systems. In contrast to classical spoken language interfaces users interact with mobile robots mostly in a multi-modal way. In this paper we will present the integration of several modalities in the dialog system of BIRON - the Bielefeld Robot Companion. Besides speech as the main modality the system integrates deictic gestures and visual scene information in order to resolve object references in a task oriented dialog. We will present example interactions with BIRON and first qualitative results from the \"home-tour\" scenario defined within the COGNIRON project.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-387"
  },
  "reithinger05_interspeech": {
   "authors": [
    [
     "Norbert",
     "Reithinger"
    ],
    [
     "Daniel",
     "Sonntag"
    ]
   ],
   "title": "An integration framework for a mobile multimodal dialogue system accessing the semantic web",
   "original": "i05_0841",
   "page_count": 4,
   "order": 388,
   "p1": "841",
   "pn": "844",
   "abstract": [
    "Advanced intelligent multimodal interface systems usually comprise many sub-systems. For the integration of already existing software components in the Smartweb system we developed an integration framework, the IHUB. It allows us to reuse already existing components for interpretation and processing of multimodal user interactions. The framework facilitates the integration of the user in the interpretation loop by controlling the message flow in the system which is important in our domain, the multimodal access to the Semantic Web. A technical evaluation of the framework shows the efficient routing of messages to make real-time interactive editing of semantic queries possible.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-388"
  },
  "nisimura05_interspeech": {
   "authors": [
    [
     "Ryuichi",
     "Nisimura"
    ],
    [
     "Akinobu",
     "Lee"
    ],
    [
     "Masashi",
     "Yamada"
    ],
    [
     "Kiyohiro",
     "Shikano"
    ]
   ],
   "title": "Operating a public spoken guidance system in real environment",
   "original": "i05_0845",
   "page_count": 4,
   "order": 389,
   "p1": "845",
   "pn": "848",
   "abstract": [
    "Takemaru-kun system is a practical speech-oriented guidance system developed to examine spoken interface through long-term operation in a public place that collected natural human-machine interaction data. In (2)004 the following advances improving reliability of the system were introduced, which conduced acquiring positive increase of access from users: (1) Rejection of unintended speech based on Gaussian Mixture Models (GMMs); (2) Removal of short, unnecessary inputs of impulsive noise; (3) Child or adult user discrimination; (4) Web-based monitoring mechanisms. This paper summarizes the Takemaru-kun system and analysis of 177,789 data collected by two-years actual operation. Experiments with the collected data proved that a combination of GMM-based verification and short input removal can excise 85% of the invalid inputs, including laughter, incomprehensible utterances, and even some background utterances.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-389"
  },
  "salonen05_interspeech": {
   "authors": [
    [
     "Esa-Pekka",
     "Salonen"
    ],
    [
     "Markku",
     "Turunen"
    ],
    [
     "Jaakko",
     "Hakulinen"
    ],
    [
     "Leena",
     "Helin"
    ],
    [
     "Perttu",
     "Prusi"
    ],
    [
     "Anssi",
     "Kainulainen"
    ]
   ],
   "title": "Distributed dialogue management for smart terminal devices",
   "original": "i05_0849",
   "page_count": 4,
   "order": 390,
   "p1": "849",
   "pn": "852",
   "abstract": [
    "Mobile devices, such as smartphones and personal digital assistants, can be used to implement efficient speech-based and multimodal interfaces. Most of the systems are server-based, and there is a need to distribute the dialogue management tasks between the terminal devices and the server. Since the technologies are not mature and the platforms are constantly changing, approaches that support generic and reusable components are needed. We describe a model where the tasks of the dialogue management are divided into two parts. The server handles the overall coordination of the dialogue by generating dialogue task descriptions. Smart terminal devices realize the low-level decisions by executing individual dialogue tasks. A prototype of multimodal dialogue application using the model is presented. It has two different user interface realizations using standard telephony equipment and smartphones.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-390"
  },
  "hakulinen05_interspeech": {
   "authors": [
    [
     "Jaakko",
     "Hakulinen"
    ],
    [
     "Markku",
     "Turunen"
    ],
    [
     "Esa-Pekka",
     "Salonen"
    ]
   ],
   "title": "Visualization of spoken dialogue systems for demonstration, debugging and tutoring",
   "original": "i05_0853",
   "page_count": 4,
   "order": 391,
   "p1": "853",
   "pn": "856",
   "abstract": [
    "Graphical elements have been found very useful when spoken dialogue systems are developed and demonstrated. However, most of the spoken dialogue systems are designed for speech-only interaction and are very hard to extent to contain graphical elements. We introduce a general model to visualize speech interfaces. Based on the model we present an implemented visualization framework, and several example visualizations for demonstrations, debugging, and interactive tutoring of speech applications.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-391"
  },
  "gonzalezferreras05_interspeech": {
   "authors": [
    [
     "César",
     "González-Ferreras"
    ],
    [
     "Valentín",
     "Cardeñoso-Payo"
    ]
   ],
   "title": "Development and evaluation of a spoken dialog system to access a newspaper web site",
   "original": "i05_0857",
   "page_count": 4,
   "order": 392,
   "p1": "857",
   "pn": "860",
   "abstract": [
    "This paper presents a system that provides access to a newspaper web site. The system is based on an interaction model, that uses browse and search strategies, and on an information model, which is made up of a tree and indexes. A frame-based approach is used to control the dialog flow. VoiceXML is used as a language to describe dialog turns. The systems works for Spanish language. To measure the usability of the system we evaluated the performance using objective measures and user satisfaction using SASSI questionnaire. The results of the evaluation show a task success rate of 92% and a WER of 18.09%. Overall user satisfaction is positive: users perceive the system as useful and easy to use. We conclude the paper with a discussion of the most relevant issues about the development and evaluation of the system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-392"
  },
  "pietquin05_interspeech": {
   "authors": [
    [
     "Olivier",
     "Pietquin"
    ],
    [
     "Richard",
     "Beaufort"
    ]
   ],
   "title": "Comparing ASR modeling methods for spoken dialogue simulation and optimal strategy learning",
   "original": "i05_0861",
   "page_count": 4,
   "order": 393,
   "p1": "861",
   "pn": "864",
   "abstract": [
    "Speech enabled interfaces are nowadays becoming ubiquitous. The most advanced ones rely on probabilistic pattern matching systems and especially on automatic speech recognition systems. Because of their statistical nature, performances of such systems never reach one hundred percent of correct recognition results. Performances are linked to environmental noise and to intraand inter-speaker variability of course, but also to the acoustical similarities inside the vocabulary of allowed speech entries, which is usually contextual in the case of man-machine dialogue systems. A good dialogue strategy should therefore dynamically handle the potentiality of recognition errors. In this paper, we compare different methods to model ASR systems in the framework of automatic dialogue strategy optimization and we especially emphasize on a context-dependent ASR modeling method.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-393"
  },
  "chu05_interspeech": {
   "authors": [
    [
     "Shiu-Wah",
     "Chu"
    ],
    [
     "Ian",
     "O'Neill"
    ],
    [
     "Philip",
     "Hanna"
    ],
    [
     "Michael",
     "McTear"
    ]
   ],
   "title": "An approach to multi-strategy dialogue management",
   "original": "i05_0865",
   "page_count": 4,
   "order": 394,
   "p1": "865",
   "pn": "868",
   "abstract": [
    "Existing dialogue systems typically use only one dialogue strategy: finite-state based, frame-based or agent-based. This paper describes research done on a dialogue manager (DM) that uses all three dialogue strategies. This DM can determine the most suitable dialogue strategy to be used according to circumstances, and change to a different strategy whenever required. The research puts emphasis on how such a multi-strategy DM improves the naturalness of human-computer interaction. Determining best use of different dialogue strategies, rather than tailoring dialogue management to a particular business domain, motivates the research.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-394"
  },
  "hjalmarsson05_interspeech": {
   "authors": [
    [
     "Anna",
     "Hjalmarsson"
    ]
   ],
   "title": "Towards user modelling in conversational dialogue systems: a qualitative study of the dynamics of dialogue parameters",
   "original": "i05_0869",
   "page_count": 4,
   "order": 395,
   "p1": "869",
   "pn": "872",
   "abstract": [
    "This paper presents a qualitative study of data from a 26 subject experimental study within the multi-modal, conversational dialogue system AdApt. Qualitative analysis of data is used to illustrate the dynamic variation of dialogue parameters over time. The analysis will serve as a foundation for research and future data collections in the area of adaptive dialogue systems and user modelling.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-395"
  },
  "katsurada05_interspeech": {
   "authors": [
    [
     "Kouichi",
     "Katsurada"
    ],
    [
     "Kazumine",
     "Aoki"
    ],
    [
     "Hirobumi",
     "Yamada"
    ],
    [
     "Tsuneo",
     "Nitta"
    ]
   ],
   "title": "Reducing the description amount in authoring MMI applications",
   "original": "i05_0873",
   "page_count": 4,
   "order": 396,
   "p1": "873",
   "pn": "876",
   "abstract": [
    "Although an MMI description language XISL1.1 (eXtensible Interaction Scenario Language) developed for web-based MMI applications has some desirable features such as high extensibility and controllability of modalities, it has some issues concerning capabilities for describing slot-filling style dialogs, a large amount of description for modality combination of inputs, and complicated authoring when handling XML contents. In this paper we provide a new version of XISL (XISL2.0) that resolves these issues by using VoiceXML-based syntax, modality-independent input description, and XForms-like data model. The results of comparison between XISL1.1 and XISL2.0 showed that XISL2.0 reduced the amount of description by 40% as compared with XISL1.1, and XISL2.0 succeeded to mitigate the difficulties in the authoring work of developing MMI applications.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-396"
  },
  "komatani05_interspeech": {
   "authors": [
    [
     "Kazunori",
     "Komatani"
    ],
    [
     "Naoyuki",
     "Kanda"
    ],
    [
     "Tetsuya",
     "Ogata"
    ],
    [
     "Hiroshi G.",
     "Okuno"
    ]
   ],
   "title": "Contextual constraints based on dialogue models in database search task for spoken dialogue systems",
   "original": "i05_0877",
   "page_count": 4,
   "order": 397,
   "p1": "877",
   "pn": "880",
   "abstract": [
    "This paper describes the incorporation of contextual information into spoken dialogue systems in the database search task. Appropriate dialogue modeling is required to manage automatic speech recognition (ASR) errors using dialogue-level information. We define two dialogue models: a model for dialogue flow and a model of structured dialogue history. The model for dialogue flow assumes dialogues in the database search task consist of only two modes. In the structured dialogue history model, query conditions are maintained as a tree structure, taking into consideration their inputted order. The constraints derived from these models are integrated by using a decision tree learning, so that the system can determine a dialogue act of the utterance and whether each content word should be accepted or rejected, even when it contains ASR errors. The experimental result showed that our method could interpret content words better than conventional one without the contextual information. Furthermore, it was also shown that our method was domain-independent because it achieved equivalent accuracy in another domain without any more training.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-397"
  },
  "rotaru05_interspeech": {
   "authors": [
    [
     "Mihai",
     "Rotaru"
    ],
    [
     "Diane J.",
     "Litman"
    ]
   ],
   "title": "Using word-level pitch features to better predict student emotions during spoken tutoring dialogues",
   "original": "i05_0881",
   "page_count": 4,
   "order": 398,
   "p1": "881",
   "pn": "884",
   "abstract": [
    "In this paper, we advocate for the usage of word-level pitch features for detecting user emotional states during spoken tutoring dialogues. Prior research has primarily focused on the use of turnlevel features as predictors. We compute pitch features at the word level and resolve the problem of combining multiple features per turn using a word-level emotion model. Even under a very simple word-level emotion model, our results show an improvement in prediction using word-level features over using turn-level features. We find that the advantage of word-level features lies in a better prediction of longer turns.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-398"
  },
  "raux05_interspeech": {
   "authors": [
    [
     "Antoine",
     "Raux"
    ],
    [
     "Brian",
     "Langner"
    ],
    [
     "Dan",
     "Bohus"
    ],
    [
     "Alan W.",
     "Black"
    ],
    [
     "Maxine",
     "Eskenazi"
    ]
   ],
   "title": "Let's go public! taking a spoken dialog system to the real world",
   "original": "i05_0885",
   "page_count": 4,
   "order": 399,
   "p1": "885",
   "pn": "888",
   "abstract": [
    "In this paper, we describe how a research spoken dialog system was made available to the general public. The Let's Go Public spoken dialog system provides bus schedule information to the Pittsburgh population during off-peak times. This paper describes the changes necessary to make the system usable for the general public and presents analysis of the calls and strategies we have used to ensure high performance.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-399"
  },
  "fujie05_interspeech": {
   "authors": [
    [
     "Shinya",
     "Fujie"
    ],
    [
     "Kenta",
     "Fukushima"
    ],
    [
     "Tetsunori",
     "Kobayashi"
    ]
   ],
   "title": "Back-channel feedback generation using linguistic and nonlinguistic information and its application to spoken dialogue system",
   "original": "i05_0889",
   "page_count": 4,
   "order": 400,
   "p1": "889",
   "pn": "892",
   "abstract": [
    "A conversational system which can generate back-channel feedback of proper content in proper timing by utilizing FST based early detectable decoder and prosody analysis is proposed. In human conversation, we do not take turns in order, but we give the back-channel feedbacks during the partner's speech. By receiving these feedbacks, speakers can know the partner's state and feel comfortable to speak. Therefore, spoken dialogue systems should be able to generate back-channel feedbacks in synchronization with user's utterances. The appropriateness of these feedbacks depends on the contents and the timings. The contents strongly depend on the contents of the dialogue partner's utterance, and the timings strongly depend on the prosody of the partner's utterance. In order to determine the content of the feedback earlier than the end of the utterance, we use finite state transducer based speech recognizer. We used prosody information, especially F0 and power of the utterance, to extract the proper timing of the feedback. We implemented these modules and applied them to the spoken dialogue system on the humanoid robot ROBISUKE. Experimental results show the effectiveness of our methods.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-400"
  },
  "georgila05_interspeech": {
   "authors": [
    [
     "Kallirroi",
     "Georgila"
    ],
    [
     "James",
     "Henderson"
    ],
    [
     "Oliver",
     "Lemon"
    ]
   ],
   "title": "Learning user simulations for information state update dialogue systems",
   "original": "i05_0893",
   "page_count": 4,
   "order": 401,
   "p1": "893",
   "pn": "896",
   "abstract": [
    "This paper describes and compares two methods for simulating user behaviour in spoken dialogue systems. User simulations are important for automatic dialogue strategy learning and the evaluation of competing strategies. Our methods are designed for use with \"Information State Update\" (ISU)-based dialogue systems. The first method is based on supervised learning using linear feature combination and a normalised exponential output function. The user is modelled as a stochastic process which selects user actions ({ speech act, task } pairs) based on features of the current dialogue state, which encodes the whole history of the dialogue. The second method uses n-grams of { speech act, task } pairs, restricting the length of the history considered by the order of the n-gram. Both models were trained and evaluated on a subset of the Communicator corpus, to which we added annotations for user actions and Information States. The model based on linear feature combination has a perplexity of 2.08 whereas the best n-gram (4-gram) has a perplexity of 3.58. Each one of the user models ran against a system policy trained on the same corpus with a method similar to the one used for our linear feature combination model. The quality of the simulated dialogues produced was then measured as a function of the filled slots, confirmed slots, and number of actions performed by the system in each dialogue. In this experiment both the linear feature combination model and the best n-grams (5-gram and 4-gram) produced similar quality simulated dialogues.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-401"
  },
  "martiniglesias05_interspeech": {
   "authors": [
    [
     "Darío",
     "Martín-Iglesias"
    ],
    [
     "Yago",
     "Pereiro-Estevan"
    ],
    [
     "Ana I.",
     "García-Moral"
    ],
    [
     "Ascensión",
     "Gallardo-Antolín"
    ],
    [
     "Fernando",
     "Díaz-de-María"
    ]
   ],
   "title": "Design of a voice-enabled interface for real-time access to stock exchange from a PDA through GPRS",
   "original": "i05_0897",
   "page_count": 4,
   "order": 402,
   "p1": "897",
   "pn": "900",
   "abstract": [
    "In this paper we describe the design of a voice-enabled interface embedded in a Personal Digital Assistant (PDA). It is a component of a more complex system which allows the access to real-time information (for example, prices on stock exchange) via the General Packet Radio Service (GPRS). Specifically, we have focused on the Automatic Speech Recognition (ASR) module: we have implemented a fixed-point version of the whole ASR system and recognition results showed that its performance is very similar to that achieved with its floating-point implementation. In addition, with the purpose of facilitate as much as possible the portability of the ASR system to other hand-held devices, we have dealt with the problem of adapting a set of general acoustic models to match a specific acoustic environment when few adaptation data is available.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-402"
  },
  "schuler05_interspeech": {
   "authors": [
    [
     "William",
     "Schuler"
    ],
    [
     "Tim",
     "Miller"
    ]
   ],
   "title": "Integrating denotational meaning into a DBN language model",
   "original": "i05_0901",
   "page_count": 4,
   "order": 403,
   "p1": "901",
   "pn": "904",
   "abstract": [
    "This paper describes a dynamic Bayes net (DBN) language model which allows recognition decisions to be conditioned on features of entities in some environment, to which hypothesized directives might refer. The accuracy of this model is then evaluated on spoken directives in various domains.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-403"
  },
  "bosch05_interspeech": {
   "authors": [
    [
     "Louis ten",
     "Bosch"
    ]
   ],
   "title": "Improving out-of-coverage language modelling in a multimodal dialogue system using small training sets",
   "original": "i05_0905",
   "page_count": 4,
   "order": 404,
   "p1": "905",
   "pn": "908",
   "abstract": [
    "For automatic speech recognition, the construction of an adequate language model may be difficult when only a limited amount of training text is available. Previous work has shown that in the case of small training sets statistical language models may outperform grammars on out-of-coverage utterances, while showing comparable performance on in-coverage input. In this paper, we compare the performance of an automatic speech recognition system using a grammar and a statistical language model including garbage models in the case of very limited in-domain training data. The results show that a bigram language model and a grammar show similar performance, and that the inclusion of garbage models in statistical language models enhances their performance both on in-coverage and out-of-coverage utterances.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-404"
  },
  "galibert05_interspeech": {
   "authors": [
    [
     "Olivier",
     "Galibert"
    ],
    [
     "Gabriel",
     "Illouz"
    ],
    [
     "Sophie",
     "Rosset"
    ]
   ],
   "title": "Ritel: an open-domain, human-computer dialog system",
   "original": "i05_0909",
   "page_count": 4,
   "order": 405,
   "p1": "909",
   "pn": "912",
   "abstract": [
    "The project Ritel aims at integrating a spoken language dialog system and an open-domain question answering system to allow a human to ask general questions (\"Who is currently presiding the Senate?\") and refine the search interactively. As this point in time the Ritel platform is being used to collect a human-computer dialog corpus. The user can receive factual answers to some questions (Q : who is the president of France, R : Jacques Chirac is the president for France since may 1995). This paper briefly presents the current system, the collected corpus, the problems encountered by such a system and our first answers to these problems.\n",
    "When the system is more advanced, it will allow measuring the net worth of integrating a dialog system into a QA system. Does allowing such a dialog really enables to reach faster and more precisely the \"right\" answer to a question?\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-405"
  },
  "bernsen05_interspeech": {
   "authors": [
    [
     "Niels Ole",
     "Bernsen"
    ],
    [
     "Laila",
     "Dybkjaer"
    ]
   ],
   "title": "User evaluation of conversational agent h. c. Andersen",
   "original": "i05_2473",
   "page_count": 4,
   "order": 406,
   "p1": "2473",
   "pn": "2476",
   "abstract": [
    "The Hans Christian Andersen (HCA) system is an example of a new generation of embodied conversational characters which are aimed to faithfully represent a familiar historical individual and carry out human-style conversation as that individual would have done had he or she lived today. A first prototype of fairytale author HCA was tested with representative users in January 2004. This paper reports on the user test of the second prototype which was done in February 2005, focusing on the structured user interview results.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-406"
  },
  "goronzy05_interspeech": {
   "authors": [
    [
     "Silke",
     "Goronzy"
    ],
    [
     "Nicole",
     "Beringer"
    ]
   ],
   "title": "Integrated development and on-the-fly simulation of multimodal dialogs",
   "original": "i05_2477",
   "page_count": 4,
   "order": 407,
   "p1": "2477",
   "pn": "2480",
   "abstract": [
    "This paper presents a tool that allows the integrated development of multimodal dialog systems. We show how graphical/ haptical and speech interface can be designed in an integrated way, which is in contrast to many state-of-the-art HMI design processes. Graphical/ haptical and speech interface are often designed more or less independently of each other, which can result in inconsistencies of various kinds. The integrated design results in truly multimodal interfaces, which allow a change in modality at any point. A key feature is the capability to immediately simulate the complete HMI during the specification process, which allows evaluation of all modalities in combination at very early development stages. The tool can be used for rapid prototyping of multimodal dialog systems in the research area, but at the same time fulfils several requirements that are posed to the HMI development in a mass production process, e.g. in the automotive area.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-407"
  },
  "rotaru05b_interspeech": {
   "authors": [
    [
     "Mihai",
     "Rotaru"
    ],
    [
     "Diane J.",
     "Litman"
    ],
    [
     "Katherine",
     "Forbes-Riley"
    ]
   ],
   "title": "Interactions between speech recognition problems and user emotions",
   "original": "i05_2481",
   "page_count": 4,
   "order": 408,
   "p1": "2481",
   "pn": "2484",
   "abstract": [
    "Understanding how speech recognition problems affect the interaction with the user is a topic of great interest for the spoken dialogue community. In this paper, we examine the dependencies between speech recognition problems in adjacent turns. We also examine the dependencies between speech recognition problems and student emotions within a turn and in adjacent turns. We apply Chi Square (χ2) analysis to a corpus of speech-based computer tutoring dialogues to discover these dependencies. We find that rejections are followed by more rejections than expected if there was no dependency between rejections, and that misrecognitions are followed by more misrecognitions than expected. We also find a strong dependency between recognition problems in the previous turn and user emotion in the current turn: after a system rejection there are more emotional user turns than expected. Surprisingly, in our data, we find no relationship between user emotions and recognition problems within a turn nor between previous turn user emotions and current turn recognition problems.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-408"
  },
  "feng05_interspeech": {
   "authors": [
    [
     "Junlan",
     "Feng"
    ],
    [
     "Srihari",
     "Reddy"
    ],
    [
     "Murat",
     "Saraçlar"
    ]
   ],
   "title": "Webtalk: mining websites for interactively answering questions",
   "original": "i05_2485",
   "page_count": 4,
   "order": 409,
   "p1": "2485",
   "pn": "2488",
   "abstract": [
    "This paper investigates whether a customer care dialog system can be built automatically by mining and leveraging the wealth of information on a company's website. Our objective is to allow the users to ask FAQ-like questions which may request a specific piece of information, an analytical answer, or a transaction such as checking the status of the payment. We developed an infrastructure, referred to as WebTalk, that constructs different application-specific dialog systems by taking different websites as input. In this paper, we overview the involved technologies in WebTalk, address the challenges that will be shown very different from those in traditional dialog systems, and describe our efforts to approach these challenges. We present an evaluation for one WebTalk component: website based question answering.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-409"
  },
  "moller05_interspeech": {
   "authors": [
    [
     "Sebastian",
     "Möller"
    ]
   ],
   "title": "Towards generic quality prediction models for spoken dialogue systems - a case study",
   "original": "i05_2489",
   "page_count": 4,
   "order": 410,
   "p1": "2489",
   "pn": "2492",
   "abstract": [
    "In this paper, models are investigated which aim at predicting quality perceived during the interaction with spoken dialogue systems, on the basis of instrumentally or expert-derived interaction parameters. More specifically, it will be evaluated how generic model predictions are when going from one system or user group to the next. In two experiments, user quality judgments have been collected according to a recently standardized framework, and a large number of interaction parameters have been extracted. The results show that both changes in the user group and in the system configuration significantly impact prediction performance. Potential reasons for the observed limitations are discussed.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-410"
  },
  "parthasarathy05_interspeech": {
   "authors": [
    [
     "S.",
     "Parthasarathy"
    ],
    [
     "Cyril",
     "Allauzen"
    ],
    [
     "R.",
     "Munkong"
    ]
   ],
   "title": "Robust access to large structured data using voice form-filling",
   "original": "i05_2493",
   "page_count": 4,
   "order": 411,
   "p1": "2493",
   "pn": "2496",
   "abstract": [
    "A method for accurate and scalable form-filling by voice is presented. A form consists of a number of fields. Accurate speech recognition is achieved by applying task-specific inter-field constraints. The task constraints are specified typically by providing a database of valid form-entries, such as an employee directory containing the name, location, and telephone number. Scalability to very large vocabularies, number of fields, and the ability to accept a variety of user responses, is achieved by a two-pass recognition scheme. An index-based retrieval method is used in the first-pass to produce a shortlist of form-entries. These are rescored in the second-pass to obtain the final result. Experiments on a simple corporate directory access application are presented to demonstrate that the new approach compares favorably, in terms of computing needs, with a traditional one-pass speech recognition system. Experiments on a national street address recognition application are presented to demonstrate that the new approach scales very well to large tasks.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-411"
  },
  "rochetcapellan05_interspeech": {
   "authors": [
    [
     "Amélie",
     "Rochet-Capellan"
    ],
    [
     "Jean-Luc",
     "Schwartz"
    ]
   ],
   "title": "The labial-coronal effect and CVCV stability during reiterant speech production: an acoustic analysis",
   "original": "i05_1009",
   "page_count": 4,
   "order": 412,
   "p1": "1009",
   "pn": "1012",
   "abstract": [
    "This study investigates the stability of LC (Labial-Coronal) and CL (Coronal-Labial) CVCV sequences during a repetition task at an increasing rate. Despite variability between subjects and differences linked to the various consonant manners of articulation used in the study, the results show that the LC pattern is more stable than the CL pattern. Indeed, under the rate pressure, CL sequences often evolved to LC ones (e.g. /sapa/ switched to /pasa/) whereas the reverse pattern almost never happened. Coupled with previous studies about world languages, speech development and verbal transformation experiments, this is interpreted as a possible influence of the substance on phonological forms.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-412"
  },
  "rochetcapellan05b_interspeech": {
   "authors": [
    [
     "Amélie",
     "Rochet-Capellan"
    ],
    [
     "Jean-Luc",
     "Schwartz"
    ]
   ],
   "title": "The labial-coronal effect and CVCV stability during reiterant speech production: an articulatory analysis",
   "original": "i05_1013",
   "page_count": 4,
   "order": 413,
   "p1": "1013",
   "pn": "1016",
   "abstract": [
    "In a companion paper [1], we showed that CVCV utterances with a labial consonant followed by a coronal one (LC sequences) are more stable than reverse CL sequences in speeded reiterant speech. We proposed that this could explain why human languages select LC sequences more often than CL ones (the \"LC effect\"). We provide here articulatory data explaining where the greater LC stability could come from, by investigating inter-articulator coordination during LC and CL utterances at an increasing rate. Rate increase leads variegated CVCV (e.g. /pata/) to be produced in a single jaw cycle but this is not the case for duplicated CVCV (e.g. /papa/. Furthermore, LC and CL sequences both evolve towards the same cycle with a progressive phasing of lips and tongue close together in the jaw cycle. Taken together, these results provide new elements to argue for motor control constraints shaping phonological patterns from economy principles.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-413"
  },
  "nakamura05_interspeech": {
   "authors": [
    [
     "Mitsuhiro",
     "Nakamura"
    ]
   ],
   "title": "Articulatory constraints and coronal stops: an EPG study",
   "original": "i05_1017",
   "page_count": 4,
   "order": 414,
   "p1": "1017",
   "pn": "1020",
   "abstract": [
    "This study explores articulatory and coarticulatory properties of the coronal stop articulations [t, d, R] in Japanese, using the technique of electropalatography (EPG). We examine the spatiotemporal characteristics of linguopalatal contact patterns, closure duration, and intergestural coordination, relating them to factors of the articulatory control mechanisms. The results bear on issues in models of lingual articulation, the concept of articulatory constraints and phonological feature specifications.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-414"
  },
  "robert05_interspeech": {
   "authors": [
    [
     "Vincent",
     "Robert"
    ],
    [
     "Brigitte",
     "Wrobel-Dautcourt"
    ],
    [
     "Yves",
     "Laprie"
    ],
    [
     "Anne",
     "Bonneau"
    ]
   ],
   "title": "Strategies of labial coarticulation",
   "original": "i05_1021",
   "page_count": 4,
   "order": 415,
   "p1": "1021",
   "pn": "1024",
   "abstract": [
    "In this article, we present first conclusions about labial coarticulation strategies drawn from a corpus of speech audiovisual data. The general idea is to develop a talking head which would be understandable by lip readers especially deaf persons. With a stereovision system, we recorded a corpus with ten French native speakers (5 female and 5 male speakers). Visual and audio information was analysed to extract the labial parameters (opening of the lips, stretching and protrusion). Even if the analysis shows a great variability between speakers, we nevertheless found general tendencies which would help us to develop a reliable prediction algorithm of labial coarticulation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-415"
  },
  "dang05_interspeech": {
   "authors": [
    [
     "Jianwu",
     "Dang"
    ],
    [
     "Jianguo",
     "Wei"
    ],
    [
     "Takeharu",
     "Suzuki"
    ],
    [
     "Pascal",
     "Perrier"
    ]
   ],
   "title": "Investigation and modeling of coarticulation during speech",
   "original": "i05_1025",
   "page_count": 4,
   "order": 416,
   "p1": "1025",
   "pn": "1028",
   "abstract": [
    "Coarticulation is an important phenomenon of speech production, which involves in both the physical level concerned with articulators' properties, and the planning stage for generating motor commands. The authors have proposed a model for coarticulation, named \"carrier model\", to imitate the coarticulation mechanism. The carrier model is built on an assumption that articulation can be separated into a vocalic movement and a consonantal movement, and that coarticulation takes place within and between the movements as a modulation process. This study first verifies the assumption using a reconstructed text-independent articulatory movement based on observed articulatory data. The carrier model is elaborated based on those two movements and the look-ahead mechanism is employed to materialize the coarticulation. Coefficients of the carrier model were quantified via statistical analysis of articulatory data. A numerical experiment was conducted by implementing the carrier model in a physiological articulatory model (Dang and Honda, 2004). The results showed that the carrier model gave a good performance for simulating coarticulation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-416"
  },
  "hu05b_interspeech": {
   "authors": [
    [
     "Fang",
     "Hu"
    ]
   ],
   "title": "Tongue kinematics in diphthong production in Ningbo Chinese",
   "original": "i05_1029",
   "page_count": 4,
   "order": 417,
   "p1": "1029",
   "pn": "1032",
   "abstract": [
    "This paper investigates some kinematic aspects of the tongue articulators during diphthong production in Ningbo Chinese. Acquired EMA data from six speakers show that the average velocity and the peak velocity of the lingual articulators roughly characterize the Ningbo diphthongs, given that the falling diphthongs and rising diphthongs are considered separately. The time to peak velocity serves as a better criterion in distinguishing the falling diphthongs from rising diphthongs. Generally, peak velocities of the lingual articulators occur late in falling diphthongs while occur early in rising diphthongs. The short diphthong production is characterized by an acceleration of the lingual articulator. And results suggest that the time to peak velocity is quite constant under the duration reduction.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-417"
  },
  "arai05_interspeech": {
   "authors": [
    [
     "Takayuki",
     "Arai"
    ]
   ],
   "title": "Comparing tongue positions of vowels in oral and nasal contexts",
   "original": "i05_1033",
   "page_count": 4,
   "order": 418,
   "p1": "1033",
   "pn": "1036",
   "abstract": [
    "We studied tongue positions of vowels in oral and nasal contexts. In the previous study [Arai, J. Acoust. Soc. Am., 115, p.2541 (2004)], formant frequencies were measured and bidirectional formant shifts in F1 frequency were observed: increasing F1 for high vowels and decreasing F1 for low vowels. Then, we tried to answer to the next question, that is, whether or not speakers and/or listeners compensate for the formant shifts. The perceptual experiment by Arai (2004) showed that compensation occurs when an isolated vowel has nasalization and is accompanied by formant transitions. This result agreed with the findings of Krakow et al. [J. Acoust. Soc. Am. 83, 1146-1158 (1988)]. The goal of this study is to examine the compensation effect for the formant shifts in production. In the EMMA experiment, the measurement of the positions of the articulators showed almost no compensation except for the lowest vowel /A/.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-418"
  },
  "ouni05_interspeech": {
   "authors": [
    [
     "Slim",
     "Ouni"
    ]
   ],
   "title": "Can we retrieve vocal tract dynamics that produced speech? toward a speaker articulatory strategy model",
   "original": "i05_1037",
   "page_count": 4,
   "order": 419,
   "p1": "1037",
   "pn": "1040",
   "abstract": [
    "In this paper, we argue the possibility of retrieving the vocal tract dynamics from speech as was produced by a given speaker. First, we present an inversion method that provides a complete set of articulatory solutions without excessive constraints. The selection of articulatory trajectories is not an easy task, as many trajectories are possible. Based on some inversion experiments and data observation, we show the importance of the introduction of the notion of speaker articulatory strategy to be able to model this variability.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-419"
  },
  "perrier05_interspeech": {
   "authors": [
    [
     "Pascal",
     "Perrier"
    ],
    [
     "Liang",
     "Ma"
    ],
    [
     "Yohan",
     "Payan"
    ]
   ],
   "title": "Modeling the production of VCV sequences via the inversion of a biomechanical model of the tongue",
   "original": "i05_1041",
   "page_count": 4,
   "order": 420,
   "p1": "1041",
   "pn": "1044",
   "abstract": [
    "A control model of the production of VCV sequences is presented, which consists in three main parts: a static forward model of the relations between motor commands and acoustic properties; the specification of targets in the perceptual space; a planning procedure based on optimization principles. Examples of simulations generated with this model illustrate how it can be used to assess theories and models of coarticulation in speech.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-420"
  },
  "niu05_interspeech": {
   "authors": [
    [
     "Xiaochuan",
     "Niu"
    ],
    [
     "Alexander",
     "Kain"
    ],
    [
     "Jan P. H. van",
     "Santen"
    ]
   ],
   "title": "Estimation of the acoustic properties of the nasal tract during the production of nasalized vowels",
   "original": "i05_1045",
   "page_count": 4,
   "order": 421,
   "p1": "1045",
   "pn": "1048",
   "abstract": [
    "Accurate estimation of velar movements is useful for automatic speech recognition, speech enhancement, and diagnosis of certain speech disorders. This paper reports on initial results of a project on estimation of velar movements, for two-microphone setups where the microphones are differentially positioned to pick up nasal and oral speech output. Toward this goal, we propose a method that allows detailed estimation of the acoustic properties of the nasal tract in the simplified condition where the two microphone signals exhibit complete source separation. We successfully test the method against synthetic speech, generated by an articulatory synthesizer in which the acoustic properties of the simulated nasal tract are known.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-421"
  },
  "ogata05b_interspeech": {
   "authors": [
    [
     "Kohichi",
     "Ogata"
    ]
   ],
   "title": "A web-based articulatory speech synthesis system for distance education",
   "original": "i05_1049",
   "page_count": 4,
   "order": 422,
   "p1": "1049",
   "pn": "1052",
   "abstract": [
    "We have developed a speech synthesis system based on the speech production process. In the system, control parameters for the vocal tract shape and vocal cords are interactively set in order to produce speech output. Such an intuitive system is useful not only for speech research but also for teaching materials in speech science. In this paper, a network-based articulatory speech synthesis system has been developed for distance education. In order to develop a Web-based articulatory speech synthesis system, a stand-alone type system previously developed was modified by using the program language, Java. Preliminary experiments showed the effectiveness of the system as a teaching material.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-422"
  },
  "alku05_interspeech": {
   "authors": [
    [
     "Paavo",
     "Alku"
    ],
    [
     "Matti",
     "Airas"
    ],
    [
     "Tom",
     "Bäckström"
    ],
    [
     "Hannu",
     "Pulakka"
    ]
   ],
   "title": "Group delay function as a means to assess quality of glottal inverse filtering",
   "original": "i05_1053",
   "page_count": 4,
   "order": 423,
   "p1": "1053",
   "pn": "1056",
   "abstract": [
    "The estimation of glottal flow with inverse filtering requires typically subjective evaluation of the obtained flow waveforms by the experimenter. In this paper, we propose a straightforward method that yields an objective analysis tool to be used in the assessment of the quality of inverse filtered glottal flow estimates. The method computes the negative derivative of the phase response, the group delay function, over a single glottal cycle of the estimated flow. Incorrect settings of the anti-resonances of the inverse filter result in distortion of the group delay function. This distortion is easily detectable from the group delay function and can therefore be used in a straightforward manner to analyse how well the zeros of the inverse filter match the poles of the vocal tract.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-423"
  },
  "bjorkner05_interspeech": {
   "authors": [
    [
     "Eva",
     "Björkner"
    ],
    [
     "Johan",
     "Sundberg"
    ],
    [
     "Paavo",
     "Alku"
    ]
   ],
   "title": "Subglottal pressure and NAQ variation in voice production of classically trained baritone singers",
   "original": "i05_1057",
   "page_count": 4,
   "order": 424,
   "p1": "1057",
   "pn": "1060",
   "abstract": [
    "The subglottal pressure (Ps) and voice source characteristics of five professional baritone singers were analyzed. Glottal adduction was estimated with amplitude quotient (AQ), defined as the ratio between peak-to-peak pulse amplitude and the negative peak of the differentiated flow glottogram, and with normalized amplitude quotient (NAQ), defined as AQ divided by fundamental period length. Previous studies show that NAQ and its variation with Ps represent an effective parameter in the analysis of voice source characteristics. Therefore, the present study aims at increasing our knowledge of these two parameters further by finding out how they vary with pitch and Ps in operatic baritone singers, singing at high and low pitch. Ten equally spaced Ps values were selected from three takes of the syllable [pae], repeated with a continuously decreasing vocal loudness and initiated at maximum vocal loudness. The vowel sounds following the selected Ps peaks were inverse filtered. Data on peak-to-peak pulse amplitude, maximum flow declination rate, AQ and NAQ will be presented.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-424"
  },
  "fant05_interspeech": {
   "authors": [
    [
     "Gunnar",
     "Fant"
    ],
    [
     "Anita",
     "Kruckenberg"
    ]
   ],
   "title": "Covariation of subglottal pressure, F0 and intensity",
   "original": "i05_1061",
   "page_count": 4,
   "order": 425,
   "p1": "1061",
   "pn": "1064",
   "abstract": [
    "This is a report summarising results from studies of true subglottal pressure, supraglottal pressure and speech wave data. We have derived co-variation patterns, which allow a prediction of intensity from subglottal pressure and F0, and conversely a prediction of a subglottal pressure contour from F0 and intensity. Of special interest is the significance of a mid-point in a speakers' available F0 range at which the relations change. In the lower F0 range subglottal pressure and intensity rise with F0, whilst in the upper part they tend to saturate. In connected speech we find a build-up of subglottal pressure well in advance of a stressed word, and a decay of subglottal pressure in the final part of a phrase starting already in the falling branch of an F0 peak.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-425"
  },
  "perez05_interspeech": {
   "authors": [
    [
     "Javier",
     "Pérez"
    ],
    [
     "Antonio",
     "Bonafonte"
    ]
   ],
   "title": "Automatic voice-source parameterization of natural speech",
   "original": "i05_1065",
   "page_count": 4,
   "order": 426,
   "p1": "1065",
   "pn": "1068",
   "abstract": [
    "We present here our work in automatic parameterization of natural speech by means of a pitch synchronous source-filter decomposition algorithm. The derivative glottal source is modelled using the Liljencrants-Fant (LF) model. The model parameters are obtained simultaneously with the coefficients of an all-pole filter representing the vocal tract response by means of a quadratic programming algorithm. Synthetic data has been created and analyzed in order to show the appropriate function of the estimation method. The parameterization results in high quality synthesized speech for voiced frames. Voice quality extraction is performed on basis to the LF source representation. The inherent modelling of the voice source makes it suitable for voice modification tasks. Work is in progress to add this speech representation to emotional speech synthesis and voice conversion algorithms.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-426"
  },
  "zeroual05_interspeech": {
   "authors": [
    [
     "Chakir",
     "Zeroual"
    ],
    [
     "John H.",
     "Esling"
    ],
    [
     "Lise",
     "Crevier-Buchman"
    ]
   ],
   "title": "Physiological study of whispered speech in Moroccan Arabic",
   "original": "i05_1069",
   "page_count": 4,
   "order": 427,
   "p1": "1069",
   "pn": "1072",
   "abstract": [
    "The analysis of pictures extracted from endoscopic video images filmed during isolated Moroccan Arabic /i1C1i2C2/ items (where C1 = fricatives + C2 = plosives), showed that: All the segments of a whispered item are produced with an anterior-posterior epilaryngeal compression, an adduction of the ventricular bands and of the membranous part of the glottis, and an opening at the level of its cartilaginous part. Glottal aperture differences observed in normal speech between voiceless consonants tend, in the whispered speech, to be maintained between the fricatives, and to be neutralised between the plosives. We did not observe regular laryngeal articulatory differences between all the whispered consonant pairs corresponding, in normal speech, to the linguistic \"voicing\" contrast.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-427"
  },
  "moura05_interspeech": {
   "authors": [
    [
     "C. P.",
     "Moura"
    ],
    [
     "D.",
     "Andrade"
    ],
    [
     "L. M.",
     "Cunha"
    ],
    [
     "M. J.",
     "Cunha"
    ],
    [
     "H.",
     "Vilarinho"
    ],
    [
     "H.",
     "Barros"
    ],
    [
     "Diamantino",
     "Freitas"
    ],
    [
     "M.",
     "Pais-Clemente"
    ]
   ],
   "title": "Voice quality in down syndrome children treated with rapid maxillary expansion",
   "original": "i05_1073",
   "page_count": 4,
   "order": 428,
   "p1": "1073",
   "pn": "1076",
   "abstract": [
    "Down syndrome (DS) is the most common aneuploid chromosomal disorder at birth. Phenotypic characteristics include mental retardation, general hypotonia, pharyngeal hypoplasia, frequently constricted maxillary arch with macroglossia and upper air way obstruction. Generally they have marked language difficulties and a characteristic voice. This prospective study assesses the effects of rapid maxillary expansion (RME) on voice quality in DS children. Acoustic (Praat-4.1 software) and perceptual assessment of the voice were performed in 24 children with DS, aged 4 to 12 years, who had been randomly allocated to RME and non-expanded (NE) groups. Two main evaluations were made; one prior to expansion and the second after two to four weeks of active RME plus a 5 month period of retention. Data between the two groups were compared. Perceptual analysis did not demonstrate any significant modification. RME produced significant changes in F1 frequency of vowel /a/ of DS children's voices, probably related to alteration of vertical tongue adjustment. It also showed a tendency to decrease F0 dispersion in all studied vowels, illustrating increased stability in F0 production. These results are related to the correction of some DS typical mid-face skeletal deformities such as maxillary bone widening with improvement of upper airway airflow and increased space for the tongue in the oral cavity. The acoustic analysis is a simple, rapid and non-invasive technique which is well accepted by the paediatric population; features that are relevant to its application especially in children with mental retardation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-428"
  },
  "hanquinet05_interspeech": {
   "authors": [
    [
     "Julien",
     "Hanquinet"
    ],
    [
     "Francis",
     "Grenez"
    ],
    [
     "Jean",
     "Schoentgen"
    ]
   ],
   "title": "Synthesis of disordered speech",
   "original": "i05_1077",
   "page_count": 4,
   "order": 429,
   "p1": "1077",
   "pn": "1080",
   "abstract": [
    "The presentation concerns a synthesizer of disordered voices. The synthesizer consists of a non-linear model of the phonatory excitation and a vocal tract simulation based on a concatenation of cylindrical tubelets. For each tubelet, viscous, thermal and wall vibrations losses are modeled by means of numerical filters. A conical tubelet is added at the lip-end to simulate the transition from one-dimensional to three-dimensional wave propagation. A source-tract interaction is included. The synthetic phonatory excitation signal is obtained via a shaping function that transforms an harmonic driving function into the desired waveshape. The instantaneous frequency and the spectral slope of the phonatory excitation are controlled by the instantaneous frequency and amplitude of the harmonic driving function. Several types of dysperiodicities are simulated by modulating these two parameters. The voice disorders that the synthesizer is able to simulate are pathological vocal jitter and vocal tremor, biphonation, diplophonia and random vibrations of the vocal folds. Turbulence noise is modeled via additive white noise.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-429"
  },
  "fontecave05_interspeech": {
   "authors": [
    [
     "Julie",
     "Fontecave"
    ],
    [
     "Frédéric",
     "Berthommier"
    ]
   ],
   "title": "Quasi-automatic extraction of tongue movement from a large existing speech cineradiographic database",
   "original": "i05_1081",
   "page_count": 4,
   "order": 430,
   "p1": "1081",
   "pn": "1084",
   "abstract": [
    "Automatic analysis of tongue movement in large existing cineradiographic databases can provide valuable information to understood speech production. We describe here a method for semi-automatic extraction of articulatory information from video observation in order to derive quasi-automatically a geometrical parameterization of the vocal tract movements. The algorithm starts with a limited manual processing step consisting in marking 10 points (12 degrees of freedom) on 100 chosen key images. The treatment on the whole sequence is then automatic thanks to a retro-marking method. At first, the whole database is indexed via a similarity measure performed with the key images. Then, we associate on the original images the geometrical information recovered on the key images via this indexing. Different complementary error reduction methods are also proposed. Averaging geometrical configurations of a neighborhood, temporal filtering and spline interpolation allow to reduce the reconstruction error to about 10 pixels for a tongue contour of average length of 260 pixels.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-430"
  },
  "sapir05_interspeech": {
   "authors": [
    [
     "Shimon",
     "Sapir"
    ],
    [
     "Ravit Cohen",
     "Mimran"
    ]
   ],
   "title": "The working memory token test (WMTT): preliminary findings in young adults with and without dyslexia",
   "original": "i05_1085",
   "page_count": 4,
   "order": 431,
   "p1": "1085",
   "pn": "1088",
   "abstract": [
    "The purpose of the study was to assess the validity of a new version of the Token Test (TT) which is aimed at detecting deficits in auditory verbal working memory (henceforth WMTT). Twenty eight young healthy adults, 14 with and 14 without dyslexia, all paid volunteers, took part in the study. The WMTT was found to be positively and significantly correlated with tests of digit span forward and backward, pseudoword reading performance, but not with real word reading, performance IQ, and the d-2 attention measure. The WMTT also appeared to be a more sensitive method than the original TT to detect differences in auditory verbal working memory between the two subject groups. These main findings, albeit preliminary, indicate that the WMTT is superior to the original Token Test (memory parts) in assessing auditory verbal working memory deficits.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-431"
  },
  "paulo05_interspeech": {
   "authors": [
    [
     "Sérgio",
     "Paulo"
    ],
    [
     "Luís C.",
     "Oliveira"
    ]
   ],
   "title": "Reducing the corpus-based TTS signal degradation due to speaker's word pronunciations",
   "original": "i05_1089",
   "page_count": 4,
   "order": 432,
   "p1": "1089",
   "pn": "1092",
   "abstract": [
    "The goal of producing a corpus-based synthesizer with the owner's voice can only be achieved if the system can handle recordings with less than ideal characteristics. One of the limitations is that a normal speaker does not always pronounce a word exactly as predicted by the language rules. In this work we compare two methods for handling variations on word pronunciation for corpus-based speech synthesizers. Both approaches rely on a speech corpus aligned with a phone-level segmentation tool that allows alternative word pronunciations. The first approach performs an alignment between the observed pronunciation and the canonical form used in the system's lexicon, allowing the mapping of the time labels from the observed phones into the canonical form. At synthesis time the unit selection is performed on the phone sequence predicted by the system. In the second approach, no modification is performed on the phone sequence generated by the segmentation tool. This way, at synthesis time, the words are converted into phones by using the speaker's word pronunciation, rather than the system's lexicon. Finally, both approaches are compared by evaluating the naturalness of the signals generated by each approach.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-432"
  },
  "lee05d_interspeech": {
   "authors": [
    [
     "Wai-Sum",
     "Lee"
    ]
   ],
   "title": "A phonetic study of the \"er-hua\" rimes in Beijing Mandarin",
   "original": "i05_1093",
   "page_count": 4,
   "order": 433,
   "p1": "1093",
   "pn": "1096",
   "abstract": [
    "The paper is the acoustical, articulatory, and physiological investigation of the phonetic changes of the \"er-hua\" rimes, i.e., the rimes affixed with the diminutive suffix \"er\" [Ä] child', in Beijing Mandarin. Acoustically, all the [Ä]-suffixed rimes have a low F3 value, as an indication of being rhotacized. The other changes of the rimes after [Ä]-suffixation vary, depending on the type of component segments of the rimes. The rimes are added with a sub-syllabic [Ä], when the rimes end with a non-back vowel. When the rimes end with a back vowel, no [Ä] is added to the rimes and the whole of the rimes becomes rhotacized. For the rimes which contain a diphthong or triphthong with a final vowel element [i] or [e], the final vowel element is deleted. For the rimes which end with an alveolar [n], the nasal ending is deleted. For the rimes which end with a velar [N], the nasal ending is also deleted and the whole of the rimes is nasalized.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-433"
  },
  "airas05_interspeech": {
   "authors": [
    [
     "Matti",
     "Airas"
    ],
    [
     "Hannu",
     "Pulakka"
    ],
    [
     "Tom",
     "Bäckström"
    ],
    [
     "Paavo",
     "Alku"
    ]
   ],
   "title": "A toolkit for voice inverse filtering and parametrisation",
   "original": "i05_2145",
   "page_count": 4,
   "order": 434,
   "p1": "2145",
   "pn": "2148",
   "abstract": [
    "Voice inverse filtering is a process in which the effects of the vocal tract are cancelled from the speech signal to estimate the airflow through the glottis. Although inverse filtering has many applications in both research and clinical examination of voice production, few voice inverse filtering software packages exist. In the present paper, we propose a flexible software package, HUT Voice Source Analysis and Parametrisation Toolkit (Aparat) for use in the MATLAB environment. It implements glottal inverse filtering and several time-based parameters of the voice source in a graphical user interface. The software package is available under an open source licence at http://www.acoustics.hut.fi/software/aparat/.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-434"
  },
  "sciamarella05_interspeech": {
   "authors": [
    [
     "Denisse",
     "Sciamarella"
    ],
    [
     "Christophe",
     "d'Alessandro"
    ]
   ],
   "title": "Stylization of glottal-flow spectra produced by a mechanical vocal-fold model",
   "original": "i05_2149",
   "page_count": 4,
   "order": 435,
   "p1": "2149",
   "pn": "2152",
   "abstract": [
    "A method is proposed to extract glottal-flow spectra from numerical simulations of vocal-fold behavior with a two-mass model including dynamic flow separation. The numerical spectrum, whose general form complies with that of signal glottal-flow models, allows stylization with three linear segments. The slope of the first segment remains relatively constant when source control parameters are varied, whereas the slope of the last segment (i.e. the spectral tilt) is highly sensitive to the vibrating vocal-fold mass, tension and stiffness. The phase of mobility of the flow separation position within the glottal cycle may introduce, if long enough, a dip in the glottal-flow spectrum.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-435"
  },
  "nomura05_interspeech": {
   "authors": [
    [
     "Hideyuki",
     "Nomura"
    ],
    [
     "Tetsuo",
     "Funada"
    ]
   ],
   "title": "Numerical glottal sound source model as coupled problem between vocal cord vibration and glottal flow",
   "original": "i05_2153",
   "page_count": 4,
   "order": 436,
   "p1": "2153",
   "pn": "2156",
   "abstract": [
    "Understanding of dynamics of the speech production process may lead to a breakthrough in increasing the quality of synthesized voice and contribute to medical science with respect to treating voice disorder. However, little is known about mechanisms of speech dynamics, in particular about complex interactions of glottal flow and vocal fold vibration. In this paper, a two-dimensional viscoelastic body model of the vocal folds coupled with an unsteady glottal flow is proposed, and we demonstrate the speech production process by using that model. The vocal cord was modeled by distributed mass-spring-damper elements. The speech production process is simulated by numerically solving equations of nonlinear compressible viscous fluid coupled with motion of vocal fold. The results indicated that the upper and lower edges of the vocal fold vibrated with a phase difference. Furthermore, a change in pressure in the larynx synchronized with the vocal fold vibration, and both amplitude and fundamental frequency of the speech wave slightly fluctuated with time. Although we assumed the physical property of left and right part of vocal cords to be symmetry, the each vocal cord vibration indicated some difference. From the obtained results, although there are some problems to be discussed, one may say that the proposed model is valid for the glottal sound source.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-436"
  },
  "pouplier05_interspeech": {
   "authors": [
    [
     "Marianne",
     "Pouplier"
    ],
    [
     "Maureen",
     "Stone"
    ]
   ],
   "title": "A tagged-cine MRI investigation of German vowels",
   "original": "i05_2157",
   "page_count": 4,
   "order": 437,
   "p1": "2157",
   "pn": "2160",
   "abstract": [
    "This project investigates the articulatory properties of German vowels on the basis of tagged Cine-MRI data. German has (1)5 monophthongs, which are classified into seven tense-lax pairs. Understanding the phonetic correlate of vowel tenseness has proven elusive, partly due to the difficulty of obtaining global shape information for the tongue, especially the pharyngeal portion. Tagged Cine-MRI allows motion trajectories for individual tissue points to be tracked and strains to be calculated. From these data one can infer contraction patterns of the muscles of the tongue. The current project uses this technique to compare four tense and lax vowel pairs in terms of muscular compression and expansion patterns as well as root-blade motion.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-437"
  },
  "serrurier05_interspeech": {
   "authors": [
    [
     "Antoine",
     "Serrurier"
    ],
    [
     "Pierre",
     "Badin"
    ]
   ],
   "title": "A three-dimensional linear articulatory model of velum based on MRI data",
   "original": "i05_2161",
   "page_count": 4,
   "order": 438,
   "p1": "2161",
   "pn": "2164",
   "abstract": [
    "In the framework of studies on nasality, we have attempted to develop a 3-D articulatory model of velum. Sets of 25 sagittal MRI images have been collected for one French subject sustaining 46 articulations. Contours of the velum have been manually drawn from these images; a common 3-D triangular mesh has then been fitted to these contours in order to constitute 3-D representations of the velum outline. This resulted in 46 meshes of 5239 vertices defined by their 3-D coordinates.\n",
    "In order to develop a velum model free of the influence of the tongue, the analysis was first limited to the 28 articulations where no tongue - velum contact occurs: it was found that the first component extracted from these data by Principal Component Analysis can explain as much as 79% of the variance of the complete 3-D shape of the velum, while the second component was meaningless in terms of articulatory movements. It was also found that the same model could explain 71% of the variance for the corpus of the 18 articulations where tongue - velum contact occurs. It was finally shown that the vertical coordinate of a single flesh point attached to the downward face of the velum is enough to predict the complete 3-D shape of the free velum.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-438"
  },
  "cros05_interspeech": {
   "authors": [
    [
     "Anne",
     "Cros"
    ],
    [
     "Didier",
     "Demolin"
    ],
    [
     "Ana Georgina",
     "Flesia"
    ],
    [
     "Antonio",
     "Galves"
    ]
   ],
   "title": "On the relationship between intra-oral pressure and speech sonority",
   "original": "i05_2165",
   "page_count": 4,
   "order": 439,
   "p1": "2165",
   "pn": "2168",
   "abstract": [
    "We address the question of the relationship between two time series associated to the speech signal. The first one is the sonority function which was introduced in Galves et al. (2002) as an index of the local regularity of the speech signal. The second time series is the intra-oral pressure during the production of speech. We argue that the joint evolution of both time series can be well described by a simple probabilistic model. We show that our model is in good agreement with the results obtained by analyzing a linguistic corpus with recorded sentences in French and Kinyarwanda.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-439"
  },
  "jones05_interspeech": {
   "authors": [
    [
     "Douglas",
     "Jones"
    ],
    [
     "Wade",
     "Shen"
    ],
    [
     "Elizabeth",
     "Shriberg"
    ],
    [
     "Andreas",
     "Stolcke"
    ],
    [
     "Teresa",
     "Kamm"
    ],
    [
     "Douglas",
     "Reynolds"
    ]
   ],
   "title": "Two experiments comparing reading with listening for human processing of conversational telephone speech",
   "original": "i05_1145",
   "page_count": 4,
   "order": 440,
   "p1": "1145",
   "pn": "1148",
   "abstract": [
    "We report on results of two experiments designed to compare subjects' ability to extract information from audio recordings of conversational telephone speech (CTS) with their ability to extract information from text transcripts of these conversations, with and without the ability to hear the audio recordings. Although progress in machine processing of CTS speech is well documented, human processing of these materials has not been as well studied. These experiments compare subject's processing time and comprehension of widely-available CTS data in audio and written formats - one experiment involves careful reading and one involves visual scanning for information. We observed a very modest improvement using transcripts compared with the audio-only condition for the careful reading task (speed-up by a factor of 1.2) and a much more dramatic improvement using transcripts in the visual scanning task (speed-up by a factor of 2.9). The implications of the experiments are twofold: (1) we expect to see similar gains in human productivity for comparable applications outside the laboratory environment and (2) the gains can vary widely, depending on the specific tasks involved.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-440"
  },
  "galliano05_interspeech": {
   "authors": [
    [
     "Sylvain",
     "Galliano"
    ],
    [
     "Edouard",
     "Geoffrois"
    ],
    [
     "Djamel",
     "Mostefa"
    ],
    [
     "Khalid",
     "Choukri"
    ],
    [
     "Jean-François",
     "Bonastre"
    ],
    [
     "Guillaume",
     "Gravier"
    ]
   ],
   "title": "The ESTER phase II evaluation campaign for the rich transcription of French broadcast news",
   "original": "i05_1149",
   "page_count": 4,
   "order": 441,
   "p1": "1149",
   "pn": "1152",
   "abstract": [
    "This paper gives the final results of the Ester evaluation campaign which started in 2003 and ended in January 2005. The aim of this campaign was to evaluate automatic broadcast news rich transcription systems for the French language. The evaluation tasks were divided into three main categories: orthographic transcription, event detection and tracking (e.g. speech vs. music, speaker tracking), and information extraction. The last one, limited to named entity detection in this evaluation, was a preliminary test. The paper reports on protocols and gives the results obtained in the campaign.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-441"
  },
  "saito05_interspeech": {
   "authors": [
    [
     "Takashi",
     "Saito"
    ]
   ],
   "title": "A method of multi-layered speech segmentation tailored for speech synthesis",
   "original": "i05_1153",
   "page_count": 4,
   "order": 442,
   "p1": "1153",
   "pn": "1156",
   "abstract": [
    "This paper presents a speech segmentation scheme designed to be used in creating voice inventories for speech synthesis. Just the information about phoneme segments in a given speech corpus is not sufficient for speech synthesis, but multi-layers of segments such as breath groups, accent phrases, phonemes, and pitchmarks, are all necessary to reproduce the prosody and acoustics of a given speaker. The segmentation algorithm devised here has the capability of extracting the multi-layered segmental information in a distinctly organized fashion, and is fairly robust to speaker differences and speaking styles. The experimental evaluations with on speech corpora with a fairly large variation of speaking styles show that the speech segmentation algorithm is quite accurate and robust in extracting segments of both phonemes and accentual phrases.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-442"
  },
  "paulo05b_interspeech": {
   "authors": [
    [
     "Sérgio",
     "Paulo"
    ],
    [
     "Luís C.",
     "Oliveira"
    ]
   ],
   "title": "Generation of word alternative pronunciations using weighted finite state transducers",
   "original": "i05_1157",
   "page_count": 4,
   "order": 443,
   "p1": "1157",
   "pn": "1160",
   "abstract": [
    "This paper describes a speech segmentation tool allowing alternative word pronunciations within a WFST framework. Two approaches to word pronunciation graph generation were developed and evaluated. The first approach is grapheme-based where each grapheme is converted into all the phones it can give rise to, in the form of a WFST. Word graphs are obtained by concatenating all grapheme WFSTs. In the second approach, a training corpus is used to find the different realizations of the syllable. This information is used to generate alternative syllable-level pronunciations, represented as WFSTs, that are concatenated to produce the word graphs. Both approaches were evaluated by aligning the phone sequence generated by each approach with the manually labelled phone sequence for all utterance in the corpus. This alignment was used for computing F-rate values for each phone. The syllable-based approach produced the best results.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-443"
  },
  "strik05_interspeech": {
   "authors": [
    [
     "Helmer",
     "Strik"
    ],
    [
     "Diana",
     "Binnenpoorte"
    ],
    [
     "Catia",
     "Cucchiarini"
    ]
   ],
   "title": "Multiword expressions in spontaneous speech: do we really speak like that?",
   "original": "i05_1161",
   "page_count": 4,
   "order": 444,
   "p1": "1161",
   "pn": "1164",
   "abstract": [
    "In this study, we examined the pronunciation characteristics of multiword expressions (MWEs). We first drew up an inventory of frequently occurring N-grams extracted from orthographic transcriptions of spontaneous speech contained in a large corpus of spoken Dutch. For about 10% of these N-grams phonetic transcriptions were available, which were examined. Our results show that the pronunciation of these N-grams differed to a large extent from the canonical form. In order to determine whether this is a general characteristic of spontaneous speech or rather the effect of the specific status of these N-grams, we analyzed the pronunciations of the individual words composing the N-grams in two context conditions: 1) in the N-gram context and 2) in any other context. We found that words in N-grams do indeed have peculiar pronunciation patterns. This seems to suggest that these N-grams may be considered as MWEs that should therefore be treated as lexical entries with their own specific pronunciation variants in the pronunciation lexicons used for e.g. automatic speech recognition (ASR) and automatic phonetic transcription (APT).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-444"
  },
  "kolar05_interspeech": {
   "authors": [
    [
     "Jáchym",
     "Kolár"
    ],
    [
     "Jan",
     "Svec"
    ],
    [
     "Stephanie",
     "Strassel"
    ],
    [
     "Christopher",
     "Walker"
    ],
    [
     "Dagmar",
     "Kozlíková"
    ],
    [
     "Josef",
     "Psutka"
    ]
   ],
   "title": "Czech spontaneous speech corpus with structural metadata",
   "original": "i05_1165",
   "page_count": 4,
   "order": 445,
   "p1": "1165",
   "pn": "1168",
   "abstract": [
    "This paper describes a Czech spontaneous speech corpus consisting of radio talk show recordings. As the first complete non-English MDE corpus, it has been annotated with structural metadata information beyond the words that is critical to both increasing transcript readability and allowing application of downstream NLP methods. Metadata annotation involves partitioning verbatim transcripts into syntactic/semantic units (SUs) that function to express a complete idea; and identifying fillers and edit disfluencies. Annotation guidelines for English metadata developed by Linguistic Data Consortium were taken as the starting point, with changes applied to accommodate specific phenomena of Czech. In addition to the necessary language-dependent modifications, we further propose some language-independent modifications including limited prosodic labeling at SU boundaries. Statistics about the structural metadata annotation present in the corpus and inter-annotator agreement numbers are also presented.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-445"
  },
  "burkhardt05b_interspeech": {
   "authors": [
    [
     "Felix",
     "Burkhardt"
    ],
    [
     "A.",
     "Paeschke"
    ],
    [
     "M.",
     "Rolfes"
    ],
    [
     "Walter F.",
     "Sendlmeier"
    ],
    [
     "Benjamin",
     "Weiss"
    ]
   ],
   "title": "A database of German emotional speech",
   "original": "i05_1517",
   "page_count": 4,
   "order": 446,
   "p1": "1517",
   "pn": "1520",
   "abstract": [
    "The article describes a database of emotional speech. Ten actors (5 female and 5 male) simulated the emotions, producing 10 German utterances (5 short and 5 longer sentences) which could be used in everyday communication and are interpretable in all applied emotions.\n",
    "The recordings were taken in an anechoic chamber with high-quality recording equipment. In addition to the sound electro-glottograms were recorded. The speech material comprises about 800 sentences (seven emotions * ten actors * ten sentences + some second versions).\n",
    "The complete database was evaluated in a perception test regarding the recognisability of emotions and their naturalness. Utterances recognised better than 80% and judged as natural by more than 60% of the listeners were phonetically labelled in a narrow transcription with special markers for voice-quality, phonatory and articulatory settings and articulatory features.\n",
    "The database can be accessed by the public via the internet (http://www.expressive-speech.net/emodb/).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-446"
  },
  "mareuil05_interspeech": {
   "authors": [
    [
     "Philippe Boula de",
     "Mareuil"
    ],
    [
     "Christophe",
     "d'Alessandro"
    ],
    [
     "Gerard",
     "Bailly"
    ],
    [
     "Frederic",
     "Bechet"
    ],
    [
     "Marie-Neige",
     "Garcia"
    ],
    [
     "Michel",
     "Morel"
    ],
    [
     "Romain",
     "Prudon"
    ],
    [
     "Jean",
     "Veronis"
    ]
   ],
   "title": "Evaluating the pronunciation of proper names by four French grapheme-to-phoneme converters",
   "original": "i05_1521",
   "page_count": 4,
   "order": 447,
   "p1": "1521",
   "pn": "1524",
   "abstract": [
    "This article reports on the results of a cooperative evaluation of grapheme-to-phoneme (GP) conversion for proper names in French. This work was carried out within the framework of a general evaluation campaign of various speech and language processing devices, including text-to-speech synthesis. The corpus and the methodology are described. The results of 4 systems are analysed: with 12-20% word error rates on a list of 8,000 proper names, they give a fairly accurate picture of the progress achieved, the stateof- the-art and the problems still to be solved, in the domain of GP conversion in French. In addition, the resources and collected data will be made available to the scientific and industrial community, in order to be re-used in future benchmarks.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-447"
  },
  "jurcicek05_interspeech": {
   "authors": [
    [
     "Filip",
     "Jurcicek"
    ],
    [
     "Jiri",
     "Zahradil"
    ],
    [
     "Libor",
     "Jelinek"
    ]
   ],
   "title": "A human-human train timetable dialogue corpus",
   "original": "i05_1525",
   "page_count": 4,
   "order": 448,
   "p1": "1525",
   "pn": "1528",
   "abstract": [
    "This paper describes progress in a development of the humanhuman dialogue corpus. The corpus contains transcribed user's phone calls to a train timetable information center. The phone calls consist of inquiries regarding their train traveler's plans. The corpus is based on dialogues's transcription of user's inquiries that were previously collected for a train timetable information center. We enriched this transcription by dialogue act tags. The dialogue act tags comprehend abstract semantic annotation. The corpus comprises a recorded speech of both operators and users, orthographic transcription, normalized transcription, normalized transcription with named entities, and dialogue act tags with abstract semantic annotation. A combination of a dialogue act tagset and a abstract semantic annotation is proposed. A technique of dialogue act tagging and abstract semantic annotation is described and used.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-448"
  },
  "branco05_interspeech": {
   "authors": [
    [
     "Gloria",
     "Branco"
    ],
    [
     "Luis",
     "Almeida"
    ],
    [
     "Rui",
     "Gomes"
    ],
    [
     "Nuno",
     "Beires"
    ]
   ],
   "title": "A Portuguese spoken and multi-modal dialog corpora",
   "original": "i05_1529",
   "page_count": 4,
   "order": 449,
   "p1": "1529",
   "pn": "1532",
   "abstract": [
    "This paper presents an overview of the spoken and multimodal dialog Portuguese corpora collected in the context of the FASiL (Flexible and Adaptive Spoken Language and Multi-Modal Interfaces) project. The project developed a Virtual Personal Assistant application in the Personal Information Management domain, exploiting the state-of-the-art of speech and multi-modal technology. The FASiL corpora were collected to obtain acoustic and language modelling data for the spoken language components, to study adaptative dialogue management techniques and to support multi-modal interface research tasks. As spoken and multi-modal resources availability is key to the research, development and acceptability of new multi-modal services and applications the corpora will be available to interested parties through ELRA.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-449"
  },
  "chan05b_interspeech": {
   "authors": [
    [
     "Joyce Y. C.",
     "Chan"
    ],
    [
     "P. C.",
     "Ching"
    ],
    [
     "Tan",
     "Lee"
    ]
   ],
   "title": "Development of a Cantonese-English code-mixing speech corpus",
   "original": "i05_1533",
   "page_count": 4,
   "order": 450,
   "p1": "1533",
   "pn": "1536",
   "abstract": [
    "This paper describes the design and compilation of the CUMIX Cantonese-English code-mixing speech corpus. Code-mixing is a common phenomenon in many bilingual societies and it usually involves at least two different languages within one utterance. In Hong Kong, people usually mix English words and phrases with Cantonese in their daily conversation. Although there are many monolingual corpora of Cantonese and English, code-mixing speech database of these two languages is not available. The aim of developing this corpus is to study of the effect of Cantonese accents in English, the design of effective language boundary detection algorithm in code-mixing utterances [1], and evaluation of the performance of code-mixing speech recognizers.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-450"
  },
  "zgank05_interspeech": {
   "authors": [
    [
     "Andrej",
     "Zgank"
    ],
    [
     "Darinka",
     "Verdonik"
    ],
    [
     "Aleksandra Zögling",
     "Markus"
    ],
    [
     "Zdravko",
     "Kacic"
    ]
   ],
   "title": "BNSI Slovenian broadcast news database - speech and text corpus",
   "original": "i05_1537",
   "page_count": 4,
   "order": 451,
   "p1": "1537",
   "pn": "1540",
   "abstract": [
    "This paper presents the BNSI Slovenian Broadcast News database project. The result of the project is a database with speech and text corpus oriented toward large vocabulary continuous speech recognition in general domain. The speech corpus consists of 36 hours of transcribed evening and late night news. The raw database material was captured in the archive of national broadcaster RTV Slovenia that was a partner in the project. General Broadcast News transcription conventions were supplemented with language specific rules. The Transcriber tool was used to produce the transcriptions. All additional tools needed during the annotation process were also installed on a computer. Statistics of speech corpus is presented in the paper. The BNSI text corpus is generated from broadcasts' scenarios for a period of 7 years. 600 monthly shows' collections of text are included. They will be used to improve the language modeling in highly inflectional Slovenian language. The BNSI Slovenian Broadcast News database will be available through ELRA/ELDA.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-451"
  },
  "volin05_interspeech": {
   "authors": [
    [
     "Jan",
     "Volín"
    ],
    [
     "Radek",
     "Skarnitzl"
    ],
    [
     "Petr",
     "Pollák"
    ]
   ],
   "title": "Confronting HMM-based phone labelling with human evaluation of speech production",
   "original": "i05_1541",
   "page_count": 4,
   "order": 452,
   "p1": "1541",
   "pn": "1544",
   "abstract": [
    "The paper presents outcomes of an experimental study in which various modes of HMM labelling were tested on two groups of Czech speakers differing in the quality of their performance. Apart from the highest precision for the given speaking style - in this case read connected speech - we were also looking for indications that the HMM labeller might function differently for the group of good speakers and for the group of poor speakers. It turned out that impressionistically evaluated good and poor speakers were, at least in some of the modes, labelled with a different degree of precision.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-452"
  },
  "strassel05_interspeech": {
   "authors": [
    [
     "Stephanie",
     "Strassel"
    ],
    [
     "Jáchym",
     "Kolár"
    ],
    [
     "Zhiyi",
     "Song"
    ],
    [
     "Leila",
     "Barclay"
    ],
    [
     "Meghan",
     "Glenn"
    ]
   ],
   "title": "Structural metadata annotation: moving beyond English",
   "original": "i05_1545",
   "page_count": 4,
   "order": 453,
   "p1": "1545",
   "pn": "1548",
   "abstract": [
    "The goal of metadata extraction (MDE) is to enable technology that can take raw speech-to-text output and refine it into forms that are more useful to humans and to downstream automatic processes. Starting in 2003, a structural metadata annotation task was defined for English as part of the DARPA EARS Program. A significant new challenge for MDE is the addition of new languages. This paper reports on work undertaken to apply MDE annotation to data from three very different languages: Mandarin Chinese, Levantine Arabic, and conversational Czech. Details of annotation task modifications are provided for each language; along with a general overview of data and annotation tools for non-English MDE.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-453"
  },
  "charlet05_interspeech": {
   "authors": [
    [
     "Delphine",
     "Charlet"
    ],
    [
     "Sacha",
     "Krstulovic"
    ],
    [
     "Frédéric",
     "Bimbot"
    ],
    [
     "Olivier",
     "Boëffard"
    ],
    [
     "Dominique",
     "Fohr"
    ],
    [
     "Odile",
     "Mella"
    ],
    [
     "Filip",
     "Korkmazsky"
    ],
    [
     "Djamel",
     "Mostefa"
    ],
    [
     "Khalid",
     "Choukri"
    ],
    [
     "Arnaud",
     "Vallée"
    ]
   ],
   "title": "Neologos: an optimized database for the development of new speech processing algorithms",
   "original": "i05_1549",
   "page_count": 4,
   "order": 454,
   "p1": "1549",
   "pn": "1552",
   "abstract": [
    "The Neologos project is a speech database creation project for the French language, resulting from a collaboration between universities and industrial companies and supported by the French Ministry of Research. The goal of Neologos is to re-think the design of the speech databases in order to enable the development of new algorithms in the field of speech processing. A general method is proposed to optimize the database contents in terms of diversity of the recorded voices, while reducing the number of recorded speakers.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-454"
  },
  "lin05b_interspeech": {
   "authors": [
    [
     "Cheng-Yuan",
     "Lin"
    ],
    [
     "Kuan-Ting",
     "Chen"
    ],
    [
     "J.-S. Roger",
     "Jang"
    ]
   ],
   "title": "A hybrid approach to automatic segmentation and labeling for Mandarin Chinese speech corpus",
   "original": "i05_1553",
   "page_count": 4,
   "order": 455,
   "p1": "1553",
   "pn": "1556",
   "abstract": [
    "In this paper, we propose a hybrid approach to refine the phonetic boundaries in a Mandarin speech corpus. This approach employs different sets of acoustic features for different categories of phonetic transitions, except for the most difficult case of \"periodic voiced + periodic voiced\", which is therefore handled by a heuristic scheme. Several experiments are designed to demonstrate the feasibility of the proposed approach.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-455"
  },
  "chiang05_interspeech": {
   "authors": [
    [
     "Yuang-Chin",
     "Chiang"
    ],
    [
     "Min-Siong",
     "Liang"
    ],
    [
     "Hong-Yi",
     "Lin"
    ],
    [
     "Ren-Yuan",
     "Lyu"
    ]
   ],
   "title": "The multiple pronunciations in Taiwanese and the automatic transcription of Buddhist sutra with augmented read speech",
   "original": "i05_1557",
   "page_count": 4,
   "order": 456,
   "p1": "1557",
   "pn": "1560",
   "abstract": [
    "Collection of Taiwanese text corpus with phonetic transcription suffers from the problems of multiple pronunciation, or pronunciation variation. By further augmenting the text with read speech, and using automatic speech recognition with a sausage searching net constructed from the multiple pronunciations of the text corresponding to its speech utterance, we are able to reduce the effort for phonetic transcription. Compared to general method for pronunciation variation such as the relabeling of training corpus of [1], the sausage searching net shows advantages. Two experiments are conducted using a Taiwanese Buddhist Sutra speech and text corpus.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-456"
  },
  "davel05_interspeech": {
   "authors": [
    [
     "Marelie",
     "Davel"
    ],
    [
     "Etienne",
     "Barnard"
    ]
   ],
   "title": "Bootstrapping pronunciation dictionaries: practical issues",
   "original": "i05_1561",
   "page_count": 4,
   "order": 457,
   "p1": "1561",
   "pn": "1564",
   "abstract": [
    "Bootstrapping techniques are an efficient way to develop electronic pronunciation dictionaries [1, 2], but require fast system response to be practical for medium-to-large lexicons. In addition, user errors are inevitable during this process, and it is useful if automatic means can be used to assist in the search for such errors. We describe how the Default&Refine grapheme-to-phoneme rule extraction algorithm [3] can be adapted to meet both of these goals. Experimental results demonstrate the utility of these methods.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-457"
  },
  "ward05_interspeech": {
   "authors": [
    [
     "Nigel G.",
     "Ward"
    ],
    [
     "Anais G.",
     "Rivera"
    ],
    [
     "Karen",
     "Ward"
    ],
    [
     "David G.",
     "Novick"
    ]
   ],
   "title": "Root causes of lost time and user stress in a simple dialog system",
   "original": "i05_1565",
   "page_count": 4,
   "order": 458,
   "p1": "1565",
   "pn": "1568",
   "abstract": [
    "As a priority-setting exercise, we compared interactions between users and a simple spoken dialog system to interactions between users and a human operator. We observed usability events, places in which system behavior differed from human behavior, and for each we noted the impact, root causes, and prospects for improvement. We suggest some priority issues for research, involving not only such core areas as speech recognition and synthesis and language understanding and generation, but also less-studied topics such as adaptive or flexible timeouts, turn-taking and speaking rate.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-458"
  },
  "parisi05_interspeech": {
   "authors": [
    [
     "Julie A.",
     "Parisi"
    ],
    [
     "Douglas S.",
     "Brungart"
    ]
   ],
   "title": "Evaluating communication effectiveness in team collaboration",
   "original": "i05_1569",
   "page_count": 4,
   "order": 459,
   "p1": "1569",
   "pn": "1572",
   "abstract": [
    "Effective team communication is essential for mission success in many complex operational environments. Whereas most previous research has focused primarily on the psychological factors that can cause breakdowns in team communication, this paper introduces a novel procedure designed to systematically evaluate the impact that a degraded communication transmission channel can have on team performance. This procedure measured communication effectiveness in terms of the time necessary for the team to reach a consensus about the proper ordering of unfamiliar images that were viewed in randomized order. Preliminary results indicate that this procedure can identify differences in communication channel quality that would not be detected by traditional evaluation procedures based solely on point-to-point intelligibility.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-459"
  },
  "conejero05_interspeech": {
   "authors": [
    [
     "David",
     "Conejero"
    ],
    [
     "Alan",
     "Lounds"
    ],
    [
     "Carmen Garcia",
     "Mateo"
    ],
    [
     "Leandro",
     "Rodriguez-Linares"
    ],
    [
     "Raquel",
     "Mochales"
    ],
    [
     "Asuncion",
     "Moreno"
    ]
   ],
   "title": "Bilingual aligned corpora for speech to speech translation for Spanish, English and Catalan",
   "original": "i05_1573",
   "page_count": 4,
   "order": 460,
   "p1": "1573",
   "pn": "1576",
   "abstract": [
    "In the framework of the EU-funded Project LC-STAR, a set of Language Resources (LR) for all the Speech to Speech Translation components (Speech recognition, Machine Translation and Speech Synthesis) was developed. This paper deals with the development of bilingual corpora in Spanish, US English and Catalan. The corpora were obtained from spontaneous dialogues in one of these three languages which were translated to the other two languages. The paper describes the translation methodology, specific problems of translating spontaneous dialogues to be used for MT training, formats and the validation criteria.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-460"
  },
  "boril05_interspeech": {
   "authors": [
    [
     "Hynek",
     "Boril"
    ],
    [
     "Petr",
     "Pollak"
    ]
   ],
   "title": "Design and collection of Czech Lombard speech database",
   "original": "i05_1577",
   "page_count": 4,
   "order": 461,
   "p1": "1577",
   "pn": "1580",
   "abstract": [
    "In this paper, design, collection and parameters of newly proposed Czech Lombard Speech Database (CLSD) are presented. The database focuses on analysis and modeling of Lombard effect to achieve robust speech recognition improvement. The CLSD consists of neutral speech and speech produced in various types of simulated noisy background. In comparison to available databases dealing with Lombard effect, an extensive set of utterances containing phonetically rich words and sentences was chosen to cover the whole phoneme vocabulary of the language. For the purposes of Lombard speech recording, usual 'noisy headphones configuration' was improved by addition of an operator qualifying utterance intelligibility while hearing the same noise mixed with speaker's voice of intensity lowered according to the selected virtual distance. This scenario motivated speakers to react more to the noise background. The CLSD currently consists of 26 speakers.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-461"
  },
  "kazemzadeh05_interspeech": {
   "authors": [
    [
     "Abe",
     "Kazemzadeh"
    ],
    [
     "Hong",
     "You"
    ],
    [
     "Markus",
     "Iseli"
    ],
    [
     "Barbara",
     "Jones"
    ],
    [
     "Xiaodong",
     "Cui"
    ],
    [
     "Margaret",
     "Heritage"
    ],
    [
     "Patti",
     "Price"
    ],
    [
     "Elaine",
     "Anderson"
    ],
    [
     "Shrikanth",
     "Narayanan"
    ],
    [
     "Abeer",
     "Alwan"
    ]
   ],
   "title": "TBALL data collection: the making of a young children's speech corpus",
   "original": "i05_1581",
   "page_count": 4,
   "order": 462,
   "p1": "1581",
   "pn": "1584",
   "abstract": [
    "In this paper we describe the data collection for the TBALL project (Technology Based Assessment of Language and Literacy) and report the results of our efforts. We focus on aspects of our corpus that distinguish it from currently available corpora. The speakers are children (grades K-4), largely non-native speakers of English, and from diverse socio-economic backgrounds, who are learning to read. We also describe how we adapted our methodology to accommodate these differences: our recording setup, data collection methodology, and transcription scheme. We also discuss the task this corpus was designed to serve and our research approach.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-462"
  },
  "tohyama05_interspeech": {
   "authors": [
    [
     "Hitomi",
     "Tohyama"
    ],
    [
     "Shigeki",
     "Matsubara"
    ],
    [
     "Nobuo",
     "Kawaguchi"
    ],
    [
     "Yasuyoshi",
     "Inagaki"
    ]
   ],
   "title": "Construction and utilization of bilingual speech corpus for simultaneous machine interpretation research",
   "original": "i05_1585",
   "page_count": 4,
   "order": 463,
   "p1": "1585",
   "pn": "1588",
   "abstract": [
    "This paper describes the design, analysis and utilization of a simultaneous interpretation corpus. The corpus has been constructed at the Center for Integrated Acoustic Information Research (CIAIR) of Nagoya University in order to promote the realization of the multi-lingual communication supporting environment. The size of transcribed data is about 1 million words, and the corpus would deserve to be called the simultaneous interpretation corpus of the largest-in-the-world class. The discourse tag and the utterance time tag were given to the corpus, and some software tools for corpus analysis in order to support the practical use of the corpus have been developed. Therefore, the corpus is expected to be useful not only for the development of simultaneous interpreting systems but also for the construction of an interpreting theory.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-463"
  },
  "bates05_interspeech": {
   "authors": [
    [
     "Rebecca",
     "Bates"
    ],
    [
     "Patrick",
     "Menning"
    ],
    [
     "Elizabeth",
     "Willingham"
    ],
    [
     "Chad",
     "Kuyper"
    ]
   ],
   "title": "Meeting acts: a labeling system for group interaction in meetings",
   "original": "i05_1589",
   "page_count": 4,
   "order": 464,
   "p1": "1589",
   "pn": "1592",
   "abstract": [
    "We describe a new system for labeling speech corpora with highlevel group interaction tags, called \"meeting acts.\" The system was motivated by a need to assess work seeking to automatically detect meeting style using dialog act information. We present information about the relationships seen between dialog act sequences and meeting style to motivate the labeling process. We provide a summary of the annotation system and labeling procedure, as well as preliminary inter-annotator reliability statistics on the ICSI Meeting Recorder Corpus.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-464"
  },
  "silaghi05_interspeech": {
   "authors": [
    [
     "Marius C.",
     "Silaghi"
    ],
    [
     "Rachna",
     "Vargiya"
    ]
   ],
   "title": "A new evaluation criteria for keyword spotting techniques and a new algorithm",
   "original": "i05_1593",
   "page_count": 4,
   "order": 465,
   "p1": "1593",
   "pn": "1596",
   "abstract": [
    "Keyword spotting is an efficient approach for search of relevant recordings in databases of recorded unconstrained speech. Many algorithms have been proposed in the past for this problem and several techniques claim to be very efficient and accurate. Researchers have so far attempted to correctly compare their results by using standardized Receiver Operating Characteristic (ROC) curves, and performing experiments on publicly available databases with known keywords.\n",
    "However, when it comes to compare the expected behavior of a technique for new keywords and utterances, the generalization of published comparisons is not very clear, and the choice of the benchmark-keywords has considerable effects on the comparison. In this paper we propose a new measure of the accuracy of a keyword spotter, removing the benchmark-keywords selection bias and offering a qualitative estimation of how well the technique is expected to perform on new keywords.\n",
    "We apply our evaluation scheme to compare previously known algorithms as well as a new technique that we propose now. The new technique is based on a confidence measure that evaluates a keyword match to the worst of its phoneme scores (where the score of a phoneme is taken as the ratio between the log probability of that phoneme and the length of the phoneme). It is remarkable that the newly proposed technique can detect all occurences of 100 keywords with less than .5 false alarms/keyword/hour.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-465"
  },
  "draxler05_interspeech": {
   "authors": [
    [
     "Christoph",
     "Draxler"
    ],
    [
     "Alexander",
     "Steffen"
    ]
   ],
   "title": "Phattsessionz: recording 1000 adolescent speakers in schools in Germany",
   "original": "i05_1597",
   "page_count": 4,
   "order": 466,
   "p1": "1597",
   "pn": "1600",
   "abstract": [
    "PhattSessionz is a distributed speech data collection to create a regionally balanced speech database of more than 1000 adolescent speakers in Germany. Recordings are planned in more than 35 secondary schools all over Germany. The recording equipment consists of a headset and a table microphone connected to the PC via a USB audio device with a signal quality of 22.05 KHz 16 bit. All recordings are immediately uploaded via the WWW to the PhattSessionz server at the BAS. This server also provides the prompt sheets which are customized for each recording session. The local organization of the recordings, e.g. speaker recruitment and recording schedule, is delegated to the participating schools.\n",
    "A number of technical issues arose during the first experimental recordings in Munich schools. These problems have been solved now and production recordings have started.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-466"
  },
  "abate05_interspeech": {
   "authors": [
    [
     "Solomon Teferra",
     "Abate"
    ],
    [
     "Wolfgang",
     "Menzel"
    ],
    [
     "Bairu",
     "Tafila"
    ]
   ],
   "title": "An Amharic speech corpus for large vocabulary continuous speech recognition",
   "original": "i05_1601",
   "page_count": 4,
   "order": 467,
   "p1": "1601",
   "pn": "1604",
   "abstract": [
    "Amharic is the official language of Ethiopia. It belongs to the Semitic language family and is characterized by a quite homogeneous phonology distinguishing between 234 distinct Consonant-Vowel (CV) syllables.\n",
    "Since there is no Amharic speech corpus of any kind, we developed a read-speech corpus using a phonetically rich and balanced text database. To prepare the text database, we used the archive of EthioZena website which consists of selected articles from well known newspapers and magazines published in Amharic. The archive was cleaned semi-automatically.\n",
    "Like other standard speech corpora, such as WSJCAM0, the Amharic speech corpus contains training set, speaker adaptation set, test sets (development and evaluation test sets each with 5000 and 20000 vocabulary size). The speech has been recorded in Ethiopia in an office environment and segmented semi-automatically. The corpus is now used for experiments with a syllable- and phonebased LVCSR for Amharic.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-467"
  },
  "dolfing05_interspeech": {
   "authors": [
    [
     "Hans",
     "Dolfing"
    ],
    [
     "David",
     "Reitter"
    ],
    [
     "Luís",
     "Almeida"
    ],
    [
     "Nuno",
     "Beires"
    ],
    [
     "Michael",
     "Cody"
    ],
    [
     "Rui",
     "Gomes"
    ],
    [
     "Kerry",
     "Robinson"
    ],
    [
     "Roman",
     "Zielinski"
    ]
   ],
   "title": "The FASil speech and multimodal corpora",
   "original": "i05_1605",
   "page_count": 4,
   "order": 468,
   "p1": "1605",
   "pn": "1608",
   "abstract": [
    "In the context of the FASiL project, we have studied natural language interactions in a unimodal (speech only) and multimodal (speech and graphics) interface to a personal information management database. We collected multilingual corpora to investigate these interactions in Portuguese, English and Swedish. The corpora are used to train language models, to update acoustic models, to study semantic concepts, multimodal interactions, and dialogue management strategies. The corpora are annotated in a uniform way, with timings, transcriptions, and semantics. We report on the structure and design of the corpora which are now available via ELRA.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-468"
  },
  "muller05_interspeech": {
   "authors": [
    [
     "Karin",
     "Müller"
    ]
   ],
   "title": "Revealing phonological similarities between German and dutch",
   "original": "i05_1609",
   "page_count": 4,
   "order": 469,
   "p1": "1609",
   "pn": "1612",
   "abstract": [
    "In this paper, we present an approach to automatically revealing phonological classes within historically related languages. A newly created bilingual German-Dutch pronunciation dictionary is used for learning phonological similarities between the onsets, nuclei and codas of these two languages via EM-based clustering. Our evaluation is twofold: we apply the models to predict from a German word the phonemes of a Dutch cognate. The results show that it is harder to predict the pronunciation of the nucleus and the coda than the onset. We also evaluate our approach qualitatively, finding meaningful classes caused by historical sound changes.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-469"
  },
  "ishizuka05_interspeech": {
   "authors": [
    [
     "Kentaro",
     "Ishizuka"
    ],
    [
     "Ryoko",
     "Mugitani"
    ],
    [
     "Hiroko",
     "Kato"
    ],
    [
     "Shigeaki",
     "Amano"
    ]
   ],
   "title": "A longitudinal analysis of the spectral peaks of vowels for a Japanese infant",
   "original": "i05_1169",
   "page_count": 4,
   "order": 470,
   "p1": "1169",
   "pn": "1172",
   "abstract": [
    "This paper describes a longitudinal analysis of the vowel development of a Japanese female infant between the ages of 4 and 60 months. The speech data were natural spontaneous speech recorded for at least an hour per month. Vowels were extracted from the data according to their phoneme labels by two transcribers. The two lower spectral peaks of the vowels were estimated by linear predictive coding method. Analyses of the mean values of the spectral peaks suggest that the values change with age according to the articulation positions of the tongue and the rapid development of the vocal tract. The values change greatly until the age of 18 months. In addition, the distribution of the two spectral peaks for individual vowels shows that the vowel space expands with age, and this also continues until the age of 18 months. Another investigation of the vowel distributions based on discriminant analysis confirms that the differentiation of the spectral peak distributions also continues until around the age 18 months in terms of a physical phenomenon. These results suggest that the infant becomes able to produce discriminable vowels corresponding to the rapid increase in the vocal tract length at around that age.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-470"
  },
  "zajdo05_interspeech": {
   "authors": [
    [
     "Krisztina",
     "Zajdó"
    ],
    [
     "Jeannette M. van der",
     "Stelt"
    ],
    [
     "Ton G.",
     "Wempe"
    ],
    [
     "Louis C. W.",
     "Pols"
    ]
   ],
   "title": "Cross-linguistic comparison of two-year-old children's acoustic vowel spaces: contrasting Hungarian with dutch",
   "original": "i05_1173",
   "page_count": 4,
   "order": 471,
   "p1": "1173",
   "pn": "1176",
   "abstract": [
    "Traditional hand-edited formant measurements may result in biased assessment of vowel formants in children's speech. Therefore, vowel spaces that are constructed by hand-edited formant measures may be unreliable. The recent development of an automated frequency domain analysis method allows for more reliable measurements. Thus, a valid comparison of the size and positioning of young children's vowel spaces across languages can be achieved. Contrasting the extension of the vowel space utilized by young children acquiring Hungarian and Dutch can provide information pertaining to a) children's abilities to explore the vowel space and b) potential cross-linguistic differences in exploiting the potentially available acoustic vowel space. Since a unified theory of vowel acquisition has never been developed, it is hoped that the new method will contribute to the creation of such a theory by comparing and contrasting results from diverse languages. Results suggest that two-year-old Hungarian- and Dutch-speaking children utilize the vowel space language-specifically, by exploiting different regions within the potentially available acoustic space.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-471"
  },
  "lintfert05b_interspeech": {
   "authors": [
    [
     "Britta",
     "Lintfert"
    ],
    [
     "Katrin",
     "Schneider"
    ]
   ],
   "title": "Acoustic correlates of contrastive stress in German children",
   "original": "i05_1177",
   "page_count": 4,
   "order": 472,
   "p1": "1177",
   "pn": "1180",
   "abstract": [
    "This study examines the acoustic correlates of stress in children's productions of bisyllabic and trisyllabic words, differing in stress placement. Acoustic results of the data of two children aged 4;11 to 6;1 are reported as a part of a more comprehensive study on the acquisition of stress in German. In contrast to recent findings that infants show an early preference in perception for rhythmic patterns in their own language, contrastive stress is supposed to be acquired quite late. Sufficient input must be available to the infants for building up representations of contrastive stress. We found that children around the age of five are able to perceive contrastive word stress but that they may differ in their production from one another. For German, vowel length is the most reliable correlate of word stress in both childrens' utterances. Intensity as well as the voice quality parameters glottal opening and spectral tilt, including rate of closure and skewness, tend to be used adult-like.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-472"
  },
  "salvi05_interspeech": {
   "authors": [
    [
     "Giampiero",
     "Salvi"
    ]
   ],
   "title": "Ecological language acquisition via incremental model-based clustering",
   "original": "i05_1181",
   "page_count": 4,
   "order": 473,
   "p1": "1181",
   "pn": "1184",
   "abstract": [
    "We analyse the behaviour of Incremental Model-Based Clustering on child-directed speech data, and suggest a possible use of this method to describe the acquisition of phonetic classes by an infant. The effects of two factors are analysed, namely the number of coefficients describing the speech signal, and the frame length of the incremental clustering procedure. The results show that, although the number of predicted clusters vary in different conditions, the classifications obtained are essentially consistent. Different classifications were compared using the variation of information measure.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-473"
  },
  "sudo05_interspeech": {
   "authors": [
    [
     "Tamami",
     "Sudo"
    ],
    [
     "Ken",
     "Mogi"
    ]
   ],
   "title": "Perceptual and linguistic category formation in infants",
   "original": "i05_1185",
   "page_count": 4,
   "order": 474,
   "p1": "1185",
   "pn": "1188",
   "abstract": [
    "It is known that infants are aware of the category of objects in daily life before explicit vocabulary acquisition starts. It has been suggested that the ability of categorization is likely to play an essential role in its overall cognitive development. In this study, we conducted a series of experiments using stimuli that have various attributes in order to understand the process of such a categorization in an infant's pre-linguistic development. We consider two possible cognitive processes of linguistic categorization. One possible scenario is that certain members of the categories are derived from similarity with the most representative members of a category (prototypical members). Alternatively, the coupling of each category member- ship with relevant labels can be traced to the symbol grounding problem. Our results are consistent with an interpretation that the cortical systems involved in language development is inseparably linked with prototype effects in categorization.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-474"
  },
  "dusan05_interspeech": {
   "authors": [
    [
     "Sorin",
     "Dusan"
    ],
    [
     "Larry R.",
     "Rabiner"
    ]
   ],
   "title": "On integrating insights from human speech perception into automatic speech recognition",
   "original": "i05_1233",
   "page_count": 4,
   "order": 475,
   "p1": "1233",
   "pn": "1236",
   "abstract": [
    "In spite of the effort and progress made during the last few decades, the performance of automatic speech recognition (ASR) systems still lags far behind that achieved by humans. Some researchers think that more speech data will be sufficient in order to bridge this performance gap. Others think that radical modifications to the current methods need to be made, and possible inspirations for these modifications should come from human speech perception (HSP). This paper focuses on two issues: first, it presents a comparison between HSP and ASR emphasizing some insights from HSP that could still be applied in ASR; second, it presents some ideas for extracting useful non-linguistic information from the speech signal, the so called rich transcription', which could help in selecting specialized acoustic-linguistic models that offer higher accuracy than the general models.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-475"
  },
  "scharenborg05_interspeech": {
   "authors": [
    [
     "Odette",
     "Scharenborg"
    ]
   ],
   "title": "Parallels between HSR and ASR: how ASR can contribute to HSR",
   "original": "i05_1237",
   "page_count": 4,
   "order": 476,
   "p1": "1237",
   "pn": "1240",
   "abstract": [
    "In this paper, we illustrate the close parallels between the research fields of human speech recognition (HSR) and automatic speech recognition (ASR) using a computational model of human word recognition, SpeM, which was built using techniques from ASR. We show that ASR has proven to be useful for improving models of HSR by relieving them of some of their shortcomings. However, in order to build an integrated computational model of all aspects of HSR, a lot of issues remain to be resolved. In this process, ASR algorithms and techniques definitely can play an important role.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-476"
  },
  "bosch05b_interspeech": {
   "authors": [
    [
     "Louis ten",
     "Bosch"
    ],
    [
     "Odette",
     "Scharenborg"
    ]
   ],
   "title": "ASR decoding in a computational model of human word recognition",
   "original": "i05_1241",
   "page_count": 4,
   "order": 477,
   "p1": "1241",
   "pn": "1244",
   "abstract": [
    "This paper investigates the interaction between acoustic scores and symbolic mismatch penalties in multi-pass speech decoding techniques that are based on the creation of a segment graph followed by a lexical search. The interaction between acoustic and symbolic mismatches determines to a large extent the structure of the search space of these multi-pass approaches. The background of this study is a recently developed computational model of human word recognition, called SpeM. SpeM is able to simulate human word recognition data and is built as a multi-pass speech decoder. Here, we focus on unravelling the structure of the search space that is used in SpeM and similar decoding strategies. Finally, we elaborate on the close relation between distances in this search space, and distance measures in search spaces that are based on a combination of acoustic and phonetic features.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-477"
  },
  "maier05_interspeech": {
   "authors": [
    [
     "Viktoria",
     "Maier"
    ],
    [
     "Roger K.",
     "Moore"
    ]
   ],
   "title": "An investigation into a simulation of episodic memory for automatic speech recognition",
   "original": "i05_1245",
   "page_count": 4,
   "order": 478,
   "p1": "1245",
   "pn": "1248",
   "abstract": [
    "This paper investigates a simulation of episodic memory known in the literature as minerva 2'. Minerva 2 is a computational multiple-trace memory model that successfully predicts basic findings from the schema-abstraction literature. This model has been implemented and tested on a simple ASR task using vowel formant data taken from the Peterson & Barney database. Recogni- tion results are compared to a number of state-of-the-art pattern classifiers, and it is shown that the episodic model achieves the best performance.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-478"
  },
  "foslerlussier05_interspeech": {
   "authors": [
    [
     "Eric",
     "Fosler-Lussier"
    ],
    [
     "C. Anton",
     "Rytting"
    ],
    [
     "Soundararajan",
     "Srinivasan"
    ]
   ],
   "title": "Phonetic ignorance is bliss: investigating the effects of phonetic information reduction on ASR performance",
   "original": "i05_1249",
   "page_count": 4,
   "order": 479,
   "p1": "1249",
   "pn": "1252",
   "abstract": [
    "Perception studies have long argued that phonetic confusions are more likely to happen across some phonetic features than other (e.g., place of articulation rather than manner) [1]. Similarly, we and others have noted that pronunciation variation occurs more frequently in unstressed syllables, and in syllable codas. This suggests that a phonetic information structure is at play, where for decoding purposes it is important to get phonetic information accurate in stressed syllables, but less so in unstressed syllables. In this work, we explore the role of phonetic information in clean and noisy speech by reducing the phonetic information available to the recognizer. A surprising result is that replacing some phones with manner classes in the dictionary improves recognition in one noise condition.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-479"
  },
  "holmberg05_interspeech": {
   "authors": [
    [
     "Marcus",
     "Holmberg"
    ],
    [
     "David",
     "Gelbart"
    ],
    [
     "Ulrich",
     "Ramacher"
    ],
    [
     "Werner",
     "Hemmert"
    ]
   ],
   "title": "Automatic speech recognition with neural spike trains",
   "original": "i05_1253",
   "page_count": 4,
   "order": 480,
   "p1": "1253",
   "pn": "1256",
   "abstract": [
    "A major difference between the human auditory system and automatic speech recognition (ASR) lies in their representation of sound signals: whereas ASR uses a smoothed low-dimensional temporal and spectral representation of sound signals, our hearing system relies on extremely high-dimensional but temporally sparse spike trains. A strength of the latter representation is in the inherent coding of time, which is exploited by neuronal networks along the auditory pathway. We demonstrate ASR results using features purely derived from simulated spike trains of auditory nerve fibers (ANF) and a layer of octopus neurons. Octopus neurons located in the cochlear nucleus are known for their distinct temporal processing: they not only reject steady-state excitation and fire on signal onsets but also enhance the amplitude modulations of voiced speech. With multi-condition training we do not reach the performance of conventional mel-frequency cepstral coefficients (MFCC) features. With clean training however, our spike-based features performed similarly to MFCCs. Further, recognition scores in noise were improved when features derived from ANFs, which mainly represent spectral characteristics of speech signals, were combined with features derived from spike trains of octopus neurons. This result is promising given the relatively small number of neurons we used and the limitations in how the auditory model was interfaced to the ASR back end.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-480"
  },
  "carey05_interspeech": {
   "authors": [
    [
     "Michael J.",
     "Carey"
    ],
    [
     "Tuan P.",
     "Quang"
    ]
   ],
   "title": "A speech similarity distance weighting for robust recognition",
   "original": "i05_1257",
   "page_count": 4,
   "order": 481,
   "p1": "1257",
   "pn": "1260",
   "abstract": [
    "Human listeners not only perform better than machine recognizers at the same signal to noise ratio but are also able to deal with the problems presented by non-stationary noise better. In this paper we describe a series of experiments in which the human ability to select clean segments of speech from a noisy environment is emulated by a machine recogniser. We show that a vector quantiser that incorporates speaker specific information can be used to estimate the similarity between the input signal and speech vector in a codebook and so produce a probabilistic weighting for the distances used in pattern matching. The performance of the probabilistic system is slightly worse with stationary noise than a system using noise-matched models however it is better on non-stationary noise although the matched system has knowledge of the background noise conditions while the probabilistic system has no information about the noise.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-481"
  },
  "murakami05_interspeech": {
   "authors": [
    [
     "Takao",
     "Murakami"
    ],
    [
     "Kazutaka",
     "Maruyama"
    ],
    [
     "Nobuaki",
     "Minematsu"
    ],
    [
     "Keikichi",
     "Hirose"
    ]
   ],
   "title": "Japanese vowel recognition based on structural representation of speech",
   "original": "i05_1261",
   "page_count": 4,
   "order": 482,
   "p1": "1261",
   "pn": "1264",
   "abstract": [
    "Speech acoustics varies from speaker to speaker, microphone to microphone, room to room, line to line, etc. Physically speaking, every speech sample is distorted. Socially speaking, however, speech is the easiest communication media for humans. In order to cope with the inevitable distortions, speech engineers have built HMMs with speech data of hundreds or thousands of speakers and the models are called speaker-independent models. But they often need to be adapted to the input speaker or environment and this fact claims that the speaker-independent models are not really speaker-independent. Recently, a novel acoustic representation of speech was proposed, where dimensions of the above distortions can hardly be seen. It discards every acoustic substance of speech and captures only their interrelations to represent speech acoustics structurally. The new representation can be interpreted linguistically as physical implementation of structural phonology and also psychologically as speech Gestalt. In this paper, the first recognition experiment was carried out to investigate the performance of the new representation. The results showed that the new models trained from a single speaker with no normalization can outperform the conventional models trained from 4,130 speakers with CMN.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-482"
  },
  "srinivasan05_interspeech": {
   "authors": [
    [
     "Soundararajan",
     "Srinivasan"
    ],
    [
     "DeLiang",
     "Wang"
    ]
   ],
   "title": "Modeling the perception of multitalker speech",
   "original": "i05_1265",
   "page_count": 4,
   "order": 483,
   "p1": "1265",
   "pn": "1268",
   "abstract": [
    "Listeners' ability to understand a target speaker in the presence of one or more simultaneous competing speakers is subject to two types of masking: Energetic and informational. Energetic masking occurs when target and interfering signals overlap in time and frequency resulting in portions of target becoming inaudible. Informational masking occurs when the listener is unable to segregate the target from interference, while both are audible. We present a model of multitalker speech perception that accounts for both types of masking. Human perception in the presence of energetic masking is modeled using a speech recognizer that treats the masked time-frequency units of target as missing data. The effects of informational masking on the recognizer are modeled using the output of a speech segregation system. On a systematic evaluation, the performance of the proposed model is in broad agreement with perceptual results.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-483"
  },
  "harding05_interspeech": {
   "authors": [
    [
     "Sue",
     "Harding"
    ],
    [
     "Jon",
     "Barker"
    ],
    [
     "Guy J.",
     "Brown"
    ]
   ],
   "title": "Binaural feature selection for missing data speech recognition",
   "original": "i05_1269",
   "page_count": 4,
   "order": 484,
   "p1": "1269",
   "pn": "1272",
   "abstract": [
    "The missing data' approach for robust speech recognition uses masks indicating which regions of an acoustic mixture provide reliable evidence of the target to be recognised. Binaural cues for spatial location were used to determine missing data masks for signals consisting of utterances from three concurrent male speakers in reverberant conditions, by deriving probability distributions from estimates of interaural time and level differences (ITD and ILD) for the mixed signals. In such a system, a decision must be made about whether the acoustic features used for decoding are selected from the left or right ear, or a combination of the two. Here, features were selected from the \"better ear\" (as determined by a simple heuristic) within whole time frames, or within individual time-frequency elements. A combination of left and right ear features gave better recognition performance than using either ear alone, and the best results were obtained when selecting features within individual time-frequency elements.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-484"
  },
  "wesker05_interspeech": {
   "authors": [
    [
     "Thorsten",
     "Wesker"
    ],
    [
     "Bernd",
     "Meyer"
    ],
    [
     "Kirsten",
     "Wagener"
    ],
    [
     "Jörn",
     "Anemüller"
    ],
    [
     "Alfred",
     "Mertins"
    ],
    [
     "Birger",
     "Kollmeier"
    ]
   ],
   "title": "Oldenburg logatome speech corpus (OLLO) for speech recognition experiments with humans and machines",
   "original": "i05_1273",
   "page_count": 4,
   "order": 485,
   "p1": "1273",
   "pn": "1276",
   "abstract": [
    "This paper introduces the new OLdenburg LOgatome speech corpus (OLLO) and outlines design considerations during its creation. OLLO is distinct from previous ASR corpora as it specifically targets (1) the fair comparison between human and machine speech recognition performance, and (2) the realistic representation of intrinsic variabilities in speech that are significant for automatic speech recognition (ASR) systems. To enable an unbiased human-machine comparison, OLLO is designed for recognition of individual phonemes that are embedded in logatomes, specifically, three-phoneme sequences with no semantic information. A balanced set of target-phonemes important for human and automatic speech recognition has been chosen, drawing on pilot ASR studies and cross-fertilization from the field of human speech intelligibility testing. Several intrinsic variabilities in speech are represented in OLLO, by recording from 40 speakers from four German dialect regions, and by covering six articulation characteristics. Results from preliminary phonetic time-labeling and ASR experiments are promising and consistent with corpus variabilities.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-485"
  },
  "jeon05b_interspeech": {
   "authors": [
    [
     "Je Hun",
     "Jeon"
    ],
    [
     "Minhwa",
     "Chung"
    ]
   ],
   "title": "Automatic generation of domain-dependent pronunciation lexicon with data-driven rules and rule adaptation",
   "original": "i05_1337",
   "page_count": 4,
   "order": 486,
   "p1": "1337",
   "pn": "1340",
   "abstract": [
    "In this paper, we describe a method for automatically generating a domain-dependent pronunciation lexicon using a data-driven approach. We also introduce an adaptation method to alleviate some of the errors caused by the data-driven rules which are derived from a relatively small volume of speech corpus. At first, pronunciation variation rules are extracted from a large volume of speech corpus and then are adapted using the rules derived from the target corpus. The context dependent pronunciation variants of the target lexicon are automatically generated by applying these rules to the training and language model adaptation text corpus. Then the pronunciation variants are pruned based on the likelihood of applied rules. Compared to the lexicon created by knowledge-based rules, on the Korean spontaneous speech corpus, our approach produces an absolute reduction of 0.8% of the WER. Furthermore, the size of pronunciation variants is reduced by almost 5.6% on the peak performance.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-486"
  },
  "tjalve05_interspeech": {
   "authors": [
    [
     "Michael",
     "Tjalve"
    ],
    [
     "Mark",
     "Huckvale"
    ]
   ],
   "title": "Pronunciation variation modelling using accent features",
   "original": "i05_1341",
   "page_count": 4,
   "order": 487,
   "p1": "1341",
   "pn": "1344",
   "abstract": [
    "In this paper, we propose a novel method for modelling native accented speech. As an alternative to the notion of dialect, we work with the lower level phonological components of accents, which we term accent features. This provides us with a better understanding of how pronunciation varies and it allows us to give a much more detailed picture of a person's speech.\n",
    "The accent features are included during phonological adaptation of a speaker-independent Automatic Speech Recognition system in an attempt to make it more robust when exposed to pronunciation variation thus improving recognition performance on accented speech.\n",
    "We employ a dynamic set-up in which the system first identifies the phonetic characteristics of the user's speech. It then creates a model of the speaker's phonological system and adapts the pronunciation dictionary to best match his/her speech. Recognition is subsequently carried out using the adapted pronunciation dictionary.\n",
    "Experiments on British English speech data show a significant relative improvement in error rate of 20% compared with the traditional non-adaptive method.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-487"
  },
  "truong05b_interspeech": {
   "authors": [
    [
     "Khiet P.",
     "Truong"
    ],
    [
     "Ambra",
     "Neri"
    ],
    [
     "Febe de",
     "Wet"
    ],
    [
     "Catia",
     "Cucchiarini"
    ],
    [
     "Helmer",
     "Strik"
    ]
   ],
   "title": "Automatic detection of frequent pronunciation errors made by L2-learners",
   "original": "i05_1345",
   "page_count": 4,
   "order": 488,
   "p1": "1345",
   "pn": "1348",
   "abstract": [
    "In this paper, we present an acoustic-phonetic approach to automatic pronunciation error detection. Classifiers using techniques such as Linear Discriminant Analysis and Decision Trees were developed for three sounds that are frequently pronounced incorrectly by L2-learners of Dutch: /A/, /Y/ and /x/. This paper will focus mainly on the problems with the latter phoneme. The acoustic properties of these pronunciation errors were examined so as to define a number of discriminative acoustic features to be used to train and test the classifiers. Experiments showed that the classifiers are able to discriminate correct sounds from incorrect sounds in both native and non-native speech, and therefore can be used to detect pronunciation errors in non-native speech.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-488"
  },
  "psutka05_interspeech": {
   "authors": [
    [
     "Josef",
     "Psutka"
    ],
    [
     "Pavel",
     "Ircing"
    ],
    [
     "J. V.",
     "Psutka"
    ],
    [
     "Jan",
     "Hajic"
    ],
    [
     "William J.",
     "Byrne"
    ],
    [
     "Jirí",
     "Mírovský"
    ]
   ],
   "title": "Automatic transcription of Czech, Russian, and Slovak spontaneous speech in the MALACH project",
   "original": "i05_1349",
   "page_count": 4,
   "order": 489,
   "p1": "1349",
   "pn": "1352",
   "abstract": [
    "This paper describes the 3.5-years effort put into building LVCSR systems for recognition of spontaneous speech of Czech, Russian, and Slovak witnesses of the Holocaust in the MALACH project. For processing of colloquial, highly emotional and heavily accented speech of elderly people containing many non-speech events we have developed techniques that very effectively handle both non-speech events and colloquial and accented variants of uttered words. Manual transcripts as one of the main sources for language modeling were automatically \"normalized\" using standardized lexicon, which brought about 2 to 3% reduction of the word error rate (WER). The subsequent interpolation of such LMs with models built from an additional collection (consisting of topically selected sentences from general text corpora) resulted into an additional improvement of performance of up to 3%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-489"
  },
  "dupont05_interspeech": {
   "authors": [
    [
     "Stéphane",
     "Dupont"
    ],
    [
     "Christophe",
     "Ris"
    ],
    [
     "Laurent",
     "Couvreur"
    ],
    [
     "Jean-Marc",
     "Boite"
    ],
    [
     "Jean-Marc",
     "Boite"
    ]
   ],
   "title": "A study of implicit and explicit modeling of coarticulation and pronunciation variation",
   "original": "i05_1353",
   "page_count": 4,
   "order": 490,
   "p1": "1353",
   "pn": "1356",
   "abstract": [
    "In this paper, we focus on the modeling of coarticulation and pronunciation variation in Automatic Speech Recognition systems (ASR). Most ASR systems explicitly describe these production phenomena through context-dependent phoneme models and multiple pronunciation lexicons.\n",
    "Here, we explore the potential benefit of using feature spaces covering longer time segments in terms of implicit modeling of coarticulation and pronunciation variants.\n",
    "The study is based on the analysis at the phonetic level of the performance of context-independent and context-dependent acoustic models, and more particularly the impact of modeling different time context going from 70 ms up to 310 ms on typical cases of pronunciation variants.\n",
    "Results, confirmed by word recognition experiment, put into light some ability of generic acoustic models to implicitly handle pronunciation variation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-490"
  },
  "takahashi05_interspeech": {
   "authors": [
    [
     "Shin-ya",
     "Takahashi"
    ],
    [
     "Tsuyoshi",
     "Morimoto"
    ],
    [
     "Sakashi",
     "Maeda"
    ],
    [
     "Naoyuki",
     "Tsuruta"
    ]
   ],
   "title": "Detection of coughs from user utterances using imitated phoneme model",
   "original": "i05_1357",
   "page_count": 4,
   "order": 491,
   "p1": "1357",
   "pn": "1360",
   "abstract": [
    "This paper proposes imitated phoneme models that represent non-verbal sounds, especially cough sounds here, as phoneme sequences. The purpose of this research is to detect the cough sounds from user utterances accurately for the home health care task because coughing is one of the most important barometers to check the health condition. To deal with the variety of the cough sounds, the imitated phoneme models are constructed by clustering of phoneme sequences obtained in phoneme recognition. The experimental results show that this approach can improve the correct rates and the accuracies for words and coughs compared with the approach using HMM constructed from cough waveforms.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-491"
  },
  "ramasubramanian05_interspeech": {
   "authors": [
    [
     "V.",
     "Ramasubramanian"
    ],
    [
     "P.",
     "Srinivas"
    ],
    [
     "T. V.",
     "Sreenivas"
    ]
   ],
   "title": "Stochastic pronunciation modeling by ergodic-HMM of acoustic sub-word units",
   "original": "i05_1361",
   "page_count": 4,
   "order": 492,
   "p1": "1361",
   "pn": "1364",
   "abstract": [
    "We propose a stochastic pronunciation model using an ergodic - hidden Markov model (EHMM) of automatically derived acoustic sub-word units (SWU). The proposed EHMM discovers the pronunciation structure inherent in the acoustic training data of a word without any apriori phonetic transcriptions. The EHMM is an HMM of HMMs - its states are SWU HMMs and the state-transitions compose various possible lexicon. The EHMM parameters are estimated by an iterative segmental K-means procedure which jointly optimizes the subword units (states) and the pronunciation structure parameters (state-transitions). The EHMM based pronunciation model is evaluated in an English isolated word recognition task with 70 speakers drawn from 8 highly different first languages. Results show that EHMM learns the lexicon distribution over the population of speakers for each word, thereby effectively modeling the inter-speaker pronunciation variability. EHMM offers an improvement of 8% (absolute) word recognition accuracy over a single most likely lexicon performance.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-492"
  },
  "liu05d_interspeech": {
   "authors": [
    [
     "Chen",
     "Liu"
    ],
    [
     "Lynette",
     "Melnar"
    ]
   ],
   "title": "An automated linguistic knowledge-based cross-language transfer method for building acoustic models for a language without native training data",
   "original": "i05_1365",
   "page_count": 4,
   "order": 493,
   "p1": "1365",
   "pn": "1368",
   "abstract": [
    "In this paper we describe an automated, linguistic knowledgebased method for building acoustic models for a target language for which there is no native training data. The method assumes availability of well-trained acoustic models for a number of existing source languages. It employs statistically derived phonetic and phonological distance metrics, particularly a combined phoneticphonological (CPP) metric, defined to characterize a variety of linguistic relationships between phonemes from the source languages and a target language. Using these metrics, candidate phonemes from the source languages are automatically selected for each phoneme of the target language and acoustic models are constructed. Our experiments show that this automated method can generate acoustic models with good quality, far above the general phoneme symbol-based cross-language transfer strategy, reaching the performance of models generated through acousticdistance mapping.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-493"
  },
  "bouselmi05_interspeech": {
   "authors": [
    [
     "Ghazi",
     "Bouselmi"
    ],
    [
     "Dominique",
     "Fohr"
    ],
    [
     "Irina",
     "Illina"
    ],
    [
     "Jean-Paul",
     "Haton"
    ]
   ],
   "title": "Fully automated non-native speech recognition using confusion-based acoustic model integration",
   "original": "i05_1369",
   "page_count": 4,
   "order": 494,
   "p1": "1369",
   "pn": "1372",
   "abstract": [
    "This paper presents a fully automated approach for the recognition of non-native speech based on acoustic model modification. For a native language (L1) and a spoken language (L2), pronunciation variants of the phones of L2 are automatically extracted from an existing non-native database as a confusion matrix with sequences of phones of L1. This is done using L1's and L2's ASR systems. This confusion concept deals with the problem of non existence of match between some L2 and L1 phones. The confusion matrix is then used to modify the acoustic models (HMMs) of L2 phones by integrating corresponding L1 phone models as alternative HMM paths. In this way, no lexicon modification is carried. The modified ASR system achieved an improvement between 32% and 40% (relative, L1=French and L2=English) in WER on the French non-native database used for testing.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-494"
  },
  "auberge05_interspeech": {
   "authors": [
    [
     "Véronique",
     "Aubergé"
    ],
    [
     "Albert",
     "Rilliard"
    ]
   ],
   "title": "The focus prosody: more than a simple binary function",
   "original": "i05_1373",
   "page_count": 4,
   "order": 495,
   "p1": "1373",
   "pn": "1376",
   "abstract": [
    "The prosodic focus conveys the information of the yes/no deixis function on a given linguistic domain, generally the word, quite in all languages, and presumably in an universal morphology. Our proposal is that the prosody of focus conveys together some complementary information: the domain of the deixis (phone, syllable, word, larger) and a gradient functional value of insistance which could be controlled with the illocutory force. In this aim, a corpus of isolated French sentences was recorded, with two consigns: to point on the word vs. to point on the syllable, in varying the place of the syllable in the word. Different kinds of perception tests were held A first experiment confirms the validity of the recorded corpus, shows that a focus is perceived on a word or on a syllable whatever the place of the syllable. An acoustic analysis shows that the word condition is implemented by a prominence on the first syllable (as usually attested in French) and that the acoustic profile of the syllable vs. word focus is similar in tonal description but opposable in a dynamic description.\n",
    "A categorical perception experiment, held on stimuli progressively morphed from the no focus condition to the focus condition as produced by the speaker, makes clearly appear the binary decision of the focus function processed by the listeners. The limit values of F0, intensity and duration, give the basic level of the beginning of the gradient function of focus. This focus description is integrated in a general model of prosody, based on superposition of functional gradient contours, including the linguistic and paralinguistic functions of prosody [1].\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-495"
  },
  "dalton05_interspeech": {
   "authors": [
    [
     "Martha",
     "Dalton"
    ],
    [
     "Ailbhe",
     "Ní Chasaide"
    ]
   ],
   "title": "Peak timing in two dialects of connaught irish",
   "original": "i05_1377",
   "page_count": 4,
   "order": 496,
   "p1": "1377",
   "pn": "1380",
   "abstract": [
    "A comparison of the peak location in nuclear and initial prenuclear accents was carried out for two closely related dialects of Connaught Irish: Cois Fharraige and Inis Oirr. This was done across conditions where the number of unstressed syllables following the nuclear and preceding the initial prenuclear accents was varied from 2-0. Clear differences in peak timing emerged between the two dialects. In Cois Fharraige Irish the timing of the peak is unaffected by the presence and/or number of adjacent unstressed syllables. In Inis Oirr Irish there is variability in peak timing for both prenuclear and nuclear positions. In nuclear position Inis Oirr peak realizations for the different conditions range from the left edge of the accented vowel to the post accented vowel. In prenuclear position realizations range from the left-edge of the accented vowel as far as the right edge of the post accented syllable.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-496"
  },
  "fletcher05_interspeech": {
   "authors": [
    [
     "Janet",
     "Fletcher"
    ]
   ],
   "title": "Compound rises and \"uptalk\" in spoken English",
   "original": "i05_1381",
   "page_count": 4,
   "order": 497,
   "p1": "1381",
   "pn": "1384",
   "abstract": [
    "\"Uptalk\" or the use of rising and high pitch at the end of statements is common in interactive discourse in Australian English. The distribution and discourse functions of complex and compound rising tunes were examined in a corpus of Australian English map task dialogues. Each utterance was analysed in terms of Dialogue acts (classified using DAMSL) and intonational tune. It was found that most terminal high rises were in fact part of split or compound fall-rises, and are not the same as yes/no question rises in English. There was also a strong correspondence between pitch range of the terminal elements and discourse function. For example, forward-looking communicative acts (e.g. those that influenced the upcoming discourse) usually corresponded to complex and compound fall rises that terminated at a high pitch level, whereas low range fall-rises (both complex and compound) were more likely to terminate backwards looking dialogue acts, i.e. those acts that referred to what had previously occurred in the discourse. These results support earlier claims that rising pitch in Australian English is used as a cooperative device in complex interactions. They further suggest that the entire tune of the intonational constituent needs to be taken into account, and not just the terminal element.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-497"
  },
  "yang05_interspeech": {
   "authors": [
    [
     "Li-chiung",
     "Yang"
    ]
   ],
   "title": "Duration and the temporal structure of Mandarin discourse",
   "original": "i05_1385",
   "page_count": 4,
   "order": 498,
   "p1": "1385",
   "pn": "1388",
   "abstract": [
    "Duration is a primary factor in the structural and cognitive organization of discourse. In this study, we investigate pauses and durational patterns in Mandarin Chinese spontaneous conversation, as well as investigate how reliably such elements can serve as boundary-marking predictors across different types of speech corpora, and how language activities are affected by their cognitive correlates. Our results show that pause duration is significantly correlated with specific boundary status and that syllable duration is inversely correlated with distance to phrase end, suggesting that syllable duration is a robust feature in predicting phrase boundary status. Our findings show that duration features are highly informative and provide valuable information on discourse structure and the expressiveness of cognitive state in language communication.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-498"
  },
  "wang05g_interspeech": {
   "authors": [
    [
     "Bei",
     "Wang"
    ]
   ],
   "title": "Prosodic realization of split noun phrases in Mandarin Chinese compared in topic and focus contexts",
   "original": "i05_1389",
   "page_count": 4,
   "order": 499,
   "p1": "1389",
   "pn": "1392",
   "abstract": [
    "The present study investigates the prosodic realization of split noun sentences in Chinese, like shu, wo mai le san ben. (Book, I buy ASP three CLAS. I bought three books.'). The question and answer paradigm was used to induce sentences where the noun was either the topic or the focus. In the topic context, the question was of the form, I heard you bought books and pencils. Is that true?' In the focus context, it was of the form, What did you buy three?' Speech production results show that the prosodic realization of split noun sentences makes the noun a separate intonational phrase. The intonational difference between these two contexts for split noun sentences is not significant. The interesting finding is that the pause after the noun in the topic context is significantly shorter than in the focus context (189ms and 248ms on average, respectively), which shows that the base part is more closely related to the noun in the topic context than in the focus context. In both contexts, there is a pitch accent on the modifier in the base part. These findings give prosodic evidence for Pan's (2003) argument that a focus in the base is a cue that the focused phrase is related to the topicalized split noun. Corresponding perception experiments testing similar question and answer pairs were carried out as well. Results show that the acceptance of split noun sentences is good, and that listeners prefer a longer pause after the noun in the context of focus questions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-499"
  },
  "xiong05_interspeech": {
   "authors": [
    [
     "Ziyu",
     "Xiong"
    ]
   ],
   "title": "Downstep effect on disyllabic words of citation forms in standard Chinese",
   "original": "i05_1393",
   "page_count": 4,
   "order": 500,
   "p1": "1393",
   "pn": "1396",
   "abstract": [
    "The purpose of this study is to specifically analyze the pitch contours of disyllabic words in Standard Chinese. Based on a huge speech corpus, the paper investigates the downstep effect on the second syllable with a zero initial or a voiced initial in disyllabic word of citation form. Statistical results show that when the tonal pitch features of a disyllabic word constitute a \"..H..L..H..\" tonal sequence, the H tone behind the L tone will be lowered contrasted to the preceding H tone obviously, and such phenomenon is called downstep. It is also shown in this paper that downstep happened on disyllabic words in Standard Chinese has relatively stable pitch manifestation in different kinds of tonal combinations. The results found in this paper can be applied to generate pitch contours properly to improve the naturalness of the speech synthesis.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-500"
  },
  "ni05_interspeech": {
   "authors": [
    [
     "Jinfu",
     "Ni"
    ],
    [
     "Hisashi",
     "Kawai"
    ],
    [
     "Keikichi",
     "Hirose"
    ]
   ],
   "title": "Estimation of intonation variation with constrained tone transformations",
   "original": "i05_1397",
   "page_count": 4,
   "order": 501,
   "p1": "1397",
   "pn": "1400",
   "abstract": [
    "This paper presents a method for quantitatively estimating intonation variation in Mandarin speech. Intonation variation is relative to identical lexical tone structures, and its estimation is performed on two sets of fundamental frequency (F0) contours: one for norms and the other as variants. This is done by transforming target F0 values in pairs from the norms to the variants in which the prosodic contribution to these F0 contours is analyzed as sequences of targets, all of which are confined to the basic elements of the underlying lexical tone structures. The tone transformations are constrained under an assumption of the structural formulation of F0 contours proposed previously. When the norms take the base values of the four lexical tones measured from isolated words in a neutral mood and voice, this method solves acoustic correlations of tone and intonation from the observed F0 contours. The method was implemented on a computer, and its capability of estimating intonation variation was shown through the analysis and synthesis of F0 contours.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-501"
  },
  "pan05_interspeech": {
   "authors": [
    [
     "Ho-hsien",
     "Pan"
    ]
   ],
   "title": "Voice quality of falling tones in taiwan min",
   "original": "i05_1401",
   "page_count": 4,
   "order": 502,
   "p1": "1401",
   "pn": "1404",
   "abstract": [
    "This study compared the voice quality of high and low falling unchecked and checked tones. Spectral tilt measured as H0-A2 was taken from the syllable midpoint of second, third and four syllables in SVO sentences. Results showed that the voice quality of checked tones was not always creakier than unchecked tones. The voice quality of a syllable at the end of an utterance was not creakier than a syllable at the beginning of an utterance. Narrow focus was a salient factor affecting voice quality. While voice quality of narrow focus syllables became creakier, voice quality of post-focus syllables became more breathy. Focus, a prosodic factor, affected voice quality in a more consistent manner than did lexical tone and syllable position within an utterance in Taiwan Min (Taiwanese).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-502"
  },
  "tseng05_interspeech": {
   "authors": [
    [
     "Chiu-yu",
     "Tseng"
    ],
    [
     "Bau-Ling",
     "Fu"
    ]
   ],
   "title": "Duration, intensity and pause predictions in relation to prosody organization",
   "original": "i05_1405",
   "page_count": 4,
   "order": 503,
   "p1": "1405",
   "pn": "1408",
   "abstract": [
    "Our research group has postulated a perceptually based multiphrase prosody framework for speech paragraphs in fluent speech using corporal analyses. The framework features a prosody hierarchy that organizes phrases and sentences into prosodic groups (PG) in connected speech, and specifies cross-phrase prosodic relationships in the acoustic domains [1, 2]. A corresponding fluent speech prosody model with four independent acoustic modules was also constructed [3]. The model predicts cross-phrase F0 contours, duration patterns, intensity distribution and pause insertions in accordance with prosody organization. Cumulative results from each and every prosody layer accounts for overall output prosody. We have since improved the model first by refining the duration and intensity modules through corpus analysis, and subsequently used the above improved results to facilitate better pause/break predictions. As a result, the enhanced model is now more robust than its initial version. Future works will focus on applying the improved model to synthesis of fluent connected speech.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-503"
  },
  "yuan05_interspeech": {
   "authors": [
    [
     "Jiahong",
     "Yuan"
    ],
    [
     "Jason M.",
     "Brenier"
    ],
    [
     "Daniel",
     "Jurafsky"
    ]
   ],
   "title": "Pitch accent prediction: effects of genre and speaker",
   "original": "i05_1409",
   "page_count": 4,
   "order": 504,
   "p1": "1409",
   "pn": "1412",
   "abstract": [
    "To build a robust pitch accent prediction system, we need to understand the effects of speech genre and speaker variation. This paper reports our studies on genre and speaker variation in pitch accent placement and their effects on automatic pitch accent prediction. We find some interesting accentuation pattern differences that can be attributed to speech genre, and a set of textual features that are robust to genre in accent prediction. We also find that although there is significant variation among speakers in pitch accent placement, speaker dependent models are not needed in accent prediction. Finally, we show that after taking speaker variation into account, there is little room to improve for state-of-the-art classifiers on read news speech.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-504"
  },
  "fujisaki05_interspeech": {
   "authors": [
    [
     "Hiroya",
     "Fujisaki"
    ],
    [
     "Sumio",
     "Ohno"
    ]
   ],
   "title": "Analysis and modeling of fundamental frequency contours of hindi utterances",
   "original": "i05_1413",
   "page_count": 4,
   "order": 505,
   "p1": "1413",
   "pn": "1416",
   "abstract": [
    "This paper describes the results of a preliminary study on the applicability of the command-response model to F0 contours of spoken Hindi, an official language of India with almost 400 million native speakers in the world. Analysis of observed F0 contours of a number of utterances by two native speakers indicated that the model with provisions for positive and negative accent commands applies quite well to all the utterances analyzed, and the estimated commands are found to be closely related to the linguistic contents of the utterances. One of the peculiar features of F0 contours of Hindi is the occurrence of a negative accent command at most phrase-initial positions, often followed by a positive accent command.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-505"
  },
  "govender05_interspeech": {
   "authors": [
    [
     "Natasha",
     "Govender"
    ],
    [
     "Etienne",
     "Barnard"
    ],
    [
     "Marelie",
     "Davel"
    ]
   ],
   "title": "Fundamental frequency and tone in isizulu: initial experiments",
   "original": "i05_1417",
   "page_count": 4,
   "order": 506,
   "p1": "1417",
   "pn": "1420",
   "abstract": [
    "Much theoretical work has been done on the tonal structure of languages in the Bantu family. However, most of these studies are not supported by physical measurements, or even a consistent model for mapping from linguistic constructs to such measurements. As a first step towards addressing this deficiency, we report on initial measurements regarding the relationship between fundamental frequency and linguistic tone in isiZulu. After choosing a suitable algorithm for pitch extraction, we have correlated a number of linguistically assigned tone values with measured values for fundamental frequency. These measurements indicate a fairly complex relationship between tone and pitch, and suggest that the commonly observed falling' tone in isiZulu may be a context-specific realization of the high tone.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-506"
  },
  "bishop05_interspeech": {
   "authors": [
    [
     "Judith",
     "Bishop"
    ],
    [
     "Marc",
     "Peake"
    ],
    [
     "Dmitry",
     "Sityaev"
    ]
   ],
   "title": "Intonational sequences in tuscan Italian",
   "original": "i05_1421",
   "page_count": 4,
   "order": 507,
   "p1": "1421",
   "pn": "1424",
   "abstract": [
    "Sequential distributions of intonational pitch accents in a handlabelled, read speech corpus of Tuscan Italian are analysed in relation to (1) the global frequencies of accents; (2) evidence of consistent patterns of association between accents. A subset of syntactically controlled nominal phrases are examined to determine whether the fact of their constituting a coherent syntactic unit has any effect on their patterns of sequential tone distribution, i.e. whether they are associated with a given tune. Some evidence is found for patterns of association between accents, and between sequences of accents and our chosen type of nominal phrase.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-507"
  },
  "petrone05_interspeech": {
   "authors": [
    [
     "Caterina",
     "Petrone"
    ]
   ],
   "title": "Effects of raddoppiamento sintattico on tonal alignment in Italian",
   "original": "i05_1425",
   "page_count": 4,
   "order": 508,
   "p1": "1425",
   "pn": "1428",
   "abstract": [
    "Raddoppiamento (Fono-) Sintattico (RS) is a phenomenon found in Central and Southern varieties of Italian, by which an oxytone word1 lengthens the initial consonant of word2. One possible interpretation is that RS is a resyllabification process, depending on the same constraints on syllable structure as within-word gemination. If this is correct, we also expect both phenomena to be signaled by the same cues. As a consequence of syllable structure and vowel length differences, in Neapolitan Italian the peak of the L+H nuclear accent is later in words containing a geminate (nonno) than in words containing a singleton consonant (nono). This paper reports a pilot study on effects of RS on tonal alignment in Neapolitan Italian. The results show that, as in the case of items containing geminates, the H target is later when RS is applied. This suggests that RS can be regarded as a process of syllable restructuring.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-508"
  },
  "dubeda05_interspeech": {
   "authors": [
    [
     "Tomás",
     "Dubeda"
    ],
    [
     "Jan",
     "Votrubec"
    ]
   ],
   "title": "Acoustic analysis of Czech stress: intonation, duration and intensity revisited",
   "original": "i05_1429",
   "page_count": 4,
   "order": 509,
   "p1": "1429",
   "pn": "1432",
   "abstract": [
    "By examining acoustic marks of Czech stress, this paper attempts to provide an answer to the question of whether or not perceived accents in the Czech language have an objective existence. A neural network is used to predict the position of accents without lexical information. Three parameters (intonation, duration and intensity) are considered individually, in pairs and altogether. Fundamental frequency seems to be the best predictor of stress, both alone and combined with other parameters. The analysis of the individual prediction errors allows for a closer look at factors which are critical in accent prediction.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-509"
  },
  "yeou05_interspeech": {
   "authors": [
    [
     "Mohamed",
     "Yeou"
    ]
   ],
   "title": "Variability of F0 peak alignment in moroccan Arabic accentual focus",
   "original": "i05_1433",
   "page_count": 4,
   "order": 510,
   "p1": "1433",
   "pn": "1436",
   "abstract": [
    "The present paper examines how phonetic duration due to syllable structure contributes to the alignment of F0 peaks in Moroccan Arabic. The F0 peak occurs within but near the end of the accented syllable if the vowel is phonetically long due its occurrence in a final CVC. If the accented vowel is phonetically short as in a penultimate CV, the F0 peak is aligned after the accented syllable.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-510"
  },
  "lacheret05_interspeech": {
   "authors": [
    [
     "Anne",
     "Lacheret"
    ],
    [
     "Ch.",
     "Lyche"
    ],
    [
     "Michel",
     "Morel"
    ]
   ],
   "title": "Phonological analysis of schwa and liaison within the PFC project (phonologie du fran\u0007ais contemporain): how determinant are the prosodic factors?",
   "original": "i05_1437",
   "page_count": 4,
   "order": 511,
   "p1": "1437",
   "pn": "1440",
   "abstract": [
    "The influence of prosody on segmental phenomena such as French schwa and liaison has been pointed out, but never systematically studied on speech data. In this paper we propose a comprehensive coding system to be used on a large oral corpus. Such a system aims at providing explicit and robust data which can serve as a solid base to test the different claims which have been put forward in the literature.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-511"
  },
  "barbosa05_interspeech": {
   "authors": [
    [
     "Plínio A.",
     "Barbosa"
    ],
    [
     "Pablo",
     "Arantes"
    ],
    [
     "Alexsandro R.",
     "Meireles"
    ],
    [
     "Jussara M.",
     "Vieira"
    ]
   ],
   "title": "Abstractness in speech-metronome synchronisation: P-centres as cyclic attractors",
   "original": "i05_1441",
   "page_count": 4,
   "order": 512,
   "p1": "1441",
   "pn": "1444",
   "abstract": [
    "The p-centre phenomenon is revisited here under a dynamical paradigm. It is our contention that p-centres can be seen as the projection onto the time axis of an underlying phrase stress oscillator inferred during the realisation of the task of producing (and perceiving) a sequence of syllables in time with a metronome. In this kind of inference, vowel onsets act as point attractors for the task of synchronisation. The p-centre, defined as the metronome pulse position relative to the syllable, is considered as the attractor predicted by the subject. The innovative aspects in the experiments carried out to sustain these hypotheses refer to: taking into account as many as 21 Brazilian Portuguese CV syllables; exploring the dynamical aspects of the task, such as variation of the subject's performance due to rate changes; and explaining the subject's behaviour based on period and/or phase locking between syllable production and inferred-from-metronome phrase stress oscillator. The phase differences between the p-centres and the vowel onsets are treated as on-line perturbations of energy rise detected on account of the consonants' spectral composition.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-512"
  },
  "yamada05_interspeech": {
   "authors": [
    [
     "Makoto",
     "Yamada"
    ],
    [
     "Tsuneo",
     "Kato"
    ],
    [
     "Masaki",
     "Naito"
    ],
    [
     "Hisashi",
     "Kawai"
    ]
   ],
   "title": "Improvement of rejection performance of keyword spotting using anti-keywords derived from large vocabulary considering acoustical similarity to keywords",
   "original": "i05_1445",
   "page_count": 4,
   "order": 513,
   "p1": "1445",
   "pn": "1448",
   "abstract": [
    "This paper proposes an efficient anti-keyword derivation method to improve the rejection performance of keyword spotting. In this method, each anti-keyword is derived from the large vocabulary lexicon considering acoustic similarity to keywords, making use of the confusion matrix. Experimental results show that a 3% improvement of the rejection rate is obtained compared to conventional methods that do not have our anti-keyword models.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-513"
  },
  "schluter05_interspeech": {
   "authors": [
    [
     "Ralf",
     "Schlüter"
    ],
    [
     "T.",
     "Scharrenbach"
    ],
    [
     "Volker",
     "Steinbiss"
    ],
    [
     "Hermann",
     "Ney"
    ]
   ],
   "title": "Bayes risk minimization using metric loss functions",
   "original": "i05_1449",
   "page_count": 4,
   "order": 514,
   "p1": "1449",
   "pn": "1452",
   "abstract": [
    "In this work, fundamental properties of Bayes decision rule using general loss functions are derived analytically and are verified experimentally for automatic speech recognition. It is shown that, for maximum posterior probabilities larger than 1/2, Bayes decision rule with a metric loss function always decides on the posterior maximizing class independent of the specific choice of (metric) loss function. Also for maximum posterior probabilities less than 1/2, a condition is derived under which the Bayes risk using a general metric loss function is still minimized by the posterior maximizing class. For a speech recognition task with low initial word error rate, it is shown that nearly 2/3 of the test utterances fulfil these conditions and need not be considered for Bayes risk minimization with Levenshtein loss, which reduces the computational complexity of Bayes risk minimization. In addition, bounds for the difference between the Bayes risk for the posterior maximizing class and minimum Bayes risk are derived, which can serve as cost estimates for Bayes risk minimization approaches.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-514"
  },
  "kobayashi05_interspeech": {
   "authors": [
    [
     "Akio",
     "Kobayashi"
    ],
    [
     "Kazuo",
     "Onoe"
    ],
    [
     "Shoei",
     "Sato"
    ],
    [
     "Toru",
     "Imai"
    ]
   ],
   "title": "Word error rate minimization using an integrated confidence measure",
   "original": "i05_1453",
   "page_count": 4,
   "order": 515,
   "p1": "1453",
   "pn": "1456",
   "abstract": [
    "This paper describes a new criterion of speech recognition using an integrated confidence measure for minimization of the word error rate (WER). Conventional criteria for WER minimization obtain an expected WER of a sentence hypothesis merely by comparing it with other hypotheses in an n-best list. The proposed criterion estimates the expected WER by using an integrated confidence measure with word posterior probabilities for a given acoustic input. The integrated confidence measure, which is implemented as a classifier based on maximum entropy (ME) modeling, is used to get probabilities reflecting whether the word hypotheses are correct or incorrect. The classifier comprises a variety of confidence measures and can deal with a temporal sequence of them in order to attain a more reliable confidence. Our proposed criterion achieved a WER of 7.5% and a 2.6% improvement relative to conventional n-best rescoring methods in transcribing Japanese broadcast news under noisy field and spontaneous speech conditions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-515"
  },
  "dong05_interspeech": {
   "authors": [
    [
     "Bin",
     "Dong"
    ],
    [
     "Qingwei",
     "Zhao"
    ],
    [
     "Yonghong",
     "Yan"
    ]
   ],
   "title": "Fast confidence measure algorithm for continuous speech recognition",
   "original": "i05_1457",
   "page_count": 4,
   "order": 516,
   "p1": "1457",
   "pn": "1460",
   "abstract": [
    "In this paper, a fast confidence measure algorithm for continuous speech recognition is presented. The posterior probabilities of states are selected as the main feature. The confidences of hypothesized phonemes and words are estimated using the information of corresponding states. Traditional systems use two passes and two different models for decoding and calculating confidence respectively. In this new algorithm, the confidence score is calculated during the decoding of the state graph constructed from task grammar. And in this new algorithm, the same acoustics model is used during the process of decoding and confidence estimation. The old and new computing algorithms are compared through experiments, and the result shows that performance of the new algorithm is effectively improved. The equal error rate (EER) of new fast confidence algorithm is reduced and the speed is accelerated.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-516"
  },
  "ketabdar05_interspeech": {
   "authors": [
    [
     "Hamed",
     "Ketabdar"
    ],
    [
     "Jithendra",
     "Vepa"
    ],
    [
     "Samy",
     "Bengio"
    ],
    [
     "Hervé",
     "Bourlard"
    ]
   ],
   "title": "Developing and enhancing posterior based speech recognition systems",
   "original": "i05_1461",
   "page_count": 4,
   "order": 517,
   "p1": "1461",
   "pn": "1464",
   "abstract": [
    "Local state or phone posterior probabilities are often investigated as local scores (e.g., hybrid HMM/ANN systems) or as transformed acoustic features (e.g., \"Tandem\") to improve speech recognition systems. In this paper, we present initial results towards boosting these approaches by improving posterior estimates, using acoustic context (e.g., as available in the whole utterance), as well as possible prior information (such as topological constraints). In the present work, the enhanced posterior distribution is associated with the \"gamma\" distribution typically used in standard HMMs training, and estimated from local likelihoods (GMM) or local posteriors (ANN). This approach results in a family of new HMM based systems, where only posterior probabilities are used, while also providing a new, principled, approach towards a hierarchical use/integration of these posteriors, from the frame level up to the phone and word levels, and integrating the appropriate context and prior knowledge in each level. In the present work, we used the resulting posteriors as local scores in a Viterbi decoder. On the OGI Numbers'95 database, this resulted in improved recognition performance, compared to a state-of-the-art hybrid HMM/ANN system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-517"
  },
  "liu05e_interspeech": {
   "authors": [
    [
     "Peng",
     "Liu"
    ],
    [
     "Ye",
     "Tian"
    ],
    [
     "Jian-Lai",
     "Zhou"
    ],
    [
     "Frank K.",
     "Soong"
    ]
   ],
   "title": "Background model based posterior probability for measuring confidence",
   "original": "i05_1465",
   "page_count": 4,
   "order": 518,
   "p1": "1465",
   "pn": "1468",
   "abstract": [
    "Word posterior probability (WPP) computed over LVCSR word graphs has been used successfully in measuring confidence of speech recognition output. However, for certain applications the word graph is too sparse to warrant reliable WPP estimation. In this paper, we incorporate subword units as background models to generate a subword graph for estimating posterior probability. Experiments on both English and Chinese databases show that syllable background models can repopulate the dynamic hypothesis space for effective computation of confidence measure. The resultant posterior probability confidence measure achieves 94.3% and 95.2% Out-Of-Vocabulary (OOV) word detection / rejection in English and Chinese, respectively. Correspondingly, confidence error rates are at 6.0% and 6.4%, respectively.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-518"
  },
  "tomokiyo05_interspeech": {
   "authors": [
    [
     "Laura Mayfield",
     "Tomokiyo"
    ],
    [
     "Alan W.",
     "Black"
    ],
    [
     "Kevin A.",
     "Lenzo"
    ]
   ],
   "title": "Foreign accents in synthetic speech: development and evaluation",
   "original": "i05_1469",
   "page_count": 4,
   "order": 519,
   "p1": "1469",
   "pn": "1472",
   "abstract": [
    "This paper addresses the generation and evaluation of foreignaccented speech in concatenative text-to-speech (TTS) synthesis. We describe three possible methods of building a Spanish-accented English voice, and evaluate and compare them with respect to preference, intelligibility, and smoothness. Effects of speaking rate and content are also examined.\n",
    "It is found that although using an unmodified Spanish voice to read English text is possible, the result is not highly intelligible. With some modifications to the linguistic model, a relatively high level of comprehensibility and smoothness can be achieved, not differing widely from ratings given to a native voice at a comparable stage of development. Listeners in perceptual experiments were very consistent in their preference rankings of the three voices, showing that differences in voice-building method are both detectable and contribute to synthesis quality.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-519"
  },
  "fernandez05b_interspeech": {
   "authors": [
    [
     "Raul",
     "Fernandez"
    ],
    [
     "Wei",
     "Zhang"
    ],
    [
     "Ellen",
     "Eide"
    ],
    [
     "Raimo",
     "Bakis"
    ],
    [
     "Wael",
     "Hamza"
    ],
    [
     "Yi",
     "Liu"
    ],
    [
     "Michael",
     "Picheny"
    ],
    [
     "John F.",
     "Pitrelli"
    ],
    [
     "Yong",
     "Qing"
    ],
    [
     "Zhi Wei",
     "Shuang"
    ],
    [
     "Li Qin",
     "Shen"
    ]
   ],
   "title": "Toward multiple-language TTS: experiments in English and Mandarin",
   "original": "i05_1473",
   "page_count": 4,
   "order": 520,
   "p1": "1473",
   "pn": "1476",
   "abstract": [
    "Text-to-speech systems have dramatically improved in recent years through the use of corpus-based concatenative approaches, and we are beginning to see an interest in endowing them with the ability to handle more than the native language for which they have been developed. In this paper we present ongoing work at IBM in text-to-speech systems that can produce high-quality synthesis in more than one language. We illustrate the discussion with a case study in which two systems, originally developed to support English and Mandarin respectively, have been extended to support each other's languages. We describe the challenges faced when adapting one system to a different target language, propose adaptation solutions, and present the results of perceptual tests carried out to evaluate how the approaches compare with the performance of the native systems.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-520"
  },
  "latorre05_interspeech": {
   "authors": [
    [
     "Javier",
     "Latorre"
    ],
    [
     "Koji",
     "Iwano"
    ],
    [
     "Sadaoki",
     "Furui"
    ]
   ],
   "title": "Cross-language synthesis with a polyglot synthesizer",
   "original": "i05_1477",
   "page_count": 4,
   "order": 521,
   "p1": "1477",
   "pn": "1480",
   "abstract": [
    "In this paper we examine the use of an HMM-based polyglot synthesizer for languages for which very limited or no speech data is available. In a former study, we presented a system that combines monolingual corpora from several languages to create a polyglot synthesizer. With this synthesizer we can synthesize any of the languages included in the training data with the same output voice and speech quality. In this paper, we approximate the sounds of non-included languages, by those available in the polyglot training data. Since the phonetic inventory of a polyglot synthesizer is wider than that of a monolingual one, the approximation of such non-included sounds becomes more accurate and thus the perceptual intelligibility increases. Moreover, the performance of a polyglot synthesizer can be further improved by adding a reduced amount of data from the target language.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-521"
  },
  "gakuru05_interspeech": {
   "authors": [
    [
     "Mucemi",
     "Gakuru"
    ],
    [
     "Frederick K.",
     "Iraki"
    ],
    [
     "Roger",
     "Tucker"
    ],
    [
     "Ksenia",
     "Shalonova"
    ],
    [
     "Kamanda",
     "Ngugi"
    ]
   ],
   "title": "Development of a Kiswahili text to speech system",
   "original": "i05_1481",
   "page_count": 4,
   "order": 522,
   "p1": "1481",
   "pn": "1484",
   "abstract": [
    "This paper discusses how a concatenative Kiswahili Text to Speech System (TTS) was developed based on the Festival Unit Selection Speech Synthesiser. It explains how important Kiswahili linguistic features such as phones, stress and intonation were modelled as inputs to the Festival engine. It also discusses the design, recording and segmentation of the speech database, beginning with text corpus collection and transcription. The choice of the speaker, which is crucial to realising a good TTS is discussed and also how the system was tested.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-522"
  },
  "ordinas05_interspeech": {
   "authors": [
    [
     "J. Botella",
     "Ordinas"
    ],
    [
     "V.",
     "Fischer"
    ],
    [
     "C.",
     "Waast-Richard"
    ]
   ],
   "title": "Multilingual models in the IBM bilingual text-to-speech systems",
   "original": "i05_1485",
   "page_count": 4,
   "order": 523,
   "p1": "1485",
   "pn": "1488",
   "abstract": [
    "In this paper we describe the role of multilingual models in the creation and deployment of unit selection based bilingual speech synthesizers. We first review the definition of a multilingual phonetic alphabet for the simultaneous recognition of up to fifteen languages, and then discuss synthesis specific modifications that allow a more detailed description of the synthesizers' unit inventories. Furthermore, we investigate on the use of multilingual phonological decision trees for a fast identification of candidate segments, and for the prediction of pitch and duration targets during synthesis. Experimental results for two different combinations of languages demonstrate the feasibility of multilingual modeling for speech synthesis.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-523"
  },
  "janicki05_interspeech": {
   "authors": [
    [
     "Artur",
     "Janicki"
    ],
    [
     "Piotr",
     "Herman"
    ]
   ],
   "title": "Reconstruction of Polish diacritics in a text-to-speech system",
   "original": "i05_1489",
   "page_count": 4,
   "order": 524,
   "p1": "1489",
   "pn": "1492",
   "abstract": [
    "This paper describes an approach to reconstruction of the Polish diacritic signs, needed e.g. in a speech synthesis system. Some telecommunication services (for example SMS transmission in GSM) remove diacritics from the text. Without them the text is usually still understandable to a reader, but if a TTS system reads it, the speech becomes heavily distorted. In this paper we propose to use neural networks to reconstruct the Polish diacritics. Architecture of the proposed system is described, the process of training and testing is presented. At the end a real-life implementation is described. Usage of SMS-to-speech service increased by more than 30% after implementing the proposed system of reconstructing diacritics.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-524"
  },
  "ehara05_interspeech": {
   "authors": [
    [
     "Hiroyuki",
     "Ehara"
    ],
    [
     "Toshiyuki",
     "Morii"
    ],
    [
     "Masahiro",
     "Oshikiri"
    ],
    [
     "Koji",
     "Yoshida"
    ],
    [
     "Kouichi",
     "Honma"
    ]
   ],
   "title": "Design of bandwidth scalable LSF quantization using interframe and intraframe prediction",
   "original": "i05_1493",
   "page_count": 4,
   "order": 525,
   "p1": "1493",
   "pn": "1496",
   "abstract": [
    "We developed a bandwidth scalable LSF (Line Spectral Frequency) quantizer utilizing interframe and intraframe prediction. The predictor used in the quantizer was designed as combination of first order AR (auto regressive) prediction and a codebook mapping technique, which maps a narrow band LSF codebook to a wide band LSF codebook. An upsampling process in the autocorrelation domain was utilized for conversion between narrowband LSF and wideband LSF. Codebooks for the mapping, coefficients for the predictor, and codebooks for VQ (vector quantization) of prediction residues were designed by using training database including multi languages and various kinds of background noise conditions. When 16 bits were assigned to an enhancement layer of the scalable LSF quantizer, utilization of the interframe and intraframe prediction improved the quantizer performance in spectral distortion by about 0.3 dB.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-525"
  },
  "geiser05_interspeech": {
   "authors": [
    [
     "Bernd",
     "Geiser"
    ],
    [
     "Peter",
     "Jax"
    ],
    [
     "Peter",
     "Vary"
    ]
   ],
   "title": "Artificial bandwidth extension of speech supported by watermark-transmitted side information",
   "original": "i05_1497",
   "page_count": 4,
   "order": 526,
   "p1": "1497",
   "pn": "1500",
   "abstract": [
    "In recent years research in the area of artificial bandwidth extension (BWE) of speech signals has made significant progress. The intention of BWE is to produce wideband speech with a cut-off frequency of for instance 7 kHz from a narrowband version (e.g., with telephone bandwidth, i.e., 300Hz - 3.4 kHz). The respective algorithms are based on the estimation of parameters of a source model for speech production given the knowledge of the narrowband signal. A theoretical performance bound on this estimation has been formulated in [1]. In order to overcome this boundary we propose the transmission of (compact) side information which can support the parameter estimation. Since a common additional channel would conflict with the requirement of backwards compatibility in narrowband communication systems, the side information is embedded as digital watermark into the narrowband speech.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-526"
  },
  "hu05c_interspeech": {
   "authors": [
    [
     "Rongqiang",
     "Hu"
    ],
    [
     "Venkatesh",
     "Krishnan"
    ],
    [
     "David V.",
     "Anderson"
    ]
   ],
   "title": "Speech bandwidth extension by improved codebook mapping towards increased phonetic classification",
   "original": "i05_1501",
   "page_count": 4,
   "order": 527,
   "p1": "1501",
   "pn": "1504",
   "abstract": [
    "Bandwidth limitation (0-4KHz) is a major degradation for the performance of the current speech communication systems. The narrowband speech provides much lower quality and intelligibility than wideband speech (0-8KHz). Speech bandwidth extension technology has been recently investigated to aim at artificially regenerating the missing high-band speech signal. This paper describes a robust speech bandwidth extension system by an improved codebook mapping method (CM-IPC), which includes a modified codebook training towards increased phonetic classification, marginal LSF interpolation, codebook mapping with memory, and codebook interpolation. A variety of experiments, in clean and noisy environments, were conducted to evaluate the performance of the proposed system. The results indicate the improvement in objective quality measures. The proposed system can also be applied to the feature extension estimation of speech signals.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-527"
  },
  "bansal05_interspeech": {
   "authors": [
    [
     "Dhananjay",
     "Bansal"
    ],
    [
     "Bhiksha",
     "Raj"
    ],
    [
     "Paris",
     "Smaragdis"
    ]
   ],
   "title": "Bandwidth expansion of narrowband speech using non-negative matrix factorization",
   "original": "i05_1505",
   "page_count": 4,
   "order": 528,
   "p1": "1505",
   "pn": "1508",
   "abstract": [
    "In this paper, we present a novel technique for the estimation of the high frequency components (4-8kHz) of speech signals from narrow-band (0-4 kHz) signals using convolutive Non-Negative Matrix Factorisation (NMF). The proposed technique utilizes a brief recording of simultaneous broad band and narrow band signals from a target speaker to learn a set of broad-band non-negative \"bases\" for the speaker. The low-frequency components of these bases are used to determine how the high-frequency components must be combined in order to reconstruct the high-frequency components of new narrow-band signals from the speaker. Experiments reveal that the technique is able to reconstruct broadband speech that is perceptually virtually indistinguishable from true broadband recordings.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-528"
  },
  "seltzer05_interspeech": {
   "authors": [
    [
     "Michael L.",
     "Seltzer"
    ],
    [
     "Alex",
     "Acero"
    ],
    [
     "Jasha",
     "Droppo"
    ]
   ],
   "title": "Robust bandwidth extension of noise-corrupted narrowband speech",
   "original": "i05_1509",
   "page_count": 4,
   "order": 529,
   "p1": "1509",
   "pn": "1512",
   "abstract": [
    "We present a new bandwidth extension algorithm for converting narrowband telephone speech into wideband speech using a transformation in the mel cepstral domain. Unlike previous approaches, the proposed method is designed specifically for bandwidth extension of narrowband speech that has been corrupted by environmental noise. We show that by exploiting previous research in mel cepstrum feature enhancement, we can create a unified probabilistic framework under which the feature denoising and bandwidth extension processes are tightly integrated using a single shared statistical model. By doing so, we are able to both denoise the observed narrowband speech and robustly extend its bandwidth in a jointly optimal manner. A series of experiments on clean and noise-corrupted narrowband speech is performed to validate our approach.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-529"
  },
  "cabral05b_interspeech": {
   "authors": [
    [
     "Joao P.",
     "Cabral"
    ],
    [
     "Luis C.",
     "Oliveira"
    ]
   ],
   "title": "Pitch-synchronous time-scaling for high-frequency excitation regeneration",
   "original": "i05_1513",
   "page_count": 4,
   "order": 530,
   "p1": "1513",
   "pn": "1516",
   "abstract": [
    "The goal of bandwidth extension of speech (BWE) is to extrapolate the missing low or high frequency components of the wide-band speech (50.8000 Hz) based entirely on information contained in a narrow-band signal (300.3400 Hz). In this paper we propose a new method for high-frequency regeneration of the excitation signal, using the correlation between the shape of the glottal flow waveform and the spectrum of the voice source. The high-band excitation is generated by performing a pitch-synchronous timescale (PSTS) transformation on the linear prediction narrow-band residual to generate an high-pass signal that retains the periodic characteristics of the original signal but with a larger open quotient. This method is easy to implement and does not introduce discontinuities in the spectrum of the regenerated excitation. It can be used in applications for BWE where no side information is transmitted or for low bit coding of wide-band speech.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-530"
  },
  "vergyri05_interspeech": {
   "authors": [
    [
     "Dimitra",
     "Vergyri"
    ],
    [
     "Katrin",
     "Kirchhoff"
    ],
    [
     "R.",
     "Gadde"
    ],
    [
     "Andreas",
     "Stolcke"
    ],
    [
     "Jing",
     "Zheng"
    ]
   ],
   "title": "Development of a conversational telephone speech recognizer for Levantine Arabic",
   "original": "i05_1613",
   "page_count": 4,
   "order": 531,
   "p1": "1613",
   "pn": "1616",
   "abstract": [
    "Many languages, including Arabic, are characterized by a wide variety of different dialects that often differ strongly from each other. When developing speech technology for dialect-rich languages, the portability and reusability of data, algorithms, and systemcomponents becomes extremely important. In this paper, we describe the development of a large-vocabulary speech recognition system for Levantine Arabic, which was a new dialectal recognition task for our existing system. We discuss the dialect-specific modeling choices (grapheme vs. phoneme based acoustic models, automatic vowelization techniques, and morphological language models) and investigate to what extent techniques previously tested on other languages are portable to the present task. We present state-of-the-art recognition results on the 2004 Levantine Arabic Rich Transcription evaluation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-531"
  },
  "ramabhadran05_interspeech": {
   "authors": [
    [
     "Bhuvana",
     "Ramabhadran"
    ]
   ],
   "title": "Exploiting large quantities of spontaneous speech for unsupervised training of acoustic models",
   "original": "i05_1617",
   "page_count": 4,
   "order": 532,
   "p1": "1617",
   "pn": "1620",
   "abstract": [
    "While large amounts of manually transcribed acoustic training data is available for well-known large vocabulary speech recognition tasks such as, the transcription of broadcast news and switchboard conversations, a significantly less amount is available for several large spoken collections such as the MALACH corpus (in multiple languages), meeting recordings, presentations at conferences, call center conversations, etc. However, these collections offer vast quantities of untranscribed spontaneous speech that can be used to improve recognition accuracies. Several narrow-band and broadband speech collections are currently available and carefully tuned speech recognition systems trained on several hundred hour of manually transcribed data are now able to achieve word error rates between 10% and 40%, depending on the difficulty of the collection. This paper studies the use of automatically recognized transcriptions at several levels of recognition accuracy to train acoustic models and the performance improvements obtained with such unsupervised training. This paper also proposes a recipe for selection of feature vectors at the utterance, word or fragment level for training acoustic models that provides the maximum gain in recognition accuracy. This paper demonstrates that a reduction in overall word error rate of up to 20% relative can be obtained with careful selection of acoustic training data.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-532"
  },
  "lin05c_interspeech": {
   "authors": [
    [
     "Che-Kuang",
     "Lin"
    ],
    [
     "Lin-Shan",
     "Lee"
    ]
   ],
   "title": "Improved spontaneous Mandarin speech recognition by disfluency interruption point (IP) detection using prosodic features",
   "original": "i05_1621",
   "page_count": 4,
   "order": 533,
   "p1": "1621",
   "pn": "1624",
   "abstract": [
    "In this paper, a new approach for improved spontaneous Mandarin speech recognition with disfluencies well considered is presented. The basic idea is to detect the disfluency interruption points (IPs) prior to the recognition, and then to use these information during rescoring in the recognition process. For accurate detection of disfluency interruption points (IPs), a whole set of new features were proposed and tested by carefully considering the special characteristics of Mandarin Chinese. A new approach of incorporating the decision tree into the maximum entropy model training was also developed to enhance the IP detection accuracy. Experimental results indicated that the proposed set of features and the IP detection approach were very useful, and the obtained information about disfluency actually benefited the speech recognition performance.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-533"
  },
  "ma05c_interspeech": {
   "authors": [
    [
     "Jeff Z.",
     "Ma"
    ],
    [
     "Spyros",
     "Matsoukas"
    ]
   ],
   "title": "Improvements to the BBN RT04 Mandarin conversational telephone speech recognition system",
   "original": "i05_1625",
   "page_count": 4,
   "order": 534,
   "p1": "1625",
   "pn": "1628",
   "abstract": [
    "BBN's 20 times real-time (20xRT) Mandarin conversational telephone speech (CTS) recognition system achieved the lowest character error rate (CER) in the Rich Transcription 2004 fall (RT04F) evaluation conducted by NIST. This paper focuses on the work we have done after the evaluation. The work includes porting of more new acoustic modeling technologies we had developed on English, such as long-span features, a modified HLDA-SAT, etc., diagnoses of the problems we had encountered in the evaluation, such as problems in pitch, silence chopping and automatic segmentation, and solutions we found for the problems. With all these new technologies and problem solutions incorporated and a new design of the 20xRT system architecture we achieved a 2.1% absolute reduction in CER on the RT04 evaluation test set.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-534"
  },
  "sakti05_interspeech": {
   "authors": [
    [
     "Sakriani",
     "Sakti"
    ],
    [
     "Satoshi",
     "Nakamura"
    ],
    [
     "Konstantin",
     "Markov"
    ]
   ],
   "title": "Incorporating a Bayesian wide phonetic context model for acoustic rescoring",
   "original": "i05_1629",
   "page_count": 4,
   "order": 535,
   "p1": "1629",
   "pn": "1632",
   "abstract": [
    "This paper presents a method for improving acoustic model precision by incorporating wide phonetic context units in speech recognition. The wide phonetic context model is constructed from several narrower context-dependent models based on the Bayesian framework. Such a composition is performed in order to avoid the crucial problem of a limited availability of training data and to reduce the model complexity. To enhance the model reliability due to unseen contexts and limited training data, flooring and deleted interpolation techniques are used. Experimental results show that this method gives improvement of the word accuracy with respect to the standard triphone model.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-535"
  },
  "messaoudi05_interspeech": {
   "authors": [
    [
     "Abdel",
     "Messaoudi"
    ],
    [
     "Lori",
     "Lamel"
    ],
    [
     "Jean-Luc",
     "Gauvain"
    ]
   ],
   "title": "Modeling vowels for Arabic BN transcription",
   "original": "i05_1633",
   "page_count": 4,
   "order": 536,
   "p1": "1633",
   "pn": "1636",
   "abstract": [
    "This paper describes the LIMSI Arabic Broadcast News system which produces a vowelized word transcription. The under 10x system, evaluated in the NIST RT-04F evaluation, uses a 3 pass decoding strategy with gender- and bandwidth-specific acoustic models, a vowelized 65k word class pronunciation lexicon and a word-class 4-gram language model. In order to explicitly represent the vowelized word forms, each non-vowelized word entry is considered as a word class regrouping all of its associated vowelized forms.\n",
    "Since Arabic texts are almost exclusively written without vowels, an important challenge is to be able to use these efficiently in a system producing a vowelized output. Since a portion of the acoustic training data was manually transcribed with short vowels, enabling an initial set of acoustic models to be estimated in a supervised manner. The remaining audio data, for which vowels are not annotated, were trained in an implicit manner using the recognizer to choose the preferred form. The system was trained on a total of about 150 hours of audio data and almost 600 million words of Arabic texts, and achieved word error rates of 16.0% and 18.5% on the dev04 and eval04 data, respectively.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-536"
  },
  "afify05b_interspeech": {
   "authors": [
    [
     "Mohamed",
     "Afify"
    ],
    [
     "Long",
     "Nguyen"
    ],
    [
     "Bing",
     "Xiang"
    ],
    [
     "Sherif",
     "Abdou"
    ],
    [
     "John",
     "Makhoul"
    ]
   ],
   "title": "Recent progress in Arabic broadcast news transcription at BBN",
   "original": "i05_1637",
   "page_count": 4,
   "order": 537,
   "p1": "1637",
   "pn": "1640",
   "abstract": [
    "The first part of this paper describes the BBN system that participated in the 2004 broadcast news (BN) evaluation for Arabic. The complete system description is given together with experimental results on the 2004 development, and evaluation sets. Previous Arabic speech recognition at BBN used grapheme models due to the lack of short vowel information in the acoustic transcriptions. In the second part of this paper we show how to build a phonetic system. It is demonstrated that switching to phonetic models is capable of reducing the word error rate by up to 14% relative, for different test sets, compared to the traditional grapheme based approach.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-537"
  },
  "matsoukas05_interspeech": {
   "authors": [
    [
     "Spyros",
     "Matsoukas"
    ],
    [
     "Rohit",
     "Prasad"
    ],
    [
     "Srinivas",
     "Laxminarayan"
    ],
    [
     "Bing",
     "Xiang"
    ],
    [
     "Long",
     "Nguyen"
    ],
    [
     "Richard",
     "Schwartz"
    ]
   ],
   "title": "The 2004 BBN 1xRT recognition systems for English broadcast news and conversational telephone speech",
   "original": "i05_1641",
   "page_count": 4,
   "order": 538,
   "p1": "1641",
   "pn": "1644",
   "abstract": [
    "This paper describes the BBN real-time recognition systems used in the 2004 Rich Transcription (RT) benchmark test for the English Conversational Telephone Speech (CTS) and Broadcast News (BN) tasks. We describe the system architecture, along with the algorithms we used in order to reduce computation with minimal impact on recognition accuracy. Particular choices in the design of the final system are analyzed to show the trade-offs between speed and accuracy. We also present recently developed new architecture for the real-time systems, which outperforms the systems we submitted for the RT04 benchmark tests for both domains.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-538"
  },
  "prasad05b_interspeech": {
   "authors": [
    [
     "Rohit",
     "Prasad"
    ],
    [
     "Spyros",
     "Matsoukas"
    ],
    [
     "C.-L.",
     "Kao"
    ],
    [
     "Jeff Z.",
     "Ma"
    ],
    [
     "D.-X.",
     "Xu"
    ],
    [
     "T.",
     "Colthurst"
    ],
    [
     "O.",
     "Kimball"
    ],
    [
     "Richard",
     "Schwartz"
    ],
    [
     "Jean-Luc",
     "Gauvain"
    ],
    [
     "Lori",
     "Lamel"
    ],
    [
     "Holger",
     "Schwenk"
    ],
    [
     "G.",
     "Adda"
    ],
    [
     "F.",
     "Lefevre"
    ]
   ],
   "title": "The 2004 BBN/LIMSI 20xRT English conversational telephone speech recognition system",
   "original": "i05_1645",
   "page_count": 4,
   "order": 539,
   "p1": "1645",
   "pn": "1648",
   "abstract": [
    "In this paper we describe the English Conversational Telephone Speech (CTS) recognition system jointly developed by BBN and LIMSI under the DARPA EARS program for the 2004 evaluation conducted by NIST. The 2004 BBN/LIMSI system achieved a word error rate (WER) of 13.5% at 18.3xRT (real-time as measured on Pentium 4 Xeon 3.4 GHz Processor) on the EARS progress test set. This translates into a 22.8% relative improvement in WER over the 2003 BBN/LIMSI EARS evaluation system, which was run without any time constraints. In addition to reporting on the system architecture and the evaluation results, we also highlight the significant improvements made at both sites.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-539"
  },
  "xiang05_interspeech": {
   "authors": [
    [
     "Bing",
     "Xiang"
    ],
    [
     "Long",
     "Nguyen"
    ],
    [
     "Xuefeng",
     "Guo"
    ],
    [
     "Dongxin",
     "Xu"
    ]
   ],
   "title": "The BBN Mandarin broadcast news transcription system",
   "original": "i05_1649",
   "page_count": 4,
   "order": 540,
   "p1": "1649",
   "pn": "1652",
   "abstract": [
    "In this paper, we present the state-of-the-art BBN Mandarin Broadcast News (BN) transcription system that participated in the EARS Rich Transcription evaluations. As briefly mentioned in the literature before, the BBN 2003 evaluation system achieved 47% relative improvement compared to the baseline, a significant reduction in recognition errors. Since then the system performance has been improved by another 16%, mainly due to the additional acoustic training data selected via light supervision, automatically downloaded language training data, an enhanced phoneme set and a better algorithm for pitch extraction. The current system achieves 6.3% character error rate (CER) on the EARS 2003 and 2004 development test sets, running at four times real time (4×RT).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-540"
  },
  "deleglise05_interspeech": {
   "authors": [
    [
     "Paul",
     "Deléglise"
    ],
    [
     "Yannick",
     "Estève"
    ],
    [
     "Sylvain",
     "Meignier"
    ],
    [
     "Teva",
     "Merlin"
    ]
   ],
   "title": "The LIUM speech transcription system: a CMU Sphinx III-based system for French broadcast news",
   "original": "i05_1653",
   "page_count": 4,
   "order": 541,
   "p1": "1653",
   "pn": "1656",
   "abstract": [
    "This paper presents the system used by the LIUM to participate in ESTER, the french broadcast news evaluation campaign. This system is based on the CMU Sphinx 3.3 (fast) decoder. Some tools are presented which have been added on different steps of the Sphinx recognition process: segmentation, acoustic model adaptation, word-lattice rescoring.\n",
    "Several experiments have been conducted on studying the effects of the signal segmentation on the recognition process, on injecting automatically transcribed data into training corpora, or on testing different approaches for acoustic model adaptation. The results are presented in this paper.\n",
    "With very few modifications and a simple MAP acoustic model estimation, Sphinx3.3 decoder reached a word error rate of 28.2%. The entire system developed by LIUM obtained 23.6% as official word error rate for the ESTER evaluation, and 23.4% as result of an unsubmitted system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-541"
  },
  "lamel05_interspeech": {
   "authors": [
    [
     "Lori",
     "Lamel"
    ],
    [
     "G.",
     "Adda"
    ],
    [
     "E.",
     "Bilinski"
    ],
    [
     "Jean-Luc",
     "Gauvain"
    ]
   ],
   "title": "Transcribing lectures and seminars",
   "original": "i05_1657",
   "page_count": 4,
   "order": 542,
   "p1": "1657",
   "pn": "1660",
   "abstract": [
    "This paper describes recent research carried out in the context of the FP6 Integrated Project Chil in developing a system to automatically transcribe lectures and seminars. We made use of widely available corpora to train both the acoustic and language models, since only a small amount of Chil data were available for system development. For acoustic model training made use of the transcribed portion of the TED corpus of Eurospeech recordings, as well as the ICSI, ISL, and NIST meeting corpora. For language model training, text materials were extracted from a variety of on-line conference proceedings. Word error rates of about 25% are obtained on test data extracted 12 seminars.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-542"
  },
  "hain05_interspeech": {
   "authors": [
    [
     "Thomas",
     "Hain"
    ],
    [
     "John",
     "Dines"
    ],
    [
     "Giulia",
     "Garau"
    ],
    [
     "Martin",
     "Karafiát"
    ],
    [
     "Darren",
     "Moore"
    ],
    [
     "Vincent",
     "Wan"
    ],
    [
     "Roeland",
     "Ordelman"
    ],
    [
     "Steve",
     "Renals"
    ]
   ],
   "title": "Transcription of conference room meetings: an investigation",
   "original": "i05_1661",
   "page_count": 4,
   "order": 543,
   "p1": "1661",
   "pn": "1664",
   "abstract": [
    "The automatic processing of speech collected in conference style meetings has attracted considerable interest with several large scale projects devoted to this area. In this paper we explore the use of various meeting corpora for the purpose of automatic speech recognition. In particular we investigate the similarity of these resources and how to efficiently use them in the construction of a meeting transcription system. The analysis shows distinctive features for each resource. However the benefit in pooling data and hence the similarity seems sufficient to speak of a generic \"conference meeting domain\". In this context this paper also presents work on development for the AMI meeting transcription system, a joint effort by seven sites working on the AMI (augmented multi-party interaction) project.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-543"
  },
  "gauvain05_interspeech": {
   "authors": [
    [
     "Jean-Luc",
     "Gauvain"
    ],
    [
     "G.",
     "Adda"
    ],
    [
     "Martine",
     "Adda-Decker"
    ],
    [
     "Alexandre",
     "Allauzen"
    ],
    [
     "V.",
     "Gendner"
    ],
    [
     "Lori",
     "Lamel"
    ],
    [
     "Holger",
     "Schwenk"
    ]
   ],
   "title": "Where are we in transcribing French broadcast news?",
   "original": "i05_1665",
   "page_count": 4,
   "order": 544,
   "p1": "1665",
   "pn": "1668",
   "abstract": [
    "Given the high flexional properties of the French language, transcribing French broadcast news (BN) is more challenging than English BN. This is in part due to the large number of homophones in the inflected forms. This paper describes advances in automatic processing of broadcast news speech in French based on recent improvements to the LIMSI English system. The main differences between the English and French BN systems are: a 200k vocabulary to overcome the lower lexical coverage in French (including contextual pronunciations to model liaisons), a case sensitive language model, and the use of a POS based language model to lower the impact of homophonic gender and number disagreement. The resulting system was evaluated in the first French TECHNOLANGUE-ESTER ASR benchmark test. This system achieved the lowest word error rate in this evaluation by a significant margin. We also report on a 1xRT version of this system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-544"
  },
  "scharenborg05b_interspeech": {
   "authors": [
    [
     "Odette",
     "Scharenborg"
    ],
    [
     "Stephanie",
     "Seneff"
    ]
   ],
   "title": "Two-pass strategy for handling OOVs in a large vocabulary recognition task",
   "original": "i05_1669",
   "page_count": 4,
   "order": 545,
   "p1": "1669",
   "pn": "1672",
   "abstract": [
    "This paper addresses the issue of large-vocabulary recognition in a specific word class. We propose a two-pass strategy in which only major cities are explicitly represented in the first stage lexicon. An unknown word model encoded as a phone loop is used to detect OOV city names (referred to as rare city names). After which SpeM, a tool that can extract words and word-initial cohorts from phone graphs on the basis of a large fallback lexicon, provides an N-best list of promising city names on the basis of the phone sequences generated in the first stage. This N-best list is then inserted into the second stage lexicon for a subsequent recognition pass. Experiments were conducted on a set of spontaneous telephone-quality utterances each containing one rare city name. We tested the size of the N-best list and three types of language models (LMs). The experiments showed that SpeM was able to include nearly 85% of the correct city names into an N-best list of 3000 city names when a unigram LM, which also boosted the unigram scores of a city name in a given state, was used.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-545"
  },
  "nguyen05_interspeech": {
   "authors": [
    [
     "Long",
     "Nguyen"
    ],
    [
     "Bing",
     "Xiang"
    ],
    [
     "Mohamed",
     "Afify"
    ],
    [
     "Sherif",
     "Abdou"
    ],
    [
     "Spyros",
     "Matsoukas"
    ],
    [
     "Richard",
     "Schwartz"
    ],
    [
     "John",
     "Makhoul"
    ]
   ],
   "title": "The BBN RT04 English broadcast news transcription system",
   "original": "i05_1673",
   "page_count": 4,
   "order": 546,
   "p1": "1673",
   "pn": "1676",
   "abstract": [
    "This paper describes the BBN English Broadcast News transcription system developed for the EARS Rich Transcription 2004 (RT04) evaluation. In comparison to the BBN RT03 system, we achieved around 22% relative reduction in word error rate for all EARS BN development test sets. The use of additional acoustic training data acquired through Light Supervision based on thousands of hours of found data made the biggest contribution to the improvement. Better audio segmentation, through the use of an online speaker clustering algorithm and chopping speaker turns into moderately long utterances, also contributed substantially to the improvement. Other contributions, even of modest size but adding up nicely, include using discriminative training for all acoustic models, using word duration as an additional knowledge source during N-best rescoring, and using updated lexicon and language models.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-546"
  },
  "zhang05c_interspeech": {
   "authors": [
    [
     "Rong",
     "Zhang"
    ],
    [
     "Ziad Al",
     "Bawab"
    ],
    [
     "Arthur",
     "Chan"
    ],
    [
     "Ananlada",
     "Chotimongkol"
    ],
    [
     "David",
     "Huggins-Daines"
    ],
    [
     "Alexander I.",
     "Rudnicky"
    ]
   ],
   "title": "Investigations on ensemble based semi-supervised acoustic model training",
   "original": "i05_1677",
   "page_count": 4,
   "order": 547,
   "p1": "1677",
   "pn": "1680",
   "abstract": [
    "Semi-supervised learning has been recognized as an effective way to improve acoustic model training in cases where sufficient transcribed data are not available. Different from most of existing approaches only using single acoustic model and focusing on how to refine it, this paper investigates the feasibility of using ensemble methods for semi-supervised acoustic modeling training. Two methods are investigated here, one is a generalized Boosting algorithm, a second one is based on data partitions. Both methods demonstrate substantial improvement over baseline. More than 15% relative reduction of word error rate was observed in our experiments using a large real-world meeting recognition dataset.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-547"
  },
  "nouza05_interspeech": {
   "authors": [
    [
     "Jan",
     "Nouza"
    ],
    [
     "Jindrich",
     "Zdánský"
    ],
    [
     "Petr",
     "David"
    ],
    [
     "Petr",
     "Cerva"
    ],
    [
     "Jan",
     "Kolorenc"
    ],
    [
     "Dana",
     "Nejedlová"
    ]
   ],
   "title": "Fully automated system for Czech spoken broadcast transcription with very large (300k+) lexicon",
   "original": "i05_1681",
   "page_count": 4,
   "order": 548,
   "p1": "1681",
   "pn": "1684",
   "abstract": [
    "We present a system developed for fully automated processing of Czech spoken broadcast programs. It includes modules for unsupervised segmentation of audio stream, speaker and gender recognition followed by speaker adaptation, and own speech decoder designed for extremely large vocabularies. Compared to our previous results reported in 2004, the new system reduced the WER (evaluated on the Czech part of the European COST Broadcast News Database) from 28.5% to 18.4%. This significant improvement was accomplished namely due to the larger lexicon (312K) with multiple text and pronunciation variants and multi-word entries, speaker and gender adapted acoustic matching and improved language modeling. Besides the results achieved in the Broadcast News task we refer also about the performance in other similar jobs, like the transcription of a talk show or parliament speech.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-548"
  },
  "schuster05_interspeech": {
   "authors": [
    [
     "Mike",
     "Schuster"
    ],
    [
     "Takaaki",
     "Hori"
    ],
    [
     "Atsushi",
     "Nakamura"
    ]
   ],
   "title": "Experiments with probabilistic principal component analysis in LVCSR",
   "original": "i05_1685",
   "page_count": 4,
   "order": 549,
   "p1": "1685",
   "pn": "1688",
   "abstract": [
    "This paper describes the application of Mixtures of Probabilistic Principal Component Analyzers (MPPCA) for modeling the observation distributions in a speech recognition system. The MPPCA model is a mixture of Gaussians with a constrained covariance approximating a full covariance with less effective parameters whose complexity can be controlled by the user. The paper summarizes the necessary basics of the MPPCA model, describes a simple extension of the basic model to set the user-defined complexity of the constrained covariance in a more automatic way and describes how to deal with numerical problems occurring for typical speech recognition systems. The MPPCA model is tested against a diagonal covariance and a full covariance model for our so far best acoustic model with 5000 quinphone clustered states and 80/160k Gaussians total on a large, spontaneous Japanese speech task. Results show that we can improve error rates on the standard test set from 22.4% to 19.8% by moving to full covariances. For several MPPCA models tested we reach the same error rates with less effective parameters but fail to significantly improve over using full covariances, for which possible reasons are discussed.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-549"
  },
  "vu05_interspeech": {
   "authors": [
    [
     "Thang Tat",
     "Vu"
    ],
    [
     "Dung Tien",
     "Nguyen"
    ],
    [
     "Mai Chi",
     "Luong"
    ],
    [
     "John-Paul",
     "Hosom"
    ]
   ],
   "title": "Vietnamese large vocabulary continuous speech recognition",
   "original": "i05_1689",
   "page_count": 4,
   "order": 550,
   "p1": "1689",
   "pn": "1692",
   "abstract": [
    "This paper presents an early study on building Vietnamese large vocabulary continuous speech recognition with concentration on choosing type of units and feature set. Our experiments were done using the HTK Toolkit and VOV broadcast corpus. The results show that the recognizer with mixture units achieved better performance than recognizers with initial-final units and phoneme units. Among feature sets are applied, MFCC has performance somewhat better than PLP, and the combination of MFCC and F0 features increases the accuracy of the Vietnamese recognition system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-550"
  },
  "shinozaki05_interspeech": {
   "authors": [
    [
     "Takahiro",
     "Shinozaki"
    ],
    [
     "Mari",
     "Ostendorf"
    ],
    [
     "Les",
     "Atlas"
    ]
   ],
   "title": "Data sampling for improved speech recognizer training",
   "original": "i05_1693",
   "page_count": 4,
   "order": 551,
   "p1": "1693",
   "pn": "1696",
   "abstract": [
    "Proper data selection for training a speech recognizer can be important for reducing costs of developing systems on new tasks and exploratory experiments, but it is also useful for efficient leveraging of the increasingly large speech resources available for training large vocabulary systems. In this work, we investigate various sampling methods, comparing the likelihood criterion to new acoustic measures motivated by work in child language acquisition. The acoustic criteria can be used with or without preexisting transcriptions or models. When applied to the problem of selecting a small training set, the best results are obtained using modulation spectrum features and a discriminant function trained on child vs. adult-directed speech. For large corpora, none of the methods outperforms random sampling, but reduced training costs are obtained by using multistage training and initializing with the small corpus.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-551"
  },
  "levow05_interspeech": {
   "authors": [
    [
     "Gina-Anne",
     "Levow"
    ]
   ],
   "title": "Context in multi-lingual tone and pitch accent recognition",
   "original": "i05_1809",
   "page_count": 4,
   "order": 552,
   "p1": "1809",
   "pn": "1812",
   "abstract": [
    "Tone and intonation play a crucial role across many languages. However, the use and structure of tone varies widely, ranging from lexical tone which determines word identity to pitch accent signalling information status. In this paper, we employ a uniform representation of acoustic features for recognition of both Mandarin tone and English pitch accent. The representation captures both local tone height and shape as well as contextual coarticulatory and phrasal influences. By exploiting multiclass Support Vector Machines as a discriminative classifier, we achieve competitive rates of tone and pitch accent recognition. We further demonstrate the greater importance of modeling preceding local context, which yields up to 24% reduction in error over modeling the following context.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-552"
  },
  "tamburini05_interspeech": {
   "authors": [
    [
     "Fabio",
     "Tamburini"
    ]
   ],
   "title": "Automatic prominence identification and prosodic typology",
   "original": "i05_1813",
   "page_count": 4,
   "order": 553,
   "p1": "1813",
   "pn": "1816",
   "abstract": [
    "This paper presents a follow up of a study on the automatic detection of prosodic prominence in continuous speech. Prosodic prominence involves two different prosodic features, pitch accent and stress, that are typically based on four acoustic parameters: fundamental frequency (F0) movements, overall syllable energy, syllable nuclei duration and mid-to-high-frequency emphasis. A careful measurement of these acoustic parameters, as well as the identification of their connection to prosodic parameters, makes it possible to build an automatic system capable of identifying prominent syllables in utterances with performance comparable with the inter-human agreement reported in the literature. This automatic system has been used to cast light on the actual correlation among the acoustic parameters and the prominence phenomenon from an typological point of view, by examining data derived from some stress-accented languages.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-553"
  },
  "ingulfsen05_interspeech": {
   "authors": [
    [
     "Tommy",
     "Ingulfsen"
    ],
    [
     "Tina",
     "Burrows"
    ],
    [
     "Sabine",
     "Buchholz"
    ]
   ],
   "title": "Influence of syntax on prosodic boundary prediction",
   "original": "i05_1817",
   "page_count": 4,
   "order": 554,
   "p1": "1817",
   "pn": "1820",
   "abstract": [
    "We compare the effectiveness of different syntactic features and representations for prosodic boundary prediction, setting out to clarify which representations are most suitable for this task. We took a machine learning approach, and ran a series of eight experiments. The results show that the representations have different strengths and that a combination yields the best result. We also find that linguistically deep representations do not yield clearly superior classifiers compared to classifiers obtained by extraction of shallow features.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-554"
  },
  "gretter05_interspeech": {
   "authors": [
    [
     "Roberto",
     "Gretter"
    ],
    [
     "Dino",
     "Seppi"
    ]
   ],
   "title": "Using prosodic information for disambiguation purposes",
   "original": "i05_1821",
   "page_count": 4,
   "order": 555,
   "p1": "1821",
   "pn": "1824",
   "abstract": [
    "In this work, we describe how prosodic information can be employed to improve the performance of an Automatic Speech Recognizer (ASR) for specific restricted tasks. The approach exploits additional prosodic information in a post-processing stage. Prosodic features are estimated at word level; this additional information is encoded through a feature extractor and is then modeled using a statistical classifier. To train and test this system we collected an Italian database designed to comprise specific dialogue problems like ambiguous utterances. The proposed system yields a 69.5% relative word error rate reduction compared to a traditional state-of-the-art recognizer for the task of recognizing sequences of numbers.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-555"
  },
  "gu05_interspeech": {
   "authors": [
    [
     "Wentao",
     "Gu"
    ],
    [
     "Keikichi",
     "Hirose"
    ],
    [
     "Hiroya",
     "Fujisaki"
    ]
   ],
   "title": "Analysis of the effects of word emphasis and echo question on F0 contours of Cantonese utterances",
   "original": "i05_1825",
   "page_count": 4,
   "order": 556,
   "p1": "1825",
   "pn": "1828",
   "abstract": [
    "Word emphasis and question intonation have significant effects on F0 contours for various languages. This paper employs the command-response F0 model to investigate the effects of the two factors for Cantonese, a typical tone language with nine tones. Analysis shows that the main effect of word emphasis associated with narrow focus is not on tone command but on phrase command. An additional or a larger phrase command tends to occur immediately preceding the emphasized word. For the tone command of the emphasized word, duration may be lengthened but polarity and amplitude are hardly affected so that the inherent tone patterns are kept. Also, echo question will replace the tone command in the later part of the sentence-final syllable by a positive tone command of much larger amplitude than usual. Both the effects of the two factors can be embedded in the framework of our model.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-556"
  },
  "burrows05_interspeech": {
   "authors": [
    [
     "Tina",
     "Burrows"
    ],
    [
     "Peter",
     "Jackson"
    ],
    [
     "Katherine",
     "Knill"
    ],
    [
     "Dmitry",
     "Sityaev"
    ]
   ],
   "title": "Combining models of prosodic phrasing and pausing",
   "original": "i05_1829",
   "page_count": 4,
   "order": 557,
   "p1": "1829",
   "pn": "1832",
   "abstract": [
    "This paper describes two approaches to assigning prosodic phrase structure and pauses to text and investigates the impact of errors in the assignments for different granularities of prosodic phrase structure. One approach uses a cascaded combination of models trained separately for prediction of prosodic phrase structure and pauses and the other uses a model trained for the joint prediction task directly. Objective measurements show similar performance for both approaches while perceptual evaluations show a slight preference for an optimised cascaded combination of prosodic phrase structure and pause models using a single-level encoding of prosodic phrase structure.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-557"
  },
  "hirst05b_interspeech": {
   "authors": [
    [
     "Daniel",
     "Hirst"
    ],
    [
     "Cyril",
     "Auran"
    ]
   ],
   "title": "Analysis by synthesis of speech prosody: the Prozed environment",
   "original": "i05_3225",
   "page_count": 4,
   "order": 558,
   "p1": "3225",
   "pn": "3228",
   "abstract": [
    "This paper presents ProZed, an environment for the multilingual analysis by synthesis of speech prosody. The analysis is based on the symbolic representation of prosodic form without reference to prosodic function. The parameters of the model are at present limited to fundamental frequency and duration but the same framework could be extended to accomodate other parameters such as spectral tilt or voice quality. Each parameter is defined with respect to specific domains and units for long-term and shortterm phonetic interpretation with an abstract annotation system corresponding to a level of surface phonological representation. The environment is integrated with the Praat program for analysis and the Mbrola program for synthesis.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-558"
  },
  "cox05_interspeech": {
   "authors": [
    [
     "Stephen",
     "Cox"
    ]
   ],
   "title": "A discriminative approach to phrase break modelling",
   "original": "i05_3229",
   "page_count": 4,
   "order": 559,
   "p1": "3229",
   "pn": "3232",
   "abstract": [
    "We address the problem of predicting pauses between the words in a sentence, which is of considerable interest for text to speech systems. In doing so, we show that the performance of both a generative classifier (naive Bayes, NB) and a discriminative classifier (maximum entropy, ME) can be significantly enhanced by application of the generalised probabilistic descent (GPD) algorithm. The features used for prediction of pauses in sentences are both local (derived from the neighbourhood of a word juncture) and global (derived from a parse tree of the sentence). We first compare the results of using the NB and ME classifiers on these features, and then develop the theory required for applying GPD to these classifiers. We show that GPD is particularly suitable for application within the maximum entropy framework and increases very significantly the discriminative power of both the NB and ME classifiers. The F-score of 81.2% obtained after application of GPD to an ME classifier is believed to be the best performance obtained on the Boston Radio Corpus.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-559"
  },
  "read05_interspeech": {
   "authors": [
    [
     "Ian",
     "Read"
    ],
    [
     "Stephen",
     "Cox"
    ]
   ],
   "title": "Stochastic and syntactic techniques for predicting phrase breaks",
   "original": "i05_3233",
   "page_count": 4,
   "order": 560,
   "p1": "3233",
   "pn": "3236",
   "abstract": [
    "Determining the position of breaks in a sentence is a key task for a text-to-speech (TTS) system. We describe some methods for phrase break prediction in which the whole sentence is considered, in contrast to most previous work which has focused on using local features. Three approaches are described: by analogy, where the breaks from the best-matching sentence in our training data is used for the unseen sentence; by phrase modelling, in which we build stochastic models of phrases to segment unseen sentences; and finally, using features derived from a syntactic parse tree. Our best result, obtained on the MARSEC corpus and using a combination of parse derived features and a local feature, gave an F score of 81.6%, which we believe to be the highest published on this dataset.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-560"
  },
  "xydas05_interspeech": {
   "authors": [
    [
     "Gerasimos",
     "Xydas"
    ],
    [
     "Panagiotis",
     "Zervas"
    ],
    [
     "Georgios",
     "Kouroupetroglou"
    ],
    [
     "Nikolaos",
     "Fakotakis"
    ],
    [
     "George",
     "Kokkinakis"
    ]
   ],
   "title": "Tree-based prediction of prosodic phrase breaks on top of shallow textual features",
   "original": "i05_3237",
   "page_count": 4,
   "order": 561,
   "p1": "3237",
   "pn": "3240",
   "abstract": [
    "This paper reports on the evaluation of automatic prosodic phrase break assignment. We utilize two tree-structured predictors, the commonly used CART and a C4.5, to predict break placement from sequences of easily to extract shallow textual features. We are experimenting with two 500-utterance prosodic corpora developed by two Greek universities that originate from different domains in order to focus on the differences in prediction between generic and limited domain datasets. The evaluation shows that while the limited dataset achieves better accuracy than the generic one in the CART case, this difference is lowered with the introduction of C4.5. Minor breaks proved to be the most difficult class to predict in CART case, while we achieved a 50% improvement with C4.5.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-561"
  },
  "dong05b_interspeech": {
   "authors": [
    [
     "Honghui",
     "Dong"
    ],
    [
     "Jianhua",
     "Tao"
    ],
    [
     "Bo",
     "Xu"
    ]
   ],
   "title": "Chinese prosodic phrasing with a constraint-based approach",
   "original": "i05_3241",
   "page_count": 4,
   "order": 562,
   "p1": "3241",
   "pn": "3244",
   "abstract": [
    "The linguistic constraints and phrase-length constraints are the most important factors for Chinese prosodic phrasing in the natural speech. This paper presents a linguistic constraint model and a phrase-length constraint model to describe these two processes independently. Therefore, each part can be described in detail. In the linguistic constraints model, Chunk (base phrase) is considered as an important basic unit. And an HMM is used to model the phrase-length constraints, which concerns the distribution of the prosodic phrase lengths and the relationship between the prosodic phrase and the prosodic words. Then a k-candidate method is introduced to combine these two models. This approach makes full use of the linguistic constraints and the phrase-length constraints. The experiments show that this approach achieved a perfect performance with the phrasing f-score 82.9%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-562"
  },
  "dong05c_interspeech": {
   "authors": [
    [
     "Minghui",
     "Dong"
    ],
    [
     "Kim-Teng",
     "Lua"
    ],
    [
     "Haizhou",
     "Li"
    ]
   ],
   "title": "A probabilistic approach to prosodic word prediction for Mandarin Chinese TTS",
   "original": "i05_3245",
   "page_count": 4,
   "order": 563,
   "p1": "3245",
   "pn": "3248",
   "abstract": [
    "Prosodic word is a basic rhythmic unit of Mandarin Chinese Speech. It is one of the most important factors determining the naturalness of the generated speech by a TTS system. This paper investigates the problem of predicting Chinese prosodic words from word sequence. First, we examine the patterns of Chinese prosodic words and investigate the key features for prediction. Then a baseline model of CART is used. Based on this model, the effects of the number of POS categories and the number of single word categories are investigated. Finally, a Markov chain approach is proposed. This model has the advantages of both CART approach and other statistical approaches, while the drawbacks of those approaches are avoided. Experiment shows that the proposed Markov chain approach outperforms the simple CART approach.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-563"
  },
  "teixeira05_interspeech": {
   "authors": [
    [
     "João Paulo",
     "Teixeira"
    ],
    [
     "Diamantino",
     "Freitas"
    ],
    [
     "Hiroya",
     "Fujisaki"
    ]
   ],
   "title": "Evaluation of a system for F0 contour prediction for european Portuguese",
   "original": "i05_3249",
   "page_count": 4,
   "order": 564,
   "p1": "3249",
   "pn": "3252",
   "abstract": [
    "This paper presents the evaluation of a system for speech F0 contour prediction for European Portuguese using the Fujisaki model. It is composed of two command-generating subsystems, the phrase command sub-system and the accent command sub-system. The parameters for evaluating the ability of each sub-system are described. A comparison is made between original and predicted F0 contours. Finally, the results of a perceptual test are discussed.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-564"
  },
  "li05d_interspeech": {
   "authors": [
    [
     "Ke",
     "Li"
    ],
    [
     "Yoshinori",
     "Sagisaka"
    ]
   ],
   "title": "Analysis on command sequences of a F0 generation model for Mandarin speech and its application to their automatic extraction",
   "original": "i05_3253",
   "page_count": 4,
   "order": 565,
   "p1": "3253",
   "pn": "3256",
   "abstract": [
    "In this paper, we report on Mandarin F0 characteristics analyses using an F0 generation model and present experimental results on the automatic extraction of their control parameters. To cope with difficulties in automatic extraction of control parameters for F0 generation model proposed by Fujisaki, generation command sequences were extracted and analyzed for two-syllable and threesyllable Mandarin speech samples. The analyses showed that there were only restricted number of command sequences and that they were well interpreted by command level merging characteristics. The use of these generation characteristics was turned out to be quite useful for the automatic extraction of F0 control parameters through the automatic extraction experiments for the same twosyllable and three-syllable Mandarin speech samples.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-565"
  },
  "hirose05_interspeech": {
   "authors": [
    [
     "Keikichi",
     "Hirose"
    ],
    [
     "Yusuke",
     "Furuyama"
    ],
    [
     "Nobuaki",
     "Minematsu"
    ]
   ],
   "title": "Corpus-based extraction of F0 contour generation process model parameters",
   "original": "i05_3257",
   "page_count": 4,
   "order": 566,
   "p1": "3257",
   "pn": "3260",
   "abstract": [
    "A corpus-based method was developed for automatic extraction of the F0 contour generation process model parameters (phrase and accent commands). The method first smoothes the observed F0 contour by a piecewise 3rd order polynomial function and finds points of inflection. Then several parameters related to the points are used as input parameters for the predictor of the model commands. Finally the predicted commands are tuned to the observed contour by the analysis-by-synthesis. An experiment was conducted using ATR 504 sentence speech corpus, and the performance close to a rule-based method, also developed by the authors, was obtained. An experiment was further conducted by adding linguistic information of the content of the utterance (such as accent type, depth of bunsetsu boundary) to input parameters. The performance was largely improved; the extraction rates reached around 90% for phrase commands and 84% for accent commands.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-566"
  },
  "escudero05_interspeech": {
   "authors": [
    [
     "David",
     "Escudero"
    ],
    [
     "Valentín",
     "Cardeñoso-Payo"
    ]
   ],
   "title": "Optimized selection of intonation dictionaries in corpus based intonation modelling",
   "original": "i05_3261",
   "page_count": 4,
   "order": 567,
   "p1": "3261",
   "pn": "3264",
   "abstract": [
    "Data scarcity in corpus-based intonation modelling for TTS applications is addressed. We propose to apply a searching process to a list of dictionaries of classes of intonation patterns previously trained from corpus to avoid problems associated with the scarce number of samples in the classes. Results indicate that better results are obtained in comparison with previous alternatives where the probability of predicting a less representative intonation pattern has been shown to be higher.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-567"
  },
  "sun05_interspeech": {
   "authors": [
    [
     "Qinghua",
     "Sun"
    ],
    [
     "Keikichi",
     "Hirose"
    ],
    [
     "Wentao",
     "Gu"
    ],
    [
     "Nobuaki",
     "Minematsu"
    ]
   ],
   "title": "Generation of fundamental frequency contours for Mandarin speech synthesis based on tone nucleus model",
   "original": "i05_3265",
   "page_count": 4,
   "order": 568,
   "p1": "3265",
   "pn": "3268",
   "abstract": [
    "A new method for generating sentence F0 contours of Mandarin speech is proposed. The method assumes the F0 contour generation process model, but generates the tone and phrase components in different ways and sums them to produce a sentence F0 contour. The tone component is generated concatenating F0 patterns of tone nuclei, which are predicted by a corpus-based scheme (binary decision trees). Experiments of F0 contour generation were conducted by using 100 news utterances by a female speaker. The results showed that the method could generate F0 contours close to those of target speech. A perceptual evaluation was also conducted on the synthetic speech using the F0 contours generated by the method. An average score of 4.5 in a 5-point scale indicates the high naturalness, verifying the validity of the method.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-568"
  },
  "chiang05b_interspeech": {
   "authors": [
    [
     "Chen-Yu",
     "Chiang"
    ],
    [
     "Yih-Ru",
     "Wang"
    ],
    [
     "Sin-Horng",
     "Chen"
    ]
   ],
   "title": "On the inter-syllable coarticulation effect of pitch modeling for Mandarin speech",
   "original": "i05_3269",
   "page_count": 4,
   "order": 569,
   "p1": "3269",
   "pn": "3272",
   "abstract": [
    "In this paper, a new statistics-based pitch model for Mandarin speech is proposed. The model considers three major affecting factors on the syllable pitch contour, including lexical tone, prosodic state and inter-syllable coarticulation effect. The study emphasizes on the modeling of inter-syllable coarticulation effect. Interactive affections of neighboring tones and different inter-syllable coarticulation states are considered. Experimental results show that the model performed well even for connected pitch contours of several syllables. Automatic labeling of linguistically meaningful coarticulation states is an extra benefit. So it is a promising F0 model.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-569"
  },
  "rojc05_interspeech": {
   "authors": [
    [
     "Matej",
     "Rojc"
    ],
    [
     "Pablo Daniel",
     "Aguero"
    ],
    [
     "Antonio",
     "Bonafonte"
    ],
    [
     "Zdravko",
     "Kacic"
    ]
   ],
   "title": "Training the tilt intonation model using the JEMA methodology",
   "original": "i05_3273",
   "page_count": 4,
   "order": 570,
   "p1": "3273",
   "pn": "3276",
   "abstract": [
    "This paper focuses on the estimation of the Tilt intonation model [1]. Usually, Tilt events are detected using a first estimation which is improved using gradient descent techniques. To speed up the search we propose to use a closed form expression for some of the Tilt parameters. The gradient descent search is used only for the time related parameters because a close expression cannot be found. Furthermore, the original Tilt proposal estimates the Tilt events sentence by sentence. Here we propose to estimate the events of the whole training corpus at the same time, using what we call the JEMA methodology. This approach increases the consistency of the estimation producing better intonation models. It has been tested on two different languages: Slovenian and Spanish. The experimental results reveal that the Tilt model is appropriate for these languages and that the JEMA methodology produces better prosodic models.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-570"
  },
  "wang05h_interspeech": {
   "authors": [
    [
     "Dagen",
     "Wang"
    ],
    [
     "Shrikanth",
     "Narayanan"
    ]
   ],
   "title": "Piecewise linear stylization of pitch via wavelet analysis",
   "original": "i05_3277",
   "page_count": 4,
   "order": 571,
   "p1": "3277",
   "pn": "3280",
   "abstract": [
    "In this paper, we propose a wavelet analysis based piecewise linear stylization of the pitch trajectory. We also address the often-faced difficulty in handling the tradeoff between mean squared error and the number of lines used for fitting, where a heuristic approach is typically used to make the stylization choice. We pose the piecewise linear stylization task as a minimization problem by defining a penalty function that is a linear combination of the stylization mean squared error and line number to seek an optimal tradeoff. The weights for the penalty function are selected in a semi-supervised way using a development set. We also provide an objective statistical measure based on such penalty function for evaluating the performance of the stylization problem. Results show that our algorithm provides 16.7% penalty reduction than the baseline system based on heuristics. Also, we found that the wavelet decomposition combination selection approach outperforms the low pass level selection approach by 15.6%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-571"
  },
  "romsdorfer05_interspeech": {
   "authors": [
    [
     "Harald",
     "Romsdorfer"
    ],
    [
     "Beat",
     "Pfister"
    ]
   ],
   "title": "Phonetic labeling and segmentation of mixed-lingual prosody databases",
   "original": "i05_3281",
   "page_count": 4,
   "order": 572,
   "p1": "3281",
   "pn": "3284",
   "abstract": [
    "An automatic system for segmenting speech signals used for the training of statistical prosody models is presented. Starting from a canonical transcription, the system simultaneously delivers an accurate phonetic segmentation and the matched phonetic transcription indicating pronunciation variants.\n",
    "Although the system is HMM-based, it uses only the speech signals of the prosody database which typically consists of a few hundred sentences with some 30 minutes total duration. Initial phone HMMs are generated with flat-start training using the canonical transcriptions of the sentences. Then iterative Viterbi search for best-matching pronunciation variants and HMM retraining is applied until convergence is attained.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-572"
  },
  "morais05_interspeech": {
   "authors": [
    [
     "Edmilson",
     "Morais"
    ],
    [
     "Fábio",
     "Violaro"
    ]
   ],
   "title": "Exploratory analysis of linguistic data based on genetic algorithm for robust modeling of the segmental duration of speech",
   "original": "i05_3285",
   "page_count": 4,
   "order": 573,
   "p1": "3285",
   "pn": "3288",
   "abstract": [
    "This work presents a new method for exploratory analysis of linguistic data. This new method is based on Genetic Algorithm and it is used to improve the performance of linear regression models for predicting the segmental duration of speech. The proposed method was compared with Regression Trees and with a baseline Linear Regression model (a Linear Regression with topologies selected using multivariate analysis of variance). The experimental results has shown that the proposed method presents better generalization performance (properties to deal with database imbalance) than the Regression Trees and the baseline Linear Regression model. All the evaluations presented in this article were carried out using an American English database from the Toshiba Speech Technology Laboratory in Cambridge, UK.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-573"
  },
  "gibbon05_interspeech": {
   "authors": [
    [
     "Dafydd",
     "Gibbon"
    ],
    [
     "Flaviane Romani",
     "Fernandes"
    ]
   ],
   "title": "Annotation-mining for rhythm model comparison in Brazilian portuguese",
   "original": "i05_3289",
   "page_count": 4,
   "order": 574,
   "p1": "3289",
   "pn": "3292",
   "abstract": [
    "Speech rhythm has been investigated from phonetic, phonological and signal processing perspectives, leading to a wide range of non-comparable methodologies. We assume that this is because speech rhythm is an emergent phenomenon (Emergent Rhythm Theory), due to many hidden' physiological and other factors, and that specialists make different selections from these factors. We also claim that models proposed so far are relatively arbitrary, and that their formal and empirical similarities and differences are not clarified. Although we accept Emergent Rhythm Theory, we acknowledge that it is currently too complex and inexplicit to be falsifiable, and concentrate on more highly constrained Physical Rhythm Theory approaches. We propose a set of explicanda for Physical Rhythm Theories, and formally and empirically compare selected single-parameter physical rhythm measures. We find that the selected measures show a very low degree of similarity and outline the steps necessary to improve the situation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-574"
  },
  "nagano05_interspeech": {
   "authors": [
    [
     "Tohru",
     "Nagano"
    ],
    [
     "Shinsuke",
     "Mori"
    ],
    [
     "Masafumi",
     "Nishimura"
    ]
   ],
   "title": "A stochastic approach to phoneme and accent estimation",
   "original": "i05_3293",
   "page_count": 4,
   "order": 575,
   "p1": "3293",
   "pn": "3296",
   "abstract": [
    "We present a new stochastic approach to estimate accurately phonemes and accents for Japanese TTS (Text-to-Speech) systems. Front-end process of TTS system assigns phonemes and accents to an input plain text, which is critical for creating intelligible and natural speech. Rule-based approaches that build hierarchical structures are widely used for this purpose. However, considering scalability and the ease of domain adaptation, rule-based approaches have well-known limitations. In this paper, we present a stochastic method based on an n-gram model for phonemes and accents estimation. The proposed method estimates not only phonemes and accents but word segmentation and part-of-speech (POS) simultaneously. We implemented a system for Japanese which solves tokenization, linguistic annotation, text-to-phonemes conversion, homograph disambiguation, and accents generation at the same time, and observed promising results.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-575"
  },
  "brenier05_interspeech": {
   "authors": [
    [
     "Jason M.",
     "Brenier"
    ],
    [
     "Daniel M.",
     "Cer"
    ],
    [
     "Daniel",
     "Jurafsky"
    ]
   ],
   "title": "The detection of emphatic words using acoustic and lexical features",
   "original": "i05_3297",
   "page_count": 4,
   "order": 576,
   "p1": "3297",
   "pn": "3300",
   "abstract": [
    "In this study, we describe an automatic detector for prosodically salient or emphasized words in speech. Knowledge of whether a word is emphatic or not could improve Text-to-Speech synthesis as well as spoken language summarization. Previous work on emphasis detection has focused on the automatic recognition of pitch accents. Our model extends earlier research by automatically identifying emphatic pitch accents, a subset of pitch accents that mark special discourse functions with extreme degrees of salience. The overall best performance achieved by our system was 87.8% correct, 8.0% above baseline performance. The results of a feature selection algorithm show that the top-performing features in our models are primarily acoustic measures. Our work identifies important cues for emphasis in speech and shows that it is possible for an automated system to distinguish between two levels of perceived prominence in pitch accents with a high degree of accuracy.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-576"
  },
  "surendran05_interspeech": {
   "authors": [
    [
     "Dinoj",
     "Surendran"
    ],
    [
     "Gina-Anne",
     "Levow"
    ],
    [
     "Yi",
     "Xu"
    ]
   ],
   "title": "Tone recognition in Mandarin using focus",
   "original": "i05_3301",
   "page_count": 4,
   "order": 577,
   "p1": "3301",
   "pn": "3304",
   "abstract": [
    "Native speakers of Mandarin produce and perceive tones in ways that depend on the focus with which each word is produced [1]. This paper gives one approach to improving tone recognition algorithms using focus, by training different support vector machines on syllables conditional on their position with respect to the focused word in a sentence. In a four-way tone classification task on focus-labelled laboratory Mandarin speech data collected by Xu [2], error rates improve from 15.2% without using focus to 8.7% when using focus. Using the fact that tones on syllables in focused words are especially easy to recognize, we propose a tone recognition algorithm that makes use of focus without requiring focus labels in either training or test set. The algorithm has an error rate of 9.8% on this data set.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-577"
  },
  "wypych05_interspeech": {
   "authors": [
    [
     "Mikolaj",
     "Wypych"
    ]
   ],
   "title": "An automatic intonation recognizer for the Polish language based on machine learning and expert knowledge",
   "original": "i05_3305",
   "page_count": 4,
   "order": 578,
   "p1": "3305",
   "pn": "3308",
   "abstract": [
    "In the paper a new automatic intonation recognizer for the Polish language is presented. The recognizer design combines Machine Learning and expert knowledge techniques. Machine Learning is used in pitch stylization (Artificial Neural Network), speech alignment (external design based on Hidden Markov Model) and intonation decoding (Hidden Markov Model). Expert knowledge drives phonemization, syllabification, lexical accentuation and lemmatization. In the recognizer, a recently available intonation annotation system for Polish is used. The intonation annotation system and the related expert knowledge allowed for substantial reduction in the number of designed HMM decoder parameters. Software integration problems emerging from the number of modules comprising the recognizer are approached using an original software environment for speech processing. The environment has data-centric message-driven Blackboard-like architecture with an annotation graph as a shared memory.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-578"
  },
  "sakurai05_interspeech": {
   "authors": [
    [
     "Atsuhiro",
     "Sakurai"
    ]
   ],
   "title": "Generalized envelope matching technique for time-scale modification of speech (GEM-TSM)",
   "original": "i05_3309",
   "page_count": 4,
   "order": 579,
   "p1": "3309",
   "pn": "3312",
   "abstract": [
    "A new time-domain, non-pitch-synchronous method for timescale modification targeted on broadband speech is proposed. The method is based on the SOLA (synchronous overlap-add) and EM-TSM (envelope-matching time-scale modification) methods, where the sign envelope of the EM-TSM method is replaced by a generalized envelope formed by the highest bits of the samples. (The actual number of bits will depend on word length constraints of the specific hardware.) In addition, a fixed length scheme for calculating cross-correlation is proposed, eliminating the need for normalization after computing each cross-correlation value. With these improvements, the proposed method outperforms EM-TSM both in terms of output quality and computational efficiency.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-579"
  },
  "hirschberg05_interspeech": {
   "authors": [
    [
     "Julia",
     "Hirschberg"
    ],
    [
     "Stefan",
     "Benus"
    ],
    [
     "Jason M.",
     "Brenier"
    ],
    [
     "Frank",
     "Enos"
    ],
    [
     "Sarah",
     "Friedman"
    ],
    [
     "Sarah",
     "Gilman"
    ],
    [
     "Cynthia",
     "Girand"
    ],
    [
     "Martin",
     "Graciarena"
    ],
    [
     "Andreas",
     "Kathol"
    ],
    [
     "Laura",
     "Michaelis"
    ],
    [
     "Bryan L.",
     "Pellom"
    ],
    [
     "Elizabeth",
     "Shriberg"
    ],
    [
     "Andreas",
     "Stolcke"
    ]
   ],
   "title": "Distinguishing deceptive from non-deceptive speech",
   "original": "i05_1833",
   "page_count": 4,
   "order": 580,
   "p1": "1833",
   "pn": "1836",
   "abstract": [
    "To date, studies of deceptive speech have largely been confined to descriptive studies and observations from subjects, researchers, or practitioners, with few empirical studies of the specific lexical or acoustic/prosodic features which may characterize deceptive speech. We present results from a study seeking to distinguish deceptive from non-deceptive speech using machine learning techniques on features extracted from a large corpus of deceptive and non-deceptive speech. This corpus employs an interview paradigm that includes subject reports of truth vs. lie at multiple temporal scales. We present current results comparing the performance of acoustic/prosodic, lexical, and speaker-dependent features and discuss future research directions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-580"
  },
  "liscombe05_interspeech": {
   "authors": [
    [
     "Jackson",
     "Liscombe"
    ],
    [
     "Julia",
     "Hirschberg"
    ],
    [
     "Jennifer J.",
     "Venditti"
    ]
   ],
   "title": "Detecting certainness in spoken tutorial dialogues",
   "original": "i05_1837",
   "page_count": 4,
   "order": 581,
   "p1": "1837",
   "pn": "1840",
   "abstract": [
    "What role does affect play in spoken tutorial systems and is it automatically detectable? We investigated the classification of student certainness in a corpus collected for ITSPOKE, a speechenabled Intelligent Tutorial System (ITS). Our study suggests that tutors respond to indications of student uncertainty differently from student certainty. Results of machine learning experiments indicate that acoustic-prosodic features can distinguish student certainness from other student states. A combination of acousticprosodic features extracted at two levels of intonational analysis - breath groups and turns - achieves 76.42% classification accuracy, a 15.8% relative improvement over baseline performance. Our results suggest that student certainness can be automatically detected and utilized to create better spoke dialog ITSs.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-581"
  },
  "vidrascu05_interspeech": {
   "authors": [
    [
     "Laurence",
     "Vidrascu"
    ],
    [
     "Laurence",
     "Devillers"
    ]
   ],
   "title": "Detection of real-life emotions in call centers",
   "original": "i05_1841",
   "page_count": 4,
   "order": 582,
   "p1": "1841",
   "pn": "1844",
   "abstract": [
    "In contrast to most previous studies conducted on artificial data with archetypal emotions, this paper addresses some of the challenges faced when studying real-life non-basic emotions. A 10-hour dialog corpus recorded in a French Medical emergency call center has been studied. In this paper, our aim is twofold, to study emotion mixtures in natural data and to achieve high level of performances in real-life emotion detection. An annotation scheme using two labels per segment has been used for representing emotion mixtures. A closer study of these mixtures has been carried out, revealing the presence of conflictual valence emotions. A correct detection rate of about 82% was obtained between Negative and Positive emotions using paralinguistic cues without taking into account these conflictual blended emotions. A perceptive study and the confusions analysis obtained with automatic detection enable to validate the annotation scheme with two emotion labels per segment.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-582"
  },
  "liscombe05b_interspeech": {
   "authors": [
    [
     "Jackson",
     "Liscombe"
    ],
    [
     "Giuseppe",
     "Riccardi"
    ],
    [
     "Dilek",
     "Hakkani-Tür"
    ]
   ],
   "title": "Using context to improve emotion detection in spoken dialog systems",
   "original": "i05_1845",
   "page_count": 4,
   "order": 583,
   "p1": "1845",
   "pn": "1848",
   "abstract": [
    "Most research that explores the emotional state of users of spoken dialog systems does not fully utilize the contextual nature that the dialog structure provides. This paper reports results of machine learning experiments designed to automatically classify the emotional state of user turns using a corpus of 5,690 dialogs collected with the \"How May I Help YouSM\" spoken dialog system. We show that augmenting standard lexical and prosodic features with contextual features that exploit the structure of spoken dialog and track user state increases classification accuracy by 2.6%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-583"
  },
  "yanushevskaya05_interspeech": {
   "authors": [
    [
     "Irena",
     "Yanushevskaya"
    ],
    [
     "Christer",
     "Gobl"
    ],
    [
     "Ailbhe",
     "Ní Chasaide"
    ]
   ],
   "title": "Voice quality and f0 cues for affect expression: implications for synthesis",
   "original": "i05_1849",
   "page_count": 4,
   "order": 584,
   "p1": "1849",
   "pn": "1852",
   "abstract": [
    "Synthesised stimuli were used to investigate how two notionally separable dimensions of tone-of-voice - voice quality and fundamental frequency - are involved in the expression of affect. Listeners were presented with three series of stimuli: (1) stimuli exemplifying different voice qualities, (2) stimuli all with modal voice quality but with different affect-related f0 contours, and (3) stimuli incorporating variation in both voice quality and affect-related f0 contours. A total of 15 stimuli were rated for 12 different affective attributes. Voice quality differentiation appears to account for the highest affect ratings overall, as indicated by the scores obtained for stimuli series (1) and (3). The relatively weaker affect signalling of stimuli differentiated by f0 alone corroborates findings in [2]. It also suggests that for the generation of expressive, affectively coloured speech synthesis, it is not sufficient to manipulate only f0; we also need to capture the voice quality dimension of the voice source.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-584"
  },
  "takahashi05b_interspeech": {
   "authors": [
    [
     "Toru",
     "Takahashi"
    ],
    [
     "Takeshi",
     "Fujii"
    ],
    [
     "Masashi",
     "Nishi"
    ],
    [
     "Hideki",
     "Banno"
    ],
    [
     "Toshio",
     "Irino"
    ],
    [
     "Hideki",
     "Kawahara"
    ]
   ],
   "title": "Voice and emotional expression transformation based on statistics of vowel parameters in an emotional speech database",
   "original": "i05_1853",
   "page_count": 4,
   "order": 585,
   "p1": "1853",
   "pn": "1856",
   "abstract": [
    "We propose a simple method for modifying emotional speech sounds. The method aims at real-time implementation of an emotional expression transformation system based on STRAIGHT. We developed a mapping function of spectra, fundamental frequencies (F0), and vowel durations from the statistical analysis of 1500 expressive speech sounds in an emotional speech database. The spectral mapping parameters are initially extracted at the centers of vowels and interpolated with bilinear functions. The spectral frequency warping functions are manually designed. The F0 and duration mapping functions simply transform the average values in log frequency and linear time scales. We demonstrate that the spectral distortion is small enough when Neutral' speech sounds are transformed to expressive speech sounds (i.e. Bright', Excited', Angry', and Raging' speech sounds).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-585"
  },
  "fabbrizio05_interspeech": {
   "authors": [
    [
     "Giuseppe Di",
     "Fabbrizio"
    ],
    [
     "Gokhan",
     "Tur"
    ],
    [
     "Dilek",
     "Hakkani-Tür"
    ]
   ],
   "title": "Automated wizard-of-oz for spoken dialogue systems",
   "original": "i05_1857",
   "page_count": 4,
   "order": 586,
   "p1": "1857",
   "pn": "1860",
   "abstract": [
    "Designing and building natural language spoken dialogue systems require large amounts of speech utterances, which adequately represent the intended human-machine dialogues. For this purpose, typically, first a \"Wizard-of-Oz\" data collection is performed, and then the collected data is transcribed and labeled by expert labelers. Finally, the data is used to train both the speech recognizer and the spoken language understanding stochastic models. Data collection and labeling is an expensive and time consuming manual process. In this paper we propose a completely Automated Wizard, which is capable of recognizing and understanding application independent requests reusing the previously labeled and transcribed data from similar domains, and improving the informativeness of the collected data. We demonstrate that, in the context of automated call routing, compared to the existing data collection systems, the Automated Wizard better captures the user intentions and produces substantially shorter interactions resulting in a better user experience and a less intrusive approach.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-586"
  },
  "katsurada05b_interspeech": {
   "authors": [
    [
     "Kouichi",
     "Katsurada"
    ],
    [
     "Kunitoshi",
     "Sato"
    ],
    [
     "Hiroaki",
     "Adachi"
    ],
    [
     "Hirobumi",
     "Yamada"
    ],
    [
     "Tsuneo",
     "Nitta"
    ]
   ],
   "title": "A rapid prototyping tool for constructing web-based MMI applications",
   "original": "i05_1861",
   "page_count": 4,
   "order": 587,
   "p1": "1861",
   "pn": "1864",
   "abstract": [
    "We have developed Interaction Builder (IB), a rapid prototyping tool for constructing web-based Multi-Modal Interaction (MMI) applications. The goal of IB is making it easy to develop MMI applications with speech recognition, life-like agent, speech synthesis, web browsing, etc. For this purpose, IB supports the following interface and functions: (1) GUI for implementing MMI systems without details of MMI and MMI description language, (2) functionalities for handling synchronized multimodal inputs/outputs, (3) a test run mode for run-time testing. The results of evaluation tests showed that the application development cycle using IB was significantly shortened in comparison with the time using a text editor both for MMI description language experts and for beginners.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-587"
  },
  "hanna05_interspeech": {
   "authors": [
    [
     "Philip",
     "Hanna"
    ],
    [
     "Ian",
     "O'Neill"
    ],
    [
     "Xingkun",
     "Liu"
    ],
    [
     "Michael",
     "McTear"
    ]
   ],
   "title": "Developing extensible and reusable spoken dialogue components: an examination of the Queen's communicator",
   "original": "i05_1865",
   "page_count": 4,
   "order": 588,
   "p1": "1865",
   "pn": "1868",
   "abstract": [
    "This paper presents an examination of the Queen's Communicator, a cross-domain, mixed initiative spoken dialogue system, in terms of the system's extensibility and reusability. The paper explores how an object-oriented design methodology can be used to define both high-level discourse objects and appropriate forms of relationship between discourse objects. The paper concludes that the application of an object-based approach to dialogue system design makes possible the use of a number of different mechanisms that facilitate the extension and reuse of discourse expertise.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-588"
  },
  "wang05i_interspeech": {
   "authors": [
    [
     "Ye-Yi",
     "Wang"
    ],
    [
     "Alex",
     "Acero"
    ]
   ],
   "title": "SGStudio: rapid semantic grammar development for spoken language understanding",
   "original": "i05_1869",
   "page_count": 4,
   "order": 589,
   "p1": "1869",
   "pn": "1872",
   "abstract": [
    "SGStudio (Semantic Grammar Studio) is a grammar authoring tool that facilitates the development of spoken dialog systems and speech enabled applications. It enables regular software developers with little speech/linguistic background to rapidly create quality semantic grammars for automatic speech recognition (ASR) and spoken language understanding (SLU). This paper introduces the framework of the tool as well as the component technologies, including knowledge assisted example-based grammar learning, grammar controls and configurable grammar structures. While the focus of SGStudio is to increase the productivity, experimental results show that it also improves the quality of the grammars.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-589"
  },
  "akbacak05_interspeech": {
   "authors": [
    [
     "Murat",
     "Akbacak"
    ],
    [
     "Yuqing",
     "Gao"
    ],
    [
     "Liang",
     "Gu"
    ],
    [
     "Hong-Kwang Jeff",
     "Kuo"
    ]
   ],
   "title": "Rapid transition to new spoken dialogue domains: language model training using knowledge from previous domain applications and web text resources",
   "original": "i05_1873",
   "page_count": 4,
   "order": 590,
   "p1": "1873",
   "pn": "1876",
   "abstract": [
    "In generic automatic speech recognition (ASR) systems, typically, language models (LMs) are trained to work within a broad range of input conditions. ASR systems used in domain-specific spoken dialogue systems (SDSs) are more constrained in terms of content and style. A mismatch in content and/or style between training and operating conditions results in performance degradation for the dialogue application. The main focus of this paper is to develop tools to facilitate rapid development of spoken dialogue applications within the context of language model training by focusing on the problem of automatically collecting text data that is useful to train accurate language models for the new target domain without manually collecting any in-domain data. We investigate a framework to extract useful information from previous domains and World Wide Web (WWW). We collect data by submitting queries to a search engine and then clean the resulting text via syntactic and semantic filtering. This is followed by artificial sentence generation. Without using any in-domain data, our system achieved a word error rate (WER) of 19.33%, a performance comparable to that achieved by a language model trained on manually collected 32K in-domain sentences. Using less than 1% of in-domain data along with the automatically generated text, our system achieved an ASR performance close to a language model trained on 60K in-domain sentences.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-590"
  },
  "rayner05_interspeech": {
   "authors": [
    [
     "Manny",
     "Rayner"
    ],
    [
     "Pierrette",
     "Bouillon"
    ],
    [
     "Nikos",
     "Chatzichrisafis"
    ],
    [
     "Beth Ann",
     "Hockey"
    ],
    [
     "Marianne",
     "Santaholma"
    ],
    [
     "Marianne",
     "Starlander"
    ],
    [
     "Hitoshi",
     "Isahara"
    ],
    [
     "Kyoko",
     "Kanzaki"
    ],
    [
     "Yukie",
     "Nakao"
    ]
   ],
   "title": "A methodology for comparing grammar-based and robust approaches to speech understanding",
   "original": "i05_1877",
   "page_count": 4,
   "order": 591,
   "p1": "1877",
   "pn": "1880",
   "abstract": [
    "We present a series of experiments designed to compare grammarbased and robust approaches to speech understanding, performed in the context of an Open Source medical speech translation system. We used two versions of the system, one grammar-based and one robust, trained off the same training data, and evaluated them on test data collected using both versions of the system. The experiments were constructed so as to avoid several methodological problems which occurred in earlier work reported in the literature. We found that the grammar-based version gave significantly better results than the robust version, with the difference increasing as subjects became more familiar with the system's coverage. The rate of improvement in subject performance was positively affected by providing them with an intelligent online help system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-591"
  },
  "mairesse05_interspeech": {
   "authors": [
    [
     "François",
     "Mairesse"
    ],
    [
     "Marilyn",
     "Walker"
    ]
   ],
   "title": "Learning to personalize spoken generation for dialogue systems",
   "original": "i05_1881",
   "page_count": 4,
   "order": 592,
   "p1": "1881",
   "pn": "1884",
   "abstract": [
    "One of the most robust findings of studies of human-human dialogue is that people adapt their utterances to their conversational partners. However, spoken language generators are limited in their ability to adapt to individual users. While statistical models of language generation have the potential for individual adaptation, we know of no experiments showing this. In this paper, we utilize one statistical method, boosting, to train a spoken language generator for individual users. We show that individualized models perform better than models based on sets of users, and describe differences in the learned individual models arising from the linguistic preferences of users.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-592"
  },
  "revelin05_interspeech": {
   "authors": [
    [
     "S.",
     "Revelin"
    ],
    [
     "D.",
     "Cadic"
    ],
    [
     "C.",
     "Waast-Richard"
    ]
   ],
   "title": "Optimization of text-to-speech phonetic transcriptions using a-posteriori signal comparison",
   "original": "i05_1885",
   "page_count": 4,
   "order": 593,
   "p1": "1885",
   "pn": "1888",
   "abstract": [
    "One issue arising in text-to-phone conversion is inconsistency between its output and the phonetic time-alignment of the dataset, hindering the back-end's ability to access the best units to synthesize a text. Some such inconsistency is inevitable because dataset labeling requires allowance for alternate pronunciations of words, while the front-end typically predicts a single pronunciation for a word. In this paper we describe an alternate algorithm that recovers from these inconsistencies. The front-end is modified in order to allow multiple pronunciations for a word. The selection of the best pronunciation is based on an a posteriori cost function comparison between the synthetic signals.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-593"
  },
  "salor05_interspeech": {
   "authors": [
    [
     "Özgül",
     "Salor"
    ],
    [
     "Mübeccel",
     "Demirekler"
    ]
   ],
   "title": "Voice transformation using principle component analysis based LSF quantization and dynamic programming approach",
   "original": "i05_1889",
   "page_count": 4,
   "order": 594,
   "p1": "1889",
   "pn": "1892",
   "abstract": [
    "The goal of voice transformation (VT) is to modify the speech of a source speaker such that it is perceived as if spoken by a target speaker. In this paper, we present a speaker specific line spectral frequency (LSF) quantization based on principle component analysis (PCA) and k-means clustering for VT. An LPC based source-filter model is used to model the speech. Transformation is applied to the spectral characteristics of the speaker, while pitch scaling is applied on the residual signal. PCA has been used to determine the principle components of the source and target LSFs to obtain a more efficient quantization. Only the dimensions with high variance have been quantized and those dimensions have been used to obtain the histogram matrix mapping the two speakers during training. To select the best target codeword sequence corresponding to a source codeword sequence in a sentence, a dynamic programming approach is used. Dynamic programming approach approximates the long-term behavior of LSFs of the target speaker, while it is trying to preserve the relationship between the subsequent frames of the source LSFs. Objective and subjective evaluations have shown that dimension reduction of LSFs before quantization and dynamic programming improves the voice transformation performance.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-594"
  },
  "li05e_interspeech": {
   "authors": [
    [
     "Hai Ping",
     "Li"
    ],
    [
     "Wei",
     "Zhang"
    ]
   ],
   "title": "Adapt Mandarin TTS system to Chinese dialect TTS systems",
   "original": "i05_1893",
   "page_count": 4,
   "order": 595,
   "p1": "1893",
   "pn": "1896",
   "abstract": [
    "This paper deals with how to construct Text To Speech (TTS) systems for Chinese dialects. The proposed method adapts Chinese Mandarin TTS system to dialects TTS systems through representing dialect phonology by Mandarin phonology using phonology mappings, and adding dialect vocabulary in the front end to handle lexical difference. But not all Chinese dialects are suitable for applying this adaptation. Investigations carried out in this paper show that this method can be used to construct TTS systems for the Sichuan, Shaanxi, and Dongbei dialects. Evaluation of such dialect systems are also given in this paper.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-595"
  },
  "zheng05b_interspeech": {
   "authors": [
    [
     "Min",
     "Zheng"
    ],
    [
     "Qin",
     "Shi"
    ],
    [
     "Wei",
     "Zhang"
    ],
    [
     "Lianhong",
     "Cai"
    ]
   ],
   "title": "Grapheme-to-phoneme conversion based on TBL algorithm in Mandarin TTS system",
   "original": "i05_1897",
   "page_count": 4,
   "order": 596,
   "p1": "1897",
   "pn": "1900",
   "abstract": [
    "Grapheme-to-phoneme (G2P) conversion is an important component in a Text-to-Speech (TTS) system. The difficulty in Chinese G2P conversion is to pick out one correct pronunciation from several candidates according to the context information. By evaluating the distribution of polyphones in a corpus with manually corrected pinyin transcriptions, this paper pointed out that the overall error rate of G2P conversion was greatly decreased after processing 78 key polyphones. This paper proposed a transformation-based error-driven learning (TBL) algorithm to solve G2P conversion for polyphones. The correct rates of G2P for polyphones, which originally have high accuracy or low accuracy, are both improved. Besides, two additional experiments show that the capacity of the TBL algorithm has great relationships with initial status and TBL algorithm is more suitable than decision tree to solve polyphones' G2P problem.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-596"
  },
  "massimino05_interspeech": {
   "authors": [
    [
     "Paolo",
     "Massimino"
    ],
    [
     "Alberto",
     "Pacchiotti"
    ]
   ],
   "title": "An automaton-based machine learning technique for automatic phonetic transcription",
   "original": "i05_1901",
   "page_count": 4,
   "order": 597,
   "p1": "1901",
   "pn": "1904",
   "abstract": [
    "In this work, we present an innovative approach for grapheme to phoneme conversion, which achieves very low error rates for languages like British English, American English and Dutch, and gives good generalization performances. One of the basic steps in the text-to-speech conversion performed by the speech synthesis systems is the phonetic transcription of the input text that can be considered as an intermediate symbolic representation between the graphemic text and the phones sequence that must be generated. Nevertheless, the definition of explicit rules can be very difficult for some languages. For this reason using a tool able to automatically compress and generalize the lexical knowledge into rules is very useful. In the multilanguage development of the Loquendo speech synthesis system, a machine-learning algorithm applied to the problem of phonetic transcription and extraction of grapheme-phoneme association rules has been developed. The algorithm runs on a training set built up by a lexicon made of words stored in two forms, orthographic and phonetic, and is able to learn and/or predict the phonetic form starting from the previous information: the prediction error on the training set proves to be very low (restricted to some words managed as exceptions), assuring the absolute reliability of the result on the lexicon words; with words that do not occur in the lexicon, the algorithm predicts correct or acceptable transcriptions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-597"
  },
  "soonklang05_interspeech": {
   "authors": [
    [
     "Tasanawan",
     "Soonklang"
    ],
    [
     "Robert I.",
     "Damper"
    ],
    [
     "Yannick",
     "Marchand"
    ]
   ],
   "title": "Comparative objective and subjective evaluation of three data-driven techniques for proper name pronunciation",
   "original": "i05_1905",
   "page_count": 4,
   "order": 598,
   "p1": "1905",
   "pn": "1908",
   "abstract": [
    "Automatic pronunciation of unknown words is a hard problem of great importance in speech technology. Proper names constitute an especially difficult class of words to pronounce because of their low frequency of occurrence and variable origin. In this paper, we compare three different data-driven approaches which use a dictionary of (known) proper names to infer pronunciations for unknown names, namely: pronunciation by analogy (PbA), the table look-up method by Weijters, and the improved' table look-up method by Daelemans and van den Bosch. Evaluation is both objective, in which inferred pronunciations are compared with gold standard' dictionary pronunciations, and subjective, in which listeners rated synthesised pronunciations using a 5-point scale. Objective evaluation used a leave-one-out technique on 52,911 names in the CMUDICT dictionary. Results show that PbA achieves the best performance at 63.93% names correct. In the subjective evaluation of 529 different pronunciations of 200 names, 12 listeners rated the pronunciations. Non-parametric tests of significance show that the dictionary pronunciations are rated superior to the automatically-inferred pronunciations; PbA is superior to both table look-up methods, and the improved' table look-up is superior to Weijters' original method (Walsh test, p < 0.005 in all cases).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-598"
  },
  "engwall05_interspeech": {
   "authors": [
    [
     "Olov",
     "Engwall"
    ]
   ],
   "title": "Articulatory synthesis using corpus-based estimation of line spectrum pairs",
   "original": "i05_1909",
   "page_count": 4,
   "order": 599,
   "p1": "1909",
   "pn": "1912",
   "abstract": [
    "An attempt to define a new articulatory synthesis method, in which the speech signal is generated through a statistical estimation of its relation with articulatory parameters, is presented. A corpus containing acoustic material and simultaneous recordings of the tongue and facial movements was used to train and test the articulatory synthesis of VCV words and short sentences. Tongue and facial motion data, captured with electromagnetic articulography and three-dimensional optical motion tracking, respectively, define articulatory parameters of a talking head. These articulatory parameters are then used as estimators of the speech signal, represented by line spectrum pairs. The statistical link between the articulatory parameters and the speech signal was established using either linear estimation or artificial neural networks. The results show that the linear estimation was only enough to synthesize identifiable vowels, but not consonants, whereas the neural networks gave a perceptually better synthesis.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-599"
  },
  "chen05d_interspeech": {
   "authors": [
    [
     "Aoju",
     "Chen"
    ],
    [
     "Els den",
     "Os"
    ]
   ],
   "title": "Effects of pitch accent type on interpreting information status in synthetic speech",
   "original": "i05_1913",
   "page_count": 4,
   "order": 600,
   "p1": "1913",
   "pn": "1916",
   "abstract": [
    "Unit selection synthesis has made it possible to produce speech with high quality. However, because it allows little control over intonation, it may produce speech with contextually inappropriate intonation. In the signalling of information status, intonation, in particular, choice of pitch accent, has been taken into account in a number of dialogue systems. Previous research shows that this can improve the perceived intonational appropriateness of synthetic speech. Using an eye-tracking paradigm, this study investigates how pitch accents H*L and L*H and deaccentuation affect the interpretation of information status in synthetic speech in English. It was found that H*L biases listeners' interpretation to new information but L*H, like deaccentuation, biases listeners' interpretation to given information. These results indicate that listeners can and do make use of intonational cues in the interpretation of information status in synthetic speech and lend strong support to the integration of intonational signalling of information status into unit selection synthesis.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-600"
  },
  "prusi05_interspeech": {
   "authors": [
    [
     "Perttu",
     "Prusi"
    ],
    [
     "Anssi",
     "Kainulainen"
    ],
    [
     "Jaakko",
     "Hakulinen"
    ],
    [
     "Markku",
     "Turunen"
    ],
    [
     "Esa-Pekka",
     "Salonen"
    ],
    [
     "Leena",
     "Helin"
    ]
   ],
   "title": "Towards generic spatial object model and route guidance grammar for speech-based systems",
   "original": "i05_1917",
   "page_count": 4,
   "order": 601,
   "p1": "1917",
   "pn": "1920",
   "abstract": [
    "In-vehicle route guidance is an area where commercial applications have already come to market. However, indoor guidance is still under research. We present a grammar that can be used to generate speech-based route guidance descriptions based on the spatial object model also discussed in this paper. In speech-based route guidance, proper references to objects in the environment and appropriate segmentation of the guidance are important features.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-601"
  },
  "hsia05_interspeech": {
   "authors": [
    [
     "Chi-Chun",
     "Hsia"
    ],
    [
     "Chung-Hsien",
     "Wu"
    ],
    [
     "Te-Hsien",
     "Liu"
    ]
   ],
   "title": "Duration-embedded bi-HMM for expressive voice conversion",
   "original": "i05_1921",
   "page_count": 4,
   "order": 602,
   "p1": "1921",
   "pn": "1924",
   "abstract": [
    "This paper presents a duration-embedded Bi-HMM framework for expressive voice conversion. First, Ward's minimum variance clustering method is used to cluster all the conversion units (sub-syllables) in order to reduce the number of conversion models as well as the size of the required training database. The durationembedded Bi-HMM trained with the EM algorithm is built for each sub-syllable class to convert the neutral speech into emotional speech considering the duration information. Finally, the prosodic cues are included in the modification of the spectrum-converted speech. The STRAIGHT algorithm is adopted for high-quality speech analysis and synthesis. Target emotions including happiness, sadness and anger are used. Objective and perceptual evaluations were conducted to compare the performance of the proposed approach with previous methods. The results show that the proposed method exhibits encouraging potential in expressive voice conversion.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-602"
  },
  "hirai05_interspeech": {
   "authors": [
    [
     "Toshio",
     "Hirai"
    ],
    [
     "Hisashi",
     "Kawai"
    ],
    [
     "Minoru",
     "Tsuzaki"
    ],
    [
     "Nobuyuki",
     "Nishizawa"
    ]
   ],
   "title": "Analysis of major factors of naturalness degradation in concatenative synthesis",
   "original": "i05_1925",
   "page_count": 4,
   "order": 603,
   "p1": "1925",
   "pn": "1928",
   "abstract": [
    "To effectively improve a speech synthesis system, it is important to find and focus on improving the modules whose effect on naturalness degradation in synthesized speech are the largest. In this paper, we describe the design of a perception experiment to measure the effect of each module separately. Synthesized speech stimuli whose intermediate information is modified during a synthesis process are used in the experiment. A perception experiment in which a Japanese concatenative speech synthesis system was evaluated revealed that the text processing module and a part of the feature prediction module (for the fundamental frequency) of the system were the major factors in degrading naturalness.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-603"
  },
  "tian05_interspeech": {
   "authors": [
    [
     "Jilei",
     "Tian"
    ],
    [
     "Jani",
     "Nurminen"
    ],
    [
     "Imre",
     "Kiss"
    ]
   ],
   "title": "Duration modeling and memory optimization in a Mandarin TTS system",
   "original": "i05_1929",
   "page_count": 4,
   "order": 604,
   "p1": "1929",
   "pn": "1932",
   "abstract": [
    "Current speech synthesis efforts, both in research and in applications, are dominated by methods based on concatenation of spoken units. New progress in the concatenative text-to-speech (TTS) technology can be made mainly from two directions, either by reducing the memory footprint to integrate the system into embedded system, or by improving the synthesized speech quality in terms of intelligibility and naturalness. In this paper, we are focusing on the memory footprint reduction in a Mandarin TTS system. We show that significant memory reductions can be achieved through duration modeling and memory optimization of the lexicon data. The results obtained in the experiments indicate that the memory requirements of the duration data and lexicon can be significantly reduced while keeping the speech quality unaffected. For practical embedded implementations, this is a significant step towards an efficient TTS engine implementation. The applicability of the approach is verified in the speech synthesis system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-604"
  },
  "liang05_interspeech": {
   "authors": [
    [
     "Min-Siong",
     "Liang"
    ],
    [
     "Ke-Chun",
     "Chuang"
    ],
    [
     "Rhuei-Cheng",
     "Yang"
    ],
    [
     "Yuang-Chin",
     "Chiang"
    ],
    [
     "Ren-Yuan",
     "Lyu"
    ]
   ],
   "title": "A bi-lingual Mandarin-to-taiwanese text-to-speech system",
   "original": "i05_1933",
   "page_count": 4,
   "order": 605,
   "p1": "1933",
   "pn": "1936",
   "abstract": [
    "In this paper, we describe a bi-lingual Text-to-speech (TTS) translational system, which converts Chinese Text into Taiwanese speech, by using bilingual lexicon information. This TTS system is organized as three functional modules, which contain a text analysis module, a prosody module, and a waveform synthesis module. We conducted an experiment to evaluate the text analysis and tone-sandhi modules. About 90% labeling accuracy and 65% tone-sandhi accuracy can be achieved. With this proposed Taiwanese TTS component, a talking electronic lexicon system can be built to help those who want to learn these 2 languages.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-605"
  },
  "reichel05_interspeech": {
   "authors": [
    [
     "Uwe D.",
     "Reichel"
    ],
    [
     "Florian",
     "Schiel"
    ]
   ],
   "title": "Using morphology and phoneme history to improve grapheme-to-phoneme conversion",
   "original": "i05_1937",
   "page_count": 4,
   "order": 606,
   "p1": "1937",
   "pn": "1940",
   "abstract": [
    "In this study four statistical grapheme-to-phoneme (G2P) conversion methods for canonical German are compared. The G2P models differ in terms of usage of morphologic information and of phoneme history (left context) information. In order to evaluate our models we introduce two measures, namely mean normalized Levenshtein distance for classification accuracy and conditional relative entropy for validation of phonotactic smoothness. The results show that morphologic information significantly improves G2P conversion and together with phoneme history leads to a better approximation of the original phonotactics. Furthermore with the benefit of morphology our models significantly outperform two well established G2P systems.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-606"
  },
  "goubanova05_interspeech": {
   "authors": [
    [
     "Olga",
     "Goubanova"
    ],
    [
     "Simon",
     "King"
    ]
   ],
   "title": "Predicting consonant duration with Bayesian belief networks",
   "original": "i05_1941",
   "page_count": 4,
   "order": 607,
   "p1": "1941",
   "pn": "1944",
   "abstract": [
    "Consonant duration is influenced by a number of linguistic factors such as the consonant's identity, within-word position, stress level of the previous and following vowels, phrasal position of the word containing the target consonant, its syllabic position, identity of the previous and following segments. In our work, consonant duration is predicted from a Bayesian belief network (BN) consisting of discrete nodes for the linguistic factors and a single continuous node for the consonant's duration. Interactions between factors are represented as conditional dependency arcs in this graphical model. Given the parameters of the belief network, the duration of each consonant in the test set is then predicted as the value with the maximum probability. We compare the results of the belief network model with those of sums-of-products (SoP) and classification and regression tree (CART) models using the same data. In terms of RMS error, our BN model performs better than both CART and SoP models. In terms of the correlation coefficient, our BN model performs better than SoP model, and no worse than CART model. In addition, the Bayesian model reliably predicts consonant duration in cases of missing or hidden linguistic factors.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-607"
  },
  "jande05_interspeech": {
   "authors": [
    [
     "Per-Anders",
     "Jande"
    ]
   ],
   "title": "Inducing decision tree pronunciation variation models from annotated speech data",
   "original": "i05_1945",
   "page_count": 4,
   "order": 608,
   "p1": "1945",
   "pn": "1948",
   "abstract": [
    "A model of pronunciation of words in discourse context has been induced from the annotation of a spoken language corpus. The information included in the annotation is a set of variables hypothesised to be important for the pronunciation of words in discourse context. The annotation is connected to segmentally defined units on tiers corresponding to linguistically relevant units: the discourse, the utterance, the phrase, the word, the syllable and the phoneme. The model is represented as a tree structure, making it transparent for analysis and easy to use in a speech synthesis system. Using phonemic canonical pronunciation representations to estimate the segmental string of the annotated data gives a 22.1% phone error rate. Decision tree pronunciation variation models generated in a tenfold cross validation procedure showed an average phone error rate of 9.9%. Using multiple context variables for modelling pronunciation variation could thus reduce the error rate by 55%, compared to a baseline using canonical pronunciation representations.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-608"
  },
  "wang05j_interspeech": {
   "authors": [
    [
     "Lijuan",
     "Wang"
    ],
    [
     "Yong",
     "Zhao"
    ],
    [
     "Min",
     "Chu"
    ],
    [
     "Frank K.",
     "Soong"
    ],
    [
     "Zhigang",
     "Cao"
    ]
   ],
   "title": "Phonetic transcription verification with generalized posterior probability",
   "original": "i05_1949",
   "page_count": 4,
   "order": 609,
   "p1": "1949",
   "pn": "1952",
   "abstract": [
    "Accurate phonetic transcription is critical to high quality concatenation based text-to-speech synthesis. In this paper, we propose to use generalized syllable posterior probability (GSPP) as a statistical confidence measure to verify errors in phonetic transcriptions, such as reading errors, inadequate alternatives of pronunciations in the lexicon, letter-to-sound errors in transcribing out-of-vocabulary words, idiosyncratic pronunciations, etc. in a TTS speech database. GSPP is computed based upon a syllable graph generated by a recognition decoder. Testing on two data sets, the proposed GSPP is shown to be effective in locating phonetic transcription errors. Equal error rates (EERs) of 8.2% and 8.4%, are obtained on two testing sets, respectively. It is also found that the GSPP verification performance is fairly stable over a wide range around the optimal value of acoustic model exponential weight used in computing GSPP.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-609"
  },
  "cheng05_interspeech": {
   "authors": [
    [
     "Hua",
     "Cheng"
    ],
    [
     "Fuliang",
     "Weng"
    ],
    [
     "Niti",
     "Hantaweepant"
    ],
    [
     "Lawrence",
     "Cavedon"
    ],
    [
     "Stanley",
     "Peters"
    ]
   ],
   "title": "Training a maximum entropy model for surface realization",
   "original": "i05_1953",
   "page_count": 4,
   "order": 610,
   "p1": "1953",
   "pn": "1956",
   "abstract": [
    "Most existing statistical surface realizers either make use of handcrafted grammars to provide coverage or are tuned to specific applications. This paper describes an initial effort toward building a statistical surface realization model that provides both precision and coverage. We trained a Maximum Entropy model that given a predicate-argument semantic representation, predicts the surface form for realizing a semantic concept and the ordering of sibling semantic concepts and their parent, on the Penn TreeBank and Proposition Bank corpora. Initial results have shown that the precisions for predicting surface forms and orderings reached 80% and 90% respectively, on a held-out part of Penn TreeBank. We use the model to generate sentences from our domain representations. We are in the process of evaluating the model on a corpus collected for our in-car applications.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-610"
  },
  "toda05_interspeech": {
   "authors": [
    [
     "Tomoki",
     "Toda"
    ],
    [
     "Kiyohiro",
     "Shikano"
    ]
   ],
   "title": "NAM-to-speech conversion with Gaussian mixture models",
   "original": "i05_1957",
   "page_count": 4,
   "order": 611,
   "p1": "1957",
   "pn": "1960",
   "abstract": [
    "In order to realize a new human communication style using Non- Audible Murmur (NAM) that cannot be heard by people around a speaker, we perform conversion from NAM to ordinary speech (NAM-to-Speech). NAM-to-Speech has a possibility of realizing \"non-speech telephone\" that is a technique for communicating each other by talking in NAM and hearing in speech. In this paper, we apply a statistical conversion method with Gaussian Mixture Model (GMM) to NAM-to-Speech. In advance, we train GMMs for representing correlations between acoustic features of NAM and those of speech using 50 utterance pairs of NAM and speech. In the conversion, we estimate acoustic spectral and F0 features of speech based on a maximum likelihood criterion, and then synthesize the converted speech with a vocoder. From results of subjective evaluations on intelligibility and naturalness, it is demonstrated that the NAM-to-Speech with GMMs can convert NAM to more consistently natural voice.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-611"
  },
  "savino05_interspeech": {
   "authors": [
    [
     "Michelina",
     "Savino"
    ],
    [
     "Mario",
     "Refice"
    ],
    [
     "Massimo",
     "Mitaritonna"
    ]
   ],
   "title": "Which Italian do current systems speak? a first step towards pronunciation modelling of Italian varieties",
   "original": "i05_1961",
   "page_count": 4,
   "order": 612,
   "p1": "1961",
   "pn": "1964",
   "abstract": [
    "This paper describes the first attempt to model regional accent pronunciation of Italian for grapheme-to-phoneme conversion algorithms, useful for implementing speech systems which can sound more natural to Italian end users. In particular, it reports on the variety spoken in Bari, in Southern Italy. The system has been designed in such a modular way to allow possible future extensions to other different varieties.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-612"
  },
  "oliver05_interspeech": {
   "authors": [
    [
     "Dominika",
     "Oliver"
    ],
    [
     "Robert A. J.",
     "Clark"
    ]
   ],
   "title": "Modelling pitch accent types for Polish speech synthesis",
   "original": "i05_1965",
   "page_count": 4,
   "order": 613,
   "p1": "1965",
   "pn": "1968",
   "abstract": [
    "We describe a Polish prosody modelling module for the Festival speech synthesis system. The module uses classification and regression trees for accent type prediction and a linear regression technique for F0 contour generation for these contours. The techniques used to attempt to overcome problems with the only available data are shown. We demonstrate how improvements were achieved by the use of a modified F0 stylisation, accent type clustering and language specific features. Results of a formal perception study show a significant preference for the new intonation model over the original one.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-613"
  },
  "hansakunbuntheung05_interspeech": {
   "authors": [
    [
     "C.",
     "Hansakunbuntheung"
    ],
    [
     "Ausdang",
     "Thangthai"
    ],
    [
     "Chai",
     "Wutiwiwatchai"
    ],
    [
     "Rungkarn",
     "Siricharoenchai"
    ]
   ],
   "title": "Learning methods and features for corpus-based phrase break prediction on Thai",
   "original": "i05_1969",
   "page_count": 4,
   "order": 614,
   "p1": "1969",
   "pn": "1972",
   "abstract": [
    "This paper presents applications of five famous learning methods for Thai phrase break prediction. Phrase break prediction is particularly important for our Thai text-to-speech synthesizer (TTS), where input Thai text has no word and sentence boundary. The learning methods include a POS sequence model, CART, RIPPER, SLIPPER and neural network. Features proposed for the learning machines can be extracted directly from the input text during real processing. The best method based on the CART model gives 80.14% correct-break, 94.40% juncture-correct, and 2.37% false-break scores. Comparing to our previous models based on C4.5 and RIPPER, the new optimized method achieves almost the best performance.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-614"
  },
  "taylor05_interspeech": {
   "authors": [
    [
     "Paul",
     "Taylor"
    ]
   ],
   "title": "Hidden Markov models for grapheme to phoneme conversion",
   "original": "i05_1973",
   "page_count": 4,
   "order": 615,
   "p1": "1973",
   "pn": "1976",
   "abstract": [
    "We propose a method for determining the canonical phonemic transcription of a word from its orthography using hidden Markov models. In the model, phonemes are the hidden states and graphemes the observations. Apart from one pre-processing step, the model is fully automatic. The paper describes the basic HMM framework and enhancements which use pre-processing, context dependent models and a syllable level stress model. In all cases the power of the framework lies in that training of the models (which includes alignment of graphemes and phonemes, training of transitions and training observation probabilities) is performed in a single step.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-615"
  },
  "reubold05_interspeech": {
   "authors": [
    [
     "Ulrich",
     "Reubold"
    ],
    [
     "Alexander",
     "Steffen"
    ]
   ],
   "title": "Pitch-effects in diphone recording: are logatomes inappropriate?",
   "original": "i05_2797",
   "page_count": 4,
   "order": 616,
   "p1": "2797",
   "pn": "2800",
   "abstract": [
    "The most obvious difference between recordings of German words and non-sense words (logatomes) within the BITS project was an audibly noticeable difference in pitch: diphones obtained from logatomes seemed to have a higher pitch than those from words. We proved this by measuring the pitches of diphones concatenated to sentences, and by comparing the pitches of actually uttered logatomes with pitches of words. The results show, that the prompted material has influence on pitch values, i.e. that the design of corpora is a non-trivial problem; besides this, logatomes revealed higher pitch values.\n",
    "We refer to Ellis' and Young's model of human language processing and speech production to support our hypothesis, that a higher mental workload during articulation of logatomes results in a rise of pitch. The question arises whether the usage of logatomes for obtaining high quality diphone synthesis is appropriate.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-616"
  },
  "toda05b_interspeech": {
   "authors": [
    [
     "Tomoki",
     "Toda"
    ],
    [
     "Keiichi",
     "Tokuda"
    ]
   ],
   "title": "Speech parameter generation algorithm considering global variance for HMM-based speech synthesis",
   "original": "i05_2801",
   "page_count": 4,
   "order": 617,
   "p1": "2801",
   "pn": "2804",
   "abstract": [
    "This paper describes a novel parameter generation algorithm for the HMM-based speech synthesis. The conventional algorithm generates a trajectory of static features that maximizes an output probability of a parameter sequence consisting of the static and dynamic features from HMMs under an actual constraint between the two features. The generated trajectory is often excessively smoothed due to the statistical processing. Using the over-smoothed trajectory causes the muffled sound. In order to alleviate the over-smoothing effect, we propose the generation algorithm considering not only the output probability used for the conventional method but also that of a global variance (GV) of the generated trajectory. The latter probability works as a penalty for a reduction of the variance of the generated trajectory. A result of a perceptual evaluation demonstrates that the proposed method causes large improvements of the naturalness of synthetic speech.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-617"
  },
  "tachibana05_interspeech": {
   "authors": [
    [
     "Makoto",
     "Tachibana"
    ],
    [
     "Junichi",
     "Yamagishi"
    ],
    [
     "Takashi",
     "Masuko"
    ],
    [
     "Takao",
     "Kobayashi"
    ]
   ],
   "title": "Performance evaluation of style adaptation for hidden semi-Markov model based speech synthesis",
   "original": "i05_2805",
   "page_count": 4,
   "order": 618,
   "p1": "2805",
   "pn": "2808",
   "abstract": [
    "This paper describes a style adaptation technique using hidden semi-Markov model (HSMM) based maximum likelihood linear regression (MLLR). The HSMM-based MLLR technique can estimate regression matrices for affine transform of mean vectors of output and state duration distributions which maximize likelihood of adaptation data using EM algorithm. In this study, we apply this adaptation technique to style adaptation in HSMM-based speech synthesis. From the results of several subjective tests, we show that the HSMM-based MLLR technique can perform style adaptation with maintaining naturalness of the synthetic speech compared with the conventional HMM-based MLLR technique.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-618"
  },
  "webster05_interspeech": {
   "authors": [
    [
     "Gabriel",
     "Webster"
    ],
    [
     "Tina",
     "Burrows"
    ],
    [
     "Katherine",
     "Knill"
    ]
   ],
   "title": "A comparison of methods for speaker-dependent pronunciation tuning for text-to-speech synthesis",
   "original": "i05_2809",
   "page_count": 4,
   "order": 619,
   "p1": "2809",
   "pn": "2812",
   "abstract": [
    "Unit-based text-to-speech (TTS) systems typically use a set of speech recordings that have been phonetically transcribed to create a large set of phonetic units. During synthesis, pronunciations for input text are generated and used to guide the selection of a sequence of phonetic units. The style of these system pronunciations must match the style of the phonetic transcriptions of the recorded speech database in order to maximize the quality of the synthesized speech. Furthermore, since different speakers have different speech characteristics, supporting multiple speakers for a single language generally requires applying a speaker-dependent mapping to speaker-independent pronunciations. This paper investigates three automatic methods for this process of speakerdependent pronunciation tuning: word N-grams, decision trees, and transformation-based learning. Transformation-based learning achieved the best results, lowering the phone error rate of the text pronunciations compared to the speech transcriptions by 26% over the error rate of the unmodified text transcriptions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-619"
  },
  "syrdal05_interspeech": {
   "authors": [
    [
     "Ann K.",
     "Syrdal"
    ],
    [
     "Alistair D.",
     "Conkie"
    ]
   ],
   "title": "Perceptually-based data-driven join costs: comparing join types",
   "original": "i05_2813",
   "page_count": 4,
   "order": 620,
   "p1": "2813",
   "pn": "2816",
   "abstract": [
    "Unit selection synthesis has improved the quality of synthetic speech by making it possible to concatenate speech from a large database to produce intelligible synthesis while preserving much of the naturalness of the original signal. Such synthesis is by no means perfect, however, and this paper describes work to achieve more optimal joins between concatenated units. Results from a psychoacoustic experiment, acoustic parameters and phonetic factors are analyzed and used in statistical training of join costs so that audible discontinuities at concatenation boundaries can be minimized.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-620"
  },
  "pantazis05_interspeech": {
   "authors": [
    [
     "Yannis",
     "Pantazis"
    ],
    [
     "Yannis",
     "Stylianou"
    ],
    [
     "Esther",
     "Klabbers"
    ]
   ],
   "title": "Discontinuity detection in concatenated speech synthesis based on nonlinear speech analysis",
   "original": "i05_2817",
   "page_count": 4,
   "order": 621,
   "p1": "2817",
   "pn": "2820",
   "abstract": [
    "An objective distance measure which is able to predict audible discontinuity in concatenated speech synthesis systems is very important. Previous works were primarily based on features estimated by linear and/or stationary models of speech. In this paper, we introduce two nonlinear approaches for the detection of discontinuity. The first method is based on a nonlinear harmonic model of speech while the second method is based on the demodulation of speech in an amplitude and a frequency component using the Teager energy operator. Fisher's linear discriminant was used for the separation of signals with audible discontinuity from those perceived as continuous. When we combined the two methods using Fisher's linear discriminant a detection rate of 56.5% was achieved which is an 90% improvement over previously published results on the same database.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-621"
  },
  "wang05k_interspeech": {
   "authors": [
    [
     "Longbiao",
     "Wang"
    ],
    [
     "Norihide",
     "Kitaoka"
    ],
    [
     "Seiichi",
     "Nakagawa"
    ]
   ],
   "title": "Robust distant speaker recognition based on position dependent cepstral mean normalization",
   "original": "i05_1977",
   "page_count": 4,
   "order": 622,
   "p1": "1977",
   "pn": "1980",
   "abstract": [
    "In a distant environment, channel distortion may drastically degrade speaker recognition performance. In this paper, we propose a robust speaker recognition method based on position dependent Cepstral Mean Normalization (CMN) to compensate the channel distortion depending on the speaker position. It is shown in [1] that the position dependent CMN is robust for speech recognition in a distant environment. We extend this method to the speaker recognition and show that this method is much effective to speaker recognition. In the training stage, the system measures the transmission characteristics according to the speaker positions from some grid points to the microphone in the room and estimated the compensation parameters a priori. In the recognition stage, the system estimates the speaker position and adopts the estimated compensation parameters corresponding to the estimated position, and then the system applies the CMN to the speech and performs speaker recognition. In our past study, we proposed a new text-independent speaker recognition method by combining speaker-specific Gaussian Mixture Models (GMMs) with syllablebased HMMs adapted to the speakers by MAP [2]. The robustness of this speaker recognition method for the change of the speaking style in close-talking environment was evaluated in [2]. We integrated this method to the proposed position dependent CMN for distant speaker recognition. Our experiments showed that the proposed method improved the speaker recognition performance remarkably in a distant environment.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-622"
  },
  "leeuwen05_interspeech": {
   "authors": [
    [
     "David A. van",
     "Leeuwen"
    ]
   ],
   "title": "Speaker adaptation in the NIST speaker recognition evaluation 2004",
   "original": "i05_1981",
   "page_count": 4,
   "order": 623,
   "p1": "1981",
   "pn": "1984",
   "abstract": [
    "New in the 2004 edition of the NIST Speaker Recognition Evaluation (SRE) was the condition where unsupervised adaptation of speaker models is allowed. Despite the promising results on development test material, hardly any beneficial results were obtained in the Evaluation itself. An analysis is made why this was the case, and it appears that a minimum level of performance is essential to obtain results using adaptation that improve on the performance without adaptation. Further, the system should be well calibrated. For the conditions with 8 conversation sides we have been able to find improvement using unsupervised adaptation using the NIST 2004 evaluation, both for an UBM/GMM adaptation methodology, and a novel SVM adaptation methodology. The minimum DCF for a fused system drops from 0.259 for the unadapted condition to 0.231 for the adapted condition.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-623"
  },
  "goldberger05_interspeech": {
   "authors": [
    [
     "Jacob",
     "Goldberger"
    ],
    [
     "Hagai",
     "Aronowitz"
    ]
   ],
   "title": "A distance measure between GMMs based on the unscented transform and its application to speaker recognition",
   "original": "i05_1985",
   "page_count": 4,
   "order": 624,
   "p1": "1985",
   "pn": "1988",
   "abstract": [
    "This paper proposes a dissimilarity measure between two Gaussian mixture models (GMM). Computing a distance measure between two GMMs that were learned from speech segments is a key element in speaker verification, speaker segmentation and many other related applications. A natural measure between two distributions is the Kullback-Leibler divergence. However, it cannot be analytically computed in the case of GMM. We propose an accurate and efficiently computed approximation of the KL-divergence. The method is based on the unscented transform which is usually used to obtain a better alternative to the extended Kalman filter. The suggested distance is evaluated in an experimental setup of speakers data-set. The experimental results indicate that our proposed approximations outperform previously suggested methods.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-624"
  },
  "dusan05b_interspeech": {
   "authors": [
    [
     "Sorin",
     "Dusan"
    ]
   ],
   "title": "Estimation of speaker's height and vocal tract length from speech signal",
   "original": "i05_1989",
   "page_count": 4,
   "order": 625,
   "p1": "1989",
   "pn": "1992",
   "abstract": [
    "Estimation of speaker's height and vocal tract length (VTL) from speech signal can have forensic and automatic speech recognition applications. It was suggested for a long time that there is a correlation between speaker's VTL, on one side, and speaker's height and formant frequencies, on another side. Until recently, these putative relationships have been empirically examined in studies employing relatively small numbers of speakers. Scattered studies presented intriguing results about the correlations between speaker's height and various acoustic speech parameters. Due to lack of databases, few studies presented extensive comparative results between the actual speaker's VTL and the estimated one from speech signal. This paper presents an analysis of correlations between various acoustic speech parameters and speaker's height for a large number of speakers. It also presents a new method for an optimal estimation of speaker's height and VTL from various acoustic speech parameters.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-625"
  },
  "torretoledano05_interspeech": {
   "authors": [
    [
     "Doroteo",
     "Torre Toledano"
    ],
    [
     "Carlos",
     "Fombella"
    ],
    [
     "Joaquin",
     "Gonzalez Rodriguez"
    ],
    [
     "Luis",
     "Hernandez Gomez"
    ]
   ],
   "title": "On the relationship between phonetic modeling precision and phonetic speaker recognition accuracy",
   "original": "i05_1993",
   "page_count": 4,
   "order": 626,
   "p1": "1993",
   "pn": "1996",
   "abstract": [
    "Speaker recognition techniques have traditionally relied on purely acoustic features and models. During the last few years, however, the field of speaker recognition has started to show interest in the use of higher level features. In particular, phonetic decodings modeled with statistical language models (n-grams) have already shown its effectiveness in several research works. However, the relationship between phonetic modeling precision and the accuracy of phonetic speaker recognition has not yet been sufficiently analyzed. As part of our preparation for the NIST 2005 speaker recognition evaluation, we have performed a number of experiments that show that there is a negligible correlation between phonetic modeling precision and phonetic speaker recognition accuracy. Furthermore, our experimental results show that phonetic speaker recognition results may even be better when using phonetic decodings in languages different from that of the speech.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-626"
  },
  "fortuna05_interspeech": {
   "authors": [
    [
     "J.",
     "Fortuna"
    ],
    [
     "P.",
     "Sivakumaran"
    ],
    [
     "A.",
     "Ariyaeeinia"
    ],
    [
     "A.",
     "Malegaonkar"
    ]
   ],
   "title": "Open-set speaker identification using adapted Gaussian mixture models",
   "original": "i05_1997",
   "page_count": 4,
   "order": 627,
   "p1": "1997",
   "pn": "2000",
   "abstract": [
    "This paper presents an investigation into the use of adapted Gaussian mixture models in the context of open-set, text-independent speaker identification (OSTI-SI). The study includes a scheme for using the fast-scoring method which has been proposed for speaker verification. Furthermore, it provides an evaluation of various score normalisation methods in the proposed OSTI-SI framework. The dataset used for the experimental investigation is based on NIST SRE2003 1-speaker detection task. It is shown that significant improvements can be achieved if only a single mixture is used in the fast-scoring technique. Furthermore, it is experimentally observed that comparable performance is obtained using unconstrained cohort normalisation, T-norm and TZ-norm. The paper provides a detailed description of the experimental set up, and presents an analysis of the results obtained.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-627"
  },
  "mcauley05_interspeech": {
   "authors": [
    [
     "James",
     "McAuley"
    ],
    [
     "Ji",
     "Ming"
    ],
    [
     "Pat",
     "Corr"
    ]
   ],
   "title": "Speaker verification in noisy conditions using correlated subband features",
   "original": "i05_2001",
   "page_count": 4,
   "order": 628,
   "p1": "2001",
   "pn": "2004",
   "abstract": [
    "This paper investigates the task of speaker verification in noisy conditions, assuming no prior knowledge about the noise. To achieve this we use subband features which can isolate band-limited corruption and allow verification to be performed on mainly clean subbands. To further improve speaker verification performance, we propose a novel method to model correlation between subband feature streams. In this method, all possible combinations of subbands are created and each combination is treated as a single frequency band by calculating a single feature vector for it. The resulting feature vectors capture information about every band in the combination as well as the dependency across the bands. Experiments conducted on the NIST 1998 database demonstrate improved robustness for the new model, in the presence of both stationary and non-stationary noise.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-628"
  },
  "collet05_interspeech": {
   "authors": [
    [
     "Mikaël",
     "Collet"
    ],
    [
     "Yassine",
     "Mam"
    ],
    [
     "Delphine",
     "Charlet"
    ],
    [
     "Frédéric",
     "Bimbot"
    ]
   ],
   "title": "Probabilistic anchor models approach for speaker verification",
   "original": "i05_2005",
   "page_count": 4,
   "order": 629,
   "p1": "2005",
   "pn": "2008",
   "abstract": [
    "This paper presents a probabilistic approach for representing a speaker using the anchor modelling technique and discusses the relation between this new approach and the deterministic approach. In the first part, the technique of anchor modelling is presented. Then the new approach, which modelizes the various utterances of a speaker by a normal distribution in the anchor models space, is presented. A study of the relation between the two approaches (deterministic and probabilistic) leads to the definition of a combined approach. All these approaches are evaluated on the NIST 2000 database and results show improved performance of the combined approach over the deterministic and the probabilistic one.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-629"
  },
  "arcienega05_interspeech": {
   "authors": [
    [
     "Mijail",
     "Arcienega"
    ],
    [
     "Anil",
     "Alexander"
    ],
    [
     "Philipp",
     "Zimmermann"
    ],
    [
     "Andrzej",
     "Drygajlo"
    ]
   ],
   "title": "A Bayesian network approach combining pitch and spectral envelope features to reduce channel mismatch in speaker verification and forensic speaker recognition",
   "original": "i05_2009",
   "page_count": 4,
   "order": 630,
   "p1": "2009",
   "pn": "2012",
   "abstract": [
    "The aim of this paper is to reduce the effect of mismatch in recording conditions due to the transmission channel and recording device, using conditional dependencies of prosodic and spectral envelope features. The developed system is based on a Bayesian network framework which combines statistical models of the pitch and spectral envelope features. This approach is applied to forensic automatic speaker recognition, where mismatched recording conditions pose a serious problem to the accurate estimation of the strength of voice evidence. The method is evaluated using a forensic speaker recognition database that contains three different recording conditions typical to forensic tasks. The performance of the system is evaluated using both speaker verification as well as forensic speaker recognition measures.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-630"
  },
  "yiu05_interspeech": {
   "authors": [
    [
     "Kwok-Kwong",
     "Yiu"
    ],
    [
     "Man-Wai",
     "Mak"
    ],
    [
     "Sun-Yuan",
     "Kung"
    ]
   ],
   "title": "Channel robust speaker verification via Bayesian blind stochastic feature transformation",
   "original": "i05_2013",
   "page_count": 4,
   "order": 631,
   "p1": "2013",
   "pn": "2016",
   "abstract": [
    "In telephone-based speaker verification, the channel conditions can be varied significantly from sessions to sessions. Therefore, it is desirable to estimate the channel conditions online and compensate the acoustic distortion without prior knowledge of the channel characteristics. Because no a priori knowledge is used, the estimation accuracy depends greatly on the length of the verification utterances. This paper extends the Blind Stochastic Feature Transformation (BSFT) algorithm that we recently proposed to handle the short-utterance scenario. The idea is to estimate a set of prior transformation parameters from a development set in which a wide variety of channel conditions exists in the verification utterances. The prior transformations are then incorporated into the online estimation of the BSFT parameters in a Bayesian (maximum a posteriori) fashion. The resulting transformation parameters are therefore dependent on both the prior transformations and the verification utterances. For short (long) utterances, the prior transformations play a more (less) important role. We referred the extended algorithm to as Bayesian BSFT (BBSFT) and applied it to the 2001 NIST SRE task. Results show that Bayesian BSFT outperforms BSFT for utterances shorter than or equal to 4 seconds.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-631"
  },
  "matsui05_interspeech": {
   "authors": [
    [
     "Tomoko",
     "Matsui"
    ],
    [
     "Kunio",
     "Tanabe"
    ]
   ],
   "title": "dPLRM-based speaker identification with log power spectrum",
   "original": "i05_2017",
   "page_count": 4,
   "order": 632,
   "p1": "2017",
   "pn": "2020",
   "abstract": [
    "This paper investigates speaker identification with implicit extraction of speaker characteristics relevant to discrimination from the log power spectrum of training speech by employing the inductive power of dual Penalized Logistic Regression Machine (dPLRM). The dPLRM is one of kernel methods like the support vector machine (SVM) and has the inductive power due to the mechanism of the kernel regression. In text-independent speaker identification experiments with training speech uttered by 10 male speakers in three different sessions, we compares the performances of dPLRM, SVM and Gaussian mixture model (GMM)-based methods and show that dPLRM implicitly and effectively extracts speaker characteristics from the log power spectrum. It is also shown that dPLRM outperforms the other methods especially when the amount of training data is small.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-632"
  },
  "zhang05d_interspeech": {
   "authors": [
    [
     "Xianxian",
     "Zhang"
    ],
    [
     "John H. L.",
     "Hansen"
    ],
    [
     "Pongtep",
     "Angkititrakul"
    ],
    [
     "Kazuya",
     "Takeda"
    ]
   ],
   "title": "Speaker verification using Gaussian mixture models within changing real car environments",
   "original": "i05_2021",
   "page_count": 4,
   "order": 633,
   "p1": "2021",
   "pn": "2024",
   "abstract": [
    "Gaussian Mixture Model (GMM) based speaker verification has been widely used recently. However, little research has been performed using GMMs for actual in-vehicle speaker verification. In this paper, we propose to integrate speaker verification and localization techniques for an in-vehicle speech dialog system to locate the desired speaker. The proposed solution is able to locate both desired and undesired speakers who are talking from the same position. This problem cannot be addressed by a simply speaker localization technique only. We demonstrate that using speech data collected in real car environments, the Equal Error rate (EER) performance approaches 0 using gender dependent data, and 2.35% and 13.34% using randomly selected data under idle and city noise environments, respectively.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-633"
  },
  "amino05_interspeech": {
   "authors": [
    [
     "Kanae",
     "Amino"
    ],
    [
     "Tsutomu",
     "Sugawara"
    ],
    [
     "Takayuki",
     "Arai"
    ]
   ],
   "title": "The correspondences between the perception of the speaker individualities contained in speech sounds and their acoustic properties",
   "original": "i05_2025",
   "page_count": 4,
   "order": 634,
   "p1": "2025",
   "pn": "2028",
   "abstract": [
    "This study investigates the correspondences between the differences among the phones in human speaker identification and their acoustic properties. In the speaker identification test, the Japanese CV syllables excerpted from the carrier sentences were used as the stimuli. As pointed out in the previous studies, the stimuli containing the nasal sounds were significantly effective for the identification of the speakers, compared to other stimuli containing only the oral sounds. In the acoustic analyses, we analysed the spectral properties of the stimuli in order to explain these differences in the perception test, and we found that the cepstral distances among the speakers were significantly larger in the nasal sounds than in the oral sounds. Also, there were correspondences between the rankings of the consonants in the identification test and in the cepstral distances.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-634"
  },
  "kim05d_interspeech": {
   "authors": [
    [
     "Samuel",
     "Kim"
    ],
    [
     "Sungwan",
     "Yoon"
    ],
    [
     "Thomas",
     "Eriksson"
    ],
    [
     "Hong-Goo",
     "Kang"
    ],
    [
     "Dae Hee",
     "Youn"
    ]
   ],
   "title": "A noise-robust pitch synchronous feature extraction algorithm for speaker recognition systems",
   "original": "i05_2029",
   "page_count": 4,
   "order": 635,
   "p1": "2029",
   "pn": "2032",
   "abstract": [
    "A noise-robust pitch synchronous feature extraction algorithm for speaker recognition systems is proposed in this paper. Since the pitch synchronous algorithms utilize pitch information, which is meaningful only for periodic segments, we propose a new scheme to deal with non-periodic ones such as unvoiced and noise-corrupted. The experimental results show that the proposed algorithm outperforms the conventional algorithm using fixed length of analysis window in actual identification tasks even in low SNR noisy environments.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-635"
  },
  "deng05c_interspeech": {
   "authors": [
    [
     "Jing",
     "Deng"
    ],
    [
     "Thomas Fang",
     "Zheng"
    ],
    [
     "Zhanjiang",
     "Song"
    ],
    [
     "Jian",
     "Liu"
    ]
   ],
   "title": "Modeling high-level information by using Gaussian mixture correlation for GMM-UBM based speaker recognition",
   "original": "i05_2033",
   "page_count": 4,
   "order": 636,
   "p1": "2033",
   "pn": "2036",
   "abstract": [
    "The Gaussian mixture model-universal background model (GMMUBM) has been dominant in text-independent speaker recognition tasks. However the conventional GMM-UBM method assumes that each Gaussian mixture is independent and ignores the fact that within Gaussian mixtures, there do exist some useful high-level speaker-dependent characteristics, such as word usage or speaking habits. Based on the GMM-UBM method, a method is proposed to use Gaussian mixture correlation to model the high-level information for speaker recognition tasks. In this method, we first cluster the Gaussian mixtures of the UBM into a small number of classes in terms of the mean vectors; in the following step, a universal class transition probability matrix (UCTPM) is learned which is helpful in modeling the high-level speaker's characteristics embedded in Gaussian mixture correlation. During the training phase, a speaker-dependent class transition probability matrix is adapted from the UCTPM. Experiments over two different databases show that an average 20.38% error rate reduction (ERR) can be achieved compared with the conventional GMM-UBM method.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-636"
  },
  "zhang05e_interspeech": {
   "authors": [
    [
     "Xianxian",
     "Zhang"
    ],
    [
     "John H. L.",
     "Hansen"
    ]
   ],
   "title": "In-set/out-of-set speaker identification based on discriminative speech frame selection",
   "original": "i05_2037",
   "page_count": 4,
   "order": 637,
   "p1": "2037",
   "pn": "2040",
   "abstract": [
    "In this paper, we propose a novel discriminative speech frame selection (DSFS) scheme for the problem of in-set/out-of-set speaker identification, which seeks to decrease the similarity between speaker models and background model (or anti-speaker model), and increase the accuracy of speaker identification. The working scheme of DSFS consists of two steps: speech frame analysis and discriminative frame selection. Two methods are used to perform DSFS, (i) Teager Energy Operator (TEO) energy based and (ii) MELP pitch based methods. An evaluation using both clean and noisy corpora that include single and multiple recording sessions show that both TEO energy based and MELP pitch based DSFS schemes can reduce EER (equal error rate) dramatically over a traditional GMM-UBM baseline system. Compared with traditional GMM speaker identification, the DSFS is able to select only discriminative speech frames, and therefore consider only discriminative features. This selection is able to decrease the overlap between speaker models and background model, and improve the performance of in-set/out-of-set speaker identification.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-637"
  },
  "lei05b_interspeech": {
   "authors": [
    [
     "Zhenchun",
     "Lei"
    ],
    [
     "Yingchun",
     "Yang"
    ],
    [
     "Zhaohui",
     "Wu"
    ]
   ],
   "title": "Mixture of support vector machines for text-independent speaker recognition",
   "original": "i05_2041",
   "page_count": 4,
   "order": 638,
   "p1": "2041",
   "pn": "2044",
   "abstract": [
    "In this paper, the mixture of support vector machines is proposed and applied to text-independent speaker recognition. The mixture of experts is used and is implemented by the divide-and-conquer approach. The purpose of adopting this idea is to deal with the large scale speech data and improve the performance of speaker recognition. The principle is to train several parallel SVMs on the subsets of the whole dataset and then combine them in the distance or probabilistic fashion. The experiments have been run on the YOHO database, and the results show that the mixture model is superior to the basic Gaussian mixture model.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-638"
  },
  "zhang05f_interspeech": {
   "authors": [
    [
     "Shilei",
     "Zhang"
    ],
    [
     "Junmei",
     "Bai"
    ],
    [
     "Shuwu",
     "Zhang"
    ],
    [
     "Bo",
     "Xu"
    ]
   ],
   "title": "Optimal model order selection based on regression tree in speaker identification",
   "original": "i05_2045",
   "page_count": 4,
   "order": 639,
   "p1": "2045",
   "pn": "2048",
   "abstract": [
    "In this paper we propose a new method to select the optimal model order for the initialization of Gaussian Mixture speaker Models (GMM) based on regression tree in text-independent speaker identification system. The objective is to choose the optimal number of components which is necessary to adequately model a speaker for a good speaker identification performance according to the Bayesian Information Criterion (BIC) and agglomerative clustering. One obvious advantage of such method is that it provides a flexible framework to select an optimal speaker model order based on the training data for each client speaker. The experimental results on the YOHO corpus show that adaptive model mixture components achieves better performance, especially considering the fact that different speakers have different amounts of available enrollment data.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-639"
  },
  "faundezzanuy05_interspeech": {
   "authors": [
    [
     "Marcos",
     "Faúndez-Zanuy"
    ],
    [
     "Jordi",
     "Solé-Casals"
    ]
   ],
   "title": "Speaker verification improvement using blind inversion of distortions",
   "original": "i05_2049",
   "page_count": 4,
   "order": 640,
   "p1": "2049",
   "pn": "2052",
   "abstract": [
    "In this paper we propose the inversion of nonlinear distortions in order to improve the recognition rates of a speaker recognizer system. We study the effect of saturations on the test signals, trying to take into account real situations where the training material has been recorded in a controlled situation but the testing signals present some mismatch with the input signal level (saturations). The experimental results shows that a combination of several strategies can reduce the minimum detection cost function with saturated test sentences from 6.42% to 4.15%, while the results with clean speech (without saturation) is 5.74% for one microphone and 7.02% for the other one.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-640"
  },
  "omar05_interspeech": {
   "authors": [
    [
     "Mohamed Kamal",
     "Omar"
    ],
    [
     "Jiri",
     "Navrátil"
    ],
    [
     "Ganesh N.",
     "Ramaswamy"
    ]
   ],
   "title": "Maximum conditional mutual information modeling for speaker verification",
   "original": "i05_2169",
   "page_count": 4,
   "order": 641,
   "p1": "2169",
   "pn": "2172",
   "abstract": [
    "This paper describes a novel approach for class-dependent modeling and its application to automatic text-independent speaker verification. This approach maximizes the conditional mutual information between the model scores and the class identity given some constraints on the scores. It is shown in the paper that maximizing the differential entropy of the scores generated by the classifier or the detector is an equivalent criterion. This approach allows emphasizing different features, in the feature vector used in the detection, for different target speakers. In this paper, we apply this approach to the NIST 2003 1-speaker verification task. Compared to the baseline system, around 10% relative improvement in the minimum detection cost function (DCF) is obtained.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-641"
  },
  "ferrer05_interspeech": {
   "authors": [
    [
     "Luciana",
     "Ferrer"
    ],
    [
     "Kemal",
     "Sönmez"
    ],
    [
     "Sachin",
     "Kajarekar"
    ]
   ],
   "title": "Class-dependent score combination for speaker recognition",
   "original": "i05_2173",
   "page_count": 4,
   "order": 642,
   "p1": "2173",
   "pn": "2176",
   "abstract": [
    "Many recent performance improvements in speaker recognition using higher-level features, as demonstrated in the NIST Speaker Recognition Evaluation (SRE) task, rely on combinations of multiple systems modeling a large variety of features. The diversity of the large set of features starting from short-term acoustic spectrum features all the way to habitual word usage from a large set of speakers in a multitude of settings (acoustic environment, speaking style, quantities of enrollment/test data) results in a challenging model combination task. In this work, we are presenting a classdependent score combination technique that relies on clustering of both the target models and the test utterances in a vector space defined by a set of speaker-specific transformation parameters estimated during transcription of the talker's speech by automatic speech recognition (ASR). We show that significant performance gains are obtained by using the first few principal components of a model transform for clustering the speaker verification trials into classes for (target speaker, test utterance) pairs, and then training a separate combiner for each class. We report results on the NIST SRE 2004 and FISHER datasets.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-642"
  },
  "aronowitz05_interspeech": {
   "authors": [
    [
     "Hagai",
     "Aronowitz"
    ],
    [
     "Dror",
     "Irony"
    ],
    [
     "David",
     "Burshtein"
    ]
   ],
   "title": "Modeling intra-speaker variability for speaker recognition",
   "original": "i05_2177",
   "page_count": 4,
   "order": 643,
   "p1": "2177",
   "pn": "2180",
   "abstract": [
    "In this paper we present a speaker recognition algorithm that models explicitly intra-speaker inter-session variability. Such variability may be caused by changing speaker characteristics (mood, fatigue, etc.), channel variability or noise variability. We define a session-space in which each session (either train or test session) is a vector. We then calculate a rotation of the session-space for which the estimated intra-speaker subspace is isolated and can be modeled explicitly. We evaluated our technique on the NIST-2004 speaker recognition evaluation corpus, and compared it to a GMM baseline system. Results indicate significant reduction in error rate.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-643"
  },
  "chetty05_interspeech": {
   "authors": [
    [
     "Girija",
     "Chetty"
    ],
    [
     "Michael",
     "Wagner"
    ]
   ],
   "title": "Liveness detection using cross-modal correlations in face-voice person authentication",
   "original": "i05_2181",
   "page_count": 4,
   "order": 644,
   "p1": "2181",
   "pn": "2184",
   "abstract": [
    "In this paper we show the potential of two new features as powerful anti-spoofing measures for face-voice person authentication systems. The features based on latent semantic analysis (LSA) and canonical correlation analysis (CCA), enhance the performance of the authentication system in terms of better anti-imposture abilities and guard against video replay attacks, which is a challenging type of spoof attack. Experiments conducted on 2 speaking-face databases, VidTIMIT and UCBN, show around 42% improvement in error rate with CCA features and 61% improvement with LSA features over feature-level fusion of face-voice feature vectors.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-644"
  },
  "asami05_interspeech": {
   "authors": [
    [
     "Taichi",
     "Asami"
    ],
    [
     "Koji",
     "Iwano"
    ],
    [
     "Sadaoki",
     "Furui"
    ]
   ],
   "title": "Stream-weight optimization by LDA and adaboost for multi-stream speaker verification",
   "original": "i05_2185",
   "page_count": 4,
   "order": 645,
   "p1": "2185",
   "pn": "2188",
   "abstract": [
    "This paper proposes an automatic stream-weight optimization method for noise-robust speaker verification using multi-stream HMMs integrating spectral and prosodic information. The paper first shows the effectiveness of the multi-stream technique in our speaker verification framework. Next, a stream-weight adaptation method combining the linear discriminant analysis (LDA) and Adaboost techniques is proposed. Experiments were conducted using four-connected-digit utterances of Japanese contaminated by white noise with various SNRs. Experimental results show that 1) the verification performance was improved in all SNR conditions by using stream weights estimated by the LDA and 2) the performance is further improved by using the Adaboost in 10 - 30dB SNR conditions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-645"
  },
  "solewicz05_interspeech": {
   "authors": [
    [
     "Yosef A.",
     "Solewicz"
    ],
    [
     "Moshe",
     "Koppel"
    ]
   ],
   "title": "Considering speech quality in speaker verification fusion",
   "original": "i05_2189",
   "page_count": 4,
   "order": 646,
   "p1": "2189",
   "pn": "2192",
   "abstract": [
    "This paper emphasizes the benefits of embedding data categorization within fusion of classifiers for text-independent speaker verification. A selective fusion framework is presented which considers data idiosyncrasies by assigning particular test samples to appropriate fusion schemes. As an extension, incompatible data can be spotted and excluded from inherent classification errors. In addition, it's shown that multi-resolution low-level classifiers successfully boost fusion capabilities in noise.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-646"
  },
  "stolcke05_interspeech": {
   "authors": [
    [
     "Andreas",
     "Stolcke"
    ],
    [
     "Luciana",
     "Ferrer"
    ],
    [
     "Sachin",
     "Kajarekar"
    ],
    [
     "Elizabeth",
     "Shriberg"
    ],
    [
     "Anand",
     "Venkataraman"
    ]
   ],
   "title": "MLLR transforms as features in speaker recognition",
   "original": "i05_2425",
   "page_count": 4,
   "order": 647,
   "p1": "2425",
   "pn": "2428",
   "abstract": [
    "We explore the use of adaptation transforms employed in speech recognition systems as features for speaker recognition. This approach is attractive because, unlike standard frame-based cepstral speaker recognition models, it normalizes for the choice of spoken words in text-independent speaker verification. Affine transforms are computed for the Gaussian means of the acoustic models used in a recognizer, using maximum likelihood linear regression (MLLR). The high-dimensional vectors formed by the transform coefficients are then modeled as speaker features using support vector machines (SVMs). The resulting speaker verification system is competitive, and in some cases significantly more accurate, than state-of-the-art cepstral gaussian mixture and SVM systems. Further improvements are obtained by combining baseline and MLLR-based systems.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-647"
  },
  "baker05_interspeech": {
   "authors": [
    [
     "Brendan",
     "Baker"
    ],
    [
     "Robbie",
     "Vogt"
    ],
    [
     "Sridha",
     "Sridharan"
    ]
   ],
   "title": "Gaussian mixture modelling of broad phonetic and syllabic events for text-independent speaker verification",
   "original": "i05_2429",
   "page_count": 4,
   "order": 648,
   "p1": "2429",
   "pn": "2432",
   "abstract": [
    "This paper examines the usefulness of a multilingual broad syllable-based framework for text-independent speaker verification. Syllabic segmentation is used in order to obtain a convenient unit for constrained and more detailed model generation. Gaussian mixture models are chosen as a suitable modelling paradigm for initial testing of the framework. Promising results are presented for the NIST 2003 speaker recognition evaluation corpus. The syllable-based modelling technique is shown to outperform a state-of-the-art baseline GMM system. A simple selective reduction of the syllable set is also shown to give further improvement in performance. Overall, the syllable based framework presents itself as valid alternative to text-constrained speaker verification systems, with the advantage of being multilingual. The framework allows for future testing of alternative modelling paradigms, feature sets and qualitative analysis.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-648"
  },
  "aronowitz05b_interspeech": {
   "authors": [
    [
     "Hagai",
     "Aronowitz"
    ],
    [
     "David",
     "Burshtein"
    ]
   ],
   "title": "Efficient speaker identification and retrieval",
   "original": "i05_2433",
   "page_count": 4,
   "order": 649,
   "p1": "2433",
   "pn": "2436",
   "abstract": [
    "In this paper we present techniques for efficient speaker recognition of a large population of speakers and for efficient speaker retrieval in large audio archives. We deal with aspects of both time and storage. We use Gaussian mixture modeling (GMM) for representing both train and test sessions and show how to perform speaker recognition and retrieval efficiently with only a small degradation in accuracy compared to classic GMM based recognition. We present techniques for achieving a dramatic acceleration of both tasks. Finally, we present a GMM compression algorithm that decreases considerably the storage needed for speaker retrieval.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-649"
  },
  "sinha05_interspeech": {
   "authors": [
    [
     "R.",
     "Sinha"
    ],
    [
     "S. E.",
     "Tranter"
    ],
    [
     "M. J. F.",
     "Gales"
    ],
    [
     "P. C.",
     "Woodland"
    ]
   ],
   "title": "The Cambridge University March 2005 speaker diarisation system",
   "original": "i05_2437",
   "page_count": 4,
   "order": 650,
   "p1": "2437",
   "pn": "2440",
   "abstract": [
    "This paper describes the speaker diarisation system developed at Cambridge University in March 2005. This system combines techniques used successfully in our previous speaker diarisation systems with an additional second clustering stage based on state-of-the-art speaker identification methods. Several strategies for using the new system are investigated and the final system gives a diarisation error rate of 6.9% on the RT-04 Fall diarisation evaluation data when processing all the test data together or 8.6% when processing the test data shows independently.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-650"
  },
  "zhu05_interspeech": {
   "authors": [
    [
     "Xuan",
     "Zhu"
    ],
    [
     "Claude",
     "Barras"
    ],
    [
     "Sylvain",
     "Meignier"
    ],
    [
     "Jean-Luc",
     "Gauvain"
    ]
   ],
   "title": "Combining speaker identification and BIC for speaker diarization",
   "original": "i05_2441",
   "page_count": 4,
   "order": 651,
   "p1": "2441",
   "pn": "2444",
   "abstract": [
    "This paper describes recent advances in speaker diarization by incorporating a speaker identification step. This system builds upon the LIMSI baseline data partitioner used in the broadcast news transcription system. This partitioner provides a high cluster purity but has a tendency to split the data from a speaker into several clusters, when there is a large quantity of data for the speaker. Several improvements to the baseline system have been made. Firstly, a standard Bayesian information criterion (BIC) agglomerative clustering has been integrated replacing the iterative Gaussian mixture model (GMM) clustering. Then a second clustering stage has been added, using a speaker identification method with MAP adapted GMM. A final post-processing stage refines the segment boundaries using the output of the transcription system. On the RT-04f and ESTER evaluation data, the improved multi-stage system provides between 40% and 50% reduction of the speaker error, relative to a standard BIC clustering system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-651"
  },
  "istrate05_interspeech": {
   "authors": [
    [
     "Dan",
     "Istrate"
    ],
    [
     "Nicolas",
     "Scheffer"
    ],
    [
     "Corinne",
     "Fredouille"
    ],
    [
     "Jean-François",
     "Bonastre"
    ]
   ],
   "title": "Broadcast news speaker tracking for ESTER 2005 campaign",
   "original": "i05_2445",
   "page_count": 4,
   "order": 652,
   "p1": "2445",
   "pn": "2448",
   "abstract": [
    "This paper presents the speaker tracking system of the LIA laboratory, validated during ESTER 2005 campaign on a radio broadcast news corpus of about 90 h. The LIA speaker tracking system firstly uses an acoustic class segmentation in order to suppress non speech frames and to detect the speech conditions. Secondly, a speaker diarization process is applied in order to provide speaker detection system (the last step) with speaker homogeneous segments (boundaries and clustering). The speaker detection system uses UBM/GMM likelihood ratios in order to decide if a segment belongs to one tracked speaker. The speaker tracking system is presented and some results obtained during ESTER 2005 campaign are proposed. The presented systems are based on the ALIZE platform and are available thanks to an open software licence.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-652"
  },
  "moraru05_interspeech": {
   "authors": [
    [
     "Daniel",
     "Moraru"
    ],
    [
     "Mathieu",
     "Ben"
    ],
    [
     "Guillaume",
     "Gravier"
    ]
   ],
   "title": "Experiments on speaker tracking and segmentation in radio broadcast news",
   "original": "i05_3049",
   "page_count": 4,
   "order": 653,
   "p1": "3049",
   "pn": "3052",
   "abstract": [
    "In this paper we describe the speaker tracking and clustering system that we implemented for the Ester evaluation campaign. We present some experiments on normalization in speaker tracking, in particular concerning the use of t-norm for speaker tracking in broadcast news. Results show that the use of t-norm significantly improves the performance at low false alarm rates. In a second part of the paper, we study the possible interactions between speaker tracking and speaker segmentation (also known as speaker diarization). We show that speaker segmentation benefits from the use of speaker tracking as a prior information while the contrary is not true. Using speaker tracking before clustering can decrease the speaker segmentation error by 4% absolute.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-653"
  },
  "dalmasso05_interspeech": {
   "authors": [
    [
     "Emanuele",
     "Dalmasso"
    ],
    [
     "Pietro",
     "Laface"
    ],
    [
     "Daniele",
     "Colibro"
    ],
    [
     "Claudio",
     "Vair"
    ]
   ],
   "title": "Unsupervised segmentation and verification of multi-speaker conversational speech",
   "original": "i05_3053",
   "page_count": 4,
   "order": 654,
   "p1": "3053",
   "pn": "3056",
   "abstract": [
    "This paper presents our approach to unsupervised multi-speaker conversational speech segmentation.\n",
    "Speech segmentation is obtained in two steps that employ different techniques. The first step performs a preliminary segmentation of the conversation analyzing fixed length slices, and assumes the presence in every slice of one or two speakers. The second step clusters the segments obtained by the previous step, estimates the number of speaker, and refines the segment boundaries using more accurate models.\n",
    "We evaluated our algorithms on the speaker segmentation tasks proposed by the 2000 NIST speaker recognition evaluation where the proposed approach produces state-of-the art segmentation error rates and on the 2004 NIST multi-speaker conversation tests where we compare the verification performance using automatically segmented training data with the one obtained using single speaker data.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-654"
  },
  "krstulovic05_interspeech": {
   "authors": [
    [
     "Sacha",
     "Krstulovic"
    ],
    [
     "Frédéric",
     "Bimbot"
    ],
    [
     "Delphine",
     "Charlet"
    ],
    [
     "Olivier",
     "Boëffard"
    ]
   ],
   "title": "Focal speakers: a speaker selection method able to deal with heterogeneous similarity criteria",
   "original": "i05_3057",
   "page_count": 4,
   "order": 655,
   "p1": "3057",
   "pn": "3060",
   "abstract": [
    "In the context of the Neologos speech database creation project, we have studied several methods for the selection of representative speaker recordings. These methods operate a selection by optimizing a quality criterion defined in various speaker similarity modeling frameworks. The obtained selections can be cross-validated in the modeling frameworks which were not used for the optimization. The compared methods include K-Medians clustering, Hierarchical clustering, and a new method called the selection of Focal Speakers. Among these, only the new method is able to solve the joint optimization, across all the modeling frameworks, of the selection of representative speakers.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-655"
  },
  "ben05_interspeech": {
   "authors": [
    [
     "Mathieu",
     "Ben"
    ],
    [
     "Guillaume",
     "Gravier"
    ],
    [
     "Frédéric",
     "Bimbot"
    ]
   ],
   "title": "A model space framework for efficient speaker detection",
   "original": "i05_3061",
   "page_count": 4,
   "order": 656,
   "p1": "3061",
   "pn": "3064",
   "abstract": [
    "In this paper, we investigate the use of a distance between Gaussian mixture models for speaker detection. The proposed distance is derived from the KL divergence and is defined as a Euclidean distance in a particular model space. This distance is simply computable directly from the model parameters thus leading to a very efficient scoring process. This new framework for scoring is compared to the classical log likelihood ratio score approach on a speaker verification task of the NIST 2004 evaluation and on the speaker tracking task of the ESTER french evaluation. Results show that the proposed approach is competitive and leads to computation times divided by a factor of more than 3.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-656"
  },
  "scheffer05_interspeech": {
   "authors": [
    [
     "Nicolas",
     "Scheffer"
    ],
    [
     "Jean-François",
     "Bonastre"
    ]
   ],
   "title": "Speaker detection using acoustic event sequences",
   "original": "i05_3065",
   "page_count": 4,
   "order": 657,
   "p1": "3065",
   "pn": "3068",
   "abstract": [
    "Novel approaches using high level features have recently shown up in the speaker recognition field. They basically consist in modeling speakers using linguistic features such as words, phonemes, idiolects. The benefit of these features was demonstrated in NIST campaigns. Their main disadvantage is their need of a huge amount of data to be efficient. The purpose of this study is to generalize this approach by using acoustic events, generated by a GMM, as input features. A methodology to build a dictionary and to model speakers using symbol sequences from this dictionary is derived. Different experiments on NIST SRE 2004 database show that the information produced is speaker specific and that a fusion experiment with a GMM verification system improves performance.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-657"
  },
  "tsai05_interspeech": {
   "authors": [
    [
     "Wei-Ho",
     "Tsai"
    ],
    [
     "Hsin-Min",
     "Wang"
    ]
   ],
   "title": "Speaker clustering of unknown utterances based on maximum purity estimation",
   "original": "i05_3069",
   "page_count": 4,
   "order": 658,
   "p1": "3069",
   "pn": "3072",
   "abstract": [
    "This paper addresses the problem of automatically grouping unknown speech utterances that are from the same speaker. A clustering method based on maximum purity estimation is proposed, with the aim of maximizing the similarities of voice characteristics between utterances within all the clusters. This method employs a genetic algorithm to determine the cluster where each utterance should be located, which overcomes the limitation of conventional hierarchical clustering that the final result can only reach the local optimum. The proposed clustering method also incorporates a Bayesian information criterion to determine how many clusters should be created.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-658"
  },
  "zochova05_interspeech": {
   "authors": [
    [
     "Petra",
     "Zochová"
    ],
    [
     "Vlasta",
     "Radová"
    ]
   ],
   "title": "Modified DISTBIC algorithm for speaker change detection",
   "original": "i05_3073",
   "page_count": 4,
   "order": 659,
   "p1": "3073",
   "pn": "3076",
   "abstract": [
    "The paper deals with the problem of automatic speaker change detection. A metric-based algorithm, called MDISTBIC, which means Modified DISTBIC, is proposed in this paper. The algorithm originates from the DISTBIC algorithm and modifies it in order to reach a higher efficiency. Both the DISTBIC and the MDISTBIC methods are tested in a number of experiments. As the results show, the MDISTBIC algorithm is more efficient than the DISTBIC algorithm in a majority of tests.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-659"
  },
  "gonon05_interspeech": {
   "authors": [
    [
     "Gilles",
     "Gonon"
    ],
    [
     "Rémi",
     "Gribonval"
    ],
    [
     "Frédéric",
     "Bimbot"
    ]
   ],
   "title": "Decision trees with improved efficiency for fast speaker verification",
   "original": "i05_3077",
   "page_count": 4,
   "order": 660,
   "p1": "3077",
   "pn": "3080",
   "abstract": [
    "Classification and regression trees (CART) are convenient for low complexity speaker recognition on embedded devices. However, former attempts at using trees performed quite poorly compared to state of the art results with Gaussian Mixture Models (GMM). In this article, we introduce some solutions to improve the efficiency of the tree-based approach. First, we propose to use at the tree construction level different types of information from the GMM used in state of the art techniques. Then, we model the score function within each leaf of the tree by a linear score function. Considering a baseline state of the art system with an equal error rate (EER) of 8.6% on the NIST 2003 evaluation, a previous CART method provides typical EER ranging between 16% and 18% while the proposed improvements decrease the EER to 11.5%, with a computational cost suitable for embedded devices.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-660"
  },
  "eveno05_interspeech": {
   "authors": [
    [
     "Nicolas",
     "Eveno"
    ],
    [
     "Laurent",
     "Besacier"
    ]
   ],
   "title": "A speaker independent \"liveness\" test for audio-visual biometrics",
   "original": "i05_3081",
   "page_count": 4,
   "order": 661,
   "p1": "3081",
   "pn": "3084",
   "abstract": [
    "In biometrics, it is crucial to detect impostors and thwart replay attacks. However, few researches have focused yet on the \"liveness\" verification. This test ensures that biometric cues being acquired are actual measurements from a live person who is present at the time of capture. Here, we propose a speaker independent \"liveness\" verification method for audio-video identification systems. It uses the correlation that exists between the lip movements and the speech produced. Two data analysis methods are considered to model this statistical link. Finally, according to tests carried out on the XM2VTS database, the best liveness verification EER achieved is 14.5%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-661"
  },
  "kuroiwa05_interspeech": {
   "authors": [
    [
     "Shingo",
     "Kuroiwa"
    ],
    [
     "Yoshiyuki",
     "Umeda"
    ],
    [
     "Satoru",
     "Tsuge"
    ],
    [
     "Fuji",
     "Ren"
    ]
   ],
   "title": "Distributed speaker recognition using speaker-dependent VQ codebook and earth mover's distance",
   "original": "i05_3085",
   "page_count": 4,
   "order": 662,
   "p1": "3085",
   "pn": "3088",
   "abstract": [
    "In this paper, we propose a distributed speaker recognition method using a non-parametric speaker model and Earth Mover's Distance (EMD). In distributed speaker recognition, the quantized feature vectors are sent to a server. The Gaussian mixture model (GMM), the traditional method used for speaker recognition, is trained using the maximum likelihood approach. However, it is difficult to fit continuous density functions to quantized data. To overcome this problem, the proposed method represents each speaker model with a speaker-dependent VQ code histogram designed by registered feature vectors and directly calculates the distance between the histograms of speaker models and testing quantized feature vectors. To measure the distance between each speaker model and testing data, we use EMD which can calculate the distance between histograms with different bins. We conducted text-independent speaker identification experiments using the proposed method. Compared to results using the traditional GMM, the proposed method yielded relative error reductions of 85% for quantized data.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-662"
  },
  "leung05_interspeech": {
   "authors": [
    [
     "Ka-Yee",
     "Leung"
    ],
    [
     "Man-Wai",
     "Mak"
    ],
    [
     "Manhung",
     "Siu"
    ],
    [
     "Sun-Yuan",
     "Kung"
    ]
   ],
   "title": "Speaker verification via articulatory feature-based conditional pronunciation modeling with vowel and consonant mixture models",
   "original": "i05_3089",
   "page_count": 4,
   "order": 663,
   "p1": "3089",
   "pn": "3092",
   "abstract": [
    "Articulatory feature-based conditional pronunciation modeling (AFCPM) aims to capture the pronunciation characteristics of speakers by modeling the linkage between the states of articulation during speech production and the actual phones produced by a speaker. Previous AFCPM systems use one discrete density function for each phoneme to model the pronunciation characteristics of speakers. This paper proposes using a mixture of discrete density functions for AFCPM. In particular, the pronunciation characteristics of each phoneme is modeled by two density functions: one responsible for describing the articulatory features that are more relevant to vowels and the other for consonants. Verification scores are the weighted sum of the outputs of the two models. To enhance the resolution of the pronunciation models, four articulatory properties (front-back, lip-rounding, place of articulation, and manner of articulation) are used for pronunciation modeling. The proposed AFCPM is applied to a speaker verification task. Results show that using four articulatory features achieves a lower error rate as compared to using two features (manner and place of articulation) only. It was also found that dividing the articulatory properties into two groups is an effective means of solving the data-sparseness problem encountered in the training phase of AFCPM systems.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-663"
  },
  "chen05e_interspeech": {
   "authors": [
    [
     "Jixu",
     "Chen"
    ],
    [
     "Beiqian",
     "Dai"
    ],
    [
     "Jun",
     "Sun"
    ]
   ],
   "title": "Prosodic features based on wavelet analysis for speaker verification",
   "original": "i05_3093",
   "page_count": 4,
   "order": 664,
   "p1": "3093",
   "pn": "3096",
   "abstract": [
    "Most conventional speaker recognition systems rely on short-term spectral information. But they ignore the long-term information such as prosody which also conveys speaker information. In this paper, we propose an approach that extracts prosodic features based on long-term information. First, by making wavelet analysis, we can reveal the trends of the f0 and energy contour. Subsequently, the prosodic features are extracted only from approximation coefficients. We use these features in a GMM-UBM based text-independent speaker verification system. The proposed method achieves an EER of 23.3% on the NIST2004 8sides-1side task scheme. This result is promising while the baseline system, which uses short-term f0 feature, only results in an EER of 33.49% in this task.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-664"
  },
  "mihoubi05_interspeech": {
   "authors": [
    [
     "M.",
     "Mihoubi"
    ],
    [
     "Douglas",
     "O'Shaughnessy"
    ],
    [
     "P.",
     "Dumouchel"
    ]
   ],
   "title": "Relevant information extraction for discriminative training applied to speaker identification",
   "original": "i05_3097",
   "page_count": 4,
   "order": 665,
   "p1": "3097",
   "pn": "3100",
   "abstract": [
    "Speech signals convey information from many sources, but not all information sources are relevant to describe speaker identity. In fact, speech is affected by spurious events, artifacts (mouth breath, lip clicks), and noise (channel and background). Such unwanted information sources are shared by speakers and do not contribute in distinguishing between them. Furthermore, in most cases, training data are collected from different environments and it is of great importance that such data convey relevant joint information. This paper discusses a method for removing unwanted information in order to build more robust models. Two criteria are used to extract relevant information from the speech signal: the first criterion, which we call self-information criterion, is used to extract relevant information from data collected from a given environment; the second is called joint information criterion, and it is used when collected data are from different environments. Both criteria originate from information theory. Simulations on telephone speech have revealed the high efficiency of the method.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-665"
  },
  "louradour05_interspeech": {
   "authors": [
    [
     "Jérôme",
     "Louradour"
    ],
    [
     "Khalid",
     "Daoudi"
    ]
   ],
   "title": "Conceiving a new sequence kernel and applying it to SVM speaker verification",
   "original": "i05_3101",
   "page_count": 4,
   "order": 666,
   "p1": "3101",
   "pn": "3104",
   "abstract": [
    "In this paper, we propose a new approach for sequence classification which is based on the framework of Reproducing Kernel Hilbert Spaces (RKHS). We introduce the theoretical material which leads to the formulation of an original sequence kernel, that we implement in a SVM scheme. Experiments are carried out on a speaker verification task using NIST SRE data. They show that our new sequence kernel significantly outperforms the generative approach with Gaussian Mixture Models (GMM). They also show that it generally outperforms the powerful Generalized Linear Discriminant Sequence (GLDS) kernel, while offering more efficiency and flexibility (than GLDS).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-666"
  },
  "deng05d_interspeech": {
   "authors": [
    [
     "Jing",
     "Deng"
    ],
    [
     "Thomas Fang",
     "Zheng"
    ],
    [
     "Jian",
     "Liu"
    ],
    [
     "Wenhu",
     "Wu"
    ]
   ],
   "title": "The predictive differential amplitude spectrum for robust speaker recognition in stationary noises",
   "original": "i05_3105",
   "page_count": 4,
   "order": 667,
   "p1": "3105",
   "pn": "3108",
   "abstract": [
    "The performance of any speaker identification system degrades quite seriously when the acoustic conditions for testing mismatch those for training. In this paper, we propose a method to restore clean speech from noisy speech with two steps: 1) a predictive difference function is employed to estimate the differential amplitude spectrums (DAS) from both the left-side and right-side of the amplitude spectrum of the noisy speech, so as to eliminate the noise as precisely as possible, and 2) an average of the left-side and rightside integral DASs is taken as the estimated amplitude spectrum of the original clean speech. The spectrum in the traditional MFCC calculation is then replaced with this estimated amplitude and the extracted features based on this are referred to as predictive differential amplitude spectrum (PDAS) based cepstral coefficients (PDASCCs). We compare PDASCCs with cepstral mean subtraction (CMS) based, spectral subtraction (SS) based, and differential power spectrum (DPS) based cepstral coefficients at different noise levels. Experimental results show that the PDASCCs are more effective in enhancing the robustness of a speaker recognition system, and used with the CMS method the average error rate can be reduced by 7.5%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-667"
  },
  "mason05_interspeech": {
   "authors": [
    [
     "Michael",
     "Mason"
    ],
    [
     "Robbie",
     "Vogt"
    ],
    [
     "Brendan",
     "Baker"
    ],
    [
     "Sridha",
     "Sridharan"
    ]
   ],
   "title": "Data-driven clustering for blind feature mapping in speaker verification",
   "original": "i05_3109",
   "page_count": 4,
   "order": 668,
   "p1": "3109",
   "pn": "3112",
   "abstract": [
    "Handset and channel mismatch degrades the performance of automatic speaker recognition systems significantly. This paper enhances the feature mapping technique by proposing an iterative clustering approach to context model generation which offers an improvement in the performance of feature mapping trained on labelled data and offers the potential to train feature mapping in the absence of correctly labelled background data. The performance of the clustered feature mapping models is demonstrated on an expanded version of the NIST 2003 Extended Data Task (EDT) protocol.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-668"
  },
  "zhou05_interspeech": {
   "authors": [
    [
     "Xi",
     "Zhou"
    ],
    [
     "Zhi-qiang",
     "Yao"
    ],
    [
     "Beiqian",
     "Dai"
    ]
   ],
   "title": "Improved covariance modeling for GMM in speaker identification",
   "original": "i05_3113",
   "page_count": 4,
   "order": 669,
   "p1": "3113",
   "pn": "3116",
   "abstract": [
    "Gaussian Mixture Model (GMM) with diagonal covariance matrix is commonly used in text-independent speaker identification. However, diagonal covariance matrix implies strong assumption that the feature elements are independent. Even Gaussian mixtures with diagonal covariance can model the correlation to some extent; the model precision is still limited. To alleviate this problem, this paper proposes a framework for sharing linear transformations among the components and introduces a new unsupervised hierarchical clustering algorithm to implement it. In the framework, the full covariance of each component is represented by shared linear transformation and component-specific diagonal covariance. Different linear transformation estimation approaches, i.e., PCA, LDA and MLLT, are proposed and compared. Experiments show that our algorithm using each of the approaches has achieved significant identification error reduction over the best diagonal covariance models.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-669"
  },
  "vogt05_interspeech": {
   "authors": [
    [
     "Robbie",
     "Vogt"
    ],
    [
     "Brendan",
     "Baker"
    ],
    [
     "Sridha",
     "Sridharan"
    ]
   ],
   "title": "Modelling session variability in text-independent speaker verification",
   "original": "i05_3117",
   "page_count": 4,
   "order": 670,
   "p1": "3117",
   "pn": "3120",
   "abstract": [
    "Presented is an approach to modelling session variability for GMM-based text-independent speaker verification incorporating a constrained session variability component in both the training and testing procedures. The proposed technique reduces the data labelling requirements and removes discrete categorisation needed by techniques such as feature mapping and H-Norm, while providing superior performance. Experiments on Switchboard-II conversational telephony data show improvements of as much as 48% in detection cost with a single training utterance and 68% with multiple training utterances over a baseline system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-670"
  },
  "siafarikas05_interspeech": {
   "authors": [
    [
     "Mihalis",
     "Siafarikas"
    ],
    [
     "Todor",
     "Ganchev"
    ],
    [
     "Nikolaos",
     "Fakotakis"
    ],
    [
     "George",
     "Kokkinakis"
    ]
   ],
   "title": "Overlapping wavelet packet features for speaker verification",
   "original": "i05_3121",
   "page_count": 4,
   "order": 671,
   "p1": "3121",
   "pn": "3124",
   "abstract": [
    "A generalization of the Discrete Wavelet Packet Transform (DWPT), referred to as Overlapping Discrete Wavelet Packet Transform (ODWPT), is proposed. In contrast to the traditional DWPT, the ODWPT assumes overlapping among the frequency sub-bands at various levels of the transform. Based on this overlapping strategy, a new set of speech features that is specially designed for speaker recognition is derived. The practical significance of our approach has been evaluated in comparative experiments performed on the 2001 NIST Speaker Recognition Evaluation database. The proposed feature set has proved to outperform the widely-used Mel-frequency cepstral coefficients (MFCCs) and other wavelet packet-based speech features that were successfully utilized for speaker recognition.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-671"
  },
  "yin05_interspeech": {
   "authors": [
    [
     "An-rong",
     "Yin"
    ],
    [
     "Xiang",
     "Xie"
    ],
    [
     "Jingming",
     "Kuang"
    ]
   ],
   "title": "Using Hadamard ECOC in multi-class problems based on SVM",
   "original": "i05_3125",
   "page_count": 4,
   "order": 672,
   "p1": "3125",
   "pn": "3128",
   "abstract": [
    "In this paper, we propose to apply Hadamard Error-Correcting Output Code (Hadamard ECOC) to extend binary classifier for multi-class classification problems. Hadamard ECOC is easy to construct and is suitable for any number of classes. We combine it with binary support vector machine (SVM) to solve the multi-class problem of speaker identification, which takes advantage of error correcting ability of Hadamard ECOC and powerful classification ability of SVM. Compared to the traditional \"1-against-rest\" method, the experiment result shows that Hadamard ECOC approach has much better and more stable performance for the multi-class problem and is robust on different rules mapping rules between ECOCs and classes.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-672"
  },
  "cohen05_interspeech": {
   "authors": [
    [
     "Israel",
     "Cohen"
    ]
   ],
   "title": "Supergaussian GARCH models for speech signals",
   "original": "i05_2053",
   "page_count": 4,
   "order": 673,
   "p1": "2053",
   "pn": "2056",
   "abstract": [
    "In this paper, we introduce supergaussian generalized autoregressive conditional heteroscedasticity (GARCH) models for speech signals in the short-time Fourier transform (STFT) domain. We address the problem of speech enhancement, and show that estimating the variances of the STFT expansion coefficients based on GARCH models yields higher speech quality than by using the decision-directed method, whether the fidelity criterion is minimum mean-squared error (MMSE) of the spectral coefficients or MMSE of the log-spectral amplitude (LSA). Furthermore, while a Gaussian model is inferior to Gamma and Laplacian models when estimating the variances by the decision-directed method, a Gaussian model is superior when using the GARCH modeling method. This facilitates MMSE-LSA estimation, while taking into consideration the heavy-tailed distribution.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-673"
  },
  "mouchtaris05_interspeech": {
   "authors": [
    [
     "A.",
     "Mouchtaris"
    ],
    [
     "J. Van der",
     "Spiegel"
    ],
    [
     "P.",
     "Mueller"
    ],
    [
     "P.",
     "Tsakalides"
    ]
   ],
   "title": "A spectral conversion approach to feature denoising and speech enhancement",
   "original": "i05_2057",
   "page_count": 4,
   "order": 674,
   "p1": "2057",
   "pn": "2060",
   "abstract": [
    "In this paper we demonstrate that spectral conversion can be successfully applied to the speech enhancement problem as a feature denoising method. The enhanced spectral features can be used in the context of the Kalman filter for estimating the clean speech signal. In essence, instead of estimating the clean speech features and the clean speech signal using the iterative Kalman filter, we show that is more efficient to initially estimate the clean speech features from the noisy speech features using spectral conversion (using a training speech corpus) and then apply the standard Kalman filter. Our results show an average improvement compared to the iterative Kalman filter that can reach 6 dB in the average segmental output Signal-to-Noise Ratio (SNR), in low input SNR's.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-674"
  },
  "ortega05_interspeech": {
   "authors": [
    [
     "Alfonso",
     "Ortega"
    ],
    [
     "Eduardo",
     "Lleida"
    ],
    [
     "Enrique",
     "Masgrau"
    ],
    [
     "Luis",
     "Buera"
    ],
    [
     "Antonio",
     "Miguel"
    ]
   ],
   "title": "Acoustic feedback cancellation in speech reinforcement systems for vehicles",
   "original": "i05_2061",
   "page_count": 4,
   "order": 675,
   "p1": "2061",
   "pn": "2064",
   "abstract": [
    "Passengers communication inside a car can be improved by using a speech reinforcement system. This system picks up the speech of each passenger, amplifies it and plays it back into the cabin through the loudspeakers of the car. Due to the electro-acoustic coupling between loudspeakers and microphones, a closed-loop system is created. To avoid the risk of instability due to the acoustic feedback, acoustic echo cancellation must be performed. Using the Minimum Mean Square Error (MMSE) criterion to adapt the filter, what is very common in acoustic echo cancellation, leads to inaccurate estimates of the Loudspeaker-Enclosure-Microphone (LEM) path due to the closed-loop operation of the system. In this paper, the solution obtained with the MMSE criterion for a Finite-length Impulse Response (FIR) causal adaptive filter is derived, showing that the identification error depends on the amplification factor of the system, the delay of the loop and the spectral characteristics of the excitation signal. The use of whitening filters is proposed and justified to improve the acoustic echo cancellation in speech reinforcement systems for cars. Results obtained for a one-channel speech reinforcement system are presented.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-675"
  },
  "bourgeois05_interspeech": {
   "authors": [
    [
     "Julien",
     "Bourgeois"
    ],
    [
     "Jürgen",
     "Freudenberger"
    ],
    [
     "Guillaume",
     "Lathoud"
    ]
   ],
   "title": "Implicit control of noise canceller for speech enhancement",
   "original": "i05_2065",
   "page_count": 4,
   "order": 676,
   "p1": "2065",
   "pn": "2068",
   "abstract": [
    "Widrow's interference canceller, adapted by the normalized LMS (NLMS), is a standard approach for separating signals from multiple speakers, for example from the driver (target) and the codriver (interferer) in a car. In practice, the adaptation must be carried out only when the interferer is dominant, i.e. only when some estimate of the signal-to-interference ratio (SIR) is below a certain threshold. In this paper, we present the Implicitly controlled LMS (ILMS), a modification of the NLMS. ILMS adaptation is performed continuously using a variable step-size, whose design implicitly detects dominance of the interferer over target activity. Specific measures are taken to guarantee the stability during adaptation. Theoretical analysis of the ILMS transient convergence and stability conditions prove significant improvement with respect to the original NLMS. Experimental results on real in-car data assess the predicted behavior.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-676"
  },
  "kumar05_interspeech": {
   "authors": [
    [
     "T. M. Sunil",
     "Kumar"
    ],
    [
     "T. V.",
     "Sreenivas"
    ]
   ],
   "title": "Speech enhancement using Markov model of speech segments",
   "original": "i05_2069",
   "page_count": 4,
   "order": 677,
   "p1": "2069",
   "pn": "2072",
   "abstract": [
    "It has been shown that the Iterative Weiner Filtering (IWF) requires both intra-frame and inter-frame constraints to ensure that the enhanced speech spectra possess natural characteristics of speech. One automated way to apply the intra-frame constraints is Codebook Constrained Wiener Filtering (CCWF). In the present work, we propose a new method of imposing the inter-frame constraints based on Markov modeling of speech segments. We will show that the proposed method improves both Average segmental log-likelihood ratio and Average segmental SNR of the enhanced speech even at SNRs below 0dB.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-677"
  },
  "braquet05_interspeech": {
   "authors": [
    [
     "Vladimir",
     "Braquet"
    ],
    [
     "Takao",
     "Kobayashi"
    ]
   ],
   "title": "A wavelet based noise reduction algorithm for speech signal corrupted by coloured noise",
   "original": "i05_2073",
   "page_count": 4,
   "order": 678,
   "p1": "2073",
   "pn": "2076",
   "abstract": [
    "In this paper, we present a node dependent wavelet thresholding approach in order to remove strongly coloured noises from speech signals. The noise power in each node is first estimated using a recursive method. Given the voiced or unvoiced nature of the frame, the signal is expanded onto a predefined best basis. Then a infinitely smooth soft threshold is applied depending on each node of the decomposition tree. Finally the estimated clean signal is reconstructed. Experimental results on a Japanese database, for various coloured noises, demonstrate the effectiveness of the proposed method, even at low SNR. Compared with the common level dependent method, this algorithm provides better denoising results.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-678"
  },
  "zavarehei05_interspeech": {
   "authors": [
    [
     "Esfandiar",
     "Zavarehei"
    ],
    [
     "Saeed",
     "Vaseghi"
    ]
   ],
   "title": "Speech enhancement in temporal DFT trajectories using Kalman filters",
   "original": "i05_2077",
   "page_count": 4,
   "order": 679,
   "p1": "2077",
   "pn": "2080",
   "abstract": [
    "In this paper a time-frequency estimator for enhancement of noisy speech signals in the DFT domain is introduced. This estimator is based on modelling and filtering the temporal trajectories of the DFT components of noisy speech signal using Kalman filters. The time-varying trajectory of the DFT components of speech is modelled by a low order autoregressive process incorporated in the state equation of Kalman filter. The performance of the proposed method for the enhancement of noisy speech is evaluated and compared with MMSE log-STSA estimator and parametric spectral subtraction. Evaluation results show that the incorporation of temporal information through Kalman filters results in reduced residual noise and improved perceived quality of speech.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-679"
  },
  "yan05_interspeech": {
   "authors": [
    [
     "Qin",
     "Yan"
    ],
    [
     "Saeed",
     "Vaseghi"
    ],
    [
     "Esfandiar",
     "Zavarehei"
    ],
    [
     "Ben",
     "Milner"
    ]
   ],
   "title": "Formant-tracking linear prediction models for speech processing in noisy environments",
   "original": "i05_2081",
   "page_count": 4,
   "order": 680,
   "p1": "2081",
   "pn": "2084",
   "abstract": [
    "This paper presents a formant-tracking method for estimation of the time-varying trajectories of a linear prediction (LP) model of speech in noise. The main focus of this work is on the modelling of the non-stationary temporal trajectories of the formants of speech for improved LP model estimation in noise. The proposed approach provides a systematic framework for modelling the interframe correlation of speech parameters across successive frames, the intra-frame correlations are modelled by LP parameters. The formant-tracking LP model estimation is composed of two stages: (a) a pre-cleaning intra-frame spectral amplitude estimation stage where an initial estimate of the magnitude frequency response of the LP model of clean speech is obtained and (b) an inter-frame signal processing stage where formant classification and Kalman filters are combined to estimate the trajectory of formants. The effects of car and train noise on the observations and estimation of formants tracks are investigated. The average formant tracking errors at different signal to noise ratios (SNRs) are computed. The evaluation results demonstrate that after noise reduction and Kalman filtering the formant tracking errors are significantly reduced.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-680"
  },
  "jiang05_interspeech": {
   "authors": [
    [
     "Hui",
     "Jiang"
    ],
    [
     "Qian-Jie",
     "Fu"
    ]
   ],
   "title": "Statistical noise compensation for cochlear implant processing",
   "original": "i05_2085",
   "page_count": 4,
   "order": 681,
   "p1": "2085",
   "pn": "2088",
   "abstract": [
    "A statistical noise compensation algorithm is proposed for cochlear implant processing to improve cochlear implant patients' speech performance in noise. With the well-known environmental model for speech in additive noise, the MMSE (minimum mean square error) estimation of clean speech signals was derived according to the noisy speech observation based on a linear approximation of the original nonlinear environmental model. Words-in-sentences recognition by four cochlear implant subjects was tested under different noisy listening conditions (steady white noise and 6-talker speech babble at +15, +10, +5, and 0 dB SNR) with and without the noise compensation algorithm. For steady white noise, a mean improvement of 36% correct of sentence recognition scores was obtained across the SNR levels when the noise compensation algorithm was applied to cochlear implant processing. However, the amount of improvement was highly dependent on the SNR levels with the speech babble noise. The improvement was gradually increased from 7% to 32% correct when the SNR levels increased from 0 dB to 15 dB. The results suggest that cochlear implant patients may significantly benefit from the proposed noise compensation algorithm in noisy listening.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-681"
  },
  "pham05_interspeech": {
   "authors": [
    [
     "Tuan Van",
     "Pham"
    ],
    [
     "Gernot",
     "Kubin"
    ]
   ],
   "title": "WPD-based noise suppression using nonlinearly weighted threshold quantile estimation and optimal wavelet shrinking",
   "original": "i05_2089",
   "page_count": 4,
   "order": 682,
   "p1": "2089",
   "pn": "2092",
   "abstract": [
    "A novel speech enhancement system based on wavelet packet decomposition (WPD) is proposed. Noise level is estimated based on quantile in wavelet threshold domain. To handle colored and non-stationary noises, the universal thresholds are weighted by a time-frequency dependent nonlinear function. Two nonlinear weighting methods using temporal threshold variation and kernel smoothing are proposed. The weighted thresholds are smoothed and employed for wavelet shrinking with an adaptive factor to compress noise while preserving speech quality. The proposed system is evaluated and compared with other algorithms based on spectral subtraction via objective measures and subjective tests to demonstrate its superior performance.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-682"
  },
  "li05f_interspeech": {
   "authors": [
    [
     "Weifeng",
     "Li"
    ],
    [
     "Katunobu",
     "Itou"
    ],
    [
     "Kazuya",
     "Takeda"
    ],
    [
     "Fumitada",
     "Itakura"
    ]
   ],
   "title": "Subjective and objective quality assessment of regression-enhanced speech in real car environments",
   "original": "i05_2093",
   "page_count": 4,
   "order": 683,
   "p1": "2093",
   "pn": "2096",
   "abstract": [
    "In this paper, we propose a nonlinear regression method for speech enhancement, whose idea approximates the log spectra of clean speech with the inputs of the log spectra of noisy speech and estimated noise. We compared both subjective and objective assessments on regression-enhanced speech to those obtained through spectral subtraction (SS) and short-time spectral amplitude (STSA) methods. Our subjective evaluation experiments, which included Mean Opinion Score (MOS) and Pairwise Preference Test (PPT), show that the proposed regression-based speech enhancement method provides consistent improvements in overall quality in all seven driving conditions. The proposed method also performs the best in most objective measures.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-683"
  },
  "unoki05_interspeech": {
   "authors": [
    [
     "Masashi",
     "Unoki"
    ],
    [
     "Masaaki",
     "Kubo"
    ],
    [
     "Atsushi",
     "Haniu"
    ],
    [
     "Masato",
     "Akagi"
    ]
   ],
   "title": "A model for selective segregation of a target instrument sound from the mixed sound of various instruments",
   "original": "i05_2097",
   "page_count": 4,
   "order": 684,
   "p1": "2097",
   "pn": "2100",
   "abstract": [
    "In this paper, as a first step towards constructing a selective sound segregation model for use in modeling phenomena such as the cocktail party effect, we consider a basic problem of selective sound segregation for instrument sounds using a single-channel method (monaural processing). We propose a model concept for selective sound segregation based on auditory scene analysis and then describe implementation of a model for segregating target instrument sound from the mixed sound of various instruments. The proposed model consists of two blocks: a model of segregating two acoustic sources as bottom-up processing, and selective processing based on knowledge sources as top-down processing. Two simulations were done to evaluate the proposed model combining bottom-up and top-down processing. Results showed that the model could selectively segregate the target instrument sound from the mixed sound by using prior information, and that using both the bottom-up and top-down processing was more effective than using either separately. Since these simulations can be interpreted as representing concurrent vowel segregation in the case of a speech signal, it should be possible to extend the proposed model to a selective speech segregation model.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-684"
  },
  "hendriks05_interspeech": {
   "authors": [
    [
     "Richard C.",
     "Hendriks"
    ],
    [
     "Richard",
     "Heusdens"
    ],
    [
     "Jesper",
     "Jensen"
    ]
   ],
   "title": "Improved decision directed approach for speech enhancement using an adaptive time segmentation",
   "original": "i05_2101",
   "page_count": 4,
   "order": 685,
   "p1": "2101",
   "pn": "2104",
   "abstract": [
    "Short-time Fourier transform (STFT) methods are often used to overcome the degradation of speech signals affected by noise. STFT-gain functions are usually expressed as a function of the a priori SNR, say ξ, and good techniques to estimate ξ are of vital importance for the quality of enhanced speech. Often, ξ is estimated using the so-called decision directed approach (DD). However, the DD approach builds on a number of approximations, where certain expected values of signal related quantities are approximated by instantaneous estimates. In this paper we present a method to improve these approximations by combining the DD approach with an adaptive time segmentation. Objective and subjective experiments show that the proposed method leads to significant improvements compared to the conventional DD approach. Furthermore, simulation experiments confirm a decreased amount of non-stationary residual noise.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-685"
  },
  "lollmann05_interspeech": {
   "authors": [
    [
     "Heinrich W.",
     "Lollmann"
    ],
    [
     "Peter",
     "Vary"
    ]
   ],
   "title": "Generalized filter-bank equalizer for noise reduction with reduced signal delay",
   "original": "i05_2105",
   "page_count": 4,
   "order": 686,
   "p1": "2105",
   "pn": "2108",
   "abstract": [
    "An efficient realization of a low delay filter-bank, termed as generalized filter-bank equalizer (FBE), will be proposed for noise reduction with low signal delay. The FBE is equivalent to a timedomain filter with coefficients adapted in the frequency-domain. This filter-bank structure ensures perfect signal reconstruction for a variety of spectral transforms with less restrictions than for an analysis-synthesis filter-bank (AS FB). A non-uniform frequency resolution can be achieved by an allpass transformation. In this case, the FBE has not only a lower signal delay than the AS FB, but also a lower algorithmic complexity for most parameter configurations. Another advantage of the FBE is the lower number of required delay elements (memory) compared to the AS FB. The noise reduction achieved by means of the AS FB and the FBE is approximately equal.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-686"
  },
  "roman05_interspeech": {
   "authors": [
    [
     "Nicoleta",
     "Roman"
    ],
    [
     "DeLiang",
     "Wang"
    ]
   ],
   "title": "A pitch-based model for separation of reverberant speech",
   "original": "i05_2109",
   "page_count": 4,
   "order": 687,
   "p1": "2109",
   "pn": "2112",
   "abstract": [
    "In everyday listening, both background noise and reverberation degrade the speech signal. While monaural speech separation based on periodicity has achieved considerable progress in handling additive noise, little research has been devoted to reverberant scenarios. Reverberation smears the harmonic structure of speech signals, and our evaluations using a pitch-based separation algorithm show that an increase in the room reverberation time causes degradation in performance due to the loss in periodicity for the target signal. We propose a two-stage monaural speech separation system that combines the inverse filtering of the room impulse response corresponding to target location with a pitch-based speech segregation method. As a result of the first processing stage, the harmonicity of a signal arriving from target direction is partially restored while signals arriving from other locations are further smeared, and this leads to improved separation. A systematic evaluation shows that the proposed system results in considerable signal-to-noise ratio gains across different conditions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-687"
  },
  "zhao05_interspeech": {
   "authors": [
    [
     "David Y.",
     "Zhao"
    ],
    [
     "W. Bastiaan",
     "Kleijn"
    ]
   ],
   "title": "On noise gain estimation for HMM-based speech enhancement",
   "original": "i05_2113",
   "page_count": 4,
   "order": 688,
   "p1": "2113",
   "pn": "2116",
   "abstract": [
    "To address the variation of noise level in non-stationary noise signals, we study the noise gain estimation for speech enhancement using hidden Markov models (HMM). We consider the noise gain as a stochastic process and we approximate the probability density function (PDF) to be log-normal distributed. The PDF parameters are estimated for every signal block using the past noisy signal blocks. The approximated PDF is then used in a Bayesian speech estimator minimizing the Bayes risk for a novel cost function, that allows for an adjustable level of residual noise. As a more computationally efficient alternative, we also derive the maximum likelihood (ML) estimator, assuming the noise gain to be a deterministic parameter. The performance of the proposed gain-adaptive methods are evaluated and compared to two reference methods. The experimental results show significant improvement under noise conditions with time-varying noise energy.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-688"
  },
  "deshmukh05_interspeech": {
   "authors": [
    [
     "Om",
     "Deshmukh"
    ],
    [
     "Carol",
     "Espy-Wilson"
    ]
   ],
   "title": "Speech enhancement using auditory phase opponency model",
   "original": "i05_2117",
   "page_count": 4,
   "order": 689,
   "p1": "2117",
   "pn": "2120",
   "abstract": [
    "In this work we address the problem of single-channel speech enhancement when the speech is corrupted by additive noise. The model presented here, called the Modified Phase Opponency (MPO) model, is based on the auditory PO model, proposed by Carney et. al., for detection of tones in noise. The PO model includes a physiologically realistic mechanism for processing the information in neural discharge times and exploits the frequency-dependent phase properties of the tuned filters in the auditory periphery by using a cross-auditory-nerve-fiber coincidence detection for extracting temporal cues. Initial evaluation of the MPO model on speech corrupted by white noise at different SNRs shows that the MPO model is able to enhance the spectral peaks while suppressing the noise-only regions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-689"
  },
  "mak05_interspeech": {
   "authors": [
    [
     "Brian",
     "Mak"
    ],
    [
     "Siu-Kei Au",
     "Yeung"
    ],
    [
     "Yiu-Pong",
     "Lai"
    ],
    [
     "Manhung",
     "Siu"
    ]
   ],
   "title": "High-density discrete HMM with the use of scalar quantization indexing",
   "original": "i05_2121",
   "page_count": 4,
   "order": 690,
   "p1": "2121",
   "pn": "2124",
   "abstract": [
    "With the advance in semiconductor memory and the availability of very large speech corpora (of hundreds to thousands of hours of speech), we would like to revisit the use of discrete hidden Markov model (DHMM) in automatic speech recognition. To estimate the discrete density in a DHMM state, the acoustic space is divided into bins and one simply count the relative amount of observations falling into each bin. With a very large speech corpus, we believe that the number of bins may be greatly increased to get a much higher density than before, and we will call the new models, the high-density discrete hidden Markov model (HDDHMM). Our HDDHMM is different from traditional DHMM in two aspects: firstly, the codebook will have a size in thousands or even tens of thousands; secondly, we propose a method based on scalar quantization indexing so that for a d-dimensional acoustic vector, the discrete codeword can be determined in O(d) time. During recognition, the state probability is reduced to an O(1) table look-up. The new HDDHMM was tested on WSJ0 with 5K vocabulary. Compared with a baseline 4-stream continuous density HMM system which has a WER of 9.71%, a 4-stream HDDHMM system converted from the former achieves a WER of 11.60%, with no distance or Gaussian computation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-690"
  },
  "zheng05c_interspeech": {
   "authors": [
    [
     "Jing",
     "Zheng"
    ],
    [
     "Andreas",
     "Stolcke"
    ]
   ],
   "title": "Improved discriminative training using phone lattices",
   "original": "i05_2125",
   "page_count": 4,
   "order": 691,
   "p1": "2125",
   "pn": "2128",
   "abstract": [
    "We present an efficient discriminative training procedure utilizing phone lattices. Different approaches to expediting lattice generation, statistics collection, and convergence were studied. We also propose a new discriminative training criterion, namely, minimum phone frame error (MPFE). When combined with the maximum mutual information (MMI) criterion using I-smoothing, replacing the standard minimum phone error (MPE) criterion with MPFE led to a small but consistent win in several applications. Phone-latticebased discriminative training gave around 8% to 12% relative word error rate (WER) reduction in SRI's latest English Conversational Telephone Speech and Broadcast News transcription systems developed for DARPA's EARS project.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-691"
  },
  "zhu05b_interspeech": {
   "authors": [
    [
     "Qifeng",
     "Zhu"
    ],
    [
     "Barry Y.",
     "Chen"
    ],
    [
     "Frantisek",
     "Grezl"
    ],
    [
     "Nelson",
     "Morgan"
    ]
   ],
   "title": "Improved MLP structures for data-driven feature extraction for ASR",
   "original": "i05_2129",
   "page_count": 4,
   "order": 692,
   "p1": "2129",
   "pn": "2132",
   "abstract": [
    "In this paper, we present our recent progress on multi-layer perceptron (MLP) based data-driven feature extraction using improved MLP structures. Four-layer MLPs are used in this study. Different signal processing methods are applied before the input layer of the MLP. We show that the first hidden layer of a four-layer MLP is able to detect some basic patterns from the time-frequency plane. KLT-based dimension reduction along time is applied as a modulation frequency filter. The new feature extraction was tested on a large vocabulary continuous speech recognition (LVCSR) task using the NIST 2001 evaluation set. We achieved 11.6% relative word error rate (WER) reduction compared to the traditional PLP-based baseline feature. This is also a significant improvement compared to our previously published results on the same task using MLP-based features with three-layer MLPs.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-692"
  },
  "macherey05_interspeech": {
   "authors": [
    [
     "Wolfgang",
     "Macherey"
    ],
    [
     "Lars",
     "Haferkamp"
    ],
    [
     "Ralf",
     "Schlüter"
    ],
    [
     "Hermann",
     "Ney"
    ]
   ],
   "title": "Investigations on error minimizing training criteria for discriminative training in automatic speech recognition",
   "original": "i05_2133",
   "page_count": 4,
   "order": 693,
   "p1": "2133",
   "pn": "2136",
   "abstract": [
    "Discriminative training criteria have been shown to consistently outperform maximum likelihood trained speech recognition systems. In this paper we employ the Minimum Classification Error (MCE) criterion to optimize the parameters of the acoustic model of a large scale speech recognition system. The statistics for both the correct and the competing model are solely collected on word lattices without the use of N-best lists. Thus, particularly for long utterances, the number of sentence alternatives taken into account is significantly larger compared to N-best lists. The MCE criterion is embedded in an extended unifying approach for a class of discriminative training criteria which allows for direct comparison of the performance gain obtained with the improvements of other commonly used criteria such as Maximum Mutual Information (MMI) and Minimum Word Error (MWE). Experiments conducted on large vocabulary tasks show a consistent performance gain for MCE over MMI. Moreover, the improvements obtained with MCE turn out to be in the same order of magnitude as the performance gains obtained with the MWE criterion.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-693"
  },
  "sim05_interspeech": {
   "authors": [
    [
     "K. C.",
     "Sim"
    ],
    [
     "M. J. F.",
     "Gales"
    ]
   ],
   "title": "Temporally varying model parameters for large vocabulary continuous speech recognition",
   "original": "i05_2137",
   "page_count": 4,
   "order": 694,
   "p1": "2137",
   "pn": "2140",
   "abstract": [
    "Many forms of time varying acoustic models have been investigated for speech recognition. However, there has been little success in applying these models to Large Vocabulary Continuous Speech Recognition (LVCSR). Recently, fMPE was introduced as a discriminative feature space estimation scheme for the HMM-based LVCSR. This method estimates a projection matrix from a high dimensional space (¡« 100,000) down to a standard feature space (typically 39). This projection is then added on to the original feature vector (e.g. MFCC or PLP) to yield a feature vector to train the final model. This paper considers fMPE as a time varying model for the mean vectors by applying the time varying feature offset to the Gaussian mean vectors. This approach naturally yields the update formulae for fMPE and motivates an alternative style of training systems. This concept is then extended to the temporal precision matrix modelling (pMPE). In pMPE, a temporally varying positive scale is applied to each element of the diagonal precision matrices. Experimental results are presented on a conversational telephone speech task.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-694"
  },
  "zhu05c_interspeech": {
   "authors": [
    [
     "Qifeng",
     "Zhu"
    ],
    [
     "Andreas",
     "Stolcke"
    ],
    [
     "Barry Y.",
     "Chen"
    ],
    [
     "Nelson",
     "Morgan"
    ]
   ],
   "title": "Using MLP features in SRI's conversational speech recognition system",
   "original": "i05_2141",
   "page_count": 4,
   "order": 695,
   "p1": "2141",
   "pn": "2144",
   "abstract": [
    "We describe the development of a speech recognition system for conversational telephone speech (CTS) that incorporates acoustic features estimated by multilayer perceptrons (MLP). The acoustic features are based on frame-level phone posterior probabilities, obtained by merging two different MLP estimators, one based on PLP-Tandem features, the other based on hidden activation TRAPs (HATs) features. This paper focuses on the challenges arising when incorporating these nonstandard features into a full-scale speech-to-text (STT) system, as used by SRI in the Fall 2004 DARPA STT evaluations. First, we developed a series of time-saving techniques for training feature MLPs on 1800 hours of speech. Second, we investigated which components of a multipass, multi-front-end recognition system are most profitably augmented with MLP features for best overall performance. The final system obtained achieved a 2% absolute (10% relative) WER reduction over a comparable baseline system that did not include Tandem/HATs MLP features.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-695"
  },
  "gerosa05_interspeech": {
   "authors": [
    [
     "Matteo",
     "Gerosa"
    ],
    [
     "Diego",
     "Giuliani"
    ],
    [
     "Fabio",
     "Brugnara"
    ]
   ],
   "title": "Speaker adaptive acoustic modeling with mixture of adult and children's speech",
   "original": "i05_2193",
   "page_count": 4,
   "order": 696,
   "p1": "2193",
   "pn": "2196",
   "abstract": [
    "In this paper, speaker adaptive acoustic modeling is investigated in the context of large vocabulary speech recognition by training acoustic models with adult speech, children's speech and a mixture of adult and children's speech.\n",
    "By exploiting a limited amount (9 hours) of children's speech and a more significant amount (57 hours) of adult speech, group-specific acoustic models for children and adults, respectively, were trained using several methods for speaker adaptive acoustic modeling. In addition, age-independent acoustic models were trained by exploiting adult and children's speech. Recognition experiments were performed on three speech corpora, two consisting of children's speech and one of adult speech, using 64k word and 11k word trigram language models.\n",
    "Methods for speaker adaptive acoustic modeling proved to be effective, in particular for training acoustic models on a mixture of adult and children's speech, ensuring recognition performance aligned with that achieved with group-specific models for adults and children. A 10.2% word error rate was achieved on speech collected from children in the age range 8-12, compared with the 8.2% word error rate achieved for adults uttering the same texts.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-696"
  },
  "darcy05_interspeech": {
   "authors": [
    [
     "Shona",
     "D'Arcy"
    ],
    [
     "Martin",
     "Russell"
    ]
   ],
   "title": "A comparison of human and computer recognition accuracy for children's speech",
   "original": "i05_2197",
   "page_count": 4,
   "order": 697,
   "p1": "2197",
   "pn": "2200",
   "abstract": [
    "Several studies have shown that automatic speech recognition error rates are greater for children's speech than for adult's speech. Investigations have demonstrated that word recognition error rates increase as age decreases, and that recognition performance for children's speech is more sensitive to bandwidth reduction, compared with adult speech. This paper presents the results of experiments to measure human recognition performance for children's speech. The paper compares human and machine recognition performance on the same children's speech data. It is shown that human recognition performance for children's speech exhibits similar effects of age and bandwidth to those observed for automatic systems. The results suggest that effects of age and bandwidth on automatic speech recognition accuracy are due to properties of children's speech rather than artifacts of the technology.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-697"
  },
  "cosi05_interspeech": {
   "authors": [
    [
     "Piero",
     "Cosi"
    ],
    [
     "Bryan L.",
     "Pellom"
    ]
   ],
   "title": "Italian children's speech recognition for advanced interactive literacy tutors",
   "original": "i05_2201",
   "page_count": 4,
   "order": 698,
   "p1": "2201",
   "pn": "2204",
   "abstract": [
    "This work was conducted with the specific goals of developing improved recognition of children's speech in Italian and the integration of the children's speech recognition models into the Italian version of the Colorado Literacy Tutor platform. Specifically, children's speech recognition research for Italian was conducted using the ITC-irst Children's Speech Corpus. Using the University of Colorado SONIC LVSR system, we demonstrate a phonetic recognition error rate of 16.0% for a system which incorporates Vocal Tract Length Normalization (VTLN), Speaker-Adaptive Trained phonetic models, as well as unsupervised Structural MAP Linear Regression (SMAPLR). These new acoustic models have been incorporated within an Italian version of SONIC the ASR system of the Italian Literacy Tutor program.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-698"
  },
  "addadecker05_interspeech": {
   "authors": [
    [
     "Martine",
     "Adda-Decker"
    ],
    [
     "Lori",
     "Lamel"
    ]
   ],
   "title": "Do speech recognizers prefer female speakers?",
   "original": "i05_2205",
   "page_count": 4,
   "order": 699,
   "p1": "2205",
   "pn": "2208",
   "abstract": [
    "In this contribution we examine large speech corpora of prepared broadcast and spontaneous telephone speech in American English and in French. Starting with the question whether ASR systems behave differently on male and female speech, we then try to find evidence on acoustic-phonetic, lexical and idiomatic levels to explain the observed differences. Recognition results have been analysed on 3-7h of speech in each language and speech type condition (totaling 20 hours). Results consistently show a lower word error rate on female speech ranging from 0.7 to 7% depending on the condition. An analysis of automatically produced pronunciations in speech training corpora (totaling 4000 hours of speech) revealed that female speakers tend to stick more consistently to standard pronunciations than male speakers. Concerning speech disfluencies, male speakers show larger proportions of filled pauses and repetitions, as compared to females.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-699"
  },
  "yildirim05_interspeech": {
   "authors": [
    [
     "Serdar",
     "Yildirim"
    ],
    [
     "Chul Min",
     "Lee"
    ],
    [
     "Sungbok",
     "Lee"
    ],
    [
     "Alexandros",
     "Potamianos"
    ],
    [
     "Shrikanth",
     "Narayanan"
    ]
   ],
   "title": "Detecting Politeness and frustration state of a child in a conversational computer game",
   "original": "i05_2209",
   "page_count": 4,
   "order": 700,
   "p1": "2209",
   "pn": "2212",
   "abstract": [
    "In this study, we investigate politeness and frustration behavior of children during their spoken interaction with computer characters in a game. We focus on automatically detecting frustrated, polite and neutral attitudes from the child's speech (acoustic and language) communication cues and study their differences as a function of age and gender. The study is based on a Wizard-of-Oz dialog corpus of 103 children playing a voice activated computer game. Statistical analysis revealed that there was a significant gender effect on politeness with girls in this data exhibiting more explicit politeness markers. The analysis also showed that there is a positive correlation between frustration and the number of dialog turns reflecting the fact that longer time spent solving the puzzle of the game led to a more frustrated child. By combining acoustic and language cues for the task of automatic detection of politeness and frustration, we obtain average accuracy of 84.7% and 71.3%, respectively, by using age dependent models and 85% and 72%, respectively, for gender dependent models.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-700"
  },
  "binnenpoorte05_interspeech": {
   "authors": [
    [
     "Diana",
     "Binnenpoorte"
    ],
    [
     "Christophe Van",
     "Bael"
    ],
    [
     "Els den",
     "Os"
    ],
    [
     "Louis",
     "Boves"
    ]
   ],
   "title": "Gender in everyday speech and language: a corpus-based study",
   "original": "i05_2213",
   "page_count": 4,
   "order": 701,
   "p1": "2213",
   "pn": "2216",
   "abstract": [
    "This paper presents an exploratory study on the relations between gender and everyday parlance. A \"data-mining\" approach is used to explore gender-specific characteristics in a large number of spontaneous telephone and face-to-face conversations. Our study focuses on speech rate (speaking rate and articulation rate), disfluencies (filled pauses and repetitions), pronunciation variation (phoneme substitutions, deletions and insertions), and preferences for particular parts of speech. Our study reveals interesting similarities and differences in everyday male and female speech, and proves that data-mining on large spoken language corpora is a promising approach for obtaining information on spontaneous speech phenomena and for generating new hypotheses for research.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-701"
  },
  "elenius05_interspeech": {
   "authors": [
    [
     "Daniel",
     "Elenius"
    ],
    [
     "Mats",
     "Blomberg"
    ]
   ],
   "title": "Adaptation and normalization experiments in speech recognition for 4 to 8 year old children",
   "original": "i05_2749",
   "page_count": 4,
   "order": 702,
   "p1": "2749",
   "pn": "2752",
   "abstract": [
    "An experimental offline investigation of the performance of connected digits recognition was performed on children in the age range four to eight years. Poor performance using adult models was improved significantly by adaptation and vocal tract length normalisation but not to the same level as training on children. Age dependent models were tried with limited advantage. A combined adult and child training corpus maintained the performance for the separately trained categories. Linear frequency compression for vocal tract length normalization was attempted but estimation of the warping factor was sensitive to non-speech segments and background noise. Phoneme-based word modeling outperformed the whole word models, even though the vocabulary only consisted of digits.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-702"
  },
  "jansen05_interspeech": {
   "authors": [
    [
     "Wim",
     "Jansen"
    ],
    [
     "Hugo Van",
     "Hamme"
    ]
   ],
   "title": "PROSPECT features and their application to missing data techniques for vocal tract length normalization",
   "original": "i05_2753",
   "page_count": 4,
   "order": 703,
   "p1": "2753",
   "pn": "2756",
   "abstract": [
    "Speaker normalization by (piecewise) linear warping of the frequency axis is a popular method because of its simplicity and effectiveness. However, when this so-called vocal tract length normalization is applied to map test speakers with a shorter vocal tract onto acoustic models trained on speakers with a longer vocal tract, there is important information missing in the frequency bins at the high end of the spectrum. Usually, this missing information is reconstructed by ad hoc rules or through extrapolation of the spectrum. In this paper, we present a new method to estimate the content of those bins. The proposed solution is derived from Missing Data Techniques, that are used for noise robust speech recognizers. To alleviate the accuracy loss associated with Missing Data Techniques that are usually expressed in the spectral domain, we apply the PROSPECT feature representation introduced about a year ago. We demonstrate the superiority of our approach on the TIDigits database.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-703"
  },
  "hagen05_interspeech": {
   "authors": [
    [
     "Andreas",
     "Hagen"
    ],
    [
     "Bryan L.",
     "Pellom"
    ]
   ],
   "title": "Data driven subword unit modeling for speech recognition and its application to interactive reading tutors",
   "original": "i05_2757",
   "page_count": 4,
   "order": 704,
   "p1": "2757",
   "pn": "2760",
   "abstract": [
    "This paper proposes a novel token-passing search architecture for supporting subword unit based speech recognition and a corresponding algorithm based on the well-known LZW text compression method to determine a vocabulary of subword units in an unsupervised manner. We compare our subword unit selection algorithm to an existing approach based on Minimum Description Length (MDL) modeling and also syllable representations for English. Our approach is shown to offer units which share properties similar to syllables, but are determined in a language-independent and data-driven manner. Using our novel token passing architecture which combines both word-level and subword unit representations, we applied the proposed framework to the problem of oral reading tracking within an interactive literacy tutor for children. The proposed architecture is shown to provide advantages over wholeword based speech recognition for the problem of recognizing and detecting oral reading events.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-704"
  },
  "batliner05b_interspeech": {
   "authors": [
    [
     "Anton",
     "Batliner"
    ],
    [
     "Mats",
     "Blomberg"
    ],
    [
     "Shona",
     "D'Arcy"
    ],
    [
     "Daniel",
     "Elenius"
    ],
    [
     "Diego",
     "Giuliani"
    ],
    [
     "Matteo",
     "Gerosa"
    ],
    [
     "Christian",
     "Hacker"
    ],
    [
     "Martin",
     "Russell"
    ],
    [
     "Stefan",
     "Steidl"
    ],
    [
     "Michael",
     "Wong"
    ]
   ],
   "title": "The PF_STAR children's speech corpus",
   "original": "i05_2761",
   "page_count": 4,
   "order": 705,
   "p1": "2761",
   "pn": "2764",
   "abstract": [
    "This paper describes the corpus of recordings of children's speech which was collected as part of the EU FP5 PF_STAR project. The corpus contains more than 60 hours of speech, including read and imitated native-language speech in British English, German and Swedish, read and imitated non-native-language English speech from German, Italian and Swedish children, and native-language spontaneous and emotional speech in English and German.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-705"
  },
  "bell05_interspeech": {
   "authors": [
    [
     "Linda",
     "Bell"
    ],
    [
     "Johan",
     "Boye"
    ],
    [
     "Joakim",
     "Gustafson"
    ],
    [
     "Mattias",
     "Heldner"
    ],
    [
     "Anders",
     "Lindström"
    ],
    [
     "Mats",
     "Wirén"
    ]
   ],
   "title": "The Swedish NICE corpus - spoken dialogues between children and embodied characters in a computer game scenario",
   "original": "i05_2765",
   "page_count": 4,
   "order": 706,
   "p1": "2765",
   "pn": "2768",
   "abstract": [
    "This article describes the collection and analysis of a Swedish database of spontaneous and unconstrained children-machine dialogues. The Swedish NICE corpus consists of spoken dialogues between children aged 8 to 15 and embodied fairytale characters in a computer game scenario. Compared to previously collected corpora of children's computer-directed speech, the Swedish NICE corpus contains extended interactions, including three-party conversation, in which the young users used spoken dialogue as the primary means of progression in the game.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-706"
  },
  "miyauchi05_interspeech": {
   "authors": [
    [
     "Yusuke",
     "Miyauchi"
    ],
    [
     "Nao",
     "Hodoshima"
    ],
    [
     "Keiichi",
     "Yasu"
    ],
    [
     "Nahoko",
     "Hayashi"
    ],
    [
     "Takayuki",
     "Arai"
    ],
    [
     "Mitsuko",
     "Shindo"
    ]
   ],
   "title": "A preprocessing technique for improving speech intelligibility in reverberant environments: the effect of steady-state suppression on elderly people",
   "original": "i05_2769",
   "page_count": 4,
   "order": 707,
   "p1": "2769",
   "pn": "2772",
   "abstract": [
    "In a large auditorium, perceiving speech may become difficult. One reason that reverberation degrades speech intelligibility is the effect of overlap-masking (Bolt and MacDonald, 1949; Nabelek and Robinette, 1978). Reverberation is a more critical issue for elderly people to perceive speech than it is for young people (Fitzgibbons and Gordon-Salant, 1999). Arai et al. suppressed steady-state portions of speech which have more energy but are less crucial for speech perception, and confirmed promising results for improving speech intelligibility (Arai et al., 2001, 2002). Hodoshima et al. conducted perceptual tests to confirm the effectiveness of steady-state suppression with several reverberation conditions, and obtained significant improvements with reverberation times of 0.7-1.3 s. In this study, we conducted an experiment for evaluating steadystate suppression with fifty elderly people and found that there were significant improvements. Also, steady-state suppression yielded better improvements in speech intelligibility for elderly people than it did for young people.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-707"
  },
  "matejka05_interspeech": {
   "authors": [
    [
     "Pavel",
     "Matejka"
    ],
    [
     "Petr",
     "Schwarz"
    ],
    [
     "Jan",
     "Cernocký"
    ],
    [
     "Pavel",
     "Chytil"
    ]
   ],
   "title": "Phonotactic language identification using high quality phoneme recognition",
   "original": "i05_2237",
   "page_count": 4,
   "order": 708,
   "p1": "2237",
   "pn": "2240",
   "abstract": [
    "Phoneme Recognizers followed by Language Modeling (PRLM) have consistently yielded top performance in language identification (LID) task. Parallel ordering of PRLMs (PPRLM) improves performance even more. Since tokenizer is the most important part of LID system the high quality phoneme recognizer is employed. Two different multilingual databases for training phoneme recognizers are compared and the amount of sufficient training data is studied. Reported results are on data from NIST 2003 LID evaluation. Our four PRLM systems have Equal Error Rate (EER) of 2.4% on 12 languages task. This result compares favorably to the best known result from this task.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-708"
  },
  "huang05f_interspeech": {
   "authors": [
    [
     "Rongqing",
     "Huang"
    ],
    [
     "John H. L.",
     "Hansen"
    ]
   ],
   "title": "Advances in word based dialect/accent classification",
   "original": "i05_2241",
   "page_count": 4,
   "order": 709,
   "p1": "2241",
   "pn": "2244",
   "abstract": [
    "In an earlier study, we proposed a very effective dialect/accent classification algorithm, which is named Word based Dialect Classification (WDC). The WDC works well for large size corpora and significantly outperforms traditional Large Vocabulary Continuous Speech Recognition (LVCSR) based systems, which is claimed to be the best performing system for language identification. For a small training corpus, however, it is difficult to obtain a robust statistical model for each word and each dialect. Therefore, a Context Adapted Training (CAT) algorithm is formulated here, which adapts the universal phoneme GMMs to dialect-dependent word HMMs via linear regression. Employing on a 8-dialect British English corpus-IViE, the CAT algorithm trained WDC system obtains a 35.5% relative classification error reduction from the baseline LVCSR system, and a 20.2% relative classification error reduction from the basic WDC system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-709"
  },
  "hamdi05_interspeech": {
   "authors": [
    [
     "Rym",
     "Hamdi"
    ],
    [
     "Salem",
     "Ghazali"
    ],
    [
     "Melissa",
     "Barkat-Defradas"
    ]
   ],
   "title": "Syllable structure in spoken Arabic: a comparative investigation",
   "original": "i05_2245",
   "page_count": 4,
   "order": 710,
   "p1": "2245",
   "pn": "2248",
   "abstract": [
    "The aim of this study is to demonstrate that rhythm variation across Arabic dialects is to a great extent correlated with the different types of syllabic structure observed in these dialects, especially with regard to the relative complexity of onsets and codas. The main focus is on the relationship between syllabic structures on the one hand, and rhythm classes based on segmental duration on the other. Rhythmic variations in Arabic dialects based on proportions of vocalic and consonantal intervals were described in [1] following experimental procedures put forth by [2]. The present investigation consists in computing the frequency of occurrence of the different types of syllables in Moroccan, Tunisian and Lebanese Arabic representing different areas along the dialect continuum. The experimental data is based on the production of 10 minutes of spontaneous speech by Moroccan, Tunisian and Lebanese subjects. The results show that the syllabic patterns observed could be useful in characterizing the different Arabic dialects and may also constitute a basis for discriminating between them. The occurrence of the various types of syllables is significantly different from one dialect to another; the percentage of simple syllables and long vowels (CV, CVC, CVV, CVVC...) is higher in Lebanese Arabic. Moroccan Arabic, on the other hand, shows a tendency towards a high percentage of complex syllables (CCVC, CCVCC,...etc) and short vowels.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-710"
  },
  "marcadet05_interspeech": {
   "authors": [
    [
     "J. C.",
     "Marcadet"
    ],
    [
     "V.",
     "Fischer"
    ],
    [
     "C.",
     "Waast-Richard"
    ]
   ],
   "title": "A transformation-based learning approach to language identification for mixed-lingual text-to-speech synthesis",
   "original": "i05_2249",
   "page_count": 4,
   "order": 711,
   "p1": "2249",
   "pn": "2252",
   "abstract": [
    "Recent progress in corpus-based concatenative text-to-speech synthesis has generated some interest in systems that are capable of synthesizing text from more than one language. In this paper we describe the language identification component of such a mixed-lingual text-to-speech system. Relying only on the input text, we employ two different methods, namely a transformation based learning approach and a stochastic n-gram approach, and we describe the combination of both methods. While the transformation-based learning approach already produces average error rates of less than 2 percent and outperforms the n-gram classification scheme, the combination of both methods results in a further error reduction of up to 50 percent.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-711"
  },
  "itahashi05_interspeech": {
   "authors": [
    [
     "Shuichi",
     "Itahashi"
    ],
    [
     "Shiwei",
     "Zhu"
    ],
    [
     "Mikio",
     "Yamamoto"
    ]
   ],
   "title": "Constructing family trees of multilingual speech using Gaussian mixture models",
   "original": "i05_2253",
   "page_count": 4,
   "order": 712,
   "p1": "2253",
   "pn": "2256",
   "abstract": [
    "This paper proposes a method for automatically clustering multilingual speech so as to derive language family trees. We consider that the language is the source of information which generates speech feature parameters; the probability or statistical characteristics of this information is modeled by Gaussian mixture models (GMMs); then a distance measure between the GMMs is introduced. Based on this, we construct family trees of multilingual speech which are quite similar to those considered in linguistics.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-712"
  },
  "rouas05_interspeech": {
   "authors": [
    [
     "Jean-Luc",
     "Rouas"
    ]
   ],
   "title": "Modeling long and short-term prosody for language identification",
   "original": "i05_2257",
   "page_count": 4,
   "order": 713,
   "p1": "2257",
   "pn": "2260",
   "abstract": [
    "This paper addresses the problem of modeling prosody for language identification. The main goal is to validate (or invalidate) some languages characteristics proposed by the linguists by the mean of an automatic language identification (ALI) system. In previous papers, we defined a prosodic unit, the pseudo-syllable. Static modeling has proven the relevance of the pseudo-syllable unit for ALI. In this paper, we try to model the prosody dynamics. This is achieved by the separation of long-term and short-term components of prosody and the proposing of suitable models. Experiments are made on seven languages and the efficiency of the modeling is discussed.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-713"
  },
  "wu05c_interspeech": {
   "authors": [
    [
     "Tingyao",
     "Wu"
    ],
    [
     "Dirk Van",
     "Compernolle"
    ],
    [
     "Jacques",
     "Duchateau"
    ],
    [
     "Qian",
     "Yang"
    ],
    [
     "Jean-Pierre",
     "Martens"
    ]
   ],
   "title": "Improving the discrimination between native accents when recorded over different channels",
   "original": "i05_2821",
   "page_count": 4,
   "order": 714,
   "p1": "2821",
   "pn": "2824",
   "abstract": [
    "Acoustic differences between native accents may prove to be too subtle for straightforward brute force techniques such as blindly clustered Gaussian mixture model (GMM) classifiers to yield satisfactory discrimination performance while these methods work well for classifying more pronounced differences such as language, gender or channel. In this paper it is shown that small channel differences are easier to detect by such coarse classifiers than native accent differences. Performance of native accent classification can be improved considerably by incorporating the knowledge of the underlying phoneme sequence and using phoneme specific GMMs. Further improvements are obtained if optimal feature selection is combined with the phoneme dependent GMMs, resulting in usage of less than 10% of the original features. The presented methods result in a reduction of more than 40% in relative error rate in a 5-class classification task.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-714"
  },
  "trancoso05_interspeech": {
   "authors": [
    [
     "Isabel",
     "Trancoso"
    ],
    [
     "António",
     "Serralheiro"
    ],
    [
     "Céu",
     "Viana"
    ],
    [
     "Diamantino",
     "Caseiro"
    ]
   ],
   "title": "Aligning and recognizing spoken books in different varieties of Portuguese",
   "original": "i05_2825",
   "page_count": 4,
   "order": 715,
   "p1": "2825",
   "pn": "2828",
   "abstract": [
    "This paper tries to present digital spoken books as a useful diagnostic tool for detecting alignment and recognition problems and for studying the porting of these technologies to different varieties of the same language - Portuguese, in our case. We summarize the main differences between European and Brazilian Portuguese (EP/BP) and describe how they affect the GtoP system. Despite the small size of our parallel spoken book corpus in the two varieties, our preliminary experiments confirmed our expectations in terms of the effectiveness of an EP-trained aligner used on BP spoken books. They also confirmed the inadequacy of an EP Broadcast News recognizer tested over literary contents, and the expected degradation in recognition scores caused by using that recognizer on a BP spoken book. Pronunciation adaptation was tested by adding variants derived by the BP GtoP system to our EP lexicon, resulting in a very small improvement in terms of recognition scores.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-715"
  },
  "ma05d_interspeech": {
   "authors": [
    [
     "Bin",
     "Ma"
    ],
    [
     "Haizhou",
     "Li"
    ],
    [
     "Chin-Hui",
     "Lee"
    ]
   ],
   "title": "An acoustic segment modeling approach to automatic language identification",
   "original": "i05_2829",
   "page_count": 4,
   "order": 716,
   "p1": "2829",
   "pn": "2832",
   "abstract": [
    "We propose a novel acoustic segment modeling approach to automatic language identification (LID). It is assumed that the overall sound characteristics of all spoken languages can be covered by a universal collection of acoustic segment models (ASMs) without imposing any phonetic definitions. These segment models are used to decode spoken utterances into strings of segment units. The statistics of these units and their co-occurrences are used to form ASM-derived feature vectors to discriminate individual spoken languages. We evaluate the proposed approach on the 12-language, 1996 NIST Language Recognition Evaluation (LRE) task. With testing queries of about 30 seconds long, our results show that the proposed ASM framework reduces the LID error rate quite significantly when compared with the prevailing parallel PRLM method. We achieved an accuracy of 86.1% using a set of 128 3-state ASMs, with each state characterized by a mixture Gaussian density with 32 mixture components.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-716"
  },
  "zhu05d_interspeech": {
   "authors": [
    [
     "Dong",
     "Zhu"
    ],
    [
     "Martine",
     "Adda-Decker"
    ],
    [
     "Fabien",
     "Antoine"
    ]
   ],
   "title": "Different size multilingual phone inventories and context-dependent acoustic models for language identification",
   "original": "i05_2833",
   "page_count": 4,
   "order": 717,
   "p1": "2833",
   "pn": "2836",
   "abstract": [
    "Experimental work using phonotactic and syllabotactic approaches for automatic language identification (LID) is presented. Various questions have originated this research: what is the best choice for a multilingual phone inventory? Can a syllabic unit be of interest to extend the scope of the modeling unit? Are context-dependent (CD) acoustic models, widely used for speech recognition, able to improve LID accuracy? Can the multilingual acoustic models process efficiently additional languages, which are different from the training languages? The LID system is experimentally studied using different sizes of multilingual phone sets: 73, 50 and 35 phones. Experiments are carried out on broadcast news in seven languages (German, English, Arabic, Mandarin, Spanish, French, and Italian) with 140-hours audio data for training and 7 hours for testing. It is shown that smaller phone inventories achieve higher LID accuracy and that CD models outperform CI models. Further experiments have been conducted to test generality of both the multilingual acoustic model and phonotactics methods on another 11+10 languages corpus (11 known + 10 unknown languages).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-717"
  },
  "gao05_interspeech": {
   "authors": [
    [
     "Sheng",
     "Gao"
    ],
    [
     "Bin",
     "Ma"
    ],
    [
     "Haizhou",
     "Li"
    ],
    [
     "Chin-Hui",
     "Lee"
    ]
   ],
   "title": "A text categorization approach to automatic language identification",
   "original": "i05_2837",
   "page_count": 4,
   "order": 718,
   "p1": "2837",
   "pn": "2840",
   "abstract": [
    "We propose a novel approach to spoken language identification (LID). In this framework, a group of utterances from a particular language is treated as a \"spoken document\" characterized by a \"document vector\". The collection of spoken documents in the training set from the same language forms a specific \"language identification category\". An unknown testing utterance to be identified can also be represented as a query vector, such that LID is accomplished just like in the case of associating a text document to a topic. This process is known as text categorization (TC). The key lies in tokenizing speech signals with a set of \"key terms\" so that their salient patterns and corresponding statistics can be used to discriminate individual spoken languages. To perform LID we can adopt any classifier learning and feature extraction techniques developed in the TC community. When compared with the prevailing parallel PRLM method, the proposed approach achieves a relative error reduction of about 87.5%, and reaches an error rate of 0.2% and 1.54% for 3 and 6 languages, respectively, with queries of about 10 seconds long.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-718"
  },
  "salvi05b_interspeech": {
   "authors": [
    [
     "Giampiero",
     "Salvi"
    ]
   ],
   "title": "Advances in regional accent clustering in Swedish",
   "original": "i05_2841",
   "page_count": 4,
   "order": 719,
   "p1": "2841",
   "pn": "2844",
   "abstract": [
    "The regional pronunciation variation in Swedish is analysed on a large database. Statistics over each phoneme and for each region of Sweden are computed using the EM algorithm in a hidden Markov model framework to overcome the difficulties of transcribing the whole set of data at the phonetic level. The model representations obtained this way are compared using a distance measure in the space spanned by the model parameters, and hierarchical clustering. The regional variants of each phoneme may group with those of any other phoneme, on the basis of their acoustic properties. The log likelihood of the data given the model is shown to display interesting properties regarding the choice of number of clusters, given a particular level of details. Discriminative analysis is used to find the parameters that most contribute to the separation between groups, adding an interpretative value to the discussion. Finally a number of examples are given on some of the phenomena that are revealed by examining the clustering tree.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-719"
  },
  "paulik05_interspeech": {
   "authors": [
    [
     "M.",
     "Paulik"
    ],
    [
     "Christian",
     "Fügen"
    ],
    [
     "Sebastian",
     "Stüker"
    ],
    [
     "Tanja",
     "Schultz"
    ],
    [
     "Thomas",
     "Schaaf"
    ],
    [
     "Alex",
     "Waibel"
    ]
   ],
   "title": "Document driven machine translation enhanced ASR",
   "original": "i05_2261",
   "page_count": 4,
   "order": 720,
   "p1": "2261",
   "pn": "2264",
   "abstract": [
    "In human-mediated translation scenarios a human interpreter translates between a source and a target language using either a spoken or a written representation of the source language. In this paper we improve the recognition performance on the speech of the human translator spoken in the target language by taking advantage of the source language representations. We use machine translation techniques to translate between the source and target language resources and then bias the target language speech recognizer towards the gained knowledge, hence the name Machine Translation Enhanced Automatic Speech Recognition. We investigate several different techniques among which are restricting the search vocabulary, selecting hypotheses from n-best lists, applying cache and interpolation schemes to language modeling, and combining the most successful techniques into our final, iterative system. Overall we outperform the baseline system by a relative word error rate reduction of 37.6%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-720"
  },
  "khadivi05_interspeech": {
   "authors": [
    [
     "Shahram",
     "Khadivi"
    ],
    [
     "András",
     "Zolnay"
    ],
    [
     "Hermann",
     "Ney"
    ]
   ],
   "title": "Automatic text dictation in computer-assisted translation",
   "original": "i05_2265",
   "page_count": 4,
   "order": 721,
   "p1": "2265",
   "pn": "2268",
   "abstract": [
    "In this paper, we study the incorporation of statistical machine translation models to automatic speech recognition models in the framework of computer-assisted translation. The system is given a source language text to be translated and it shows the source text to the human translator to translate it orally. The system captures the user speech which is the dictation of the target language sentence. Since the system has simultaneous access to the source language text and the speech signal of the target language text, it is possible to improve the speech recognition accuracy by incorporating the statistical machine translation models. We show that statistical translation models have a high impact on improving the speech recognition results. Using these models, we achieve a relative word error rate reduction of 17%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-721"
  },
  "rodriguez05_interspeech": {
   "authors": [
    [
     "L.",
     "Rodríguez"
    ],
    [
     "J.",
     "Civera"
    ],
    [
     "E.",
     "Vidal"
    ],
    [
     "Francisco",
     "Casacuberta"
    ],
    [
     "C.",
     "Martínez"
    ]
   ],
   "title": "On the use of speech recognition in computer assisted translation",
   "original": "i05_2269",
   "page_count": 4,
   "order": 722,
   "p1": "2269",
   "pn": "2272",
   "abstract": [
    "Computer-Assisted Translation systems can be used by human translators to increase their productivity. In these systems, the computer suggests portions of target sentence that can be accepted or amended by a human translator. In the present work we will introduce speech as a novel way to interact with these systems. The rational behind this approach is to increase the throughout and ergonomy. Different scenarios for this kind of speech interactions will be discussed along with some preliminary results.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-722"
  },
  "kathol05_interspeech": {
   "authors": [
    [
     "Andreas",
     "Kathol"
    ],
    [
     "Kristin",
     "Precoda"
    ],
    [
     "Dimitra",
     "Vergyri"
    ],
    [
     "Wen",
     "Wang"
    ],
    [
     "Susanne",
     "Riehemann"
    ]
   ],
   "title": "Speech translation for low-resource languages: the case of Pashto",
   "original": "i05_2273",
   "page_count": 4,
   "order": 723,
   "p1": "2273",
   "pn": "2276",
   "abstract": [
    "We present a number of challenges and solutions that have arisen in the development of a speech translation system for American English and Pashto, highlighting those specific to a very low resource language. In particular, we address issues posed by Pashto in the areas of written representation, corpus creation, speech recognition, speech synthesis, and grammar development for translation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-723"
  },
  "pico05_interspeech": {
   "authors": [
    [
     "David",
     "Picó"
    ],
    [
     "Jorge",
     "González"
    ],
    [
     "Francisco",
     "Casacuberta"
    ],
    [
     "Diamantino",
     "Caseiro"
    ],
    [
     "Isabel",
     "Trancoso"
    ]
   ],
   "title": "Finite-state transducer inference for a speech-input Portuguese-to-English machine translation system",
   "original": "i05_2277",
   "page_count": 4,
   "order": 724,
   "p1": "2277",
   "pn": "2280",
   "abstract": [
    "Statistical techniques and grammatical inference have been used for dealing with automatic speech recognition with success, and can also be used for speech-to-speech machine translation. In this paper, new advances on a method for finite-state transducer inference are presented. This method has been tested experimentally in a speech-input translation task using a recognizer that allows a flexible use of models by means of efficient algorithms for on-the-fly transducer composition. These are the first reported results of a speech-to-speech translation task involving European Portuguese input that we know of.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-724"
  },
  "ohta05b_interspeech": {
   "authors": [
    [
     "Kenko",
     "Ohta"
    ],
    [
     "Keiji",
     "Yasuda"
    ],
    [
     "Genichiro",
     "Kikui"
    ],
    [
     "Masuzo",
     "Yanagida"
    ]
   ],
   "title": "Quantitative evaluation of effects of speech recognition errors on speech translation quality",
   "original": "i05_2281",
   "page_count": 4,
   "order": 725,
   "p1": "2281",
   "pn": "2284",
   "abstract": [
    "This paper investigates the relationship between the quality of speech translation outputs and the errors in a speech recognition subsystem. In this study, we assume that a speech translation system is a sequential combination of speech recognition and automatic translation subsystems. We conducted speech translation experiments while changing parameters in the speech recognition subsystem to get different speech recognition results, which were then fed into the translation subsystem. We applied regression analysis to the interrelationship between the speech recognition outputs and the final translation results. We found that particular kinds of speech recognition errors, including deletion of punctuation marks and substitutions of nouns, cause severe semantic errors in speech translation. We also found that the final translation quality degrades logarithmically with respect to the number of speech recognition errors.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-725"
  },
  "matusov05_interspeech": {
   "authors": [
    [
     "E.",
     "Matusov"
    ],
    [
     "S.",
     "Kanthak"
    ],
    [
     "Hermann",
     "Ney"
    ]
   ],
   "title": "On the integration of speech recognition and statistical machine translation",
   "original": "i05_3177",
   "page_count": 4,
   "order": 726,
   "p1": "3177",
   "pn": "3180",
   "abstract": [
    "This paper focuses on the interface between speech recognition and machine translation in a speech translation system. Based on a thorough theoretical framework, we exploit word lattices of automatic speech recognition hypotheses as input to our translation system which is based on weighted finite-state transducers. We show that acoustic recognition scores of the recognized words in the lattices positively and significantly affect the translation quality. In experiments, we have found consistent improvements on three different corpora compared with translations of single best recognized results. In addition we build and evaluate a fully integrated speech translation model.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-726"
  },
  "quan05_interspeech": {
   "authors": [
    [
     "V. H.",
     "Quan"
    ],
    [
     "M.",
     "Federico"
    ],
    [
     "M.",
     "Cettolo"
    ]
   ],
   "title": "Integrated n-best re-ranking for spoken language translation",
   "original": "i05_3181",
   "page_count": 4,
   "order": 727,
   "p1": "3181",
   "pn": "3184",
   "abstract": [
    "This paper describes the application of N-best lists to a spoken language translation system. Multiple hypotheses are generated both by the speech recognizer and by the statistical machine translator; they are finally re-ranked by optimally weighting recognition and translation scores, estimated in an integrated scheme. We provide experimental results for the Italian-to-English direction on the BTEC corpus, a collection of sentences in the touristic domain developed within the C-STAR project.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-727"
  },
  "crego05_interspeech": {
   "authors": [
    [
     "Josep M.",
     "Crego"
    ],
    [
     "José B.",
     "Mariño"
    ],
    [
     "Adrià de",
     "Gispert"
    ]
   ],
   "title": "An n-gram-based statistical machine translation decoder",
   "original": "i05_3185",
   "page_count": 4,
   "order": 728,
   "p1": "3185",
   "pn": "3188",
   "abstract": [
    "In this paper we describe MARIE, an Ngram-based statistical machine translation decoder. It is implemented using a beam search strategy, with distortion (or reordering) capabilities. The underlying translation model is based on an Ngram approach, extended to introduce reordering at the phrase level. The search graph structure is designed to perform very accurate comparisons, what allows for a high level of pruning, improving the decoder efficiency. We report several techniques for efficiently prune out the search space.\n",
    "The combinatory explosion of the search space derived from the search graph structure is reduced by limiting the number of reorderings a given translation is allowed to perform, and also the maximum distance a word (or a phrase) is allowed to be reordered. We finally report translation accuracy results on three different translation tasks.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-728"
  },
  "gu05b_interspeech": {
   "authors": [
    [
     "Liang",
     "Gu"
    ],
    [
     "Yuqing",
     "Gao"
    ]
   ],
   "title": "Use of maximum entropy in natural word generation for statistical concept-based speech-to-speech translation",
   "original": "i05_3189",
   "page_count": 4,
   "order": 729,
   "p1": "3189",
   "pn": "3192",
   "abstract": [
    "Our statistical concept-based spoken language translation method consists of three cascaded components: natural language understanding, natural concept generation and natural word generation. In the previous approaches, statistical models are used only in the first two components. In this paper, a novel maximum-entropybased statistical natural word generation algorithm is proposed that takes into account both the word level and concept level context information in the source and the target language. A recursive generation scheme is further devised to integrate this statistical generation algorithm with the previously proposed maximum-entropy-based natural concept generation algorithm. The translation error rate is reduced by 14%-20% in our speech-tospeech translation experiments.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-729"
  },
  "gispert05_interspeech": {
   "authors": [
    [
     "Adrià de",
     "Gispert"
    ],
    [
     "José B.",
     "Mariño"
    ],
    [
     "Josep M.",
     "Crego"
    ]
   ],
   "title": "Improving statistical machine translation by classifying and generalizing inflected verb forms",
   "original": "i05_3193",
   "page_count": 4,
   "order": 730,
   "p1": "3193",
   "pn": "3196",
   "abstract": [
    "This paper introduces a rule-based classification of single-word and compound verbs into a statistical machine translation approach. By substituting verb forms by the lemma of their head verb, the data sparseness problem caused by highly-inflected languages can be successfully addressed. On the other hand, the information of seen verb forms can be used to generate new translations for unseen verb forms. Translation results for an English to Spanish task are reported, producing a significant performance improvement.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-730"
  },
  "bozarov05_interspeech": {
   "authors": [
    [
     "Abdulvohid",
     "Bozarov"
    ],
    [
     "Yoshinori",
     "Sagisaka"
    ],
    [
     "Ruiqiang",
     "Zhang"
    ],
    [
     "Genichiro",
     "Kikui"
    ]
   ],
   "title": "Improved speech recognition word lattice translation by confidence measure",
   "original": "i05_3197",
   "page_count": 4,
   "order": 731,
   "p1": "3197",
   "pn": "3200",
   "abstract": [
    "In conventional speech translation systems, Automatic Speech Recognition (ASR) produces a single hypothesis which is then translated by the SMT system. The translation results of SMT system are impaired by the word errors of the first best hypothesis in this approach more or less. To improve speech translation, we use a new word lattice translation approach which integrates multiple information sources from the speech recognition word lattice to discount the misrecognition. Furthermore, in order to improve speech translation and to reduce computation, we used N-bests cutoff, merging of identical word ids, and confidence measure. Experiments of Japanese-to-English speech translation showed that the proposed word lattice translation outperforms the conventional single best method.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-731"
  },
  "lotter05_interspeech": {
   "authors": [
    [
     "Thomas",
     "Lotter"
    ],
    [
     "Bastian",
     "Sauert"
    ],
    [
     "Peter",
     "Vary"
    ]
   ],
   "title": "A stereo input-output superdirective beamformer for dual channel noise reduction",
   "original": "i05_2285",
   "page_count": 4,
   "order": 732,
   "p1": "2285",
   "pn": "2288",
   "abstract": [
    "This contribution presents a stereo input-output beamformer for dual channel noise reduction. The computationally very efficient beamformer adapts superdirective filter design techniques to binaural input signals to optimally enhance signals from a given spatial direction. The beamformer outputs a stereo enhanced signal and thus preserves the spatial impression. Experiments in a real environment using a dummy head and various speech sources indicate that the proposed algorithm is capable of improving the speech intelligibility significantly.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-732"
  },
  "klee05_interspeech": {
   "authors": [
    [
     "Ulrich",
     "Klee"
    ],
    [
     "Tobias",
     "Gehrig"
    ],
    [
     "John",
     "McDonough"
    ]
   ],
   "title": "Kalman filters for time delay of arrival-based source localization",
   "original": "i05_2289",
   "page_count": 4,
   "order": 733,
   "p1": "2289",
   "pn": "2292",
   "abstract": [
    "In this work, we propose an algorithm for acoustic source localization based on time delay of arrival (TDOA) estimation. In earlier work by other authors, an initial closed-form approximation was first used to estimate the true position of the speaker followed by a Kalman filtering stage to smooth the time series of estimates. In the proposed algorithm, this closed-form approximation is eliminated by employing a Kalman filter to directly update the speaker position estimate based on the observed TDOAs. In particular, the TDOAs comprise the observation associated with an extended Kalman filter whose state corresponds to the speaker position. We tested our algorithm on a data set consisting of seminars held by actual speakers. Our experiments revealed that the proposed algorithm provides source localization accuracy superior to the standard spherical and linear intersection techniques. Moreover, the proposed algorithm, although relying on an iterative optimization scheme, proved efficient enough for real-time operation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-733"
  },
  "ichikawa05_interspeech": {
   "authors": [
    [
     "Osamu",
     "Ichikawa"
    ],
    [
     "Masafumi",
     "Nishimura"
    ]
   ],
   "title": "Simultaneous adaptation of echo cancellation and spectral subtraction for in-car speech recognition",
   "original": "i05_2293",
   "page_count": 4,
   "order": 734,
   "p1": "2293",
   "pn": "2296",
   "abstract": [
    "For noise robustness of in-car speech recognition, most of the current systems are based on the assumption that there is only a stationary cruising noise. Therefore, the recognition rate is greatly reduced when there is music or news coming from a radio or a CD player in the car. Since reference signals are available from such in-vehicle units, there is great hope that echo cancellers can eliminate the echo component in the observed noisy signals. However, previous research reported that the performance of an echo canceller is degraded in very noisy conditions. This implies it is desirable to combine the processes of echo cancellation and noise reduction. In this paper, we propose a system that uses echo cancellation and spectral subtraction simultaneously. A stationary noise component for spectral subtraction is estimated through the adaptation of an echo canceller. In our experiments, this system significantly reduced the errors in automatic speech recognition compared with the conventional combination of echo cancellation and spectral subtraction.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-734"
  },
  "hu05d_interspeech": {
   "authors": [
    [
     "Rong",
     "Hu"
    ],
    [
     "Yunxin",
     "Zhao"
    ]
   ],
   "title": "Variable step size adaptive decorrelation filtering for competing speech separation",
   "original": "i05_2297",
   "page_count": 4,
   "order": 735,
   "p1": "2297",
   "pn": "2300",
   "abstract": [
    "Two variable step size (VSS) techniques are proposed for adaptive decorrelation filtering (ADF) to improve the performance of competing speech separation. The first VSS method applies gradient adaptive step-size (GAS) to increase ADF convergence rate. Under some simplifying assumptions, the GAS technique is generalized to allow the combination with additional VSS techniques for ADF algorithm. The second VSS method is based on error analysis of ADF estimates under a simplified signal model to decrease steady state filter error. An integration of both techniques into ADF was tested with TIMIT speech data convolutively mixed by reverberant room impulse responses. Experimental results showed that the proposed algorithm significantly increased ADF convergence rate and improved gain in both target-to-interference ratio (TIR) and phone recognition accuracy of the target speech.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-735"
  },
  "saitoh05_interspeech": {
   "authors": [
    [
     "Daisuke",
     "Saitoh"
    ],
    [
     "Atsunobu",
     "Kaminuma"
    ],
    [
     "Hiroshi",
     "Saruwatari"
    ],
    [
     "Tsuyoki",
     "Nishikawa"
    ],
    [
     "Akinobu",
     "Lee"
    ]
   ],
   "title": "Speech extraction in a car interior using frequency-domain ICA with rapid filter adaptations",
   "original": "i05_2301",
   "page_count": 4,
   "order": 736,
   "p1": "2301",
   "pn": "2304",
   "abstract": [
    "This paper describes two new algorithms for blind source separation (BSS) based on frequency-domain independent component analysis (FDICA). One is FDICA with pre-filtering by a speech sub-band passing filter to slow down the learning speed in low signal-to-noise ratio (SNR) sub-bands. The other is FDICA with sub-band selection learning to reduce the number of iterations for those sub-bands. The results of speech recognition experiments show that each method can improve word accuracy by as much as 7% and that the second method can increase the speed by approximately 60%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-736"
  },
  "hu05e_interspeech": {
   "authors": [
    [
     "Rongqiang",
     "Hu"
    ],
    [
     "Sunil D.",
     "Kamath"
    ],
    [
     "David V.",
     "Anderson"
    ]
   ],
   "title": "Speech enhancement using non-acoustic sensors",
   "original": "i05_2305",
   "page_count": 4,
   "order": 737,
   "p1": "2305",
   "pn": "2308",
   "abstract": [
    "This paper describes a speech enhancement system that significantly improves speech intelligibility of noisy speech in the context of a speech coder in low SNR conditions. The system uses two state-of-the-art non-acoustic sensors, a general electromagnetic motion sensor (GEMS) that detects the internal motions of glottis, and a physiological microphone (P-mic) that measures vibrations of the skin associated with speech. Both sensors are relatively immune to ambient acoustic noise, but provide incomplete information of speech. In the proposed system, the strengths of two algorithms , a perceptually motivated constant-Q (CQ) algorithm and an enhanced glottal correlation (GCORR) algorithm, are combined. The CQ algorithm employs a perceptually inspired signal detection technique to estimate the presence of speech cues in low SNR conditions. To reduce annoying artifacts, a state-dependent mechanism discriminating the distinct acoustic properties of each phoneme, and a psychoacoustic masking model are used to control enhancement gains. The enhanced glottal correlation algorithm extracts the desired speech signal from the noisy mixture, using a modified speech-GEMS correlation estimation of the speech signal with the glottal waveform supplied by GEMS. Both subjective and objective experiments were performed in a variety of noise conditions to indicate the improvement relative to the EMSR algorithm.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-737"
  },
  "delcroix05_interspeech": {
   "authors": [
    [
     "Marc",
     "Delcroix"
    ],
    [
     "Takafumi",
     "Hikichi"
    ],
    [
     "Masato",
     "Miyoshi"
    ]
   ],
   "title": "Improved blind dereverberation performance by using spatial information",
   "original": "i05_2309",
   "page_count": 4,
   "order": 738,
   "p1": "2309",
   "pn": "2312",
   "abstract": [
    "In this paper we consider the numerical problems faced by a blind dereverberation algorithm based on a multi-channel linear prediction. One hypothesis frequently incorporated in multi-microphone dereverberation algorithms is that channels do not share common zeros. However, it is known that real room transfer functions have a large number of zeros close to the unit circle on the z-plane, and thus many zeros are expected to be very close to each other. Consequently if few microphones are used, the channels would present numerically overlapping zeros and dereverberation algorithms would perform poorly. We study this phenomenon using the previously reported LInear-predictive Multi-input Equalization (LIME) algorithm. Spatial information can be used to deal with the problem of overlapping zeros. We describe the improved dereverberation performance that we achieve by increasing the number of microphones.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-738"
  },
  "li05g_interspeech": {
   "authors": [
    [
     "Junfeng",
     "Li"
    ],
    [
     "Masato",
     "Akagi"
    ]
   ],
   "title": "A hybrid microphone array post-filter in a diffuse noise field",
   "original": "i05_2313",
   "page_count": 4,
   "order": 739,
   "p1": "2313",
   "pn": "2316",
   "abstract": [
    "This paper proposes a hybrid post-filter for microphone arrays with the assumption of a diffuse noise field, where few post-filter performs well, to suppress correlated as well as uncorrelated noises. In the proposed post-filter, a modified Zelinski post-filter is applied to the high frequencies to suppress spatially uncorrelated noise and a single-channel Wiener post-filter is applied to the low frequencies for cancellation of spatially correlated noise. In theory, the proposed post-filter follows the framework of the multi-channel Wiener filter. In practice, experiments using multi-channel noise recordings were conducted and results show that the proposed hybrid post-filter gives the highest SNR improvements and lowest speech distortion among the tested post-filters in various car environments.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-739"
  },
  "krishnan05_interspeech": {
   "authors": [
    [
     "Venkatesh",
     "Krishnan"
    ],
    [
     "Phil S.",
     "Whitehead"
    ],
    [
     "David V.",
     "Anderson"
    ],
    [
     "Mark A.",
     "Clements"
    ]
   ],
   "title": "A framework for estimation of clean speech by fusion of outputs from multiple speech enhancement systems",
   "original": "i05_2317",
   "page_count": 4,
   "order": 740,
   "p1": "2317",
   "pn": "2320",
   "abstract": [
    "A novel multiple-input Kalman filtering (MIKF) framework is presented that estimates the clean speech signal by fusion of outputs from multiple speech enhancement systems. The MIKF framework generates a sample-by-sample minimum mean-square error estimate of the clean speech signal from these outputs. The residual noise in each input to the MIKF is modeled as an autoregressive (AR) process so that non-white noise can be accommodated, and the noise model is dynamically updated to handle non-stationary noise. Speech is also modeled as an AR process whose parameters are estimated from a codebook of suitably designed prototype AR parameters. Constraining the AR parameters via a codebook improves the quality and makes it easy to integrate the MIKF system with a speech coder. The proposed framework also has the flexibility to apply user-defined, heuristic weights to the inputs to the MIKF, which are the outputs of the contributing speech enhancement systems. Perceptual quality tests and objective measures (segmental signal-to-noise ratio) both demonstrate that the estimate of the clean speech signal generated by the MIKF is superior to any of its inputs.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-740"
  },
  "denda05_interspeech": {
   "authors": [
    [
     "Yuki",
     "Denda"
    ],
    [
     "Takanobu",
     "Nishiura"
    ],
    [
     "Yoichi",
     "Yamashita"
    ]
   ],
   "title": "A study of weighted CSP analysis with average speech spectrum for noise robust talker localization",
   "original": "i05_2321",
   "page_count": 4,
   "order": 741,
   "p1": "2321",
   "pn": "2324",
   "abstract": [
    "This paper describes a new method of noise robust talker localization for the front-end processing of microphone array steering. Conventional talker localization methods cannot localize a target talker accurately in higher noisy environments. To deal with this problem, in this paper, we propose an weighted CSP (Cross-power Spectrum Phase) analysis with an average speech spectrum. The proposed method consists of two processes. At first, CSP coefficients are weighted by analysis weight coefficients based on average speech spectrum, which is trained with speech database, in advance. Next, the interference noises are reduced on spatial domain by CSP coefficients subtraction. As a result of evaluation experiments in a real room, we confirmed that the proposed method could provide better talker localization performance than the conventional methods.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-741"
  },
  "kim05e_interspeech": {
   "authors": [
    [
     "Young-Ik",
     "Kim"
    ],
    [
     "Sung Jun",
     "An"
    ],
    [
     "Rhee Man",
     "Kil"
    ],
    [
     "Hyung-Min",
     "Park"
    ]
   ],
   "title": "Sound segregation based on binaural zero-crossings",
   "original": "i05_2325",
   "page_count": 4,
   "order": 742,
   "p1": "2325",
   "pn": "2328",
   "abstract": [
    "This paper presents a new method of sound segregation based on zero-crossings generated from binaural filter-bank outputs. In our approach, sound source directions are identified using the spatial cues such as inter-aural time differences (ITDs) and inter-aural intensity differences (IIDs). The estimation of ITDs is performed using zero-crossings generated from binaural filter-bank outputs to get more reliable ITD samples in noisy environments. We also consider the estimation of ITDs with the aid of IID samples to cope with the phase ambiguities of ITD samples in high frequencies. As a result, the proposed method is able to provide an accurate estimate of sound source directions which gives us a good masking scheme for sound segregation while offering significantly less computational complexity compared to cross-correlation based methods.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-742"
  },
  "freudenberger05_interspeech": {
   "authors": [
    [
     "Jürgen",
     "Freudenberger"
    ],
    [
     "Klaus",
     "Linhard"
    ]
   ],
   "title": "A two-microphone diversity system and its application for hands-free car kits",
   "original": "i05_2329",
   "page_count": 4,
   "order": 743,
   "p1": "2329",
   "pn": "2332",
   "abstract": [
    "In this paper we consider a two-channel diversity technique that combines the processed signals of two separate microphones. For in-car applications, this enables a better compromise for the microphone positions. The advantage of the proposed system is its insensitivity with respect to varying speaker sizes or local noise sources. To achieve this we choose the microphone position in that way that one microphone is optimum for a tall speaker, and the second one is suitable for a small speaker. For local noise sources we may apply a similar design to choose the microphone position in accordance with the location of the noise sources. A corresponding signal combiner has to tasks: compensation of phase shifts and weighting proportional to the signal strength. We propose solutions for both problems and demonstrate the effectiveness of diversity combining.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-743"
  },
  "murakami05b_interspeech": {
   "authors": [
    [
     "Takahiro",
     "Murakami"
    ],
    [
     "Kiyoshi",
     "Kurihara"
    ],
    [
     "Yoshihisa",
     "Ishida"
    ]
   ],
   "title": "Directionally constrained minimization of power algorithm for speech signals",
   "original": "i05_2333",
   "page_count": 4,
   "order": 744,
   "p1": "2333",
   "pn": "2336",
   "abstract": [
    "A novel algorithm of adaptive beamforming with a microphone array is presented. Beamforming is one of the simplest methods for discriminating between different signals based on the physical location of sources. The directionally constrained minimization of power (DCMP) algorithm is well-known as a reliable approach to extracting some signals incoming from specific directions. In the DCMP algorithm, the output power of the sensor array is minimized under the constraint of a constant response to the direction of arrival (DOA) of the target signal. Although the conventional DCMP is applied only to narrowband signals, our approach extends this algorithm so that the DCMP criterion is applicable to the broadband signals such as speech. The proposed method can be achieved by a relatively small hardware amount of the system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-744"
  },
  "brutti05_interspeech": {
   "authors": [
    [
     "Alessio",
     "Brutti"
    ],
    [
     "Maurizio",
     "Omologo"
    ],
    [
     "Piergiorgio",
     "Svaizer"
    ]
   ],
   "title": "Oriented global coherence field for the estimation of the head orientation in smart rooms equipped with distributed microphone arrays",
   "original": "i05_2337",
   "page_count": 4,
   "order": 745,
   "p1": "2337",
   "pn": "2340",
   "abstract": [
    "This paper proposes a new method for estimating the talker's head orientation in a smart room equipped with microphone arrays. The acoustic processing is based on the use of a coherence measure derived from the Cross-power spectrum phase analysis, commonly used for speaker localization and tracking purposes. An Oriented Global Coherence Field function is then introduced to assign to a given point in space different scores according to the possible orientation of the acoustic source.\n",
    "Preliminary experiments were conducted in the CHIL smart room available at ITC-irst laboratories. A small database was collected, given one speaker uttering a given sentence in different directions. Although the database is rather small, results show that the proposed method is promising and ensures a good estimation of the head orientation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-745"
  },
  "madhu05_interspeech": {
   "authors": [
    [
     "Nilesh",
     "Madhu"
    ],
    [
     "Rainer",
     "Martin"
    ]
   ],
   "title": "Robust speaker localization through adaptive weighted pair TDOA (AWEPAT) estimation",
   "original": "i05_2341",
   "page_count": 4,
   "order": 746,
   "p1": "2341",
   "pn": "2344",
   "abstract": [
    "Time delay of arrival (TDOA) estimation between signals input to two or more microphones plays an important role in speaker localization. Most methods employ a linear array of two or more microphones and use the generalized cross correlation method or eigenspace analysis (AEDA) methods. TDOA estimation with linear arrays, however, is highly sensitive to estimation errors when the signals arrive from an endfire direction. In this paper we propose a novel adaptive algorithm which makes use of a three-microphone planar array. This algorithm exhibits a much smaller estimation error over the complete azimuth range of 0-360 degrees as compared to other algorithms. The computational complexity of this approach is comparable to other state-of-the-art algorithms.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-746"
  },
  "lathoud05_interspeech": {
   "authors": [
    [
     "Guillaume",
     "Lathoud"
    ],
    [
     "Mathew",
     "Magimai-Doss"
    ],
    [
     "Bertrand",
     "Mesot"
    ]
   ],
   "title": "A spectrogram model for enhanced source localization and noise-robust ASR",
   "original": "i05_2345",
   "page_count": 4,
   "order": 747,
   "p1": "2345",
   "pn": "2348",
   "abstract": [
    "This paper proposes a simple, computationally efficient 2-mixture model approach to discrimination between speech and background noise. It is directly derived from observations on real data, and can be used in a fully unsupervised manner, with the EM algorithm. A first application to sector-based, joint audio source localization and detection, using multiple microphones, confirms that the model can provide major enhancement. A second application to the single channel speech recognition task in a noisy environment yields major improvement on stationary noise and promising results on non-stationary noise.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-747"
  },
  "srinivasan05b_interspeech": {
   "authors": [
    [
     "Sriram",
     "Srinivasan"
    ],
    [
     "Mattias",
     "Nilsson"
    ],
    [
     "W. Bastiaan",
     "Kleijn"
    ]
   ],
   "title": "Denoising through source separation and minimum tracking",
   "original": "i05_2349",
   "page_count": 4,
   "order": 748,
   "p1": "2349",
   "pn": "2352",
   "abstract": [
    "In this paper, we develop a multi-channel noise reduction algorithm based on blind source separation (BSS). In contrast to general BSS algorithms that attempt to recover all the signals, we explicitly estimate only the speech signal. By tracking the minimum of the spectral density of the microphone signals, noise-only segments are identified. The coefficients of the unmixing matrix that are necessary to separate the speech are identified from these segments through the optimization of an appropriate energy criterion. Since the proposed method explicitly estimates the speech signal from the noisy mixture, it does not suffer from the permutation problem that is typical to conventional BSS techniques. The method is applicable to both instantaneous and convolutive mixtures and achieves the separation in a single step, without the need for iterations. Experimental results show superior performance compared to a general BSS algorithm.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-748"
  },
  "grisoni05_interspeech": {
   "authors": [
    [
     "Louisa Busca",
     "Grisoni"
    ],
    [
     "John H. L.",
     "Hansen"
    ]
   ],
   "title": "Collaborative voice activity detection for hearing aids",
   "original": "i05_2353",
   "page_count": 4,
   "order": 749,
   "p1": "2353",
   "pn": "2356",
   "abstract": [
    "Most binaural hearing aid systems have separate processing for each ear. Collaborative processing allows the left and right hearing aids to share information and resources. In this paper, collaborative processing is used to take advantage of head shadow effects to improve Voice Activity Detection (VAD). We emphasize here that while a variety of improvements in VAD and speech enhancement are possible, contributions have meaning only if the advances fit into the framework of the limited computing resources available in current hearing aid devices.\n",
    "The proposed approach leads to an absolute speech hit rate improvement of up to 25% at low SNR in the ear closest to noise source. The cost is a lower noise hit rate in the other ear. It is however sufficient to ensure proper noise spectrum magnitude estimation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-749"
  },
  "robledoarnuncio05_interspeech": {
   "authors": [
    [
     "Enrique",
     "Robledo-Arnuncio"
    ],
    [
     "Biing-Hwang",
     "Juang"
    ]
   ],
   "title": "Using inter-frequency decorrelation to reduce the permutation inconsistency problem in blind source separation",
   "original": "i05_2357",
   "page_count": 4,
   "order": 750,
   "p1": "2357",
   "pn": "2360",
   "abstract": [
    "One of the most important problems in frequency domain blind source separation is the inconsistency across frequency in the permutation of the source estimates. In this paper we present a new algorithm that simultaneously diagonalizes the intra-frequency and the inter-frequency correlation matrices of the source estimates. Experimental results, using speech signals, reveal that the algorithm achieves a highly improved alignment of the permutations between neighbor frequency bins.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-750"
  },
  "subramanya05b_interspeech": {
   "authors": [
    [
     "Amarnag",
     "Subramanya"
    ],
    [
     "Zhengyou",
     "Zhang"
    ],
    [
     "Zicheng",
     "Liu"
    ],
    [
     "Jasha",
     "Droppo"
    ],
    [
     "Alex",
     "Acero"
    ]
   ],
   "title": "A graphical model for multi-sensory speech processing in air-and-bone conductive microphones",
   "original": "i05_2361",
   "page_count": 4,
   "order": 751,
   "p1": "2361",
   "pn": "2364",
   "abstract": [
    "In continuation of our previous work on using an air-and-boneconductive microphone for speech enhancement, in this paper we propose a graphical model based approach to estimating the clean speech signal given the noisy observations in the air sensor. We also show how the same model can be used as a speech/nonspeech classifier. With the aid of MOS (mean opinion score) tests we show, that the performance of the proposed model is better in comparison to our previously proposed direct filtering algorithm.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-751"
  },
  "dusan05c_interspeech": {
   "authors": [
    [
     "Sorin",
     "Dusan"
    ]
   ],
   "title": "On the nature of acoustic information in identification of coarticulated vowels",
   "original": "i05_2449",
   "page_count": 4,
   "order": 752,
   "p1": "2449",
   "pn": "2452",
   "abstract": [
    "Perceptual studies involving modified syllables showed that the information necessary to identify coarticulated vowels is distributed throughout the duration of the vowels and the adjacent transitions from and to consonants. Different hypotheses were formulated to explain the nature of this dynamic spectral information. This paper presents an analysis of average spectral trajectories of nine vowels in three left and three right consonant contexts in American English. The results of this analysis are discussed in connection with previously proposed hypotheses. These results, connected together, suggest a more comprehensive explanation of the nature of the acoustic (spectral, dynamic, temporal) information in the identification of coarticulated vowels. This explanation also appears to hold true for other classes of coarticulated sounds, in addition to vowels.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-752"
  },
  "gendrot05_interspeech": {
   "authors": [
    [
     "Cédric",
     "Gendrot"
    ],
    [
     "Martine",
     "Adda-Decker"
    ]
   ],
   "title": "Impact of duration on F1/F2 formant values of oral vowels: an automatic analysis of large broadcast news corpora in French and German",
   "original": "i05_2453",
   "page_count": 4,
   "order": 753,
   "p1": "2453",
   "pn": "2456",
   "abstract": [
    "Formant values of oral vowels are automatically measured in a total of 50000 segments from four hours of journalistic broadcast speech in French and German. After automatic segmentation using the LIMSI speech alignment system, formant values are automatically extracted using PRAAT. Vowels with unlikely formant values are discarded (4%). The measured values are exposed with respect to segment duration and language identity. The gap between the measured mean Fi values and reference Fi values is inversely proportional to vowel duration: a tendency to reduction for vowels of short duration clearly emerges for both languages. These results are briefly discussed in terms of centralisation and coarticulation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-753"
  },
  "quene05_interspeech": {
   "authors": [
    [
     "Hugo",
     "Quene"
    ]
   ],
   "title": "Modeling of between-speaker and within-speaker variation in spontaneous speech tempo",
   "original": "i05_2457",
   "page_count": 4,
   "order": 754,
   "p1": "2457",
   "pn": "2460",
   "abstract": [
    "Speech tempo (speaking rate) varies both between and within speakers. Previous research suggests several relevant factors and predictors. The present study investigates all these factors combined, both between and within speakers, in a large corpus of spoken Dutch interviews. This is done by means of multi-level modeling of sex, age, and dialect region (all between speakers) and phrase length and sequential position of phrase within session (both within speakers). Results show that speech tempo depends mainly on phrase length, and not on between-speaker factors sex, age, or dialect region. Within-speaker tempo variations exceed the JND. Separate modeling of phrase length itself reveals significant negative effects of age and of sequential position, but not of region or sex. When taken together, these results underline the phonetic and communicative importance of within-speaker variations in speech tempo.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-754"
  },
  "komatsu05_interspeech": {
   "authors": [
    [
     "Masahiko",
     "Komatsu"
    ],
    [
     "Makiko",
     "Aoyagi"
    ]
   ],
   "title": "Vowel devoicing vs. mora-timed rhythm in spontaneous Japanese - inspection of phonetic labels of OGI_TS",
   "original": "i05_2461",
   "page_count": 4,
   "order": 755,
   "p1": "2461",
   "pn": "2464",
   "abstract": [
    "This paper discusses two well-known phonetic-phonological phenomena of Japanese, vowel devoicing and mora-timed rhythm. Regarding vowel devoicing as a process of vowels getting consonantal, it may change the mora-templated syllable structures of Japanese, resulting in the deviation from mora-timed rhythm. To investigate spontaneous Japanese with comparison to other languages, the label data of OGI_TS was analyzed. First, it seemed that vowel devoicing occurs more frequently in spontaneous Japanese than in controlled speech. In spontaneous speech, vowel devoicing frequently occurs even in the environments non-typical of vowel devoicing. Next, rhythm analysis showed that it is close to syllable-timed rhythm. These results suggest that vowel devoicing caused spontaneous Japanese deviates from mora-timed rhythm moving to syllable-timed.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-755"
  },
  "altamimi05_interspeech": {
   "authors": [
    [
     "Jalal-Eddin",
     "Al-Tamimi"
    ],
    [
     "Emmanuel",
     "Ferragne"
    ]
   ],
   "title": "Does vowel space size depend on language vowel inventories? evidence from two Arabic dialects and French",
   "original": "i05_2465",
   "page_count": 4,
   "order": 756,
   "p1": "2465",
   "pn": "2468",
   "abstract": [
    "The aim of this paper is to study the effect of the number of vowels a language has on the size of its vocalic space, in the production of speech from two Arabic dialects, Moroccan and Jordanian Arabic, and from French. 5 speakers per language (or dialect) recorded a list of vowels in 3 conditions: vowels were embedded in 1) Words, 2) Syllables, and 3) produced in Isolation. For each condition, there were 3 consonantal contexts: /b d k/. Our results corroborate the hypothesis that vowel inventory size affects the size of the acoustic vowel spaces: the larger the vowel inventory, the bigger the acoustic vowel space.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-756"
  },
  "shih05_interspeech": {
   "authors": [
    [
     "Chilin",
     "Shih"
    ]
   ],
   "title": "Understanding phonology by phonetic implementation",
   "original": "i05_2469",
   "page_count": 4,
   "order": 757,
   "p1": "2469",
   "pn": "2472",
   "abstract": [
    "Many phonological processes originated as phonetic reduction phenomena. An informative way to understand these processes and the related issue of the phonology/phonetics interface is to build a phonetic implementation model that simulates these processes. This paper investigates production data of Mandarin tone2 (a rising tone) in an environment that may induce Mandarin tone2 sandhi, and shows that the sandhi rule is a phonetic reduction phenomenon occurring in prosodically weak positions. The data supports a model where speech variation arises as the speaker balances the trade-off relationship between ease of articulatory effort and communication accuracy. A weight representing prosodic strength is introduced in the simulation model to capture this trade-off relationship. The shifting prosodic weight from unit to unit controls surface phonetic variations which match the production data.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-757"
  },
  "moates05_interspeech": {
   "authors": [
    [
     "Danny R.",
     "Moates"
    ],
    [
     "Zinny S.",
     "Bond"
    ],
    [
     "Russell",
     "Fox"
    ],
    [
     "Verna",
     "Stockmal"
    ]
   ],
   "title": "The feature [sonorant] in lexical access",
   "original": "i05_2869",
   "page_count": 4,
   "order": 758,
   "p1": "2869",
   "pn": "2872",
   "abstract": [
    "Does the feature [sonorant] influence access to lexical entries? We created two nonwords by replacing a fricative in a real word with either another fricative (match condition) or a resonant (mismatch condition). Participants heard the nonwords and were asked to recover the real word. Errors were greater in the mismatch condition, suggesting that [-sonorant] is represented in the target fricative. Similar effects appeared when we used stops instead of fricatives. Resonant targets showed no difference in match and mismatch conditions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-758"
  },
  "mikuteit05_interspeech": {
   "authors": [
    [
     "Simone",
     "Mikuteit"
    ]
   ],
   "title": "Voice and aspiration in German and east bengali stops: a cross-language study",
   "original": "i05_2873",
   "page_count": 4,
   "order": 759,
   "p1": "2873",
   "pn": "2876",
   "abstract": [
    "This production study investigates temporal and F0 cues (closure duration, release duration and F0 perturbations of the following vowel) in respect to voice and aspiration in German and East Bengali geminate stop consonants. The objective is to compare the stops of the two languages according to their phonetic correspondence on the one side and to their phonological correspondence on the other. The results of the phonetic comparison show that the German and East Bengali stops behave analogically for closure duration and release duration, while phonologically there are some differences for these temporal properties. The F0 analysis revealed different curves for the stops of the corresponding phonetic classification but similarity for stops of the same phonological classification. The latter findings suggest that speakers actively adjust or influence F0 height, in order to match it to the stop's phonological classification in the particular language.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-759"
  },
  "jacobi05_interspeech": {
   "authors": [
    [
     "Irene",
     "Jacobi"
    ],
    [
     "Louis C. W.",
     "Pols"
    ],
    [
     "Jan",
     "Stroop"
    ]
   ],
   "title": "Polder dutch: aspects of the /ei/-lowering in standard dutch",
   "original": "i05_2877",
   "page_count": 4,
   "order": 760,
   "p1": "2877",
   "pn": "2880",
   "abstract": [
    "This paper is an initial report on the systematic analysis of changes within the vowel system of Standard Dutch. The work focuses on the recent lowering of the diphthong /Ei/, known as 'Polder Dutch' (Poldernederlands).\n",
    "The purpose was to find an automatizable method to reliably analyze and compare speakers of a large corpus of Dutch spontaneous speech. Diphthong variants of twelve native speakers were compared by measuring both formants and spectral energy distributions. The resulting pc1.pc2 plane of a principal component analysis (PCA) on the barkfiltered spectrum of anchor vowels /a/, /i/, /u/ was comparable to the F1-F2 bark plane of the same segments and explained 90% of the variance in the data. The differences between the speakers' vowel spaces appeared not to depend on sex, so male and female data could be calculated within the same analysis.\n",
    "The distance of the diphthong /Ei/ to the anchor vowels in the pc1.pc2 plane, together with the dipthong dynamics were the major cues to diphthong quality. Further indication of a sound change in progress for the 'Polder Dutch' speakers was the position of /E/ in the vowel space.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-760"
  },
  "castelli05_interspeech": {
   "authors": [
    [
     "Eric",
     "Castelli"
    ],
    [
     "René",
     "Carré"
    ]
   ],
   "title": "Production and perception of Vietnamese vowels",
   "original": "i05_2881",
   "page_count": 4,
   "order": 761,
   "p1": "2881",
   "pn": "2884",
   "abstract": [
    "It is well known that vowels can be produced in isolation, acoustically stable in such a way that they are represented as points in the F1-F2 plane. But, in a preceding study [1], following the predictions of the DRM model of speech production, the area function space and the corresponding acoustic space were already shown to be dynamically structured. Privileged formant trajectories were as follows: [ai], [ay], [au] and [AW] for the [a, E, e, i] [a, oe, o, y], [!, O, o, u] and [A, 2, G, W] vowel sets respectively. The present study examines further evidence in support of the DRM model, based on data pertaining to the production and perception of Vietnamese vowels. Results will be discussed in terms of vowel representation as sub-products of vowel-to-vowel trajectories. Two specific Vietnamese vowels '.a' & 'a' will be studied.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-761"
  },
  "ngoc05_interspeech": {
   "authors": [
    [
     "Tuan Vu",
     "Ngoc"
    ],
    [
     "Christophe",
     "d'Alessandro"
    ],
    [
     "Alexis",
     "Michaud"
    ]
   ],
   "title": "Using open quotient for the characterisation of vietnamese glottalised tones",
   "original": "i05_2885",
   "page_count": 4,
   "order": 762,
   "p1": "2885",
   "pn": "2888",
   "abstract": [
    "Vietnamese is a tone language in which the tone is a complex bundle of pitch and voice quality characteristics. The present study is restricted to Falling tones (i.e. it does not cover tone C2, called nga in Vietnamese spelling, which has medial glottalisation and ends on relatively high pitch), and deals mainly with tone C1 (hoi). This tone is generally described as falling then rising, but interestingly, some speakers realise it simply as falling. The primary aim of this paper is to investigate how these speakers maintain tone C1 distinct from two tones which are similar in terms of pitch: tones A2 (huyen) and B2 (nang). Analysis of audio and electroglottographic recordings of 7 speakers (441 syllables) confirms that there exist two types of strategies in the realisation of tone C1, and that in the falling realisation of C1, the voice quality at the end of the syllable differs from that of tones A2 and B2. It is further observed that this voice quality contrast cannot be captured by measurement of the open quotient alone, leading to general observations on the use of the open quotient in the characterisation of phenomena of glottalisation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-762"
  },
  "hajek05_interspeech": {
   "authors": [
    [
     "John",
     "Hajek"
    ],
    [
     "Mary",
     "Stevens"
    ]
   ],
   "title": "On the acoustic characterization of ejective stops in Waima'a",
   "original": "i05_2889",
   "page_count": 4,
   "order": 763,
   "p1": "2889",
   "pn": "2892",
   "abstract": [
    "We examine some acoustic properties of ejective stops in Waima'a (an Austronesian language spoken in East Timor), and compare them with other voiceless stop types that occur in the language. Previous studies of ejectives in other languages have suggested that they may fall into two classes, strong and weak. We compare our Waima'a results with some existing findings in the literature, and suggest that while Waima'a ejectives might appear to be more appropriately characterized as strong on some criteria, they do not sit squarely in either category.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-763"
  },
  "stevens05_interspeech": {
   "authors": [
    [
     "Mary",
     "Stevens"
    ],
    [
     "John",
     "Hajek"
    ]
   ],
   "title": "Spirantization of /p t k/ in Sienese Italian and so-called semi-fricatives",
   "original": "i05_2893",
   "page_count": 4,
   "order": 764,
   "p1": "2893",
   "pn": "2896",
   "abstract": [
    "This paper presents the results of a first acoustic phonetic investigation into voiceless spirantization in the variety of Tuscan Italian spoken in Siena. Based on spontaneous speech data (6 speakers), we focus upon occurrences of a phonetic variant, previously referred to as a 'semi-fricative'. Intermediate between a voiceless stop and a voiceless fricative, it is reported to occur in Pisan (e.g. [1], [2]) but not Florentine Italian (e.g. [3]).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-764"
  },
  "gilifivela05_interspeech": {
   "authors": [
    [
     "Barbara",
     "Gili Fivela"
    ],
    [
     "Claudio",
     "Zmarich"
    ]
   ],
   "title": "Italian geminates under speech rate and focalization changes: kinematic, acoustic, and perception data",
   "original": "i05_2897",
   "page_count": 4,
   "order": 765,
   "p1": "2897",
   "pn": "2900",
   "abstract": [
    "Italian geminate consonants have been studied as for both their phonetic characteristics and their phonological status. As for the phonetic aspects, though, few works focus on kinematic data. This study relates on both acoustic and kinematic material, as well as on perception data, in order to get evidence on the phonological status of geminate consonants.\n",
    "A read speech corpus was collected and analyzed both acoustically and kinematically, after being checked perceptually. The corpus is represented by utterances containing target words with bilabial and labiodental consonants, realized as singleton, geminates and within clusters, and produced at a natural speech rate and under speech rate and focalization changes. Data show that the lower lip opening gesture is similarly characterized in geminates and clusters, offering additional phonetic evidence that geminates may be given an heterosyllabic interpretation. Productions in focused contexts confirm results obtained in relation to the analysis of natural speech rate stimuli, while the increasing in speech rate shows that kinematic correlates of gemination appear to be more stable than acoustic ones, and that geminates still show characteristics similar to heterosyllabic segments, i.e. clusters.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-765"
  },
  "kim05f_interspeech": {
   "authors": [
    [
     "Sunhee",
     "Kim"
    ]
   ],
   "title": "Durational characteristics of Korean Lombard speech",
   "original": "i05_2901",
   "page_count": 4,
   "order": 766,
   "p1": "2901",
   "pn": "2904",
   "abstract": [
    "This paper aims to examine the durational characteristics of Korean Lombard speech using speech data, which consist of 500 Lombard words and 500 normal words of 10 speakers (5 males and 5 females). Each word was segmented and labeled manually and the duration of each segment and each word was extracted for comparison. The durational change of Lombard speech in comparison with normal speech was analyzed using a statistical method. The results show that durational characteristics of Korean Lombard speech mostly correspond to the results reported in previous research. The duration of words in Lombard speech is increased in comparison with normal speech, and the average unvoiced consonantal duration is decreased while the average vocalic duration is increased. Female speakers show a stronger tendency to lengthen the duration in Lombard speech, but the difference between females and males was not statistically significant. Finally, this study also shows that the speakers of Lombard speech could be classified according to their different duration rates, which demonstrates speaker dependent characteristics.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-766"
  },
  "iseijaakkola05_interspeech": {
   "authors": [
    [
     "Toshiko",
     "Isei-Jaakkola"
    ],
    [
     "Satoshi",
     "Asakawa"
    ]
   ],
   "title": "A cross-linguistic study of vowel quantity in different word structures: Japanese, Finnish and Czech",
   "original": "i05_2905",
   "page_count": 4,
   "order": 767,
   "p1": "2905",
   "pn": "2908",
   "abstract": [
    "The durational patterns between short and long vowels were investigated at the segmental and lexical level using four different word structures in Japanese, Finnish and Czech. The results confirmed that the Japanese segmental ratios between short and long vowels were longer than in Finnish and Czech; the Czech durational ratios in the second syllable were longer than those in the first syllable; the surrounding consonantal difference affected more Finnish for the short vowels, but more Japanese for the long vowels; the significance was observed in word structural differences more than language differences, and in the vowels occurring in the first syllable. Concerning the lexical durational ratios according to the different word structures, Japanese showed more isochronal increasing patterns; the temporal control of segments within a word were similar between Japanese and Finnish; Finnish and Czech vowel quantity might be more controlled by stress position within a word and phonotactics.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-767"
  },
  "mori05b_interspeech": {
   "authors": [
    [
     "Laura",
     "Mori"
    ],
    [
     "Melissa",
     "Barkat-Defradas"
    ]
   ],
   "title": "Acoustic properties of foreign accent: VOT variations in Moroccan-accented Italian",
   "original": "i05_2909",
   "page_count": 4,
   "order": 768,
   "p1": "2909",
   "pn": "2912",
   "abstract": [
    "The present study investigates the temporal parameter of VOT from a cross-language perspective, as far as native Moroccan, native Italian and Moroccan-accented Italian are concerned. The comparative analysis carried out underlines a language effect on the VOT duration across the three language varieties. The statistical test points out VOT as one of the acoustic properties that characterize the foreign accent, being the long-lag /t/ and the long-lag aspirated /k/ interlanguage markers due to L1 phonetic interference.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-768"
  },
  "rauber05_interspeech": {
   "authors": [
    [
     "Andréia S.",
     "Rauber"
    ],
    [
     "Paola",
     "Escudero"
    ],
    [
     "Ricardo A. H.",
     "Bion"
    ],
    [
     "Barbara O.",
     "Baptista"
    ]
   ],
   "title": "The interrelation between the perception and production of English vowels by native speakers of Brazilian Portuguese",
   "original": "i05_2913",
   "page_count": 4,
   "order": 769,
   "p1": "2913",
   "pn": "2916",
   "abstract": [
    "This study investigated the relationship between the perception and production of English vowels by highly proficient Brazilian EFL speakers. Two experiments were carried out: A production test to measure the first two formants of the learners' English and Brazilian Portuguese (BP) vowels, and an oddity discrimination test to investigate the L2 (second language) categorial perception of English vowels. The production results reveal that the learners' F1 and F2 values for English resembled the F1 and F2 values for BP. The results of the L2 perception test indicate that the learners discriminated different English vowels with different degrees of accuracy. Importantly, some relationship between perception and production was found because the two English (i.e., the learners' L2) vowel sounds that were poorly discriminated were produced with F1 and F2 values similar to those of a single vowel in the learners' L1 (first language).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-769"
  },
  "hoelterhoff05_interspeech": {
   "authors": [
    [
     "Julia",
     "Hoelterhoff"
    ]
   ],
   "title": "Recognition of German obstruents",
   "original": "i05_2917",
   "page_count": 4,
   "order": 770,
   "p1": "2917",
   "pn": "2920",
   "abstract": [
    "The aim of the present investigation was to find characteristics in the speech signal that allow distinguishing the German obstruents /pf/ and /ts/, /f/ and /s/ and /p/ and /t/. Two acoustic analysis techniques were chosen to separate the target phonemes according to place and manner of articulation: measuring duration distinguished them in manner (fricative, stop and affricate), whereas the calculation of the relative amplitude of particular frequency bands distinguished them according to place of articulation (labial and alveolar). The speech data was recorded in a natural dialog situation to gain features that are robust towards variation in the speech signal. The characteristics found will serve for the improvement of the \"FUL\" automatic speech recognition system [1]. The FUL speech recognition system is based on underlying phonological features providing robustness for all kinds of variations in the speech signal.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-770"
  },
  "skarnitzl05_interspeech": {
   "authors": [
    [
     "Radek",
     "Skarnitzl"
    ],
    [
     "Jan",
     "Volín"
    ]
   ],
   "title": "Czech voiced labiodental continuant discrimination from basic acoustic data",
   "original": "i05_2921",
   "page_count": 4,
   "order": 771,
   "p1": "2921",
   "pn": "2924",
   "abstract": [
    "The Czech voiced labiodental continuant has a special position within the system of Czech consonants. Due to its past it sometimes behaves phonologically as a sonorant, while its canonical articulatory qualities place it among obstruents. It is quite difficult to decide how much of its sound is sonorant and how much is obstruent by mere auditory analysis. We investigated whether basic acoustic parameters extracted from fluent reading material can help to discriminate if the Czech [v] leans more towards the obstruent or the sonorant class of consonants. Dynamic profile data were processed by factor analysis and, together with duration and harmonicity measurements, used as an input for cluster analysis, discriminant analysis, and classification-tree analysis. The results show that acoustically, the Czech intervocalic [v] in connected speech is more in line with its sonorant past rather than its obstruent presence.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-771"
  },
  "maj05_interspeech": {
   "authors": [
    [
     "Jean-Baptiste",
     "Maj"
    ],
    [
     "Anne",
     "Bonneau"
    ],
    [
     "Dominique",
     "Fohr"
    ],
    [
     "Yves",
     "Laprie"
    ]
   ],
   "title": "An elitist approach for extracting automatically well-realized speech sounds with high confidence",
   "original": "i05_2925",
   "page_count": 4,
   "order": 772,
   "p1": "2925",
   "pn": "2928",
   "abstract": [
    "This paper presents an 'elitist approach' for extracting automatically well-realized speech sounds with high confidence. The elitist approach uses a speech recognition system based on Hidden Markov Models (HMM). The HMM are trained on speech sounds which are systematically well-detected in an iterative procedure. The results show that, by using the HMM models defined in the training phase, the speech recognizer detects reliably specific speech sounds with a small rate of errors.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-772"
  },
  "tyson05_interspeech": {
   "authors": [
    [
     "Na'im R.",
     "Tyson"
    ]
   ],
   "title": "Applying multiple regression models for predicting word duration in a corpus of spontaneous speech",
   "original": "i05_2929",
   "page_count": 4,
   "order": 773,
   "p1": "2929",
   "pn": "2932",
   "abstract": [
    "Using word duration as a representative of pronunciation variation, the objective of this research was to delineate a set of variables known to affect word duration and determine the total amount of variation in duration accounted for by them in a multiple linear regression model. More importantly, computing the amount of variation each variable contributes (independently of the others) is crucial in proving its predictive power. Authors such as [1] claim that probabilistic measures such as unigram probability greatly affect whether a word is likely to be reduced in its pronunciation (i.e. the more likely a word is to appear, the greater the chance of it being reduced). However, after performing a regression analysis on word durations from the Variation in Conversation (ViC) corpus of spontaneous speech, and computing partial correlation coefficients of each factor, the results showed that probabilistic measures such as unigram and bigram probability account for less than 1% of the variation in word duration. This finding suggests that the predictive power of certain variables is dependent on the type of corpus being examined - in the case of the spontaneous speech studies in [1], the examined corpus consisted of phone conversations, while the ViC corpus contains monologues.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-773"
  },
  "oliveira05_interspeech": {
   "authors": [
    [
     "Catarina",
     "Oliveira"
    ],
    [
     "Lurdes Castro",
     "Moutinho"
    ],
    [
     "António J. S.",
     "Teixeira"
    ]
   ],
   "title": "On european Portuguese automatic syllabification",
   "original": "i05_2933",
   "page_count": 4,
   "order": 774,
   "p1": "2933",
   "pn": "2936",
   "abstract": [
    "This paper presents three methods for dividing European Portuguese (EP) words into syllables, two of them handling graphemes as input, the other processing phone sequences. All three try to incorporate linguistic knowledge about EP syllable structure, but in different degrees. Experimental results showed, for the best method, percentage of correctly recognized syllable boundaries above 99.5%, and comparable word accuracy. The much simpler finite state transducer based method also achieved a good performance, making it suitable for applications more interested in speed and memory footprint. Being syllabification an essential component of many speech and language processing systems, proposed methods can be useful to researchers working with the EP language.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-774"
  },
  "chalamandaris05_interspeech": {
   "authors": [
    [
     "A.",
     "Chalamandaris"
    ],
    [
     "S.",
     "Raptis"
    ],
    [
     "Pirros",
     "Tsiakoulis"
    ]
   ],
   "title": "Rule-based grapheme-to-phoneme method for the Greek",
   "original": "i05_2937",
   "page_count": 4,
   "order": 775,
   "p1": "2937",
   "pn": "2940",
   "abstract": [
    "This paper describes a trainable method for generating letter to sound rules for the Greek language, for producing the pronunciation of out-of-vocabulary words. Several approaches have been adopted over the years for grapheme-to-phoneme conversion, such as hand-seeded rules, finite state transducers, neural networks, HMMs etc, nevertheless it has been proved that the most reliable method is a rule-based one. Our approach is based on a semiautomatically pre-transcribed lexicon, from which we derived rules for automatic transcription. The efficiency and robustness of our method are proved by experiments on out-of-vocabulary words which resulted in over than 98% accuracy on a word-base criterion.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-775"
  },
  "kalimeris05_interspeech": {
   "authors": [
    [
     "Constandinos",
     "Kalimeris"
    ],
    [
     "George",
     "Mikros"
    ],
    [
     "Stelios",
     "Bakamidis"
    ]
   ],
   "title": "Assimilation and deletion phenomena involving word-final /n/ and word-initial /p, t, k/ in modern Greek: a codification of the observed variation intended for use in TTS synthesis",
   "original": "i05_2941",
   "page_count": 4,
   "order": 776,
   "p1": "2941",
   "pn": "2944",
   "abstract": [
    "The present paper is a preliminary attempt to codify assimilation and deletion phenomena in Modern Greek involving the word-final /n/ of certain high-frequency function words and the word-initial voiceless stop (/p/, /t/ or /k/) of the following word. In natural speech, every relevant phoneme combination affected by these post-lexical phonological processes will generate one phonetic realisation from a set of legitimate variants. The variants' distribution is not random but subject to both linguistic and extralinguistic conditions. This work also explores possible ways to exploit the findings of recent sociolinguistic research on Modern Greek with a view to accommodating the observed variation and the rules codifying it within the framework of text-to-speech (TTS) synthesis. Such a development is expected to improve synthetic speech output, both in terms of naturalness and intelligibility.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-776"
  },
  "weiss05_interspeech": {
   "authors": [
    [
     "Christian",
     "Weiss"
    ],
    [
     "Bianca",
     "Aschenberner"
    ]
   ],
   "title": "A German viseme-set for automatic transcription of input text used for audio-visual speech synthesis",
   "original": "i05_2945",
   "page_count": 4,
   "order": 777,
   "p1": "2945",
   "pn": "2948",
   "abstract": [
    "In this paper, we introduce a German viseme inventory for visemically transcribing text according to phonetic transcription. A viseme set like the one presented in this work is essential for speech-driven audio-visual synthesis due to the fact that the selection of appropriate video segments is based on the visemically transcribed input text.\n",
    "For text-to-speech synthesis, a transcription of the input text into the phonemic representation is used, in order to avoid ambiguous meanings and to acquire the correct pronunciation of the underlying input text and to serve as labels in unit-selection-based synthesis systems. Likewise, the visual synthesis requires a transcription that represents - analogue to the phonemes - the visual counterpart which is called viseme in related literature and which also serves as a unit label in our data-driven video-realistic audio-visual synthesis system.\n",
    "We worked out an inventory of German viseme classes in a SAMPA-like labelling and trained a model for automatic visemic transcription of given input text.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-777"
  },
  "roy05_interspeech": {
   "authors": [
    [
     "Johanna-Pascale",
     "Roy"
    ]
   ],
   "title": "Visual perception of anticipatory rounding gestures in French",
   "original": "i05_2949",
   "page_count": 4,
   "order": 778,
   "p1": "2949",
   "pn": "2952",
   "abstract": [
    "This work is a contribution to studies on anticipation in speech, in the visual perception domain. Anticipatory gestures produced by two French speakers are analysed using kinematic data for VunroundedCnVrounded sequences, in order to examine the timing of labial anticipation and its expansion. Our results show that anticipatory gestures can be initiated within configurations related to the unrounded vowel. Moreover, our data do not confirm the predictions of dominant models for anticipatory production, except for those of the Movement Expansion Model. The perceptual efficiency of these gestures in the visual modality was tested. Results confirm that visual perception of anticipatory gestures follow their expansion. In the visual modality, a few conditions have to be fulfilled in order to allow recognition of the upcoming rounded vowel: A high velocity; At least 60 ms from the beginning of the anticipatory movement; In order to enhance recognition, anticipatory movements should accelerate, then decelerate near the final target. In other words, when movements are performed with several minor velocity peaks, the rounded is recognised later. Thus, in dynamic conditions, perceptual efficiency of anticipatory gesture is independent of the duration preceding the onset of the rounded vowel, static configuration or absolute kinematic event.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-778"
  },
  "levin05_interspeech": {
   "authors": [
    [
     "Esther",
     "Levin"
    ],
    [
     "Alex",
     "Levin"
    ]
   ],
   "title": "Spoken dialog system for real-time data capture",
   "original": "i05_2497",
   "page_count": 4,
   "order": 779,
   "p1": "2497",
   "pn": "2500",
   "abstract": [
    "This papers reports on the development of spoken dialog system as a tool for real-time data collection for healthcare, life and behavioral science. Specifically, we implemented a dialog system, Pain Monitoring Voice Diary, for monitoring chronic pain patients. We discuss the requirements and characteristics of this application, specifically, the needs for adaptive level of user support and high and controllable accuracy of data capture, and show their implications for the design of Pain Monitoring Voice Diary. In usability study involving 118 dialog sessions with 24 volunteers we measured 98% data capture and 80% dialog efficiency, as estimated by percentage of task-oriented prompts.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-779"
  },
  "pucher05_interspeech": {
   "authors": [
    [
     "Michael",
     "Pucher"
    ],
    [
     "Peter",
     "Fröhlich"
    ]
   ],
   "title": "A user study on the influence of mobile device class, synthesis method, data rate and lexicon on speech synthesis quality",
   "original": "i05_2501",
   "page_count": 4,
   "order": 780,
   "p1": "2501",
   "pn": "2504",
   "abstract": [
    "In this paper, we report on a comparative user study about the quality of mobile speech synthesis methods. We measured the impact of device class, data rate, synthesis method (diphone vs. non-uniform unit-selection) and lexicon usage on speech quality (word comprehension and several subjective satisfaction metrics). Seven practically relevant speech synthesis implementations and one natural voice were evaluated, applying the method recommended in ITU-T P.85, with additional pairwise comparisons. As a general result, although the overall subjective ratings of the synthetic voices differed significantly, the word comprehension rates were quite similar. We found a significant impact of data rate and synthesis method on the mean subjective speech quality, but not on word comprehension. The use of a lexicon in embedded speech synthesis slightly improved the perceived pronunciation quality.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-780"
  },
  "chen05f_interspeech": {
   "authors": [
    [
     "Fang",
     "Chen"
    ],
    [
     "Yael",
     "Katzenellenbogen"
    ]
   ],
   "title": "User's experience of a commercial speech dialogue system",
   "original": "i05_2505",
   "page_count": 4,
   "order": 781,
   "p1": "2505",
   "pn": "2508",
   "abstract": [
    "Following up with the speech technology development, spoken dialogue systems are getting commercialized. Many public service companies, such as call centres, tourist information centres, travel agencies, etc., are applying spoken dialogue systems for customer communication. However, there are many usability problems with these spoken dialogue systems. A study of the usability and user's experience of a consumer Swedish spoken dialog system is reported here. Six users went through three different scenarios in a home environment during the tests. The analysis of the participant's conversation with the dialogue system, observations, interviews and questionnaire were used as the testing methods. The result shows that the system is working, since all the users concluded their tasks. However, there are a few usability problems were found. These problems are: the understanding between the user and the system, the system's flexibility, naturalness, and correctness, error handling and the user's control of the system, as well as the information overloading to user's memory. Concerning the user's experience, the study showed that the variables as elegance, emotion, encouragement, intelligence or personality were not present. These variables would affect users' attitude towards the system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-781"
  },
  "levin05b_interspeech": {
   "authors": [
    [
     "Esther",
     "Levin"
    ],
    [
     "Amir M.",
     "Mané"
    ]
   ],
   "title": "Voice user interface design for automated directory assistance",
   "original": "i05_2509",
   "page_count": 4,
   "order": 782,
   "p1": "2509",
   "pn": "2512",
   "abstract": [
    "This paper focuses on the challenges that one encounters when building for commercial deployment an automated system for Directory Assistance (DA.) The design for an automated DA system needs to take into account constraints and requirements that arise from three distinct aspects of the application, namely, the business drivers, the user needs, and the strengths and weaknesses of voice technologies.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-782"
  },
  "alvarezryan05_interspeech": {
   "authors": [
    [
     "Maria Gabriela",
     "Alvarez-Ryan"
    ],
    [
     "Narendra",
     "Gupta"
    ],
    [
     "Barbara",
     "Hollister"
    ],
    [
     "Tirso",
     "Alonso"
    ]
   ],
   "title": "Optimizing user experience through design of the spoken language understanding (SLU) module",
   "original": "i05_2513",
   "page_count": 4,
   "order": 783,
   "p1": "2513",
   "pn": "2516",
   "abstract": [
    "We describe a case study depicting two different strategies for creating a spoken language understanding (SLU) module for a complex telecommunications natural language application. During Phase 1 we created an Annotation Guide that defines semantic call-types and uses these call-types to train the SLU. In Phase 2 we separated the process of creating the Annotation Guide from the training of the SLU. We designed the Annotation Guide with granular and hierarchical call-types that could be combined easily. After the data was labeled, we followed an iterative process of building SLUs with different configurations of the Annotation Guide call-types until we found the SLU that performed the best, given the application requirements. To evaluate the performance of the SLU, we relied on the overall F-Measure traditionally used for technical evaluations. In addition, we considered the F-Measures for the individual call-types. The goal was to have the largest number of application-specific call-types with F-Measures of 70.0 and higher. The results indicated that the Phase 2 SLU far surpassed the performance of the Phase 1 SLU.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-783"
  },
  "wright05_interspeech": {
   "authors": [
    [
     "Jeremy",
     "Wright"
    ],
    [
     "David",
     "Kapilow"
    ],
    [
     "Alicia",
     "Abella"
    ]
   ],
   "title": "Interactive visualization of human-machine dialogs",
   "original": "i05_2517",
   "page_count": 4,
   "order": 784,
   "p1": "2517",
   "pn": "2520",
   "abstract": [
    "Automated spoken dialog systems require systematic procedures for evaluating performance and diagnosing problems. We present an interactive tool that provides graphical views of how callers navigate through such systems, enabling fine-grained analysis for system evaluation and business intelligence. The input is a feed of call-logs. The output is an empirical dialog trajectory analysis represented as stochastic finite state machines, accessible via the web. Complexity is managed by an automatic tokenization procedure that hides fine details until needed. Users can generate selective views of parts of the dialog at high resolution (with access to call data), or zoom out to a summary. The tool provides dialog system developers with all the information they need from a single source, and is in use with directed-dialog and natural-language applications.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-784"
  },
  "aylett05_interspeech": {
   "authors": [
    [
     "Matthew P.",
     "Aylett"
    ]
   ],
   "title": "Synthesising hyperarticulation in unit selection TTS",
   "original": "i05_2521",
   "page_count": 4,
   "order": 785,
   "p1": "2521",
   "pn": "2524",
   "abstract": [
    "Within speech synthesis we often wish to give extra focus to words which carry important information, such as names, dates and amounts. In this paper we look carefully at cost functions that can be used to bias unit selection in favour of hyper-articulated speech in order to give this impression of focus. Hyper-articulated speech tends to be accented, emphatic and requires more articulatory effort. We apply two cost functions to try to force the selection of hyper-articulated speech. The first operates on the duration of units in the unit selection database, the second on the language redundancy (word trigram predictability) of the word containing the unit. We estimate their relative importance in selecting hyperarticulated speech in unit selection speech synthesis. A listening test was carried out where these cost functions were applied to one random content word in a haskins anomalous sentence. Listeners were asked to select the two clearest and most focused words from the sentence. The duration increasing cost function was significantly related to an increase in perceived prominence whereas low redundancy, and a combination of both approaches did not produce significant results. Thus, although a significant correlation exists between the average duration and redundancy of diphones and perceived prominence, such a correlation was not smoothly translated into error free method for altering such perceived prominence.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-785"
  },
  "tihelka05_interspeech": {
   "authors": [
    [
     "Daniel",
     "Tihelka"
    ]
   ],
   "title": "Symbolic prosody driven unit selection for highly natural synthetic speech",
   "original": "i05_2525",
   "page_count": 4,
   "order": 786,
   "p1": "2525",
   "pn": "2528",
   "abstract": [
    "In the effort to obtain synthetic speech with the quality near to natural, and potentially, to be able to build expressive synthesis, the unit selection approach has become very important. To increase the naturalness of our native TTS system ARTIC we employed the specific version of the approach. It is driven by the high-level symbolic prosody description, defined according to the phenomena of prosodic synonymy and homonymy. The concrete prosody of a synthesized phrase is not explicitly set here, but emerges on the basis of the target and concatenation costs. Although this is our first treatment requiring some simplification, and for the synonymy/homonymy phenomena only the basics are defined, the first results have already shown that there is a significant shift towards high quality. Listening tests comparing speech from single-instance version to selection-based version of ARTIC showed clear preference of the selection-based version. In addition, the level of naturalness was on average assessed as \"close to natural\".\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-786"
  },
  "matousek05_interspeech": {
   "authors": [
    [
     "Jindrich",
     "Matousek"
    ],
    [
     "Zdenek",
     "Hanzlícek"
    ],
    [
     "Daniel",
     "Tihelka"
    ]
   ],
   "title": "Hybrid syllable/triphone speech synthesis",
   "original": "i05_2529",
   "page_count": 4,
   "order": 787,
   "p1": "2529",
   "pn": "2532",
   "abstract": [
    "In this paper, the syllable, an alternative phonetic unit to the phone, is researched in the context of speech synthesis. Several approaches to syllable modelling within the statistical approach (using hidden Markov models) to the acoustic unit inventory creation are proposed and evaluated. To be able to synthesize an arbitrary text, the syllable inventories were supplemented with triphones resulting in hybrid syllable/triphone inventories. Listening tests were accomplished both to assess the quality of the resulting synthetic speech produced using the hybrid syllable/ triphone inventories and to choose the best approach to syllable modelling. The resulting synthetic speech is highly intelligible and fluent. Although the synthetic speech generated using the baseline triphone inventory was assessed slightly better, the results of the very first experiments with syllable modelling are very promising.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-787"
  },
  "campillodiaz05_interspeech": {
   "authors": [
    [
     "Francisco",
     "Campillo Díaz"
    ],
    [
     "José Luis",
     "Alba"
    ],
    [
     "Eduardo",
     "Rodríguez Banga"
    ]
   ],
   "title": "A neural network approach for the design of the target cost function in unit-selection speech synthesis",
   "original": "i05_2533",
   "page_count": 4,
   "order": 788,
   "p1": "2533",
   "pn": "2536",
   "abstract": [
    "Corpus-based speech synthesis performance depends on the skill to model and represent appropriately all the characteristics of the speech units that serve as a basis for concatenation. Although there is usually general agreement in the set of essential features (fundamental frequency, duration, power and phonetic context), it is still an open question the proper way of modelling them and considering their respective contributions to the cost functions, specially with regards to those related to the phonetic context. Precisely, this paper presents a new approach for modeling the phonetic context that also simplifies the hard task of training the corresponding weights to the different features in the target cost function.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-788"
  },
  "weiss05b_interspeech": {
   "authors": [
    [
     "Christian",
     "Weiss"
    ]
   ],
   "title": "FSM and k-nearest-neighbor for corpus based video-realistic audio-visual synthesis",
   "original": "i05_2537",
   "page_count": 4,
   "order": 789,
   "p1": "2537",
   "pn": "2540",
   "abstract": [
    "In this paper we introduce a corpus based 2D video-realistic audiovisual synthesis system. The system combines a concatenative Text-to-Speech (TTS) System with a concatenative Text-to-Visual (TTV) System to an audio lip-movement synchronized Text-to- Audio-Visual-Speech System (TTAVS). For the concatenative TTS we are using a Finite State Machine approach to select non-uniform variable-size audio segments. Analogue to the TTS a k-Nearest- Neighbor algorithm is applied to select the visual segments where we perform image filtering previous to the selection process to extract features which are used for the Euclidian distance measure to minimize distortions while concatenating the visual segments. We consider only the particular start-frame and end-frame between potential video-frame sequences for the Euclidian metric. The selection of the visual equivalence of the selected segments is based on a visemic transcription according to the phonemic transcription of the given input text. Due to using independent source databases for speech and video we synchronize the generated signals in a linear way. The resulting audio-visual utterance is audio lip-movement synchronized audio-visual speech. The system is adaptable easily to new speakers whether using a different speech or video source.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-789"
  },
  "chen05g_interspeech": {
   "authors": [
    [
     "Gui-Lin",
     "Chen"
    ],
    [
     "Ke-Song",
     "Han"
    ],
    [
     "Zhen-Li",
     "Yu"
    ],
    [
     "Dong-Jian",
     "Yue"
    ],
    [
     "Yi-Qing",
     "Zu"
    ]
   ],
   "title": "An embedded and concatenative approach to TTS of multiple languages",
   "original": "i05_2541",
   "page_count": 4,
   "order": 790,
   "p1": "2541",
   "pn": "2544",
   "abstract": [
    "This paper presents an embedded and concatenative approach to multilingual text-to-speech system (ECMTTS). Under a uniform architecture, the TTS modules are separated into language dependent and independent ones. A specifically defined super phonetic symbol set enables to use uniform speech unit for concatenation, and an elaborately indexing and storing approach can reduce the size of speech inventory. The TTS system employs an improved cost function-based unit selection strategy, an efficient speech synthesizer, and refined concatenation approach to balance the speech quality and memory size as well as computation requirement on embedded platforms.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-790"
  },
  "ezzat05_interspeech": {
   "authors": [
    [
     "Tony",
     "Ezzat"
    ],
    [
     "Ethan",
     "Meyers"
    ],
    [
     "James",
     "Glass"
    ],
    [
     "Tomaso",
     "Poggio"
    ]
   ],
   "title": "Morphing spectral envelopes using audio flow",
   "original": "i05_2545",
   "page_count": 4,
   "order": 791,
   "p1": "2545",
   "pn": "2548",
   "abstract": [
    "We present a method for morphing between smooth spectral magnitude envelopes of speech. An important element of our method is the notion of audio flow, which is inspired by similar notions of optical flow computed between images in computer vision applications. Audio flow defines the correspondence between two smooth spectral magnitude envelopes, and encodes the formant shifting that occurs from one sound to another. We present several algorithms for the automatic computation of audio flow from a small 20 second corpus of speech. In addition, we present an algorithm for morphing smoothly between any two spectral magnitude envelopes, given the computed audio flow between them.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-791"
  },
  "colotte05_interspeech": {
   "authors": [
    [
     "Vincent",
     "Colotte"
    ],
    [
     "Richard",
     "Beaufort"
    ]
   ],
   "title": "Linguistic features weighting for a text-to-speech system without prosody model",
   "original": "i05_2549",
   "page_count": 4,
   "order": 792,
   "p1": "2549",
   "pn": "2552",
   "abstract": [
    "This paper presents a Non-Uniform Units selection-based Text- To-Speech synthesizer. Nowadays, systems use prosodic models that do not allow the prosody to vary as far as we should hope, involving a listening comfort degradation. Our system has the advantage to avoid the using of prosodic model. Speech units selection builds its features set exclusively from the linguistic information generated by the natural language analysis. We also present an original method to automatically weight these features. Therefore, selected units are not restricted by a predetermined prosody. With only using linguistic features, we obtain a various prosody and the units concatenation is performed without resort to heavy signal processing.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-792"
  },
  "amdal05_interspeech": {
   "authors": [
    [
     "Ingunn",
     "Amdal"
    ],
    [
     "Torbjørn",
     "Svendsen"
    ]
   ],
   "title": "Unit selection synthesis database development using utterance verification",
   "original": "i05_2553",
   "page_count": 4,
   "order": 793,
   "p1": "2553",
   "pn": "2556",
   "abstract": [
    "Accurate annotation of the unit inventory database is of vital importance to the quality of unit selection text-to-speech synthesis. The time consuming manual work involved in database development limits the ability to produce new voices quickly and at low cost. Automatic annotation is therefore more and more in use. Misalignments due to mismatch between the predicted and pronounced unit sequence require manual correction to achieve natural sounding synthesis. This paper proposes a new annotation assessment method using log likelihood ratio based utterance verification on the recorded database. The utterance verification is applied to detect utterances where there is a likely mismatch between the predicted pronunciation and what is actually spoken, or where an automated procedure for phonemic labelling misaligns the phone labels and the acoustic content. In a fully automated procedure, utterances failing the verification test can be discarded. In semi-automatic procedures, the utterance verification can be applied to select utterances that need to be manually inspected, thereby reducing the manual effort. Preliminary experiments are presented that show promising figures for correct rejections.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-793"
  },
  "zhao05b_interspeech": {
   "authors": [
    [
     "Yong",
     "Zhao"
    ],
    [
     "Lijuan",
     "Wang"
    ],
    [
     "Min",
     "Chu"
    ],
    [
     "Frank K.",
     "Soong"
    ],
    [
     "Zhigang",
     "Cao"
    ]
   ],
   "title": "Refining phoneme segmentations using speaker-adaptive context dependent boundary models",
   "original": "i05_2557",
   "page_count": 4,
   "order": 794,
   "p1": "2557",
   "pn": "2560",
   "abstract": [
    "Consistent phoneme segmentation is essential in building high quality Text-to-Speech (TTS) voice fonts. In this paper we propose to adapt an existing well-trained Context Dependent Boundary Model (CDBM) for refining segment boundaries to a new speaker with limited, manually segmented data. Three adaptation approaches: MLLR, MAP, and a combination of the two, are studied. The combined one, MLLR+MAP, delivers the best boundary refinement performance. In comparison with other boundary segmentation methods, the adapted CDBM yields better results, especially with a limited amount of adaptation data. Given 400 manually segmented boundary tokens in about 20 sentences as a development set, the segmentation precision can reach 90% of human labeled boundaries within a tolerance of 20 ms.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-794"
  },
  "chen05h_interspeech": {
   "authors": [
    [
     "Yining",
     "Chen"
    ],
    [
     "Yong",
     "Zhao"
    ],
    [
     "Min",
     "Chu"
    ]
   ],
   "title": "Customizing base unit set with speech database in TTS systems",
   "original": "i05_2561",
   "page_count": 4,
   "order": 795,
   "p1": "2561",
   "pn": "2564",
   "abstract": [
    "In unit selection based speech synthesizer, defining a good unit set is crucial to the speech quality. In this paper, a method of customizing the TTS base unit set with a specific speech corpus is proposed. Multi-phoneme units are boosted from the initial phoneme-sized unit. A new multi-phoneme unit is added to the inventory based upon its own frequency count and the affected frequency count of other units. As a result, a large base unit set, which contains many multi-phoneme units, is formed when the speech corpus is large. While, for a small speech corpus, only a few bi-phoneme or tri-phoneme are found. Such a scalable base unit set makes it possible to achieve better smoothness in concatenation while maintain the naturalness of prosody. Evaluation results show that, after replacing the phone-sized base unit set with the customized set, the search speed is improved by 5 times and 59% preference score is obtained.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-795"
  },
  "rouibia05_interspeech": {
   "authors": [
    [
     "Soufiane",
     "Rouibia"
    ],
    [
     "Olivier",
     "Rosec"
    ]
   ],
   "title": "Unit selection for speech synthesis based on a new acoustic target cost",
   "original": "i05_2565",
   "page_count": 4,
   "order": 796,
   "p1": "2565",
   "pn": "2568",
   "abstract": [
    "This paper presents a new approach to unit selection for corpusbased speech synthesis, in which the units are selected according to acoustic criteria. In a learning stage, an acoustic clustering is carried out using context dependent HMM. During synthesis, an acoustic target is generated and segmented in the required diphone sequence. For each diphone to be synthesized, a pre-selection module determines the N-best instances that match this acoustic target. From these candidates, the optimal unit sequence is then obtained by minimizing a concatenation cost through dynamic programming. Objective as well as subjective tests are carried out which shows the relevance of the proposed method.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-796"
  },
  "chazan05_interspeech": {
   "authors": [
    [
     "Dan",
     "Chazan"
    ],
    [
     "Ron",
     "Hoory"
    ],
    [
     "Zvi",
     "Kons"
    ],
    [
     "Ariel",
     "Sagi"
    ],
    [
     "Slava",
     "Shechtman"
    ],
    [
     "Alexander",
     "Sorin"
    ]
   ],
   "title": "Small footprint concatenative text-to-speech synthesis system using complex spectral envelope modeling",
   "original": "i05_2569",
   "page_count": 4,
   "order": 797,
   "p1": "2569",
   "pn": "2572",
   "abstract": [
    "In this paper we present a method for speech modeling and its utilization in IBM's small footprint concatenative text-to-speech system. The method is based on frequency-domain, complex spectral envelope modeling, where the phase component plays a crucial role in attaining high quality speech synthesis. The modeling scheme presented enables low bit rate compression of the amplitude and phase information and low-complexity reconstruction of high quality speech with wide range pitch modification. Listening tests conducted for the overall text-to-speech system show a major improvement in MOS, compared to a previous, MFCC-based, system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-797"
  },
  "alias05_interspeech": {
   "authors": [
    [
     "Francesc",
     "Alías"
    ],
    [
     "Ignasi",
     "Iriondo"
    ],
    [
     "Lluís",
     "Formiga"
    ],
    [
     "Xavier",
     "Gonzalvo"
    ],
    [
     "Carlos",
     "Monzo"
    ],
    [
     "Xavier",
     "Sevillano"
    ]
   ],
   "title": "High quality Spanish restricted-domain TTS oriented to a weather forecast application",
   "original": "i05_2573",
   "page_count": 4,
   "order": 798,
   "p1": "2573",
   "pn": "2576",
   "abstract": [
    "A restricted domain text-to-speech system oriented to a weather forecast application is presented. This TTS system is embedded in a multimedia interactive service accessible from different media, such as TV, Internet and mobile devices. The requirements of this application give rise to several particularities in the design and implementation of the TTS system, which are discussed throughout this paper. Several tests have been conducted to analyze the TTS system in terms of performance and speech quality.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-798"
  },
  "bjrkan05_interspeech": {
   "authors": [
    [
     "Ingmund",
     "Bjørkan"
    ],
    [
     "Torbjørn",
     "Svendsen"
    ],
    [
     "Snorre",
     "Farner"
    ]
   ],
   "title": "Comparing spectral distance measures for join cost optimization in concatenative speech synthesis",
   "original": "i05_2577",
   "page_count": 4,
   "order": 799,
   "p1": "2577",
   "pn": "2580",
   "abstract": [
    "In concatenative synthesis the join cost function can be related to the probability of a perceived discontinuity at the join. Therefore it is important that the distance measures in the cost function correlate highly with human perceived discontinuities. In this paper the results of a listening test on joins in two Norwegian long vowels: /A:/ and /e:/, is presented. Five spectral distance measures and the F0 difference are compared as predictors of the human perceived discontinuities using Receiver Operating Characteristic (ROC) curves. In addition, a linear join cost function is optimized by means of stepwise linear regression.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-799"
  },
  "barros05_interspeech": {
   "authors": [
    [
     "Maria João",
     "Barros"
    ],
    [
     "Ranniery",
     "Maia"
    ],
    [
     "Keiichi",
     "Tokuda"
    ],
    [
     "Fernando Gil",
     "Resende"
    ],
    [
     "Diamantino",
     "Freitas"
    ]
   ],
   "title": "HMM-based european Portuguese TTS system",
   "original": "i05_2581",
   "page_count": 4,
   "order": 800,
   "p1": "2581",
   "pn": "2584",
   "abstract": [
    "The main purpose of the work here presented consists in applying a speech synthesis technique based on Hidden Markov Models (HMMs) to European Portuguese (EP), in order to build an HMMbased EP Text-to-Speech (TTS) System.\n",
    "The HMM-based speech synthesis technique (HSS) was originally tested for Japanese, but it has already been tested for other languages, such as English and Brazilian Portuguese. In fact, HSS comprises a compact module, being able to be applied to different languages.\n",
    "In the implemented TTS system, the speech waveform is generated by means of HMMs trained for an EP database, that are selected according to a set of automatically generated binary decision trees.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-800"
  },
  "hamza05b_interspeech": {
   "authors": [
    [
     "Wael",
     "Hamza"
    ],
    [
     "John F.",
     "Pitrelli"
    ]
   ],
   "title": "Combining the flexibility of speech synthesis with the naturalness of pre-recorded audio: a comparison of two approaches to phrase-splicing TTS",
   "original": "i05_2585",
   "page_count": 4,
   "order": 801,
   "p1": "2585",
   "pn": "2588",
   "abstract": [
    "Many applications of TTS incorporate both unpredictable words, which require the flexibility of TTS, and static phrases, for which the quality of recorded speech is unmatched by TTS. \"Phrase-splicing\" TTS attempts to provide the optimal combination of the two, by customizing concatenative TTS to such applications by incorporating application-specific recordings at the word or phrase level while resorting to smaller-unit synthesis to fill the gaps not covered by those recordings. In the past, we have achieved this by using a word-level search on the application-specific recordings followed by a general-purpose TTS search, in our case using sub-phonetic units, to fill the gaps. However, recent trends toward larger-unit roles in general-purpose TTS suggest a single-search approach for phrase splicing. A listening test shows that we achieve at least as high quality with the new one-search algorithm as with two-search.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-801"
  },
  "strecha05_interspeech": {
   "authors": [
    [
     "Guntram",
     "Strecha"
    ],
    [
     "Oliver",
     "Jokisch"
    ],
    [
     "Matthias",
     "Eichner"
    ],
    [
     "Rüdiger",
     "Hoffmann"
    ]
   ],
   "title": "Codec integrated voice conversion for embedded speech synthesis",
   "original": "i05_2589",
   "page_count": 4,
   "order": 802,
   "p1": "2589",
   "pn": "2592",
   "abstract": [
    "Voice conversion technologies transform individual characteristics of speech patterns while preserving the original content, and can be widely used in speech processing. Considering limited system resources, in particular, of embedded concatenative speech synthesis, voice conversion may reduce the memory consumption of the acoustic database. Voice conversion enables the intra-gender or cross-gender generation of new voices by using an existing high-quality voice.\n",
    "Usually, voice conversion is based on modification of spectral properties in accord with pitch manipulation. Warping functions in the frequency domain aiming at a reverse vocal tract length normalization (VTLN) is a simplified approach. Consequently, voice conversion itself generates a critical calculation complexity which contradicts the practical constraints of typical embedded and mobile applications.\n",
    "The authors propose a novel approach for voice conversion by reusing features of a common speech codec. Such a codec is already available in typical mobile applications and the resulting voice quality is widely accepted. The paper investigates the manipulation of the immittance spectral frequencies (ISF) provided by the Adaptive Multi Rate Wideband codec (AMR-WB). This algorithm has been integrated into the embedded speech synthesizer microDRESS.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-802"
  },
  "sundermann05_interspeech": {
   "authors": [
    [
     "David",
     "Sundermann"
    ],
    [
     "Guntram",
     "Strecha"
    ],
    [
     "Antonio",
     "Bonafonte"
    ],
    [
     "Harald",
     "Höge"
    ],
    [
     "Hermann",
     "Ney"
    ]
   ],
   "title": "Evaluation of VTLN-based voice conversion for embedded speech synthesis",
   "original": "i05_2593",
   "page_count": 4,
   "order": 803,
   "p1": "2593",
   "pn": "2596",
   "abstract": [
    "Recently, we demonstrated that vocal tract length normalization (VTLN) can be applied to voice conversion tasks. In particular, when the conversion algorithm is performed in time domain, this technique is very resource-efficient and, consequently, suitable for embedded applications. In this paper, we use VTLN-based voice conversion as a novel feature of a small footprint speech synthesizer running on mobile devices. The characteristics of this feature are investigated by means of extensive subjective tests.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-803"
  },
  "isogai05_interspeech": {
   "authors": [
    [
     "Juri",
     "Isogai"
    ],
    [
     "Junichi",
     "Yamagishi"
    ],
    [
     "Takao",
     "Kobayashi"
    ]
   ],
   "title": "Model adaptation and adaptive training using ESAT algorithm for HMM-based speech synthesis",
   "original": "i05_2597",
   "page_count": 4,
   "order": 804,
   "p1": "2597",
   "pn": "2600",
   "abstract": [
    "In speaker adaptation for HMM-based speech synthesis, model adaptation and adaptive training techniques play key roles. For reducing dependency on an initial model and adapting the model to wide-ranging target speakers, we propose speaker adaptation and adaptive training algorithms based on ESAT algorithm for HMM-based speech synthesis. The ESAT algorithm estimates contributing rate of several given initial models and combines them depending on likelihood of adaptation data for the target speaker. In this study, we incorporate the ESAT algorithm into a framework of hidden semi-Markov model (HSMM) to adapt both state output and duration distributions and convert both voice characteristics and prosodic features. From the results of subjective tests, we show that the ESAT algorithm lessen the dependence of synthetic speech quality on the initial model and has the potential ability for a wider range of the target speakers.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-804"
  },
  "fung05_interspeech": {
   "authors": [
    [
     "Tien-Ying",
     "Fung"
    ],
    [
     "Yuk-Chi",
     "Li"
    ],
    [
     "Eddie",
     "Sio"
    ],
    [
     "Icarus",
     "Lee"
    ],
    [
     "Helen",
     "Meng"
    ],
    [
     "P. C.",
     "Ching"
    ]
   ],
   "title": "Embedded Cantonese TTS for multi-device access to web content",
   "original": "i05_2601",
   "page_count": 4,
   "order": 805,
   "p1": "2601",
   "pn": "2604",
   "abstract": [
    "This paper describes the development of an embedded Cantonese text-to-speech synthesizer to enable multi-device access to Chinese Web content. Advancements in wireless communication is driving Web visitors from using desktop PCs to mobile handheld devices. Significant reduction in the form factors of the client devices tends to shift information delivery from the visual to the aural modality. This calls for synthesizers that can run on relatively stringent computational and storage resources of handheld devices. We report on the migration of our Cantonese synthesizer, CU VOCAL, from the desktop to the embedded platform. Migration preserves the support for speech synthesis markups (SSML), ensures code compatibility and lowers the storage requirements of the syllable inventory. Results from listening tests indicate no signification deterioration in synthesis quality of embedded CU VOCAL when compared to its desktop counterpart.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-805"
  },
  "schnell05_interspeech": {
   "authors": [
    [
     "Karl",
     "Schnell"
    ],
    [
     "Arild",
     "Lacroix"
    ]
   ],
   "title": "Model based analysis of a diphone database for improved unit concatenation",
   "original": "i05_2605",
   "page_count": 4,
   "order": 806,
   "p1": "2605",
   "pn": "2608",
   "abstract": [
    "One crucial point of concatenation approaches using diphones is to handle the discontinuities between the concatenated units. This problem is treated by a suitable analysis of the diphones for a parametric synthesis. The model of the parametric synthesis is the lossy tube model, which is an extension of the standard lattice filter considering frequency dependent vocal tract losses. The parameters of the tube model are estimated from diphones by an optimization algorithm. The discontinuities of the model parameters at the diphone joints decrease the quality of the synthesis results. To reduce the mismatch of the parameter configurations at the diphone boundaries a specific analysis of a diphone database is proposed, analyzing each diphone with respect to other diphones containing the phonemes of the respective diphone. The parameter mismatches at the diphone joints are reduced improving the concatenation results considerably.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-806"
  },
  "so05b_interspeech": {
   "authors": [
    [
     "Stephen",
     "So"
    ],
    [
     "Kuldip K.",
     "Paliwal"
    ]
   ],
   "title": "Switched split vector quantisation of line spectral frequencies for wideband speech coding",
   "original": "i05_2705",
   "page_count": 4,
   "order": 807,
   "p1": "2705",
   "pn": "2708",
   "abstract": [
    "In this paper, we investigate the use of the switched split vector quantiser (SSVQ) for coding short-term spectral envelope information for wideband speech coding. The SSVQ is the hybrid of a switch vector quantiser and split vector quantiser, which has been shown in previous studies to be more efficient, in terms of ratedistortion, as well as possessing low computational complexity, than the split vector quantiser. In our experiments, the SSVQ is used to quantise line spectral frequencies from the TIMIT database and its spectral distortion performance is compared with the split vector quantiser, the split-multistage vector quantiser (S-MSVQ) with MA predictor from the AMR-WB speech coder (ITU-T G.722.2), and PDF-optimised scalar quantisers. We show the SSVQ, which is a memoryless scheme, to achieve comparable spectral distortion to the S-MSVQ with MA predictor at 46 bits/frame. The five-part SSVQ requires 42 bits/frame and 17.7 kflops/frame for transparent coding, compared with 46 bits/frame and 40.96 kflops/frame for the five-part split vector quantiser.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-807"
  },
  "bao05_interspeech": {
   "authors": [
    [
     "Changchun",
     "Bao"
    ],
    [
     "Jason",
     "Lukasiak"
    ],
    [
     "Christian",
     "Ritz"
    ]
   ],
   "title": "A novel voicing cut-off determination for low bit-rate harmonic speech coding",
   "original": "i05_2709",
   "page_count": 4,
   "order": 808,
   "p1": "2709",
   "pn": "2712",
   "abstract": [
    "Generally, phonetic classification for low rate speech coding is restricted to either a simple binary voiced/unvoiced classification of entire speech frames, or alternatively, a more complicated estimation of the voicing for a set of frequency bands. A good compromise between these two techniques is estimation of a single cut-off frequency that separates the spectrum into voiced (below) and unvoiced (above) regions. Many existing cut-off frequency estimation methods use a fixed periodic spectrum to model voiced harmonics. However, due to pitch jitter, voiced harmonics do not always appear at regular intervals in the spectrum. In this paper a voicing cut-off estimation approach that combines speech energy, speech auto-correlation between frames and residual harmonic matching is proposed. Objective evaluation indicates that the algorithm is accurate and reliable. Subjective results obtained by embedding the algorithm in a low rate harmonic speech coder indicate that the technique is suitable for supporting high quality low rate speech synthesis. The proposed algorithm also requires relatively low complexity and introduces only a single frame of algorithmic delay.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-808"
  },
  "kruger05b_interspeech": {
   "authors": [
    [
     "Hauke",
     "Krüger"
    ],
    [
     "Peter",
     "Vary"
    ]
   ],
   "title": "A partial decorrelation scheme for improved predictive open loop quantization with noise shaping",
   "original": "i05_2713",
   "page_count": 4,
   "order": 809,
   "p1": "2713",
   "pn": "2716",
   "abstract": [
    "In this contribution a modified scheme for linear prediction analysis is presented which controls the degree of decorrelation by a parameter α. In consideration of this parameter the signal to noise ratio of a linear predictive coding scheme is investigated and finally maximized for open loop quantization. Also, the new parameter can be used to control the spectral shape of the quantization error in the decoder output. As result the signal to noise ratio of the reconstructed signal can be increased for open loop quantization compared to conventional prediction analysis, and at the same time the perceptual encoding quality benefits from a moderate spectral shaping of the quantization noise in the decoded signal.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-809"
  },
  "krishnan05b_interspeech": {
   "authors": [
    [
     "Venkatesh",
     "Krishnan"
    ],
    [
     "Thomas P.",
     "Barnwell III"
    ],
    [
     "David V.",
     "Anderson"
    ]
   ],
   "title": "Using dynamic codebook re-ordering to exploit inter-frame correlation in MELP coders",
   "original": "i05_2717",
   "page_count": 4,
   "order": 810,
   "p1": "2717",
   "pn": "2720",
   "abstract": [
    "Model based speech coders such as the mixed-excitation linear prediction (MELP) coder encode parameters of the autoregressive model for short-duration frames of the speech signal. Typically, parameters extracted from successive frames by the MELP coder exhibit strong correlation. Reduction in the transmitted data-rates can be achieved if the encoders for these parameters effectively exploit this inter-frame correlation. In this paper, we apply a procedure, called dynamic codebook re-ordering (DCR) to reduce the entropy in the distribution of the symbols generated by the vector quantization encoders used in coding the MELP parameters. The entropy reduction is achieved by exploiting the correlation between the vectors of MELP parameters derived from successive speech frames. The advantages of the DCR procedure over other techniques that exploit inter-frame correlation stem from the fact that it significantly reduces the data-rates without introducing any additional coding delays or increasing the distortion and it is simple and elegant.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-810"
  },
  "durey05_interspeech": {
   "authors": [
    [
     "Adriane Swalm",
     "Durey"
    ],
    [
     "Venkatesh",
     "Krishnan"
    ],
    [
     "Thomas P.",
     "Barnwell III"
    ]
   ],
   "title": "Enhanced speech coding based on phonetic class segmentation",
   "original": "i05_2721",
   "page_count": 4,
   "order": 811,
   "p1": "2721",
   "pn": "2724",
   "abstract": [
    "Given a baseline speech coder and speech with an available phonetic class segmentation, a number of potential enhancements to that coder become possible. While the quality of speech segmentation by phoneme and phonetic class is constantly improving, we use TIMIT to generate phonetic class segmentation as a basis for initial testing of these techniques. Using coders drawn from the MELP family, we explore specialized phonetic codebooks, phoneticallydriven superframing, and improved modeling of specific phonetic classes and the transitions between them. We compare the reconstructed speech from these enhancements against the base coder using the metrics of computational cost, transmission cost, and the quality of the reconstructed speech. In most cases, we find that segmentation-based coders can produce speech with quality comparable to that of MELP, using fewer transmitted bits and at no additional computational cost. With phonetic codebooks and transition modeling, CCR tests show these segmentation-based coders produce speech of better quality than is produced by MELP.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-811"
  },
  "ertan05_interspeech": {
   "authors": [
    [
     "Ali Erdem",
     "Ertan"
    ],
    [
     "Thomas P.",
     "Barnwell III"
    ]
   ],
   "title": "A pitch-synchronous pitch-cycle modification method for designing a hybrid i-MELP/waveform-matching speech coder",
   "original": "i05_2725",
   "page_count": 4,
   "order": 812,
   "p1": "2725",
   "pn": "2728",
   "abstract": [
    "In this paper, we introduce a new approach to solve encoding method switching problems in a parametric/waveform-matching hybrid coder. This method only requires modification of the waveform-matching coder's target signal. For this purpose, we introduce a new pitch-synchronous cycle length modification method using the constant pitch transformation (CPT) and a frequency domain zero-phase equalization filter. An experimental I-MELP/PCM coder is used to explore the largest achievable quality improvement over a MELP coder. Test results show that encoding transitions with waveform-encoding techniques improves the quality over a fully parametric coder.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-812"
  },
  "chang05_interspeech": {
   "authors": [
    [
     "Joon-Hyuk",
     "Chang"
    ],
    [
     "Jong-Won",
     "Shin"
    ],
    [
     "Seung Yeol",
     "Lee"
    ],
    [
     "Nam Soo",
     "Kim"
    ]
   ],
   "title": "A new structural preprocessor for low-bit rate speech coding",
   "original": "i05_2729",
   "page_count": 4,
   "order": 813,
   "p1": "2729",
   "pn": "2732",
   "abstract": [
    "In this paper, we apply a new structural approach to generalized analysis-by-synthesis (GAbS) for system identification as a preprocessor of a low-bit-rate speech coder. In our approach, the coder-decoder (CODEC) system is separately estimated and then applied to modify the current input signal. This is different from that originally proposed where the CODEC system is sequentially estimated and then applied to the next input signal. The proposed estimation scheme is compared to the conventional method in terms of the signal modification approach under the various noise data and in several SNR conditions, and shows better performance.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-813"
  },
  "falk05_interspeech": {
   "authors": [
    [
     "Tiago H.",
     "Falk"
    ],
    [
     "Wai-Yip",
     "Chan"
    ],
    [
     "Peter",
     "Kabal"
    ]
   ],
   "title": "An improved GMM-based voice quality predictor",
   "original": "i05_2733",
   "page_count": 4,
   "order": 814,
   "p1": "2733",
   "pn": "2736",
   "abstract": [
    "A voice quality prediction method based on Gaussian mixture models (GMMs) is improved by constructing a feature selection algorithm to provide the best GMM-based prediction quality. The proposed sequential selection algorithm performs N-survivor search, allowing for trading between design complexity and performance. Simulation shows that predictors designed using the proposed algorithm outperform two benchmark selection algorithms. Performance improvements over the ITU-T P.862 PESQ standard are also attained.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-814"
  },
  "erkelens05_interspeech": {
   "authors": [
    [
     "Jan",
     "Erkelens"
    ]
   ],
   "title": "High-quality memoryless subband coding of impulse responses at 22 bits per frame",
   "original": "i05_2737",
   "page_count": 4,
   "order": 815,
   "p1": "2737",
   "pn": "2740",
   "abstract": [
    "A method for efficient quantization of speech spectral models is proposed. The impulse response of the model is coded in two spectral subbands of unequal width. The distortion measure used is a squared error between impulse responses. The quality obtained at 22 bits per frame is as good as that of Split Vector Quantization of Line Spectral Frequencies at 24 bits per frame.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-815"
  },
  "chen05i_interspeech": {
   "authors": [
    [
     "Shi-Han",
     "Chen"
    ],
    [
     "Kuo-Guan",
     "Wu"
    ],
    [
     "Chih-Chung",
     "Kuo"
    ]
   ],
   "title": "A study of variable pulse allocation for MPE and CELP coders based on PESQ analysis",
   "original": "i05_2741",
   "page_count": 4,
   "order": 816,
   "p1": "2741",
   "pn": "2744",
   "abstract": [
    "A novel scheme of allocating variable pulses for each frame is proposed to reduce the bit-rate of MPE and CELP coders while maintaining the same speech quality. Since speech signal is not stationary, the required pulse number in a speech coder should be variable frame by frame. In this paper we tried to approximate the optimal pulse allocation by greedy search algorithm based on the criterion of perceptual disturbance value derived by PESQ analysis. In the experiments the proposed scheme was used to reduce the pulse numbers of two standard speech coders, G.723.1 and MPEG-4 CELP. The results show that the proposed scheme can achieve over 30% bit-rate reduction in fixed codebook (FCB) and about 20% in all for both coders while maintaining the same speech quality in both objective and subjective measure. We also designed several methods to accelerate the optimal search, which could largely reduce the execution time by 120 times in the best case.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-816"
  },
  "perezcordoba05_interspeech": {
   "authors": [
    [
     "José L.",
     "Pérez-Córdoba"
    ],
    [
     "Antonio M.",
     "Peinado"
    ],
    [
     "Angel M.",
     "Gómez"
    ],
    [
     "Antonio J.",
     "Rubio"
    ]
   ],
   "title": "Joint source-channel coding of LSP parameters for bursty channels",
   "original": "i05_2745",
   "page_count": 4,
   "order": 817,
   "p1": "2745",
   "pn": "2748",
   "abstract": [
    "This work present a joint source-channel technique based on Channel Optimized Vector Quantization (COVQ) for transmission over bursty channels applied to LSP parameters coding. The bursty channel is modeled as a Finite State Channel (FSC) with two states. We call Bursty COVQ (BCOVQ) to the resulting quantization technique. The case in which channel state information is only available at the receiver is considered. The application of BCOVQ to LSP (Line Spectrum Pair) parameters coding in wideband speech coding is presented. Our experimental results show that BCOVQ maintains a good performance through all tested channel conditions, while other reference techniques (traditional COVQ, Split Vector Quantization and Split Multi-Stage Vector Quantization) are more dependent on these conditions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-817"
  },
  "pfleger05_interspeech": {
   "authors": [
    [
     "Norbert",
     "Pfleger"
    ],
    [
     "Markus",
     "Löckelt"
    ]
   ],
   "title": "Synchronizing dialogue contributions of human users and virtual characters in a virtual reality environment",
   "original": "i05_2773",
   "page_count": 4,
   "order": 818,
   "p1": "2773",
   "pn": "2776",
   "abstract": [
    "Synchronizing user and system actions in a real-time virtual reality environment is a challenging task. Key components of a dialogue system like speech recognition, discourse processing, speech generation and synthesis all contribute significant delays to the response time. With human interlocutors, however, a continuous flow of conversation is important as any implausible gap may cause confusion with respect to floor management. In this paper, we describe some cases of how to make sensible use of information available at each processing step to be able to give early useful feedback even while processing.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-818"
  },
  "venkataraman05_interspeech": {
   "authors": [
    [
     "Anand",
     "Venkataraman"
    ],
    [
     "Yang",
     "Liu"
    ],
    [
     "Elizabeth",
     "Shriberg"
    ],
    [
     "Andreas",
     "Stolcke"
    ]
   ],
   "title": "Does active learning help automatic dialog act tagging in meeting data?",
   "original": "i05_2777",
   "page_count": 4,
   "order": 819,
   "p1": "2777",
   "pn": "2780",
   "abstract": [
    "Knowledge of Dialog Acts (DAs) is important for the automatic understanding and summarization of meetings. Current approaches rely on a lot of hand labeled data to train automatic taggers. One approach that has been successful in reducing the amount of training data in other areas of NLP is active learning. We ask if active learning with lexical cues can help for this task and this domain. To better address this question, we explore active learning for two different types of DA models - hidden Markov models (HMMs) and maximum entropy (maxent).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-819"
  },
  "bohus05_interspeech": {
   "authors": [
    [
     "Dan",
     "Bohus"
    ],
    [
     "Alexander I.",
     "Rudnicky"
    ]
   ],
   "title": "A principled approach for rejection threshold optimization in spoken dialog systems",
   "original": "i05_2781",
   "page_count": 4,
   "order": 820,
   "p1": "2781",
   "pn": "2784",
   "abstract": [
    "A common design pattern in spoken dialog systems is to reject an input when the recognition confidence score falls below a preset rejection threshold. However, this introduces a potentially non-optimal tradeoff between various types of errors such as misunderstandings and false rejections. In this paper, we propose a data-driven method for determining the relative costs of these errors, and then use these costs to optimize state-specific rejection thresholds. We illustrate the use of this approach with data from a spoken dialog system that handles conference room reservations. The results obtained confirm our intuitions about the costs of the errors, and are consistent with anecdotal evidence gathered throughout the use of the system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-820"
  },
  "perezpinarlopez05_interspeech": {
   "authors": [
    [
     "David",
     "Pérez-Piñar López"
    ],
    [
     "Carmen",
     "García Mateo"
    ]
   ],
   "title": "Application of confidence measures for dialogue systems through the use of parallel speech recognizers",
   "original": "i05_2785",
   "page_count": 4,
   "order": 821,
   "p1": "2785",
   "pn": "2788",
   "abstract": [
    "To assess the correctness of a recognizer output in any instance of a dialogue is a complex task that has been studied thoroughly during the past decade. Its importance relays on the need for robust dialogue systems, capable of dealing with difficulties inherent to human-machine communications: user errors and corrections, speech recognizer errors, error recovery techniques, etc.\n",
    "In this paper, we present a novel approach to the problem of deciding what the user has said. We use confidence measures derived from low level knowledge sources (acoustic and linguistic information) and generated in parallel from several topic-adapted speech recognizers. Each recognizer is aimed to the recognition of a particular topic, and confidence measures are compared through the use of a classifier that lead to a most probable solution.\n",
    "This approach shows to be specially suited for difficult topics, such as proper names or confirmations, which are highly meaningful for error correction tasks. These topics present high error rates when using an application-wide speech recognizer, but recognition correction is greatly enhanced through the use of parallel recognizers. Moreover, the use of topic-adapted recognizers seems to help also in the identification of the user intention and in the detection of out-of-application utterances.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-821"
  },
  "rosset05_interspeech": {
   "authors": [
    [
     "Sophie",
     "Rosset"
    ],
    [
     "Delphine",
     "Tribout"
    ]
   ],
   "title": "Multi-level information and automatic dialog acts detection in human-human spoken dialogs",
   "original": "i05_2789",
   "page_count": 4,
   "order": 822,
   "p1": "2789",
   "pn": "2792",
   "abstract": [
    "This paper reports on our experience in annotating and the automatically detecting dialog acts in human-human spoken dialog. Our work is based on three hypotheses: first, the dialog act succession is strongly constrained; second, initial word and semantic class of word are more important than the exact word in identifying the dialog act; third, information is encoded in specific entities. We also used historical information in order to account for the dialogical structure. A memory based learning approach is used to detect dialog acts. Experiments have been conducted using different kind of information levels. In order to verify our hypotheses, the model trained on a French corpus was tested on an English corpus for a similar task and on a French corpus from a different domain. A correct dialog act detection rate of about 87% is obtained for the same domain/ language condition and 80% for the cross-language and cross-domain conditions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-822"
  },
  "akker05_interspeech": {
   "authors": [
    [
     "Rieks op den",
     "Akker"
    ],
    [
     "Harry",
     "Bunt"
    ],
    [
     "Simon",
     "Keizer"
    ],
    [
     "Boris van",
     "Schooten"
    ]
   ],
   "title": "From question answering to spoken dialogue: towards an information search assistant for interactive multimodal information extraction",
   "original": "i05_2793",
   "page_count": 4,
   "order": 823,
   "p1": "2793",
   "pn": "2796",
   "abstract": [
    "This paper gives an overview of issues related to extending simple question answering (QA) with dialogue capabilities, when designing a multimodal interactive information extraction system for a large, though restricted, domain.\n",
    "We present the way in which these issues are approached in the IMIX program. The IMIX demonstrator system, under development in this program, may be considered the most difficult case of QA, answering non-factoid questions in a large domain, and accepting speech input as well. We describe our approach to the addition of dialogue capabilities to this system.\n",
    "We will look at QA from a dialogue system perspective and from a HCI perspective, and consider the consequences of our choice of interaction metaphor, the information search assistant'.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-823"
  },
  "wesseling05_interspeech": {
   "authors": [
    [
     "Wieneke",
     "Wesseling"
    ],
    [
     "Rob J. J. H. van",
     "Son"
    ]
   ],
   "title": "Timing of experimentally elicited minimal responses as quantitative evidence for the use of intonation in projecting TRPs",
   "original": "i05_3389",
   "page_count": 4,
   "order": 824,
   "p1": "3389",
   "pn": "3392",
   "abstract": [
    "In an RT experiment, subjects were asked to respond with minimal responses to prerecorded dialogs and a manipulated version of these dialogs that contained only intonation and pause information. Response delays and, especially, variances were higher to the impoverished, intonation only, stimuli than to the original recordings. It was also found that intonation only utterances ending in a mid-frequency pitch induced significantly longer response delays than utterances ending in a low pitch. These results are interpreted as evidence that just the intonation and pauses of a conversation already contain sufficient information to project end-of-utterance TRPs. However this information is measurably impoverished with respect to full speech to an extent that increases the \"processing\" time by 10%. Our subjects seemed to fall back to reacting to pauses when presented with intonation only utterances ending in a mid-frequency tone. This suggests that, in contrast to low or high end-tones, intonation contours that end in a mid-frequency tone might not contain any useful information for predicting end-of-utterance TRPs.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-824"
  },
  "yamada05b_interspeech": {
   "authors": [
    [
     "Shinya",
     "Yamada"
    ],
    [
     "Toshihiko",
     "Itoh"
    ],
    [
     "Kenji",
     "Araki"
    ]
   ],
   "title": "Linguistic and acoustic features depending on different situations - the experiments considering speech recognition rate",
   "original": "i05_3393",
   "page_count": 4,
   "order": 825,
   "p1": "3393",
   "pn": "3396",
   "abstract": [
    "This paper presents the characteristic differences of linguistic and acoustic features observed in different spoken dialogue situations and with different dialogue partners: human-human vs. humanmachine interactions. We compare the linguistic and acoustic features of the user's speech to a spoken dialogue system and to a human operator in several goal setting and destination database searching tasks for a car navigation system. It has been pointed out that speech-based interaction has the potential to distract the driver's attention and degrade safety. On the other hand, it is not clear enough whether different dialogue situations and different dialogue partners cause any differences of linguistic or acoustic features on one's utterances in a speech interface system. Additionally, research about influence of speech recognition rate is not enough either. We collected a set of spoken dialogues by 12 subject speakers for each experiment under several dialogue situations. For a car driving situation, we prepared a virtual driving simulation system. We also prepared two patterns where we have two dialogue partners with different speech recognition rate (100% and about 80%). We analyzed the characteristic differences of user utterances caused by different dialogue situations and with different dialogue partners in two above mentioned patterns.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-825"
  },
  "buhler05_interspeech": {
   "authors": [
    [
     "Dirk",
     "Bühler"
    ],
    [
     "Stefan W.",
     "Hamerich"
    ]
   ],
   "title": "Towards voiceXML compilation for portable embedded applications in ubiquitous environments",
   "original": "i05_3397",
   "page_count": 4,
   "order": 826,
   "p1": "3397",
   "pn": "3400",
   "abstract": [
    "In this paper we present an approach to embedding VoiceXML applications by an off-line compilation scheme. Our primary motivation is that while VoiceXML is an established standard for voice applications, the complexity and resource requirements of VoiceXML interpretation have so far limited its spread to application areas other than telephony-based services. In many contexts, such as ubiquitous computing, being able to embed voice applications into a larger framework is more important than the dynamic creation of VoiceXML documents that makes the processing particularly resource demanding. For these environments, we propose off-line VoiceXML compilation into ECMAScript as a solution for reducing the resource demands, thus making VoiceXML a viable option for the many small devices that replace the desktop in ubiquitous computing. We also describe our portable Java-based run-time platform that makes it possible to run a complete spoken language dialogue system on any computer supporting Java.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-826"
  },
  "strangert05_interspeech": {
   "authors": [
    [
     "Eva",
     "Strangert"
    ]
   ],
   "title": "Prosody in public speech: analyses of a news announcement and a Political interview",
   "original": "i05_3401",
   "page_count": 4,
   "order": 827,
   "p1": "3401",
   "pn": "3404",
   "abstract": [
    "The study concerns informative and argumentative functions of prosody in the public domain. Analyses were based on speech samples from a professional news announcer and a well-known politician, each with a reputation as a highly skilled speaker. Results show both speakers to use prosody in a very efficient way. In the news reading, prosodic variation mainly serves informative demands, primarily the focusing of important information. The interview allows for a greater variety of expressions and the skill of the speaker manifests itself in her use of a wide repertoire of argumentative and emotionally colored expressive acts conveyed by prosody. The goal of this research is to shed light on what makes \"a good speaker\" and also how speech can be optimally adjusted to the demands of the situation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-827"
  },
  "nanavati05_interspeech": {
   "authors": [
    [
     "Amit Anil",
     "Nanavati"
    ],
    [
     "Nitendra",
     "Rajput"
    ]
   ],
   "title": "Characterising dialogue call-flows for pervasive environments",
   "original": "i05_3405",
   "page_count": 4,
   "order": 828,
   "p1": "3405",
   "pn": "3408",
   "abstract": [
    "Algorithm design and analysis rapidly attained maturity with the introduction of time and space complexity measures. Dialogue call-flow design and analysis for pervasive devices is a relatively nascent art, complicated by the variation in resource constraints. We investigate the problem of dialogue call-flow characterisation for pervasive devices, with the objective of defining complexity measures for dialogue call-flows.\n",
    "A dialogue call-flow executing on a pervasive device is an interplay among the device, the human and the speech system, and can therefore be completely characterised by the {resource,usability,technology} triple. Typical examples of resource are memory and energy; usability is indicated in the number of questions; and technology is exemplified in the accuracy of the speech recognition system. These are characteristics of the call-flow, but the call-flow characterisation itself is {device,human,speech system}-independent.\n",
    "We instantiate {r,u,t} with {memory,questions,accuracy} to introduce the {m,q,a}-characterisation of dialogue call-flows. We select m, q, a metrics to define various {m,q,a}-complexity measures for a call-flow. Every call-flow thus has a complexity measure associated with it - a feature indispensable for the systematic analysis and design of dialogue call-flows.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-828"
  },
  "faruquie05_interspeech": {
   "authors": [
    [
     "Tanveer",
     "Faruquie"
    ],
    [
     "Pankaj",
     "Kankar"
    ],
    [
     "Nitendra",
     "Rajput"
    ],
    [
     "Abhishek",
     "Verma"
    ]
   ],
   "title": "An architecture for pluggable disambiguation mechanism for RDC based voice applications",
   "original": "i05_3409",
   "page_count": 4,
   "order": 829,
   "p1": "3409",
   "pn": "3412",
   "abstract": [
    "Building speech-based conversational systems involves the development of several speech specific control mechanisms such as validation, confirmation, disambiguation in addition to the actual application call-flow. We present an architecture for pluggable disambiguation mechanism for speech based conversational systems. The architecture provides a mechanism to decouple the disambiguation from the voice application. Several disambiguation strategies can be designed to disambiguate a user input. These strategies can be applied to the user input in a seamless manner. The disambiguated value from one component can be passed on to another component for further disambiguation. We implement the architecture by using it in the Reusable Dialog Component framework. Several illustrative examples are presented to highlight the effectiveness of having a pluggable disambiguation mechanism for voice applications.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-829"
  },
  "rajput05_interspeech": {
   "authors": [
    [
     "Nitendra",
     "Rajput"
    ],
    [
     "Amit Anil",
     "Nanavati"
    ],
    [
     "Abhishek",
     "Kumar"
    ],
    [
     "Neeraj",
     "Chaudhary"
    ]
   ],
   "title": "Adapting dialog call-flows for pervasive devices",
   "original": "i05_3413",
   "page_count": 4,
   "order": 830,
   "p1": "3413",
   "pn": "3416",
   "abstract": [
    "There is an increasing variety of pervasive devices in use today. More and more applications are being supported on such devices. This requires device-specific application adaptation. We address the problem of speech application adaptation by dialog call-flow reorganisation for pervasive devices with different memory constraints. Given a dialog call-flow C and device memory size M, we present deterministic algorithms that alter C to create Cm that fits M by increasing the number of questions and splitting the underlying grammar. We can split a grammar by exposing the intermediate non-terminals in the grammar. The following observation forms the cornerstone of this paper: An and-grammar can be split \"horizontally\", and an or-grammar can be split \"vertically\" into its components to reduce the memory requirement of the call-flow, at the expense of increasing the number of prompts (questions). We present algorithm G-split, explain its implementation with example call-flows authored in VXML containing SRGS grammars.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-830"
  },
  "krum05_interspeech": {
   "authors": [
    [
     "Ulf",
     "Krum"
    ],
    [
     "Hartwig",
     "Holzapfel"
    ],
    [
     "Alex",
     "Waibel"
    ]
   ],
   "title": "Clarification questions to improve dialogue flow and speech recognition in spoken dialogue systems",
   "original": "i05_3417",
   "page_count": 4,
   "order": 831,
   "p1": "3417",
   "pn": "3420",
   "abstract": [
    "Within human-machine conversation, clarification is vital and may consist of various forms, as it is may by due to many different effects on different levels of communication. In this paper, we present a strategy for detecting situations where a need for clarification exists in a natural spoken dialogue system. We define rule sets which enable us, via an anomaly analysis, to detect these critical situations. Through the use of such rule sets, we show that it is possible to enhance the strategy in such a manner that more different situations are detected. In a user test, we evaluate the success of the strategy and show that strategies with explicit clarification improve the naturalness of human-machine interaction.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-831"
  },
  "fernandez05c_interspeech": {
   "authors": [
    [
     "Fernando",
     "Fernández"
    ],
    [
     "Javier",
     "Ferreiros"
    ],
    [
     "Valentín",
     "Sama"
    ],
    [
     "Juan Manuel",
     "Montero"
    ],
    [
     "Rubén San",
     "Segundo"
    ],
    [
     "Javier",
     "Macías-Guarasa"
    ],
    [
     "Rafael",
     "García"
    ]
   ],
   "title": "Speech interface for controlling an hi-fi audio system based on a Bayesian belief networks approach for dialog modeling",
   "original": "i05_3421",
   "page_count": 4,
   "order": 832,
   "p1": "3421",
   "pn": "3424",
   "abstract": [
    "This paper presents the development of a speech interface for controlling a high fidelity system from natural language sentences. A Bayesian Belief Network approach is proposed for dialog modeling. This solution is applied to infer the user's goals corresponding to the processed utterances. Subsequently, from the inferred goals, missing or spurious concepts are automatically detected. This is used to drive the dialog prompting for missing concepts and clarifying for spurious concepts allowing more flexible and natural dialogs. A dialog strategy which makes use of the dialog history and the system's state is also presented.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-832"
  },
  "pearce05_interspeech": {
   "authors": [
    [
     "David",
     "Pearce"
    ],
    [
     "Jonathan",
     "Engelsma"
    ],
    [
     "James",
     "Ferrans"
    ],
    [
     "John",
     "Johnson"
    ]
   ],
   "title": "An architecture for seamless access to distributed multimodal services",
   "original": "i05_2845",
   "page_count": 4,
   "order": 833,
   "p1": "2845",
   "pn": "2848",
   "abstract": [
    "In this paper we present a standards-based architecture that enables ubiquitous access to distributed speech and multimodal services. Motorola's \"Seamless Mobility\" initiatives are focused on giving people continuity of services regardless of context, location, device, or type of network connectivity. Speech technology is a key aspect of seamless mobility: not only does it make it easier to enter information on small devices, but it allows devices to be used in contexts where eyes and hands cannot be used. But the visual modality comprised of keypad input and display output is also vital: speech input can't be used in very noisy environments or where privacy is an issue, and speech output is difficult to remember. Multimodal systems combining visual, speech, and other modalities are therefore crucial for seamless mobility. The architecture depends on standard VoIP protocols such as SIP and RTP. It also uses standard web languages for voice dialogs (VoiceXML) and visual dialogs (XHTML, J2ME). And to provide very high performance speech recognition we use the Distributed Speech Recognition (DSR) standards. The result is a responsive system that places minimal demands on the device and maximally leverages existing mobile content ecosystems.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-833"
  },
  "tan05_interspeech": {
   "authors": [
    [
     "Zheng-Hua",
     "Tan"
    ],
    [
     "Paul",
     "Dalsgaard"
    ],
    [
     "Børge",
     "Lindberg"
    ],
    [
     "Haitian",
     "Xu"
    ]
   ],
   "title": "Robust speech recognition in ubiquitous networking and context-aware computing",
   "original": "i05_2849",
   "page_count": 4,
   "order": 834,
   "p1": "2849",
   "pn": "2852",
   "abstract": [
    "The introduction of ubiquitous computing and networking has fostered automatic speech recognition (ASR) systems of a distributed nature. The major challenge in deploying ubiquitous ASR is that the operating environments may change rapidly leaving the ASR system very vulnerable. This paper deals with the concept of making ASR systems context-aware with the aim of improving robustness against varying conditions such as dynamic network constraints and environmental noise. To fully benefit from a variety of networks with different characteristics, a number of distributed speech recognition (DSR) schemes are presented each of which is applicable to a specific network context. To increase ASR system robustness in varying environmental noise context, a multiple-model framework for noise-robust ASR is presented where multiple HMM model sets are trained, one for each noise type and each specific signal-to-noise ratio (SNR) that characterise the noise context. Experimental results show that the performance of ASR is largely improved by exploiting the context information.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-834"
  },
  "ion05_interspeech": {
   "authors": [
    [
     "Valentin",
     "Ion"
    ],
    [
     "Reinhold",
     "Haeb-Umbach"
    ]
   ],
   "title": "Unified probabilistic approach to error concealment for distributed speech recognition",
   "original": "i05_2853",
   "page_count": 4,
   "order": 835,
   "p1": "2853",
   "pn": "2856",
   "abstract": [
    "The transmission errors in a wireless or packet oriented network may dramatically decrease the performance of a distributed speech recognition (DSR) system. Error concealment has been shown to be an effective way to maintain an acceptable word error rate when dealing with error prone communication channels. In this paper we propose an extension of our previously introduced soft features approach for the case that the soft-output of the channel decoder is not available at the server side of the DSR system. We found a simple method to estimate bit reliability information which still gives good speech recognition results. It is shown that some other error concealment schemes turn out to be special cases of the method proposed here.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-835"
  },
  "james05_interspeech": {
   "authors": [
    [
     "Alastair",
     "James"
    ],
    [
     "Ben",
     "Milner"
    ]
   ],
   "title": "Combining packet loss compensation methods for robust distributed speech recognition",
   "original": "i05_2857",
   "page_count": 4,
   "order": 836,
   "p1": "2857",
   "pn": "2860",
   "abstract": [
    "This paper presents a combined packet loss compensation system for distributed speech recognition (DSR). Compensation is applied at three stages within the DSR process beginning with interleaving on the terminal device to reduce burst lengths in the received feature vector stream. On the receiver side estimation of missing vectors is applied to reconstruct the feature vector stream prior to recognition. Finally, the decoding process of the recogniser is modified to take into account the varying reliability of these estimated feature vectors. Experiments performed on both the Aurora connected digits task and the WSJCAM0 large vocabulary task show substantial gains in recognition accuracy across a range of packet loss conditions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-836"
  },
  "skogstad05_interspeech": {
   "authors": [
    [
     "Trond",
     "Skogstad"
    ],
    [
     "Torbjørn",
     "Svendsen"
    ]
   ],
   "title": "Distributed ASR using speech coder data for efficient feature vector representation",
   "original": "i05_2861",
   "page_count": 4,
   "order": 837,
   "p1": "2861",
   "pn": "2864",
   "abstract": [
    "This paper proposes an alternative approach to distributed speech recognition in scenarios where both reliable feature vectors and the reconstruction of the speech signal are required. By transmitting the difference between speech coded information and the desired feature vectors, this system achieves both excellent quality speech reconstruction and ASR recognition performance. Experiments show that a transparent recognition rate is achieved with as little as 0.6 kbps of additional information supplementing the AMR speech coder operating at 4.75 kbps. The total rate is comparable to the ETSI 202 211 extended front-end standard.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-837"
  },
  "furui05_interspeech": {
   "authors": [
    [
     "Sadaoki",
     "Furui"
    ],
    [
     "Tomohisa",
     "Ichiba"
    ],
    [
     "Takahiro",
     "Shinozaki"
    ],
    [
     "Edward W. D.",
     "Whittaker"
    ],
    [
     "Koji",
     "Iwano"
    ]
   ],
   "title": "Cluster-based modeling for ubiquitous speech recognition",
   "original": "i05_2865",
   "page_count": 4,
   "order": 838,
   "p1": "2865",
   "pn": "2868",
   "abstract": [
    "In order to realize speech recognition systems that can achieve high recognition accuracy for ubiquitous speech, it is crucial to make the systems flexible enough to cope with a large variability of spontaneous speech. This paper investigates two speech recognition methods that can adapt to speech variation using a large number of models trained based on clustering techniques; one automatically builds a model adapted to input speech using recognition hypotheses and clustered models, and the other directly uses clustered models in parallel. Both methods have been confirmed to be effective by evaluation experiments using presentation speech. Although the latter method needs a large amount of computation, it has an advantage in that it can be applied to online recognition, since it does not need recognition hypotheses. The former method can also be applied to online recognition, if the text of proceedings for the presentation can be used in place of recognition hypotheses.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-838"
  },
  "takahashi05c_interspeech": {
   "authors": [
    [
     "Akira",
     "Takahashi"
    ],
    [
     "Atsuko",
     "Kurashima"
    ],
    [
     "Chiharu",
     "Morioka"
    ],
    [
     "Hideaki",
     "Yoshino"
    ]
   ],
   "title": "Objective quality assessment of wideband speech by an extension of ITU-t recommendation p.862",
   "original": "i05_3153",
   "page_count": 4,
   "order": 839,
   "p1": "3153",
   "pn": "3156",
   "abstract": [
    "We discuss objective estimation methodologies for subjective quality of wideband speech. First, the wideband extension of the current ITU-T Recommendation P.862, which provides a means for objectively estimating subjective quality, is evaluated from the viewpoint of assessing the effects of band limitation. Then, the validity of the objective assessment is discussed based on the subjective quality database, which includes practical degradations assumed in VoIP applications.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-839"
  },
  "werner05_interspeech": {
   "authors": [
    [
     "Marc",
     "Werner"
    ],
    [
     "Peter",
     "Vary"
    ]
   ],
   "title": "Quality control for UMTS-AMR speech channels",
   "original": "i05_3157",
   "page_count": 4,
   "order": 840,
   "p1": "3157",
   "pn": "3160",
   "abstract": [
    "In UMTS speech transmissions, the Adaptive Multi-Rate (AMR) speech codec is employed to allow for a dynamic assignment of data rates to individual users. The control of AMR modes is based on quality measurements of the transmission channel and aims at the maximization of speech quality by selecting the mode which is best suited to the current interference situation. In contrast to GSM, a reduction of AMR modes also leads to an increased cell capacity in UMTS. In this paper, a decentralized method for AMR mode switching is presented which is based on individual softbit measurements at the Viterbi channel decoder and optimizes the speech quality for a wide range of channel conditions. This method was developed by optimizing the AMR mode switching thresholds with respect to PESQ speech quality scores. The second part of the paper describes a new instrumental non-intrusive method for speech quality measurement by the evaluation of UMTS transmission parameters for application to UMTS-AMR speech transmissions. High correlations with the reference PESQ scores were observed.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-840"
  },
  "chen05j_interspeech": {
   "authors": [
    [
     "Wei",
     "Chen"
    ],
    [
     "Peter",
     "Kabal"
    ],
    [
     "Turaj Z.",
     "Shabestary"
    ]
   ],
   "title": "Perceptual postfilter estimation for low bit rate speech coders using Gaussian mixture models",
   "original": "i05_3161",
   "page_count": 4,
   "order": 841,
   "p1": "3161",
   "pn": "3164",
   "abstract": [
    "A novel perceptual postfilter is introduced. For each frame, the filter gains, z, are estimated given a vector, y, of the quantized LSFs and the long-term prediction gain of the corresponding frame. The proposed perceptual postfilter is derived from an optimal MMSE estimator, i.e. the estimated gain vector is z = E{z|y}. The MMSE estimator is based on the conditional pdf of z given y, which is computed from the joint pdf modelled by a GMM. The proposed perceptual postfilter improves the speech naturalness comparing with the conventional adaptive postfilter, while maintaining the property of being an \"add-on\" postfilter without modification to the current encoder.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-841"
  },
  "fujita05_interspeech": {
   "authors": [
    [
     "Kengo",
     "Fujita"
    ],
    [
     "Tsuneo",
     "Kato"
    ],
    [
     "Hideaki",
     "Yamada"
    ],
    [
     "Hisashi",
     "Kawai"
    ]
   ],
   "title": "SNR-dependent background noise compensation of PESQ values for cellular phone speech",
   "original": "i05_3165",
   "page_count": 4,
   "order": 842,
   "p1": "3165",
   "pn": "",
   "abstract": [
    "To evaluate the speech quality of actual cellular phone systems with an objective assessment, PESQ values were compared with MOS values for speech with background noises via four cellular phone systems used in Japan. As PESQ value errors were observed to be SNR-dependent, two SNR-dependent background noise compensation methods for PESQ values are proposed. Applying the compensation methods to the speech for four cellular phone systems, the RMSEs between MOS and compensated PESQ values were reduced to less than half of the original RMSEs for all four cellular phone systems. They were equal to the level of RMSE of MOS values.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-842"
  },
  "lee05e_interspeech": {
   "authors": [
    [
     "Gil Ho",
     "Lee"
    ],
    [
     "Jae Sam",
     "Yoon"
    ],
    [
     "Hong Kook",
     "Kim"
    ]
   ],
   "title": "A MFCC-based CELP speech coder for server-based speech recognition in network environments",
   "original": "i05_3169",
   "page_count": 4,
   "order": 843,
   "p1": "3169",
   "pn": "3172",
   "abstract": [
    "Existing standard speech coders can provide speech communication of high quality while they degrade the performance of speech recognition systems that use the reconstructed speech by the coders. The main cause of the degradation is that the spectral envelope parameters in speech coding are optimized to speech quality rather than to the performance of speech recognition. For example, mel-frequency cepstral coefficient (MFCC) is generally known to provide better speech recognition performance than linear prediction coefficient (LPC) that is a typical parameter set in speech coding. In this paper, we propose a speech coder using MFCC instead of LPC to improve the performance of a server-based speech recognition system in network environments. However, the main drawback of using MFCC is to develop the efficient MFCC quantization with a low-bit rate. First, we explore the interframe correlation of MFCCs, which results in the predictive quantization of MFCC. Second, a safety-net scheme is proposed to make the MFCC-based speech coder robust to channel error. As a result, we propose a 8.7 kbps MFCC-based CELP coder. It is shown from a PESQ test that the proposed speech coder has a comparable speech quality to 8 kbps G.729 while it is expected that the performance of speech recognition using the proposed speech coder is better than that using G.729.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-843"
  },
  "grancharov05_interspeech": {
   "authors": [
    [
     "Volodya",
     "Grancharov"
    ],
    [
     "Jonas",
     "Samuelsson"
    ],
    [
     "W. Bastiaan",
     "Kleijn"
    ]
   ],
   "title": "Distortion measures for vector quantization of noisy spectrum",
   "original": "i05_3173",
   "page_count": 4,
   "order": 844,
   "p1": "3173",
   "pn": "3176",
   "abstract": [
    "In this paper we address the problem of vector quantization of speech in a noisy environment. We show that the performance of a vector quantization system can be improved by adapting the distortion measure to the changing environmental conditions. The proposed method emphasizes the distortion in spectral regions where the speech signal dominates. The method functions well even when conventional pre-processor methods fail because the noise statistics cannot be estimated reliably from speech pauses (as, e.g., in tandeming operations). Objective tests confirm that the use of environmentally adaptive measures significantly improves estimation accuracy in noisy speech, while preserving the quality in the case of clean input.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-844"
  },
  "mokhtari05_interspeech": {
   "authors": [
    [
     "Parham",
     "Mokhtari"
    ],
    [
     "Tatsuya",
     "Kitamura"
    ],
    [
     "Hironori",
     "Takemoto"
    ],
    [
     "Kiyoshi",
     "Honda"
    ]
   ],
   "title": "Vocal tract area function inversion by linear regression of cepstrum",
   "original": "i05_3201",
   "page_count": 4,
   "order": 845,
   "p1": "3201",
   "pn": "3204",
   "abstract": [
    "Vocal tract data from 3D cine-MRI are used together with synchronised acoustics to evaluate a linear regression model for inversion. The first two principal components of vocalic area functions are predicted with correlations 0.99 and 0.97 respectively, from 24 FFT-cepstra measured in the frequency band 0-4 kHz. This best regression model together with the two component representation yields mean absolute errors of 0.37 cm2 in section area and 0.15 cm in vocal tract length.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-845"
  },
  "engwall05b_interspeech": {
   "authors": [
    [
     "Olov",
     "Engwall"
    ]
   ],
   "title": "Introducing visual cues in acoustic-to-articulatory inversion",
   "original": "i05_3205",
   "page_count": 4,
   "order": 846,
   "p1": "3205",
   "pn": "3208",
   "abstract": [
    "The contribution of facial measures in a statistical acoustic-toarticulatory inversion has been investigated. The tongue contour was estimated using a linear estimation from either acoustics or acoustics and facial measures. Measures of the lateral movement of lip corners and the vertical movement of the upper and lower lip and the jaw gave a substantial improvement over the audio-only case. It was further found that adding the corresponding articulatory measures that could be extracted from a profile view of the face; i.e. the protrusion of the lips, lip corners and the jaw, did not give any additional improvement of the inversion result. The present study hence suggests that audiovisual-to-articulatory inversion can as well be performed using front view monovision of the face, rather than stereovision of both the front and profile view.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-846"
  },
  "sorokin05_interspeech": {
   "authors": [
    [
     "Viktor N.",
     "Sorokin"
    ],
    [
     "A. S.",
     "Leonov"
    ],
    [
     "I. S.",
     "Makarov"
    ],
    [
     "A. I.",
     "Tsyplikhin"
    ]
   ],
   "title": "Speech inversion and re-synthesis",
   "original": "i05_3209",
   "page_count": 4,
   "order": 847,
   "p1": "3209",
   "pn": "3212",
   "abstract": [
    "Inverse problems with respect to parameters of the articulatory model are solved for all types of sounds: vowels, semi-vowels, nasals, stops and fricatives in various contexts. Acoustical parameters of the speech signal and trajectories of some reference points inside the vocal tract serve as input data. 3.7%, 3.8% and 2.6% average approximation error for the first three formants, 8.5% for the specific frequencies of fricative spectra, 2.8% for the coordinates of reference points for all kinds of phonemes are obtained when both - acoustic and articulatory data are used. 1.8%, 1.6%, and 1.1% error for the first three formant frequencies, and 6% for the coordinates of reference points are obtained when only acoustic data are used. Original and re-synthesized utterances are found to be very similar in appearance, according to subjective assessment.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-847"
  },
  "huckvale05_interspeech": {
   "authors": [
    [
     "Mark",
     "Huckvale"
    ],
    [
     "Ian",
     "Howard"
    ]
   ],
   "title": "Teaching a vocal tract simulation to imitate stop consonants",
   "original": "i05_3213",
   "page_count": 4,
   "order": 848,
   "p1": "3213",
   "pn": "3216",
   "abstract": [
    "The imitation of spoken stop consonants by an articulatory synthesizer using only general learning principles addresses significant issues in speech inversion and speech acquisition. Stop consonants are relatively large, complex acoustic events resulting from discrete articulations, so inversion based on the use of small time windows or based on the minimisation of average articulatory error across multiple places of articulation will not provide a satisfactory solution. This paper explores the effect of variation in inversion window size and the use of smoothing constraints on the quality of imitation of the stops [b], [d] and [g]. However good results are only obtained when inversion is supplemented by a phonetic labelling performed over a large time window. This source of additional phonetic information allows inversion to exploit different discrete gestures for the different places of articulation. The results demonstrate the importance of a phonological layer of perceptual analysis prior to imitation and speech acquisition.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-848"
  },
  "potard05_interspeech": {
   "authors": [
    [
     "Blaise",
     "Potard"
    ],
    [
     "Yves",
     "Laprie"
    ]
   ],
   "title": "Using phonetic constraints in acoustic-to-articulatory inversion",
   "original": "i05_3217",
   "page_count": 4,
   "order": 849,
   "p1": "3217",
   "pn": "3220",
   "abstract": [
    "The goal of this work is to recover articulatory information from the speech signal by acoustic-to-articulatory inversion. One of the main difficulties with inversion is that the problem is underdetermined and inversion methods generally offer no guarantee on the phonetical realism of the inverse solutions. A way to address this issue is to use additional phonetic constraints.\n",
    "Knowledge of the phonetic characteristics of French vowels enable the derivation of reasonable articulatory domains in the space of Maeda parameters: given the formants frequencies (F1,F2,F3) of a speech sample, and thus the vowel identity, an \"ideal\" articulatory domain can be derived. The space of formants frequencies is partitioned into vowels, using either speaker-specific data or generic information on formants. Then, to each articulatory vector can be associated a phonetic score varying with the distance to the \"ideal domain\" associated with the corresponding vowel.\n",
    "Inversion experiments were conducted on isolated vowels and vowel-to-vowel transitions. Articulatory parameters were compared with those obtained without using these constraints and those measured from X-ray data.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-849"
  },
  "toutios05_interspeech": {
   "authors": [
    [
     "Asterios",
     "Toutios"
    ],
    [
     "Konstantinos",
     "Margaritis"
    ]
   ],
   "title": "A support vector approach to the acoustic-to-articulatory mapping",
   "original": "i05_3221",
   "page_count": 4,
   "order": 850,
   "p1": "3221",
   "pn": "3224",
   "abstract": [
    "We report work on mapping the acoustic speech signal, parametrized using Mel Frequency Cepstral Analysis, onto electromagnetic articulography trajectories from the MOCHA database. We employ the machine learning technique of Support Vector Regression, contrasting previous works that applied Neural Networks to the same task. Our results are comparable to those older attempts, even though, due to training time considerations, we use a much smaller training set, derived by means of clustering the acoustic data.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-850"
  },
  "liu05f_interspeech": {
   "authors": [
    [
     "Yang",
     "Liu"
    ],
    [
     "Elizabeth",
     "Shriberg"
    ],
    [
     "Andreas",
     "Stolcke"
    ],
    [
     "Mary",
     "Harper"
    ]
   ],
   "title": "Comparing HMM, maximum entropy, and conditional random fields for disfluency detection",
   "original": "i05_3313",
   "page_count": 4,
   "order": 851,
   "p1": "3313",
   "pn": "3316",
   "abstract": [
    "Automatic detection of disfluencies in spoken language is important for making speech recognition output more readable, and for aiding downstream language processing modules. We compare a generative hidden Markov model (HMM)-based approach and two conditional models - a maximum entropy (Maxent) model and a conditional random field (CRF) - for detecting disfluencies in speech. The conditional modeling approaches provide a more principled way to model correlated features. In particular, the CRF approach directly detects the reparandum regions, and thus avoids the use of ad-hoc heuristic rules. We evaluate performance of these three models across two different corpora (conversational speech and broadcast news) and for two types of transcriptions (human transcriptions and recognition output). Overall we find that the conditional modeling approaches (Maxent and CRF) tend to outperform (with one exception) the HMM approach. Effects of speaking style, word recognition errors, and future directions are also discussed.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-851"
  },
  "raj05_interspeech": {
   "authors": [
    [
     "Bhiksha",
     "Raj"
    ],
    [
     "Rita",
     "Singh"
    ],
    [
     "Paris",
     "Smaragdis"
    ]
   ],
   "title": "Recognizing speech from simultaneous speakers",
   "original": "i05_3317",
   "page_count": 4,
   "order": 852,
   "p1": "3317",
   "pn": "3320",
   "abstract": [
    "In this paper we present and evaluate factored methods for recognition of simultaneous speech from multiple speakers in single-channel recordings. Factored methods decompose the problem of jointly recognizing the speech from each of the speakers by separately recognizing the speech from each speaker. In order to achieve this, the signal components of the target speaker in each case must be enhanced in some manner. We do this in two ways: using an NMF-based speaker separation algorithm that generates separated spectra for each speaker, and a mask estimation method that generates spectral masks for each speaker that must be used in conjunction with a missing-feature method that can recognize speech from partial spectral data. Experiments on synthetic mixtures of signals from the Wall Street Journal corpus show that both approaches can greatly improve the recognition of the individual signals in the mixture.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-852"
  },
  "wan05b_interspeech": {
   "authors": [
    [
     "Vincent",
     "Wan"
    ],
    [
     "James",
     "Carmichael"
    ]
   ],
   "title": "Polynomial dynamic time warping kernel support vector machines for dysarthric speech recognition with sparse training data",
   "original": "i05_3321",
   "page_count": 4,
   "order": 853,
   "p1": "3321",
   "pn": "3324",
   "abstract": [
    "This paper describes a new formulation of a polynomial sequence kernel based on dynamic time warping (DTW) for support vector machine (SVM) classification of isolated words given very sparse training data. The words are uttered by dysarthric speakers who suffer from debilitating neurological conditions that make the collection of speech samples a time-consuming and low-yield process. Data for building dysarthric speech recognition engines are therefore limited. Simulations show that the SVM based approach is significantly better than standard DTW and hidden Markov model (HMM) approaches when given sparse training data. In conditions where the models were constructed from three examples of each word, the SVM approach recorded a 45% lower error rate (relative) than the DTW approach and a 35% lower error rate than the HMM approach.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-853"
  },
  "lejeune05_interspeech": {
   "authors": [
    [
     "R.",
     "Lejeune"
    ],
    [
     "J.",
     "Baude"
    ],
    [
     "C.",
     "Tchong"
    ],
    [
     "H.",
     "Crepy"
    ],
    [
     "C.",
     "Waast-Richard"
    ]
   ],
   "title": "Flavoured acoustic model and combined spelling to sound for asymmetrical bilingual environment",
   "original": "i05_3325",
   "page_count": 4,
   "order": 854,
   "p1": "3325",
   "pn": "3328",
   "abstract": [
    "The most common target of multilingual ASR aims at covering various speakers from various languages. The problem we address in this article is more specifically an asymmetrical bilingual scenario, where the same speaker may insert in his speech some foreign words using foreign pronunciations. This is a frequent situation for French as spoken in Canada, where English proper names are often spoken using English pronunciations. We explore in this article a new way of using multilingual models by enhancing a monolingual system in a measured manner (Flavoured Acoustic Models). We also present an innovative bilingual spelling to sound system based on separate decision trees, providing balanced alternatives in both languages. Our ASR results over the telephony channel show that both technologies associated with one another outperform by up to 75% monolingual systems on English pronunciations without degrading word error rate on French pronunciations.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-854"
  },
  "bartels05_interspeech": {
   "authors": [
    [
     "Chris",
     "Bartels"
    ],
    [
     "Kevin",
     "Duh"
    ],
    [
     "Jeff",
     "Bilmes"
    ],
    [
     "Katrin",
     "Kirchhoff"
    ],
    [
     "Simon",
     "King"
    ]
   ],
   "title": "Genetic triangulation of graphical models for speech and language processing",
   "original": "i05_3329",
   "page_count": 4,
   "order": 855,
   "p1": "3329",
   "pn": "3332",
   "abstract": [
    "Graphical models are an increasingly popular approach for speech and language processing. As researchers design ever more complex models it becomes crucial to find triangulations that make inference problems tractable. This paper presents a genetic algorithm for triangulation search that is well-suited for speech and language graphical models. It is unique in two ways: First, it can find triangulations appropriate for graphs with a mix of stochastic and deterministic dependencies. Second, the search is guided by optimizing the inference speed (CPU runtime) on real data. We show results on 10 real-world speech and language graphs and demonstrate inference speed-ups over standard triangulation methods.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-855"
  },
  "aradilla05_interspeech": {
   "authors": [
    [
     "Guillermo",
     "Aradilla"
    ],
    [
     "Jithendra",
     "Vepa"
    ],
    [
     "Hervé",
     "Bourlard"
    ]
   ],
   "title": "Improving speech recognition using a data-driven approach",
   "original": "i05_3333",
   "page_count": 4,
   "order": 856,
   "p1": "3333",
   "pn": "3336",
   "abstract": [
    "In this paper, we investigate the possibility of enhancing state-ofthe- art HMM-based speech recognition systems using data-driven techniques, where whole set of training utterances is used as reference models and recognition is then performed through the well-known template matching technique, DTW. This approach allows us to better capture the temporal dynamics of the speech signal while avoiding some of the HMM assumptions such as the piecewise stationarity. Potentially, such data-driven techniques also allow us to better exploit meta-data and environmental information, such as speaker, gender, accent and noise conditions. However, we cannot entirely abandon HMMs, which are very powerful and scalable models. Thus, we investigate one way to combine and take advantage of both the approaches, combining scores of HMMs and reference templates. Experiments on the Numbers95 database showed that this combination yields 22% relative improvement in word error rate over the baseline HMM performance. Applying K-means clustering to the acoustic vectors speeds up the decoding, while still retaining a significant improvement in the recognition accuracy.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-856"
  },
  "matsuda05_interspeech": {
   "authors": [
    [
     "Shigeki",
     "Matsuda"
    ],
    [
     "Wolfgang",
     "Herbordt"
    ],
    [
     "Satoshi",
     "Nakamura"
    ]
   ],
   "title": "Outlier detection for acoustic model training using robust statistics",
   "original": "i05_3337",
   "page_count": 4,
   "order": 857,
   "p1": "3337",
   "pn": "3340",
   "abstract": [
    "In this paper, we propose an acoustic model training technique which is robust against outliers such as clipping, unexpected noise, poorly pronounced word segments, or mis-transcriptions, which deteriorate the quality of the acoustic models and in turn decrease speech recognition performance. The outlier-robust acoustic model training technique is based on a maximum likelihood (ML) criterion and automatically detects and removes outliers from the training data. Experiments with artificially contaminated mis-transcribed training data show that nearly the same word error rate can be obtained for contaminated data using the proposed technique as for uncontaminated data. Application to a dialogue speech database with unknown outliers reduces the errors by 4.03%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-857"
  },
  "roux05_interspeech": {
   "authors": [
    [
     "Jonathan Le",
     "Roux"
    ],
    [
     "Erik",
     "McDermott"
    ]
   ],
   "title": "Optimization methods for discriminative training",
   "original": "i05_3341",
   "page_count": 4,
   "order": 858,
   "p1": "3341",
   "pn": "3344",
   "abstract": [
    "Discriminative training applied to hidden Markov model (HMM) design can yield significant benefits in recognition accuracy and model compactness. However, compared to Maximum Likelihood based methods, discriminative training typically requires much more computation, as all competing candidates must be considered, not just the correct one. The choice of the algorithm used to optimize the discriminative criterion function is thus a key issue. We investigated several algorithms and used them for discriminative training based on the Minimum Classification Error (MCE) framework. In particular, we examined on-line, batch, and semi-batch Probabilistic Descent (PD), as well as Quickprop, Rprop and BFGS. We describe each algorithm and present comparative results on the TIMIT phone classification task and on the 230 hour Corpus of Spontaneous Japanese (CSJ) 30K word continuous speech recognition task.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-858"
  },
  "cardinal05_interspeech": {
   "authors": [
    [
     "Patrick",
     "Cardinal"
    ],
    [
     "Gilles",
     "Boulianne"
    ],
    [
     "Michel",
     "Comeau"
    ]
   ],
   "title": "Segmentation of recordings based on partial transcriptions",
   "original": "i05_3345",
   "page_count": 4,
   "order": 859,
   "p1": "3345",
   "pn": "3348",
   "abstract": [
    "In this paper, we present the approach we used to produce a training database from a set of recorded newscasts for which we had inaccurate transcriptions. These transcribed segments correspond to a set of prepared anchor texts and journalist stories, not necessarily in chronological order of their actual presentation. No segmental time boundary information is provided. Our main concern is thus to establish time marks that delimit the audio segments of the corresponding texts. To resolve this problem, we have developed a time marking procedure using our speech recognition engine. We obtain a segmentation accuracy of 80%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-859"
  },
  "seid05_interspeech": {
   "authors": [
    [
     "Hussien",
     "Seid"
    ],
    [
     "Björn",
     "Gambäck"
    ]
   ],
   "title": "A speaker independent continuous speech recognizer for Amharic",
   "original": "i05_3349",
   "page_count": 4,
   "order": 860,
   "p1": "3349",
   "pn": "3352",
   "abstract": [
    "The paper discusses an Amharic speaker independent continuous speech recognizer based on an HMM/ANN hybrid approach. The model was constructed at a context dependent phone part subword level with the help of the CSLU Toolkit. A promising result of 74.28% word and 39.70% sentence recognition rate was achieved. These are the best figures reported so far for speech recognition for the Amharic language.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-860"
  },
  "ogawa05_interspeech": {
   "authors": [
    [
     "Tetsuji",
     "Ogawa"
    ],
    [
     "Tetsunori",
     "Kobayashi"
    ]
   ],
   "title": "Optimizing the structure of partly-hidden Markov models using weighted likelihood-ratio maximization criterion",
   "original": "i05_3353",
   "page_count": 4,
   "order": 861,
   "p1": "3353",
   "pn": "3356",
   "abstract": [
    "A structure of Partly-Hidden Markov Model (PHMM) is optimized.\n",
    "PHMM was proposed in our previous work to deal with the complicated temporal changes of acoustic features. It can realize the observation dependent behaviors in both observations and state transitions. In the formulation of previous PHMM, we used a common structure in all model categories. However, it is well known that the optimal structure which gives best performance differs from category to category.\n",
    "In this paper, we designed a new structure optimization method in which the state-observation dependences in PHMM are optimally defined with respect to each category using Weighted Likelihood- Ratio Maximization (WLRM) criterion. WLRM criterion induces sparse and discriminative structures, and therefore gives the resulting structurally discriminative models. We define the model structure combination which gives maximum weighted likelihoodratio for any possible structure patterns as the optimal structures, and Genetic Algorithm is applied to an optimal approximation of search.\n",
    "As the results of continuous speech recognition aiming at lecture talks, the effectiveness of the proposed structure optimization is shown: it reduced the word errors compared to HMM and PHMM with common structure for all categories.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-861"
  },
  "kumar05b_interspeech": {
   "authors": [
    [
     "C. Santhosh",
     "Kumar"
    ],
    [
     "V. P.",
     "Mohandas"
    ],
    [
     "Haizhou",
     "Li"
    ]
   ],
   "title": "Multilingual speech recognition: a unified approach",
   "original": "i05_3357",
   "page_count": 4,
   "order": 862,
   "p1": "3357",
   "pn": "3360",
   "abstract": [
    "In this paper, we present a unified approach for hidden markov model based multilingual speech recognition. The proposed approach could be used across acoustically similar as well as diverse languages. We use an automatic phone mapping algorithm to map phones across languages and reduce the effective number of phones in the multilingual acoustic model. We experimentally verify the effectiveness of the approach using two acoustically similar languages, Tamil and Hindi and also American English which is very different from the other two languages acoustically. The experimental results are very encouraging and demonstrate the effectiveness of the approach in building a universal multilingual speech recognition system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-862"
  },
  "bartos05_interspeech": {
   "authors": [
    [
     "Tomás",
     "Bartos"
    ],
    [
     "Ludek",
     "Müller"
    ]
   ],
   "title": "Detection of recognition errors based on classifiers trained on artificially created data",
   "original": "i05_3361",
   "page_count": 4,
   "order": 863,
   "p1": "3361",
   "pn": "3364",
   "abstract": [
    "This paper wishes to contribute to the solution of the problem occurring when an automatic speech recognition system does not recognize an input utterance correctly. The solution is usually based on a utilization of a confidence measure (CM) which is assigned to each recognized word and which informs a user or a higher level module on the belief that the recognized word has been really said. The task becomes more difficult if the vocabulary contains acoustically similar words which differ for example only in one phoneme. To cope with this problem, we introduced a new confidence measure based on our previous experiments. The basic elementary unit for which the presented CM is investigated is a phone.\n",
    "The first part of the article shortly describes the used speech recognition system and the previously used confidence measures. The main part of this article deals with description of creation of the new CM based on utilization of artificially created training data and also with description of the used classification features and the classifiers based on this CM.\n",
    "A rejection technique based on the described CM was evaluated on the Czech yellow-pages database. Experimental results show that the proposed rejection technique achieves approximately 5% equal error rate (ERR) for phone rejection and about 6-16% EER for word rejection.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-863"
  },
  "li05h_interspeech": {
   "authors": [
    [
     "Jinyu",
     "Li"
    ],
    [
     "Chin-Hui",
     "Lee"
    ]
   ],
   "title": "On designing and evaluating speech event detectors",
   "original": "i05_3365",
   "page_count": 4,
   "order": 864,
   "p1": "3365",
   "pn": "3368",
   "abstract": [
    "We study issues related to designing speech event detectors for automatic speech recognition. Event detection is a critical component of a recently proposed automatic speech attribute transcription (ASAT) paradigm for speech research. Similar to keyword spotting and non-keyword rejection, a good detector needs to effectively detect speech attributes of interest while rejecting extraneous events. We compare frame and segment based detectors, study their properties in detecting manners of articulation, and propose new performance measures. We test these detectors on the TIMIT database with several evaluation criteria. Our results indicate that segment based detectors outperform frame based detectors in several key aspects of speech detector design. We also show that the performance can be significantly enhanced by incorporating discriminative training into designing speech event detectors.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-864"
  },
  "razik05_interspeech": {
   "authors": [
    [
     "Joseph",
     "Razik"
    ],
    [
     "Odile",
     "Mella"
    ],
    [
     "Dominique",
     "Fohr"
    ],
    [
     "Jean-Paul",
     "Haton"
    ]
   ],
   "title": "Local word confidence measure using word graph and n-best list",
   "original": "i05_3369",
   "page_count": 4,
   "order": 865,
   "p1": "3369",
   "pn": "3372",
   "abstract": [
    "This paper presents some confidence measures for large vocabulary speech recognition which are based on word graph or N-Best List structures. More and more applications need fast estimation of any measures in order to stay real-time. We propose some simple and fast measures, locally computed, that can be directly used within the first decoding recognition process. We also define some other measures combining word graph and N-Best List. Experimental results on a French broadcast news corpus are also presented.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-865"
  },
  "ren05_interspeech": {
   "authors": [
    [
     "Xiaolin",
     "Ren"
    ],
    [
     "Xin",
     "He"
    ],
    [
     "Yaxin",
     "Zhang"
    ]
   ],
   "title": "Mandarin/English mixed-lingual name recognition for mobile phone",
   "original": "i05_3373",
   "page_count": 4,
   "order": 866,
   "p1": "3373",
   "pn": "3376",
   "abstract": [
    "Speaker independent name speech recognition has become hot application in handheld devices such as mobile phones and personal digit assistants (PDAs). This paper presents a new mixed-lingual ASR system that will enable Chinese mobile phone users to conduct Mandarin and English name speech recognition simultaneously without switching language modes. We created an elaborately designed mixed acoustic model set which consists of two monolingual acoustic model sets, i.e. context-dependent Mandarin models and context-independent English models. Design of such mixed acoustic models for mobile phones is to balance the complexity and performance of the ASR system. We also described Mandarin and English mixed-lingual letter-to-sound converter in this paper. We demonstrated the effectiveness of the mixed-lingual name speech recognition system through off-line ASR experiments. Furthermore, the mixed-lingual name ASR system has also been integrated in Motorola new PDA phones and showed good performance in real world application.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-866"
  },
  "ferreiros05_interspeech": {
   "authors": [
    [
     "Javier",
     "Ferreiros"
    ],
    [
     "Rubén San",
     "Segundo"
    ],
    [
     "Fernando",
     "Fernández"
    ],
    [
     "Luis-Fernando",
     "D'Haro"
    ],
    [
     "Valentín",
     "Sama"
    ],
    [
     "Roberto",
     "Barra"
    ],
    [
     "Pedro",
     "Mellén"
    ]
   ],
   "title": "New word-level and sentence-level confidence scoring using graph theory calculus and its evaluation on speech understanding",
   "original": "i05_3377",
   "page_count": 4,
   "order": 867,
   "p1": "3377",
   "pn": "3380",
   "abstract": [
    "A lot of work has been devoted to the estimation of confidence measures for speech recognizers. In the quite extended case where a word-graph speech recognizer is in use, we will present new confidence measures employing the graph theory that shows us how to estimate some interesting characteristics about the different paths through the graph that constitute the recognition solutions, without the need of expanding them all.\n",
    "We will take advantage of some of these features to generate confidence scores both at the word and sentence level. We will also compare this new confidence scoring to more traditional ones and will find similar behavior with less computational load and with an increase in the simplicity of the approach that will lead to more generalization power of the confidence estimation to different applications of the recognizer.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-867"
  },
  "nakamura05b_interspeech": {
   "authors": [
    [
     "Masanobu",
     "Nakamura"
    ],
    [
     "Koji",
     "Iwano"
    ],
    [
     "Sadaoki",
     "Furui"
    ]
   ],
   "title": "Analysis of spectral space reduction in spontaneous speech and its effects on speech recognition performances",
   "original": "i05_3381",
   "page_count": 4,
   "order": 868,
   "p1": "3381",
   "pn": "3384",
   "abstract": [
    "Although speech, derived from reading texts, and similar types of speech, e.g. that from reading newspapers or that from news broadcast, can be recognized with high accuracy, recognition accuracy drastically decreases for spontaneous speech. This is due to the fact that spontaneous speech and read speech are significantly different acoustically as well as linguistically. This paper analyzes differences in acoustic features between spontaneous speech and read speech using a large-scale spontaneous speech database \"Corpus of Spontaneous Japanese (CSJ)\". Experimental results show that spontaneous speech can be characterized by reduced size of spectral space in comparison with that of read speech. It has also been found that there is a strong correlation between mean spectral distance between phonemes and phoneme recognition accuracy. This indicates that spectral reduction is one major reason for the decrease of recognition accuracy of spontaneous speech.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-868"
  },
  "king05_interspeech": {
   "authors": [
    [
     "Simon",
     "King"
    ],
    [
     "Chris",
     "Bartels"
    ],
    [
     "Jeff",
     "Bilmes"
    ]
   ],
   "title": "SVitchboard 1: small vocabulary tasks from Switchboard",
   "original": "i05_3385",
   "page_count": 4,
   "order": 869,
   "p1": "3385",
   "pn": "3388",
   "abstract": [
    "We present a conversational telephone speech data set designed to support research on novel acoustic models. Small vocabulary tasks from 10 words up to 500 words are defined using subsets of the Switchboard-1 corpus; each task has a completely closed vocabulary (an OOV rate of 0%). We justify the need for these tasks, describe the algorithm for selecting them from a large corpus, give a statistical analysis of the data and present baseline whole-word hidden Markov model recognition results. The goal of the paper is to define a common data set and to encourage other researchers to use it.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2005-869"
  }
 },
 "sessions": [
  {
   "title": "Keynote Papers",
   "papers": [
    "clark05_interspeech",
    "pereira05_interspeech",
    "shriberg05_interspeech"
   ]
  },
  {
   "title": "Speech Recognition - Language Modelling I-III",
   "papers": [
    "tam05_interspeech",
    "seneviratne05_interspeech",
    "mori05_interspeech",
    "gruenstein05_interspeech",
    "wang05_interspeech",
    "chien05_interspeech",
    "chueh05_interspeech",
    "bisani05_interspeech",
    "jeong05_interspeech",
    "lin05_interspeech",
    "schwenk05_interspeech",
    "xu05_interspeech",
    "kuo05_interspeech",
    "ghaoui05_interspeech",
    "siciliagarcia05_interspeech",
    "diegueztirado05_interspeech",
    "sethy05_interspeech",
    "troncoso05_interspeech",
    "duchateau05_interspeech",
    "allauzen05_interspeech",
    "siivola05_interspeech",
    "huning05_interspeech",
    "broman05_interspeech",
    "vaiciunas05_interspeech",
    "gorrell05_interspeech",
    "jensson05_interspeech",
    "witschel05_interspeech"
   ]
  },
  {
   "title": "Prosody in Language Performance I, II",
   "papers": [
    "hirst05_interspeech",
    "ohsuga05_interspeech",
    "watanabe05_interspeech",
    "schneider05_interspeech",
    "grimm05_interspeech",
    "meyer05_interspeech",
    "kim05_interspeech",
    "welby05_interspeech",
    "chow05_interspeech",
    "fale05_interspeech",
    "wagner05_interspeech",
    "jensen05_interspeech",
    "edlund05_interspeech",
    "jilka05_interspeech",
    "broggelwirth05_interspeech",
    "biersack05_interspeech",
    "arbisikelm05_interspeech",
    "lintfert05_interspeech",
    "dohen05_interspeech",
    "barkhuysen05_interspeech",
    "tanaka05_interspeech"
   ]
  },
  {
   "title": "Spoken Language Extraction / Retrieval I, II",
   "papers": [
    "siohan05_interspeech",
    "makhoul05_interspeech",
    "chelba05_interspeech",
    "akiba05_interspeech",
    "kolluru05_interspeech",
    "huang05_interspeech",
    "taniguchi05_interspeech",
    "murray05_interspeech",
    "hessen05_interspeech",
    "favre05_interspeech",
    "kurimo05_interspeech",
    "gonzalez05_interspeech",
    "yamaguchi05_interspeech",
    "hori05_interspeech",
    "maskey05_interspeech",
    "li05_interspeech",
    "zibert05_interspeech",
    "szoke05_interspeech",
    "misu05_interspeech",
    "moreau05_interspeech"
   ]
  },
  {
   "title": "The Blizzard Challenge 2005",
   "papers": [
    "black05_interspeech",
    "sakai05_interspeech",
    "kominek05_interspeech",
    "bunnell05_interspeech",
    "zen05_interspeech",
    "hamza05_interspeech",
    "clark05b_interspeech",
    "bennett05_interspeech"
   ]
  },
  {
   "title": "New Applications",
   "papers": [
    "chen05_interspeech",
    "katoh05_interspeech",
    "moore05_interspeech",
    "haebumbach05_interspeech",
    "sugiyama05_interspeech",
    "nwe05_interspeech",
    "ogata05_interspeech",
    "olszewski05_interspeech",
    "ezzaidi05_interspeech",
    "abad05_interspeech",
    "fredouille05_interspeech",
    "chaudhari05_interspeech"
   ]
  },
  {
   "title": "E-learning and Spoken Language Processing",
   "papers": [
    "forbesriley05_interspeech",
    "litman05_interspeech",
    "asakawa05_interspeech",
    "chou05_interspeech",
    "ito05_interspeech",
    "sethy05b_interspeech",
    "zhang05_interspeech"
   ]
  },
  {
   "title": "E-inclusion and Spoken Language Processing I, II",
   "papers": [
    "brndsted05_interspeech",
    "williams05_interspeech",
    "vovos05_interspeech",
    "jitsuhiro05_interspeech",
    "falavigna05_interspeech",
    "hawley05_interspeech",
    "granstrom05_interspeech",
    "tucker05_interspeech",
    "andersen05_interspeech",
    "kvale05_interspeech"
   ]
  },
  {
   "title": "Acoustic Processing for ASR I-III",
   "papers": [
    "wolfel05_interspeech",
    "tyagi05_interspeech",
    "faria05_interspeech",
    "zheng05_interspeech",
    "barrault05_interspeech",
    "lihan05_interspeech",
    "pelaezmoreno05_interspeech",
    "kohler05_interspeech",
    "meinedo05_interspeech",
    "marcheret05_interspeech",
    "tuske05_interspeech",
    "murase05_interspeech",
    "deng05_interspeech",
    "kocharov05_interspeech",
    "watanabe05b_interspeech",
    "tsao05_interspeech",
    "afify05_interspeech",
    "gunawardana05_interspeech",
    "jonas05_interspeech",
    "ouellet05_interspeech",
    "keshet05_interspeech",
    "leppanen05_interspeech",
    "suchato05_interspeech",
    "toth05_interspeech",
    "povey05_interspeech",
    "lei05_interspeech",
    "han05_interspeech",
    "temko05_interspeech",
    "stadermann05_interspeech",
    "honig05_interspeech",
    "pinto05_interspeech",
    "diehl05_interspeech",
    "miguel05_interspeech",
    "dimitriadis05_interspeech",
    "hifny05_interspeech",
    "erdogan05_interspeech",
    "wang05b_interspeech",
    "fu05_interspeech",
    "liu05_interspeech",
    "munich05_interspeech",
    "heracleous05_interspeech",
    "frankel05_interspeech"
   ]
  },
  {
   "title": "Speech Recognition - Adaptation I, II",
   "papers": [
    "zhang05b_interspeech",
    "bakker05_interspeech",
    "hu05_interspeech",
    "garau05_interspeech",
    "umesh05_interspeech",
    "cui05_interspeech",
    "raut05_interspeech",
    "liu05b_interspeech",
    "nishida05_interspeech",
    "ye05_interspeech",
    "gomez05_interspeech",
    "choi05_interspeech",
    "visweswariah05_interspeech",
    "li05b_interspeech",
    "mandal05_interspeech",
    "hsiao05_interspeech",
    "wang05c_interspeech",
    "luo05_interspeech"
   ]
  },
  {
   "title": "Signal Analysis, Processing and Feature Estimation I-III",
   "papers": [
    "liu05c_interspeech",
    "banhalmi05_interspeech",
    "lee05_interspeech",
    "kepesi05_interspeech",
    "hosom05_interspeech",
    "milner05_interspeech",
    "barbot05_interspeech",
    "drepper05_interspeech",
    "vincent05_interspeech",
    "alsteris05_interspeech",
    "muralishankar05_interspeech",
    "ferreira05_interspeech",
    "pincas05_interspeech",
    "hecht05_interspeech",
    "firouzmand05_interspeech",
    "hermansky05_interspeech",
    "jeon05_interspeech",
    "kristjansson05_interspeech",
    "gomez05b_interspeech",
    "sarkar05_interspeech",
    "nagarajan05_interspeech",
    "motlicek05_interspeech",
    "korhonen05_interspeech",
    "vijayalakshmi05_interspeech",
    "zdansky05_interspeech",
    "molla05_interspeech",
    "tsuzaki05_interspeech",
    "arias05_interspeech",
    "evangelopoulos05_interspeech",
    "kominek05b_interspeech",
    "valente05_interspeech",
    "borys05_interspeech",
    "vijayalakshmi05b_interspeech",
    "weruaga05_interspeech",
    "karabetsos05_interspeech",
    "ivanov05_interspeech",
    "gianfelici05_interspeech",
    "katsamanis05_interspeech",
    "darch05_interspeech",
    "prasanna05_interspeech",
    "cabral05_interspeech",
    "ohishi05_interspeech"
   ]
  },
  {
   "title": "Robust Speech Recognition I-IV",
   "papers": [
    "pettersen05_interspeech",
    "yared05_interspeech",
    "buera05_interspeech",
    "chen05b_interspeech",
    "nakajima05_interspeech",
    "subramanya05_interspeech",
    "haebumbach05b_interspeech",
    "potamitis05_interspeech",
    "shozakai05_interspeech",
    "lim05_interspeech",
    "huang05b_interspeech",
    "choi05b_interspeech",
    "tolba05_interspeech",
    "so05_interspeech",
    "nasersharif05_interspeech",
    "deng05b_interspeech",
    "stouten05_interspeech",
    "wan05_interspeech",
    "chung05_interspeech",
    "kobashikawa05_interspeech",
    "prasad05_interspeech",
    "suzuki05_interspeech",
    "xu05b_interspeech",
    "song05_interspeech",
    "graciarena05_interspeech",
    "droppo05_interspeech",
    "kruger05_interspeech",
    "barreaud05_interspeech",
    "colibro05_interspeech",
    "schutte05_interspeech",
    "ma05_interspeech",
    "epps05_interspeech",
    "gemello05_interspeech",
    "kida05_interspeech",
    "ding05_interspeech",
    "morales05_interspeech",
    "misra05_interspeech",
    "kim05b_interspeech",
    "coy05_interspeech",
    "szymanski05_interspeech",
    "heracleous05b_interspeech",
    "ruzanski05_interspeech",
    "kobayakawa05_interspeech",
    "wang05d_interspeech",
    "tanaka05b_interspeech",
    "alvarez05_interspeech",
    "setiawan05_interspeech",
    "mihajlik05_interspeech",
    "chen05c_interspeech",
    "ito05b_interspeech",
    "kraft05_interspeech",
    "krebber05_interspeech",
    "hirsch05_interspeech",
    "walsh05_interspeech",
    "liao05_interspeech",
    "vanhoucke05_interspeech",
    "lee05b_interspeech",
    "fukuda05_interspeech",
    "kinoshita05_interspeech",
    "wolfel05b_interspeech"
   ]
  },
  {
   "title": "Speech Perception I, II",
   "papers": [
    "alexander05_interspeech",
    "ma05b_interspeech",
    "mixdorff05_interspeech",
    "mixdorff05b_interspeech",
    "cutler05_interspeech",
    "huang05c_interspeech",
    "tran05_interspeech",
    "schwanhauer05_interspeech",
    "fale05b_interspeech",
    "braun05_interspeech",
    "aikawa05_interspeech",
    "omata05_interspeech",
    "svanfeldt05_interspeech",
    "mayo05_interspeech",
    "terasawa05_interspeech",
    "kacha05_interspeech",
    "irino05_interspeech",
    "hayashi05_interspeech",
    "wong05_interspeech",
    "jongmans05_interspeech",
    "brown05_interspeech",
    "janse05_interspeech",
    "jacquier05_interspeech",
    "grataloup05_interspeech",
    "tokuma05_interspeech",
    "vasilescu05_interspeech",
    "pell05_interspeech"
   ]
  },
  {
   "title": "Spoken Language Understanding I, II",
   "papers": [
    "lane05_interspeech",
    "boulis05_interspeech",
    "sudoh05_interspeech",
    "sarikaya05_interspeech",
    "kuo05b_interspeech",
    "thomae05_interspeech",
    "thomae05b_interspeech",
    "wang05e_interspeech",
    "surdeanu05_interspeech",
    "zitouni05_interspeech",
    "eun05_interspeech",
    "lieb05_interspeech",
    "ohno05_interspeech",
    "sako05_interspeech",
    "bonneaumaynard05_interspeech",
    "engel05_interspeech",
    "kobus05_interspeech",
    "seabralopes05_interspeech",
    "wu05_interspeech"
   ]
  },
  {
   "title": "Paralinguistic and Nonlinguistic Information in Speech",
   "papers": [
    "campbell05_interspeech",
    "blouin05_interspeech",
    "fernandez05_interspeech",
    "cichosz05_interspeech",
    "ishi05_interspeech",
    "truong05_interspeech",
    "batliner05_interspeech",
    "luengo05_interspeech",
    "lee05c_interspeech",
    "hofer05_interspeech",
    "tesser05_interspeech",
    "burkhardt05_interspeech",
    "rosenberg05_interspeech",
    "greenberg05_interspeech",
    "braun05b_interspeech",
    "audibert05_interspeech",
    "schotz05_interspeech",
    "alm05_interspeech",
    "kawahara05_interspeech",
    "yonezawa05_interspeech"
   ]
  },
  {
   "title": "Issues in Large Vocabulary Decoding",
   "papers": [
    "hetherington05_interspeech",
    "saon05_interspeech",
    "yu05_interspeech",
    "hori05b_interspeech",
    "nanjo05_interspeech",
    "chan05_interspeech",
    "massonie05_interspeech",
    "novak05_interspeech",
    "ahn05_interspeech",
    "pylkkonen05_interspeech",
    "fabian05_interspeech"
   ]
  },
  {
   "title": "Spoken Language Acquisition, Development and Learning I, II",
   "papers": [
    "heeren05_interspeech",
    "you05_interspeech",
    "heeren05b_interspeech",
    "zmarich05_interspeech",
    "hirano05_interspeech",
    "hincks05_interspeech",
    "amano05_interspeech",
    "jia05_interspeech",
    "wang05f_interspeech",
    "henrichsen05_interspeech",
    "ohta05_interspeech"
   ]
  },
  {
   "title": "Multi-modal / Multi-media Processing I, II",
   "papers": [
    "campbell05b_interspeech",
    "wrigley05_interspeech",
    "huang05d_interspeech",
    "tisato05_interspeech",
    "ejarque05_interspeech",
    "schubert05_interspeech",
    "kumaran05_interspeech",
    "daubias05_interspeech",
    "shdaifat05_interspeech",
    "kitaoka05_interspeech",
    "matsusaka05_interspeech",
    "li05c_interspeech",
    "cooke05_interspeech",
    "gurbuz05_interspeech",
    "huang05e_interspeech",
    "mixdorff05c_interspeech",
    "seymour05_interspeech"
   ]
  },
  {
   "title": "Emotional speech analysis and synthesis",
   "papers": [
    "beskow05_interspeech",
    "turk05_interspeech",
    "bulut05_interspeech",
    "schuller05_interspeech",
    "kim05c_interspeech",
    "douglascowie05_interspeech"
   ]
  },
  {
   "title": "Spoken / Multi-modal Dialogue Systems I, II",
   "papers": [
    "torres05_interspeech",
    "black05b_interspeech",
    "kral05_interspeech",
    "wu05b_interspeech",
    "oria05_interspeech",
    "toptsis05_interspeech",
    "reithinger05_interspeech",
    "nisimura05_interspeech",
    "salonen05_interspeech",
    "hakulinen05_interspeech",
    "gonzalezferreras05_interspeech",
    "pietquin05_interspeech",
    "chu05_interspeech",
    "hjalmarsson05_interspeech",
    "katsurada05_interspeech",
    "komatani05_interspeech",
    "rotaru05_interspeech",
    "raux05_interspeech",
    "fujie05_interspeech",
    "georgila05_interspeech",
    "martiniglesias05_interspeech",
    "schuler05_interspeech",
    "bosch05_interspeech",
    "galibert05_interspeech",
    "bernsen05_interspeech",
    "goronzy05_interspeech",
    "rotaru05b_interspeech",
    "feng05_interspeech",
    "moller05_interspeech",
    "parthasarathy05_interspeech"
   ]
  },
  {
   "title": "Speech Production I",
   "papers": [
    "rochetcapellan05_interspeech",
    "rochetcapellan05b_interspeech",
    "nakamura05_interspeech",
    "robert05_interspeech",
    "dang05_interspeech",
    "hu05b_interspeech",
    "arai05_interspeech",
    "ouni05_interspeech",
    "perrier05_interspeech",
    "niu05_interspeech",
    "ogata05b_interspeech",
    "alku05_interspeech",
    "bjorkner05_interspeech",
    "fant05_interspeech",
    "perez05_interspeech",
    "zeroual05_interspeech",
    "moura05_interspeech",
    "hanquinet05_interspeech",
    "fontecave05_interspeech",
    "sapir05_interspeech",
    "paulo05_interspeech",
    "lee05d_interspeech",
    "airas05_interspeech",
    "sciamarella05_interspeech",
    "nomura05_interspeech",
    "pouplier05_interspeech",
    "serrurier05_interspeech",
    "cros05_interspeech"
   ]
  },
  {
   "title": "Spoken Language Resources and Technology Evaluation I, II",
   "papers": [
    "jones05_interspeech",
    "galliano05_interspeech",
    "saito05_interspeech",
    "paulo05b_interspeech",
    "strik05_interspeech",
    "kolar05_interspeech",
    "burkhardt05b_interspeech",
    "mareuil05_interspeech",
    "jurcicek05_interspeech",
    "branco05_interspeech",
    "chan05b_interspeech",
    "zgank05_interspeech",
    "volin05_interspeech",
    "strassel05_interspeech",
    "charlet05_interspeech",
    "lin05b_interspeech",
    "chiang05_interspeech",
    "davel05_interspeech",
    "ward05_interspeech",
    "parisi05_interspeech",
    "conejero05_interspeech",
    "boril05_interspeech",
    "kazemzadeh05_interspeech",
    "tohyama05_interspeech",
    "bates05_interspeech",
    "silaghi05_interspeech",
    "draxler05_interspeech",
    "abate05_interspeech",
    "dolfing05_interspeech",
    "muller05_interspeech"
   ]
  },
  {
   "title": "Early Language Acquisition",
   "papers": [
    "ishizuka05_interspeech",
    "zajdo05_interspeech",
    "lintfert05b_interspeech",
    "salvi05_interspeech",
    "sudo05_interspeech"
   ]
  },
  {
   "title": "Bridging the Gap ASR-HSR",
   "papers": [
    "dusan05_interspeech",
    "scharenborg05_interspeech",
    "bosch05b_interspeech",
    "maier05_interspeech",
    "foslerlussier05_interspeech",
    "holmberg05_interspeech",
    "carey05_interspeech",
    "murakami05_interspeech",
    "srinivasan05_interspeech",
    "harding05_interspeech",
    "wesker05_interspeech"
   ]
  },
  {
   "title": "Speech Recognition - Pronunciation Modelling",
   "papers": [
    "jeon05b_interspeech",
    "tjalve05_interspeech",
    "truong05b_interspeech",
    "psutka05_interspeech",
    "dupont05_interspeech",
    "takahashi05_interspeech",
    "ramasubramanian05_interspeech",
    "liu05d_interspeech",
    "bouselmi05_interspeech"
   ]
  },
  {
   "title": "Prosodic Structure",
   "papers": [
    "auberge05_interspeech",
    "dalton05_interspeech",
    "fletcher05_interspeech",
    "yang05_interspeech",
    "wang05g_interspeech",
    "xiong05_interspeech",
    "ni05_interspeech",
    "pan05_interspeech",
    "tseng05_interspeech",
    "yuan05_interspeech",
    "fujisaki05_interspeech",
    "govender05_interspeech",
    "bishop05_interspeech",
    "petrone05_interspeech",
    "dubeda05_interspeech",
    "yeou05_interspeech",
    "lacheret05_interspeech",
    "barbosa05_interspeech"
   ]
  },
  {
   "title": "Applications of Confidence Related Measures to ASR",
   "papers": [
    "yamada05_interspeech",
    "schluter05_interspeech",
    "kobayashi05_interspeech",
    "dong05_interspeech",
    "ketabdar05_interspeech",
    "liu05e_interspeech"
   ]
  },
  {
   "title": "Multilingual TTS",
   "papers": [
    "tomokiyo05_interspeech",
    "fernandez05b_interspeech",
    "latorre05_interspeech",
    "gakuru05_interspeech",
    "ordinas05_interspeech",
    "janicki05_interspeech"
   ]
  },
  {
   "title": "Speech Bandwidth Extension",
   "papers": [
    "ehara05_interspeech",
    "geiser05_interspeech",
    "hu05c_interspeech",
    "bansal05_interspeech",
    "seltzer05_interspeech",
    "cabral05b_interspeech"
   ]
  },
  {
   "title": "Large Vocabulary Speech Recognition Systems",
   "papers": [
    "vergyri05_interspeech",
    "ramabhadran05_interspeech",
    "lin05c_interspeech",
    "ma05c_interspeech",
    "sakti05_interspeech",
    "messaoudi05_interspeech",
    "afify05b_interspeech",
    "matsoukas05_interspeech",
    "prasad05b_interspeech",
    "xiang05_interspeech",
    "deleglise05_interspeech",
    "lamel05_interspeech",
    "hain05_interspeech",
    "gauvain05_interspeech",
    "scharenborg05b_interspeech",
    "nguyen05_interspeech",
    "zhang05c_interspeech",
    "nouza05_interspeech",
    "schuster05_interspeech",
    "vu05_interspeech",
    "shinozaki05_interspeech"
   ]
  },
  {
   "title": "Prosody Modelling and Speech Technology I, II",
   "papers": [
    "levow05_interspeech",
    "tamburini05_interspeech",
    "ingulfsen05_interspeech",
    "gretter05_interspeech",
    "gu05_interspeech",
    "burrows05_interspeech",
    "hirst05b_interspeech",
    "cox05_interspeech",
    "read05_interspeech",
    "xydas05_interspeech",
    "dong05b_interspeech",
    "dong05c_interspeech",
    "teixeira05_interspeech",
    "li05d_interspeech",
    "hirose05_interspeech",
    "escudero05_interspeech",
    "sun05_interspeech",
    "chiang05b_interspeech",
    "rojc05_interspeech",
    "wang05h_interspeech",
    "romsdorfer05_interspeech",
    "morais05_interspeech",
    "gibbon05_interspeech",
    "nagano05_interspeech",
    "brenier05_interspeech",
    "surendran05_interspeech",
    "wypych05_interspeech",
    "sakurai05_interspeech"
   ]
  },
  {
   "title": "Detecting and Synthesizing Speaker State",
   "papers": [
    "hirschberg05_interspeech",
    "liscombe05_interspeech",
    "vidrascu05_interspeech",
    "liscombe05b_interspeech",
    "yanushevskaya05_interspeech",
    "takahashi05b_interspeech"
   ]
  },
  {
   "title": "Rapid Development of Spoken Dialogue Systems",
   "papers": [
    "fabbrizio05_interspeech",
    "katsurada05b_interspeech",
    "hanna05_interspeech",
    "wang05i_interspeech",
    "akbacak05_interspeech",
    "rayner05_interspeech"
   ]
  },
  {
   "title": "Text-to-Speech I, II",
   "papers": [
    "mairesse05_interspeech",
    "revelin05_interspeech",
    "salor05_interspeech",
    "li05e_interspeech",
    "zheng05b_interspeech",
    "massimino05_interspeech",
    "soonklang05_interspeech",
    "engwall05_interspeech",
    "chen05d_interspeech",
    "prusi05_interspeech",
    "hsia05_interspeech",
    "hirai05_interspeech",
    "tian05_interspeech",
    "liang05_interspeech",
    "reichel05_interspeech",
    "goubanova05_interspeech",
    "jande05_interspeech",
    "wang05j_interspeech",
    "cheng05_interspeech",
    "toda05_interspeech",
    "savino05_interspeech",
    "oliver05_interspeech",
    "hansakunbuntheung05_interspeech",
    "taylor05_interspeech",
    "reubold05_interspeech",
    "toda05b_interspeech",
    "tachibana05_interspeech",
    "webster05_interspeech",
    "syrdal05_interspeech",
    "pantazis05_interspeech"
   ]
  },
  {
   "title": "Speaker Characterization and Recognition I-IV",
   "papers": [
    "wang05k_interspeech",
    "leeuwen05_interspeech",
    "goldberger05_interspeech",
    "dusan05b_interspeech",
    "torretoledano05_interspeech",
    "fortuna05_interspeech",
    "mcauley05_interspeech",
    "collet05_interspeech",
    "arcienega05_interspeech",
    "yiu05_interspeech",
    "matsui05_interspeech",
    "zhang05d_interspeech",
    "amino05_interspeech",
    "kim05d_interspeech",
    "deng05c_interspeech",
    "zhang05e_interspeech",
    "lei05b_interspeech",
    "zhang05f_interspeech",
    "faundezzanuy05_interspeech",
    "omar05_interspeech",
    "ferrer05_interspeech",
    "aronowitz05_interspeech",
    "chetty05_interspeech",
    "asami05_interspeech",
    "solewicz05_interspeech",
    "stolcke05_interspeech",
    "baker05_interspeech",
    "aronowitz05b_interspeech",
    "sinha05_interspeech",
    "zhu05_interspeech",
    "istrate05_interspeech",
    "moraru05_interspeech",
    "dalmasso05_interspeech",
    "krstulovic05_interspeech",
    "ben05_interspeech",
    "scheffer05_interspeech",
    "tsai05_interspeech",
    "zochova05_interspeech",
    "gonon05_interspeech",
    "eveno05_interspeech",
    "kuroiwa05_interspeech",
    "leung05_interspeech",
    "chen05e_interspeech",
    "mihoubi05_interspeech",
    "louradour05_interspeech",
    "deng05d_interspeech",
    "mason05_interspeech",
    "zhou05_interspeech",
    "vogt05_interspeech",
    "siafarikas05_interspeech",
    "yin05_interspeech"
   ]
  },
  {
   "title": "Single-channel Speech Enhancement",
   "papers": [
    "cohen05_interspeech",
    "mouchtaris05_interspeech",
    "ortega05_interspeech",
    "bourgeois05_interspeech",
    "kumar05_interspeech",
    "braquet05_interspeech",
    "zavarehei05_interspeech",
    "yan05_interspeech",
    "jiang05_interspeech",
    "pham05_interspeech",
    "li05f_interspeech",
    "unoki05_interspeech",
    "hendriks05_interspeech",
    "lollmann05_interspeech",
    "roman05_interspeech",
    "zhao05_interspeech",
    "deshmukh05_interspeech"
   ]
  },
  {
   "title": "Acoustic Modelling for LVCSR",
   "papers": [
    "mak05_interspeech",
    "zheng05c_interspeech",
    "zhu05b_interspeech",
    "macherey05_interspeech",
    "sim05_interspeech",
    "zhu05c_interspeech"
   ]
  },
  {
   "title": "Gender and Age Issues in Speech and Language Research I, II",
   "papers": [
    "gerosa05_interspeech",
    "darcy05_interspeech",
    "cosi05_interspeech",
    "addadecker05_interspeech",
    "yildirim05_interspeech",
    "binnenpoorte05_interspeech",
    "elenius05_interspeech",
    "jansen05_interspeech",
    "hagen05_interspeech",
    "batliner05b_interspeech",
    "bell05_interspeech",
    "miyauchi05_interspeech"
   ]
  },
  {
   "title": "Language and Dialect Identification I, II",
   "papers": [
    "matejka05_interspeech",
    "huang05f_interspeech",
    "hamdi05_interspeech",
    "marcadet05_interspeech",
    "itahashi05_interspeech",
    "rouas05_interspeech",
    "wu05c_interspeech",
    "trancoso05_interspeech",
    "ma05d_interspeech",
    "zhu05d_interspeech",
    "gao05_interspeech",
    "salvi05b_interspeech"
   ]
  },
  {
   "title": "Spoken Language Translation I, II",
   "papers": [
    "paulik05_interspeech",
    "khadivi05_interspeech",
    "rodriguez05_interspeech",
    "kathol05_interspeech",
    "pico05_interspeech",
    "ohta05b_interspeech",
    "matusov05_interspeech",
    "quan05_interspeech",
    "crego05_interspeech",
    "gu05b_interspeech",
    "gispert05_interspeech",
    "bozarov05_interspeech"
   ]
  },
  {
   "title": "Multi-channel Speech Enhancement",
   "papers": [
    "lotter05_interspeech",
    "klee05_interspeech",
    "ichikawa05_interspeech",
    "hu05d_interspeech",
    "saitoh05_interspeech",
    "hu05e_interspeech",
    "delcroix05_interspeech",
    "li05g_interspeech",
    "krishnan05_interspeech",
    "denda05_interspeech",
    "kim05e_interspeech",
    "freudenberger05_interspeech",
    "murakami05b_interspeech",
    "brutti05_interspeech",
    "madhu05_interspeech",
    "lathoud05_interspeech",
    "srinivasan05b_interspeech",
    "grisoni05_interspeech",
    "robledoarnuncio05_interspeech",
    "subramanya05b_interspeech"
   ]
  },
  {
   "title": "Phonetics and Phonology I, II",
   "papers": [
    "dusan05c_interspeech",
    "gendrot05_interspeech",
    "quene05_interspeech",
    "komatsu05_interspeech",
    "altamimi05_interspeech",
    "shih05_interspeech",
    "moates05_interspeech",
    "mikuteit05_interspeech",
    "jacobi05_interspeech",
    "castelli05_interspeech",
    "ngoc05_interspeech",
    "hajek05_interspeech",
    "stevens05_interspeech",
    "gilifivela05_interspeech",
    "kim05f_interspeech",
    "iseijaakkola05_interspeech",
    "mori05b_interspeech",
    "rauber05_interspeech",
    "hoelterhoff05_interspeech",
    "skarnitzl05_interspeech",
    "maj05_interspeech",
    "tyson05_interspeech",
    "oliveira05_interspeech",
    "chalamandaris05_interspeech",
    "kalimeris05_interspeech",
    "weiss05_interspeech",
    "roy05_interspeech"
   ]
  },
  {
   "title": "Human factors, User Experience and Natural Language Application Design",
   "papers": [
    "levin05_interspeech",
    "pucher05_interspeech",
    "chen05f_interspeech",
    "levin05b_interspeech",
    "alvarezryan05_interspeech",
    "wright05_interspeech"
   ]
  },
  {
   "title": "TTS Inventory",
   "papers": [
    "aylett05_interspeech",
    "tihelka05_interspeech",
    "matousek05_interspeech",
    "campillodiaz05_interspeech",
    "weiss05b_interspeech",
    "chen05g_interspeech",
    "ezzat05_interspeech",
    "colotte05_interspeech",
    "amdal05_interspeech",
    "zhao05b_interspeech",
    "chen05h_interspeech",
    "rouibia05_interspeech",
    "chazan05_interspeech",
    "alias05_interspeech",
    "bjrkan05_interspeech",
    "barros05_interspeech",
    "hamza05b_interspeech",
    "strecha05_interspeech",
    "sundermann05_interspeech",
    "isogai05_interspeech",
    "fung05_interspeech",
    "schnell05_interspeech"
   ]
  },
  {
   "title": "Speech Coding",
   "papers": [
    "so05b_interspeech",
    "bao05_interspeech",
    "kruger05b_interspeech",
    "krishnan05b_interspeech",
    "durey05_interspeech",
    "ertan05_interspeech",
    "chang05_interspeech",
    "falk05_interspeech",
    "erkelens05_interspeech",
    "chen05i_interspeech",
    "perezcordoba05_interspeech"
   ]
  },
  {
   "title": "Discourse and Dialogue I, II",
   "papers": [
    "pfleger05_interspeech",
    "venkataraman05_interspeech",
    "bohus05_interspeech",
    "perezpinarlopez05_interspeech",
    "rosset05_interspeech",
    "akker05_interspeech",
    "wesseling05_interspeech",
    "yamada05b_interspeech",
    "buhler05_interspeech",
    "strangert05_interspeech",
    "nanavati05_interspeech",
    "faruquie05_interspeech",
    "rajput05_interspeech",
    "krum05_interspeech",
    "fernandez05c_interspeech"
   ]
  },
  {
   "title": "Speech Recognition in Ubiquitous Networking and Context-Aware Computing",
   "papers": [
    "pearce05_interspeech",
    "tan05_interspeech",
    "ion05_interspeech",
    "james05_interspeech",
    "skogstad05_interspeech",
    "furui05_interspeech"
   ]
  },
  {
   "title": "Speech Coding and Quality Assessment",
   "papers": [
    "takahashi05c_interspeech",
    "werner05_interspeech",
    "chen05j_interspeech",
    "fujita05_interspeech",
    "lee05e_interspeech",
    "grancharov05_interspeech"
   ]
  },
  {
   "title": "Speech Inversion",
   "papers": [
    "mokhtari05_interspeech",
    "engwall05b_interspeech",
    "sorokin05_interspeech",
    "huckvale05_interspeech",
    "potard05_interspeech",
    "toutios05_interspeech"
   ]
  },
  {
   "title": "Topics in Speech Recognition",
   "papers": [
    "liu05f_interspeech",
    "raj05_interspeech",
    "wan05b_interspeech",
    "lejeune05_interspeech",
    "bartels05_interspeech",
    "aradilla05_interspeech",
    "matsuda05_interspeech",
    "roux05_interspeech",
    "cardinal05_interspeech",
    "seid05_interspeech",
    "ogawa05_interspeech",
    "kumar05b_interspeech",
    "bartos05_interspeech",
    "li05h_interspeech",
    "razik05_interspeech",
    "ren05_interspeech",
    "ferreiros05_interspeech",
    "nakamura05b_interspeech",
    "king05_interspeech"
   ]
  }
 ],
 "doi": "10.21437/Interspeech.2005"
}