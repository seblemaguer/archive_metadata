{
 "title": "Interdisciplinary Workshop on Feedback Behaviors in Dialog",
 "location": "Skamania Lodge, Stevenson, WA, USA",
 "startDate": "7/9/2012",
 "endDate": "8/9/2012",
 "conf": "FBID",
 "year": "2012",
 "name": "fbid_2012",
 "series": "",
 "SIG": "",
 "title1": "Interdisciplinary Workshop on Feedback Behaviors in Dialog",
 "date": "7-8 September 2012",
 "booklet": "fbid_2012.pdf",
 "papers": {
  "hirschberg12_fbid": {
   "authors": [
    [
     "Julia",
     "Hirschberg"
    ]
   ],
   "title": "When do we say ‘mhmm’?: backchannel feedback in dialogue",
   "original": "fbid_001",
   "page_count": 0,
   "order": 1,
   "p1": "1",
   "pn": "",
   "abstract": [
    "In human-human dialogues, speakers regular indicate that they are attending to their conversational partner by producing oral backchannels like ‘mhmm’ and by facial and other gestures. One intriguing question for students of conversation is how speakers decide when to produce such signals. A number of us at Columbia, UPenn, the University of Buenos Aires, and Constantine the Philosopher University, have been studying the sort of acoustic-prosodic behavior of speakers which precede the production of oral backchannels by their partners to identify backchannel-preceding cues, which may give us some insight into partners’ decisions to backchannel or not. We have also been examining how backchannel behavior on the part of one partner is reflected in the backchannel behavior of the other partner in conversation, as speakers entrain on each others’ backchannel-preceding cues. I will present results of these investigations and explore their consequences for Spoken Dialogue Systems.\n",
    ""
   ]
  },
  "bodie12_fbid": {
   "authors": [
    [
     "Graham D.",
     "Bodie"
    ]
   ],
   "title": "Machines don't listen (but neither do people)",
   "original": "fbid_002",
   "page_count": 0,
   "order": 2,
   "p1": "2",
   "pn": "",
   "abstract": [
    "Listening is the capacity to discern the underlying habitual character and attitudes of people with whom we communicate. It goes beyond perception and sensation of sound and is more than the mere comprehension of another’s utterance (Bodie, Worthington, Imhof, & Cooper, 2008). At its best, listening brings about a sense of shared experience and mutual understanding through the co-creating of rules based on sharing of meaningful and conscientious dialogue (Bodie & Crick, 2012). As such, it is something humans do innately but not necessarily something we all do well. When we are “listened to” we experience a range of positive outcomes from feeling better about ourselves to improved immunological function and better psychological well-being (Bodie, 2012). When we feel misunderstood or otherwise ignored, however, our health and relationships suffer.   The importance of listening is something that we all know on an intuitive level. Perhaps that is why selfhelp gurus and academics alike hold a central place for listening in their advice for how to improve at a wide range of life tasks. Unfortunately, it is far easier to praise listening than to articulate a clear idea of just what listening is or to detail just what listeners do in order to be perceived as competent and to engender the myriad positive associated outcomes. Being a good listener is important to parenting, marital relationships, salesperson performance, customer satisfaction, and healthcare provision; and the list could go on. Good listeners can enhance others’ ability to cope with and remember events; they are more liked and garner more trust than those less proficient; and they have higher academic achievement, better socioemotional development, and a higher likelihood of upward mobility in the workplace (for review see Bodie, 2012). But what specific messages and behaviors lead to impressions of individuals as good listeners? This question has been largely ignored in the extant literature.   My colleagues and I have begun to answer this important question by building an empirical database of the attributes (what listening is) and behaviors (what listeners do) associated with effective listening in two contexts, initial interactions (Bodie et al., 2012; Bodie, St. Cyr, Pence, Rold, & Honeycutt, 2012) and supportive conversations (Bodie & Jones, in press; Bodie, Jones, & Vickery, 2012; Bodie, Vickery, & Gearhart, in press). This talk will outline those behaviors most important to perceiving others as “good” listeners in order to spur discussion about how to apply our work to contexts outside of interpersonal interaction and about the inherently interdisciplinary future of listening research. This talk will additionally posit that although research is underway by several to create “humanlike” machines, machines do not and never will, in fact, “listen.”\n",
    "s Bodie, G. D. (2012). Listening as positive communication. In T. Socha & M. Pitts (Eds.), The positive side of interpersonal communication (pp. 109-125). New York: Peter Lang. Bodie, G. D., & Crick, N. (2012). Making listening clear: Charles Sanders Peirce and the phenomenological foundations of communication. Unpublished manuscript submitted for publication. Department of Communication Studies. Louisiana State University. Baton Rouge, LA. Bodie, G. D., Worthington, D. L., Imhof, M., & Cooper, L. (2008). What would a unified field of listening look like? A proposal linking past perspectives and future endeavors. International Journal of Listening, 22, 103-122. doi: 10.1080/10904010802174867\n",
    ""
   ]
  },
  "baumann12_fbid": {
   "authors": [
    [
     "Timo",
     "Baumann"
    ]
   ],
   "title": "Feedback in adaptive interactive storytelling",
   "original": "fbid_003",
   "page_count": 2,
   "order": 3,
   "p1": "3",
   "pn": "4",
   "abstract": [
    "Telling stories is different from reading out text: a speaker reponds to the listener’s feedback and incorporates this into the ongoing talk. However, current computer systems are unable to do this and instead just non-attentively read out a text, disregarding all feedback (or the absence thereof). I propose and discuss an idea for a small research project and a plan for how an attentive listening storyteller can be built.\n",
    "Index Terms: Incrementality, Feedback, Storytelling, Adaptation, Prosody\n",
    ""
   ]
  },
  "bavelas12_fbid": {
   "authors": [
    [
     "Janet Beavin",
     "Bavelas"
    ],
    [
     "Peter De",
     "Jong"
    ],
    [
     "Harry",
     "Korman"
    ],
    [
     "Sara Smock",
     "Jordan"
    ]
   ],
   "title": "Beyond back-channels: a three-step model of grounding in face-to-face dialogue",
   "original": "fbid_005",
   "page_count": 2,
   "order": 4,
   "p1": "5",
   "pn": "6",
   "abstract": [
    "Feedback is not an individual behavior or skill; it is part of the collaborative process of grounding in which speaker and addressee coordinate their contributions to ensure mutual understanding. Based on our microanalysis of psychotherapy and experimental videos, we propose that grounding is a threestep process of observable behaviors, with traditional backchannels in the middle: The speaker presents information; the addressee displays understanding (or not understanding), and the speaker acknowledges (or corrects) the addressee’s display.\n",
    ""
   ]
  },
  "buschmeier12_fbid": {
   "authors": [
    [
     "Hendrik",
     "Buschmeier"
    ],
    [
     "Stefan",
     "Kopp"
    ]
   ],
   "title": "Adapting language production to listener feedback behaviour",
   "original": "fbid_007",
   "page_count": 4,
   "order": 5,
   "p1": "7",
   "pn": "10",
   "abstract": [
    "Listeners use linguistic feedback to provide evidence of understanding to speakers. They, in turn, use it to reason about listeners’ mental states, to determine the groundedness of communicated information and to adapt subsequent utterances to the listeners’ needs. We describe a probabilistic model for the interpretation of listener feedback in its dialogue context that enables a speaker to evaluate the listener’s mental state and gauge common ground. We then discuss levels and mechanisms of adaptation that speaker’s commonly use in reaction to listener feedback.\n",
    "Index Terms: communicative feedback; Bayesian listener state; adaptation mechanisms\n",
    ""
   ]
  },
  "chiba12_fbid": {
   "authors": [
    [
     "Yuya",
     "Chiba"
    ],
    [
     "Masashi",
     "Ito"
    ],
    [
     "Akinori",
     "Ito"
    ]
   ],
   "title": "Effect of linguistic contents on human estimation of internal state of dialog system users",
   "original": "fbid_011",
   "page_count": 4,
   "order": 6,
   "p1": "11",
   "pn": "14",
   "abstract": [
    "We have studied estimation of dialog system users’ internal state before the input utterance. In a practical use of a dialogue-based system, a user is often perplexed with the prompt. An ordinary system provides more detailed information to the user taking time to input, but the help is meddlesome for the user considering the answer to the prompt. To make an appropriate response, the spoken dialogue system needs to consider the user’s internal state before the user’s input. In the previous paper, we proposed a method for estimating the internal state using multi-modal cues; however, we did not separate effects of several factors (e.g. linguistic information of the prompt, visual information, and acoustic information). Thus, it was not clear which factor affects the evaluation of the dialog session to what extent. In this paper, we examined more detailed evaluation by human evaluators, separating linguistic contents (the system’s prompt utterance and the user’s reply utterance) from the other non-verbal behavior, and assessed the effect of the linguistic contents on the estimation of the user’s internal state.\n",
    "Index Terms: multi-modal interface, user modeling, non-verbal information\n",
    ""
   ]
  },
  "kok12_fbid": {
   "authors": [
    [
     "Iwan de",
     "Kok"
    ],
    [
     "Dirk",
     "Heylen"
    ]
   ],
   "title": "A survey on evaluation metrics for backchannel prediction models",
   "original": "fbid_015",
   "page_count": 4,
   "order": 7,
   "p1": "15",
   "pn": "18",
   "abstract": [
    "In this paper we give an overview of the evaluation metrics used to measure the performance of backchannel prediction models. Both objective and subjective evaluation metrics are discussed. The survey shows that almost every backchannel prediction model is evaluated with a different evaluation metric. This makes comparison between developed models unreliable, even beside the other variables in play, such as different corpora, language, conversational setting, amount of data and/or definition of the term backchannel.\n",
    "Index Terms: backchannel, machine learning, evaluation metrics\n",
    ""
   ]
  },
  "edlund12_fbid": {
   "authors": [
    [
     "Jens",
     "Edlund"
    ],
    [
     "Mattias",
     "Heldner"
    ],
    [
     "Anna",
     "Hjalmarsson"
    ]
   ],
   "title": "3<sup>rd</sup> party observer gaze during backchannels",
   "original": "fbid_019",
   "page_count": 4,
   "order": 8,
   "p1": "19",
   "pn": "22",
   "abstract": [
    "This paper describes a study of how the gazes of 3rd party observers of dialogue move when a speaker is taking the turn and producing a back-channel, respectively. The data is collected and basic processing is complete, but the results section for the paper is not yet in place. It will be in time for the workshop, however, and will be presented there, should this paper outline be accepted..\n",
    "Index Terms: speech synthesis, unit selection, joint costs\n",
    ""
   ]
  },
  "gargett12_fbid": {
   "authors": [
    [
     "Andrew",
     "Gargett"
    ]
   ],
   "title": "Feedback and activity in dialogue: signals or symptoms?",
   "original": "fbid_023",
   "page_count": 4,
   "order": 9,
   "p1": "23",
   "pn": "26",
   "abstract": [
    "This paper presents new approaches to modelling both linguistic and non-linguistic feedback during instruction giving in a virtual domain. Our approach enables finegrained investigation of how language and actions are conditioned by task-level and domain-level features of dialogue. In a preliminary study, we examine the interaction between pauses in linguistic and non-linguistic activity. As far as we know, ours is the first analysis of pauses across modalities. In the longer term, we aim to use these techniques as a window on the underlying processes conditioning feedback, and for such applications as the generation of situated forms of listening, such as instruction following.\n",
    "Index Terms: feedback, linguistic and actional pauses, virtual worlds\n",
    ""
   ]
  },
  "guardiola12_fbid": {
   "authors": [
    [
     "Mathilde",
     "Guardiola"
    ],
    [
     "Roxane",
     "Bertrand"
    ],
    [
     "Robert",
     "Espesser"
    ],
    [
     "Stéphane",
     "Rauzy"
    ]
   ],
   "title": "Listener's responses during storytelling in French conversation",
   "original": "fbid_027",
   "page_count": 4,
   "order": 10,
   "p1": "27",
   "pn": "30",
   "abstract": [
    "This study concerns the evolution of listeners’ production during narratives in a French conversational corpus. Using the method of Conversational analysis in a first part of the study, we show that listeners use different discursive devices throughout the narrative. In a second part, we attempt to estimate this behavior in a systematic way by measuring the richness of morphosyntactic categories. We confirm the presence of specific discursive devices as repetition or reported speech produced by listeners in the end of the narrative while only a slight tendency is observed concerning the increasing density of the richer morphosyntactic categories.\n",
    "Index Terms: back-channel, conversation analysis, storytelling, French, convergence\n",
    ""
   ]
  },
  "huang12_fbid": {
   "authors": [
    [
     "Lixing",
     "Huang"
    ],
    [
     "Jonathan",
     "Gratch"
    ]
   ],
   "title": "Crowdsourcing backchannel feedback: understanding the individual variability from the crowds",
   "original": "fbid_031",
   "page_count": 4,
   "order": 11,
   "p1": "31",
   "pn": "34",
   "abstract": [
    "During conversation, listeners often provide so-called backchannel feedback (e.g., nods and filled pauses) during their partner’s speech and these behaviors serve important interactional functions. For example, the presence of backchannels has been shown to cause increased rapport, speech fluency and speech intimacy, even when produced by computer-generated listeners. Prior work by us and others has shown that specific acoustic and visual features predict when backchannels are likely to occur, but there is also considerably individual variability not explained by such models. Here we explore a data collection framework known as Parasocial Consensus Sampling (PCS) to examine and characterize some of this individual variability. Our results indicate that common personality traits can capture much of this variability. This suggests we can build models that capture individual differences in backchannel “style” and possibly identify individual traits from observations of backchannel behavior.\n",
    "Index Terms: Backchannel, crowdsourcing\n",
    ""
   ]
  },
  "kawahara12_fbid": {
   "authors": [
    [
     "Tatsuya",
     "Kawahara"
    ],
    [
     "Takuma",
     "Iwatate"
    ],
    [
     "Takanori",
     "Tsuchiya"
    ],
    [
     "Katsuya",
     "Takanashi"
    ]
   ],
   "title": "Can we predict who in the audience will ask what kind of questions with their feedback behaviors in poster conversation?",
   "original": "fbid_035",
   "page_count": 4,
   "order": 12,
   "p1": "35",
   "pn": "38",
   "abstract": [
    "We investigate feedback behaviors in conversations in poster sessions, specifically whether it is possible to predict who in the audience will ask questions, and also what kind of questions. We focus on verbal backchannels and non-verbal noddings by the audience as well as joint eye-gaze events by the presenter and the audience. We first show how these patterns are correlated with turntaking by the audience. Then, questions made by the audience are classified into two kinds: confirming questions and substantive questions. It is suggested that only verbal backchannels are useful for distinguishing them.\n",
    ""
   ]
  },
  "kousidis12_fbid": {
   "authors": [
    [
     "Spyros",
     "Kousidis"
    ],
    [
     "Thies",
     "Pfeiffer"
    ],
    [
     "Zofia",
     "Malisz"
    ],
    [
     "Petra",
     "Wagner"
    ],
    [
     "David",
     "Schlangen"
    ]
   ],
   "title": "Evaluating a minimally invasive laboratory architecture for recording multimodal conversational data",
   "original": "fbid_039",
   "page_count": 4,
   "order": 13,
   "p1": "39",
   "pn": "42",
   "abstract": [
    "This paper presents ongoing work on the design, deployment and evaluation of a multimodal data acquisition architecture which utilises minimally invasive motion, head, eye and gaze tracking alongside high-quality audiovisual recording of human interactions. The different data streams are centrally collected and visualised at a single point and in real time by means of integration in a virtual reality (VR) environment. The overall aim of this endeavour is the implementation of a multimodal data acquisition facility for the purpose of studying non-verbal phenomena such as feedback gestures, hand and pointing gestures and multi-modal alignment. In the first part of this work that is described here, a series of tests were performed in order to evaluate the feasibility of tracking feedback head gestures using the proposed architecture.\n",
    "Index Terms: Multimodal interaction, feedback, virtual reality\n",
    ""
   ]
  },
  "lundholmfors12_fbid": {
   "authors": [
    [
     "Kristina",
     "Lundholm Fors"
    ]
   ],
   "title": "The temporal relationship between feedback and pauses: a pilot study",
   "original": "fbid_043",
   "page_count": 3,
   "order": 14,
   "p1": "43",
   "pn": "45",
   "abstract": [
    "In this pilot study we investigated the temporal relationship between pauses and feedback. We found that the majority of feedback items occur in the proximity of trppauses (pauses that occur at a trp, within a speaker’s turn), but also that most intraturn pauses do not coincide with feedback units. This suggests that when modeling feedback in human-computer interaction, a method to identify trp-pauses will also provide suitable places for feedback.\n",
    "Index Terms: feedback, back-channels, pauses\n",
    ""
   ]
  },
  "neiberg12_fbid": {
   "authors": [
    [
     "Daniel",
     "Neiberg"
    ],
    [
     "Joakim",
     "Gustafson"
    ]
   ],
   "title": "Cues to perceived functions of acted and spontaneous feedback expressions",
   "original": "fbid_046",
   "page_count": 4,
   "order": 15,
   "p1": "46",
   "pn": "49",
   "abstract": [
    "We present a two step study where the first part aims to determine the phonemic prior bias (conditioned on “ah”, “m-hm”, “m-m”, “n-hn”, “oh”, “okay”, “u-hu”, “yeah” and “yes”) in subjects perception of six feedback functions (acknowledgment, continuer, disagreement, surprise, enthusiasm and uncertainty). The results showed a clear phonemic prior bias for some tokens, e.g “ah” and “oh” is commonly interpreted as surprise but “yeah” and “yes” less so. The second part aims to examine determinants to judged typicality, or graded structure, within the six functions of “okay”. Typicality was correlated to four determinants: prosodic central tendency within the function (CT); phonemic prior bias as an approximation to frequency instantiation (FI), the posterior i.e. CT x FI and judged Ideality (ID), i.e. similarity to ideals associated with the goals served by its function. The results tentatively suggests that acted expressions are more effectively communicated and that the functions of feedback to a greater extent constitute goal-based categories determined by ideals and to a lesser extent a taxonomy determined by CT and FI. However, it is possible to automatically predict typicality with a correlation of r=0.52 via the posterior.\n",
    "Index Terms: feedback, functions of feedback, goal driven categories, taxonomy\n",
    ""
   ]
  },
  "neiberg12b_fbid": {
   "authors": [
    [
     "Daniel",
     "Neiberg"
    ],
    [
     "Joakim",
     "Gustafson"
    ]
   ],
   "title": "Exploring the implications for feedback of a neurocognitive theory of overlapped speech",
   "original": "fbid_050",
   "page_count": 4,
   "order": 16,
   "p1": "50",
   "pn": "53",
   "abstract": [
    "Neurocognitive evidence suggests that the cognitive load caused by decoding interlocutors speech while one self is talking is dependent on two factors: the type of incoming speech, i.e. nonlexical feedback, lexical feedback or non-feedback; and the duration of the speech segment. This predicts that the fraction of overlap should be high for non-lexical feedback, medium for lexical feedback and low for non-feedback, and that short segments has a higher fraction of overlapped speech than long segments. By normalizing for duration, it is indeed shown that the fraction of overlap is 32% for non-lexical feedback, 27% for lexical feedback and 12% for non-feedback. Investigating nonfeedback tokens for the durational factor gives that the fraction of overlap can be modeled by linear regression and logarithmic transform of duration giving a R2=0.57 (p<0.01 for F-test) and a slope b(2)=-0.04 (p<0.01 for T-test). However, it is not enough to take duration into account when modeling overlap in feedback tokens.\n",
    "Index Terms: neurocognitive theory, feedback, lexical feedback, non-lexical feedback\n",
    ""
   ]
  },
  "novick12_fbid": {
   "authors": [
    [
     "David G.",
     "Novick"
    ]
   ],
   "title": "Paralinguistic behaviors in dialog as a continuous process",
   "original": "fbid_054",
   "page_count": 4,
   "order": 17,
   "p1": "54",
   "pn": "57",
   "abstract": [
    "Prior research on gaze, turn-taking, and backchannels suggests that the speaker’s gaze cues the listener’s paralinguistic responses, including feedback behaviors. To explore how conversants use feedback cues and responses, I studied a corpus of face-to-face conversational interaction, primarily using a conversation-analytic approach. Analysis of the dialogs suggests that paralinguistic behaviors express meaning at a level of granularity often smaller than dialog control acts. Behaviors such as gaze and nodding can be seen as continuous rather than discrete actions. Moreover, speaker gaze shift toward the listener is a polysemous expression that can cue a range of behaviors in the listener, including continued attention, head nods as backchannels, utterances as backchannels, and turn-taking. The analysis also suggests that gaze, from both speakers and listeners, can express a state rather than a discrete act.\n",
    "Index Terms: dialog, grounding, feedback, gaze, nod\n",
    ""
   ]
  },
  "plant12_fbid": {
   "authors": [
    [
     "Nicola",
     "Plant"
    ],
    [
     "Patrick G. T.",
     "Healey"
    ]
   ],
   "title": "Empathy and feedback in conversations about felt experience",
   "original": "fbid_058",
   "page_count": 5,
   "order": 18,
   "p1": "58",
   "pn": "62",
   "abstract": [
    "When we talk about felt experiences, such as physical pains and pleasures, we normally expect our conversational partners to provide empathetic feedback of some kind. Some models of human interaction predict that this feedback should be similar in form to our original production; the gestures, expressions and other non-verbal signals we use to explain our experience should be mirrored in the empathic displays of our conversational partners. Here, we test this idea using data from a corpus of interactions in which people describe experiences that vary in their degree of unpleasantness. Speakers in this situation produce more gestures when describing more unpleasant experiences. In contrast to this, their listeners provide less non-verbal feedback and use more verbal feedback as the expressed experience becomes more negative. These findings suggest a socially strategic use of emphatic feedback that is not explained by the operation of an automatic perception-behaviour link.\n",
    "Index Terms: empathetic feedback, motor mimicry, perception-behaviour link, imitation\n",
    ""
   ]
  },
  "prevot12_fbid": {
   "authors": [
    [
     "Laurent",
     "Prévot"
    ],
    [
     "Roxane",
     "Bertrand"
    ]
   ],
   "title": "Cofee - toward a multidimensional analysis of conversational feedback, the case of French language",
   "original": "fbid_063",
   "page_count": 4,
   "order": 19,
   "p1": "63",
   "pn": "66",
   "abstract": [
    "Conversational feedback is mostly performed through short utterances such as yeah, mhmm, okay not produced by the main speaker but by one of the other participants of a con- versation. Such utterances are among the most frequent in conversational data. They also have been described in psycho-linguistic models of communication as a crucial communicative tool for achieving coordination or align- ment in dialogue. The newly funded project described in this paper addresses this issue from a linguistic view- point by combining fine-grained corpus linguistic anal- yses of semi-controlled data with formal and statistical modeling. The impoverished aspect of the linguistic ma- terial present in these utterances allows for a truly multi- dimensional analysis that can explain how different lin- guistic domains combine to convey meaning and achieve communicative goals.\n",
    "Index Terms: Feedback, Backchannel, Semantics, Prag- matics, French Language\n",
    ""
   ]
  },
  "scherer12_fbid": {
   "authors": [
    [
     "Stefan",
     "Scherer"
    ],
    [
     "Derya",
     "Ozkan"
    ],
    [
     "Louis-Philippe",
     "Morency"
    ]
   ],
   "title": "Investigating the influence of pause fillers for automatic backchannel prediction",
   "original": "fbid_067",
   "page_count": 2,
   "order": 20,
   "p1": "67",
   "pn": "68",
   "abstract": [
    "Hesitations, and pause fillers (e.g. “um”, “uh”), occur frequently in everyday conversations or monologues. They can be observed for a wide range of reasons including: lexical access, structuring of utterances, and requesting feedback from the listener. In this study we investigate the usefulness of pause fillers as a feature for the prediction of backchannels using conditional random fields (CRF) within a large corpus of interactions.\n",
    ""
   ]
  },
  "skantze12_fbid": {
   "authors": [
    [
     "Gabriel",
     "Skantze"
    ]
   ],
   "title": "A testbed for examining the timing of feedback using a map task",
   "original": "fbid_069",
   "page_count": 4,
   "order": 21,
   "p1": "69",
   "pn": "72",
   "abstract": [
    "In this paper, we present a fully automated spoken dialogue system that can perform the Map Task with a user. By implementing a trick, the system can convincingly act as an attentive listener, without any speech recognition. An initial study is presented where we let users interact with the system and recorded the interactions. Using this data, we have then trained a Support Vector Machine on the task of identifying appropriate locations to give feedback, based on automatically extractable prosodic and contextual features. 200 ms after the end of the user’s speech, the model may identify response locations with an accuracy of 75%, as compared to a baseline of 56.3%.\n",
    ""
   ]
  },
  "stoyanchev12_fbid": {
   "authors": [
    [
     "Svetlana",
     "Stoyanchev"
    ],
    [
     "Alex",
     "Liu"
    ],
    [
     "Julia",
     "Hirschberg"
    ]
   ],
   "title": "Clarification questions with feedback",
   "original": "fbid_073",
   "page_count": 4,
   "order": 22,
   "p1": "73",
   "pn": "76",
   "abstract": [
    "In this paper, we investigate how people construct clarification questions. Our goal is to develop similar strategies for handling errors in automatic spoken dia- logue systems in order to make error recovery strategies more efficient. Using a crowd-sourcing tool [7], we col- lect a dataset of user responses to clarification questions when presented with sentences in which some words are missing. We find that, in over 60% of cases, users choose to continue the conversation without asking a clarification question. However, when users do ask a question, our findings support earlier research showing that users are more likely to ask a targeted clarification question than a generic question. Using the dataset we have collected, we are exploring machine learning approaches for determin- ing which system responses are most appropriate in dif- ferent contexts and developing strategies for constructing clarification questions.\n",
    "Index Terms: clarification, question\n",
    ""
   ]
  },
  "trouvain12_fbid": {
   "authors": [
    [
     "Jürgen",
     "Trouvain"
    ],
    [
     "Khiet P.",
     "Truong"
    ]
   ],
   "title": "Acoustic, morphological, and functional aspects of “yeah/ja” in Dutch, English and German",
   "original": "fbid_077",
   "page_count": 4,
   "order": 23,
   "p1": "77",
   "pn": "80",
   "abstract": [
    "We explore different forms and functions of one of the most common feedback expressions in Dutch, English, and German, namely “yeah/ja” which is known for its multi-functionality and ambiguous usage in dialog. For example, it can be used as a yes-answer, or as a pure continuer, or as a way to show agreement. In addition, “yeah/ja” can be used in its single form, but it can also be combined with other particles, forming multi-word expressions, especially in Dutch and German. We have found substantial differences on the morpho-lexical level between the three related languages which enhances the ambiguous character of “yeah/ja”. An explorative analysis of the prosodic features of “yeah/ja” has shown that mainly a higher intensity is used to signal speaker incipiency across the inspected languages.\n",
    "Index Terms: feedback, yeah, ja, dialog act, prosody, crosslinguistic, speaker incipiency\n",
    ""
   ]
  },
  "ward12_fbid": {
   "authors": [
    [
     "Nigel G.",
     "Ward"
    ]
   ],
   "title": "Possible lexical cues for backchannel responses",
   "original": "fbid_081",
   "page_count": 4,
   "order": 24,
   "p1": "81",
   "pn": "84",
   "abstract": [
    "Looking for words that might cue backchannel feedback, I did a statistical analysis of the interlocutors’ words preceding 3363 instances of uh-huh in the Switchboard corpus. No clear cueing words were found, but collateral findings include the existence of semantic classes that slightly increase the likelihood of an upcoming uh-huh, the fact that different classes have their effects at different time lags, and the existence of words which strongly counter-indicate a subsequent uh-huh.\n",
    "Index Terms: uh-huh, feedback, elicitors, temporal distributional analysis, dialog dynamics\n",
    ""
   ]
  },
  "ward12b_fbid": {
   "authors": [
    [
     "Nigel G.",
     "Ward"
    ],
    [
     "Joshua L.",
     "McCartney"
    ]
   ],
   "title": "Visualizations supporting the discovery of prosodic contours related to turn-taking",
   "original": "fbid_085",
   "page_count": 4,
   "order": 25,
   "p1": "85",
   "pn": "88",
   "abstract": [
    "Some meaningful prosodic patterns can be usefully represented with pitch contours, however developing such descriptions is a labor-intensive process. To assist in the discovery of contour representations, visualization tools may be helpful. Edlund et al. [1] proposed the superimposition of hundreds of pitch curves from a corpus to reveal the general patterns. In this paper we refine and extend this method, and illustrate its utility in the discovery of a prosodic cue for back-channels in Chinese.\n",
    "Index Terms: prosodic cue, tune, turn-taking, back-channel, Chinese, bitmap cluster, overlay, superimpose\n",
    "",
    "",
    "J. Edlund, M. Heldner, and A. Pelcé, “Prosodic features of very short utterances in dialogue,” in Nordic Prosody - Proceedings of the Xth Conference, pp. 56–68, 2009.\n",
    ""
   ]
  },
  "ward12c_fbid": {
   "authors": [
    [
     "Nigel G.",
     "Ward"
    ],
    [
     "David G.",
     "Novick"
    ],
    [
     "Alejandro",
     "Vega"
    ]
   ],
   "title": "Where in dialog space does <i>uh-huh</i> occur?",
   "original": "fbid_089",
   "page_count": 4,
   "order": 26,
   "p1": "89",
   "pn": "92",
   "abstract": [
    "In what dialog situations and contexts do backchannels commonly occur? This paper examines this question using a newly developed notion of dialog space, defined by orthogonal, prosody-derived dimensions. Taking 3363 instances of uh-huh, found in the Switchboard corpus, we examine where in this space they tend to occur. While the results largely agree with previous descriptions and observations, we find several novel aspects, relating to rhythm, polarity, and the details of the low-pitch cue.\n",
    "Index Terms: backchannels, feedback, prosody, context, principal component analysis, dimensions, dialog activities\n",
    ""
   ]
  },
  "wodarczak12_fbid": {
   "authors": [
    [
     "Marcin",
     "Włodarczak"
    ],
    [
     "Hendrik",
     "Buschmeier"
    ],
    [
     "Zofia",
     "Malisz"
    ],
    [
     "Stefan",
     "Kopp"
    ],
    [
     "Petra",
     "Wagner"
    ]
   ],
   "title": "Listener head gestures and verbal feedback expressions in a distraction task",
   "original": "fbid_093",
   "page_count": 4,
   "order": 27,
   "p1": "93",
   "pn": "96",
   "abstract": [
    "We report on the functional and timing relations between head movements and the overlapping verbal-vocal feedback expressions. We investigate the effect of a distraction task on head gesture behaviour and the co-occurring verbal feedback. The results show that head movements overlapping with verbal expressions in a distraction task differ in terms of several features from a default, non-perturbed conversational situations, e.g.: frequency and type of movement and verbal to nonverbal display ratios.\n",
    "Index Terms: communicative feedback; head gestures; dialogue; attentiveness; distraction task\n",
    ""
   ]
  }
 },
 "sessions": [
  {
   "title": "Keynote Papers",
   "papers": [
    "hirschberg12_fbid",
    "bodie12_fbid"
   ]
  },
  {
   "title": "Regular Papers",
   "papers": [
    "baumann12_fbid",
    "bavelas12_fbid",
    "buschmeier12_fbid",
    "chiba12_fbid",
    "kok12_fbid",
    "edlund12_fbid",
    "gargett12_fbid",
    "guardiola12_fbid",
    "huang12_fbid",
    "kawahara12_fbid",
    "kousidis12_fbid",
    "lundholmfors12_fbid",
    "neiberg12_fbid",
    "neiberg12b_fbid",
    "novick12_fbid",
    "plant12_fbid",
    "prevot12_fbid",
    "scherer12_fbid",
    "skantze12_fbid",
    "stoyanchev12_fbid",
    "trouvain12_fbid",
    "ward12_fbid",
    "ward12b_fbid",
    "ward12c_fbid",
    "wodarczak12_fbid"
   ]
  }
 ]
}