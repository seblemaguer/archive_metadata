{
 "location": "Vienna, Austria",
 "startDate": "11/9/2015",
 "endDate": "13/9/2015",
 "conf": "AVSP",
 "year": "2015",
 "name": "avsp_2015",
 "series": "AVSP",
 "SIG": "AVISA",
 "title": "Auditory-Visual Speech Processing",
 "title1": "Auditory-Visual Speech Processing",
 "date": "11-13 September 2015",
 "booklet": "avsp_2015.pdf",
 "papers": {
  "helzle15_avsp": {
   "authors": [
    [
     "Volker",
     "Helzle"
    ]
   ],
   "title": "An artistic and tool-driven approach for believable digital characters",
   "original": "av15_401",
   "page_count": 0,
   "order": 1,
   "p1": "0",
   "pn": "",
   "abstract": [
    "This talk focuses on the practical tools developed at the Filmakademie for the creation of believable digital characters. We will discuss solutions that have been implemented to achieve realistic and physically plausible facial deformations during a short setup time. We will also look into new applications that made use of these characters like the cloud based animated messaging service for mobile devices (Emote), an interactive installation where animated characters recite poetry in an emotional way, or the approach we are taking to use stylized animated faces in the research on Autism.\n",
    "Volker Helzle is in charge of Research and Development at the Institute of Animation at Filmakademie Baden-Württemberg. After graduating from HDM Stuttgart Media University (Dipl. Ing. AV-Medien) in 2000 he moved to California and for three years worked at Eyematic Interfaces (later acquired by Google) where his team pioneered facial performance capture substantially contributing to the engineering and development of the Eyematic Facestation. In 2003 he joined Filmakademie where he supervises the research and development department at the Institute of Animation. The primary focus of his first few years at Filmakademie has been the development of facial animation tools. This led to one of the first plausible technology tests, realizing a virtual actor in an exemplary VFX production. In addition to the technical research Volker is supervising the curriculum for the postgraduate Technical Director (TD) course at Filmakademie. TDs tackle the technological challenges of Animation, VFX and Transmedia productions at Filmakademie. The close relation to the research group allows students to engage in multidisciplinary projects. As a program consultant he contributes to the organization of the annual FMX conference. Being a C-64 kid of the 80ties, Volker's life was strongly influenced by video games and early computer graphics. To this day he is a passionate gamer but also finds interest in completely analogical activities like mountain hiking, gardening or yoga.\n",
    ""
   ]
  },
  "costaorvalho15_avsp": {
   "authors": [
    [
     "Verónica",
     "Costa Orvalho"
    ]
   ],
   "title": "How to create a look-a-like avatar pipeline using low-cost equipment",
   "original": "av15_402",
   "page_count": 0,
   "order": 2,
   "p1": "0",
   "pn": "",
   "abstract": [
    "Creating a 3D avatar that looks like a specific person is time-consuming, requires expert artists, expensive equipment and a complex pipeline. In this talk I will walk you through the avatar animation pipeline created at PIC (Porto Interactive Center, www.portointeractivecenter.org) for the VERE (Virtual Embodiment and Robotic re-Embodiment, http://www.vereproject.eu/) European Project. This new pipeline does not require the user to have artistic knowledge, uses regular cameras to create the 3D avatar and a web cam to generate the animation. In this talk i will explain how we designed and created the look-a-like system at each stage: modelling, rigging and animation. I will also describe the challenge we had to overcome and the current status of the system. I will show some of our current avatar results, which could be used for example in games, interactive applications and virtual reality. I look forward to seeing you at the talk!\n",
    "Verónica Costa Orvalho holds a Ph.D in Software Development (Computer Graphics) from Universitat Politécnica de Catalunya (2007), where her research centred on \"Facial Animation for CG Films and Videogames\". She has been working in IT companies for the past 15 years, such as IBM and Ericsson, and Film companies, including Patagonik Film Argentina. She has given many workshops and has international publications related to game design and character animation in conferences such as SIGGRAPH. She has received international awards for several projects: \"Photorealistic facial animation and recognition\", \"Face Puppet\" and \"Face In Motion\". She has received the 2010 IBM Scientific Award for her work of facial rig retargeting. Now, she is a full time professor of Porto University. In 2010 she founded Porto Interactive Center (www.portointeractivecenter.org) at Porto University, which is the host of several International and national projects as project coordinator or participant. She has strong connections with the film and game companies and provided consulting and participated in several productions like Fable 2, The Simpsons Ride. She has current and past close collaboration with film and game companies such as: Blur Studios, Electronic Arts and Microsoft. Her main research interests are in developing new methods related to motion capture, geometric modeling and deformation, facial emotion synthesis and analysis, real time animation for virtual environments and the study of intelligent avatars.\n",
    ""
   ]
  },
  "schwartz15_avsp": {
   "authors": [
    [
     "Jean-Luc",
     "Schwartz"
    ]
   ],
   "title": "Audiovisual binding in speech perception",
   "original": "av15_403",
   "page_count": 0,
   "order": 3,
   "p1": "0",
   "pn": "",
   "abstract": [
    "We have been elaborating in the last years in Grenoble a series of experimental works in which we attempt to show that audiovisual speech perception comprises an \"audiovisual binding\" stage before fusion and decision. This stage would be in charge to extract and associate the auditory and visual cues corresponding to a given speech source, before further categorisation processes could take place at a higher stage. We developed paradigms to characterize audiovisual binding in terms of both \"streaming\" and \"chunking\" adequate pieces of information. This can lead to elements of a possible computational model, in relation with a larger theoretical perceptuo-motor framework for speech perception, the \"Perception-for-Action-Control\" Theory.\n",
    "Jean-Luc Schwartz, Research Director at CNRS, has been leading ICP (Institut de la Communication Parlée, Grenoble France) from 2003 to 2006. His main areas of research involve perceptual processing, perceptuo-motor interactions, audiovisual speech perception, phonetic bases of phonological systems and the emergence of language, with publications in cognitive psychology (e.g. Cognition, Perception & Psychophysics, Behavioral & Brain Sciences, Hearing Research), neurosciences (e.g. Neuroimage or Human Brain Mapping), signal processing and computational modelling (e.g. IEEE Trans. Speech and Audio Processing, JASA, C omputer Speech and Language, Language and Cognitive Processes), and phonetics in relation with phonology (e.g. Journal of Phonetics or Phonology Laboratory). He has been involved in many national and European projects, and responsible of some of them. He coordinated a number of special issues of journals such as Speech Communication, Primatology, Philosophical Transactions of the Royal Society B, Frontiers in Psychology, Journal of Phonetics. He organized several international workshops on Audiovisual Speech Processing, Language Emergence or Face-to-Face Communication.\n",
    ""
   ]
  },
  "soong15_avsp": {
   "authors": [
    [
     "Frank",
     "Soong"
    ],
    [
     "Lijuan",
     "Wang"
    ]
   ],
   "title": "From text-to-speech (TTS) to talking head - a machine learning approach to A/V speech modeling and rendering.",
   "original": "av15_404",
   "page_count": 0,
   "order": 4,
   "p1": "0",
   "pn": "",
   "abstract": [
    "In this talk, we will present our research results in A/V speech modeling and rendering via a statistical, machine learning approach. A Gaussian Mixture Model (GMM) based Hidden Markov Model (HMM) will be reviewed first in speech modeling where GMM is for modeling the stochastic nature of speech production while HMM, for characterizing the Markovian nature of speech parameter trajectories. All speech parametric models are estimated via an EM algorithm based maximum likelihood procedure and the resultant models are used to generate speech parameter trajectories for a given text input, say a sentence, in the maximum probability sense. Thus generated parameters is then used to synthesize corresponding speech waveforms via a vocoder or to render high quality output speech by our \"trajectory tiling algorithm\" where appropriate segments of the training speech database are used to \"tile\" the generated trajectory optimally. Similarly, the lips movement of a talking head, along with the jointly moving articulatory parts like jaw, tongue and teeth, can also be trained and rendered according to the optimization procedure. The visual parameters of a talking head can be collected via 2D- or 3D-video (via stereo, multi-camera recording equipment or consumer grade, capturing devices like Microsoft Kinect) and the corresponding visual trajectories of intensity, color and spatial coordinates are modeled and synthesized similarly. Recently, feedforward Deep Neural Net (DNN) and Recurrent Neural Net machine learning algorithms have been applied to speech modeling for both recognition and synthesis applications. We have deployed both forms of neural nets in TTS training successfully. The RNN, particularly, with a longer memory can model speech prosody of longer contexts in speech, say in a sentence, better. We will also cover the topics of cross-lingual TTS and talking head modeling, where audio and visual data collected in one source language can be used to train a TTS or talking head in a different target language. The mouth shapes of a mono-lingual speaker have also been found adequate for rendering synced lips movement of talking heads in different languages. Various demos of TTS and talking head will be shown to illustrate our research findings.\n",
    "Frank K. Soong is a Principal Researcher and Research Manager, Speech Group, Microsoft Research Asia (MSRA), Beijing, China, where he works on fundamental research on speech and its practical applications. His professional research career spans over 30 years, first with Bell Labs, US, then with ATR, Japan, before joining MSRA in 2004. At Bell Labs, he worked on stochastic modeling of speech signals, optimal decoder algorithm, speech analysis and coding, speech and speaker recognition. He was responsible for developing the recognition algorithm which was developed into voice-activated mobile phone products rated by the Mobile Office Magazine (Apr. 1993) as the \"outstandingly the best\". He is a co-recipient of the Bell Labs President Gold Award for developing the Bell Labs Automatic Speech Recognition (BLASR) software package. He has served as a member of the Speech and Language Technical Committee, IEEE Signal Processing Society and other society functions, including Associate Editor of the IEEE Speech and Audio Transactions and chairing IEEE Workshop. He published extensively with more than 200 papers and co-edited a widely used reference book, Automatic Speech and Speech Recognition- Advanced Topics, Kluwer, 1996. He is a visiting professor of the Chinese University of Hong Kong (CUHK) and a few other top-rated universities in China. He is also the co-Director of the National MSRA-CUHK Joint Research Lab. He got his BS, MS and PhD from National Taiwan Univ., Univ. of Rhode Island, and Stanford Univ, all in Electrical Eng. He is an IEEE Fellow \"for contributions to digital processing of speech\".\n",
    "Lijuan Wang received B.E. from Huazhong Univ. of Science and Technology and Ph.D. from Tsinghua Univ., China in 2001 and 2006 respectively. In 2006, she joined the speech group of Microsoft Research Asia, where she is currently a lead researcher. Her research areas include audio-visual speech synthesis, deep learning (feedforward and recurrent neural networks), and speech synthesis (TTS)/recognition. She has published more than 25 papers on top conferences and journals and she is the inventor/co-inventor of more than 10 granted/pending USA patents. She is a senior member of IEEE and a member of ISCA.\n",
    ""
   ]
  },
  "visser15_avsp": {
   "authors": [
    [
     "Mandy",
     "Visser"
    ],
    [
     "Emiel",
     "Krahmer"
    ],
    [
     "Marc",
     "Swerts"
    ]
   ],
   "title": "Children’s spontaneous emotional expressions while receiving (un)wanted prizes in the presence of peers",
   "original": "av15_001",
   "page_count": 6,
   "order": 5,
   "p1": "1",
   "pn": "6",
   "abstract": [
    "In this research, we studied the course of emotional expressions of 8- and 11-year-old children after winning a (large) first prize or a (substantially smaller) consolation prize, while playing a game competing the computer or a physically co-present peer. We analyzed their emotional reactions by conducting two perception tests in which participants rated children’s level of happiness. Results showed that co-presence positively affected children’s happiness only when receiving the first prize. Moreover, for children who were in the presence of a peer, we found that eye contact affected expressions of happiness of 8-year-old children negatively and that of 11-year-old children positively. Finally, this study showed that having eye contact with their co-present peer affected children’s emotional expressions. Overall, we can conclude that, as children grow older and their social awareness increases, the presence of a peer affects their nonverbal expressions, regardless of their appreciation of their prize.\n",
    "Index Terms: nonverbal emotional expressions, contextual factors, social presence, development, (re)appraisals.\n",
    ""
   ]
  },
  "fort15_avsp": {
   "authors": [
    [
     "Mathilde",
     "Fort"
    ],
    [
     "Anira",
     "Escrichs"
    ],
    [
     "Alba",
     "Ayneto-Gimeno"
    ],
    [
     "Núria",
     "Sebastián-Gallés"
    ]
   ],
   "title": "You can raise your eyebrows, I don’t mind: are monolingual and bilingual infants equally good at learning from the eyes region of a talking face?",
   "original": "av15_007",
   "page_count": 5,
   "order": 6,
   "p1": "7",
   "pn": "11",
   "abstract": [
    "In this study we investigate whether paying attention to a speaker’s mouth impacts 15- and 18-month-old infants’ ability to process visual information displayed in the talker’s eyes or mouth region. Our results showed that both monolingual and bilingual 15 month-olds could detect the apparition of visual information appearing in the eyes/mouth region but only 15-month-old monolinguals and 18-month-old bilinguals could learn to anticipate its appearance in the eyes region. Overall, we demonstrate that specific language constrains (i.e., bilingualism) not only influences how infants selectively deploy their attention to different region of human faces, but also impact their ability to learn from them.\n",
    "Index Terms: attention, eyes, mouth, talking faces, early language acquisition, bilingualism, infancy\n",
    ""
   ]
  },
  "parocosta15_avsp": {
   "authors": [
    [
     "Paula D.",
     "Paro Costa"
    ],
    [
     "Daniella",
     "Batista"
    ],
    [
     "Mayara",
     "Toffoli"
    ],
    [
     "Keila A.",
     "Baraldi Knobel"
    ],
    [
     "Cíntia",
     "Alves Salgado"
    ],
    [
     "José Mario De",
     "Martino"
    ]
   ],
   "title": "Comparison of visual speech perception of sampled-based talking heads: adults and children with and without developmental dyslexia",
   "original": "av15_012",
   "page_count": 5,
   "order": 7,
   "p1": "12",
   "pn": "16",
   "abstract": [
    "Among other applications, videorealistic talking heads are envisioned as a programmable tool to train skills which involve the observation of human face. This work presents partial results of a study conducted with adults and children with and without developmental dyslexia to evaluate the level of speech intelligibility provided by a sample-based talking head model in comparison with unimodal auditory and real video stimuli. The results obtained so far indicate that dyslexic children, when compared to control group, are less capable of dealing with the imperfections of a synthetic facial animation visual speech model. The present study is motivated by the hypothesis that a training program using facial animation stimuli could improve the phoneme awareness of dyslexic children, a skill that is considered significantly related to success in the early stages of reading and spelling.\n",
    "Index Terms: dyslexia, facial animation, speech intelligibility, assistive technology\n",
    ""
   ]
  },
  "simonetti15_avsp": {
   "authors": [
    [
     "Simone",
     "Simonetti"
    ],
    [
     "Jeesun",
     "Kim"
    ],
    [
     "Chris",
     "Davis"
    ]
   ],
   "title": "Cross-modality matching of linguistic prosody in older and younger adults",
   "original": "av15_017",
   "page_count": 5,
   "order": 8,
   "p1": "17",
   "pn": "21",
   "abstract": [
    "Older adults perform worse than younger adults in recognising auditory linguistic prosody. Such problems may result from deterioration at the sensory level (e.g., hearing loss). The current study used a novel approach to examine this, by determining older adult’s performance on a visual prosody task. If older adults are able to process visual prosody this suggests that any difficulty they show when processing auditory linguistic expressions could be related to hearing loss. The current study presented 18 younger and 11 older adults with pairs of sentences spoken by the same talker. They decided whether the pair contained the same or different prosody in a simple AX matching task. Sentence pairs were presented in four different ways; auditory-auditory (AA), visual-visual (VV), audio-visual (AV), and visual-audio (VA). Compared to older adults, younger adults exhibited similar performance for focused statements but showed better performance for questions. We suggest that problems processing questioning expressions might result from hearing loss, problems perceiving pitch, or problems at the cognitive level.\n",
    ""
   ]
  },
  "huyse15_avsp": {
   "authors": [
    [
     "Aurélie",
     "Huyse"
    ],
    [
     "Frédéric",
     "Berthommier"
    ],
    [
     "Jacqueline",
     "Leybaert"
    ]
   ],
   "title": "“I don't see what you are saying”: reduced visual influence on audiovisual speech integration in children with Specific Language Impairment",
   "original": "av15_022",
   "page_count": 6,
   "order": 9,
   "p1": "22",
   "pn": "27",
   "abstract": [
    "The impact of language impairment on audio-visual integration of speech in noise is examined here by testing the influence of the degradation of the auditory and the visual speech cue. Fourteen children with specific language impairment (SLI) and 14 age-matched children with typical language development (TLD) had to identify /aCa/ syllables presented in auditory only (AO), visual only (VO) and audiovisual (AV) congruent and incongruent (McGurk stimuli) conditions, embedded either in stationary noise (ST) or amplitude modulated noise (AM), in a masking release paradigm. Visual cues were either reduced (VR) or clear (VCL). In the AO modality, children with SLI had poorer performance than TLD children in AM noise but not in ST noise, leading to a weaker masking release effect. In VO modality, children with SLI had weaker performance both in VCL and VR conditions. Analyses revealed reduced AV gains in children with SLI compared to control children. In the McGurk trials, SLI children showed a decreased influence of visual cues on AV perception in the SLI group compared to the TLD group. Data analysis in the framework of the Fuzzy- Logical Model of Perception suggested that children with SLI had preserved integration abilities; the differences with TLD children were rather due to differences in the unisensory modalities. An increased weight of audition in the VR condition compared to the VCL condition was observed in both groups, suggesting that participants awarded more weight to audition when the visual input was degraded.\n",
    "Index Terms: multisensory speech perception, spe\n",
    ""
   ]
  },
  "best15_avsp": {
   "authors": [
    [
     "Catherine T.",
     "Best"
    ],
    [
     "Christian H.",
     "Kroos"
    ],
    [
     "Karen E.",
     "Mulak"
    ],
    [
     "Shaun",
     "Halovic"
    ],
    [
     "Mathilde",
     "Fort"
    ],
    [
     "Christine",
     "Kitamura"
    ]
   ],
   "title": "Message vs. messenger effects on cross-modal matching for spoken phrases",
   "original": "av15_028",
   "page_count": 6,
   "order": 10,
   "p1": "28",
   "pn": "33",
   "abstract": [
    "A core issue in speech perception and word recognition research is the nature of information perceivers use to identify spoken utterances across indexical variations in their phonetic details, such as talker and accent differences. Separately, a crucial question in audio-visual research is the nature of information perceivers use to recognize phonetic congruency between the audio and visual (talking face) signals that arise from speaking. We combined these issues in a study examining how differences between connected speech utterances (messages) versus between talkers and accents (messenger characteristics) contribute to recognition of crossmodal articulatory 2congruence between audio-only (AO) and video-only (VO) components of spoken utterances. Participants heard AO phrases in their native regional English accent or another English accent, and then saw two synchronous VO displays of point-light talking faces from which they had to select the one that corresponded to the audio target. The incorrect video in each pair was either the same or a different phrase as the audio target, produced by the same or a different talker, who spoke in either the same or a different English accent. Results indicate that cross-modal articulatory correspondence is more accurately and quickly detected for message content than for messenger details, suggesting that recognising the linguistic message is more fundamental than messenger features is to cross-modal detection of audio-visual articulatory congruency. Nonetheless, messenger characteristics, especially accent, affected performance to some degree, analogous to recent findings in AO speech research.\n",
    "Index Terms: cross-modal congruency, talker and accent effects, point-light talkers, articulatory information\n",
    ""
   ]
  },
  "barbulescu15_avsp": {
   "authors": [
    [
     "Adela",
     "Barbulescu"
    ],
    [
     "Gérard",
     "Bailly"
    ],
    [
     "Rémi",
     "Ronfard"
    ],
    [
     "Maël",
     "Pouget"
    ]
   ],
   "title": "Audiovisual generation of social attitudes from neutral stimuli",
   "original": "av15_034",
   "page_count": 6,
   "order": 11,
   "p1": "34",
   "pn": "39",
   "abstract": [
    "The focus of this study is the generation of expressive audiovisual speech from neutral utterances for 3D virtual actors. Taking into account the segmental and suprasegmental aspects of audiovisual speech, we propose and compare several computational frameworks for the generation of expressive speech and face animation. We notably evaluate a standard framebased conversion approach with two other methods that postulate the existence of global prosodic audiovisual patterns that are characteristic of social attitudes. The proposed approaches are tested on a database of ”Exercises in Style” [1] performed by two semi-professional actors and results are evaluated using crowdsourced perceptual tests. The first test performs a qualitative validation of the animation platform while the second is a comparative study between several expressive speech generation methods. We evaluate how the expressiveness of our audiovisual performances is perceived in comparison to resynthesized original utterances and the outputs of a purely framebased conversion system.\n",
    "Index Terms: virtual actors, expressive speech animation, audiovisual prosody, GMM, superposition of functional contours\n",
    ""
   ]
  },
  "honemann15_avsp": {
   "authors": [
    [
     "Angelika",
     "Hönemann"
    ],
    [
     "Hansjörg",
     "Mixdorff"
    ],
    [
     "Albert",
     "Rilliard"
    ]
   ],
   "title": "Classification of auditory-visual attitudes in German",
   "original": "av15_202",
   "page_count": 6,
   "order": 12,
   "p1": "202",
   "pn": "207",
   "abstract": [
    "This paper presents results from an auditory-visual recognition experiment employing short utterances of German produced with varying attitudinal expressions. It is based on 16 different kinds of social and/or propositional attitudes which place speakers in various social interactions with a partner of inferior, equal or superior status, and having a communication aim with a positive, neutral or negative, valence. Data from ten German subjects were classified by native perceivers regarding the attitude portrayed. Participants were given five choices: The intended attitude, two closely related attitudes, and two randomly chosen ones. Higher recognition scores were obtained in audio-visual presentations (45%), over 36% with audio-only stimuli. The best recognized attitudes were doubt, (neutral) statement, surprise and irritation which all yielded audio-visual recognition scores over 50%. Lowest recognition scores were obtained for irony, ‘walking-on-eggs’ and politeness. A hierarchical clustering based on correspondence analysis showed that groupings of stimuli in one cluster are consistent with their original labels - these consistent stimuli yield better recognition scores. Conversely, clusters with heterogeneous populations simply aggregate bad performances.\n",
    ""
   ]
  },
  "stelle15_avsp": {
   "authors": [
    [
     "Elizabeth",
     "Stelle"
    ],
    [
     "Caroline L.",
     "Smith"
    ],
    [
     "Eric",
     "Vatikiotis-Bateson"
    ]
   ],
   "title": "Delayed auditory feedback with static and dynamic visual feedback",
   "original": "av15_040",
   "page_count": 6,
   "order": 13,
   "p1": "40",
   "pn": "45",
   "abstract": [
    "Visual speech information influences the accuracy and content of auditory speech perception, with both the static and dynamic components of the visual signal playing a role. However, less is known about the effect of visual information when it is presented as a source of speech production feedback. This paper presents results from a delayed auditory feedback paradigm which contrasted the presentation of static and dynamic visual feedback. Participants repeated short sentences in six conditions, and speech rate and speech errors were measured. Speech rate was reliably reduced when dynamic visual feedback was paired with delayed auditory feedback, and there was a weak effect for speech errors, which were reduced when dynamic visual feedback was paired with normal auditory feedback. Withincondition trends raise questions about adaptation to different types of feedback. Our results suggest that dynamic, but not static, visual feedback affects speech production. Even if the auditory and visual feedback are misaligned, it is important that temporal dynamics be present in both signals.\n",
    "Index Terms: speech production, visual feedback, delayed auditory feedback\n",
    ""
   ]
  },
  "chong15_avsp": {
   "authors": [
    [
     "Chee Seng",
     "Chong"
    ],
    [
     "Jeesun",
     "Kim"
    ],
    [
     "Chris",
     "Davis"
    ]
   ],
   "title": "Visual vs. auditory emotion information: how language and culture affect our bias towards the different modalities",
   "original": "av15_046",
   "page_count": 6,
   "order": 14,
   "p1": "46",
   "pn": "51",
   "abstract": [
    "This study investigated if familiarity with a language that an emotion is expressed in, affects how information from the different sensory modalities are weighed in auditory-visual (AV) processing. The rationale for this study is that visual information may drive multisensory perception of emotion when a person is unfamiliar with a language, and this visual dominance effect may be reduced when a person is able to understand and extract emotion information from the language. To test this, Cantonese, English and Malay speakers were presented spoken Cantonese and English emotion expressions (angry, happy, sad, disgust and surprise) in AO, VO or AV conditions. Response matrices were examined to see if patterns of responses changed as a function of whether the expressions were produced in their native or non-native language. Our results show that the visual dominance effect for Cantonese and Malay participants changed depending on the language an emotion was expressed in, while the English participants showed a strong visual dominance effect regardless of the language of expression.\n",
    "Index Terms: vocal emotion perception, tone language, nontone language, AV perception, Cantonese, English, Malay\n",
    ""
   ]
  },
  "takagi15_avsp": {
   "authors": [
    [
     "Sachiko",
     "Takagi"
    ],
    [
     "Shiho",
     "Miyazawa"
    ],
    [
     "Elisabeth",
     "Huis In 't Veld"
    ],
    [
     "Beatrice de",
     "Gelder"
    ],
    [
     "Akihiro",
     "Tanaka"
    ]
   ],
   "title": "Comparison of multisensory display rules in expressing complex emotions between cultures",
   "original": "av15_057",
   "page_count": 6,
   "order": 15,
   "p1": "57",
   "pn": "62",
   "abstract": [
    "Previous studies have suggested that there are cultural differences in display rules and decoding rules of the emotions. In this study, we examined the cultural differences in the multisensory display rules of the six basic emotions (happiness, anger, disgust, sadness, fear and surprise) and the six complex emotions (interest, contempt, embarrassment, shame, guilt, and envy) between Japanese and Dutch. In the experiment, we used the six kinds of faces and voices showing the six basic emotions. Japanese and Dutch participants were asked to create audiovisual movies expressing each of twelve emotions by combining one of the six faces and one of the six voices. Our results showed cultural differences in expressing complex emotions. Specifically, there are two ways to express complex emotion by combining a face and voice showing the basic emotions: connection or substitution. Japanese participants tend to use the way of connection and combine the face and voice showing different emotions. On the other hand, Dutch participants tend to use the way of substitution and combine the face and voice showing the same basic emotions. Our findings indicate differential display rules and decoding rules between cultures in expressing complex emotions.\n",
    "Index Terms: facial expression, vocal expression, complex emotions, display rules\n",
    ""
   ]
  },
  "tanaka15_avsp": {
   "authors": [
    [
     "Akihiro",
     "Tanaka"
    ],
    [
     "Sachiko",
     "Takagi"
    ],
    [
     "Saori",
     "Hiramatsu"
    ],
    [
     "Elisabeth",
     "Huis In 't Veld"
    ],
    [
     "Beatrice de",
     "Gelder"
    ]
   ],
   "title": "Towards the development of facial and vocal expression database in east Asian and Western cultures",
   "original": "av15_063",
   "page_count": 4,
   "order": 16,
   "p1": "63",
   "pn": "66",
   "abstract": [
    "The purpose of this study is to develop a stimulus set, in which facial and vocal expressions by East Asian and Western speakers are recorded. In the recording session, facial and vocal expressions, in which the six basic emotions were expressed, were recorded from Japanese and Dutch models, using equivalent linguistic phrases and identical recording settings and procedures. After selection, facial and vocal expressions from eight Japanese and eight Dutch models were evaluated by Japanese and Dutch students, respectively. Results showed that accuracies for facial expressions were above 60% except for fear in both Japanese and Dutch groups. Although the facial and vocal expressions were recorded simultaneously, accuracies for vocal expressions were not so high as facial expressions. This study is the first step towards the development of a new stimulus set of facial and vocal expressions from East Asian and Western models. We expect our stimulus set to be used in future studies comparing between facial and vocal emotion perception, and those comparing unisensory and/or multisensory emotion perception between cultures.\n",
    "Index Terms: emotion perception, database, facial expression, vocal expression, cultural difference\n",
    ""
   ]
  },
  "fenwick15_avsp": {
   "authors": [
    [
     "Sarah",
     "Fenwick"
    ],
    [
     "Chris",
     "Davis"
    ],
    [
     "Catherine T.",
     "Best"
    ],
    [
     "Michael D.",
     "Tyler"
    ]
   ],
   "title": "The effect of modality and speaking style on the discrimination of non-native phonological and phonetic contrasts in noise",
   "original": "av15_067",
   "page_count": 6,
   "order": 17,
   "p1": "67",
   "pn": "72",
   "abstract": [
    "Auditory speech is difficult to discern in degraded listening conditions, however the addition of visual speech can improve perception. The Perceptual Assimilation Model [1] suggests that non-native contrasts involving a native phonological difference (two-category assimilation) should be discriminated more accurately than those involving a phonetic goodness-offit difference (category-goodness assimilation), but it is not known whether auditory-visual (AV) benefit is greater for phonological than phonetic differences when the acoustic signal is degraded by speech-shaped-noise. In auditory-only (AO) and AV conditions, monolingual Australian English participants completed AXB discrimination tasks on twocategory versus category-goodness Sindhi contrasts. We also examined the relative influences of phonetic feature difference (laryngeal vs. place-of-articulation [POA]) and speaking style (clear vs. citation speech) on discrimination accuracy. AV benefit was found for POA contrasts, but no effect of speaking style, and AV benefit was larger for two-category than category-goodness contrasts. For laryngeal contrasts, AV benefit was found for the two-category contrasts (across speaking style), but for the category-goodness contrast only when it was clearly articulated. These results indicate that nonnative perceivers use visual speech to their advantage, and to a greater extent for phonological contrasts, but speaking style contributes in AV conditions only for a less salient phonetic contrast.\n",
    "Index Terms: speaking style, modality, cross-language speech discrimination\n",
    ""
   ]
  },
  "wang15_avsp": {
   "authors": [
    [
     "Rui",
     "Wang"
    ],
    [
     "Biao",
     "Zeng"
    ],
    [
     "Simon",
     "Thompson"
    ]
   ],
   "title": "Audio-visual perception of Mandarin lexical tones in AX same-different judgment task",
   "original": "av15_073",
   "page_count": 5,
   "order": 18,
   "p1": "73",
   "pn": "77",
   "abstract": [
    "Two same-different discrimination tasks were conducted to test whether Mandarin and English native speakers use visual cues to facilitate Mandarin lexical tone perception. In the experiments, the stimuli were presented in 3 modes: audioonly (AO), audio-video (AV) and video-only (VO) under the clear and two levels of signal-to-noise ratio (SNR) -6dB and - 9dB noise condition. If the speakers’ perception of AV is better than that of AO, the extra visual information of lexical tones contributes tone perception. In Experiment 1 and 2, we found that Mandarin speakers had no visual augmentation under clear and noise conditions. For English speakers, on the other hand, extra visual information hindered their tone perception (visual reduction) under SNR -9dB noise. This suggests that English speakers rely more on auditory information to perceive lexical tones. Tone pairs analysis in both experiments found that visual reduction in tone pair T2- T3 and visual augmentation in tone pair T3-T4. It indicates that acoustic tone features (e.g. duration, contour) can be seen and be involved in the process of audiovisual perception. Visual cues facilitate or inhibit tone perception depends on whether the presented visual features of the tone pairs are distinctively recognised or highly confusing to each other.\n",
    "Index Terms: audiovisual speech perception, Mandarin, lexical tone\n",
    ""
   ]
  },
  "ding15_avsp": {
   "authors": [
    [
     "Yu",
     "Ding"
    ],
    [
     "Catherine",
     "Pelachaud"
    ]
   ],
   "title": "Lip animation synthesis: a unified framework for speaking and laughing virtual agent",
   "original": "av15_078",
   "page_count": 6,
   "order": 19,
   "p1": "78",
   "pn": "83",
   "abstract": [
    "This paper proposes a unified statistical framework to synthesize speaking and laughing lip animations for virtual agents in real time. Our lip animation synthesis model takes as input the decomposition of a spoken text into phonemes as well as their duration. Our model can be used with synthesized speech. First, Gaussian mixture models (GMMs), called lip shape GMMs, are used to model the relationship between phoneme duration and lip shape from human motion capture data; then an interpolation function is learnt from human motion capture data, which is based on hidden Markov models (HMMs), called HMMs interpolation. In the synthesis step, lip shapeGMMs are used to infer a first lip shape stream from the inputs; then this lip shape stream is smoothed by the learnt HMMs interpolation, to obtain the synthesized lip animation. The effectiveness of the proposed framework is confirmed in the objective evaluation.\n",
    "Index Terms: lip animation, speech to animation, interactive virtual agent, laughter, speech, Gaussian mixture models (GMMs), hidden Markov models (HMMs)\n",
    ""
   ]
  },
  "schabus15_avsp": {
   "authors": [
    [
     "Dietmar",
     "Schabus"
    ],
    [
     "Michael",
     "Pucher"
    ]
   ],
   "title": "Comparison of dialect models and phone mappings in HSMM-based visual dialect speech synthesis",
   "original": "av15_084",
   "page_count": 4,
   "order": 20,
   "p1": "84",
   "pn": "87",
   "abstract": [
    "In this paper we evaluate two different methods for the visual synthesis of Austrian German dialects with parametric Hidden- Semi-Markov-Model (HSMM) based speech synthesis. One method uses visual dialect data, i.e. visual dialect recordings that are annotated with dialect phonetic labels, the other methods uses a standard visual model and maps dialect phones to standard phones. This second method is more easily applicable since most often visual dialect data is not available. Both methods employ contextual information via decision tree based visual clustering of dialect or standard visual data. We show that both models achieve a similar performance on a subjective pair-wise comparison test. This shows that visual dialect data is not necessarily needed for visual modeling of dialects if a dialect to standard mapping can be used that exploits the contextual information of the standard language.\n",
    "Index Terms: visual speech synthesis, dialect\n",
    ""
   ]
  },
  "thangthai15_avsp": {
   "authors": [
    [
     "Ausdang",
     "Thangthai"
    ],
    [
     "Barry-John",
     "Theobald"
    ]
   ],
   "title": "HMM-based visual speech synthesis using dynamic visemes",
   "original": "av15_088",
   "page_count": 5,
   "order": 21,
   "p1": "88",
   "pn": "92",
   "abstract": [
    "In this paper we incorporate dynamic visemes into hidden Markov model (HMM)-based visual speech synthesis. Dynamic visemes represent intuitive visual gestures identified automatically by clustering purely visual speech parameters. They have the advantage of spanning multiple phones and so they capture the effects of visual coarticulation explicitly within the unit. The previous application of dynamic visemes to synthesis used a sample-based approach, where cluster centroids were concatenated to form parameter trajectories corresponding to novel visual speech. In this paper we generalize the use of these units to create more flexible and dynamic animation using a HMM-based synthesis framework. We show using objective and subjective testing that aHMMsynthesizer trained using dynamic visemes can generate better visual speech than HMM synthesizers trained using either phone or traditional viseme units.\n",
    "Index Terms: visual speech synthesis, hidden Markov model, dynamic visemes\n",
    ""
   ]
  },
  "alghamdi15_avsp": {
   "authors": [
    [
     "Najwa",
     "Alghamdi"
    ],
    [
     "Steve",
     "Maddock"
    ],
    [
     "Guy J.",
     "Brown"
    ],
    [
     "Jon",
     "Barker"
    ]
   ],
   "title": "Investigating the impact of artificial enhancement of lip visibility on the intelligibility of spectrally-distorted speech",
   "original": "av15_093",
   "page_count": 6,
   "order": 22,
   "p1": "93",
   "pn": "98",
   "abstract": [
    "The intelligibility of visual speech can be affected by a number of facial visual signals, e.g. lip emphasis, teeth and tongue visibility, and facial hair. This paper focuses on lip visibility. In the study presented in this paper, we use spectrally-distorted speech to train groups of non-native, English-speaking Saudi listeners using three different forms of speech: audio-only, audiovisual, and enhanced audiovisual, which is achieved by artificially colouring the lips of the speaker to improve lip visibility. The reason for using spectrally-distorted speech is that the longer term aim of our work is to employ these ideas in a training system for hearing-impaired users, in particular cochlear-implant users. Our initial work uses non-native Saudi listeners based on the assumption that their reduced processing abilities for native speech can be compared to the reduced processing abilities of cochlear implant users as a result of the inherent noise in the processing of sound by a cochlear implant. The results suggest that using enhanced audiovisual speech during auditory training improves the training gain when subsequently listening to audio-only spectrally-distorted speech. The results also suggest that spectrally-distorted speech intelligibility during training is improved when an enhanced visual signal is used.\n",
    ""
   ]
  },
  "davis15_avsp": {
   "authors": [
    [
     "Chris",
     "Davis"
    ],
    [
     "Jeesun",
     "Kim"
    ],
    [
     "Vincent",
     "Aubanel"
    ],
    [
     "Greg",
     "Zelic"
    ],
    [
     "Yatin",
     "Mahajan"
    ]
   ],
   "title": "The stability of mouth movements for multiple talkers over multiple sessions",
   "original": "av15_099",
   "page_count": 4,
   "order": 23,
   "p1": "99",
   "pn": "102",
   "abstract": [
    "To examine the stability of visible speech articulation (a potentially useful biometric) we examined the degree of similarity of a speaker’s mouth movements when uttering the same sentence on six different occasions. We tested four speakers of differing language background and compared within- and across speaker variability. We obtained mouth motion data using an inexpensive 3D close range sensor and commercial face motion capture software. These data were exported as c3d files and the analysis was based on guided principal components derived from custom Matlab scripts. We showed that within-speaker repetitions were more similar than between speaker ones; that language background did not affect the stability of the utterances and that the patterns of articulation from different speakers were relatively distinctive.\n",
    "Index Terms: biometrics, speech articulation, mouth motion; 3D face motion capture\n",
    ""
   ]
  },
  "cornu15_avsp": {
   "authors": [
    [
     "Thomas Le",
     "Cornu"
    ],
    [
     "Ben",
     "Milner"
    ]
   ],
   "title": "Voicing classification of visual speech using convolutional neural networks",
   "original": "av15_103",
   "page_count": 6,
   "order": 24,
   "p1": "103",
   "pn": "108",
   "abstract": [
    "The application of neural network and convolutional neural network (CNN) architectures is explored for the tasks of voicing classification (classifying frames as being either non-speech, unvoiced, or voiced) and voice activity detection (VAD) of visual speech. Experiments are conducted for both speaker dependent and speaker independent scenarios.   A Gaussian mixture model (GMM) baseline system is developed using standard image-based two-dimensional discrete cosine transform (2D-DCT) visual speech features, achieving speaker dependent accuracies of 79% and 94 %, for voicing classification and VAD respectively. Additionally, a singlelayer neural network system trained using the same visual features achieves accuracies of 86% and 97 %. A novel technique using convolutional neural networks for visual speech feature extraction and classification is presented. The voicing classification and VAD results using the system are further improved to 88% and 98% respectively.   The speaker independent results show the neural network system to outperform both theGMMand CNN systems, achieving accuracies of 63% for voicing classification, and 79% for voice activity detection.\n",
    "Index Terms: convolutional neural networks, voicing classification, visual speech\n",
    ""
   ]
  },
  "petridis15_avsp": {
   "authors": [
    [
     "Stavros",
     "Petridis"
    ],
    [
     "Varun",
     "Rajgarhia"
    ],
    [
     "Maja",
     "Pantic"
    ]
   ],
   "title": "Comparison of single-model and multiple-model prediction-based audiovisual fusion",
   "original": "av15_109",
   "page_count": 6,
   "order": 25,
   "p1": "109",
   "pn": "114",
   "abstract": [
    "Prediction-based fusion is a recently proposed audiovisual fusion approach which outperforms feature-level fusion on laughter-vs-speech discrimination. One set of predictive models is trained per class which learns the audio-to-visual and visual-to-audio feature mapping together with the time evolution of audio and visual features. Classification of a new input is performed via prediction. All the class predictors produce a prediction of the expected audio / visual features and their prediction errors are combined for each class. The model which best describes the audiovisual feature relationship, i.e., results in the lowest prediction error, provides its label to the input. In all the previous works, a single set of predictors was trained on the entire training set for each class. In this work, we investigate the use of multiple sets of predictors per class. The main idea is that since models are trained on clusters of data, they will be more specialised and they will produce lower prediction errors which can in turn enhance the classification performance. We experimented with subject-based clustering and clustering based on different types of laughter, voiced and unvoiced. Results are presented on laughter-vs-speech discrimination on a cross-database experiment using the AMI and MAHNOB databases. The use of multiple sets of models results in a significant performance increase with the latter clustering approach achieving the best performance. Overall, an increase of over 4% and 10% is observed for F1 speech and laughter, respectively, for both datasets.\n",
    "Index Terms: Prediction-based fusion, Audiovisual fusion, Nonlinguistic Information Processing\n",
    ""
   ]
  },
  "bear15_avsp": {
   "authors": [
    [
     "Helen L.",
     "Bear"
    ],
    [
     "Richard",
     "Harvey"
    ],
    [
     "Yuxuan",
     "Lan"
    ]
   ],
   "title": "Finding phonemes: improving machine lip-reading",
   "original": "av15_115",
   "page_count": 6,
   "order": 26,
   "p1": "115",
   "pn": "120",
   "abstract": [
    "In machine lip-reading there is continued debate and research around the correct classes to be used for recognition.   In this paper we use a structured approach for devising speaker-dependent viseme classes, which enables the creation of a set of phoneme-to-viseme maps where each has a different quantity of visemes ranging from two to 45. Viseme classes are based upon the mapping of articulated phonemes, which have been confused during phoneme recognition, into viseme groups.   Using these maps, with the LiLIR dataset, we show the effect of changing the viseme map size in speaker-dependent machine lip-reading, measured by word recognition correctness and so demonstrate that word recognition with phoneme classifiers is not just possible, but often better than word recognition with viseme classifiers. Furthermore, there are intermediate units between visemes and phonemes which are better still.\n",
    "Index Terms: visual-only speech recognition, computer lipreading, visemes, classification, pattern recognition\n",
    ""
   ]
  },
  "cox15_avsp": {
   "authors": [
    [
     "Stephen",
     "Cox"
    ]
   ],
   "title": "Discovering patterns in visual speech",
   "original": "av15_121",
   "page_count": 6,
   "order": 27,
   "p1": "121",
   "pn": "126",
   "abstract": [
    "We know that an audio speech signal can be unambiguously decoded by any native speaker of the language it is uttered in, provided that it meets some quality conditions. But we do not know if this is the case with visual speech, because the process of lipreading is rather mysterious and seems to rely heavily on the use of context and non-speech cues. How much information about the speech content is there in a visual speech signal? We make some attempt to provide an answer to this question by ‘discovering’ matching segments of phoneme sequences that represent recurring words and phrases in audio and visual representations of the same speech. We use a modified version of the technique of segmental dynamic programming that was introduced by Park and Glass. Comparison of the results shows that visual speech displays rather less matching content than the audio, and reveals some interesting differences in the phonetic content of the information recovered by the two modalities.\n",
    "Index Terms: automatic lip reading, visual speech processing, speech recognition\n",
    ""
   ]
  },
  "thangthai15b_avsp": {
   "authors": [
    [
     "Kwanchiva",
     "Thangthai"
    ],
    [
     "Richard",
     "Harvey"
    ],
    [
     "Stephen",
     "Cox"
    ],
    [
     "Barry-John",
     "Theobald"
    ]
   ],
   "title": "Improving lip-reading performance for robust audiovisual speech recognition using DNNs",
   "original": "av15_127",
   "page_count": 5,
   "order": 28,
   "p1": "127",
   "pn": "131",
   "abstract": [
    "This paper presents preliminary experiments using the Kaldi toolkit to investigate audiovisual speech recognition (AVSR) in noisy environments using deep neural networks (DNNs). In particular we use a single-speaker large vocabulary, continuous audiovisual speech corpus to compare the performance of visual-only, audio-only and audiovisual speech recognition. The models trained using the Kaldi toolkit are compared with the performance of models trained using conventional hidden Markov models (HMMs). In addition, we compare the performance of a speech recognizer both with and without visual features over nine different SNR levels of babble noise ranging from 20dB down to -20dB. The results show that the DNN outperforms conventional HMMs in all experimental conditions, especially for the lip-reading only system, which achieves a gain of 37.19% accuracy (84.67% absolute word accuracy). Moreover, the DNN provides an effective improvement of 10 and 12dB SNR respectively for both the single modal and bimodal speech recognition systems. However, integrating the visual features using simple feature fusion is only effective in SNRs at 5dB and above. Below this the degradion in accuracy of an audiovisual system is similar to the audio only recognizer.\n",
    "Index Terms: lip-reading, speech reading, audiovisual speech recognition\n",
    ""
   ]
  },
  "bayard15_avsp": {
   "authors": [
    [
     "Clémence",
     "Bayard"
    ],
    [
     "Cécile",
     "Colin"
    ],
    [
     "Jacqueline",
     "Leybart"
    ]
   ],
   "title": "Integration of auditory, labial and manual signals in cued speech perception by deaf adults: an adaptation of the McGurk paradigm",
   "original": "av15_163",
   "page_count": 6,
   "order": 29,
   "p1": "163",
   "pn": "168",
   "abstract": [
    "Among deaf individuals fitted with a cochlear implant, some use Cued Speech (CS; a system in which each syllable is uttered with a complementary manual gesture) and therefore, have to combine auditory, labial and manual information to perceive speech. We examined how audio-visual (AV) speech integration is affected by the presence of manual cues and on which form of information (auditory, labial or manual) the CS receptors primarily rely depending on labial ambiguity. To address this issue, deaf CS users (N=36) and deaf CS naïve (N=35) participants were submitted to an identification task of two AV McGurk stimuli (either with a plosive or with a fricative consonant). Manual cues were congruent with either auditory information, lip information or the expected fusion. Results revealed that deaf individuals can merge audio and labial information into a single unified percept. Without manual cues, participants gave a high proportion of fusion response (particularly with ambiguous plosive McGurk stimuli). Results also suggested that manual cues can modify the AV integration and that their impact differs between plosive and fricative McGurk stimuli.\n",
    ""
   ]
  },
  "aubanel15_avsp": {
   "authors": [
    [
     "Vincent",
     "Aubanel"
    ],
    [
     "Chris",
     "Davis"
    ],
    [
     "Jeesun",
     "Kim"
    ]
   ],
   "title": "Explaining the visual and masked-visual advantage in speech perception in noise: the role of visual phonetic cues",
   "original": "av15_132",
   "page_count": 5,
   "order": 30,
   "p1": "132",
   "pn": "136",
   "abstract": [
    "Visual enhancement of speech intelligibility, although clearly established, still resists a clear description. We attempt to contribute to solving that problem by proposing a simple account based on phonetically motivated visual cues. This work extends a previous study quantifying the visual advantage in sentence intelligibility across three conditions with varying degrees of visual information available: auditory only, auditory visual orally masked and auditory-visual. We explore the role of lexical as well as visual factors, the latter derived from groupings in visemes. While lexical factors play an undiscriminative role across modality conditions, some measure of viseme confusability seems to capture part of the performance results. A simple characterisation of the phonetic content of sentences in terms of visual information occurring exclusively inside the mask region was found to be the strongest predictor for the auditory-visual masked condition only, demonstrating a direct link between localised visual information and auditory-visual speech processing performance.\n",
    "Index Terms: Auditory-visual speech processing, visemes, sentence intelligibility, visual advantage\n",
    ""
   ]
  },
  "websdale15_avsp": {
   "authors": [
    [
     "Danny",
     "Websdale"
    ],
    [
     "Ben",
     "Milner"
    ]
   ],
   "title": "Analysing the importance of different visual feature coefficients",
   "original": "av15_137",
   "page_count": 6,
   "order": 31,
   "p1": "137",
   "pn": "142",
   "abstract": [
    "A study is presented to determine the relative importance of different visual features for speech recognition which includes pixel-based, model-based, contour-based and physical features. Analysis to determine the discriminability of features is performed through F-ratio and J-measures for both static and temporal derivatives, the results of which were found to correlate highly with speech recognition accuracy (r=0.97). Principal component analysis is then used to combine all visual features into a single feature vector, of which further analysis is performed on the resulting basis functions. An optimal feature vector is obtained which outperforms the best individual feature (AAM) with 93.5% word accuracy.\n",
    "Index Terms: Visual features, speech recognition, F-ratio, J-measure, PCA\n",
    ""
   ]
  },
  "scarbel15_avsp": {
   "authors": [
    [
     "Lucie",
     "Scarbel"
    ],
    [
     "Denis",
     "Beautemps"
    ],
    [
     "Jean-Luc",
     "Schwartz"
    ],
    [
     "Marc",
     "Sato"
    ]
   ],
   "title": "Auditory and audiovisual close-shadowing in normal and cochlear-implanted hearing impaired subjects",
   "original": "av15_143",
   "page_count": 4,
   "order": 32,
   "p1": "143",
   "pn": "146",
   "abstract": [
    "This study takes place in the theoretical background of perceptuo-motor linkage in speech perception. A closeshadowing experiment has been carried out on post-lingual cochlear implanted (CI) and normal-hearing (NH) adults in order to evaluate sensory-motor interactions during joint perception and production of speech. To this aim, participants have to categorize audio (A) and audiovisual (AV) syllables as quickly as possible, with two modes of responses, oral or manual. Overall, responses from NH were globally faster and more precise than those of CI, although adding the visual modality led to a gain in performance in CI. Critically, oral responses were faster but less precise than manual responses in the two groups, with a stronger difference observed for CI than for NH. Despite auditory deprivation, these results suggest the involvement of sensory-motor interactions during speech perception in CI, albeit possibly less efficient than in NH.\n",
    "Index Terms: speech perception, cochlear implant, perceptuomotor linkage, close-shadowing\n",
    ""
   ]
  },
  "vandeventer15_avsp": {
   "authors": [
    [
     "Jason",
     "Vandeventer"
    ],
    [
     "Andrew J.",
     "Aubrey"
    ],
    [
     "Paul L.",
     "Rosin"
    ],
    [
     "David",
     "Marshall"
    ]
   ],
   "title": "4D Cardiff Conversation Database (4D CCDb): a 4D database of natural, dyadic conversations",
   "original": "av15_157",
   "page_count": 6,
   "order": 33,
   "p1": "157",
   "pn": "162",
   "abstract": [
    "The 4D Cardiff Conversation Database (4D CCDb) is the first 4D (3D Video) audio-visual database containing natural conversations between pairs of people. This publicly available database contains 17 conversations which have been fully annotated for speaker and listener activity: conversational facial expressions, head motion, and verbal/non-verbal utterances. It can be accessed at http://www.cs.cf.ac.uk/CCDb. In this paper we describe the data collection and annotation process. We also provide results of a baseline classification experiment distinguishing frontchannel from backchannel smiles, using 3D Active Appearance Models for feature extraction, polynomial fitting for representing the data as 4D sequences, and Support Vector Machines for classification. We believe this expression-rich, audio-visual database of natural conversations will make a useful contribution to the computer vision, affective computing, and cognitive science communities by providing raw data, features, annotations, and baseline comparisons.\n",
    "Index Terms: 4D Databases, Affective Computing, Face and Gesture Recognition, Speech Analysis\n",
    ""
   ]
  },
  "rademan15_avsp": {
   "authors": [
    [
     "Christiaan",
     "Rademan"
    ],
    [
     "Thomas",
     "Niesler"
    ]
   ],
   "title": "Improved visual speech synthesis using dynamic viseme k-means clustering and decision trees",
   "original": "av15_169",
   "page_count": 6,
   "order": 34,
   "p1": "169",
   "pn": "174",
   "abstract": [
    "We present a decision tree-based viseme clustering technique that allows visual speech synthesis after training on a small dataset of phonetically-annotated audiovisual speech. The decision trees allow improved viseme grouping by incorporating k-means clustering into the training algorithm. The use of overlapping dynamic visemes, defined by tri-phone time-varying oral pose boundaries, allows improved modelling of coarticulation effects. We show that our approach leads to a clear improvement over a comparable baseline in perceptual tests.   The avatar is based on the freely available MakeHuman and Blender software components.\n",
    "Index Terms: conversational agent, talking head, visual speech synthesis, lip animation, coarticulation modelling, CART-based viseme clustering, audio-visual speech data corpus.\n",
    ""
   ]
  },
  "marcheret15_avsp": {
   "authors": [
    [
     "Etienne",
     "Marcheret"
    ],
    [
     "Gerasimos",
     "Potamianos"
    ],
    [
     "Josef",
     "Vopicka"
    ],
    [
     "Vaibhava",
     "Goel"
    ]
   ],
   "title": "Scattering vs. discrete cosine transform features in visual speech processing",
   "original": "av15_175",
   "page_count": 6,
   "order": 35,
   "p1": "175",
   "pn": "180",
   "abstract": [
    "Appearance-based feature extraction constitutes the dominant approach for visual speech representation in a variety of problems, such as automatic speechreading, visual speech detection, and others. To obtain the necessary visual features, typically a rectangular region-of-interest (ROI) containing the speaker’s mouth is first extracted, followed, most commonly, by a discrete cosine transform (DCT) of the ROI pixel values and a feature selection step. The approach, although algorithmically simple and computationally efficient, suffers from lack of DCT invariance to typical ROI deformations, stemming, primarily, from speaker’s head pose variability and small tracking inaccuracies. To address the problem, in this paper, the recently introduced scattering transform is investigated as an alternative to DCT within the appearance-based framework for ROI representation, suitable for visual speech applications. A number of such tasks are considered, namely, visual-only speech activity detection, visual-only and audio-visual sub-phonetic classification, as well as audio-visual speech synchrony detection, all employing deep neural network classifiers with either DCT or scattering-based visual features. Comparative experiments of the resulting systems are conducted on a large audio-visual corpus of frontal face videos, demonstrating, in all cases, the scattering transform superiority over the DCT.\n",
    "Index Terms: scattering transform, discrete cosine transform, deep neural networks, visual speech activity detection, automatic speechreading, audio-visual synchrony.\n",
    ""
   ]
  },
  "bear15b_avsp": {
   "authors": [
    [
     "Helen L.",
     "Bear"
    ],
    [
     "Stephen J.",
     "Cox"
    ],
    [
     "Richard W.",
     "Harvey"
    ]
   ],
   "title": "Speaker-independent machine lip-reading with speaker-dependent viseme classifiers",
   "original": "av15_190",
   "page_count": 6,
   "order": 36,
   "p1": "190",
   "pn": "195",
   "abstract": [
    "In machine lip-reading, which is identification of speech from visual-only information, there is evidence to show that visual speech is highly dependent upon the speaker [1]. Here, we use a phoneme-clustering method to form new phoneme-to-viseme maps for both individual and multiple speakers. We use these maps to examine how similarly speakers talk visually. We conclude that broadly speaking, speakers have the same repertoire of mouth gestures, where they differ is in the use of the gestures.\n",
    "Index Terms: visual-only speech recognition, computer lipreading, visemes, classification, pattern recognition, speakerindependence\n",
    ""
   ]
  },
  "ukai15_avsp": {
   "authors": [
    [
     "Kazuto",
     "Ukai"
    ],
    [
     "Satoshi",
     "Tamura"
    ],
    [
     "Satoru",
     "Hayamizu"
    ]
   ],
   "title": "Stream weight estimation using higher order statistics in multi-modal speech recognition",
   "original": "av15_181",
   "page_count": 4,
   "order": 37,
   "p1": "181",
   "pn": "184",
   "abstract": [
    "In this paper, stream weight optimization for multi-modal speech recognition using audio information and visual infor- mation is examined. In a conventional multi-stream Hidden Markov Model (HMM) used in multi-modal speech recogni- tion, a constraint in which the summation of audio and visual weight factors should be one is employed. This means bal- ance between transition and observation probabilities of HMM is fixed. We study an effective weight estimation indicator when releasing the constraint. Recognition experiments were conducted using an audio-visual corpus CENSREC-1-AV [1]. In noisy environments, effectiveness of deactivating the con- straint is clarified for improving recognition accuracy. Sub- sequently higher-order statistical parameter (kurtosis) based stream weights were proposed and tested. Through recognition experiments, it is found proposed stream weights are successful.\n",
    "Index Terms: stream weight optimization, multi-modal speech recognition, kurtosis, multi-stream HMM.\n",
    ""
   ]
  },
  "rao15_avsp": {
   "authors": [
    [
     "Hrishikesh",
     "Rao"
    ],
    [
     "Zhefan",
     "Ye"
    ],
    [
     "Yin",
     "Li"
    ],
    [
     "Mark A.",
     "Clements"
    ],
    [
     "Agata",
     "Rozga"
    ],
    [
     "James M.",
     "Rehg"
    ]
   ],
   "title": "Combining acoustic and visual features to detect laughter in adults’ speech",
   "original": "av15_153",
   "page_count": 4,
   "order": 38,
   "p1": "153",
   "pn": "156",
   "abstract": [
    "Laughter can not only convey the affective state of the speaker but also be perceived differently based on the context in which it is used. In this paper, we focus on detecting laughter in adults’ speech using the MAHNOB laughter database. The paper explores the use of novel long-term acoustic features to capture the periodic nature of laughter and the use of computer vision-based smile features to analyze laughter. The classification accuracy of the leave-one-speaker-out cross-validation using a cost-sensitive learning approach with a random forest classifier with 100 trees for detecting laughter in adults’ speech was 93.06% using acoustic features alone. Using only the visual features, the accuracy was 89.48%. Early fusion of audio and visual features resulted in an absolute improvement in the accuracy, compared to using only acoustic features, by 3.79% to 96.85%. The results indicate that the novel acoustic features do capture the repetitive characteristics of laughter, and the vision-based smile features can provide complementary visual cues to discriminate between speech and laughter. The significant finding of the study is the improvement of not only the accuracy, but a reduction in the false positives using the fusion of audio-visual features.\n",
    "Index Terms: paralinguistics, laughter, syllable-level features, smile, multi-modal\n",
    ""
   ]
  },
  "takahashi15_avsp": {
   "authors": [
    [
     "Maiko",
     "Takahashi"
    ],
    [
     "Akihiro",
     "Tanaka"
    ]
   ],
   "title": "Optimal timing of audio-visual text presentation: the role of attention",
   "original": "av15_185",
   "page_count": 5,
   "order": 39,
   "p1": "185",
   "pn": "189",
   "abstract": [
    "This study investigates the optimal timing of audio-visual presentation for text comprehension. #In Experiment 1, participants were asked to read and/or hear the expository passages in three conditions; visually presented, auditorily presented, and audio-visually presented condition. Comprehension performance in audio-visual presentation condition was not different from that in visual or auditory condition, raising the possibility that cognitive load on processing both visual and auditory information negated the positive effect of multi-modal presentation. To reduce the load of processing dual information simultaneously, we proposed to delay the iming of one of the two modality presentation and to direct participants’ attention to one of the two modality information during audio-visual text presentation. In Experiment 2 to 4, passages were presented audio-visually in three conditions; auditorily preceding, simultaneous, and visually preceding conditions. Participants were instructed to direct their attention to whole information (Exp. 2), visual information (Exp. 3), and auditory information (Exp. 4). The results showed that comprehension performance was higher in the visually preceding condition when their attention was directed whole or visual information. Based on the results, the integration process of audio-visually presented text information was discussed.\n",
    "Index Terms: reading comprehension, listening comprehension, audio-visual integration, multimedia learning\n",
    ""
   ]
  },
  "mixdorff15_avsp": {
   "authors": [
    [
     "Hansjörg",
     "Mixdorff"
    ],
    [
     "Angelika",
     "Hönemann"
    ],
    [
     "Jeesun",
     "Kim"
    ],
    [
     "Chris",
     "Davis"
    ]
   ],
   "title": "Anticipation of turn-Switching in auditory-visual dialogs",
   "original": "av15_052",
   "page_count": 5,
   "order": 40,
   "p1": "52",
   "pn": "56",
   "abstract": [
    "This paper presents an experiment in which we examined whether German and Australian English perceivers were able to predict imminent turn-switching in Australian English auditory-visual dialogs. Subjects were presented excerpts of one and four second duration either preceding a switch or taken from inside a turn and had to decide which condition they saw. Stimuli were either A/V, video-only or audio-only. Results on the one second excerpts were close to random. In general we found a preference for non-switching. Australian subjects outperformed the German subjects in the audio-only condition, but outcomes were almost equal on the A/V stimuli. Analysis regarding the syntactic and prosodic properties of the stimuli showed that phrase-final statement as well as question intonation facilitated recognition presumably due to these acting as markers of turn-switch preparation; whereas incomplete sentences and non-terminal intonation were indicative of turn-internal excerpts. As to visual cues signaling a following switch results were rather varied. An open mouth on the part of the listener more often preceded switches than not.\n",
    "Index Terms: auditory-visual prosody, dialog, turn-switching\n",
    ""
   ]
  },
  "tsankova15_avsp": {
   "authors": [
    [
     "Elena",
     "Tsankova"
    ],
    [
     "Eva",
     "Krumhuber"
    ],
    [
     "Andrew J.",
     "Aubrey"
    ],
    [
     "Arvid",
     "Kappas"
    ],
    [
     "Guido",
     "Möllering"
    ],
    [
     "David",
     "Marshall"
    ],
    [
     "Paul L.",
     "Rosin"
    ]
   ],
   "title": "The multi-modal nature of trustworthiness perception",
   "original": "av15_147",
   "page_count": 6,
   "order": 41,
   "p1": "147",
   "pn": "152",
   "abstract": [
    "Most past work on trustworthiness perception has focused on the structural features of the human face. The present study investigates the interplay of dynamic information from two channels – the face and the voice. By systematically varying the level of trustworthiness in each channel, 49 participants were presented with either facial or vocal information, or the combination of both, and made explicit judgements with respect to trustworthiness, dominance, and emotional valence. For most measures results revealed a primacy effect of facial over vocal cues. In examining the exact nature of the trustworthiness - emotion link we further found that emotional valence functioned as a significant mediator in impressions of trustworthiness. The findings extend previous correlational evidence and provide important knowledge of how trustworthiness in its dynamic and multi-modal form is decoded by the human perceiver.\n",
    "Index Terms: trustworthiness, face, voice, emotion, dynamic, multi-modal\n",
    ""
   ]
  },
  "bethamcherla15_avsp": {
   "authors": [
    [
     "Vasudev",
     "Bethamcherla"
    ],
    [
     "Will",
     "Paul"
    ],
    [
     "Cecilia Ovesdotter",
     "Alm"
    ],
    [
     "Reynold",
     "Bailey"
    ],
    [
     "Joe",
     "Geigel"
    ],
    [
     "Linwei",
     "Wang"
    ]
   ],
   "title": "Face-speech sensor fusion for non-invasive stress detection",
   "original": "av15_196",
   "page_count": 6,
   "order": 42,
   "p1": "196",
   "pn": "201",
   "abstract": [
    "We describe a human-centered multimodal framework for automatically measuring cognitive changes. As a proof-ofconcept, we test our approach on the use case of stress detection. We contribute a method that combines non-intrusive behavioral analysis of facial expressions with speech data, enabling detection without the use of wearable devices. We compare these modalities’ effectiveness against galvanic skin response (GSR) collected simultaneously from the subject group using a wristband sensor. Data was collected with a modified version of the Stroop test, in which subjects perform the test both with and without the inclusion of stressors. Our study attempts to distinguish stressed and unstressed behaviors during constant cognitive load. The best improvement in accuracy over the majority class baseline was 38%, which was only 5% behind the best GSR result on the same data. This suggests that reliable markers of cognitive changes can be captured by behavioral data that are more suitable for group settings than wearable devices, and that combining modalities is beneficial.\n",
    ""
   ]
  },
  "irwin15_avsp": {
   "authors": [
    [
     "Julia",
     "Irwin"
    ],
    [
     "Lawrence",
     "Brancazio"
    ]
   ],
   "title": "The development of patterns of gaze to a speaking face",
   "original": "av15_208",
   "page_count": 5,
   "order": 43,
   "p1": "208",
   "pn": "212",
   "abstract": [
    "Pattern of gaze to a speaking face was examined in typically developing children ranging from 5-10 years of age and in adults. Children viewed the speaking face in a visual only (speechreading) condition, in the presence of auditory noise and in an audiovisual mismatch (McGurk) condition. Amount of gaze to the face and fixation on the mouth of the speaker were examined over the course of a trial where a consonantvowel /ma/ or /na/ was spoken. Results indicate an increase in gaze on the face, and more specifically in fixations on the mouth of a speaker, between the ages of 5 and 10. These results reveal changes in pattern of gaze to the face with development. Further, pattern of gaze to the face of the speaker may help account for previous findings in the literature showing that visual influence on heard speech increases with development.\n",
    "Index Terms: speech development; audiovisual speech perception; eye tracking\n",
    ""
   ]
  }
 },
 "sessions": [
  {
   "title": "Keynotes",
   "papers": [
    "helzle15_avsp",
    "costaorvalho15_avsp",
    "schwartz15_avsp",
    "soong15_avsp"
   ]
  },
  {
   "title": "Life Span",
   "papers": [
    "visser15_avsp",
    "fort15_avsp",
    "parocosta15_avsp",
    "simonetti15_avsp",
    "huyse15_avsp"
   ]
  },
  {
   "title": "Emotion, Personality, and Dialogue",
   "papers": [
    "best15_avsp",
    "barbulescu15_avsp",
    "honemann15_avsp",
    "stelle15_avsp",
    "chong15_avsp"
   ]
  },
  {
   "title": "Culture and Language",
   "papers": [
    "takagi15_avsp",
    "tanaka15_avsp",
    "fenwick15_avsp",
    "wang15_avsp"
   ]
  },
  {
   "title": "Visual Speech Synthesis",
   "papers": [
    "ding15_avsp",
    "schabus15_avsp",
    "thangthai15_avsp",
    "alghamdi15_avsp",
    "davis15_avsp"
   ]
  },
  {
   "title": "Audio-Visual Speech Recognition",
   "papers": [
    "cornu15_avsp",
    "petridis15_avsp",
    "bear15_avsp",
    "cox15_avsp",
    "thangthai15b_avsp"
   ]
  },
  {
   "title": "Visual Speech Perception",
   "papers": [
    "bayard15_avsp",
    "aubanel15_avsp",
    "websdale15_avsp",
    "scarbel15_avsp"
   ]
  },
  {
   "title": "Poster Sessions",
   "papers": [
    "vandeventer15_avsp",
    "rademan15_avsp",
    "marcheret15_avsp",
    "bear15b_avsp",
    "ukai15_avsp",
    "rao15_avsp",
    "takahashi15_avsp",
    "mixdorff15_avsp",
    "tsankova15_avsp",
    "bethamcherla15_avsp",
    "irwin15_avsp"
   ]
  }
 ]
}