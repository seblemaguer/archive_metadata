{
 "title": "Interspeech 2008",
 "location": "Brisbane, Australia",
 "startDate": "22/9/2008",
 "endDate": "26/9/2008",
 "chair": "General Chair: Dennis Burnham",
 "conf": "Interspeech",
 "year": "2008",
 "name": "interspeech_2008",
 "series": "Interspeech",
 "SIG": "",
 "title1": "Interspeech 2008",
 "date": "22-26 September 2008",
 "booklet": "interspeech_2008.pdf",
 "papers": {
  "fujisaki08_interspeech": {
   "authors": [
    [
     "Hiroya",
     "Fujisaki"
    ]
   ],
   "title": "In search of models in speech communication research",
   "original": "i08_0001",
   "page_count": 10,
   "order": 1,
   "p1": "1",
   "pn": "10",
   "abstract": [
    "This paper first presents the author's personal view on the importance of modeling in scientific research in general, and then describes two of his works toward modeling certain aspects of human speech communication. The first work is concerned with the physiological and physical mechanisms of controlling the voice fundamental frequency of speech, which is an important parameter for expressing information on tone, accent, and intonation. The second work is concerned with the cognitive processes involved in a discrimination test of speech stimuli, which gives rise to the phenomenon of so-called categorical perception. They are meant to illustrate the power of models based on deep understanding and precise formulation of the functions of the mechanisms/processes that underlie observed phenomena. Finally, it also presents the author's view on some models that are yet to be developed.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-1"
  },
  "alwan08_interspeech": {
   "authors": [
    [
     "Abeer",
     "Alwan"
    ]
   ],
   "title": "Dealing with limited and noisy data in ASR: a hybrid knowledge-based and statistical approach",
   "original": "i08_0011",
   "page_count": 5,
   "order": 2,
   "p1": "11",
   "pn": "15",
   "abstract": [
    "In this talk, I will focus on the importance of integrating knowledge of human speech production and speech perception mechanisms, and language-specific information with statistically-based, datadriven approaches to develop robust and scalable automatic speech recognition (ASR) systems. As we will demonstrate, the need for such hybrid systems is especially critical when the ASR system is dealing with noisy data, when adaptation data are limited (for the case of speaker normalization and adaptation), and when dealing with accents.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-2"
  },
  "gonzalezrodriguez08_interspeech": {
   "authors": [
    [
     "Joaquin",
     "Gonzalez-Rodriguez"
    ]
   ],
   "title": "Forensic automatic speaker recognition: fiction or science?",
   "original": "i08_0016",
   "page_count": 2,
   "order": 3,
   "p1": "16",
   "pn": "17",
   "abstract": [
    "Hollywood films and CSI-like movies show a technology landscape far from real, both in forensic speaker recognition and other identification-of-the-source forensic areas. Lay persons are used to good-looking scientist-and-investigators performing voice identifications (\"we got a match!\") or smart fancy devices producing voice transformations causing one actor to instantaneously talk with the voice of other. Simultaneously, Forensic Identification Science is facing a global challenge impelled firstly by progressively higher requirements for admissibility of expert testimony in Court and secondly by the transparent and testable nature of DNA typing, which is now seen as the new gold-standard model of a scientifically defensible approach to be emulated by all other identification-of-the-source areas. In this presentation we will show how forensic speaker recognition can comply with the requirements of transparency and testability in forensic science This will lead to fulfilling the court requirements about role separation between scientists and judges/juries, and bring about integration in a forensically adequate framework in which the scientist provides the appropriate information necessary to the court's decision processes.\n",
    ""
   ]
  },
  "cassell08_interspeech": {
   "authors": [
    [
     "Justine",
     "Cassell"
    ]
   ],
   "title": "Modelling rapport in embodied conversational agents",
   "original": "i08_0018",
   "page_count": 2,
   "order": 4,
   "p1": "18",
   "pn": "19",
   "abstract": [
    "In this talk I report on a series of studies that attempt to characterize the role of language and nonverbal behavior in relationship-building and rapport in humans, and then to use the results to implement embodied conversational agents capable of rapport with their users. In particular, we are implementing virtual survey interviewers that can use rapport to elicit truthful responses, and virtual direction-giving agents that behave differently as they give directions over the lifetime of use. We are implementing virtual peers that can engage in collaborative learning with children within different dialect communities, virtual peers that can scaffold the learning of rapport behaviors in children with autism spectrum disorder, and virtual peers that can be used to assess the social skills deficits of children with autism spectrum disorder so as to better plan their treatment. The goal of the research program is to better understand linguistic and nonverbal coordination devices from the utterance level to the relationship level: how they work in humans, how they can be modeled in virtual humans, and how virtual humans can be implemented to help humans have productive and satisfying relationships, with machines and with one another, over long perids of time\n",
    ""
   ]
  },
  "han08_interspeech": {
   "authors": [
    [
     "Kyu J.",
     "Han"
    ],
    [
     "Shrikanth S.",
     "Narayanan"
    ]
   ],
   "title": "Agglomerative hierarchical speaker clustering using incremental Gaussian mixture cluster modeling",
   "original": "i08_0020",
   "page_count": 4,
   "order": 5,
   "p1": "20",
   "pn": "23",
   "abstract": [
    "This paper proposes a novel cluster modeling method for intercluster distance measurement within the framework of agglomerative hierarchical speaker clustering, namely, incremental Gaussian mixture cluster modeling. This method uses a single Gaussian distribution to model each initial cluster, but represents any newly merged cluster using a distribution whose pdf is the weighted sum of the pdf's of the respective model distributions for the clusters involved in the particular merging process. As a result, clusters are smoothly transitioned to be modeled by Gaussian mixtures whose components are incremented as merging recursions continue during clustering. The proposed method can overcome the limited cluster representation capability of conventional single Gaussian cluster modeling. Through experiments on various sets of initial clusters, it is demonstrated that our approach consequently improves the reliability of speaker clustering performance.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-3"
  },
  "benharush08_interspeech": {
   "authors": [
    [
     "Oshry",
     "Ben-Harush"
    ],
    [
     "Itshak",
     "Lapidot"
    ],
    [
     "Hugo",
     "Guterman"
    ]
   ],
   "title": "Weighted segmental k-means initialization for SOM-based speaker clustering",
   "original": "i08_0024",
   "page_count": 4,
   "order": 6,
   "p1": "24",
   "pn": "27",
   "abstract": [
    "A new approach for initial assignment of data in a speaker clustering application is presented. This approach employs Weighted Segmental K-Means clustering algorithm prior to competitive based learning. The clustering system relies on Self-Organizing Maps (SOM) for speaker modeling and likelihood estimation. Performance is evaluated on 108 two speaker conversations taken from LDC CALLHOME American English Speech corpus using NIST criterion and shows an improvement of approximately 48% in Cluster Error Rate (CER) relative to the randomly initialized clustering system. The number of iterations was reduced significantly, which contributes to both speed and efficiency of the clustering system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-4"
  },
  "ikbal08_interspeech": {
   "authors": [
    [
     "Shajith",
     "Ikbal"
    ],
    [
     "Karthik",
     "Visweswariah"
    ]
   ],
   "title": "Learning essential speaker sub-space using hetero-associative neural networks for speaker clustering",
   "original": "i08_0028",
   "page_count": 4,
   "order": 7,
   "p1": "28",
   "pn": "31",
   "abstract": [
    "In this paper, we present a novel approach to speaker clustering involving the use of hetero-associative neural network (HANN) to compute very low dimensional speaker discriminatory features (in our case 1-dimensional) in a data-driven manner. A HANN trained to map input feature space onto speaker labels through a bottle-neck hidden layer is expected to learn very low dimensional feature subspace essentially containing speaker information. The lower dimensional features are further used in a simple k-means clustering algorithm to obtain speaker segmentation. Evaluation of this approach on a database of real-life conversational speech from call-centers show that clustering performance achieved is similar to that of the state-of-the-art systems, although our approach uses just 1-dimensional features. Augmenting these features with the traditional mel-frequency cepstral coefficients (MFCC) features in the state-of-the-art system resulted in improved clustering performance.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-5"
  },
  "boakye08_interspeech": {
   "authors": [
    [
     "Kofi",
     "Boakye"
    ],
    [
     "Oriol",
     "Vinyals"
    ],
    [
     "Gerald",
     "Friedland"
    ]
   ],
   "title": "Two's a crowd: improving speaker diarization by automatically identifying and excluding overlapped speech",
   "original": "i08_0032",
   "page_count": 4,
   "order": 8,
   "p1": "32",
   "pn": "35",
   "abstract": [
    "We present an update to our initial work [1] on overlapped speech detection for improving speaker diarization. Specifically, we describe the addition of new features and feature warping techniques that improve segmenter and, consequently, diarization performance. We also demonstrate improved diarization performance by additionally using overlap segment information in a new diarization pre-processing step which excludes overlap segments from speaker clustering. On a subset of the AMI Meeting Corpus we show that this overlap exclusion step nearly triples the relative improvement of diarization error rate as compared to overlap segment post-processing alone.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-6"
  },
  "nguyen08_interspeech": {
   "authors": [
    [
     "Trung Hieu",
     "Nguyen"
    ],
    [
     "Eng Siong",
     "Chng"
    ],
    [
     "Haizhou",
     "Li"
    ]
   ],
   "title": "T-test distance and clustering criterion for speaker diarization",
   "original": "i08_0036",
   "page_count": 4,
   "order": 9,
   "p1": "36",
   "pn": "39",
   "abstract": [
    "In this paper, we present an application of student's t-test to measure the similarity between two speaker models. The measure is evaluated by comparing with other distance metrics: the Generalized Likelihood Ratio, the Cross Likelihood Ratio and the Normalized Cross Likelihood Ratio in speaker detection task. We also propose an objective criterion for speaker clustering. The criterion deduces the number of speakers automatically by maximizing the separation between intra-speaker distances and inter-speaker distances. It requires no development data and works well with various distance metrics. We then report the performance of our proposed similarity distance measure and objective criterion in speaker diarization task. The system produces competitive results: low speaker diarization error rate and high accuracy in detecting number of speakers.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-7"
  },
  "vijayasenan08_interspeech": {
   "authors": [
    [
     "Deepu",
     "Vijayasenan"
    ],
    [
     "Fabio",
     "Valente"
    ],
    [
     "Hervé",
     "Bourlard"
    ]
   ],
   "title": "Integration of TDOA features in information bottleneck framework for fast speaker diarization",
   "original": "i08_0040",
   "page_count": 4,
   "order": 10,
   "p1": "40",
   "pn": "43",
   "abstract": [
    "In this paper we address the combination of multiple feature streams in a fast speaker diarization system for meeting recordings. Whenever Multiple Distant Microphones (MDM) are used, it is possible to estimate the Time Delay of Arrival (TDOA) for different channels. In [1], it is shown that TDOA can be used as additional features together with conventional spectral features for improving speaker diarization. We investigate here the combination of TDOA and spectral features in a fast diarization system based on the Information Bottleneck principle. We evaluate the algorithm on the NIST RT06 diarization task. Adding TDOA features to spectral features reduces the speaker error by 7% absolute. Results are comparable to those of conventional HMM/GMM based systems with consistent reduction in computational complexity.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-8"
  },
  "ramasubramanian08_interspeech": {
   "authors": [
    [
     "V.",
     "Ramasubramanian"
    ],
    [
     "D.",
     "Harish"
    ]
   ],
   "title": "Low complexity near-optimal unit-selection algorithm for ultra low bit-rate speech coding based on n-best lattice and Viterbi search",
   "original": "i08_0044",
   "page_count": 1,
   "order": 11,
   "p1": "44",
   "pn": "",
   "abstract": [
    "We propose a low complexity unit-selection algorithm for ultra low bit-rate speech coding based on a first-stage N-best prequantization lattice and a second-stage run-length constrained Viterbi search to efficiently approximate the complete search space of the fully-optimal unit-selection algorithm recently proposed by us. By this, the proposed low complexity algorithm continues to be near-optimal in terms of rate-distortion performance while having highly reduced complexity.\n",
    ""
   ]
  },
  "eksler08_interspeech": {
   "authors": [
    [
     "Vaclav",
     "Eksler"
    ],
    [
     "Redwan",
     "Salami"
    ],
    [
     "Milan",
     "Jelinek"
    ]
   ],
   "title": "A new fast algebraic fixed codebook search algorithm in CELP speech coding",
   "original": "i08_0045",
   "page_count": 4,
   "order": 12,
   "p1": "45",
   "pn": "48",
   "abstract": [
    "This paper introduces a very fast search algorithm of algebraic fixed codebook in CELP-based speech codecs. The proposed method searches codebook pulses sequentially, and recomputes the fixed codebook gain, the so-called backward filtered target vector, and a certain reference signal after each new pulse is determined. This results in a significant complexity reduction compared to existing methods while preserving the same speech quality. The presented algorithm is used in the new embedded speech and audio codec (G.EV-VBR) being currently standardized by the ITU-T.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-9"
  },
  "xu08_interspeech": {
   "authors": [
    [
     "Hao",
     "Xu"
    ],
    [
     "Changchun",
     "Bao"
    ]
   ],
   "title": "A novel transcoding algorithm between 3GPP AMR-NB (7.95kbit/s) and ITU-t g.729a (8kbit/s)",
   "original": "i08_0049",
   "page_count": 4,
   "order": 13,
   "p1": "49",
   "pn": "52",
   "abstract": [
    "In this paper, a novel transcoding algorithm specially related to codebook gain conversion is proposed between AMR-NB at 7.95kb/s and G.729a. It can bypass the gain predictive process and directly convert codebook gain parameters. Additionally, the new gain parameter conversion method can be extended to the other rate modes of AMR-NB while transcoding with G.729a. The experimental results show that the quality of transcoded speech using the proposed algorithm is improved greatly and the computational complexity is reduced by 85% compared with DTE (Decode then Encode) method. A 5ms look-ahead delay is avoided as well.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-10"
  },
  "noureldin08_interspeech": {
   "authors": [
    [
     "Amr H.",
     "Nour-Eldin"
    ],
    [
     "Peter",
     "Kabal"
    ]
   ],
   "title": "Mel-frequency cepstral coefficient-based bandwidth extension of narrowband speech",
   "original": "i08_0053",
   "page_count": 4,
   "order": 14,
   "p1": "53",
   "pn": "56",
   "abstract": [
    "We present a novel MFCC-based scheme for the Bandwidth Extension (BWE) of narrowband speech. BWE is based on the assumption that narrowband speech (0.3.3.4 kHz) correlates closely with the highband signal (3.4.7 kHz), enabling estimation of the highband frequency content given the narrow band. While BWE schemes have traditionally used LP-based parametrizations, our recent work has shown that MFCC parametrization results in higher correlation between both bands reaching twice that using LSFs. By employing high-resolution IDCT of highband MFCCs obtained from narrowband MFCCs by statistical estimation, we achieve high-quality highband power spectra from which the time-domain speech signal can be reconstructed. Implementing this scheme for BWE translates the higher correlation advantage of MFCCs into BWE performance superior to that obtained using LSFs, as shown by improvements in log-spectral distortion as well as Itakura-based measures (the latter improving by up to 13%).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-11"
  },
  "garcia08_interspeech": {
   "authors": [
    [
     "Jean-Luc",
     "Garcia"
    ],
    [
     "Claude",
     "Marro"
    ],
    [
     "Balazs",
     "Kövesi"
    ]
   ],
   "title": "A PCM coding noise reduction for ITU-t g.711.1",
   "original": "i08_0057",
   "page_count": 4,
   "order": 15,
   "p1": "57",
   "pn": "60",
   "abstract": [
    "The ITU-T G.711.1 embedded wideband speech codec was approved by ITU-T in March 2008. This codec generates a bitstream comprised of three layers: a G.711 compatible core layer with noise shaping, a lower band enhancement layer and an MDCT-based higher band enhancement layer. It contains also an optional postprocessing module called Appendix I designed to improve quality of the decoded speech in case of interoperability condition with legacy G.711 encoder. The improvement is achieved by a novel low complexity PCM quantization noise reduction technique described in this article. Subjective test results show that the quality of the interoperability mode with the legacy G.711 codec is significantly better when the Appendix I is activated.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-12"
  },
  "waltermann08_interspeech": {
   "authors": [
    [
     "Marcel",
     "Wältermann"
    ],
    [
     "Kirstin",
     "Scholz"
    ],
    [
     "Sebastian",
     "Möller"
    ],
    [
     "Lu",
     "Huo"
    ],
    [
     "Alexander",
     "Raake"
    ],
    [
     "Ulrich",
     "Heute"
    ]
   ],
   "title": "An instrumental measure for end-to-end speech transmission quality based on perceptual dimensions: framework and realization",
   "original": "i08_0061",
   "page_count": 4,
   "order": 16,
   "p1": "61",
   "pn": "64",
   "abstract": [
    "In this contribution, a new instrumental measure for end-to-end speech transmission quality is presented which is based on perceptually relevant dimensions. The paper describes the complete scientific development process of such a measure, starting off from the general framework and concluding with the concrete realization. The measure is based on the dimensions \"discontinuity\", \"noisiness\", and \"coloration\", which were identified through multidimensional analyses. Three dimension estimators are introduced which are capable to predict so-called dimension impairment factors on the basis of signal parameters. Each dimension impairment factor reflects the degradation with respect to a single perceptual dimension. By combining the impairment factors, integral quality can be estimated. A maximum correlation of r = 0.9 with auditory test results is achieved for a wide range of perceptually different conditions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-13"
  },
  "peters08_interspeech": {
   "authors": [
    [
     "Benno",
     "Peters"
    ],
    [
     "Hartmut R.",
     "Pfitzinger"
    ]
   ],
   "title": "Duration and F0 interval of utterance-final intonation contours in the perception of German sentence modality",
   "original": "i08_0065",
   "page_count": 4,
   "order": 17,
   "p1": "65",
   "pn": "68",
   "abstract": [
    "This paper investigates the influence of duration and F0 interval of the utterance-final F0 contour on the perception of sentence modality, i.e. declarative or interrogative sentence. An utterancefinal rising contour with a constant F0 interval of 2 semitones or more and a voicing duration of at least 50 ms leads to unanimously identified interrogative modality. Even at durations of 20 and 30 ms a significant number of listeners is able to consistently identify sentence modality. F0 interval seems to better predict perceived sentence modality than F0 slope.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-14"
  },
  "braun08_interspeech": {
   "authors": [
    [
     "Bettina",
     "Braun"
    ],
    [
     "Lara",
     "Tagliapietra"
    ],
    [
     "Anne",
     "Cutler"
    ]
   ],
   "title": "Contrastive utterances make alternatives salient - cross-modal priming evidence",
   "original": "i08_0069",
   "page_count": 1,
   "order": 18,
   "p1": "69",
   "pn": "",
   "abstract": [
    "Sentences with contrastive intonation are assumed to presuppose contextual alternatives to the accented elements. Two cross-modal priming experiments tested in Dutch whether such contextual alternatives are automatically available to listeners. Contrastive associates - but not non-contrastive associates - were facilitated only when primes were produced in sentences with contrastive intonation, indicating that contrastive intonation makes unmentioned contextual alternatives immediately available. Possibly, contrastive contours trigger a \"presupposition resolution mechanism\" by which these alternatives become salient.\n",
    ""
   ]
  },
  "ishizaki08_interspeech": {
   "authors": [
    [
     "Masato",
     "Ishizaki"
    ],
    [
     "Yasuharu",
     "Den"
    ],
    [
     "Senshi",
     "Fukashiro"
    ]
   ],
   "title": "Exploring a mechanism of speech sychronization using auditory delayed experiments",
   "original": "i08_0070",
   "page_count": 4,
   "order": 19,
   "p1": "70",
   "pn": "73",
   "abstract": [
    "This paper investigated how speakers synchronize their speech by experiments in which the participants naturally and simultaneously recited under auditory delayed conditions. Statistical analysis revealed that the speakers changed strategies to adjust the timing of their utterances. This finding constitutes one fundamental mechanism for coordinating verbal behavior that can contribute to designing comfortable interactions with virtual agents or robots.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-15"
  },
  "ponbarry08_interspeech": {
   "authors": [
    [
     "Heather",
     "Pon-Barry"
    ]
   ],
   "title": "Prosodic manifestations of confidence and uncertainty in spoken language",
   "original": "i08_0074",
   "page_count": 4,
   "order": 20,
   "p1": "74",
   "pn": "77",
   "abstract": [
    "We present a project aimed at understanding the acoustic and prosodic correlates of confidence and uncertainty in spoken language. We elicited speech produced under varying levels of certainty and performed perceptual and statistical analyses on the speech data to determine which prosodic features (e.g., pitch, energy, timing) are associated with a speaker's level of certainty and where these prosodic manifestations occur relative to the location of the word or phrase that the speaker is confident or uncertain about. Our findings suggest that prosodic manifestations of confidence and uncertainty occur both in the local region that causes the uncertainty as well as in its surrounding context.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-16"
  },
  "fernandez08_interspeech": {
   "authors": [
    [
     "Raquel",
     "Fernandez"
    ],
    [
     "Matthew",
     "Frampton"
    ],
    [
     "John",
     "Dowding"
    ],
    [
     "Anish",
     "Adukuzhiyil"
    ],
    [
     "Patrick",
     "Ehlen"
    ],
    [
     "Stanley",
     "Peters"
    ]
   ],
   "title": "Identifying relevant phrases to summarize decisions in spoken meetings",
   "original": "i08_0078",
   "page_count": 4,
   "order": 21,
   "p1": "78",
   "pn": "81",
   "abstract": [
    "We address the problem of identifying words and phrases that accurately capture, or contribute to, the semantic gist of decisions made in multi-party human-human meetings. We first describe our approach to modelling decision discussions in spoken meetings and then compare two approaches to extracting information from these discussions. The first one uses an open-domain semantic parser that identifies candidate phrases for decision summaries and then employs machine learning techniques to select from those candidate phrases. The second one uses categorical and sequential classifiers that exploit simple syntactic and semantic features to identify words and phrases relevant for decision summarization.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-17"
  },
  "laskowski08_interspeech": {
   "authors": [
    [
     "Kornel",
     "Laskowski"
    ],
    [
     "Tanja",
     "Schultz"
    ]
   ],
   "title": "Recovering participant identities in meetings from a probabilistic description of vocal interaction",
   "original": "i08_0082",
   "page_count": 4,
   "order": 22,
   "p1": "82",
   "pn": "85",
   "abstract": [
    "An important decision in the design of automatic conversation understanding systems is the level at which information streams representing specific participants are merged. In the current work, we explore participant-dependence of low-level interactive aspects of conversation, namely the observed contextual preferences for talkspurt deployment. We argue that strong participant-dependence at this level gives cause for merging participant streams as early as possible. We demonstrate that our probabilistic description of talkspurt deployment preferences is strongly participant-dependent, and frequently predictive of participant identity.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-18"
  },
  "fletcher08_interspeech": {
   "authors": [
    [
     "Janet",
     "Fletcher"
    ],
    [
     "Deborah",
     "Loakes"
    ],
    [
     "Andrew",
     "Butcher"
    ]
   ],
   "title": "Coarticulation in nasal and lateral clusters in Warlpiri",
   "original": "i08_0086",
   "page_count": 4,
   "order": 23,
   "p1": "86",
   "pn": "89",
   "abstract": [
    "Indigenous Australian languages are said to show remarkable stability in C1C2 sequences with no evidence of assimilation of place of articulation. An EPG corpus of Warlpiri was examined to test the extent of spatio-temporal modification in a series of nasal and lateral/oral stop clusters that differed in place of articulation. There was evidence of limited anticipatory coarticulation in nasal clusters. Laminal palatal sonorants also exerted the strongest carryover coarticulatory effects on the following consonant although place contrasts were maintained showing that the extent of coarticulation (both spatially and temporally) is somewhat constrained by the phonological structures of the language.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-19"
  },
  "loakes08_interspeech": {
   "authors": [
    [
     "Deborah",
     "Loakes"
    ],
    [
     "Andrew",
     "Butcher"
    ],
    [
     "Janet",
     "Fletcher"
    ],
    [
     "Hywel",
     "Stoakes"
    ]
   ],
   "title": "Phonetically prestopped laterals in Australian languages: a preliminary investigation of Warlpiri",
   "original": "i08_0090",
   "page_count": 4,
   "order": 24,
   "p1": "90",
   "pn": "93",
   "abstract": [
    "Phonologically prestopped nasals occur primarily in central and southern Australian languages. Phonetically prestopped nasals on the other hand, occur in a large number of Australian languages and are not isolated to one particular region. Phonetically prestopped nasals have been analysed as a preservation of spectral characteristics at vowel-sonorant boundaries in languages which have a comparatively large number of sonorant contrasts. In this paper we describe acoustic, articulatory and durational characteristics of rarely mentioned phonetically prestopped laterals, in the Australian language Warlpiri. We conclude that like prestopped nasals, prestopped laterals are likely to be the outcome of a coarticulatory avoidance strategy to preserve the left-edge of the sonorant. While not auditorily salient, we report on the frequent distribution and very distinctive phonetic characteristics associated with prestopped laterals.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-20"
  },
  "ingram08_interspeech": {
   "authors": [
    [
     "John",
     "Ingram"
    ],
    [
     "Mary",
     "Laughren"
    ],
    [
     "Jeff",
     "Chapman"
    ]
   ],
   "title": "Connected speech processes in Warlpiri",
   "original": "i08_0094",
   "page_count": 1,
   "order": 25,
   "p1": "94",
   "pn": "",
   "abstract": [
    "Connected speech processes (CSP) in Warlpiri, an indigenous language of central Australia, taken from two fluent 'dreaming' monologues are analyzed with the aim of observing the influence of language particular phonological constraints and prosody upon phonetic processes of lenition, co-articulation and selective segmental enhancement.\n",
    ""
   ]
  },
  "pentland08_interspeech": {
   "authors": [
    [
     "Christina",
     "Pentland"
    ]
   ],
   "title": "Consonant enhancement in Lamalama, an initial-dropping language of Cape York Peninsula, North Queensland",
   "original": "i08_0095",
   "page_count": 1,
   "order": 26,
   "p1": "95",
   "pn": "",
   "abstract": [
    "In this paper I describe the consonant system of Lamalama and show that it is an extreme example of an initial dropping language in which consonants are enhanced or strengthened in word-initial position. Initial dropping languages have undergone a series of radical sound changes including - but not limited to - the loss of word-initial consonants. Lamalama has lost entire word-initial syllables with the result that most roots are monosyllabic C(C)V(C) forms. In a reduced phonological system various strategies are used to enhance word-initial consonants and maintain consonantal contrasts.\n",
    ""
   ]
  },
  "turpin08_interspeech": {
   "authors": [
    [
     "Myfany",
     "Turpin"
    ]
   ],
   "title": "Text, rhythm and metrical form in an Aboriginal song series",
   "original": "i08_0096",
   "page_count": 3,
   "order": 27,
   "p1": "96",
   "pn": "98",
   "abstract": [
    "Setting words to (musical) rhythm is an attempt to match rhythmic positions and syllables in an aesthetically appealing manner. In English songs acceptability is based on two separate but interactive judgments: matching stress with metrically strong positions, and matching prosodic constituents with rhythmic constituents [1]. This paper investigates a genre of Aboriginal songs and finds that while prosodic and rhythmic constituents match, there is no requirement to match stress. Instead, the placement of syllables is conditioned by a caesura (word boundary rule) and a hierarchy whereby rhythmical units with fewer notes must not precede ones with more.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-21"
  },
  "ishizuka08_interspeech": {
   "authors": [
    [
     "Kentaro",
     "Ishizuka"
    ],
    [
     "Shoko",
     "Araki"
    ],
    [
     "Tatsuya",
     "Kawahara"
    ]
   ],
   "title": "Statistical speech activity detection based on spatial power distribution for analyses of poster presentations",
   "original": "i08_0099",
   "page_count": 4,
   "order": 28,
   "p1": "99",
   "pn": "102",
   "abstract": [
    "This paper proposes a microphone array based statistical speech activity detection (SAD) method for analyses of poster presentations recorded in the presence of noise. Such poster presentations are a kind of multi-party conversation, where the number of speakers and speaker location are unrestricted, and directional noise sources affect the direction of arrival of the target speech signals. To detect speech activity in such cases without a priori knowledge about the speakers and noise environments, we applied a likelihood ratio test based SAD method to spatial power distributions. The proposed method can exploit the enhanced signals obtained from time-frequency masking, and work even in the presence of environmental noise by utilizing the a priori signal-to-noise ratios of the spatial power distributions. Experiments with recorded poster presentations confirmed that the proposed method significantly improves the SAD accuracies compared with those obtained with a frequency spectrum based statistical SAD method.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-22"
  },
  "kang08_interspeech": {
   "authors": [
    [
     "Sang-Ick",
     "Kang"
    ],
    [
     "Ji-Hyun",
     "Song"
    ],
    [
     "Kye-Hwan",
     "Lee"
    ],
    [
     "Yun-Sik",
     "Park"
    ],
    [
     "Joon-Hyuk",
     "Chang"
    ]
   ],
   "title": "A statistical model-based voice activity detection employing minimum classification error technique",
   "original": "i08_0103",
   "page_count": 4,
   "order": 29,
   "p1": "103",
   "pn": "106",
   "abstract": [
    "In this paper, we apply a discriminative weight training to a statistical model-based voice activity detection (VAD). In our approach, the VAD decision rule is expressed as the geometric mean of optimally weighted likelihood ratios (LRs) based on a minimum classification error (MCE) method. That approach is different from that of previous works in that different weights are assigned to each frequency bin and is considered to be more realistic. According to the experimental results, the proposed approach is found to be effective for the statistical model-based VAD using the LR test.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-23"
  },
  "ding08_interspeech": {
   "authors": [
    [
     "Hongfei",
     "Ding"
    ],
    [
     "Koichi",
     "Yamamoto"
    ],
    [
     "Masami",
     "Akamine"
    ]
   ],
   "title": "Comparative evaluation of different methods for voice activity detection",
   "original": "i08_0107",
   "page_count": 4,
   "order": 30,
   "p1": "107",
   "pn": "110",
   "abstract": [
    "This paper presents a comparative evaluation of different methods for voice activity detection (VAD). A novel feature set is proposed in order to improve VAD performance in diverse noisy environments. Furthermore, three classifiers for VAD are evaluated. The three classifiers are Gaussian Mixture Model (GMM), Support Vector Machine (SVM) and Decision Tree (DT). Experimental results show that the proposed feature set achieves better performance than spectral entropy. In the comparison of the classifiers, DT shows the best performance in terms of frame-based VAD accuracy as well as computational cost.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-24"
  },
  "shafiee08_interspeech": {
   "authors": [
    [
     "Soheil",
     "Shafiee"
    ],
    [
     "Farshad",
     "Almasganj"
    ],
    [
     "Ayyoob",
     "Jafari"
    ]
   ],
   "title": "Speech/non-speech segments detection based on chaotic and prosodic features",
   "original": "i08_0111",
   "page_count": 4,
   "order": 31,
   "p1": "111",
   "pn": "114",
   "abstract": [
    "Every speech recognition system contains a speech/non-speech detection stage. Detected speech sequences are only passed through the speech recognition stage later on. In a noisy environment, non-speech segments can be an important source of error. In this work, we introduce a new speech/non-speech detection system based on fractal dimension and prosodic features plus the common used MFCC features. We evaluated our system performance using neural network and SVM classifiers on TIMIT speech database with a HMM based speech recognizer. Experimental results show very good performance in speech/non-speech detection.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-25"
  },
  "zieger08_interspeech": {
   "authors": [
    [
     "Christian",
     "Zieger"
    ],
    [
     "Maurizio",
     "Omologo"
    ]
   ],
   "title": "Acoustic event classification using a distributed microphone network with a GMM/SVM combined algorithm",
   "original": "i08_0115",
   "page_count": 4,
   "order": 32,
   "p1": "115",
   "pn": "118",
   "abstract": [
    "This work proposes a system for acoustic event classification using signals acquired by a Distributed Microphone Network (DMN). The system is based on the combination of Gaussian Mixture Models (GMM) and Support Vector Machines (SVM). The acoustic event list includes both speech and non-speech events typical of seminars and meetings. The robustness of the system was investigated by considering two scenarios characterized by different types of trained models and testing conditions. Experimental results were obtained by using real-world data collected at two sites. The results in terms of classification error rate show that in each scenario the proposed system outperforms any single classifier based system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-26"
  },
  "obuchi08_interspeech": {
   "authors": [
    [
     "Yasunari",
     "Obuchi"
    ],
    [
     "Masahito",
     "Togami"
    ],
    [
     "Takashi",
     "Sumiyoshi"
    ]
   ],
   "title": "Intentional voice command detection for completely hands-free speech interface in home environments",
   "original": "i08_0119",
   "page_count": 4,
   "order": 33,
   "p1": "119",
   "pn": "122",
   "abstract": [
    "We introduce a new class of speech processing, called Intentional Voice Command Detection (IVCD). It is necessary to reject not only noises but also unintended voices to achieve completely hands-free speech interface. Conventional VAD framework is not sufficient for such purpose, and we discuss how we should define IVCD and how we can realize it. We investigate implementation of IVCD from the viewpoint of feature extraction and classification, and show that the combination of various features and SVM can achieve IVCD accuracy of 93.2% for a large-scale audio database in real home environments.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-27"
  },
  "butko08_interspeech": {
   "authors": [
    [
     "Taras",
     "Butko"
    ],
    [
     "Andrey",
     "Temko"
    ],
    [
     "Climent",
     "Nadeu"
    ],
    [
     "Cristian",
     "Canton"
    ]
   ],
   "title": "Fusion of audio and video modalities for detection of acoustic events",
   "original": "i08_0123",
   "page_count": 4,
   "order": 34,
   "p1": "123",
   "pn": "126",
   "abstract": [
    "Detection of acoustic events (AED) that take place in a meetingroom environment becomes a difficult task when signals show a large proportion of temporal overlap of sounds, like in seminartype data, where the acoustic events often occur simultaneously with speech. Whenever the event that produces the sound is related to a given position or movement, video signals may be a useful additional source of information for AED. In this work, we aim at improving the AED accuracy by using two complementary audio-based AED systems, built with SVM and HMM classifiers, and also a video-based AED system, which employs the output of a 3D video tracking algorithm to improve detection of steps. Fuzzy integral is used to fuse the outputs of the three classification systems in two stages. Experimental results using the CLEAR'07 evaluation data show that the detection rate increases by fusing the two audio information sources, and it is further improved by including video information.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-28"
  },
  "weiss08_interspeech": {
   "authors": [
    [
     "Ron J.",
     "Weiss"
    ],
    [
     "Trausti",
     "Kristjansson"
    ]
   ],
   "title": "DySANA: dynamic speech and noise adaptation for voice activity detection",
   "original": "i08_0127",
   "page_count": 4,
   "order": 35,
   "p1": "127",
   "pn": "130",
   "abstract": [
    "We describe a method of simultaneously tracking noise and speech levels for signal-to-noise ratio adaptive speech endpoint detection. The method is based on the Kalman filter framework with switching observations and uses a dynamic distribution that 1) limits the rate of change of these levels 2) enforces a range on the values for the two levels and 3) enforces a ratio between the noise and the signal levels. We call this a Lombard dynamic distribution since it encodes the expectation that a speaker will increase his or her vocal intensity in noise. The method also employs a state transition matrix which encodes a prior on the states and provides a continuity constraint. The new method provides 46.1% relative improvement in WER over a baseline GMM based endpointer at 20 dB SNR.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-29"
  },
  "petrick08_interspeech": {
   "authors": [
    [
     "Rico",
     "Petrick"
    ],
    [
     "Masashi",
     "Unoki"
    ],
    [
     "Anish",
     "Mittal"
    ],
    [
     "Carlos",
     "Segura"
    ],
    [
     "Rüdiger",
     "Hoffmann"
    ]
   ],
   "title": "A comprehensive study on the effects of room reverberation on fundamental frequency estimation",
   "original": "i08_0131",
   "page_count": 4,
   "order": 36,
   "p1": "131",
   "pn": "134",
   "abstract": [
    "This paper takes note of the major influences of room acoustic effects on the fundamental frequency F0 of speech and its determination. A detailed description of room acoustic measures and effects is given. As a conclusion of those the dependency of the speaker to microphone distance (SMD) is to be studied combined with the reverberation time T60. Evaluation experiments aiming to find dependencies of the room acoustic effects are accomplished. In contrast to most of the studies which deal with reverberation, this paper proves that T60 cannot be the only measure to describe the behavior of systems in reverberant environments. Experiments studying the dependency on the SMD are new contributions within this paper. The experiments are an extension of the previous studies on the dependency on artificial reverberation, where twelve F0 estimation methods are compared. Here, two further methods are added and the systematic use of real measured room impulse responses (RIR) is applied. Apart from the SMD dependency other main contributions are the experiments of different disturbing effects on high and low F0. The results show male F0 estimation is significantly more sensitive to reverberation than female.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-30"
  },
  "hussein08_interspeech": {
   "authors": [
    [
     "H.",
     "Hussein"
    ],
    [
     "M.",
     "Wolff"
    ],
    [
     "Oliver",
     "Jokisch"
    ],
    [
     "F.",
     "Duckhorn"
    ],
    [
     "G.",
     "Strecha"
    ],
    [
     "Rüdiger",
     "Hoffmann"
    ]
   ],
   "title": "A hybrid speech signal based algorithm for pitch marking using finite state machines",
   "original": "i08_0135",
   "page_count": 4,
   "order": 37,
   "p1": "135",
   "pn": "138",
   "abstract": [
    "Pitch marking is a major task in speech processing. Thus, an accurate detection of pitch marks (PM) is required. In this paper, we propose a hybrid method for pitch marking that combines outputs of two different speech signal based pitch marking algorithms (PMA). We use a finite state machine (FSM) to represent and combine the pitch marks. The hybrid PMA is implemented in four stages: preprocessing, alignment, selection and postprocessing. In the alignment stage, the preprocessed pitch marks are shifted to a local minimum of the speech signal and the confidence score for every pitch mark is calculated. The confidence scores are used as transition weights for the FSM. The PMA outputs are combined into a single sequence of pitch marks. The more accurate pitch marks with the highest confidence score are chosen in the selection stage. A PM reference database contains 10 minutes speech including manually adjusted PM. The evaluation results indicate that the proposed hybrid method outperforms the single PMAs but also other current state-of-the-art algorithms which have been evaluated on a second reference database containing 44 speakers.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-31"
  },
  "ohishi08_interspeech": {
   "authors": [
    [
     "Yasunori",
     "Ohishi"
    ],
    [
     "Hirokazu",
     "Kameoka"
    ],
    [
     "Kunio",
     "Kashino"
    ],
    [
     "Kazuya",
     "Takeda"
    ]
   ],
   "title": "Parameter estimation method of F0 control model for singing voices",
   "original": "i08_0139",
   "page_count": 4,
   "order": 38,
   "p1": "139",
   "pn": "142",
   "abstract": [
    "In this paper, we propose a novel representation of F0 contours that provides a computationally efficient algorithm for automatically estimating the parameters of a F0 control model for singing voices. Although the best known F0 control model, based on a second-order system with a piece-wise constant function as its input, can generate F0 contours of natural singing voices, this model has no means of learning the model parameters from observed F0 contours automatically. Therefore, by modeling the piece-wise constant function by Hidden Markov Models (HMM) and approximating the second order differential equation by the difference equation, we estimate model parameters optimally based on iteration of Viterbi training and an LPC-like solver. Our representation is a generative model and can identify both the target musical note sequence and the dynamics of singing behaviors included in the F0 contours. Our experimental results show that the proposed method can separate the dynamics from the target musical note sequence and generate the F0 contours using estimated model parameters.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-32"
  },
  "vishnubhotla08_interspeech": {
   "authors": [
    [
     "Srikanth",
     "Vishnubhotla"
    ],
    [
     "Carol Y.",
     "Espy-Wilson"
    ]
   ],
   "title": "An algorithm for multi-pitch tracking in co-channel speech",
   "original": "i08_0143",
   "page_count": 4,
   "order": 39,
   "p1": "143",
   "pn": "146",
   "abstract": [
    "Most multi-pitch algorithms are tested for performance only in voiced regions of speech, and are prone to yield pitch estimates even when the participating speakers are unvoiced. This paper presents a multi-pitch algorithm that detects the voiced and unvoiced regions in a mixture of two speakers, identifies the number of speakers in voiced regions, and yields the pitch estimates of each speaker in those regions. The algorithm relies on the 2-Dimensional AMDF for estimating the periodicity of the signal, and uses the temporal evolution of the 2-D AMDF to estimate the number of speakers present in periodic regions. Evaluation of this algorithm on a frame-wise basis demonstrates accurate voiced / unvoiced decisions and also gives pitch estimation results comparable to the state of the art. The pitch estimation errors are quantitatively analyzed and shown to be resulting partly from speaker domination & pitch matching between speakers.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-33"
  },
  "wohlmayr08_interspeech": {
   "authors": [
    [
     "Michael",
     "Wohlmayr"
    ],
    [
     "Franz",
     "Pernkopf"
    ]
   ],
   "title": "Multipitch tracking using a factorial hidden Markov model",
   "original": "i08_0147",
   "page_count": 4,
   "order": 40,
   "p1": "147",
   "pn": "150",
   "abstract": [
    "In this paper, we present an approach to track the pitch of two simultaneous speakers. Using a well-known feature extraction method based on the correlogram, we track the resulting data using a factorial hidden Markov model (FHMM). In contrast to the recently developed multipitch determination algorithm [1], which is based on a HMM, we can accurately associate estimated pitch points with their corresponding source speakers. We evaluate our approach on the \"Mocha-TIMIT\" database [2] of speech utterances mixed at 0dB, and compare the results to the multipitch determination algorithm [1] used as a baseline. Experiments show that our FHMM tracker yields good performance for both pitch estimation and correct speaker assignment.\n",
    "s> Wu M., Wang D. and Brown G.J., \"A Multipitch Tracking Algorithm for Noisy Speech\", IEEE Transactions On Speech and Audio Processing, 11(3):229-241, 2003.\n",
    "Wrench A., \"A multichannel/multispeaker articulatory database for continuous speech recognition research\", Phonus, 5:3-17, 2000\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-34"
  },
  "li08_interspeech": {
   "authors": [
    [
     "Ming",
     "Li"
    ],
    [
     "Chuan",
     "Cao"
    ],
    [
     "Di",
     "Wang"
    ],
    [
     "Ping",
     "Lu"
    ],
    [
     "Qiang",
     "Fu"
    ],
    [
     "Yonghong",
     "Yan"
    ]
   ],
   "title": "Cochannel speech separation using multi-pitch estimation and model based voiced sequential grouping",
   "original": "i08_0151",
   "page_count": 4,
   "order": 41,
   "p1": "151",
   "pn": "154",
   "abstract": [
    "In this paper, a new cochannel speech separation algorithm using multi-pitch extraction and speaker model based sequential grouping is proposed. After auditory segmentation based on onset and offset analysis, robust multi-pitch estimation algorithm is performed on each segment and the corresponding voiced portions are segregated. Then speaker pair model based on support vector machine (SVM) is employed to determine the optimal sequential grouping alignments and group the speaker homogeneous segments into pure speaker streams. Systematic evaluation on the speech separation challenge database shows significant improvement over the baseline performance.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-35"
  },
  "martin08_interspeech": {
   "authors": [
    [
     "Philippe",
     "Martin"
    ]
   ],
   "title": "Crosscorrelation of adjacent spectra enhances fundamental frequency tracking",
   "original": "i08_0155",
   "page_count": 4,
   "order": 42,
   "p1": "155",
   "pn": "158",
   "abstract": [
    "All fundamental frequency estimators based on spectral analysis rely heavily on a proper harmonic selection of the voice analyzed. Since in practice other spectral peaks pertaining to sources external to the speech considered may be present in the signal, various schemes have been designed to ensure a satisfactory elimination of non pertinent harmonics.\n",
    "This paper introduces a new harmonic selection algorithm based on the brush method, which ensures a very good selection of voice harmonics, and gives satisfactory results even in the presence of another speech source. The algorithm has been implemented and incorporated in the speech analysis program WinPitch, and tested on various examples provided in the Speech Separation Challenge.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-36"
  },
  "malek08_interspeech": {
   "authors": [
    [
     "Jiri",
     "Malek"
    ],
    [
     "Zbynek",
     "Koldovsky"
    ],
    [
     "Jindrich",
     "Zdansky"
    ],
    [
     "Jan",
     "Nouza"
    ]
   ],
   "title": "Enhancement of noisy speech recordings via blind source separation",
   "original": "i08_0159",
   "page_count": 4,
   "order": 43,
   "p1": "159",
   "pn": "162",
   "abstract": [
    "We propose an improved time-domain Blind Source Separation method and apply it to speech signal enhancement using multiple microphone recordings. The improvement consists in utilization of fuzzy clustering instead of a hard one, which is verified by experiments where real-world mixtures of two audio signals are separated from two microphones. Performance of the method is demonstrated by recognizing mixed and separated utterances from the Czech part of the European broadcast news database using our Czech LVCSR system. The separation allows significantly better recognition, e.g., by 32% when the jammer signal is a Gaussian noise and the input signal-to-noise ratio is 10dB.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-37"
  },
  "ishibashi08_interspeech": {
   "authors": [
    [
     "Takaaki",
     "Ishibashi"
    ],
    [
     "Hidetoshi",
     "Nakashima"
    ],
    [
     "Hiromu",
     "Gotanda"
    ]
   ],
   "title": "Studies on estimation of the number of sources in blind source separation",
   "original": "i08_0163",
   "page_count": 4,
   "order": 44,
   "p1": "163",
   "pn": "166",
   "abstract": [
    "ICA (Independent Component Analysis) can estimate unknown source signals from their mixtures under the assumption that the source signals are statistically independent. However, in a real environment, the separation performance is often deteriorated because the number of the source signals is different from that of the sensors. In this paper, we propose an estimation method for the number of the sources based on the joint distribution of the observed signals under two-sensor configuration. From several simulation results, it is found that the number of the sources is coincident to that of peaks in the histogram of the distribution. The proposed method can estimate the number of the sources even if it is larger than that of the observed signals.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-38"
  },
  "ramasubramanian08b_interspeech": {
   "authors": [
    [
     "V.",
     "Ramasubramanian"
    ],
    [
     "Deepak",
     "Vijaywargi"
    ]
   ],
   "title": "Speech enhancement based on hypothesized Wiener filtering",
   "original": "i08_0167",
   "page_count": 4,
   "order": 45,
   "p1": "167",
   "pn": "170",
   "abstract": [
    "We propose a novel speech enhancement technique based on the hypothesized Wiener filter (HWF) methodology. The proposed HWF algorithm selects a filter for enhancing the input noisy signal by first 'hypothesizing' a set of filters and then choosing the most appropriate one for the actual filtering. We show that the proposed HWF can intrinsically offer superior performance to conventional Wiener filtering (CWF) algorithms, which typically perform a selection of a filter based only on the noisy input signal which results in a sub-optimal choice of the filter. We present results showing the advantages of HWF based speech enhancement over CWF, particularly with respect to the baseline performances achievable by HWF and with respect to the type of clean frames used, namely, codebooks vs a large number of clean frames. We show the consistently better performance of HWF based speech enhancement (over CWF) in terms of spectral distortion at various input SNR levels.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-39"
  },
  "li08b_interspeech": {
   "authors": [
    [
     "Junfeng",
     "Li"
    ],
    [
     "Hui",
     "Jiang"
    ],
    [
     "Masato",
     "Akagi"
    ]
   ],
   "title": "Psychoacoustically-motivated adaptive β-order generalized spectral subtraction based on data-driven optimization",
   "original": "i08_0171",
   "page_count": 4,
   "order": 46,
   "p1": "171",
   "pn": "174",
   "abstract": [
    "To mitigate the performance limitations caused by the constant spectral order β in the traditional spectral subtraction methods, we previously presented an adaptive β-order generalized spectral subtraction (GSS) in which the spectral order β is updated in a heuristic way. In this paper, we propose a psychoacousticallymotivated adaptive β-order GSS, by considering that different frequency bands contribute different amounts to speech intelligibility (i.e., the band-importance function). Specifically, in this proposed adaptive β-order GSS, the tendency of spectral order β to change with the input local signal-to-noise ratio (SNR) is quantitatively approximated by a sigmoid function, which is derived through a data-driven optimization procedure by minimizing the intelligibility-weighted distance between the desired speech spectrum and its estimate. The inherent parameters of the sigmoid function are further optimized with the data-driven optimization procedure. Experimental results indicate that the proposed psychoacoustically-motivated adaptive β-order GSS yields great improvements over the traditional spectral subtraction methods with the intelligibility-weighted measures.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-40"
  },
  "nandk08_interspeech": {
   "authors": [
    [
     "Krishna",
     "Nand K"
    ],
    [
     "T. V.",
     "Sreenivas"
    ]
   ],
   "title": "Two stage iterative Wiener filtering for speech enhancement",
   "original": "i08_0175",
   "page_count": 4,
   "order": 47,
   "p1": "175",
   "pn": "178",
   "abstract": [
    "We formulate a two-stage Iterative Wiener filtering (IWF) approach to speech enhancement, bettering the performance of constrained IWF, reported in literature. The codebook constrained IWF (CCIWF) has been shown to be effective in achieving convergence of IWF in the presence of both stationary and non-stationary noise. To this, we include a second stage of unconstrained IWF and show that the speech enhancement performance can be improved in terms of average segmental SNR (SSNR), Itakura-Saito (IS) distance and Linear Prediction Coefficients (LPC) parameter coincidence. We also explore the tradeoff between the number of CCIWF iterations and the second stage IWF iterations.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-41"
  },
  "ding08b_interspeech": {
   "authors": [
    [
     "Pei",
     "Ding"
    ],
    [
     "Jie",
     "Hao"
    ]
   ],
   "title": "Assessment of correlation between objective measures and speech recognition performance in the evaluation of speech enhancement",
   "original": "i08_0179",
   "page_count": 4,
   "order": 48,
   "p1": "179",
   "pn": "182",
   "abstract": [
    "Speech enhancement is widely used to improve the perceptual quality of noisy speech by suppressing the interfering ambient noise and is commonly evaluated via objective quality measures. Automatic speech recognition (ASR) systems also use such speech enhancement technologies in front-end to improve their noise robustness. If the objective measures have a high correlation with speech recognition accuracy, we can effectively predict the ASR performance according to objective quality measures in advance and flexibly optimize the enhancement algorithms in the stage of system design. Motivated by such idea, this paper investigates the correlation between the ASR performance and several traditional objective measures based on Aurora2 database. In the Experimental results the highest correlation coefficient, 0.962, is provided by weighted spectral slope measure (WSS).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-42"
  },
  "lyons08_interspeech": {
   "authors": [
    [
     "James G.",
     "Lyons"
    ],
    [
     "Kuldip K.",
     "Paliwal"
    ]
   ],
   "title": "Effect of compressing the dynamic range of the power spectrum in modulation filtering based speech enhancement",
   "original": "i08_0387",
   "page_count": 4,
   "order": 49,
   "p1": "387",
   "pn": "390",
   "abstract": [
    "In the modulation-filtering based speech enhancement method, noise suppression is achieved by bandpass filtering the temporal trajectories of the power spectrum. In the literature, some authors use the power spectrum directly for modulation filtering, while others use different compression functions for reducing the dynamic range of the power spectrum prior to its modulation filtering. This paper compares systematically different dynamic range compression functions applied to the power spectrum for speech enhancement. Subjective listening tests and objective measures are used to evaluate the quality as well as the intelligibility of the enhanced speech. The quality is measured objectively in terms of the Perceptual Estimation of Speech Quality (PESQ) measure and the intelligibility in terms of the Speech Transmission Index (STI) measure. It is found that P0.3333 (power spectrum raised to power 1/3) results in the highest speech quality and intelligibility.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-43"
  },
  "so08_interspeech": {
   "authors": [
    [
     "Stephen",
     "So"
    ],
    [
     "Kuldip K.",
     "Paliwal"
    ]
   ],
   "title": "A long state vector kalman filter for speech enhancement",
   "original": "i08_0391",
   "page_count": 4,
   "order": 50,
   "p1": "391",
   "pn": "394",
   "abstract": [
    "In this paper, we investigate a long state vector Kalman filter for the enhancement of speech that has been corrupted by white and coloured noise. It has been reported in previous studies that a vector Kalman filter achieves better enhancement than the scalar Kalman filter and it is expected that by increasing the state vector length, one may improve the enhancement performance even further. However, any enhancement improvement that may result from an increase in state vector length is constrained by the typical use of short, non-overlapped speech frames, as the autocorrelation coefficient estimates tend to become less reliable at higher lags. We propose to overcome this problem by incorporating an analysismodification- synthesis framework, where long, overlapped frames are used instead. Our enhancement experiments based on the NOIZEUS corpus show that the proposed long state vector Kalman filter achieves higher mean SNR and PESQ scores than the scalar and short state vector Kalman filter, therefore fulfilling the notion that a longer state vector can lead to better enhancement.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-44"
  },
  "kundu08_interspeech": {
   "authors": [
    [
     "Achintya",
     "Kundu"
    ],
    [
     "Saikat",
     "Chatterjee"
    ],
    [
     "T. V.",
     "Sreenivas"
    ]
   ],
   "title": "Subspace based speech enhancement using Gaussian mixture model",
   "original": "i08_0395",
   "page_count": 4,
   "order": 51,
   "p1": "395",
   "pn": "398",
   "abstract": [
    "Traditional subspace based speech enhancement (SSE) methods use linear minimum mean square error (LMMSE) estimation that is optimal if the Karhunen Loeve transform (KLT) coefficients of speech and noise are Gaussian distributed. In this paper, we investigate the use of Gaussian mixture (GM) density for modeling the non-Gaussian statistics of the clean speech KLT coefficients. Using Gaussian mixture model (GMM), the optimum minimum mean square error (MMSE) estimator is found to be nonlinear and the traditional LMMSE estimator is shown to be a special case. Experimental results show that the proposed method provides better enhancement performance than the traditional subspace based methods.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-45"
  },
  "das08_interspeech": {
   "authors": [
    [
     "Amit",
     "Das"
    ],
    [
     "John H. L.",
     "Hansen"
    ]
   ],
   "title": "Generalized parametric spectral subtraction using weighted Euclidean distortion",
   "original": "i08_0399",
   "page_count": 4,
   "order": 52,
   "p1": "399",
   "pn": "402",
   "abstract": [
    "An improved version of the original parametric formulation of the generalized spectral subtraction method is presented in this study. The original formulation uses parameters that minimize the mean-square error (MSE) between the estimated and true speech spectral amplitudes. However, the MSE does not take into account any perceptual measure. We propose two new short-time spectral amplitude estimators based on a perceptual error criterion . the weighted Euclidean distortion. The error function is easily adaptable to penalize spectral peaks and valleys differently. Performance evaluations were performed using two noise types over four SNR levels and compared to the original parametric formulation. Results demonstrate that for most cases the proposed estimators achieve greater noise suppression without introducing speech distortion.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-46"
  },
  "miyake08_interspeech": {
   "authors": [
    [
     "Nobuyuki",
     "Miyake"
    ],
    [
     "Tetsuya",
     "Takiguchi"
    ],
    [
     "Yasuo",
     "Ariki"
    ]
   ],
   "title": "Sudden noise reduction based on GMM with noise power estimation",
   "original": "i08_0403",
   "page_count": 4,
   "order": 53,
   "p1": "403",
   "pn": "406",
   "abstract": [
    "This paper describes a method for reducing sudden noise using noise detection and classification methods, and noise power estimation. Sudden noise detection and classification have been dealt with in our previous study. In this paper, noise classification is improved to classify more kinds of noises based on k-means clustering, and GMM-based noise reduction is performed using the detection and classification results. As a result of classification, we can determine the kind of noise we are dealing with, but the power is unknown. In this paper, this problem is solved by combining an estimation of noise power with the noise reduction method. In our experiments, the proposed method achieved good performance for recognition of utterances overlapped by sudden noises.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-47"
  },
  "alam08_interspeech": {
   "authors": [
    [
     "Md. Jahangir",
     "Alam"
    ],
    [
     "Sid-Ahmed",
     "Selouani"
    ],
    [
     "Douglas",
     "O'Shaughnessy"
    ],
    [
     "Sofia Ben",
     "Jebara"
    ]
   ],
   "title": "Speech enhancement using a wiener denoising technique and musical noise reduction",
   "original": "i08_0407",
   "page_count": 4,
   "order": 54,
   "p1": "407",
   "pn": "410",
   "abstract": [
    "Speech enhancement methods using spectral subtraction have the drawback of generating an annoying residual noise with musical character. In this paper a frequency domain optimal linear estimator with perceptual post filtering is proposed which incorporates the masking properties of the human auditory system to make the residual noise distortion inaudible. The performance of the proposed enhancement algorithm is evaluated by the Segmental SNR, Log Spectral Distance (LSD) and Perceptual Evaluation of Speech Quality (PESQ) measures under various noisy environments and yields better results compared to the Wiener denoising technique.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-48"
  },
  "wilson08_interspeech": {
   "authors": [
    [
     "Kevin W.",
     "Wilson"
    ],
    [
     "Bhiksha",
     "Raj"
    ],
    [
     "Paris",
     "Smaragdis"
    ]
   ],
   "title": "Regularized non-negative matrix factorization with temporal dependencies for speech denoising",
   "original": "i08_0411",
   "page_count": 4,
   "order": 55,
   "p1": "411",
   "pn": "414",
   "abstract": [
    "We present a technique for denoising speech using temporally regularized nonnegative matrix factorization (NMF). In previous work [1], we used a regularized NMF update to impose structure within each audio frame. In this paper, we add frame-to-frame regularization across time and show that this additional regularization can also improve our speech denoising results. We evaluate our algorithm on a range of nonstationary noise types and outperform a state-of-the-art Wiener filter implementation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-49"
  },
  "zou08_interspeech": {
   "authors": [
    [
     "Xin",
     "Zou"
    ],
    [
     "Peter",
     "Jančovič"
    ],
    [
     "Munevver",
     "Kokuer"
    ],
    [
     "Martin J.",
     "Russell"
    ]
   ],
   "title": "ICA-based MAP speech enhancement with multiple variable speech distribution models",
   "original": "i08_0415",
   "page_count": 4,
   "order": 56,
   "p1": "415",
   "pn": "418",
   "abstract": [
    "This paper proposes a novel ICA-based MAP speech enhancement algorithm using multiple variable speech distribution models. The proposed algorithm consists of two stages, primary and advanced enhancement. The primary enhancement is performed by employing a single distribution model obtained from all speech signals. The advanced enhancement first employs multiple models of speech signals, each modeling a specific type of speech, and then adapts these model parameters for each speech frame by employing the enhanced signal from the primary estimation. A statistical noise adaptation technique has been employed to better model the noise in non-stationary case. The proposed algorithm has been evaluated on speech from TIMIT database corrupted by various noises and it has shown significantly improved performance over using the single speech distribution model.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-50"
  },
  "weiss08b_interspeech": {
   "authors": [
    [
     "Ron J.",
     "Weiss"
    ],
    [
     "Michael I.",
     "Mandel"
    ],
    [
     "Daniel P. W.",
     "Ellis"
    ]
   ],
   "title": "Source separation based on binaural cues and source model constraints",
   "original": "i08_0419",
   "page_count": 4,
   "order": 57,
   "p1": "419",
   "pn": "422",
   "abstract": [
    "We describe a system for separating multiple sources from a two-channel recording based on interaural cues and known characteristics of the source signals. We combine a probabilistic model of the observed interaural level and phase differences with a prior model of the source statistics and derive an EM algorithm for finding the maximum likelihood parameters of the joint model. The system is able to separate more sound sources than there are observed channels. In simulated reverberant mixtures of three speakers the proposed algorithm gives a signal-to-noise ratio improvement of 2.1 dB over a baseline algorithm using only interaural cues.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-51"
  },
  "kumatani08_interspeech": {
   "authors": [
    [
     "Kenichi",
     "Kumatani"
    ],
    [
     "John",
     "McDonough"
    ],
    [
     "Barbara",
     "Rauch"
    ],
    [
     "Philip N.",
     "Garner"
    ],
    [
     "Weifeng",
     "Li"
    ],
    [
     "John",
     "Dines"
    ]
   ],
   "title": "Maximum kurtosis beamforming with the generalized sidelobe canceller",
   "original": "i08_0423",
   "page_count": 4,
   "order": 58,
   "p1": "423",
   "pn": "426",
   "abstract": [
    "This paper presents an adaptive beamforming application based on the capture of far-field speech data from a real single speaker in a real meeting room. After the position of a speaker is estimated by a speaker tracking system, we construct a subband-domain beamformer in generalized sidelobe canceller (GSC) configuration. In contrast to conventional practice, we then optimize the active weight vectors of the GSC so that the distribution of an output signal is as non-Gaussian as possible. We consider kurtosis in order to measure the degree of non-Gaussianity. Our beamforming algorithms can suppress noise and reverberation without the signal cancellation problems encountered in conventional beamforming algorithms. We demonstrate the effectiveness of our proposed techniques through a series of far-field automatic speech recognition experiments on the Multi-Channel Wall Street Journal Audio Visual Corpus (MC-WSJ-AV). The beamforming algorithm proposed here achieved a 13.6% WER, whereas the simple delay-and-sum beamformer provided a WER of 17.8%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-52"
  },
  "furuya08_interspeech": {
   "authors": [
    [
     "Ken'ichi",
     "Furuya"
    ],
    [
     "Akitoshi",
     "Kataoka"
    ],
    [
     "Yoichi",
     "Haneda"
    ]
   ],
   "title": "Noise robust speech dereverberation using constrained inverse filter",
   "original": "i08_0427",
   "page_count": 4,
   "order": 59,
   "p1": "427",
   "pn": "430",
   "abstract": [
    "A noise robust dereverberation method is presented for speech enhancement in noisy reverberant conditions. This method introduces the constraint of minimizing the noise power in the inverse filter computation of dereverberation. It is shown that there exists a tradeoff between reducing the reverberation and reducing the noise; this tradeoff can be controlled by the constraint. Inverse filtering reduces early reflections and directional noise. In addition, spectral subtraction is used to suppress the tail of the inversefiltered reverberation and residual noise. The performance of our method is objectively and subjectively evaluated in experiments using measured room impulse responses. The results indicate that this method provides better speech quality than the conventional methods.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-53"
  },
  "rahmani08_interspeech": {
   "authors": [
    [
     "Mohsen",
     "Rahmani"
    ],
    [
     "Ahmad",
     "Akbari"
    ],
    [
     "Beghdad",
     "Ayad"
    ]
   ],
   "title": "A dual microphone coherence based method for speech enhancement in headsets",
   "original": "i08_0431",
   "page_count": 4,
   "order": 60,
   "p1": "431",
   "pn": "434",
   "abstract": [
    "The performance of two-microphone coherence based methods degrades if two captured noises are correlated. The Cross Power Spectrum Subtraction (CPSS) is an adapted coherence method for noise correlated environments. In this paper, we propose a new technique for estimation of speech cross power spectrum density and we exploit it in CPSS. The proposed speech enhancement method is evaluated as a speech recognition preprocessing system and as an independent speech enhancement system. The enhancement results show the practical superiority of the proposed method comparing with the previous solutions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-54"
  },
  "tashev08_interspeech": {
   "authors": [
    [
     "Ivan",
     "Tashev"
    ],
    [
     "Slavy",
     "Mihov"
    ],
    [
     "Tyler",
     "Gleghorn"
    ],
    [
     "Alex",
     "Acero"
    ]
   ],
   "title": "Sound capture system and spatial filter for small devices",
   "original": "i08_0435",
   "page_count": 4,
   "order": 61,
   "p1": "435",
   "pn": "438",
   "abstract": [
    "Usage of cellular phones and small form factor devices as PDAs and other handhelds has been increasing rapidly. Their use is varied, with scenarios such as communication, internet browsing, audio and video recording just to name a few. This requires better sound capturing system as the sound source is already at larger distance from the device's microphone. In this paper we propose sound capture system for small devices which uses two unidirectional microphones placed back-to-back close to each other. The processing part consists of beamformer and a non-linear spatial filter. The speech enhancement processing achieves an improvement of 0.39 MOS points in the perceptual sound quality and 10.8 dB improvement in SNR.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-55"
  },
  "cheng08_interspeech": {
   "authors": [
    [
     "Ning",
     "Cheng"
    ],
    [
     "Wen-ju",
     "Liu"
    ],
    [
     "Peng",
     "Li"
    ],
    [
     "Bo",
     "Xu"
    ]
   ],
   "title": "An effective microphone array post-filter in arbitrary environments",
   "original": "i08_0439",
   "page_count": 4,
   "order": 62,
   "p1": "439",
   "pn": "442",
   "abstract": [
    "The theoretic foundation of traditional microphone array postfilters is the signal model in which the noise between sensors is assumed to be uncorrelated. However, this model is inaccurate in real environments since the correlated noise exists. In this paper, a more generalized signal model which considers both the correlated and uncorrelated noise is introduced. A general expression of the microphone array post-filter is proposed for this model. For better residual noise shaping, the human auditory property is incorporated into the post-filter estimation process. In experiments with real noise microphone array recordings, the proposed technique has shown to produce impressive results in terms of quality measures of the enhanced speech.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-56"
  },
  "cho08_interspeech": {
   "authors": [
    [
     "Kook",
     "Cho"
    ],
    [
     "Hajime",
     "Okumura"
    ],
    [
     "Takanobu",
     "Nishiura"
    ],
    [
     "Yoichi",
     "Yamashita"
    ]
   ],
   "title": "Localization of multiple sound sources based on inter-channel correlation using a distributed microphone system",
   "original": "i08_0443",
   "page_count": 4,
   "order": 63,
   "p1": "443",
   "pn": "446",
   "abstract": [
    "Recently the importance of hands-free speech interfaces is increasingly recognized. However, in real environments, the presence of ambient noises and room reverberations seriously degrades the performance of the hands-free speech recognition. Reliable sound source localization is necessary to maximize the effect of noise reduction. This paper proposes a new method of multiple sound source localization using a distributed microphone system that is a recording system with multiple microphones dispersed to a wide space. The proposed method localizes a sound source by finding the position that maximizes the accumulated correlation coefficient between multiple channel pairs. After the estimation of the first sound source, a model of the accumulated correlation for a single sound source is subtracted from the observed distribution of the accumulated correlation. Subsequently, the second sound source is searched again. To evaluate the effectiveness of the proposed method, experiments of multiple sound source localization were carried out in an actual office room. The result shows that multiple sound source localization accuracy is about 96.0%. The proposed method could realize the multiple sound source localization robustly and stably.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-57"
  },
  "zhang08_interspeech": {
   "authors": [
    [
     "Heng",
     "Zhang"
    ],
    [
     "Qiang",
     "Fu"
    ],
    [
     "Yonghong",
     "Yan"
    ]
   ],
   "title": "A frequency domain approach for speech enhancement with directionality using compact microphone array",
   "original": "i08_0447",
   "page_count": 4,
   "order": 64,
   "p1": "447",
   "pn": "450",
   "abstract": [
    "In this paper, a novel two-element-microphone-array-based speech enhancement algorithm is proposed. This algorithm is designed to achieve better overall performance with relatively small array size. A frequency domain adaptive null-forming is used, in which adaptive noise cancelation is implemented in auditory subbands. And an OM-LSA based postfiltering stage further purifies the output. The algorithm also features interaction between the array processing and the postfilter to make the filter adaptation more robust. This approach achieves considerable improvement on signal-to-noise ratio (SNR) and subjective quality of the desired speech. Experiments confirm the effectiveness of the proposed system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-58"
  },
  "komatani08_interspeech": {
   "authors": [
    [
     "Kazunori",
     "Komatani"
    ],
    [
     "Tatsuya",
     "Kawahara"
    ],
    [
     "Hiroshi G.",
     "Okuno"
    ]
   ],
   "title": "Predicting ASR errors by exploiting barge-in rate of individual users for spoken dialogue systems",
   "original": "i08_0183",
   "page_count": 4,
   "order": 65,
   "p1": "183",
   "pn": "186",
   "abstract": [
    "We exploit the barge-in rate of individual users to predict automatic speech recognition (ASR) errors. A barge-in is a situation in which a user starts speaking during a system prompt, and it can be detected even when ASR results are not reliable. Such features not using ASR results can be a clue for managing a situation in which user utterances cannot be successfully recognized. Since individual users in our system can be identified by their phone numbers, we accumulate how often each user barges in and use this rate as a user profile for determining whether a current \"barge-in\" utterance should be accepted or not. We furthermore set a window that reflects the temporal transition of the user's behavior as they get accustomed to the system. Experimental results show that setting the window improves the prediction accuracy of whether the utterance should be accepted or not. The experiments also clarify the minimum window width for improving accuracy.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-59"
  },
  "katsumaru08_interspeech": {
   "authors": [
    [
     "Masaki",
     "Katsumaru"
    ],
    [
     "Kazunori",
     "Komatani"
    ],
    [
     "Tetsuya",
     "Ogata"
    ],
    [
     "Hiroshi G.",
     "Okuno"
    ]
   ],
   "title": "Expanding vocabulary for recognizing user's abbreviations of proper nouns without increasing ASR error rates in spoken dialogue systems",
   "original": "i08_0187",
   "page_count": 4,
   "order": 66,
   "p1": "187",
   "pn": "190",
   "abstract": [
    "Users often abbreviate long words when using spoken dialogue systems, which results in automatic speech recognition (ASR) errors. We define abbreviated words as sub-words of the original word, and add them into an ASR dictionary. The first problem is that proper nouns cannot be correctly segmented by general morphological analyzers, although long and compounded words need to be segmented in agglutinative languages such as Japanese. The second is that, as vocabulary increases, adding many abbreviated words degrades the ASR accuracy. We develop two methods, (1) to segment words by using conjunction probabilities between characters, and (2) to manipulate occurrence probabilities of generated abbreviated words on the basis of the phonological similarities between abbreviated and original words. By our method, the ASR accuracy is improved by 24.2 points for utterances containing abbreviated words, and degraded by only a 0.1 point for those containing original words.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-60"
  },
  "williams08_interspeech": {
   "authors": [
    [
     "Jason D.",
     "Williams"
    ]
   ],
   "title": "Exploiting the ASR n-best by tracking multiple dialog state hypotheses",
   "original": "i08_0191",
   "page_count": 4,
   "order": 67,
   "p1": "191",
   "pn": "194",
   "abstract": [
    "When the top ASR hypothesis is incorrect, often the correct hypothesis is listed as an alternative in the ASR N-Best list. Whereas traditional spoken dialog systems have struggled to exploit this information, this paper argues that a dialog model that tracks a distribution over multiple dialog states can improve dialog accuracy by making use of the entire N-Best list. The key element of the approach is a generative model of the N-Best list given the user's true hidden action. An evaluation on real dialog data verifies that dialog accuracy rates are improved by making use of the entire N-Best list.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-61"
  },
  "makalic08_interspeech": {
   "authors": [
    [
     "Enes",
     "Makalic"
    ],
    [
     "Ingrid",
     "Zukerman"
    ],
    [
     "Michael",
     "Niemann"
    ]
   ],
   "title": "A spoken language interpretation component for a robot dialogue system",
   "original": "i08_0195",
   "page_count": 4,
   "order": 68,
   "p1": "195",
   "pn": "198",
   "abstract": [
    "The DORIS project aims to develop a spoken dialogue module for an autonomous robotic agent. This paper examines the techniques used by Scusi?, the speech interpretation component of DORIS, to postulate and assess hypotheses regarding the meaning of a spoken utterance. The results of our evaluation are encouraging, yielding good interpretation performance for utterances of different types and lengths.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-62"
  },
  "cesari08_interspeech": {
   "authors": [
    [
     "Federico",
     "Cesari"
    ],
    [
     "Horacio",
     "Franco"
    ],
    [
     "Gregory K.",
     "Myers"
    ],
    [
     "Harry",
     "Bratt"
    ]
   ],
   "title": "MUESLI: multiple utterance error correction for a spoken language interface",
   "original": "i08_0199",
   "page_count": 4,
   "order": 69,
   "p1": "199",
   "pn": "202",
   "abstract": [
    "We propose a method for using all available information to help correct recognition errors in tasks that use constrained grammars of the kind used in the domain of Command and Control (CC) systems. In current spoken language CC systems, if there is a recognition error, the user repeats the same phrase multiple times until a correct recognition is achieved. This interaction can be frustrating for the user, especially at high levels of ambient noise. We aim to improve the accuracy of the error correction process by using all the previous information available at a given point, this being the previous utterances of the same input phrase and the knowledge that the previous result contained an error.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-63"
  },
  "conrod08_interspeech": {
   "authors": [
    [
     "Sarah",
     "Conrod"
    ],
    [
     "Sara",
     "Basson"
    ],
    [
     "Dimitri",
     "Kanevsky"
    ]
   ],
   "title": "Methods to optimize transcription of on-line media",
   "original": "i08_0203",
   "page_count": 4,
   "order": 70,
   "p1": "203",
   "pn": "206",
   "abstract": [
    "This paper outlines the growing need to provide fast and low cost methods for providing transcripts of audio and video media to people who are deaf and hard of hearing. Outlined are three different methods for creating such transcripts including traditional manual transcription and two automatic speech recognition (ASR) methods: a semi-automatic process called shadowing and a web-based automatic transcription tool created by IBM. A pilot examining the three different methods was conducted and the results of these tests are provided and discussed, as well as potential future studies regarding the efficacy and usability of the outputs from the various methods.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-64"
  },
  "ito08_interspeech": {
   "authors": [
    [
     "Akinori",
     "Ito"
    ],
    [
     "Toyomi",
     "Meguro"
    ],
    [
     "Shozo",
     "Makino"
    ],
    [
     "Motoyuki",
     "Suzuki"
    ]
   ],
   "title": "Discrimination of task-related words for vocabulary design of spoken dialog systems",
   "original": "i08_0207",
   "page_count": 4,
   "order": 71,
   "p1": "207",
   "pn": "210",
   "abstract": [
    "This paper describes a method used to determine if a specific word is related to a certain spoken dialog task. In most ordinary spoken dialog systems, only the words that are actually used to achieve the task are included in the vocabulary. Therefore, the system cannot recognize utterances that contain OOV words that are related to the task. Therefore, we developed a method for determining the words that are related to a specified task in order to augment the system's vocabulary. Our method is based on word similarity. We examined three similarities: word occurrence frequency on the Web, distance in a thesaurus and word similarity using LSA. The experiment revealed that the thesaurus-based and LSA-based methods have an OOV problem. To solve the problem, we developed a way to combine these two methods with theWeb-based method. In addition, we tried combining the methods using the AdaBoost algorithm.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-65"
  },
  "hori08_interspeech": {
   "authors": [
    [
     "Chiori",
     "Hori"
    ],
    [
     "Kiyonori",
     "Ohtake"
    ],
    [
     "Teruhisa",
     "Misu"
    ],
    [
     "Hideki",
     "Kashioka"
    ],
    [
     "Satoshi",
     "Nakamura"
    ]
   ],
   "title": "Dialog management using weighted finite-state transducers",
   "original": "i08_0211",
   "page_count": 4,
   "order": 72,
   "p1": "211",
   "pn": "214",
   "abstract": [
    "We are aiming to construct an expandable and adaptable dialog system which handles multiple tasks and senses users' intention via multiple modalities. A flexible platform to integrate different dialog strategies and modalities is indispensable for this purpose. In this paper, we propose an efficient approach to manage a dialog system using a weighted finite-state transducer (WFST) in which users' concept and system's action tags are input and output of the transducer, respectively. By incorporating WFSTs in dialog management, different components can easily be integrated and work on a common platform. We have constructed a prototype spoken dialog system of the Kyoto tour guide which assists users in making a plan for one-day trip to sightsee through interaction. A WFST for dialog management was created based on the annotated transcript of the Kyoto tour guide dialog corpus we recorded. The WFST was then composed with a word-to-concept WFST for language understanding, and optimized. We have confirmed our WFST-based dialog manager accepted recognition results from a speech recognizer well and worked as we designed.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-66"
  },
  "yoshimi08_interspeech": {
   "authors": [
    [
     "Yoshitaka",
     "Yoshimi"
    ],
    [
     "Ryota",
     "Kakitsuba"
    ],
    [
     "Yoshihiko",
     "Nankaku"
    ],
    [
     "Akinobu",
     "Lee"
    ],
    [
     "Keiichi",
     "Tokuda"
    ]
   ],
   "title": "Probabilistic answer selection based on conditional random fields for spoken dialog system",
   "original": "i08_0215",
   "page_count": 4,
   "order": 73,
   "p1": "215",
   "pn": "218",
   "abstract": [
    "A probabilistic answer selection for a spoken dialog system based on Conditional Random Fields (CRFs) is described. The probabilities of answers for a question is trained by CRFs based on the lexical and morphological properties of each word, the most likely answer against the recognized word sequence of question utterance will be chosen as the system output. Various set of feature functions were evaluated on the real data of a speech oriented information kiosk system, and it is shown that the morphological properties introduces positive effects on the response accuracy. Training with recognizer output of training database instead of manual transcription was also investigated. It was also shown that this proposed scheme can achieve higher accuracy than a conventional keyword-based answer selection.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-67"
  },
  "eskenazi08_interspeech": {
   "authors": [
    [
     "Maxine",
     "Eskenazi"
    ],
    [
     "Alan W.",
     "Black"
    ],
    [
     "Antoine",
     "Raux"
    ],
    [
     "Brian",
     "Langner"
    ]
   ],
   "title": "Let's go lab: a platform for evaluation of spoken dialog systems with real world users",
   "original": "i08_0219",
   "page_count": 1,
   "order": 74,
   "p1": "219",
   "pn": "",
   "abstract": [
    "This short paper is intended to advertise Let's Go Lab, a plat-form for the evaluation of spoken dialog research. Unlike other dialog platforms, in addition to example dialog data and a portable software system, Let's Go Lab affords evaluation with real users. Let's Go has served the Pittsburgh public with bus schedule information since 2005, answering more than 52,000 calls to date.\n",
    ""
   ]
  },
  "batista08_interspeech": {
   "authors": [
    [
     "Fernando",
     "Batista"
    ],
    [
     "Nuno",
     "Mamede"
    ],
    [
     "Isabel",
     "Trancoso"
    ]
   ],
   "title": "The impact of language dynamics on the capitalization of broadcast news",
   "original": "i08_0220",
   "page_count": 4,
   "order": 75,
   "p1": "220",
   "pn": "223",
   "abstract": [
    "This paper investigates the impact of language dynamics on the capitalization of transcriptions of broadcast news. Most of the capitalization information is provided by a large newspaper corpus. Three different speech corpora subsets, from different time periods, are used for evaluation, assessing the importance of available training data in nearby time periods. Results are provided both for manual and automatic transcriptions, showing also the impact of the recognition errors in the capitalization task. Our approach is based on maximum entropy models, uses unlimited vocabulary, and is suitable for language adaptation. The language model for a given language period is produced by retraining a previous language model with data from that time period. The language model produced with this approach can be sorted and then pruned, in order to reduce computational resources, without much impact in the final results.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-68"
  },
  "paulik08_interspeech": {
   "authors": [
    [
     "Matthias",
     "Paulik"
    ],
    [
     "Alex",
     "Waibel"
    ]
   ],
   "title": "Lightly supervised acoustic model training on EPPS recordings",
   "original": "i08_0224",
   "page_count": 4,
   "order": 76,
   "p1": "224",
   "pn": "227",
   "abstract": [
    "Debates in the European Parliament are simultaneously translated into the official languages of the Union. These interpretations are broadcast live via satellite on separate audio channels. After several months, the parliamentary proceedings are published as final text editions (FTE). FTEs are formatted for an easy readability and can differ significantly from the original speeches and the live broadcast interpretations. We examine the impact on German word error rate (WER) when introducing supervision based on German FTEs and supervision based on German automatic translations extracted from the English and Spanish audio. We show that FTE based supervision and additional interpretation based supervision provide significant reductions in WER. We successfully apply FTE supervised acoustic model (AM) training using 143h of recordings. Combining the new AM with the mentioned supervision techniques, we achieve a significant WER reduction of 13.3% relative.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-69"
  },
  "servan08_interspeech": {
   "authors": [
    [
     "Christophe",
     "Servan"
    ],
    [
     "Frédéric",
     "Bechet"
    ]
   ],
   "title": "Fast call-classification system development without in-domain training data",
   "original": "i08_0228",
   "page_count": 4,
   "order": 77,
   "p1": "228",
   "pn": "231",
   "abstract": [
    "This paper presents a new method for the fast development of call-routing systems based on pre-existing corpora and knowledge databases. This method pushes forward the reduction of specific data collection and annotation for developing a new call-classification system. No specific data collection is needed for training both for the Automatic Speech Recognition (ASR) and classification models. The main idea is to re-use existing data to train the models, according to a priori knowledge on the task targeted. The experimental framework used in this study is a call-routing system applied to a civil service information telephone application. All the a priori knowledge used to develop the system is extracted from the civil service information website as well as pre-existing corpora. The evaluation of our strategy has been made on a test corpus containing 216 utterances recorded by 10 different speakers.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-70"
  },
  "hoffmeister08_interspeech": {
   "authors": [
    [
     "Björn",
     "Hoffmeister"
    ],
    [
     "Ralf",
     "Schlüter"
    ],
    [
     "Hermann",
     "Ney"
    ]
   ],
   "title": "iCNC and iROVER: the limits of improving system combination with classification?",
   "original": "i08_0232",
   "page_count": 4,
   "order": 78,
   "p1": "232",
   "pn": "235",
   "abstract": [
    "We show how ROVER and confusion network combination (CNC) can be improved with classification. The general idea of improving combination with classification is that each word is assigned to a certain location and at each location a classifier decides which of the provided alternatives is most likely correct. We investigate four variations of this idea and three different classifiers, which are trained on various features derived from ASR lattices. For our experiments, we use highly optimized ROVER and CNC systems as baseline, which already give a relative reduction in WER of more than 20% for the TC-Star 2007 English task. With our methods we can further improve the result of the corresponding standard combination method.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-71"
  },
  "hahn08_interspeech": {
   "authors": [
    [
     "Stefan",
     "Hahn"
    ],
    [
     "Patrick",
     "Lehnen"
    ],
    [
     "Hermann",
     "Ney"
    ]
   ],
   "title": "System combination for spoken language understanding",
   "original": "i08_0236",
   "page_count": 4,
   "order": 79,
   "p1": "236",
   "pn": "239",
   "abstract": [
    "One of the first steps in an SLU system usually is the extraction of flat concepts. Within this paper, we present five methods for concept tagging and give experimental results on the state-ofthe- art MEDIA corpus for both, manual transcriptions (REF) and ASR input (ASR). Compared to previous publications, some single systems could be improved and the ASR results are presented for the first time. We could improve the tagging performance of the best known result on this task by approx. 7% relatively from 16.2% to 15.0% CER for REF using light-weight system combination (ROVER). For the ASR task, we achieve improvements by approx. 3% relatively from 29.8% to 28.9% CER. An analysis of the differences in performance on both tasks is also given.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-72"
  },
  "takeuchi08_interspeech": {
   "authors": [
    [
     "Shota",
     "Takeuchi"
    ],
    [
     "Tobias",
     "Cincarek"
    ],
    [
     "Hiromichi",
     "Kawanami"
    ],
    [
     "Hiroshi",
     "Saruwatari"
    ],
    [
     "Kiyohiro",
     "Shikano"
    ]
   ],
   "title": "Question and answer database optimization using speech recognition results",
   "original": "i08_0451",
   "page_count": 4,
   "order": 80,
   "p1": "451",
   "pn": "454",
   "abstract": [
    "The aim of this research is a human-oriented spoken dialog system which provides replies to a variety of users' utterances. The example-based response generation method searches a question and answer database (QADB) for the example question most similar to a user utterance. With this method, the system can answer a question difficult for a model to express. A QADB is constructed from question and answer pairs (QA pairs) by employing a large corpus. In order to enhance robustness to recognition errors of inarticulate utterances such as children utterances, we propose to use speech recognition results, instead of manual transcriptions, as example questions. We also introduce an optimization method that removes inappropriate QA pairs from a QADB to maximize response accuracy. We show that our method improves the response accuracy of utterances especially for children utterances in the open test.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-73"
  },
  "saruwatari08_interspeech": {
   "authors": [
    [
     "Hiroshi",
     "Saruwatari"
    ],
    [
     "Yu",
     "Takahashi"
    ],
    [
     "Hiroyuki",
     "Sakai"
    ],
    [
     "Shota",
     "Takeuchi"
    ],
    [
     "Tobias",
     "Cincarek"
    ],
    [
     "Hiromichi",
     "Kawanami"
    ],
    [
     "Kiyohiro",
     "Shikano"
    ]
   ],
   "title": "Development and evaluation of hands-free spoken dialogue system for railway station guidance",
   "original": "i08_0455",
   "page_count": 4,
   "order": 81,
   "p1": "455",
   "pn": "458",
   "abstract": [
    "In this paper, we describe development and evaluation of handsfree spoken dialogue system which is used for railway station guidance. In the application at the railway station, noise robustness is the most essential issue for the dialogue system. To address the problem, we introduce two key techniques in our proposed hands-free system; (a) blind spatial subtraction array (BSSA) as a preprocessing, which can efficiently reduce nonstationary and diffuse noises in real-time, and (b) robust voice activity detection (VAD) based on speech decoding for further improvement of speech recognition accuracy. The experimental assessment of the proposed dialogue system reveals that the combination of real-time BSSA and robust VAD can provide the recognition accuracy of more than 80% under adverse railway-station noise conditions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-74"
  },
  "stent08_interspeech": {
   "authors": [
    [
     "Amanda J.",
     "Stent"
    ],
    [
     "Srinivas",
     "Bangalore"
    ]
   ],
   "title": "Statistical shared plan-based dialog management",
   "original": "i08_0459",
   "page_count": 4,
   "order": 82,
   "p1": "459",
   "pn": "462",
   "abstract": [
    "In this paper we describe a statistical shared plan-based approach to dialog modeling and dialog management. We apply this approach to a corpus of human-human spoken dialogs. We compare the performance of models trained on transcribed and automatically recognized speech, and present ideas for further improving the models.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-75"
  },
  "herm08_interspeech": {
   "authors": [
    [
     "Ota",
     "Herm"
    ],
    [
     "Alexander",
     "Schmitt"
    ],
    [
     "Jackson",
     "Liscombe"
    ]
   ],
   "title": "When calls go wrong: how to detect problematic calls based on log-files and emotions?",
   "original": "i08_0463",
   "page_count": 4,
   "order": 83,
   "p1": "463",
   "pn": "466",
   "abstract": [
    "Traditionally, the prediction of problematic calls in Interactive Voice Response systems in call centers has been based either on dialog state transitions and recognition log data, or on caller emotion. We present a combined model incorporating both types of feature sets that achieved 79.22% classification accuracy of problematic and non-problematic calls after only the first four turns in a human-computer dialogue. We found that using acoustic features to indicate caller emotion did not yield any significant increase of accuracy.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-76"
  },
  "gillick08_interspeech": {
   "authors": [
    [
     "Dan",
     "Gillick"
    ],
    [
     "Dilek",
     "Hakkani-Tür"
    ],
    [
     "Michael",
     "Levit"
    ]
   ],
   "title": "Unsupervised learning of edit parameters for matching name variants",
   "original": "i08_0467",
   "page_count": 4,
   "order": 84,
   "p1": "467",
   "pn": "470",
   "abstract": [
    "Since named entities are often written in different ways, question answering (QA) and other language processing tasks stand to benefit from entity matching. We address the problem of finding equivalent person names in unstructured text. Our approach is a generalization of spelling correction: We compare to candidate matches by applying a set of edits to an input name. We introduce a novel unsupervised method for learning spelling edit probabilities which improves overall F-Measure on our own name-matching task by 12%. Relevance is demonstrated by application to the GALE Distillation task.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-77"
  },
  "cevik08_interspeech": {
   "authors": [
    [
     "Mert",
     "Cevik"
    ],
    [
     "Fuliang",
     "Weng"
    ],
    [
     "Chin-Hui",
     "Lee"
    ]
   ],
   "title": "Detection of repetitions in spontaneous speech in dialogue sessions",
   "original": "i08_0471",
   "page_count": 4,
   "order": 85,
   "p1": "471",
   "pn": "474",
   "abstract": [
    "We present a study on detecting repeated speech segments in consecutive utterances in a user session of a dialogue application. Such repetition patterns often carry important information of a user's expectation from or frustration about the dialogue system and therefore provide a source of clues for the system to adjust its dialogue strategies when such repetitions occur in order to improve system performance. We propose a recursive dynamictime warping based pattern comparison algorithm with no fixed end-points to find similar parts within the two utterances, called original and correction utterances. Candidate reference patterns are generated from the correction utterance by an unsupervised segmentation scheme. When no prior information about the position of the repeated parts is used, each reference pattern is compared with the original utterance from the beginning to the end. Experiments are conducted on 190 utterances of spontaneous speech from simulated dialogue sessions. The proposed algorithm achieves detection and rejection rates of 80.3% and 85.2% for the repeated and non-repeated parts, respectively. When segmentation information provided by the recognizer in the dialog system is incorporated into preparing the reference patterns, the rates are increased to 93.8% and 91.1%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-78"
  },
  "camelin08_interspeech": {
   "authors": [
    [
     "Nathalie",
     "Camelin"
    ],
    [
     "Geraldine",
     "Damnati"
    ],
    [
     "Frédéric",
     "Bechet"
    ],
    [
     "Renato De",
     "Mori"
    ]
   ],
   "title": "Automatic customer feedback processing: alarm detection in open question spoken messages",
   "original": "i08_0475",
   "page_count": 4,
   "order": 86,
   "p1": "475",
   "pn": "478",
   "abstract": [
    "This paper describes an alarm detection system dedicated to process automatically customer feedbacks in call-centers. Previous studies presented a strategy that consists in the robust detection of subjective opinions about a particular topic in a spoken message. In the present study, we focus on the alarm detection problem in a customer spoken feedback application. We want to characterize each customer's survey with a degree of emergency. All the messages considered as urgent need a quick answer from the call-center service in order to satisfy the customer. The strategy proposed is based on a classification scheme that takes into account all the features that can characterize a survey: answers to the closed questions, topics and opinions detected in the open question spoken message, confidence scores from the Automatic Speech Recognition (ASR) and Spoken Language Understanding (SLU) modules. A field trial realized among real customers has shown that despite the ASR robustness issues, our system efficiently ranks the most urgent messages and brings a finer analysis on the surveys than the one provided by processing the closed questions alone.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-79"
  },
  "balakrishna08_interspeech": {
   "authors": [
    [
     "Mithun",
     "Balakrishna"
    ],
    [
     "Marta",
     "Tatu"
    ],
    [
     "Dan",
     "Moldovan"
    ]
   ],
   "title": "Minimal training based semantic categorization in a voice activated question answering (VAQA) system",
   "original": "i08_0479",
   "page_count": 4,
   "order": 87,
   "p1": "479",
   "pn": "482",
   "abstract": [
    "In this paper, we develop a knowledge based methodology that maps Automatic Speech Recognizer (ASR) transcriptions to predefined semantic categories in a Voice Activated Question Answering (VAQA) system. The proposed semantic categorization methodology, SemCat, uses a novel lexical chains/ontology based algorithm and relies heavily on customized but domain independent Natural Language Processing (NLP) tools and does not require any domainspecific utterance collections or manually annotated text data. SemCat requires minimal manual intervention during training, relying only on the semantics encoded in a brief, manually-created description for each predefined category/ slot. SemCat uses these descriptions along with the eXtended WordNet Knowledge Base (XWN-KB) and several domain independent NLP tools including XWN lexical chains to accurately extract information andmap user utterances to predefined categories. SemCat also uses the domain ontologies created automatically by the Jaguar knowledge acquisition tool to accurately extract domain/customer specific language/terms.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-80"
  },
  "thomson08_interspeech": {
   "authors": [
    [
     "B.",
     "Thomson"
    ],
    [
     "M.",
     "Gašić"
    ],
    [
     "S.",
     "Keizer"
    ],
    [
     "F.",
     "Mairesse"
    ],
    [
     "J.",
     "Schatzmann"
    ],
    [
     "K.",
     "Yu"
    ],
    [
     "Steve",
     "Young"
    ]
   ],
   "title": "User study of the Bayesian update of dialogue state approach to dialogue management",
   "original": "i08_0483",
   "page_count": 4,
   "order": 88,
   "p1": "483",
   "pn": "486",
   "abstract": [
    "This paper presents the results of a comparative user evaluation of various approaches to dialogue management. The major contribution is a comparison of traditional systems against a system that uses a Bayesian Update of Dialogue State approach. This approach is based on the Partially Observable Markov Decision Process (POMDP), which has previously been shown to give improved robustness in simulation experiments. Results from this paper show that the benefits demonstrated in simulation experiments are also obtained when testing a live system with real users.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-81"
  },
  "ikeda08_interspeech": {
   "authors": [
    [
     "Satoshi",
     "Ikeda"
    ],
    [
     "Kazunori",
     "Komatani"
    ],
    [
     "Tetsuya",
     "Ogata"
    ],
    [
     "Hiroshi G.",
     "Okuno"
    ],
    [
     "Hiroshi G.",
     "Okuno"
    ]
   ],
   "title": "Extensibility verification of robust domain selection against out-of-grammar utterances in multi-domain spoken dialogue system",
   "original": "i08_0487",
   "page_count": 4,
   "order": 89,
   "p1": "487",
   "pn": "490",
   "abstract": [
    "We developed a robust domain selection method and verified its extensibility. An issue in domain selection is its robustness against out-of-grammar utterances. It is essential to generate correct system responses because such utterances often cause domain selection errors. We therefore integrated the topic estimation results and the dialogue history to construct a robust domain classifier. Another issue is that domain selection should be performed within an extensible framework, because the system is often modified and extended. That is, the classifier should still have high performance without reconstructing it after adding new domains. The extensibility of our method was not experimentally verified yet, because it requires a lot of effort to collect new dialogue data after extending the system. Therefore, we verified extensibility without collecting new data. We constructed the classifier by leaving out some domains in the dialogue data and then evaluated its accuracy as the classifier for the data where the left-out domains were virtually added.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-82"
  },
  "jan08_interspeech": {
   "authors": [
    [
     "Ea-Ee",
     "Jan"
    ],
    [
     "Osamuyimen",
     "Stewart"
    ],
    [
     "Raymond",
     "Co"
    ],
    [
     "David",
     "Lubensky"
    ]
   ],
   "title": "Improving large scale alphanumeric string recognition using redundant information",
   "original": "i08_0491",
   "page_count": 4,
   "order": 90,
   "p1": "491",
   "pn": "494",
   "abstract": [
    "This paper describes a framework for improving recognition performance and user experience in large scale alphanumeric listings commonly used in conversational speech applications for enterprise. The performance of these speech recognition grammars is severely impacted due to the poor recognition of alphabets. We propose a new approach based on augmenting performance through redundant semantic information. This provides additional acoustic features, which, although is redundant in the semantic space, improves performance by 30% in Canadian postal code application and serial number recognition. The additional queries for redundant semantic information are asked only when necessary: when the system makes false acceptance errors. This ensures that user satisfaction is not interrupted through needless questioning. Furthermore, we propose a way to compress the listing grammar by at least 85% in footprint with minimum performance impact due to good performance in digit recognition. This framework can be extended for general large scale alphanumeric listing grammars.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-83"
  },
  "demuynck08_interspeech": {
   "authors": [
    [
     "Kris",
     "Demuynck"
    ],
    [
     "Jan",
     "Roelens"
    ],
    [
     "Dirk Van",
     "Compernolle"
    ],
    [
     "Patrick",
     "Wambacq"
    ]
   ],
   "title": "SPRAAK: an open source \"SPeech recognition and automatic annotation kit\"",
   "original": "i08_0495",
   "page_count": 1,
   "order": 91,
   "p1": "495",
   "pn": "",
   "abstract": [
    "SPRAAK is a new open source speech recognition package. It is derived from the HMM package that has been developed over the past 15 years at ESAT, KULeuven and which has been in use by a number of other institutions for several years.\n",
    ""
   ]
  },
  "vacher08_interspeech": {
   "authors": [
    [
     "Michel",
     "Vacher"
    ],
    [
     "Anthony",
     "Fleury"
    ],
    [
     "Jean-François",
     "Serignat"
    ],
    [
     "Norbert",
     "Noury"
    ],
    [
     "Hubert",
     "Glasson"
    ]
   ],
   "title": "Preliminary evaluation of speech/sound recognition for telemedicine application in a real environment",
   "original": "i08_0496",
   "page_count": 4,
   "order": 92,
   "p1": "496",
   "pn": "499",
   "abstract": [
    "Improvements in medicine increase life expectancy and the number of elderly persons, but the institutions able to welcome them are not sufficient. A lot of projects work on ways allowing elderly persons to stay at home. This article describes the implementation of a sound classification and speech recognition system equipping a real flat. This system has been evaluated in uncontrolled conditions for distinguishing normal sentences from distress ones; these sentences are uttered by heterogeneous speakers. The detected signals are first classified as sound and speech. The sounds are clustered in eight classes (object fall, doors clap, phone ringing, steps, dishes, doors lock, screams and glass breaking). As for speech signals, an input utterance (in French) is recognized and a subsequent process classifies it in normal or distress, by analysing the presence of distress key words. In the same way, some sound classes are related to a possible distress situation. An experimental protocol was defined and tested in real conditions inside the flat. Finally, we discuss the results of this experiment, where ten subjects were involved.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-84"
  },
  "turunen08_interspeech": {
   "authors": [
    [
     "Markku",
     "Turunen"
    ],
    [
     "Aleksi",
     "Melto"
    ],
    [
     "Anssi",
     "Kainulainen"
    ],
    [
     "Jaakko",
     "Hakulinen"
    ]
   ],
   "title": "Mobidic - a mobile dictation and notetaking application",
   "original": "i08_0500",
   "page_count": 4,
   "order": 93,
   "p1": "500",
   "pn": "503",
   "abstract": [
    "Mobile devices have become ubiquitous and reasonably powerful and well connected. However, their physical size limits possibilities of interaction, especially document creation. Dictation in mobile setting provides one solution, but limited processing power requires that the actual speech recognition is distributed to a server computer. We present MobiDic, a distributed mobile dictation application. Its user interface solutions solve problems that arise from the technical limitation of mobile devices and mobile user context.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-85"
  },
  "hain08_interspeech": {
   "authors": [
    [
     "Thomas",
     "Hain"
    ],
    [
     "Asmaa El",
     "Hannani"
    ],
    [
     "Stuart N.",
     "Wrigley"
    ],
    [
     "Vincent",
     "Wan"
    ]
   ],
   "title": "Automatic speech recognition for scientific purposes - webASR",
   "original": "i08_0504",
   "page_count": 4,
   "order": 94,
   "p1": "504",
   "pn": "507",
   "abstract": [
    "We present 'webASR', an online interface to our state-of-the-art automatic speech recognition (ASR) systems. It aims to provide the wider scientific research community with an interface to speech transcription for domains and applications where the generation of such transcripts was not previously feasible. The webASR interface allows the upload of audio files and, in turn, the download of automatically generated ASR transcripts. Depending upon the specification given for an audio file, the system will transcribe using an appropriate speech recogniser chosen from one of the many available, such as a NIST RT evaluation system. The transcripts will be available for download after processing.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-86"
  },
  "meinedo08_interspeech": {
   "authors": [
    [
     "Hugo",
     "Meinedo"
    ],
    [
     "Marcio",
     "Viveiros"
    ],
    [
     "Joao",
     "Neto"
    ]
   ],
   "title": "Evaluation of a live broadcast news subtitling system for portuguese",
   "original": "i08_0508",
   "page_count": 4,
   "order": 95,
   "p1": "508",
   "pn": "511",
   "abstract": [
    "Broadcast news play an important role in our lives providing access to news, information and entertainment. The existence of subtitles is an important medium for inclusion of people with special needs and also an advantage on noisy and populated environments. In this work we will describe and evaluate a system for subtitling live broadcast news for RTP (Radio Televisao de Portugal) the Portuguese public broadcast company. Developing a fully automatic subtitling system is a huge breakthrough which results from the convergence of different research models and software developments to create a working system. Our online system has 12% word error rate for the displayed subtitles working under real time with an average latency of just 6.5 seconds.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-87"
  },
  "suzuki08_interspeech": {
   "authors": [
    [
     "Tomoko",
     "Suzuki"
    ],
    [
     "Machiko",
     "Ikemoto"
    ],
    [
     "Tomoko",
     "Sano"
    ],
    [
     "Toshihiko",
     "Kinoshita"
    ]
   ],
   "title": "Multidimensional features of emotional speech",
   "original": "i08_0240",
   "page_count": 1,
   "order": 96,
   "p1": "240",
   "pn": "",
   "abstract": [
    "The purpose of this study is to investigate the features of emotional speech by means of multidimensional scaling procedure(MDS) based on visual-perceived similarity of vocal parameters. We extracted three vocal parameters (pitch, intensity and spectrogram) from speeches expressed emotions. Three researchers grouped together the cards of parameters in view of visual similarity. According to the result of MDS of spectrogram, we found two dimensions, plesureness (positive-negative) and activation(high activation - low activation), which are similar in structure to auditory perception in vocal emotions. Finally, we concluded that features of spectrogram related to pleasureness.\n",
    ""
   ]
  },
  "boufaden08_interspeech": {
   "authors": [
    [
     "Narjes",
     "Boufaden"
    ],
    [
     "Pierre",
     "Dumouchel"
    ]
   ],
   "title": "Leveraging emotion detection using emotions from yes-no answers",
   "original": "i08_0241",
   "page_count": 4,
   "order": 97,
   "p1": "241",
   "pn": "244",
   "abstract": [
    "We present a new approach for the detection of negative versus non-negative emotions from Human-computer dialogs in the specific domain of call centers. We argue that it is possible to improve emotion detection without using additional information being linguistic or contextual. We show that no-answers are emotional salient words and that it is possible to improve the accuracy of the classification of Human-computer dialogs by taking advantage of the high accuracy achieved on no-answer turns. We also show that stacked generalization using neural networks and SVM as base models improves the accuracy of each model while the combination of the no-model and the dialog model improves the accuracy of the dialog-model alone by 13%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-88"
  },
  "millhouse08_interspeech": {
   "authors": [
    [
     "Thomas J.",
     "Millhouse"
    ],
    [
     "Dianna T.",
     "Kenny"
    ]
   ],
   "title": "Vowel placement during operatic singing: 'come si parla' or 'aggiustamento'?",
   "original": "i08_0245",
   "page_count": 4,
   "order": 98,
   "p1": "245",
   "pn": "248",
   "abstract": [
    "This study explored two tenets of the Italian Bel Canto operatic singing technique. \"Come si parla\" and \"aggiustamento.\" Articulatory changes in the lower formant vowel space of 11 spoken and sung vowels were systematically examined in six male singers. Results showed that singers influence the placement of the lowest formant frequencies in the sung vowel space using both a lowered larynx and modified vowel articulation (aggiustamento) with rising pitch, especially above 220Hz.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-89"
  },
  "kato08_interspeech": {
   "authors": [
    [
     "Yumiko O.",
     "Kato"
    ],
    [
     "Yoshifumi",
     "Hirose"
    ],
    [
     "Takahiro",
     "Kamai"
    ]
   ],
   "title": "Study on strained rough voice as a conveyer of rage",
   "original": "i08_0249",
   "page_count": 4,
   "order": 99,
   "p1": "249",
   "pn": "252",
   "abstract": [
    "It is important to be able to determine anger and its degree for dialog management in an interactive speech interface. We investigated the characteristics of a strained rough voice as a conveyer of a speaker's degree of anger. In hot anger speech in Japanese, a rough voice with high glottal tension is observed frequently, and the rate of occurrence of the strained rough voice increases according to the degree of anger. In a typical male speaker's speech sample, amplitude fluctuations observed in a strained rough voice were periodic; the frequency was around between 40.80 Hz. The modulation ratio in rage speech was larger than in other emotional states, suggesting the possibility of determining the speaker's anger and its degree by detecting strained rough voice.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-90"
  },
  "begum08_interspeech": {
   "authors": [
    [
     "Mumtaz",
     "Begum"
    ],
    [
     "Raja N.",
     "Ainon"
    ],
    [
     "Roziati",
     "Zainuddin"
    ],
    [
     "Zuraidah M.",
     "Don"
    ],
    [
     "Gerry",
     "Knowles"
    ]
   ],
   "title": "Integrating rule and template-based approaches for emotional Malay speech synthesis",
   "original": "i08_0253",
   "page_count": 4,
   "order": 100,
   "p1": "253",
   "pn": "256",
   "abstract": [
    "The manipulation of prosody, including pitch, duration and intensity, is one of the leading approaches in synthesizing emotion. This paper reports work on the development of a Malay Emotional synthesizer capable of expressing four basic emotions, namely happiness, anger, sadness and fear for any form of text input with various intonation patterns using the prosody manipulation principle. The synthesizer makes use of prosody templates and prosody parametric manipulation for different types of sentence structure.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-91"
  },
  "busso08_interspeech": {
   "authors": [
    [
     "Carlos",
     "Busso"
    ],
    [
     "Shrikanth S.",
     "Narayanan"
    ]
   ],
   "title": "The expression and perception of emotions: comparing assessments of self versus others",
   "original": "i08_0257",
   "page_count": 4,
   "order": 101,
   "p1": "257",
   "pn": "260",
   "abstract": [
    "In the study of expressive speech communication, it is commonly accepted that the emotion perceived by the listener is a good approximation of the intended emotion conveyed by the speaker. This paper analyzes the validity of this assumption by comparing the mismatches between the assessments made by naive listeners and by the speakers that generated the data. The analysis is based on the hypothesis that people are better decoders of their own emotions. Therefore, self-assessments will be closer to the intended emotions. Using the IEMOCAP database, discrete (categorical) and continuous (attribute) emotional assessments evaluated by the actors and naive listeners are compared. The results indicate that there is a mismatch between the expression and perception of emotion. The speakers in the database assigned their own emotions to more specific emotional categories, which led to more extreme values in the activation-valence space.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-92"
  },
  "krahmer08_interspeech": {
   "authors": [
    [
     "Emiel",
     "Krahmer"
    ],
    [
     "Marc",
     "Swerts"
    ]
   ],
   "title": "On the role of acting skills for the collection of simulated emotional speech",
   "original": "i08_0261",
   "page_count": 4,
   "order": 102,
   "p1": "261",
   "pn": "264",
   "abstract": [
    "We experimentally compared non-simulated with simulated expressions of emotion produced both by inexperienced and by experienced actors. Contrary to our expectations, in a perception experiment participants rated the expressions of experienced actors as more extreme and less like non-simulated (\"real\") expressions than those produced by non-professional actors.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-93"
  },
  "schuller08_interspeech": {
   "authors": [
    [
     "Björn",
     "Schuller"
    ],
    [
     "Matthias",
     "Wimmer"
    ],
    [
     "Dejan",
     "Arsic"
    ],
    [
     "Tobias",
     "Moosmayr"
    ],
    [
     "Gerhard",
     "Rigoll"
    ]
   ],
   "title": "Detection of security related affect and behaviour in passenger transport",
   "original": "i08_0265",
   "page_count": 4,
   "order": 103,
   "p1": "265",
   "pn": "268",
   "abstract": [
    "Surveillance of drivers, pilots or passengers possesses significant potential for increased security within passenger transport. In an automotive setting the interaction can e.g. be improved by social awareness of an MMI. As further example security marshals can be efficiently positioned guided by according systems. Within this scope the detection of security relevant behavior patterns as aggressiveness or stress is discussed. The focus lies on real-life usage respecting online processing, subject independency, and noise robustness. The approach introduced employs multivariate time-series analysis for the synchronization and data reduction of audio and video by brute-force feature generation. By combined optimization of the large audiovisual space accuracy is boosted. Extensive results are reported on aviation behavior, as well as in particular for the audio channel on numerous standard corpora. The influence of noise will be discussed by representative car-noise overlay.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-94"
  },
  "goudbeek08_interspeech": {
   "authors": [
    [
     "Martijn",
     "Goudbeek"
    ],
    [
     "Jean Philippe",
     "Goldman"
    ],
    [
     "Klaus R.",
     "Scherer"
    ]
   ],
   "title": "Emotions and articulatory precision",
   "original": "i08_0317",
   "page_count": 1,
   "order": 104,
   "p1": "317",
   "pn": "",
   "abstract": [
    "The influence of emotion on articulatory precision was investigated in a newly established corpus of acted emotional utterances. The area of the vocalic triangle between the vowels /i/, /u/, and /a/ was measured and shown to be significantly affected by emotion. Furthermore, this area correlated significantly with the potency dimension of a large scale study of emotion words, reflecting the predictions of the component process model of emotion.\n",
    ""
   ]
  },
  "truong08_interspeech": {
   "authors": [
    [
     "Khiet P.",
     "Truong"
    ],
    [
     "Mark A.",
     "Neerincx"
    ],
    [
     "David A. van",
     "Leeuwen"
    ]
   ],
   "title": "Assessing agreement of observer- and self-annotations in spontaneous multimodal emotion data",
   "original": "i08_0318",
   "page_count": 4,
   "order": 105,
   "p1": "318",
   "pn": "321",
   "abstract": [
    "We investigated inter-observer agreement and the reliability of self-reported emotion ratings (i.e., self-raters judging their own emotions) in spontaneous multimodal emotion data. During a multiplayer video game, vocal and facial expressions were recorded (including the game content itself) and were annotated by the players themselves on arousal and valence scales. In a perception experiment, observers rated a small part of the data that was provided in 4 conditions: audio only, visual only, audiovisual and audiovisual plus context. Inter-observer agreements varied between 0.32 and 0.52 when the ratings were scaled. Providing multimodal information usually increased agreement. Finally, we found that the averaged agreement between the self-rater and the observers was somewhat lower than the inter-observer agreement.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-95"
  },
  "arimoto08_interspeech": {
   "authors": [
    [
     "Yoshiko",
     "Arimoto"
    ],
    [
     "Hiromi",
     "Kawatsu"
    ],
    [
     "Sumio",
     "Ohno"
    ],
    [
     "Hitoshi",
     "Iida"
    ]
   ],
   "title": "Emotion recognition in spontaneous emotional speech for anonymity-protected voice chat systems",
   "original": "i08_0322",
   "page_count": 4,
   "order": 106,
   "p1": "322",
   "pn": "325",
   "abstract": [
    "For the purpose of determining emotion recognition by acoustic information, we recorded natural dialogs made by two or three players of online games to construct an emotional speech database. Two evaluators categorized recorded utterances in a certain emotion, which were defined with reference to the eight primary emotions of Plutchik's three-dimensional circumplex model. Furthermore, 14 evaluators graded utterances using a 5-point scale of subjective evaluation to obtain reference degrees of emotion. Eleven acoustic features were extracted from utterances and analysis of variance (ANOVA) was conducted to assess significant differences between emotions. Based on the results of ANOVA, we conducted discriminant analysis to discriminate one emotion from the others. Moreover, the experiment estimating emotional degree was conducted with multiple linear regression analysis to estimate emotional degree for each utterance. As a result of discriminant analysis, high correctness values of 79.12% for Surprise and 70.11% for Sadness were obtained, and over 60% correctness were obtained for most of the other emotions. As for emotional degree estimation, values of the adjusted R square (.R2) for each emotion ranged from 0.05 (Disgust) to 0.55 (Surprise) for closed sets, and values of root mean square (RMS) of residual for open sets ranged from 0.39 (Acceptance) to 0.59 (Anger).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-96"
  },
  "shaikh08_interspeech": {
   "authors": [
    [
     "Mostafa Al Masum",
     "Shaikh"
    ],
    [
     "Md. Khademul Islam",
     "Molla"
    ],
    [
     "Keikichi",
     "Hirose"
    ]
   ],
   "title": "Assigning suitable phrasal tones and pitch accents by sensing affective information from text to synthesize human-like speech",
   "original": "i08_0326",
   "page_count": 4,
   "order": 107,
   "p1": "326",
   "pn": "329",
   "abstract": [
    "We have carried out several perceptual and objective experiments that show that the present Text-To-Speech (TTS) systems are weak in the relevance of prosody and segmental spectrum in the characterization and expression of emotions. Since it is known that the emotional state of a speaker usually alters the way s/he speaks, the TTS systems need to be improved to generate human-like pitch accents to express the subtle features of emotions. This paper describes a pitch accent assignment technique which places appropriate pitch accents on elements of the utterance that require particular emphasis or stress. Our pitch accenting technique utilizes commonsense knowledge-base and a linguistic tool to recognize emotion conveyed though the text itself. From these it determines whether the content of the utterance has a connotation to a particular emotion (e.g., happy, sad, surprise etc.), good or bad concepts, praiseworthy or blameworthy actions, common or vital information. It can then assign an appropriate pitch accent to one word in each prosodic phrase. The TTS component then determines the appropriate syllable to be accented in the word. Our approach can well support a TTS system's synthesis, allowing the system to generate affective version of the spoken text.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-97"
  },
  "yanushevskaya08_interspeech": {
   "authors": [
    [
     "Irena",
     "Yanushevskaya"
    ],
    [
     "Ailbhe",
     "Ní Chasaide"
    ],
    [
     "Christer",
     "Gobl"
    ]
   ],
   "title": "Cross-language study of vocal correlates of affective states",
   "original": "i08_0330",
   "page_count": 4,
   "order": 108,
   "p1": "330",
   "pn": "333",
   "abstract": [
    "This paper is concerned with a cross-cultural study of vocal correlates of affect. Speakers of 4 languages, Irish-English, Russian, Spanish and Japanese, were asked to judge affective content of synthesised stimuli of three types: (1) stimuli varying in voice quality, with a neutral pitch contour; (2) stimuli with affect-related f0 contours and modal voice; and (3) stimuli in which specific voice qualities and affect-related f0 contours were combined. Some of the main results are illustrated and point to similarities among the language groups as well as some striking cross-language/culture differences in how these stimuli map to affect.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-98"
  },
  "swerts08_interspeech": {
   "authors": [
    [
     "Marc",
     "Swerts"
    ],
    [
     "Emiel",
     "Krahmer"
    ]
   ],
   "title": "Gender-related differences in the production and perception of emotion",
   "original": "i08_0334",
   "page_count": 4,
   "order": 109,
   "p1": "334",
   "pn": "337",
   "abstract": [
    "This article discusses a study into gender-related differences in the experience, display and perception of positive and negative emotions. The problem is addressed by a combination of a production and a perception experiment. The production study makes use of a mood induction procedure which aims to elicit positive and negative emotions in subjects by letting them view different kinds of movie clips. This method turned out to be extremely effective, and also revealed that women experience emotions more intensively than men. Video clips collected through the production study were then shown to independent observers, whose task it was to judge whether the recorded speakers were in a negative or positive condition. Overall, judges were very accurate in estimating whether a displayed speaker was positive or negative. Moreover, there were again gender differences in that female speakers were more expressive than male speakers, and female observers were more accurate in their judgments than male observers.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-99"
  },
  "li08c_interspeech": {
   "authors": [
    [
     "Jinyu",
     "Li"
    ],
    [
     "Zhi-Jie",
     "Yan"
    ],
    [
     "Chin-Hui",
     "Lee"
    ],
    [
     "Ren-Hua",
     "Wang"
    ]
   ],
   "title": "Soft margin estimation with various separation levels for LVCSR",
   "original": "i08_0269",
   "page_count": 4,
   "order": 110,
   "p1": "269",
   "pn": "272",
   "abstract": [
    "We continue our previous work on soft margin estimation (SME) to large vocabulary continuous speech recognition (LVCSR) in two new aspects. The first is to formulate SME with different unit separation. SME methods focusing on string-, word-, and phone-level separation are defined. The second is to compare SME with all the popular conventional discriminative training (DT) methods, including maximum mutual information estimation (MMIE), minimum classification error (MCE), and minimum word/phone error (MWE/MPE). Tested on the 5k-word Wall Street Journal task, all the SME methods achieves a relative word error rate (WER) reduction from 17% to 25% over our baseline. Among them, phone-level SME obtains the best performance. Its performance is slightly better than that of MPE, and much better than those of other conventional DT methods. With the comprehensive comparison with conventional DT methods, SME demonstrates its success on LVCSR tasks.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-100"
  },
  "heigold08_interspeech": {
   "authors": [
    [
     "Georg",
     "Heigold"
    ],
    [
     "Patrick",
     "Lehnen"
    ],
    [
     "Ralf",
     "Schlüter"
    ],
    [
     "Hermann",
     "Ney"
    ]
   ],
   "title": "On the equivalence of Gaussian and log-linear HMMs",
   "original": "i08_0273",
   "page_count": 4,
   "order": 111,
   "p1": "273",
   "pn": "276",
   "abstract": [
    "The acoustic models of conventional state-of-the-art speech recognition systems use generative Gaussian HMMs. In the past few years, discriminative models like for example Conditional Random Fields (CRFs) have been proposed to refine the acoustic models. CRFs directly model the class posteriors, the quantities of interest in recognition. CRFs are undirected models, and CRFs do not assume local normalization constraints as HMMs do. This paper addresses the issue to what extent such less restricted models add flexiblity to the model compared with the generative counterpart. This work extends our previous work in that it provides the technical details used for showing the equivalence of Gaussian and log-linear HMMs. The correctness of the proposed equivalence transformation for conditional probabilities is demonstrated on a simple concept tagging task.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-101"
  },
  "kanevsky08_interspeech": {
   "authors": [
    [
     "Dimitri",
     "Kanevsky"
    ],
    [
     "Tara N.",
     "Sainath"
    ],
    [
     "Bhuvana",
     "Ramabhadran"
    ],
    [
     "David",
     "Nahamoo"
    ]
   ],
   "title": "Generalization of extended baum-welch parameter estimation for discriminative training and decoding",
   "original": "i08_0277",
   "page_count": 4,
   "order": 112,
   "p1": "277",
   "pn": "280",
   "abstract": [
    "We demonstrate the generalizability of the Extended Baum-Welch (EBW) algorithm not only for HMM parameter estimation but for decoding as well. We show that there can exist a general function associated with the objective function under EBW that reduces to the well-known auxiliary function used in the Baum-Welch algorithm for maximum likelihood estimates. We generalize representation for the updates of model parameters by making use of a differentiable function (such as arithmetic or geometric mean) on the updated and current model parameters and describe their effect on the learning rate during HMM parameter estimation. Improvements on speech recognition tasks are also presented here.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-102"
  },
  "liu08_interspeech": {
   "authors": [
    [
     "Peng",
     "Liu"
    ],
    [
     "Frank K.",
     "Soong"
    ]
   ],
   "title": "An ellipsoid constrained quadratic programming perspective to discriminative training of HMMs",
   "original": "i08_0281",
   "page_count": 4,
   "order": 113,
   "p1": "281",
   "pn": "284",
   "abstract": [
    "In this paper, we reformulate the optimization in discriminative training (DT) of HMMs as an ellipsoid constrained quadratic programming (ECQP) problem, where a second order of the non-linear space is adopted. We show that the unique optimal solution of ECQP can be obtained by an efficient line search and no relaxation is needed as in a general quadratically constrained quadratic programming (QCQP). Moreover, a subspace combination condition is introduced to further simplify it under certain cases. The concrete ECQP form of DT of HMMs is given based on a locality constraint and reasonable assumptions, and the algorithm can be conducted to update Gaussians jointly or separately in either sequential or batch mode. Under the perspective of ECQP, relationships between various popular DT optimization algorithms are discussed. Experimental results on two recognition tasks show that ECQP considerably outperforms other popular algorithms in terms of final recognition accuracy and convergence speed in iterations.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-103"
  },
  "yu08_interspeech": {
   "authors": [
    [
     "Dong",
     "Yu"
    ],
    [
     "Li",
     "Deng"
    ],
    [
     "Yifan",
     "Gong"
    ],
    [
     "Alex",
     "Acero"
    ]
   ],
   "title": "Discriminative training of variable-parameter HMMs for noise robust speech recognition",
   "original": "i08_0285",
   "page_count": 4,
   "order": 114,
   "p1": "285",
   "pn": "288",
   "abstract": [
    "We propose a new type of variable-parameter hidden Markov model (VPHMM) whose mean and variance parameters vary each as a continuous function of additional environment-dependent parameters. Different from the polynomial-function-based VPHMM proposed by Cui and Gong (2007), the new VPHMM uses cubic splines to represent the dependency of the means and variances of Gaussian mixtures on the environment parameters. Importantly, the new model no longer requires quantization in estimating the model parameters and it supports parameter sharing and instantaneous conditioning parameters directly. We develop and describe a growth-transformation algorithm that discriminatively learns the parameters in our cubic-spline-based VPHMM (CS-VPHMM), and evaluate the model on the Aurora-3 corpus with our recently developed MFCC-MMSE noise suppressor applied. Our experiments show that the proposed CS-VPHMM outperforms the discriminatively trained and maximum-likelihood trained conventional HMMs with relative word error rate (WER) reduction of 14% and 20% respectively under the well-matched conditions when both mean and variances are updated.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-104"
  },
  "droppo08_interspeech": {
   "authors": [
    [
     "Jasha",
     "Droppo"
    ],
    [
     "Michael L.",
     "Seltzer"
    ],
    [
     "Alex",
     "Acero"
    ],
    [
     "Yu-Hsiang Bosco",
     "Chiu"
    ]
   ],
   "title": "Towards a non-parametric acoustic model: an acoustic decision tree for observation probability calculation",
   "original": "i08_0289",
   "page_count": 4,
   "order": 115,
   "p1": "289",
   "pn": "292",
   "abstract": [
    "Modern automatic speech recognition systems use Gaussian mixture models (GMM) on acoustic observations to model the probability of producing a given observation under any one of many hidden discrete phonetic states. This paper investigates the feasibility of using an acoustic decision tree to directly model these probabilities. Unlike the more common phonetic decision tree, which asks questions about phonetic context, an acoustic decision tree asks questions about the vector-valued observations. Three different types of acoustic questions are proposed and evaluated, including LDA, PCA, and MMI questions. Frame classification experiments are run on a subset of the Switchboard corpus. On these experiments, the acoustic decision tree produces slightly better results than maximum likelihood trained GMMs, with significantly less computation. Some theoretical advantages of the acoustic decision tree are discussed, including more economical use of the training data and reduced mismatch between the acoustic model and the true probability distribution of the phonetic labels.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-105"
  },
  "bell08_interspeech": {
   "authors": [
    [
     "Peter",
     "Bell"
    ],
    [
     "Simon",
     "King"
    ]
   ],
   "title": "A shrinkage estimator for speech recognition with full covariance HMMs",
   "original": "i08_0910",
   "page_count": 4,
   "order": 116,
   "p1": "910",
   "pn": "913",
   "abstract": [
    "We consider the problem of parameter estimation in full-covariance Gaussian mixture systems for automatic speech recognition. Due to the high dimensionality of the acoustic feature vector, the standard sample covariance matrix has a high variance and is often poorly-conditioned when the amount of training data is limited. We explain how the use of a shrinkage estimator can solve these problems, and derive a formula for the optimal shrinkage intensity. We present results of experiments on a phone recognition task, showing that the estimator gives a performance improvement over a standard full-covariance system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-106"
  },
  "bell08b_interspeech": {
   "authors": [
    [
     "Peter",
     "Bell"
    ],
    [
     "Simon",
     "King"
    ]
   ],
   "title": "Covariance updates for discriminative training by constrained line search",
   "original": "i08_0914",
   "page_count": 1,
   "order": 117,
   "p1": "914",
   "pn": "",
   "abstract": [
    "We investigate the recent Constrained Line Search algorithm for discriminative training of HMMs and propose an alternative formula for variance update. We compare the method to standard techniques on a phone recognition task.\n",
    ""
   ]
  },
  "mak08_interspeech": {
   "authors": [
    [
     "Brian",
     "Mak"
    ],
    [
     "Tom",
     "Ko"
    ]
   ],
   "title": "Min-max discriminative training of decoding parameters using iterative linear programming",
   "original": "i08_0915",
   "page_count": 4,
   "order": 118,
   "p1": "915",
   "pn": "918",
   "abstract": [
    "In automatic speech recognition, the decoding parameters - grammar factor and word insertion penalty - are usually hand-tuned to give the best recognition performance. This paper investigates an automatic procedure to determine their values using an iterative linear programming (LP) algorithm. LP naturally implements discriminative training by mapping linear discriminants into LP constraints. A min-max cost function is also defined to get more stable and robust result. Empirical evaluations on the RM1 and WSJ0 speech recognition tasks show that decoding parameters found by the proposed algorithm are as good as those found by a brute-force grid search; their optimal values also seem to be independent of the initial values set to start the iterative LP algorithm.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-107"
  },
  "willett08_interspeech": {
   "authors": [
    [
     "Daniel",
     "Willett"
    ],
    [
     "Chuang",
     "He"
    ]
   ],
   "title": "Discriminative training for complementariness in system combination",
   "original": "i08_0919",
   "page_count": 1,
   "order": 119,
   "p1": "919",
   "pn": "",
   "abstract": [
    "In recent years, techniques of output combination from multiple speech recognizers for improved overall performance have gained popularity. Most commonly, the combined systems are established independently. This paper describes our attempt to directly target joint system performance in the discriminative training objective of acoustic model parameter estimation. It also states first promising results.\n",
    ""
   ]
  },
  "saon08_interspeech": {
   "authors": [
    [
     "George",
     "Saon"
    ],
    [
     "Daniel",
     "Povey"
    ]
   ],
   "title": "Penalty function maximization for large margin HMM training",
   "original": "i08_0920",
   "page_count": 4,
   "order": 120,
   "p1": "920",
   "pn": "923",
   "abstract": [
    "We perform large margin training of HMM acoustic parameters by maximizing a penalty function which combines two terms. The first term is a scale which gets multiplied with the Hamming distance between HMM state sequences to form a multi-label (or sequence) margin. The second term arises from constraints on the training data that the joint log-likelihoods of acoustic and correct word sequences exceed the joint log-likelihoods of acoustic and incorrect word sequences by at least the multi-label margin between the corresponding Viterbi state sequences. Using the soft-max trick, we collapse these constraints into a boosted MMI-like term. The resulting objective function can be efficiently maximized using extended Baum-Welch updates. Experimental results on multiple LVCSR tasks show a good correlation between the objective function and the word error rate.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-108"
  },
  "bolanos08_interspeech": {
   "authors": [
    [
     "Daniel",
     "Bolaños"
    ],
    [
     "Wayne",
     "Ward"
    ]
   ],
   "title": "Implicit state-tying for support vector machines based speech recognition",
   "original": "i08_0924",
   "page_count": 4,
   "order": 121,
   "p1": "924",
   "pn": "927",
   "abstract": [
    "In this article we take a step forward towards the application of Support Vector Machines (SVMs) to continuous speech recognition. As in previous work, we use SVMs to estimate emission probabilities in the context of an SVM/HMM system. However, training pairwise classifiers to discriminate between some of the HMM-states of very close phonetic classes produce unsatisfactory results. We propose a data-driven approach for selecting the HMM-states for which SVMs are trained and those ones that are implicitly tied.\n",
    "Additionally we introduce an algorithm that is incorporated into the decoder for dynamically selecting the subset of SVMs used to estimate the emission probabilities. This algorithm reduces the number of SVMs evaluated at the frame level dramatically while preserving recognition accuracy. We present results in a very challenging corpora composed of children's speech. Our approach not only outperforms comparable GMM/HMM based systems but other SVM/HMM systems proposed to date.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-109"
  },
  "aradilla08_interspeech": {
   "authors": [
    [
     "Guillermo",
     "Aradilla"
    ],
    [
     "Hervé",
     "Bourlard"
    ],
    [
     "Mathew Magimai",
     "Doss"
    ]
   ],
   "title": "Using KL-based acoustic models in a large vocabulary recognition task",
   "original": "i08_0928",
   "page_count": 4,
   "order": 122,
   "p1": "928",
   "pn": "931",
   "abstract": [
    "Posterior probabilities of sub-word units have been shown to be an effective front-end for ASR. However, attempts to model this type of features either do not benefit from modeling context-dependent phonemes, or use an inefficient distribution to estimate the state likelihood. This paper presents a novel acoustic model for posterior features that overcomes these limitations. The proposed model can be seen as a HMM where the score associated with each state is the KL divergence between a distribution characterizing the state and the posterior features from the test utterance. This KL-based acoustic model establishes a framework where other models for posterior features such as hybrid HMM/MLP and discrete HMM can be seen as particular cases. Experiments on the WSJ database show that the KL-based acoustic model can significantly outperform these latter approaches. Moreover, the proposed model can obtain comparable results to complex systems, such as HMM/GMM, using significantly fewer parameters.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-110"
  },
  "shiota08_interspeech": {
   "authors": [
    [
     "Sayaka",
     "Shiota"
    ],
    [
     "Kei",
     "Hashimoto"
    ],
    [
     "Heiga",
     "Zen"
    ],
    [
     "Yoshihiko",
     "Nankaku"
    ],
    [
     "Akinobu",
     "Lee"
    ],
    [
     "Keiichi",
     "Tokuda"
    ]
   ],
   "title": "Acoustic modeling based on model structure annealing for speech recognition",
   "original": "i08_0932",
   "page_count": 4,
   "order": 123,
   "p1": "932",
   "pn": "935",
   "abstract": [
    "This paper proposes an HMM training technique using multiple phonetic decision trees and evaluates it in speech recognition. In the use of context dependent models, the decision tree based context clustering is applied to find a parameter tying structure. However, the clustering is usually performed based on statistics of HMM state sequences which are obtained by unreliable models without context clustering. To avoid this problem, we optimize the decision trees and HMM state sequences simultaneously. In the proposed method, this is performed by maximum likelihood (ML) estimation of a newly defined statistical model which includes multiple decision trees as hidden variables. Applying the deterministic annealing expectation maximization (DAEM) algorithm and using multiple decision trees in early stage of model training, state sequences are reliably estimated. In continuous phoneme recognition experiments, the proposed method can improve the recognition performance.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-111"
  },
  "hashimoto08_interspeech": {
   "authors": [
    [
     "Kei",
     "Hashimoto"
    ],
    [
     "Heiga",
     "Zen"
    ],
    [
     "Yoshihiko",
     "Nankaku"
    ],
    [
     "Akinobu",
     "Lee"
    ],
    [
     "Keiichi",
     "Tokuda"
    ]
   ],
   "title": "Bayesian context clustering using cross valid prior distribution for HMM-based speech recognition",
   "original": "i08_0936",
   "page_count": 4,
   "order": 124,
   "p1": "936",
   "pn": "939",
   "abstract": [
    "This paper proposes a prior distribution determination technique using cross validation for speech recognition based on the Bayesian approach. The Bayesian method is a statistical technique for estimating reliable predictive distributions by marginalizing model parameters and its approximate version, the variational Bayesian method has been applied to HMM-based speech recognition. Since prior distributions representing prior information about model parameters affect the posterior distributions and model selection, the determination of prior distributions is an important problem. However, it has not been thoroughly investigate in speech recognition. The proposed method can determine reliable prior distributions without tuning parameters and select an appropriate model structure dependently on the amount of training data. Continuous phoneme recognition experiments show that the proposed method achieved a higher performance than the conventional methods.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-112"
  },
  "ajmera08_interspeech": {
   "authors": [
    [
     "Jitendra",
     "Ajmera"
    ],
    [
     "Masami",
     "Akamine"
    ]
   ],
   "title": "Speech recognition using soft decision trees",
   "original": "i08_0940",
   "page_count": 4,
   "order": 125,
   "p1": "940",
   "pn": "943",
   "abstract": [
    "This paper presents recent developments at our site toward speech recognition using decision tree based acoustic models. Previously, robust decision trees have been shown to achieve better performance compared to standard Gaussian mixture model (GMM) acoustic models. This was achieved by converting hard questions (decisions) of a standard tree into soft questions using sigmoid function. In this paper, we report our work where soft-decision trees are trained from scratch. These soft-decision trees are shown to yield better speech recognition accuracy compared to standard GMM acoustic models on Aurora digit recognition task.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-113"
  },
  "shi08_interspeech": {
   "authors": [
    [
     "Yu",
     "Shi"
    ],
    [
     "Frank",
     "Seide"
    ],
    [
     "Frank K.",
     "Soong"
    ]
   ],
   "title": "GPU-accelerated Gaussian clustering for fMPE discriminative training",
   "original": "i08_0944",
   "page_count": 4,
   "order": 126,
   "p1": "944",
   "pn": "947",
   "abstract": [
    "The Graphics Processing Unit (GPU) has extended its applications from its original graphic rendering to more general scientific computation. Through massive parallelization, state-of-the-art GPUs can deliver 200 billion floating-point operations per second (0.2 TFLOPS) on a single consumer-priced graphics card. This paper describes our attempt in leveraging GPUs for efficient HMM model training. We show that using GPUs for a specific example of Gaussian clustering, as required in fMPE, or feature-domain Minimum Phone Error discriminative training, can be highly desirable. The clustering of huge number of Gaussians is very time consuming due to the enormous model size in current LVCSR systems. Comparing an NVidia Geforce 8800 Ultra GPU against an Intel Pentium 4 implementation, we find that our brute-force GPU implementation is 14 times faster overall than a CPU implementation that uses approximate speed-up heuristics. GPU accelerated fMPE reduces the WER 6% relatively, compared to the maximum-likelihood trained baseline on two conversational-speech recognition tasks.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-114"
  },
  "hifny08_interspeech": {
   "authors": [
    [
     "Yasser",
     "Hifny"
    ],
    [
     "Yuqing",
     "Gao"
    ]
   ],
   "title": "Discriminative training using the trusted expectation maximization",
   "original": "i08_0948",
   "page_count": 4,
   "order": 127,
   "p1": "948",
   "pn": "951",
   "abstract": [
    "We present the Trusted Expectation-Maximization (TEM), a new discriminative training scheme, for speech recognition applications. In particular, the TEM algorithm may be used for Hidden Markov Models (HMMs) based discriminative training. The TEM algorithm has a form similar to the Expectation-Maximization (EM) algorithm, which is an efficient iterative procedure to perform maximum likelihood in the presence of hidden variables [1]. The TEM algorithm has been empirically shown to increase a rational objective function. In the concave regions of a rational function, it can be shown that the maximization steps of the TEM algorithm and the hypothesized EM algorithm are identical. In the TIMIT phone recognition task, preliminary experimental results show competitive optimization performance over the conventional discriminative training approaches (in terms of speech and accuracy).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-115"
  },
  "huang08_interspeech": {
   "authors": [
    [
     "Jui-Ting",
     "Huang"
    ],
    [
     "Mark",
     "Hasegawa-Johnson"
    ]
   ],
   "title": "Maximum mutual information estimation with unlabeled data for phonetic classification",
   "original": "i08_0952",
   "page_count": 4,
   "order": 128,
   "p1": "952",
   "pn": "955",
   "abstract": [
    "This paper proposes a new training framework for mixed labeled and unlabeled data and evaluates it on the task of binary phonetic classification. Our training objective function combines Maximum Mutual Information (MMI) for labeled data and Maximum Likelihood (ML) for unlabeled data. Through the modified training objective, MMI estimates are smoothed with ML estimates obtained from unlabeled data. On the other hand, our training criterion can also help the existing model adapt to new speech characteristics from unlabeled speech. In our experiments of phonetic classification, there is a consistent reduction of error rate from MLE to MMIE with I-smoothing, and then to MMIE with unlabeled-smoothing. Error rates can be further reduced by transductive-MMIE. We also experimented with the gender-mismatched case, in which the best improvement shows MMIE with unlabeled data has a 9.3% absolute lower error rate than MLE and a 2.35% absolute lower error rate than MMIE with I-smoothing.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-116"
  },
  "tyagi08_interspeech": {
   "authors": [
    [
     "Vivek",
     "Tyagi"
    ]
   ],
   "title": "Maximum accept and reject (MARS) training of HMM-GMM speech recognition systems",
   "original": "i08_0956",
   "page_count": 4,
   "order": 129,
   "p1": "956",
   "pn": "959",
   "abstract": [
    "This paper describes a new discriminative HMM parameter estimation technique. It supplements the usual ML optimization function with the emission (accept) likelihood of the aligned state (phone) and the rejection likelihoods from the rest of the states (phones). Intuitively, this new optimization function takes into the account as to how well the other states are rejecting the current frame that has been aligned with a given state. This simple scheme, termed as Maximum Accept and Reject (MARS), implicitly brings in the discriminative information and hence performs better than the ML trained models. As is well known, maximum mutual information (MMI)[3, 4] training needs a language model (lattice), encoding all possible sentences[7, 9], that could occur in the test conditions. MMI training uses this language model (lattice) to identify the confusable segments of speech in the form of the so-called \"denominator\" state occupation statistics [7]. However, this implicitly ties the MMI trained acoustic model to a particular task-domain. MARS training does not face this constraint as it finds the confusable states at the frame level and hence does not use a language model (lattice) during training.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-117"
  },
  "srinivasan08_interspeech": {
   "authors": [
    [
     "Sundar",
     "Srinivasan"
    ],
    [
     "Tao",
     "Ma"
    ],
    [
     "Daniel",
     "May"
    ],
    [
     "Georgios",
     "Lazarou"
    ],
    [
     "Joseph",
     "Picone"
    ]
   ],
   "title": "Nonlinear mixture autoregressive hidden Markov models for speech recognition",
   "original": "i08_0960",
   "page_count": 4,
   "order": 130,
   "p1": "960",
   "pn": "963",
   "abstract": [
    "Gaussian mixture models are a very successful method for modeling the output distribution of a state in a hidden Markov model (HMM). However, this approach is limited by the assumption that the dynamics of speech features are linear and can be modeled with static features and their derivatives. In this paper, a nonlinear mixture autoregressive model is used to model state output distributions (MAR-HMM). Estimation of model parameters is extended to handle vector features. MAR-HMMs are shown to provide superior performance to comparable Gaussian mixture model-based HMMs (GMM-HMM) with lower complexity on two pilot classification tasks.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-118"
  },
  "cardinal08_interspeech": {
   "authors": [
    [
     "Patrick",
     "Cardinal"
    ],
    [
     "Pierre",
     "Dumouchel"
    ],
    [
     "Gilles",
     "Boulianne"
    ],
    [
     "Michel",
     "Comeau"
    ]
   ],
   "title": "GPU accelerated acoustic likelihood computations",
   "original": "i08_0964",
   "page_count": 4,
   "order": 131,
   "p1": "964",
   "pn": "967",
   "abstract": [
    "This paper introduces the use of Graphics Processors Unit (GPU) for computing acoustic likelihoods in a speech recognition system. In addition to their high availability, GPUs provide high computing performance at low cost. We have used a NVidia GeForce 8800GTX programmed with the CUDA (Compute Unified Device Architecture) which shows the GPU as a parallel coprocessor. The acoustic likelihoods are computed as dot products, operations for which GPUs are highly efficient. The implementation in our speech recognition system shows that GPU is 5x faster than the CPU SSE-based implementation. This improvement led to a speed up of 35% on a large vocabulary task.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-119"
  },
  "zhang08b_interspeech": {
   "authors": [
    [
     "Qingqing",
     "Zhang"
    ],
    [
     "Ta",
     "Li"
    ],
    [
     "Jielin",
     "Pan"
    ],
    [
     "Yonghong",
     "Yan"
    ]
   ],
   "title": "Nonnative speech recognition based on state-candidate bilingual model modification",
   "original": "i08_2366",
   "page_count": 4,
   "order": 132,
   "p1": "2366",
   "pn": "2369",
   "abstract": [
    "The speech recognition accuracy has been observed to decrease for nonnative speakers, especially those who are just beginning to learn foreign language or who have heavy accents. This paper presents a novel bilingual model modification approach to improve nonnative speech recognition, considering these great variations of accented pronunciations. Each state of the baseline nonnative acoustic models is modified with several candidate states from the auxiliary acoustic models, which are trained by speakers' mother language. State mapping criterion and n-best candidates are investigated based on a grammar-constrained speech recognition system. Using the state-candidate bilingual model modification approach, compared to the nonnative acoustic models which have already been well trained by adaptation technique MAP, a Relative reduction of 7.87% in Phrase Error Rate (RPhrER) was further achieved.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-120"
  },
  "schuller08b_interspeech": {
   "authors": [
    [
     "Björn",
     "Schuller"
    ],
    [
     "Xiaohua",
     "Zhang"
    ],
    [
     "Gerhard",
     "Rigoll"
    ]
   ],
   "title": "Prosodic and spectral features within segment-based acoustic modeling",
   "original": "i08_2370",
   "page_count": 4,
   "order": 133,
   "p1": "2370",
   "pn": "2373",
   "abstract": [
    "Apart from the usually employed MFCC, PLP, and energy feature information, also duration, low order formants, pitch, and center-of-gravity-based features are known to carry valuable information for phoneme recognition. This work investigates their individual performance within segment-based acoustic modeling. Also, experiments optimizing a feature space spanned by this set, exclusively, are reported, using CFSS feature space optimization and speaker adaptation. All tests are carried out with SVM on the open IFA-corpus of 47 Dutch hand-labeled phonemes with a total of 178k instances. Extensive speaker dependent vs. independent test-runs are discussed as well as four different speaking styles reaching from informal to formal: informal and retold story telling, and read aloud with fixed and variable content. Results show the potential of these rather uncommon features, as e.g. based on F3 or pitch.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-121"
  },
  "ma08_interspeech": {
   "authors": [
    [
     "Jeff",
     "Ma"
    ],
    [
     "Richard",
     "Schwartz"
    ]
   ],
   "title": "Unsupervised versus supervised training of acoustic models",
   "original": "i08_2374",
   "page_count": 4,
   "order": 134,
   "p1": "2374",
   "pn": "2377",
   "abstract": [
    "In this paper we report unsupervised training experiments we have conducted on large amounts of the English Fisher conversational telephone speech. A great amount of work has been reported on unsupervised training, but the major difference of this work is that we compared behaviors of unsupervised training with supervised training on exactly the same data. This comparison reveals surprising results. First, as the amount of training data increases, unsupervised training, even bootstrapped with a very limited amount (1 hour) of manual data, improves recognition performance faster than supervised training does, and it converges to supervised training. Second, bootstrapping unsupervised training with more manual data is not of significance if a large amount of un-transcribed data is available.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-122"
  },
  "sainath08_interspeech": {
   "authors": [
    [
     "Tara N.",
     "Sainath"
    ],
    [
     "Victor",
     "Zue"
    ]
   ],
   "title": "A comparison of broad phonetic and acoustic units for noise robust segment-based phonetic recognition",
   "original": "i08_2378",
   "page_count": 4,
   "order": 135,
   "p1": "2378",
   "pn": "2381",
   "abstract": [
    "In this paper, we compare speech recognition performance using broad phonetically- and acoustically-motivated units as a preprocessor in designing a novel noise robust landmark detection and segmentation algorithm. We introduce a cluster evaluation method to measure acoustic unit cluster quality. On the noisy TIMIT task, we find that the acoustic and phonetic segmentation approaches offer significant improvements over two baseline methods used in the SUMMIT segment-based speech recognizer, a sinusoidal model method and a spectral change approach. In addition, we find that the acoustic method has much faster computation time in stationary noises, while the phonetic approach is faster in non-stationary noise conditions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-123"
  },
  "shinozaki08_interspeech": {
   "authors": [
    [
     "Takahiro",
     "Shinozaki"
    ],
    [
     "Sadaoki",
     "Furui"
    ],
    [
     "Tatsuya",
     "Kawahara"
    ]
   ],
   "title": "Aggregated cross-validation and its efficient application to Gaussian mixture optimization",
   "original": "i08_2382",
   "page_count": 4,
   "order": 136,
   "p1": "2382",
   "pn": "2385",
   "abstract": [
    "We have previously proposed a cross-validation (CV) based Gaussian mixture optimization method that efficiently optimizes the model structure based on CV likelihood. In this study, we propose aggregated cross-validation (AgCV) that introduces a bagging-like approach in the CV framework to reinforce the model selection ability. While a single model is used in CV to evaluate a held-out subset, AgCV uses multiple models to reduce the variance in the score estimation. By integrating AgCV instead of CV in the Gaussian mixture optimization algorithm, an AgCV likelihood based Gaussian mixture optimization algorithm is obtained. The algorithm works efficiently by using sufficient statistics and can be applied to large models such as Gaussian mixture HMM. The proposed algorithm is evaluated by speech recognition experiments on oral presentations and it is shown that lower word error rates are obtained by the AgCV optimization method when compared to CV and MDL based methods.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-124"
  },
  "matton08_interspeech": {
   "authors": [
    [
     "Mike",
     "Matton"
    ],
    [
     "Dirk Van",
     "Compernolle"
    ],
    [
     "Ronald",
     "Cools"
    ]
   ],
   "title": "A minimum classification error based distance measure for template based speech recognition",
   "original": "i08_2386",
   "page_count": 4,
   "order": 137,
   "p1": "2386",
   "pn": "2389",
   "abstract": [
    "In this paper we investigate the minimum classification error (MCE) criterion for the training of distance measures for template based speech recognition. These MCE-based distance measures are illustrated with example experiments on the Wall Street Journal 5k benchmark for continuous speech recognition.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-125"
  },
  "siniscalchi08_interspeech": {
   "authors": [
    [
     "Sabato Marco",
     "Siniscalchi"
    ],
    [
     "Torbjørn",
     "Svendsen"
    ],
    [
     "Chin-Hui",
     "Lee"
    ]
   ],
   "title": "A penalized logistic regression approach to detection based phone classification",
   "original": "i08_2390",
   "page_count": 4,
   "order": 138,
   "p1": "2390",
   "pn": "2393",
   "abstract": [
    "Recently, we have proposed a detection-based speech recognizer which has two main components: a bank of phonetic feature detectors implemented with hidden Markov models (HMMs), and an event merger. Each detector generates a score that pertains to some phonetic features, e.g. voicing. The merger combines all these scores to generate phone labels. The parameters of the detectors and the merger can be optimized either separately or jointly, and we showed that penalized logistic regression machine (PLRM) is a convenient tool for joint optimization. We validated our approach on a rescoring scheme. In this work, we tackle the phone classification problem and show that high level phone accuracy can be achieved without a direct modeling of the phones when PLRM is used. We also show that better results can be obtained by increasing the number of phonetic features, and that our method outperforms phone classifiers trained either by maximum likelihood estimation, or maximum mutual information.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-126"
  },
  "abad08_interspeech": {
   "authors": [
    [
     "Alberto",
     "Abad"
    ],
    [
     "João",
     "Neto"
    ]
   ],
   "title": "Incorporating acoustical modelling of phone transitions in an hybrid ANN/HMM speech recognizer",
   "original": "i08_2394",
   "page_count": 4,
   "order": 139,
   "p1": "2394",
   "pn": "2397",
   "abstract": [
    "Speech recognition based on connectionist approaches is one of the most successful alternatives to widespread Gaussian systems. One of the main claims against hybrid recognizers is the increased complexity for context-dependent phone modelling, which is a key aspect in medium to large size vocabulary tasks. In this paper, a baseline hybrid system based on monophone recognition units is improved by incorporating acoustical modelling of phone transitions. First, a single state monophone model is extended to multiple state sub-phoneme modelling. Then, a reduced set of diphone recognition units is incorporated to model phone transitions. The proposed approach shows a 26.8% and 23.8% relative word error rate reduction compared to baseline hybrid system in two selected WSJ evaluation test sets. Additionally, improved performance compared to a reference Gaussian system based on word-internal context-dependent triphones and comparable results to cross-word triphone system are reported.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-127"
  },
  "mcdermott08_interspeech": {
   "authors": [
    [
     "Erik",
     "McDermott"
    ],
    [
     "Atsushi",
     "Nakamura"
    ]
   ],
   "title": "Flexible discriminative training based on equal error group scores obtained from an error-indexed forward-backward algorithm",
   "original": "i08_2398",
   "page_count": 4,
   "order": 140,
   "p1": "2398",
   "pn": "2401",
   "abstract": [
    "This article presents a new approach to discriminative training that uses equal error groups of word strings as the unit of weighted error modeling. The proposed approach, Minimum Group Error (MGE), is based on a novel error-indexed Forward-Backward algorithm that can be used to generate group scores efficiently over standard recognition lattices. The approach offers many possibilities for group occupancy scaling, enabling, for instance, the boosting of error groups with low occupancies. Preliminary experiments examined the new approach using both uniformly and non-uniformly scaled group scores. Results for the new approach evaluated on the Corpus of Spontaneous Japanese (CSJ) lecture speech transcription task were compared with results for standard Minimum Classification Error (MCE), Minimum Phone Error (MPE) and Maximum Mutual Information (MMI), in tandem with I-smoothing. It was found that non-uniform scaling of group scores outperformed MPE when no I-smoothing is used.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-128"
  },
  "garau08_interspeech": {
   "authors": [
    [
     "Giulia",
     "Garau"
    ],
    [
     "Steve",
     "Renals"
    ]
   ],
   "title": "Pitch adaptive features for LVCSR",
   "original": "i08_2402",
   "page_count": 4,
   "order": 141,
   "p1": "2402",
   "pn": "2405",
   "abstract": [
    "We have investigated the use of a pitch adaptive spectral representation on large vocabulary speech recognition, in conjunction with speaker normalisation techniques. We have compared the effect of a smoothed spectrogram to the pitch adaptive spectral analysis by decoupling these two components of Straight. Experiments performed on a large vocabulary meeting speech recognition task highlight the importance of combining a pitch adaptive spectral representation with a conventional fixed window spectral analysis. We found evidence that Straight pitch adaptive features are more speaker independent than conventional MFCCs without pitch adaptation, thus they also provide better performances when combined using feature combination techniques such as Heteroscedastic Linear Discriminant Analysis.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-129"
  },
  "bartels08_interspeech": {
   "authors": [
    [
     "Chris D.",
     "Bartels"
    ],
    [
     "Jeff A.",
     "Bilmes"
    ]
   ],
   "title": "Using syllable nuclei locations to improve automatic speech recognition in the presence of burst noise",
   "original": "i08_2406",
   "page_count": 4,
   "order": 142,
   "p1": "2406",
   "pn": "2409",
   "abstract": [
    "In this work we combine a conventional phone-based automatic speech recognizer with a classifier that detects syllable locations. This is done using a dynamic Bayesian network. Using oracle syllable detections we achieve a 17% relative reduction in word error rate on the 500 word task of the SVitchboard corpus. Using estimated locations we achieve a 2.1% relative reduction which is significant at the 0.02 level. The improvement in the estimated case is from reducing insertions caused by burst noise.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-130"
  },
  "hong08_interspeech": {
   "authors": [
    [
     "Hyejin",
     "Hong"
    ],
    [
     "Sunhee",
     "Kim"
    ],
    [
     "Minhwa",
     "Chung"
    ]
   ],
   "title": "Effects of allophones on the performance of Korean speech recognition",
   "original": "i08_2410",
   "page_count": 4,
   "order": 143,
   "p1": "2410",
   "pn": "2413",
   "abstract": [
    "This paper investigates the effects of allophones on the performance of Korean speech recognition systems. Along with a baseline phone-like unit (PLU) set consisting of phonemes, 31 allophone-based PLU sets are designed by systematically considering 5 major Korean allophonic constraints which can describe all the PLU sets currently used for Korean speech recognition systems. Experiments for phone, word, and continuous speech recognition are performed using the proposed PLU sets. The results show that the allophone-based PLU sets improve recognition performance compared to using a baseline phoneme-based PLU set. The performance improvement is clearly evident in phone recognition for isolated speech and in isolated word and continuous speech recognition using context independent units. As predicted, the performance improvement is less evident when context dependent (CD) units are used in the experiments, since the allophonic information is internalized in the CD units. Finally, the constraint Voicing-Lax is observed as playing a positive role compared to other constraints that are only partly influential.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-131"
  },
  "pinto08_interspeech": {
   "authors": [
    [
     "Joel",
     "Pinto"
    ],
    [
     "Hynek",
     "Hermansky"
    ]
   ],
   "title": "Combining evidence from a generative and a discriminative model in phoneme recognition",
   "original": "i08_2414",
   "page_count": 4,
   "order": 144,
   "p1": "2414",
   "pn": "2417",
   "abstract": [
    "We investigate the use of the log-likelihood of the features obtained from a generative Gaussian mixture model, and the posterior probability of phonemes from a discriminative multilayered perceptron in multi-stream combination for recognition of phonemes. Multistream combination techniques, namely early integration and late integration are used to combine the evidence from these models. By using multi-stream combination, we obtain a phoneme recognition accuracy of 74% on the standard TIMIT database, an absolute improvement of 2.5% over the single best stream.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-132"
  },
  "thambiratnam08_interspeech": {
   "authors": [
    [
     "K.",
     "Thambiratnam"
    ],
    [
     "Frank",
     "Seide"
    ]
   ],
   "title": "Fragmented context-dependent syllable acoustic models",
   "original": "i08_2418",
   "page_count": 4,
   "order": 145,
   "p1": "2418",
   "pn": "2421",
   "abstract": [
    "Though touted as an excellent candidate, past work has yet to demonstrate the value of the syllable for acoustic modeling. One reason is that critical factors such as context-dependency and model clustering are typically neglected in syllable works. This paper presents fragmented syllable models, a means to realize context-dependency for the syllable while constraining the implied explosion in training data requirements. Fragmented syllables only expose their head/tail phones as context, and thus limit the context space for triphone expansion. Furthermore, decision-tree clustering can be used to share data between parts, or fragments, of syllables, to better exploit training data for data-sparse syllables. The best resulting system achieves a 1.8% absolute (5.4% relative) reduction in WER over a baseline triphone acoustic model on a Switchboard-1 conversational telephone speech task.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-133"
  },
  "hu08_interspeech": {
   "authors": [
    [
     "Hongwei",
     "Hu"
    ],
    [
     "Martin J.",
     "Russell"
    ]
   ],
   "title": "Speech recognition using non-linear trajectories in a formant-based articulatory layer of a multiple-level segmental HMM",
   "original": "i08_2422",
   "page_count": 4,
   "order": 146,
   "p1": "2422",
   "pn": "2425",
   "abstract": [
    "This paper describes how non-linear formant trajectories, based on 'trajectory HMM' proposed by Tokuda et al., can be exploited under the framework of multiple-level segmental HMMs. In the resultant model, named a non-linear/linear multiple-level segmental HMM, speech dynamics are modeled as non-linear smooth trajectories in the formant-based intermediate layer. These formant trajectories are mapped into the acoustic layer using a set of one or more linear mappings. The N-best rescoring paradigm is employed to evaluate the performance of the non-linear formant trajectories. The rescoring results on TIMIT corpus show that the introduction of non-linear formant trajectories results in improvement on recognition phone accuracy compared with linear trajectories.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-134"
  },
  "plahl08_interspeech": {
   "authors": [
    [
     "Ch.",
     "Plahl"
    ],
    [
     "Björn",
     "Hoffmeister"
    ],
    [
     "M.-Y.",
     "Hwang"
    ],
    [
     "D.",
     "Lu"
    ],
    [
     "Georg",
     "Heigold"
    ],
    [
     "Jonas",
     "Loof"
    ],
    [
     "Ralf",
     "Schlüter"
    ],
    [
     "Hermann",
     "Ney"
    ]
   ],
   "title": "Recent improvements of the RWTH GALE Mandarin LVCSR system",
   "original": "i08_2426",
   "page_count": 4,
   "order": 147,
   "p1": "2426",
   "pn": "2429",
   "abstract": [
    "This paper describes the current improvements of the RWTH Mandarin LVCSR system. We introduce a new reduced toneme set developed at RWTH. We are using different toneme sets and pronunciation lexica. For the purpose of discriminative training we will show a fast way to transform word lattices between systems using different toneme sets and pronunciation lexica. In addition to various acoustic front-ends, the current systems use different kinds of neural network toneme posterior features. While different kinds of systems are developed, a two stage decoding framework for combining these systems is applied. We show detailed recognition results of the development cycle of the systems. Finally, two methods to integrate tonal features are compared.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-135"
  },
  "vicsi08_interspeech": {
   "authors": [
    [
     "Klára",
     "Vicsi"
    ],
    [
     "György",
     "Szaszák"
    ]
   ],
   "title": "Using prosody for the improvement of ASR - sentence modality recognition",
   "original": "i08_2877",
   "page_count": 4,
   "order": 148,
   "p1": "2877",
   "pn": "2880",
   "abstract": [
    "In the Laboratory of Speech Acoustics ASR research has been prepared, in which we were searching for the possibility to contribute to the higher linguistic processing levels of ASR - at syntactic, and semantic level - by acoustical preprocessing of the supra-segmental (prosodic) features. The subject of our current article is a semantic level processing, built on supra-segmental parameters. HMM models of modality types of sentences were built by training the recognizer with speech databases processed according to the types of modality, and a simple set of connection rules of modalities were used as linguistic model. The best recognition results were obtained, when state numbers of HMM clause type-models were 11, and each state had 2 Gaussian components. With these adjustments the accuracy of recognized types of modalities was 71 % for Hungarian, and 78% for German, even though the database was small for both languages.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-136"
  },
  "darcy08_interspeech": {
   "authors": [
    [
     "Shona",
     "D'Arcy"
    ],
    [
     "Martin J.",
     "Russell"
    ]
   ],
   "title": "Experiments with the ABI (accents of the british isles) speech corpus",
   "original": "i08_0293",
   "page_count": 4,
   "order": 149,
   "p1": "293",
   "pn": "296",
   "abstract": [
    "The ABI (Accents of the British Isles) speech corpus contains approximately 90 hours of speech from approximately 280 speakers representing 14 different regional accents of British and Irish English. ABI includes a combination of applications-oriented and linguistically-motivated material. This paper describes experiments in which the ABI corpus is used to study the effects of these regional accents on vowel formant frequencies and automatic speech recogniser performance, and to explore inter-accent variability.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-137"
  },
  "castaldo08_interspeech": {
   "authors": [
    [
     "Fabio",
     "Castaldo"
    ],
    [
     "Emanuele",
     "Dalmasso"
    ],
    [
     "Pietro",
     "Laface"
    ],
    [
     "Daniele",
     "Colibro"
    ],
    [
     "Claudio",
     "Vair"
    ]
   ],
   "title": "Politecnico di Torino system for the 2007 NIST language recognition evaluation",
   "original": "i08_0297",
   "page_count": 4,
   "order": 150,
   "p1": "297",
   "pn": "300",
   "abstract": [
    "This paper describes the system submitted by Politecnico di Torino for the 2007 NIST Language Recognition Evaluation. The system, which was among the best participants in this evaluation, is a combination of classifiers based on three acoustic models and on two sets of Parallel Phone tokenizers. It exploits several state-of-the-art techniques that have been successfully applied in recent years both in speaker and in language recognition. We illustrate the models, the classification techniques and the performance of the system components, and of their combination, in the NIST-07 close-set 30 sec General Language Recognition task. We also highlight the difficulties in setting appropriate decision thresholds whenever the training data of a language are scarce, or the test data are collected through previously unseen channels.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-138"
  },
  "hubeika08_interspeech": {
   "authors": [
    [
     "Valiantsina",
     "Hubeika"
    ],
    [
     "Lukáš",
     "Burget"
    ],
    [
     "Pavel",
     "Matějka"
    ],
    [
     "Petr",
     "Schwarz"
    ]
   ],
   "title": "Discriminative training and channel compensation for acoustic language recognition",
   "original": "i08_0301",
   "page_count": 4,
   "order": 151,
   "p1": "301",
   "pn": "304",
   "abstract": [
    "This paper describes the acoustic language recognition subsystems of Brno University of Technology (BUT) which contributed to the BUT main submission to the NIST LRE 2007. Two main techniques are employed in the subsystems discriminative training in terms of Maximum Mutual Information, and channel compensation in terms of eigenchannel adaptation in both, model and feature domain. The complementarity of the approaches is analyzed.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-139"
  },
  "wu08_interspeech": {
   "authors": [
    [
     "Tingyao",
     "Wu"
    ],
    [
     "Peter",
     "Karsmakers"
    ],
    [
     "Hugo",
     "Van hamme"
    ],
    [
     "Dirk Van",
     "Compernolle"
    ]
   ],
   "title": "Comparison of variable selection methods and classifiers for native accent identification",
   "original": "i08_0305",
   "page_count": 4,
   "order": 152,
   "p1": "305",
   "pn": "308",
   "abstract": [
    "Acoustic differences are so subtle in a native accent identification (AID) task that a brute force frame-based Gaussian Mixture Model (GMM) fails to discover the tiny distinctions [1]. Apart from the frame-based framework, in this paper we propose a vector-based speaker modeling method, to which common support vector machine (SVM) kernels can be applied. The vector-based speaker model is composed of the concatenation of the average acoustic representations of all phonemes. SVM and GMM classifiers are compared on the speaker models. Moreover, based on the observation that accents only differ in a limited number of phonemes, a variable selection framework is indispensable to select accent relevant features. We investigate a forward selection method, Analysis of Variance (ANOVA) , and a backward selection method, SVM- Recursive Feature Elimination (SVM-RFE). We find that the multiclass SVM-RFE achieves comparable performance with the ANOVA on optimally selected variable sets, while it obtains excellent performance with very few features in low dimensions. Results demonstrate the effectiveness of the proposed speaker models together with the SVM classifier both in low dimensions and in high dimensions as well as the necessity of variable selection.\n",
    "",
    "",
    "J-P. Martens, \"Improving the discrimination between native accents when recorded over different channels,\" in InterSpeech, Lisbon, Portugal, Sept 2005, pp. 2821-2824. (ISCA Archive, http://www.isca-speech.org/archive/interspeech_2005)\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-140"
  },
  "campbell08_interspeech": {
   "authors": [
    [
     "W. M.",
     "Campbell"
    ],
    [
     "Douglas E.",
     "Sturim"
    ],
    [
     "Pedro A.",
     "Torres-Carrasquillo"
    ],
    [
     "Douglas A.",
     "Reynolds"
    ]
   ],
   "title": "A comparison of subspace feature-domain methods for language recognition",
   "original": "i08_0309",
   "page_count": 4,
   "order": 153,
   "p1": "309",
   "pn": "312",
   "abstract": [
    "Compensation of cepstral features for mismatch due to dissimilar train and test conditions has been critical for good performance in many speech applications. Mismatch is typically due to variability from changes in speaker, channel, gender, and environment. Common methods for compensation include RASTA, mean and variance normalization, VTLN, and feature warping. Recently, a new class of subspace methods for model compensation have become popular in language and speaker recognition - nuisance attribute projection (NAP) and factor analysis. A feature space version of latent factor analysis has been proposed. In this work, a feature space version of NAP is presented. This new approach, fNAP, is contrasted with feature domain latent factor analysis (fLFA). Both of these methods are applied to a NIST language recognition task. Results show the viability of the new fNAP method. Also, results indicate when the different methods perform best.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-141"
  },
  "benzeghiba08_interspeech": {
   "authors": [
    [
     "Mohamed Faouzi",
     "BenZeghiba"
    ],
    [
     "Jean-Luc",
     "Gauvain"
    ],
    [
     "Lori",
     "Lamel"
    ]
   ],
   "title": "Context-dependent phone models and models adaptation for phonotactic language recognition",
   "original": "i08_0313",
   "page_count": 4,
   "order": 154,
   "p1": "313",
   "pn": "316",
   "abstract": [
    "The performance of a PPRLM language recognition system depends on the quality and the consistency of phone decoders. To improve the performance of the decoders, this paper investigates the use of context-dependent instead of context-independent phone models, and the use of CMLLR for model adaptation. This paper also discusses several improvements to the LIMSI 2007 NIST LRE system, including the use of a 4-gram language model, score calibration and fusion using the FoCal Multi-class toolkit (with large development data) and better decoding parameters such as phone insertion penalty. The improved system is evaluated on the NIST LRE-2005 and the LRE-2007 evaluation data sets. Despite its simplicity, the system achieves for the 30s condition a Cavg of 2.4% and 1.6% on these data sets, respectively.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-142"
  },
  "watson08_interspeech": {
   "authors": [
    [
     "Catherine I.",
     "Watson"
    ],
    [
     "Margaret",
     "Maclagan"
    ],
    [
     "Jeanette",
     "King"
    ],
    [
     "Ray",
     "Harlow"
    ]
   ],
   "title": "The English pronunciation of successive groups of Maori speakers",
   "original": "i08_0338",
   "page_count": 4,
   "order": 155,
   "p1": "338",
   "pn": "341",
   "abstract": [
    "We have acoustically analysed the English of four groups of Maori males, representing the English spoken by Maori over a 100 year time span. Three of these four groups had English as a second language and Maori as their first language; for the forth group the situation was reversed. We have shown that the English pronunciation of the groups for whom English is a second language, is affected by their Maori pronunciation. However we have also shown that the well documented changes in NZE over the last 100 years, can also be seen in the speech of the Maori speakers.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-143"
  },
  "cox08_interspeech": {
   "authors": [
    [
     "Felicity",
     "Cox"
    ],
    [
     "Sallyanne",
     "Palethorpe"
    ]
   ],
   "title": "Reversal of short front vowel raising in Australian English",
   "original": "i08_0342",
   "page_count": 4,
   "order": 156,
   "p1": "342",
   "pn": "345",
   "abstract": [
    "In this paper we present the results of a trend analysis comparing acoustic vowel data collected from Australian English speakers over the past 40 years. Results reveal the final stage of short front vowel raising and provide evidence for subsequent lowering as a change in progress. We argue that this result reflects the reversal of a series of vowel changes that have been in progress for over 100 years. These findings raise interesting theoretical questions about the nature of vowel shifts and challenge Bybee's [1] assertion that sound change reversals cannot occur.\n",
    "",
    "",
    "] Bybee, J., Phonology and Language Use, CUP, 2001.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-144"
  },
  "price08_interspeech": {
   "authors": [
    [
     "Jennifer",
     "Price"
    ]
   ],
   "title": "GOOSE on the move: a study of /u/-fronting in Australian news speech",
   "original": "i08_0346",
   "page_count": 1,
   "order": 157,
   "p1": "346",
   "pn": "",
   "abstract": [
    "This study is concerned with sound change over time in Australian English. The aim was to determine via acoustic analysis whether there is evidence of /u/-fronting in a speaker population of elderly and middle-aged male newsreaders. Results showed that the middle-aged group was producing /u/ further towards the front of the vowel space than the elderly group in their original recording, and that several individual speakers have also fronted their realisation of /u/ over time. This would seem to confirm a move away from the 'Cultivated' accent previously required in broadcasting towards the 'General' sociolect of AusE.\n",
    ""
   ]
  },
  "butcher08_interspeech": {
   "authors": [
    [
     "Andrew",
     "Butcher"
    ],
    [
     "Victoria",
     "Anderson"
    ]
   ],
   "title": "The vowels of Australian Aboriginal English",
   "original": "i08_0347",
   "page_count": 4,
   "order": 158,
   "p1": "347",
   "pn": "350",
   "abstract": [
    "Basilectal varieties of Australian Aboriginal English (AAE), which are heavily influenced by the indigenous substrate, may have a very restricted set of vowels compared to Standard Australian English (SAE). A comparison of the vowels of a small group of acrolectal AAE speakers with those of the standard accent suggests that even in varieties with the same set of phonemes as SAE, speakers are using a somewhat smaller phonetic vowel space. The lower boundaries of the AAE and indigenous language spaces are very similar and, whereas the SAE vowel space represents an expansion in all directions compared with the indigenous space, the AAE space represents an expansion in an 'upward' (lower F1) direction only. Within their respective spaces, the relative positions of the monophthongs are quite similar in SAE and AAE. Diphthong trajectories are also similar, except that some have shorter trajectories (more centralised second targets) in AAE. Most of the differences there are can be viewed as more conservative features in the AAE accent.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-145"
  },
  "mannell08_interspeech": {
   "authors": [
    [
     "Robert H.",
     "Mannell"
    ]
   ],
   "title": "Perception and production of /i:/, /i@/ and /e:/ in australian English",
   "original": "i08_0351",
   "page_count": 4,
   "order": 159,
   "p1": "351",
   "pn": "354",
   "abstract": [
    "The progressive offglide reduction of /I@/ and /e:/ or onglide reduction of /i:/ in Australian English is the major focus of this study. This paper examines, using synthetic speech tokens, the patterns of vowel perception of female and male speakers of Australian English in (approximately) 1990 and 2007. The relationship between production and perception in 2007 is also examined. This paper provides evidence that monophthongisation of /e:/ precedes that of /I@/. Females show a stronger pattern, than males, of offglide production for /I@/ and /e:/ (in hV context) and onglide production for /i:/ (in hVd context). Females, but not males, show a significant negative correlation between /e:/ perception and production patterns. Females also show significantly stronger degrees of /I@/ monophthong perception in hVd contexts than males, and there was evidence for a significant change in this pattern between 1990 and 2007.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-146"
  },
  "zajic08_interspeech": {
   "authors": [
    [
     "Zbyněk",
     "Zajíc"
    ],
    [
     "Lukáš",
     "Machlica"
    ],
    [
     "Aleš",
     "Padrta"
    ],
    [
     "Jan",
     "Vaněk"
    ],
    [
     "Vlasta",
     "Radová"
    ]
   ],
   "title": "An expert system in speaker verification task",
   "original": "i08_0355",
   "page_count": 4,
   "order": 160,
   "p1": "355",
   "pn": "358",
   "abstract": [
    "The article introduces an expert system for the speaker verification task. Our main purpose was to design a tool for the combination of various speaker verification systems proposed for various operating conditions. First of all, the essential ideas are explained that made us design the expert system. Next section describes the structure of a rule-based expert system and subsequently an oriented graph is proposed for the representation of the topology of the system. The expert rules exploited by the system are derived automatically from the input data and we have implemented also a certainty factor to acquire more reliable decisions. The experiments show that the proposed system has the capability to significantly improve the verification results in trials with various operating conditions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-147"
  },
  "dean08_interspeech": {
   "authors": [
    [
     "David",
     "Dean"
    ],
    [
     "Sridha",
     "Sridharan"
    ],
    [
     "Patrick",
     "Lucey"
    ]
   ],
   "title": "Cascading appearance-based features for visual speaker verification",
   "original": "i08_0359",
   "page_count": 4,
   "order": 161,
   "p1": "359",
   "pn": "362",
   "abstract": [
    "The cascading appearance-based (CAB) feature extraction technique has established itself as the state of the art in extracting dynamic visual speech features for speech recognition. In this paper, we will focus on investigating the effectiveness of this technique for the related speaker verification application. By investigating the speaker verification ability of each stage of the cascade we will demonstrate that the same steps taken to reduce static speaker and environmental information for the speech recognition application also provide similar improvements for speaker recognition. These results suggest that visual speaker recognition can improve considerable when conducted solely through a consideration of the dynamic speech information rather than the static appearance of the speaker's mouth region.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-148"
  },
  "markov08_interspeech": {
   "authors": [
    [
     "Konstantin",
     "Markov"
    ],
    [
     "Satoshi",
     "Nakamura"
    ]
   ],
   "title": "Improved novelty detection for online GMM based speaker diarization",
   "original": "i08_0363",
   "page_count": 4,
   "order": 162,
   "p1": "363",
   "pn": "366",
   "abstract": [
    "Detection of speakers which have not been seen before is an essential part of every online speaker diarization system. New speaker detection accuracy has direct impact on the overall diarization performance. In our previous system, for novelty detection we used global GMM likelihood ratio (LR) threshold. However, as the system analysis showed, the optimal threshold depends on the speaker gender as well as on the number of registered speakers. In this paper, we present the results of this analysis and the approach we have taken to solve this problem. First, we use different thresholds for male and female speakers, and second, for each gender before the thresholding we apply likelihood ratio mean and variance normalization. This greatly reduced the threshold dependency on the number of speakers and allowed to use fixed threshold for each gender. The LR distribution statistics are collected online and updated every time new likelihood ratio is calculated. Experiments on the TC-STAR database showed that compared with the previous global threshold method, the new novelty detection approach reduces the speaker diarization error rate up to 35%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-149"
  },
  "mezaache08_interspeech": {
   "authors": [
    [
     "Salah Eddine",
     "Mezaache"
    ],
    [
     "Jean-François",
     "Bonastre"
    ],
    [
     "Driss",
     "Matrouf"
    ]
   ],
   "title": "Analysis of impostor tests with high scores in NIST-SRE context",
   "original": "i08_0367",
   "page_count": 4,
   "order": 163,
   "p1": "367",
   "pn": "370",
   "abstract": [
    "In speaker recognition, performance of a system is usually estimated globally on a large set of tests, even if it is well known that some subsets of tests could show a very different behavior from the complete set. In fact, a small subset of tests could represent the main part of the reported errors. In this work, we highlight a such subset of tests, for which impostors obtain very high recognition scores. We evaluate if the problem comes from the involved speakers, from the voice excerpts or from the client model estimation technique. We also propose a strategy in order to minimize the effects of the observed phenomena on the overall performance of the system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-150"
  },
  "larcher08_interspeech": {
   "authors": [
    [
     "Anthony",
     "Larcher"
    ],
    [
     "Jean-François",
     "Bonastre"
    ],
    [
     "John S. D.",
     "Mason"
    ]
   ],
   "title": "Reinforced temporal structure information for embedded utterance-based speaker recognition",
   "original": "i08_0371",
   "page_count": 4,
   "order": 164,
   "p1": "371",
   "pn": "374",
   "abstract": [
    "Embedded speaker recognition in mobile devices could involve several ergonomic constraints and a limited amount of computing resources. Even if they have proved their efficiency in more classical contexts, GMM/UBM based systems show their limits in such situations, with good accuracy demanding a relatively large quantity of speech data, but with negligible harnessing of linguistic content. The proposed approach addresses these limitations and takes advantage from the linguistic nature of the speech material into the GMM/UBM framework by using client-customised utterances. The GMM/UBM is then reinforced with new temporal information.\n",
    "Experiments on the MyIdea database are performed when impostors know the client-utterance and also when they do not, highlighting the potential of this new approach. A relative gain up to 45% in terms of EER is achieved when impostors do not know the client utterance and performance is equivalent to the GMM/UBM baseline system in other configurations.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-151"
  },
  "gerber08_interspeech": {
   "authors": [
    [
     "Michael",
     "Gerber"
    ],
    [
     "Beat",
     "Pfister"
    ]
   ],
   "title": "Fast search for common segments in speech signals for speaker verification",
   "original": "i08_0375",
   "page_count": 4,
   "order": 165,
   "p1": "375",
   "pn": "378",
   "abstract": [
    "We pursue an approach to text-independent speaker verification which takes the decision whether two speech-signals are from the same speaker or not by matching segments that are common to both signals. A previous system using this pattern matching approach has shown good results but the used algorithm to seek for common segments was not satisfactory in terms of speed. In this paper we present an alternative algorithm which we developed to make this search faster. The approach which is based on hidden Markov models is unconstrained with respect to the size of the common segments which can be detected and it is basically language-independent. With the algorithm presented in this paper we could speed up the segment search by a factor of 5 while even improving the segments regarding their matching quality.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-152"
  },
  "chetty08_interspeech": {
   "authors": [
    [
     "Girija",
     "Chetty"
    ],
    [
     "Michael",
     "Wagner"
    ]
   ],
   "title": "Audio-visual multilevel fusion for speech and speaker recognition",
   "original": "i08_0379",
   "page_count": 4,
   "order": 166,
   "p1": "379",
   "pn": "382",
   "abstract": [
    "In this paper we propose a robust audio-visual speech-and-speaker recognition system with liveness checks based on audio-visual fusion of audio-lip motion and depth features. The liveness verification feature added here guards the system against advanced spoofing attempts such as manufactured or replayed videos. For visual features, a new tensor-based representation of lip motion features, extracted from an intensity and depth subspace of 3D video sequences, is fused used with the audio features. A multilevel fusion paradigm involving first a Support Vector Machine for speech (digit) recognition and then a Gaussian Mixture Model for speaker verification with liveness checks allowed a significant performance improvement over single-mode features. Experimental evaluation for different scenarios with AVOZES, a 3D stereovision speaking-face database, shows favourable results with recognition accuracies of 70.90% for the digit recognition task, and EERs of 5% and 3% for the speaker verification and liveness check tasks respectively.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-153"
  },
  "luque08_interspeech": {
   "authors": [
    [
     "J.",
     "Luque"
    ],
    [
     "Carlos",
     "Segura"
    ],
    [
     "Javier",
     "Hernando"
    ]
   ],
   "title": "Clustering initialization based on spatial information for speaker diarization of meetings",
   "original": "i08_0383",
   "page_count": 4,
   "order": 167,
   "p1": "383",
   "pn": "386",
   "abstract": [
    "This paper proposes an initialization for an agglomerative system applied to speaker diarization in the meeting environment. The initialization is based on a previous clustering of the temporal sequence generated by the estimation of the Time Delay of Arrival (TDOA) among pair of sensors. That initial clustering has the purpose of obtaining initial classes with speaker information from a sole speaker. The aim is to ensure the purity of the initial segments based on the position of the speakers in a meeting along time. The TDOA initialization was tested with the dataset used in the RT07s evaluation where an improvement of the diariazation error rate is obtained with respect to the classical uniform initialization. The most of the experiments show that the purity of the beginning segments leads to a better clustering on the posterior hierarchical strategy based on cepstral features.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-154"
  },
  "beskow08_interspeech": {
   "authors": [
    [
     "Jonas",
     "Beskow"
    ],
    [
     "Gösta",
     "Bruce"
    ],
    [
     "Laura",
     "Enflo"
    ],
    [
     "Björn",
     "Granström"
    ],
    [
     "Susanne",
     "Schötz"
    ]
   ],
   "title": "Recognizing and modelling regional varieties of Swedish",
   "original": "i08_0512",
   "page_count": 4,
   "order": 168,
   "p1": "512",
   "pn": "515",
   "abstract": [
    "Our recent work within the research project SIMULEKT (Simulating Intonational Varieties of Swedish) includes two approaches. The first involves a pilot perception test, used for detecting tendencies in human clustering of Swedish dialects. 30 Swedish listeners were asked to identify the geographical origin of Swedish native speakers by clicking on a map of Sweden. Results indicate for example that listeners from the south of Sweden are better at recognizing some major Swedish dialects than listeners from the central part of Sweden, which includes the capital area. The second approach concerns a method for modelling intonation using the newly developed SWING (SWedish INtonation Generator) tool, where annotated speech samples are resynthesized with rule based intonation and audio-visually analysed with regards to the major intonational varieties of Swedish. We consider both approaches important in our aim to test and further develop the Swedish prosody model.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-155"
  },
  "hajek08_interspeech": {
   "authors": [
    [
     "John",
     "Hajek"
    ],
    [
     "Mary",
     "Stevens"
    ]
   ],
   "title": "Vowel duration, compression and lengthening in stressed syllables in central and southern varieties of standard Italian",
   "original": "i08_0516",
   "page_count": 4,
   "order": 169,
   "p1": "516",
   "pn": "519",
   "abstract": [
    "This study is the first investigation of the effects of regional accent on temporal organization, specifically of vowel duration, in stressed syllables in standard Italian. We examine possible compression effects on the duration of stressed vowels according to word-position (final, penult and antepenult) and syllable type (open vs. closed) in central v. southern varieties of (standard) Italian. Our results show significant regional differences in some contexts, i.e. closed syllables, and antepenultimate position, but not in others. We consider the implications of our results for the phonological description and phonetic investigation of Italian, and the extent to which any such differences may be accounted for.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-156"
  },
  "ma08b_interspeech": {
   "authors": [
    [
     "Joan K.-Y.",
     "Ma"
    ],
    [
     "Valter",
     "Ciocca"
    ],
    [
     "Tara L.",
     "Whitehill"
    ]
   ],
   "title": "Acoustic cues for the perception of intonation in Cantonese",
   "original": "i08_0520",
   "page_count": 4,
   "order": 170,
   "p1": "520",
   "pn": "523",
   "abstract": [
    "This study aimed at identifying acoustic cues in intonation perception in Cantonese. Carriers and final syllables contrasting in intonation were selected from a previous study in which listeners identified the stimuli as either questions or statements. Acoustic analyses were used to determine the F0, duration and intensity variation of the carriers and the F0 and duration contrasts of the final syllables. Results showed that carriers consistently identified as questions or statements contrasted in F0 patterns but not in duration or intensity variations. Carriers perceived as questions with above-chance-level accuracy had a higher average F0 level than the other stimuli. At the final position of an utterance, differences were found in both F0 contour and level. The final syllables of questions showed a rising F0 contour, regardless of the original tone and a higher F0 level than their counterparts in statements. These findings are discussed in the context of 'biological codes'.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-157"
  },
  "leemann08_interspeech": {
   "authors": [
    [
     "Adrian",
     "Leemann"
    ],
    [
     "Beat",
     "Siebenhaar"
    ]
   ],
   "title": "Perception of dialectal prosody",
   "original": "i08_0524",
   "page_count": 4,
   "order": 171,
   "p1": "524",
   "pn": "527",
   "abstract": [
    "Previous studies on the perception of language prosody and dialectal prosody have shown that languages and regional dialects can be identified by prosodic cues alone. This pilot study tests this for 4 Swiss German dialects. 70 participants were presented with filtered speech material devoid of segmental cues. The filter was applied for frequencies between 250 Hz.7000 Hz. Despite this filtering, 3 of 4 dialects were recognized by the participants. Identification rates were considerably higher for dialects which present distinct prosodic features, in this case relatively slow speech rate in one instance and high pitch range in the other.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-158"
  },
  "kroos08_interspeech": {
   "authors": [
    [
     "Christian",
     "Kroos"
    ],
    [
     "Ashlie",
     "Dreves"
    ]
   ],
   "title": "Does the Mcgurk effect rely on processing time constraints?",
   "original": "i08_0528",
   "page_count": 1,
   "order": 172,
   "p1": "528",
   "pn": "",
   "abstract": [
    "This study investigated whether the McGurk effect breaks down if the duration of the target consonant is substantially extended beyond the durations occurring in normal speech. It was found that the McGurk effect persisted without diminishing in strength up to the maximum duration tested of 3.9 seconds.\n",
    ""
   ]
  },
  "kuratate08_interspeech": {
   "authors": [
    [
     "Takaaki",
     "Kuratate"
    ],
    [
     "Kathryn",
     "Ayers"
    ],
    [
     "Jeesun",
     "Kim"
    ],
    [
     "Denis",
     "Burnham"
    ]
   ],
   "title": "Exploring the Uncanny Valley Effect with talking heads",
   "original": "i08_0529",
   "page_count": 1,
   "order": 173,
   "p1": "529",
   "pn": "",
   "abstract": [
    "Here the \"Uncanny Valley\", where falling just short of perfection in creating synthetic humans exacts a large negative reaction, is explored with talking head animations focusing on naturalness in speech, face model and face motion. We discuss possible techniques to manipulate naturalness for each of these aspects. Outcomes of this method will provide insights for the choice of the degree of realism or naturalness required for various talking heads.\n",
    ""
   ]
  },
  "kvale08_interspeech": {
   "authors": [
    [
     "Knut",
     "Kvale"
    ],
    [
     "Ragnhild",
     "Halvorsrud"
    ]
   ],
   "title": "How do the elderly talk to a natural language call routing system?",
   "original": "i08_0530",
   "page_count": 4,
   "order": 174,
   "p1": "530",
   "pn": "533",
   "abstract": [
    "This paper investigates spontaneous utterances in response to a telecom IVR prompting the caller with \"Please tell me in a few words what your inquiry is about\". In responses from a typical caller base (n=29830, age-mixed) the median utterance length was 3 words. A qualitative analysis (n=74) of senior speakers (age 70+) and adult speakers (age 50 and below) revealed that seniors were significantly more verbose (median of 11 and 3 words per utterance, respectively). The seniors generally expressed themselves more formally than those below 50. Our findings suggest that some seniors confuse the IVR with a telephone answering machine.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-159"
  },
  "nishimura08_interspeech": {
   "authors": [
    [
     "Ryota",
     "Nishimura"
    ],
    [
     "Norihide",
     "Kitaoka"
    ],
    [
     "Seiichi",
     "Nakagawa"
    ]
   ],
   "title": "Analysis of relationship between impression of human-to-human conversations and prosodic change and its modeling",
   "original": "i08_0534",
   "page_count": 4,
   "order": 175,
   "p1": "534",
   "pn": "537",
   "abstract": [
    "If a dialog system could respond to a user as naturally as a human, the interaction would be smoother. Imitating human prosodic characteristics of utterances is important in computer-to-human natural interaction. To develop a cooperative/friendly spoken dialog system, we analyzed the correlation between the fundamental frequency's synchrony tendency, or overlap frequency, and subjective measures of \"liveliness\", \"familiarity\", and \"informality\" in human-to-human dialogs. We also modeled the properties of these features to realize chat-like conversations in our spoken dialog system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-160"
  },
  "saarni08_interspeech": {
   "authors": [
    [
     "Tuomo",
     "Saarni"
    ],
    [
     "Jussi",
     "Hakokari"
    ],
    [
     "Jouni",
     "Isoaho"
    ],
    [
     "Tapio",
     "Salakoski"
    ]
   ],
   "title": "Utterance-level normalization for relative articulation rate analysis",
   "original": "i08_0538",
   "page_count": 4,
   "order": 176,
   "p1": "538",
   "pn": "541",
   "abstract": [
    "This study describes a computational method for studying variation in articulation rate in a qualitatively mixed speech corpus. The method works within the scope of individual utterances, replacing each single speech sound's time information with a coefficient based on its duration relative to its environment. It can be used to generalize and determine points of acceleration and deceleration in articulation at the phone level, even when the general speaking rate varies greatly due to speaker, style, and utterance length related effects. To demonstrate the usability of the proposed method, we track observed deceleration of articulation rate (a form of final lengthening) towards the ends of utterances in a linguistically uncontrolled Finnish-language speech corpus with several speakers and styles.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-161"
  },
  "tietze08_interspeech": {
   "authors": [
    [
     "Martin",
     "Tietze"
    ],
    [
     "Vera",
     "Demberg"
    ],
    [
     "Johanna D.",
     "Moore"
    ]
   ],
   "title": "Syntactic complexity induces explicit grounding in the Maptask corpus",
   "original": "i08_0542",
   "page_count": 1,
   "order": 177,
   "p1": "542",
   "pn": "",
   "abstract": [
    "This paper provides evidence for theories of grounding and dialogue management in human conversation. For each utterance in a corpus of task-oriented dialogues, we calculated integration costs, which are based on syntactic sentence complexity. We compared the integration costs and grounding behavior under two conditions, namely face-to-face and a no-eye-contact condition. The results show that integration costs were significantly higher for explicitly grounded utterances in the no-eye-contact condition, but not in the face-to-face condition.\n",
    ""
   ]
  },
  "winterboer08_interspeech": {
   "authors": [
    [
     "Andi",
     "Winterboer"
    ],
    [
     "Johanna D.",
     "Moore"
    ],
    [
     "Fernanda",
     "Ferreira"
    ]
   ],
   "title": "Do discourse cues facilitate recall in information presentation messages?",
   "original": "i08_0543",
   "page_count": 1,
   "order": 178,
   "p1": "543",
   "pn": "",
   "abstract": [
    "This paper describes an experiment comparing the effect of two different approaches to information presentation on item recall. The results show that using discourse cues facilitates recalling the presented information.\n",
    ""
   ]
  },
  "hattori08_interspeech": {
   "authors": [
    [
     "Noriko",
     "Hattori"
    ]
   ],
   "title": "Structured heterogeneity of English stress variants",
   "original": "i08_0544",
   "page_count": 1,
   "order": 179,
   "p1": "544",
   "pn": "",
   "abstract": [
    "This is an attempt to show structured heterogeneity of English stress variants by applying a Labovian quantitative study to suprasegmental variation. Compared with the relatively frequent occurrence of segmental variation such as [IN].[In] in -ing, the low frequency of suprasegmental variation makes it difficult to observe differences of pronunciation in different styles. A detailed analysis of the recorded speech of twenty-one subjects suggests that the apparent anomalous distribution of stress variants is by no means random, and acoustic measurements reveal two kinds of strategies available to the speaker for avoiding stress clash.\n",
    ""
   ]
  },
  "sato08_interspeech": {
   "authors": [
    [
     "Shota",
     "Sato"
    ],
    [
     "Taro",
     "Kimura"
    ],
    [
     "Yasuo",
     "Horiuchi"
    ],
    [
     "Masafumi",
     "Nishida"
    ],
    [
     "Shingo",
     "Kuroiwa"
    ],
    [
     "Akira",
     "Ichikawa"
    ]
   ],
   "title": "A method for automatically estimating F0 model parameters and a speech re-synthesis tool using F0 model and STRAIGHT",
   "original": "i08_0545",
   "page_count": 4,
   "order": 180,
   "p1": "545",
   "pn": "548",
   "abstract": [
    "In this paper, we describe a speech re-synthesis tool using the fundamental frequency (F0) generation model proposed by Fujisaki et al. and STRAIGHT, designed by Kawahara, which can be used for listening experiments by modifying F0 model parameters. To create the tool, we first established a method for automatically estimating F0 model parameters by using genetic algorithms. Next, we combined the proposed method and STRAIGHT. We can change the prosody of input speech by manually modifying the F0 model parameters with the tool and evaluate the relation between human perception and F0 model parameters. We confirmed the ability of this tool to make natural speech data that have various prosodic parameters.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-162"
  },
  "stark08_interspeech": {
   "authors": [
    [
     "Anthony P.",
     "Stark"
    ],
    [
     "Kamil K.",
     "Wojcicki"
    ],
    [
     "James G.",
     "Lyons"
    ],
    [
     "Kuldip K.",
     "Paliwal"
    ],
    [
     "Kuldip K.",
     "Paliwal"
    ]
   ],
   "title": "Noise driven short-time phase spectrum compensation procedure for speech enhancement",
   "original": "i08_0549",
   "page_count": 4,
   "order": 181,
   "p1": "549",
   "pn": "552",
   "abstract": [
    "Typical speech enhancement algorithms operate on the short-time magnitude spectrum, while keeping the short-time phase spectrum unchanged for synthesis. Recently, a novel approach to speech enhancement has been proposed where the noisy magnitude spectrum is recombined with a changed phase spectrum to produce a modified complex spectrum. During synthesis the low energy components of the modified complex spectrum cancel out more than the high energy components, thus reducing background noise. In the present work, a procedure that employs noise estimates to compensate the phase spectrum for additive noise distortion is formulated. The proposed approach is objectively evaluated against several popular speech enhancement methods under various noise conditions and is shown to compare favourably.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-163"
  },
  "faubel08_interspeech": {
   "authors": [
    [
     "Friedrich",
     "Faubel"
    ],
    [
     "John",
     "McDonough"
    ],
    [
     "Dietrich",
     "Klakow"
    ]
   ],
   "title": "A phase-averaged model for the relationship between noisy speech, clean speech and noise in the log-mel domain",
   "original": "i08_0553",
   "page_count": 4,
   "order": 182,
   "p1": "553",
   "pn": "556",
   "abstract": [
    "In this work, we demonstrate that the most widely-used model for the relationship between noisy speech, clean speech and noise in the log-Mel domain is inaccurate due to its disregard of the phase. Moreover, we show how a more exact model can be derived by averaging over the phase in the log-Mel domain, and how this can profitably be applied to particle filter based sequential noise compensation. Experimental results confirm the superiority of the phase-averaged model for both clean speech estimation in general and the particle filter in particular. Reductions in word error rate of up to 17% relative were obtained on a large vocabulary task.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-164"
  },
  "brouckxon08_interspeech": {
   "authors": [
    [
     "Henk",
     "Brouckxon"
    ],
    [
     "Werner",
     "Verhelst"
    ],
    [
     "Bart De",
     "Schuymer"
    ]
   ],
   "title": "Time and frequency dependent amplification for speech intelligibility enhancement in noisy environments",
   "original": "i08_0557",
   "page_count": 4,
   "order": 183,
   "p1": "557",
   "pn": "560",
   "abstract": [
    "When speech is presented through loudspeakers in a noisy environment, the background noise can significantly decrease speech intelligibility. Because the amplitude and spectrum of the background noise can vary over time (and because high loudness levels are to be avoided for listener comfort), choosing proper speech equalization and master gain settings for a public address system can be a difficult task. In this paper, we propose an adaptive digital signal processing algorithm that applies a frequency and time dependent gain strategy to the speech signal in order to enhance its intelligibility in noise with a minimal increase of the overall sound energy level. An alternative version of the system can also be used to maximise speech intelligibility without increasing the overall energy level of the signal. The proposed algorithm makes use of the psycho-acoustic masking properties of the human hearing system and relies on the importance of the formant information for speech intelligibility.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-165"
  },
  "mohammadi08_interspeech": {
   "authors": [
    [
     "Mahdi",
     "Mohammadi"
    ],
    [
     "Behzad",
     "Zamani"
    ],
    [
     "Babak",
     "Nasersharif"
    ],
    [
     "Mohsen",
     "Rahmani"
    ],
    [
     "Ahmad",
     "Akbari"
    ]
   ],
   "title": "A wavelet based speech enhancement method using noise classification and shaping",
   "original": "i08_0561",
   "page_count": 4,
   "order": 184,
   "p1": "561",
   "pn": "564",
   "abstract": [
    "Speech enhancement systems performing in Fourier or wavelet domain usually generate musical noise and distortion. It is possible to reduce musical noise and speech distortion by shaping residual noise. In this paper, we propose to implement a noise shaping method for a wavelet based noise reduction system. For noise shaping, we propose a noise classification method based on spectral shape of input noise. Using this classification method, we transform input noise to another noise which is more acceptable from listening point of view. Objective and subjective test results show that using noise shaping method; we obtain less distortion in speech signal in comparison to basic wavelet noise reduction system. Furthermore, listening test results illustrate that background shaped noise is less annoying.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-166"
  },
  "alam08b_interspeech": {
   "authors": [
    [
     "Md. Jahangir",
     "Alam"
    ],
    [
     "Douglas",
     "O'Shaughnessy"
    ],
    [
     "Sid-Ahmed",
     "Selouani"
    ]
   ],
   "title": "Speech enhancement based on novel two-step a priori SNR estimators",
   "original": "i08_0565",
   "page_count": 4,
   "order": 185,
   "p1": "565",
   "pn": "568",
   "abstract": [
    "A widely used method to determine the a priori SNR from noisy speech is the decision directed (DD) approach, but the a priori SNR follows the a posteriori SNR with a delay of one frame in speech frames. As a consequence, the performance of the noise reduction system degrades. In order to overcome this artifact, we propose three computationally simple and efficient two-step methods based on the minimum mean square error (MMSE), the maximum a posteriori (MAP) and the joint MAP criteria for the estimation of the a priori SNR for speech enhancement. The proposed methods avoid the delay problem while keeping the advantages of the DD method. The performance of the proposed a priori SNR estimation methods are evaluated and compared with the conventional DD method by extensive objective quality measures and yield better results than the DD approach-based speech enhancement system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-167"
  },
  "du08_interspeech": {
   "authors": [
    [
     "Jun",
     "Du"
    ],
    [
     "Qiang",
     "Huo"
    ]
   ],
   "title": "A speech enhancement approach using piecewise linear approximation of an explicit model of environmental distortions",
   "original": "i08_0569",
   "page_count": 4,
   "order": 186,
   "p1": "569",
   "pn": "572",
   "abstract": [
    "This paper presents a speech enhancement approach derived by using a piecewise linear approximation (PLA) of an explicit model of environmental distortions. PLA is a generalization of two traditional approaches, namely vector Taylor series (VTS) and MAX approximations. Formulations are described for both maximum likelihood (ML) estimation of noise model parameters and minimum mean-squared error (MMSE) estimation of clean speech. Evaluation experiments are conducted to enhance speech signals corrupted by several types of additive noises. Compared to the traditional MAX-approximation based approach, our PLA-based speech enhancement approach achieves better performance in terms of two objective quality measures, namely segmental SNR and log-spectral distortion.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-168"
  },
  "ling08_interspeech": {
   "authors": [
    [
     "Zhen-Hua",
     "Ling"
    ],
    [
     "Korin",
     "Richmond"
    ],
    [
     "Junichi",
     "Yamagishi"
    ],
    [
     "Ren-Hua",
     "Wang"
    ]
   ],
   "title": "Articulatory control of HMM-based parametric speech synthesis driven by phonetic knowledge",
   "original": "i08_0573",
   "page_count": 4,
   "order": 187,
   "p1": "573",
   "pn": "576",
   "abstract": [
    "This paper presents a method to control the characteristics of synthetic speech flexibly by integrating articulatory features into a Hidden Markov Model (HMM)-based parametric speech synthesis system. In contrast to model adaptation and interpolation approaches for speaking style control, this method is driven by phonetic knowledge, and target speech samples are not required. The joint distribution of parallel acoustic and articulatory features considering cross-stream feature dependency is estimated. At synthesis time, acoustic and articulatory features are generated simultaneously based on the maximum-likelihood criterion. The synthetic speech can be controlled flexibly by modifying the generated articulatory features according to arbitrary phonetic rules in the parameter generation process. Our experiments show that the proposed method is effective in both changing the overall character of synthesized speech and in controlling the quality of a specific vowel.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-169"
  },
  "wu08b_interspeech": {
   "authors": [
    [
     "Yi-Jian",
     "Wu"
    ],
    [
     "Keiichi",
     "Tokuda"
    ]
   ],
   "title": "Minimum generation error training with direct log spectral distortion on LSPs for HMM-based speech synthesis",
   "original": "i08_0577",
   "page_count": 4,
   "order": 188,
   "p1": "577",
   "pn": "580",
   "abstract": [
    "A minimum generation error (MGE) criterion had been proposed to solve the issues related to maximum likelihood (ML) based HMM training in HMM-based speech synthesis. In this paper, we improve the MGE criterion by imposing a log spectral distortion (LSD) instead of the Euclidean distance to define the generation error between the original and generated line spectral pair (LSP) coefficients. Moreover, we investigate the effect of different sampling strategies to calculate the integration of the LSD function. From the experimental results, using the LSDs calculated by sampling at LSPs achieved the best performance, and the quality of synthesized speech after the MGE-LSD training was improved over the original MGE training.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-170"
  },
  "yamagishi08_interspeech": {
   "authors": [
    [
     "Junichi",
     "Yamagishi"
    ],
    [
     "Zhen-Hua",
     "Ling"
    ],
    [
     "Simon",
     "King"
    ]
   ],
   "title": "Robustness of HMM-based speech synthesis",
   "original": "i08_0581",
   "page_count": 4,
   "order": 189,
   "p1": "581",
   "pn": "584",
   "abstract": [
    "As speech synthesis techniques become more advanced, we are able to consider building high-quality voices from data collected outside the usual highly-controlled recording studio environment. This presents new challenges that are not present in conventional text-to-speech synthesis: the available speech data are not perfectly clean, the recording conditions are not consistent, and/or the phonetic balance of the material is not ideal. Although a clear picture of the performance of various speech synthesis techniques (e.g., concatenative, HMM-based or hybrid) under good conditions is provided by the Blizzard Challenge, it is not well understood how robust these algorithms are to less favourable conditions. In this paper, we analyse the performance of several speech synthesis methods under such conditions. This is, as far as we know, a new research topic: \"Robust speech synthesis.\" As a consequence of our investigations, we propose a new robust training method for the HMM-based speech synthesis in for use with speech data collected in unfavourable conditions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-171"
  },
  "conkie08_interspeech": {
   "authors": [
    [
     "Alistair",
     "Conkie"
    ],
    [
     "Ann",
     "Syrdal"
    ],
    [
     "Yeon-Jun",
     "Kim"
    ],
    [
     "Mark",
     "Beutnagel"
    ]
   ],
   "title": "Improving preselection in unit selection synthesis",
   "original": "i08_0585",
   "page_count": 4,
   "order": 190,
   "p1": "585",
   "pn": "588",
   "abstract": [
    "Unit selection synthesis is a method of selecting and concatenating speech segments from a large single-speaker audio database to synthesize utterances. Selection is based on assigning target and concatenation costs to units and then finding a lowest cost sequence of units that will synthesize a given utterance. In order to synthesize efficiently, it is necessary to limit the number of units considered in the unit selection cost network, a part of the process called preselection. This paper examines the role of preselection in unit selection synthesis. We refine the existing process of preselection by adding multiple phone sets to the list of features considered. We present experimental results that demonstrate better database usage and significantly increased synthesis quality using this new method.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-172"
  },
  "ding08c_interspeech": {
   "authors": [
    [
     "Feng",
     "Ding"
    ],
    [
     "Jani",
     "Nurminen"
    ],
    [
     "Jilei",
     "Tian"
    ]
   ],
   "title": "Efficient join cost computation for unit selection based TTS systems",
   "original": "i08_0589",
   "page_count": 4,
   "order": 191,
   "p1": "589",
   "pn": "592",
   "abstract": [
    "A new efficient join cost calculation technique for unit selection based synthesis is proposed. The acoustic features representing the spectral content at the unit boundaries are encoded using multi-stage vector quantization. After applying pseudo-gray coding, the join costs are directly approximated based on the stage-wise codebook indices. As a result, both the memory requirement and the computation complexity are effectively reduced at the same time, making the technique especially suitable for embedded text-to-speech systems. Experiments are carried out comparing the proposed scheme with the original baseline technique that operates in a lossless manner using the uncompressed acoustic data and similarity measurement. Based on the experimental findings, the use of the proposed technique seems to perfectly maintain the speech quality despite the considerable reduction in complexity and memory usage.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-173"
  },
  "yanagisawa08_interspeech": {
   "authors": [
    [
     "Kayoko",
     "Yanagisawa"
    ],
    [
     "Mark",
     "Huckvale"
    ]
   ],
   "title": "A phonetic assessment of cross-language voice conversion",
   "original": "i08_0593",
   "page_count": 4,
   "order": 192,
   "p1": "593",
   "pn": "596",
   "abstract": [
    "Cross-language voice conversion maps the speech of speaker S1 in language L1 to the voice of speaker S2 using knowledge only of how S2 speaks a different language L2. This mapping is usually performed using speech material from S1 and S2 that has been deemed \"equivalent\" in either acoustic or phonetic terms. This study investigates the issue of equivalence in more detail, and contrasts the performance of a voice conversion system operating in both mono-lingual and cross-lingual modes using Japanese and English. We show that voice conversion impacts the intelligibility of the converted speech, but to a significantly greater degree for cross-language conversion. A phonetic comparison of the monolingual and cross-language converted speech suggests that consonantal information is degraded in both conditions, but vowel information is degraded more in the cross-language condition.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-174"
  },
  "pollet08_interspeech": {
   "authors": [
    [
     "Vincent",
     "Pollet"
    ],
    [
     "Andrew",
     "Breen"
    ]
   ],
   "title": "Synthesis by generation and concatenation of multiform segments",
   "original": "i08_1825",
   "page_count": 4,
   "order": 193,
   "p1": "1825",
   "pn": "1828",
   "abstract": [
    "Machine generated speech can be produced in different ways however there are two basic methods for synthesizing speech in widespread use. One method generates speech from models, while the other method concatenates pre-stored speech segments. This paper presents a speech synthesis technique where these two basic synthesis methods are combined in a statistical framework. Synthetic speech is constructed by generation and concatenation of so-called \"multiform segments\". Multiform segments are different speech signal representations; synthesis models, templates and synthesis models augmented with template information. An evaluation of the multiform segment synthesis technique shows improvements over traditional concatenative methods of synthesis.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-175"
  },
  "cabral08_interspeech": {
   "authors": [
    [
     "João P.",
     "Cabral"
    ],
    [
     "Steve",
     "Renals"
    ],
    [
     "Korin",
     "Richmond"
    ],
    [
     "Junichi",
     "Yamagishi"
    ]
   ],
   "title": "Glottal spectral separation for parametric speech synthesis",
   "original": "i08_1829",
   "page_count": 4,
   "order": 194,
   "p1": "1829",
   "pn": "1832",
   "abstract": [
    "The great advantage of using a glottal source model in parametric speech synthesis is the degree of parametric flexibility it gives to transform and model aspects of voice quality and speaker identity. However, few studies have addressed how the glottal source affects the quality of synthetic speech.\n",
    "Here, we have developed the Glottal Spectral Separation (GSS) method which consists of separating the glottal source effects from the spectral envelope of the speech. It enables us to compare the LF-model with the simple impulse excitation, using the same spectral envelope to synthesize speech. The results of a perceptual evaluation showed that the LF-model clearly outperformed the impulse. The GSS method was also used to successfully transform a modal voice into a breathy or tense voice, by modifying the LF-parameters.\n",
    "The proposed technique could be used to improve the speech quality and source parametrization of HMM-based speech synthesizers, which use an impulse excitation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-176"
  },
  "kominek08_interspeech": {
   "authors": [
    [
     "John",
     "Kominek"
    ],
    [
     "Sameer",
     "Badaskar"
    ],
    [
     "Tanja",
     "Schultz"
    ],
    [
     "Alan W.",
     "Black"
    ]
   ],
   "title": "Improving speech systems built from very little data",
   "original": "i08_1833",
   "page_count": 4,
   "order": 195,
   "p1": "1833",
   "pn": "1836",
   "abstract": [
    "This paper studies two ways for helping non-specialist users develop speech systems from limited data for new languages. Focused web re-crawling finds additional examples of text matching the domain as specified by the user. This improves the language model and cuts word error rate nearly in half. Iterative voice building with interleaved lexicon construction uses the voice from a previous iteration to help construct an improved voice. 4.5 hours of the user's time reduces transcription error rate from 32% to 4%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-177"
  },
  "saito08_interspeech": {
   "authors": [
    [
     "Daisuke",
     "Saito"
    ],
    [
     "Satoshi",
     "Asakawa"
    ],
    [
     "Nobuaki",
     "Minematsu"
    ],
    [
     "Keikichi",
     "Hirose"
    ]
   ],
   "title": "Structure to speech conversion - speech generation based on infant-like vocal imitation",
   "original": "i08_1837",
   "page_count": 4,
   "order": 196,
   "p1": "1837",
   "pn": "1840",
   "abstract": [
    "This paper proposes a new framework of speech generation by imitating \"infants' vocal imitation\". Most of the speech synthesizers take a phoneme sequence as input and generate speech by converting each of the phonemes into a sound sequentially. In other words, they simulate a human process of reading text out. However, infants usually acquire speech generation ability without text or phoneme sequences. Since their phonemic awareness is very immature, they can hardly decompose a word utterance into a sequence of phones. In this situation, as developmental psychology states, infants acquire the holistic sound pattern of words from the utterances of their parents, called word Gestalt, and they reproduce it with their vocal tubes. This behavior is called vocal imitation. In our previous studies, the word Gestalt was defined physically and a method of extracting it from an utterance was proposed and used successfully for ASR and CALL. In this paper, a method of converting the word Gestalt back to speech is proposed and evaluated. Unlike a reading machine, our proposal simulates infants' vocal imitation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-178"
  },
  "tiomkin08_interspeech": {
   "authors": [
    [
     "Stas",
     "Tiomkin"
    ],
    [
     "David",
     "Malah"
    ]
   ],
   "title": "Statistical text-to-speech synthesis with improved dynamics",
   "original": "i08_1841",
   "page_count": 4,
   "order": 197,
   "p1": "1841",
   "pn": "1844",
   "abstract": [
    "In statistical TTS systems (STTS), speech features dynamics is modeled by first- and second-order feature frame differences, which, typically, do not satisfactorily represent frame to frame feature dynamics present in natural speech. The reduced dynamics results in over smoothing of speech features, often sounding as muffled synthesized speech. To improve feature dynamics a Global Variance approach has been suggested. However, it is computationally complex. We propose a different approach for modeling feature dynamics based on applying the DFT to the whole set of feature frames representing a phoneme. In the transform domain the inter-frame feature dynamics is then expressed in terms of inter-harmonic content, which can be modified to statistically match the dynamics of natural speech. To synthesize a whole utterance we propose a method for smoothly combining the enhanced-dynamics phonemes, which improves synthesized speech quality of STTS with similar complexity to conventional STTS.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-179"
  },
  "webster08_interspeech": {
   "authors": [
    [
     "Gabriel",
     "Webster"
    ],
    [
     "Norbert",
     "Braunschweiler"
    ]
   ],
   "title": "An evaluation of non-standard features for grapheme-to-phoneme conversion",
   "original": "i08_1845",
   "page_count": 4,
   "order": 198,
   "p1": "1845",
   "pn": "1848",
   "abstract": [
    "Machine learning methods for grapheme-to-phoneme (G2P) conversion are popular, but the features used in the literature are most often simply a window of context letters, despite the availability of other features. In this paper, a set of features beyond the sevenletter window, termed non-standard features, are systematically evaluated for American English, using decision trees. The results show that adding non-standard features to a seven-letter window gives clear improvements for English, with the most important features being the previous three phone sequences predicted, an initial prediction of lexical stress location, and a window of vowel letters around the current letter.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-180"
  },
  "agiomyrgiannakis08_interspeech": {
   "authors": [
    [
     "Yannis",
     "Agiomyrgiannakis"
    ],
    [
     "Olivier",
     "Rosec"
    ]
   ],
   "title": "Towards flexible speech coding for speech synthesis: an LF + modulated noise vocoder",
   "original": "i08_1849",
   "page_count": 4,
   "order": 199,
   "p1": "1849",
   "pn": "1852",
   "abstract": [
    "This paper presents an ARX-LF-based model of speech that is amenable to low-bit-rate quantization and speech modifications directly at the parametric domain. The new model successfully addresses the non-deterministic part of voiced speech by modulating noise with the glottal flow, while unvoiced speech and transients are synthesized by modulating noise with a signal-derived time envelope. The presented work is essentially a high-quality vocoder that can be used for low complexity coding/synthesis/modification of speech suitable for embedded text-to-speech applications.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-181"
  },
  "silen08_interspeech": {
   "authors": [
    [
     "Hanna",
     "Silen"
    ],
    [
     "Elina",
     "Helander"
    ],
    [
     "Jani",
     "Nurminen"
    ],
    [
     "Moncef",
     "Gabbouj"
    ]
   ],
   "title": "Evaluation of Finnish unit selection and HMM-based speech synthesis",
   "original": "i08_1853",
   "page_count": 4,
   "order": 200,
   "p1": "1853",
   "pn": "1856",
   "abstract": [
    "Unit selection and hidden Markov model (HMM) based synthesis have become the dominant techniques in text-to-speech (TTS) research. In this work, we combine HMM-based signal generation with the front end originally designed for unit selection based Finnish TTS and we evaluate the prosody of the output generated by the two synthesis techniques using the same speech database. Furthermore, we study the effect that the training set size has for the prosody and intelligibility in HMM-based synthesis. The results indicate that the HMM-based approach is capable of providing better prosody than unit selection even if the training set size is severely limited. The size of the training set, however, affects the prosodic quality and intelligibility of the HMM-based synthesizer.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-182"
  },
  "theobald08_interspeech": {
   "authors": [
    [
     "Barry-John",
     "Theobald"
    ],
    [
     "Nicholas",
     "Wilkinson"
    ]
   ],
   "title": "A probabilistic trajectory synthesis system for synthesising visual speech",
   "original": "i08_1857",
   "page_count": 4,
   "order": 201,
   "p1": "1857",
   "pn": "1860",
   "abstract": [
    "We describe an unsupervised probabilistic approach for synthesising visual speech from audio. Acoustic features representing a training corpus are clustered and the probability density function (PDF) of each cluster is modelled as a Gaussian mixture model (GMM). A visual target in the form of a short-term parameter trajectory is generated for each cluster. Synthesis involves combining the cluster targets based on the likelihood of novel acoustic feature vectors, then cross-blending neighbouring regions of the synthesised short-term trajectories. The advantage of our approach is coarticulation effects are explicitly captured by the mapping. The influence of cluster targets naturally increase and decrease with the likelihood of the acoustic feature vectors.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-183"
  },
  "cadic08_interspeech": {
   "authors": [
    [
     "Didier",
     "Cadic"
    ],
    [
     "Lionel",
     "Segalen"
    ]
   ],
   "title": "Paralinguistic elements in speech synthesis",
   "original": "i08_1861",
   "page_count": 4,
   "order": 202,
   "p1": "1861",
   "pn": "1864",
   "abstract": [
    "Corpus based text-to-speech systems currently produce very natural synthetic sentences, though limited to a neutral inexpressive speaking style. Paralinguistic elements are some of the expressive features one would most like to introduce. In this paper, we describe a new method for introducing laughter and hesitation in synthetic speech. Thanks to a small dedicated acoustic database, this method can successfully render transitions between speech and paralinguistic elements. We validate it here for French but extension to other languages should be straightforward.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-184"
  },
  "raghavendra08_interspeech": {
   "authors": [
    [
     "E Veera",
     "Raghavendra"
    ],
    [
     "B.",
     "Yegnanarayana"
    ],
    [
     "Alan W.",
     "Black"
    ],
    [
     "Kishore",
     "Prahallad"
    ]
   ],
   "title": "Building sleek synthesizers for multi-lingual screen reader",
   "original": "i08_1865",
   "page_count": 4,
   "order": 203,
   "p1": "1865",
   "pn": "1868",
   "abstract": [
    "In this paper, we are investigating the unit size: syllable, halfphone and quarter-phone to be used for speech synthesis in multi-lingual screen reader in phonetic languages such as Telugu and non-phonetic language English. Perceptual studies show that syllable-level unit performs better for Telugu and half-phone units perform better for English. While syllable based synthesizers produce better sounding speech, the coverage of all syllables is a non-trivial issue. We address the issue of coverage of syllables through approximate matching of syllable and show that such approximation produces intelligible and better quality speech than diphone units. In this paper, we also propose a hybrid synthesizer within the framework of unit selection and also show that the hybrid synthesizer built from pruned database performs as well as hybrid synthesizer built from unpruned database.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-185"
  },
  "king08_interspeech": {
   "authors": [
    [
     "Simon",
     "King"
    ],
    [
     "Keiichi",
     "Tokuda"
    ],
    [
     "Heiga",
     "Zen"
    ],
    [
     "Junichi",
     "Yamagishi"
    ]
   ],
   "title": "Unsupervised adaptation for HMM-based speech synthesis",
   "original": "i08_1869",
   "page_count": 4,
   "order": 204,
   "p1": "1869",
   "pn": "1872",
   "abstract": [
    "It is now possible to synthesise speech using HMMs with a comparable quality to unit-selection techniques. Generating speech from a model has many potential advantages over concatenating waveforms. The most exciting is model adaptation. It has been shown that supervised speaker adaptation can yield high-quality synthetic voices with an order of magnitude less data than required to train a speaker-dependent model or to build a basic unit-selection system. Such supervised methods require labelled adaptation data for the target speaker. In this paper, we introduce a method capable of unsupervised adaptation, using only speech from the target speaker without any labelling.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-186"
  },
  "strom08_interspeech": {
   "authors": [
    [
     "Volker",
     "Strom"
    ],
    [
     "Simon",
     "King"
    ]
   ],
   "title": "Investigating festival's target cost function using perceptual experiments",
   "original": "i08_1873",
   "page_count": 4,
   "order": 205,
   "p1": "1873",
   "pn": "1876",
   "abstract": [
    "We describe an investigation of the target cost used in the Festival unit selection speech synthesis system [1]. Our ultimate goal is to automatically learn a perceptually optimal target cost function. In this study, we investigated the behaviour of the target cost for one segment type. The target cost is based on counting the mismatches in several context features. A carrier sentence (\"My name is Roger\") was synthesised using all 147,820 possible combinations of the diphones /n_ei/ and /ei_m/. 92 representative versions were selected and presented to listeners as 460 pairwise comparisons. The listeners' preference votes were used to analyse the behaviour of the target cost, with respect to the values of its component linguistic context features.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-187"
  },
  "neubarth08_interspeech": {
   "authors": [
    [
     "Friedrich",
     "Neubarth"
    ],
    [
     "Michael",
     "Pucher"
    ],
    [
     "Christian",
     "Kranzler"
    ]
   ],
   "title": "Modeling Austrian dialect varieties for TTS",
   "original": "i08_1877",
   "page_count": 4,
   "order": 206,
   "p1": "1877",
   "pn": "1880",
   "abstract": [
    "In this paper we discuss certain strategies for building adapted TTS systems for dialectal or regional varieties from a given standard source. The basic question is how much re-coding is necessary for a given transfer and to what extent it is possible to rely on the speech data alone. It will turn out that there are ambiguities that cannot be resolved without a certain amount of linguistic engineering. For exemplification we present two experiments dealing with Austrian standard German and Viennese dialect on the level of phonetic lexicon and orthography.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-188"
  },
  "raitio08_interspeech": {
   "authors": [
    [
     "Tuomo",
     "Raitio"
    ],
    [
     "Antti",
     "Suni"
    ],
    [
     "Hannu",
     "Pulakka"
    ],
    [
     "Martti",
     "Vainio"
    ],
    [
     "Paavo",
     "Alku"
    ]
   ],
   "title": "HMM-based Finnish text-to-speech system utilizing glottal inverse filtering",
   "original": "i08_1881",
   "page_count": 4,
   "order": 207,
   "p1": "1881",
   "pn": "1884",
   "abstract": [
    "This paper describes an HMM-based speech synthesis system that utilizes glottal inverse filtering for generating natural sounding synthetic speech. In the proposed system, speech is first parametrized into spectral and excitation features using a glottal inverse filtering based method. The parameters are fed into an HMM system for training and then generated from the trained HMM according to text input. Glottal flow pulses extracted from real speech are used as a voice source, and the voice source is further modified according to the all-pole model parameters generated by the HMM. Preliminary experiments show that the proposed system is capable of generating natural sounding speech, and the quality is clearly better compared to a system utilizing a conventional impulse train excitation model.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-189"
  },
  "sarkar08_interspeech": {
   "authors": [
    [
     "Tanuja",
     "Sarkar"
    ],
    [
     "Sachin",
     "Joshi"
    ],
    [
     "Sathish Chandra",
     "Pammi"
    ],
    [
     "Kishore",
     "Prahallad"
    ]
   ],
   "title": "LTS using decision forest of regression trees and neural networks",
   "original": "i08_1885",
   "page_count": 4,
   "order": 208,
   "p1": "1885",
   "pn": "1888",
   "abstract": [
    "Letter-to-sound (LTS) rules play a vital role in building a speech synthesis system. In this paper, we apply various Machine Learning approaches like Classification and Regression Trees (CART), Decision Forest, forest of Artificial Neural Network (ANN) and Auto Associative Neural Networks (AANN) for LTS rules. We used these techniques mainly for Schwa deletion in Hindi. We empirically show that the LTS using Decision Forest and Forest of ANNs outperforms the previous CART and normal ANN approaches respectively, and the non discriminative learning technique of AANN could not capture the LTS rules as efficiently as discriminative techniques. We explore use of syllabic features, namely, syllabic structure, onset of the syllable, number of syllables and place of Schwa along with primary contextual features. The results showed that use of these features leads to good performance. The Decision Forest and forest of ANNs approaches yielded phone accuracy of 92.86% and 93.18% respectively using the newly incorporated features for Hindi LTS.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-190"
  },
  "rustullet08_interspeech": {
   "authors": [
    [
     "Silvia",
     "Rustullet"
    ],
    [
     "Daniela",
     "Braga"
    ],
    [
     "João",
     "Nogueira"
    ],
    [
     "Miguel",
     "Sales Dias"
    ]
   ],
   "title": "Automatic word stress marking and syllabification for Catalan TTS",
   "original": "i08_1889",
   "page_count": 4,
   "order": 209,
   "p1": "1889",
   "pn": "1892",
   "abstract": [
    "Stress and syllabification are essential attributes for several components in text-to speech (TTS) systems. They are responsible for improving grapheme-to-phoneme conversion rules and for enhancing the synthetic intelligibility, since stress and syllable are key units in prosody prediction. This paper presents three linguistically rule-based automatic algorithms for Catalan text-to-speech conversion: a word stress marker, an orthographic syllabification algorithm and a phonological syllabification algorithm. The systems were implemented and tested. The results gave rise to the following word accuracy rates: 100% for the stress marker algorithm, 99.7% for the orthographic syllabification algorithm and 99.8% for the phonological syllabification algorithm.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-191"
  },
  "wollmer08_interspeech": {
   "authors": [
    [
     "Martin",
     "Wöllmer"
    ],
    [
     "Florian",
     "Eyben"
    ],
    [
     "Stephan",
     "Reiter"
    ],
    [
     "Björn",
     "Schuller"
    ],
    [
     "Cate",
     "Cox"
    ],
    [
     "Ellen",
     "Douglas-Cowie"
    ],
    [
     "Roddy",
     "Cowie"
    ]
   ],
   "title": "Abandoning emotion classes - towards continuous emotion recognition with modelling of long-range dependencies",
   "original": "i08_0597",
   "page_count": 4,
   "order": 210,
   "p1": "597",
   "pn": "600",
   "abstract": [
    "Class based emotion recognition from speech, as performed in most works up to now, entails many restrictions for practical applications. Human emotion is a continuum and an automatic emotion recognition system must be able to recognise it as such. We present a novel approach for continuous emotion recognition based on Long Short-Term Memory Recurrent Neural Networks which include modelling of long-range dependencies between observations and thus outperform techniques like Support-Vector Regression. Transferring the innovative concept of additionally modelling emotional history to the classification of discrete levels for the emotional dimensions \"valence\" and \"activation\" we also apply Conditional Random Fields which prevail over the commonly used Support-Vector Machines. Experiments conducted on data that was recorded while humans interacted with a Sensitive Artificial Listener prove that for activation the derived classifiers perform as well as human annotators.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-192"
  },
  "seppi08_interspeech": {
   "authors": [
    [
     "Dino",
     "Seppi"
    ],
    [
     "Anton",
     "Batliner"
    ],
    [
     "Björn",
     "Schuller"
    ],
    [
     "Stefan",
     "Steidl"
    ],
    [
     "Thurid",
     "Vogt"
    ],
    [
     "Johannes",
     "Wagner"
    ],
    [
     "Laurence",
     "Devillers"
    ],
    [
     "Laurence",
     "Vidrascu"
    ],
    [
     "Noam",
     "Amir"
    ],
    [
     "Vered",
     "Aharonson"
    ]
   ],
   "title": "Patterns, prototypes, performance: classifying emotional user states",
   "original": "i08_0601",
   "page_count": 4,
   "order": 211,
   "p1": "601",
   "pn": "604",
   "abstract": [
    "In this paper, we report on classification results for emotional user states (4 classes, German database of children interacting with a pet robot). Starting with 5 emotion labels per word, we obtained chunks with different degrees of prototypicality. Six sites computed acoustic and linguistic features independently from each other. A total of 4232 features were pooled together and grouped into 10 low level descriptor types. For each of these groups separately and for all taken together, classification results using Support Vector Machines are reported for 150 features each with the highest individual Information Gain Ratio, for a scale of prototypicality. With both acoustic and linguistic features, we obtained a relative improvement of up to 27.6%, going from low to higher prototypicality.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-193"
  },
  "he08_interspeech": {
   "authors": [
    [
     "Ling",
     "He"
    ],
    [
     "Margaret",
     "Lech"
    ],
    [
     "Sheeraz",
     "Memon"
    ],
    [
     "Nicholas",
     "Allen"
    ]
   ],
   "title": "Recognition of stress in speech using wavelet analysis and Teager energy operator",
   "original": "i08_0605",
   "page_count": 4,
   "order": 212,
   "p1": "605",
   "pn": "608",
   "abstract": [
    "The automatic recognition and classification of speech under stress has applications in behavioural and mental health sciences, human to machine communication and robotics. The majority of recent studies are based on a linear model of the speech signal. In this study, the nonlinear Teager Energy Operator (TEO) analysis was used to derive the classification features. Moreover, the TEO analysis was combined with the Discrete Wavelet Transform, Wavelet Packet and Perceptual Wavelet Packet transforms to produce the Normalised TEO Autocorrelation Envelope Area coefficients for the classification process. The classification was performed using a Gaussian Mixture Model under speaker-independent conditions. The speech was classified into two classes: neutral and stressed. The best overall performance was observed for the features extracted using TEO analysis in combination with the Perceptual Wavelet Packet method. The accuracy in this case ranges from 94% to 96% depending on the type of mother wavelet.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-194"
  },
  "shriberg08_interspeech": {
   "authors": [
    [
     "Elizabeth",
     "Shriberg"
    ],
    [
     "Martin",
     "Graciarena"
    ],
    [
     "Harry",
     "Bratt"
    ],
    [
     "Andreas",
     "Kathol"
    ],
    [
     "Sachin S.",
     "Kajarekar"
    ],
    [
     "Huda",
     "Jameel"
    ],
    [
     "Colleen",
     "Richey"
    ],
    [
     "Fred",
     "Goodman"
    ]
   ],
   "title": "Effects of vocal effort and speaking style on text-independent speaker verification",
   "original": "i08_0609",
   "page_count": 4,
   "order": 213,
   "p1": "609",
   "pn": "612",
   "abstract": [
    "We study the question of how intrinsic variations (associated with the speaker rather than the recording environment) affect text-independent speaker verification performance. Experiments using the SRI-FRTIV corpus,which systematically varies both vocal effort and speaking style, reveal that (1) \"furtive\" speech poses a significant challenge; (2) conversations and interviews, despite stylistic differences, are well matched; (3) high-effort oration, in contrast to high-effort read speech, shares characteristics with conversational and interview styles; and (4) train/test pairings are generally symmetrical. Implications for further work in the area are discussed.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-195"
  },
  "farrus08_interspeech": {
   "authors": [
    [
     "Mireia",
     "Farrús"
    ],
    [
     "Michael",
     "Wagner"
    ],
    [
     "Jan",
     "Anguita"
    ],
    [
     "Javier",
     "Hernando"
    ]
   ],
   "title": "Robustness of prosodic features to voice imitation",
   "original": "i08_0613",
   "page_count": 4,
   "order": 214,
   "p1": "613",
   "pn": "616",
   "abstract": [
    "Prosody plays an important role in the human recognition process; therefore, prosodic elements are normally used by impersonators aiming to resemble someone else. Since such voice imitation is one of the potential threats to security systems relying on automatic speaker recognition, and prosodic features have been considered for state-of-the-art recognition systems in recent years, the question arises as to what extent a mimicker is able to get close the prosodic characteristics of a target speaker. To this end, two experiments are conducted for twelve individual features in order to determine how a prosodic speaker identification system would perform against professionally imitated voices. The results show that the identification error rate increases for all the features except F0 range when the impersonators' modified voices are used instead of the impersonators natural voices. Moreover, it seems easier to copy prosody on the basis of a whole sentence than for a specific word.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-196"
  },
  "sethu08_interspeech": {
   "authors": [
    [
     "Vidhyasaharan",
     "Sethu"
    ],
    [
     "Eliathamby",
     "Ambikairajah"
    ],
    [
     "Julien",
     "Epps"
    ]
   ],
   "title": "Phonetic and speaker variations in automatic emotion classification",
   "original": "i08_0617",
   "page_count": 4,
   "order": 215,
   "p1": "617",
   "pn": "620",
   "abstract": [
    "The speech signal contains information that characterises the speaker and the phonetic content, together with the emotion being expressed. This paper looks at the effect of this speakerand phoneme-specific information on speech-based automatic emotion classification. The performances of a classification system using established acoustic and prosodic features for different phonemes are compared, in both speaker-dependent and speakerindependent modes, using the LDC Emotional Prosody speech corpus. Results from these evaluations indicate that speaker variability is more significant than phonetic variations. They also suggest that some phonemes are easier to classify than others.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-197"
  },
  "mattock08_interspeech": {
   "authors": [
    [
     "Karen",
     "Mattock"
    ]
   ],
   "title": "Infants' native and nonnative tone perception",
   "original": "i08_0621",
   "page_count": 1,
   "order": 216,
   "p1": "621",
   "pn": "",
   "abstract": [
    "Infants' native and nonnative tone perception was investigated in three studies. In Study 1, 6- and 9-month-old English (non-tone) and Chinese (tone) infants were tested for discrimination of Thai tones in a training procedure. Study 2 examined English and French infants' Thai tone perception in a more natural looking/listening task. Study 3 investigated English infants' discrimination of Mandarin tones. The findings provide robust evidence for the reorganization of tone perception as a function of language experience. Age-related decline in nonnative tone discrimination was found for English and French infants. Chinese infants showed stable tone perception across age.\n",
    ""
   ]
  },
  "krishnan08_interspeech": {
   "authors": [
    [
     "Ananthanarayan",
     "Krishnan"
    ],
    [
     "Jackson",
     "Gandour"
    ],
    [
     "Jayaganesh",
     "Swaminathan"
    ]
   ],
   "title": "Language experience dependent plasticity for pitch representation in the human brainstem",
   "original": "i08_0622",
   "page_count": 1,
   "order": 217,
   "p1": "622",
   "pn": "",
   "abstract": [
    "Brainstem frequencies following responses (FFR) were recorded from Chinese and English participants in response to Mandarin tonal patterns presented in a speech and a non-speech context. Results showed that the Chinese group exhibits stronger pitch representation and smoother pitch tracking than the English group. Moreover, the Chinese group exhibited relatively more robust pitch representation of rapidly-changing pitch segments. These findings support the view that at early preattentive stages of subcortical processing,neural mechanisms underlying pitch representation are shaped by particular features of the auditory stream rather than speech per se.\n",
    ""
   ]
  },
  "ciocca08_interspeech": {
   "authors": [
    [
     "Valter",
     "Ciocca"
    ],
    [
     "Vivian W.-K.",
     "Ip"
    ]
   ],
   "title": "Development of tone perception and tone production in Cantonese-learning children aged 2 to 5 years",
   "original": "i08_0623",
   "page_count": 1,
   "order": 218,
   "p1": "623",
   "pn": "",
   "abstract": [
    "The development of lexical tone perception and production was investigated in sixty normally developing Cantonese-speaking children (aged 2 to 5 years). Overall, the accuracy of tone identification improved from 2 to 5 years of age. Children's production of lexical tones was measured through phonetic transcriptions. A particular tone was judged to have been acquired when transcription accuracy was 90% or better. On the basis of this criterion, the acquisition of tone production was completed by 4 years of age.\n",
    ""
   ]
  },
  "xu08b_interspeech": {
   "authors": [
    [
     "Nan",
     "Xu"
    ],
    [
     "Denis",
     "Burnham"
    ]
   ],
   "title": "Tone hyperarticulation in Cantonese infant-directed speech",
   "original": "i08_0624",
   "page_count": 1,
   "order": 219,
   "p1": "624",
   "pn": "",
   "abstract": [
    "There is extensive evidence for vowel hyperarticulation in infant-directed speech (IDS) [1, 2, 3], but little is known about lexical tones in tonal IDS. Here, longitudinal recordings of Cantonese IDS revealed that tones, like vowels, are hyperarticulated. Implications of this finding for infant linguistic development are discussed.\n",
    "s Liu, H.-M., Kuhl, P. K., & Tsao, F.-, M. (2003). An association between mothers' speech clarity and infants' speech discrimination skills. Dev. Sci., 6(3), F1-F10. Kuhl, P. K., et al. (1997). Cross-language analysis of phonetic units in language addressed to infants. Science, 277, 684-686. Burnham, D., Kitamura, C., & Vollmer-Conna, U. (2002). What's new, pussycat? On talking to babies and animals. Science, 296, 1435.\n",
    ""
   ]
  },
  "zerbian08_interspeech": {
   "authors": [
    [
     "Sabine",
     "Zerbian"
    ],
    [
     "Etienne",
     "Barnard"
    ]
   ],
   "title": "Influences on tone in Sepedi, a southern Bantu language",
   "original": "i08_0625",
   "page_count": 1,
   "order": 220,
   "p1": "625",
   "pn": "",
   "abstract": [
    "Tone in Bantu languages is rarely studied experimentally. This paper reports a production study which reveals the intricate interaction of tonal context and morphological structure in surface tone realization in Sepedi, a South African Bantu language.\n",
    ""
   ]
  },
  "ishihara08_interspeech": {
   "authors": [
    [
     "Shunichi",
     "Ishihara"
    ]
   ],
   "title": "An acoustic-phonetic comparative analysis of Osaka and Kagoshima Japanese tonal phenomena",
   "original": "i08_0626",
   "page_count": 4,
   "order": 221,
   "p1": "626",
   "pn": "629",
   "abstract": [
    "This paper, first of all, demonstrates that the LHL and the L(L)H pitch patterns exhibit significantly different F0 realisations between Osaka Japanese (OJ) and Kagoshima Japanese (KJ) which typologically belong to different accent groups. Secondly, it is argued that the observed different results from the phonological differences between OJ and KJ in terms of their tonal representations. Finally, some implications of the observed difference to the tonal representations of KJ are discussed. This study is based on the acoustic-phonetic descriptions of the LHL and the L(L)H pitch patterns using a normalisation technique based on 12 OJ and 14 KJ speakers.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-198"
  },
  "vinyals08_interspeech": {
   "authors": [
    [
     "Oriol",
     "Vinyals"
    ],
    [
     "Gerald",
     "Friedland"
    ]
   ],
   "title": "Modulation spectrogram features for improved speaker diarization",
   "original": "i08_0630",
   "page_count": 4,
   "order": 222,
   "p1": "630",
   "pn": "633",
   "abstract": [
    "We propose the use of modulation spectrogram features in speaker diarization. These features carry longer term characteristics of the acoustic signals than the widely used MFCCs, thus providing potential improvement by using both features in combination. Using the state-of-the-art ICSI speaker diarization system, an improvement of 20.77% relative DER is obtained on the NIST Rich Transcription 2007 task with respect to the MFCC only system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-199"
  },
  "falk08_interspeech": {
   "authors": [
    [
     "Tiago H.",
     "Falk"
    ],
    [
     "Wai-Yip",
     "Chan"
    ]
   ],
   "title": "Spectro-temporal features for robust far-field speaker identification",
   "original": "i08_0634",
   "page_count": 4,
   "order": 223,
   "p1": "634",
   "pn": "637",
   "abstract": [
    "Features derived from an auditory spectro-temporal representation of speech are proposed for robust far-field speaker identification. The auditory representation is obtained by first filtering the speech signal with a gammatone filterbank. A modulation filterbank is then applied to the temporal envelope of each gammatone filter output. Compared to commonly used mel-frequency cepstral coefficients (MFCC), the proposed features are shown to be more robust to mismatched conditions between enrollment and test data and are less sensitive to increasing reverberation time (RT). Experiments with simulated and recorded far-field speech show that a Gaussian mixture model based identification system, trained on the proposed features, attains an average improvement in identification accuracy of 15% relative to a system trained on MFCC. Improvements of up to 85% are attained for larger RT.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-200"
  },
  "wu08c_interspeech": {
   "authors": [
    [
     "Siqing",
     "Wu"
    ],
    [
     "Tiago H.",
     "Falk"
    ],
    [
     "Wai-Yip",
     "Chan"
    ]
   ],
   "title": "Long-term spectro-temporal information for improved automatic speech emotion classification",
   "original": "i08_0638",
   "page_count": 4,
   "order": 224,
   "p1": "638",
   "pn": "641",
   "abstract": [
    "This paper investigates the contribution of features which convey long-term spectro-temporal (ST) information for the purpose of automatic emotional speech classification. The ST representation is obtained by means of a modulation filterbank decomposition of long-term temporal envelopes of the outputs of a gammatone filterbank. The two-dimensional discrete cosine transform is used to reduce the dimensionality of the representation; candidate features are then derived from statistics computed from the DCT coefficients. Sequential forward feature selection is used to select the most salient features. Two types of experiments are described which use the Berlin emotional speech database to test the performance of the ST features alone and in combination with prosodic features. In a multi-class experiment, simulation results with a support vector classifier show that a 44% reduction in classification error is attained once prosodic features are combined with the proposed ST features. Additionally, in a one-against-all experiment, an average increase in F-score of 33% is attained when the proposed ST features are included.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-201"
  },
  "kubo08_interspeech": {
   "authors": [
    [
     "Yotaro",
     "Kubo"
    ],
    [
     "Shigeki",
     "Okawa"
    ],
    [
     "Akira",
     "Kurematsu"
    ],
    [
     "Katsuhiko",
     "Shirai"
    ]
   ],
   "title": "A comparative study on AM and FM features",
   "original": "i08_0642",
   "page_count": 4,
   "order": 225,
   "p1": "642",
   "pn": "645",
   "abstract": [
    "In this paper, we investigate the advantages of frequency modulation (FM) features by conducting speech recognition experiments and statistical analysis. The importance of temporal aspects in speech recognition has been discussed along with the importance of amplitude modulation (AM) and frequency modulation. Recently, we have proposed a speech recognition system that is based on the combination of AM and FM features and confirmed its efficiency experimentally. Although the proposed speech recognizer assumes complementarity between the AM and FM features, it was not evaluated in previous studies. In this paper, in order to evaluate the complementarity between two types of features, we conducted continuous digit recognition tasks in artificial noisy conditions. We confirmed that the error rates of each classifier are significantly different depending to kind of noise. Then, we evaluated the statistical independency between these two types of features. We confirmed that the behaviors of these features are independent in realistic noisy environments.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-202"
  },
  "markaki08_interspeech": {
   "authors": [
    [
     "Maria",
     "Markaki"
    ],
    [
     "Yannis",
     "Stylianou"
    ]
   ],
   "title": "Dimensionality reduction of modulation frequency features for speech discrimination",
   "original": "i08_0646",
   "page_count": 4,
   "order": 226,
   "p1": "646",
   "pn": "649",
   "abstract": [
    "We describe a dimensionality reduction method for modulation spectral features, which keeps the time-varying information of interest to the classification task. Due to the varying degrees of redundancy and discriminative power of the acoustic and modulation frequency subspaces, we first employ a generalization of SVD to tensors (Higher Order SVD) to reduce dimensions. Projection of modulation spectral features on the principal axes with the higher energy in each subspace results in a compact feature set. We further estimate the relevance of these projections to speech discrimination based on mutual information to the target class. Reconstruction of modulation spectrograms from the \"best\" 22 features back to the initial dimensions, shows that modulation spectral features close to syllable and phoneme rates as well as pitch values of speakers are preserved.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-203"
  },
  "kawahara08_interspeech": {
   "authors": [
    [
     "Hideki",
     "Kawahara"
    ],
    [
     "Masanori",
     "Morise"
    ],
    [
     "Hideki",
     "Banno"
    ],
    [
     "Toru",
     "Takahashi"
    ],
    [
     "Ryuichi",
     "Nisimura"
    ],
    [
     "Toshio",
     "Irino"
    ]
   ],
   "title": "Spectral envelope recovery beyond the nyquist limit for high-quality manipulation of speech sounds",
   "original": "i08_0650",
   "page_count": 4,
   "order": 227,
   "p1": "650",
   "pn": "653",
   "abstract": [
    "A simple new method to recover details in a spectral envelope is proposed based on a recently introduced speech analysis, modification and resynthesis framework called TANDEM-STRAIGHT. Spectral envelope recovery of voiced sounds is a discrete-to-analog conversion in the frequency domain. However, there is a fundamental problem because the spatial frequency contents of vocal tract functions generally exceed the Nyquist limit of the equivalent sampling rate determined by the fundamental frequency. TANDEM-STRAIGHT yields a method to recover a spectral envelope based on the consistent sampling theory and provides base information for exceeding this limit. At the final stage, the AR spectral envelope estimated from the TANDEM-STRAIGHT spectrum is divided by the F0 adaptively smoothed version of itself to supply the missing high-spatial-frequency details of the envelope. The underlying principle of the proposed method can also be applied to other speech synthesis frameworks.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-204"
  },
  "yin08_interspeech": {
   "authors": [
    [
     "Hui",
     "Yin"
    ],
    [
     "Xiang",
     "Xie"
    ],
    [
     "Jingming",
     "Kuang"
    ]
   ],
   "title": "Adaptive-order fractional Fourier transform features for speech recognition",
   "original": "i08_0654",
   "page_count": 4,
   "order": 228,
   "p1": "654",
   "pn": "657",
   "abstract": [
    "We propose an acoustic feature for speech recognition based on the combination of MFCC and fractional Fourier transform (FrFT). Since the transform order is critical for the performance of FrFT, we use the ambiguity function to adaptively determine the optimal orders of FrFT for each frame. The performance of the proposed feature is compared with traditional MFCCs on recognizing speech of isolated and connected digits under both clean and noisy backgrounds. The recognition results and detailed confusion matrices are given and analyzed, which implies that the proposed feature is promising in certain speech processing fields.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-205"
  },
  "petrick08b_interspeech": {
   "authors": [
    [
     "Rico",
     "Petrick"
    ],
    [
     "Xugang",
     "Lu"
    ],
    [
     "Masashi",
     "Unoki"
    ],
    [
     "Masato",
     "Akagi"
    ],
    [
     "Rüdiger",
     "Hoffmann"
    ]
   ],
   "title": "Robust front end processing for speech recognition in reverberant environments: utilization of speech characteristics",
   "original": "i08_0658",
   "page_count": 4,
   "order": 229,
   "p1": "658",
   "pn": "661",
   "abstract": [
    "This paper proposes two methods for robust automatic speech recognition (ASR) in reverberant environments. Unlike other methods which mostly apply inverse filtering by blindly estimated room impulse responses to achieve dereverberation, the proposed methods are based on the utilization of the characteristics of speech. The first method - Harmonicity based Feature Analysis - takes advantage of the harmonic components of speech, which are assumed to be undistorted. The second method . Temporal Power Envelope Feature Analysis - utilizes the temporal modulation structure of speech, representing the phoneme level temporal events which contain most intelligibility information. Both methods increase the recognition performance remarkably in a different way. Combining both of them connects their individual advantages. In order to examine the performance of utilizing harmonicity and modulation temporal structure for reverberant ASR, the methods are tested in clean and reverberant training. As results show, even in strong reverberant conditions both methods obtain practical applicable performance for reverberant training. In addition, besides testing their performance in dependency on the reverberation time, their performance considering the speaker-to-microphone distance is tested, which is another new contribution in this paper.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-206"
  },
  "sivaram08_interspeech": {
   "authors": [
    [
     "G. S. V. S.",
     "Sivaram"
    ],
    [
     "Hynek",
     "Hermansky"
    ]
   ],
   "title": "Introducing temporal asymmetries in feature extraction for automatic speech recognition",
   "original": "i08_0890",
   "page_count": 4,
   "order": 230,
   "p1": "890",
   "pn": "893",
   "abstract": [
    "We propose a new auditory inspired feature extraction technique for automatic speech recognition (ASR). Features are extracted by filtering the temporal trajectory of spectral energies in each critical band of speech by a bank of finite impulse response (FIR) filters. Impulse responses of these filters are derived from a modified Gabor envelope in order to emulate asymmetries of the temporal receptive field (TRF) profiles observed in higher level auditory neurons. We obtain 11.4% relative improvement in word error rate on OGI-Digits database and, 3.2% relative improvement in phoneme error rate on TIMIT database over the MRASTA technique.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-207"
  },
  "heckmann08_interspeech": {
   "authors": [
    [
     "Martin",
     "Heckmann"
    ],
    [
     "Xavier",
     "Domont"
    ],
    [
     "Frank",
     "Joublin"
    ],
    [
     "Christian",
     "Goerick"
    ]
   ],
   "title": "A closer look on hierarchical spectro-temporal features (HIST)",
   "original": "i08_0894",
   "page_count": 4,
   "order": 231,
   "p1": "894",
   "pn": "897",
   "abstract": [
    "Speech recognition robust against interfering noise remains a difficult task. We previously presented a set of spectro-temporal speech features which we termed Hierarchical Spectro-Temporal (Hist) features showing improved robustness, especially when combined with Rasta-Plp. They are inspired by the receptive fields found in the mammalian auditory cortex and are organized in two hierarchical levels. A set of filters learned via ICA captures local variations and constitutes the first layer of the hierarchy. In the second layer these local variations are combined to form larger receptive fields learned via Non Negative Sparse Coding. In this paper we introduce a non-linear smoothing along the time axis of the spectrograms at the input to the hierarchy and, additionally, a more thorough performance analysis on an isolated and a continuous digit recognition task. The results show that the combination of Hist and Rasta-Plp features yields improved recognition scores in noise.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-208"
  },
  "zhao08_interspeech": {
   "authors": [
    [
     "Sherry Y.",
     "Zhao"
    ],
    [
     "Nelson",
     "Morgan"
    ]
   ],
   "title": "Multi-stream spectro-temporal features for robust speech recognition",
   "original": "i08_0898",
   "page_count": 4,
   "order": 232,
   "p1": "898",
   "pn": "901",
   "abstract": [
    "A multi-stream approach to utilizing the inherently large number of spectro-temporal features for speech recognition is investigated in this study. Instead of reducing the feature-space dimension, this method divides the features into streams so that each represents a patch of information in the spectro-temporal response field. When used in combination with MFCCs for speech recognition under both clean and noisy conditions, multi-stream spectro-temporal features provide roughly a 30% relative improvement in word-error rate over using MFCCs alone. The result suggests that the multistream approach may be an effective way to handle and utilize spectro-temporal features for speech applications.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-209"
  },
  "wang08_interspeech": {
   "authors": [
    [
     "Huan",
     "Wang"
    ],
    [
     "David",
     "Gelbart"
    ],
    [
     "Hans-Günter",
     "Hirsch"
    ],
    [
     "Werner",
     "Hemmert"
    ]
   ],
   "title": "The value of auditory offset adaptation and appropriate acoustic modeling",
   "original": "i08_0902",
   "page_count": 4,
   "order": 233,
   "p1": "902",
   "pn": "905",
   "abstract": [
    "A critical step in encoding sound for neuronal processing occurs when the analog pressure wave is coded into discrete nerve-action potentials. Recent pool models of the inner hair cell synapse do not reproduce the dead time period after an intense stimulus, so we used visual inspection and automatic speech recognition (ASR) to investigate an offset adaptation (OA) model proposed by Zhang et al. [1].\n",
    "OA improved phase locking in the auditory nerve (AN) and raised ASR accuracy for features derived from AN fibers (ANFs). We also found that OA is crucial for auditory processing by onset neurons (ONs) in the next neuronal stage, the auditory brainstem. Multi-layer perceptrons (MLPs) performed much better than standard Gaussian mixture models (GMMs) for both our ANF-based and ON-based auditory features. Similar results were previously obtained with MSG (Modulation-filtered SpectroGram) auditory features[2]. Thus we believe researchers working with novel features should consider trying MLPs.\n",
    "s X. Zhang and L. H. Carney, \"Analysis of models for the synapse between the inner hair cell and the auditory nerve,\" J. Acoust. Soc. Am., vol. 118, pp. 1540-53, 2005.\n",
    "S. Sharma, D. Ellis, S. Kajarekar, P. Jain, and H. Hermansky, \"Feature extraction using non-linear transformation for robust speech recognition on the Aurora database,\" in ICASSP, 2000.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-210"
  },
  "meyer08_interspeech": {
   "authors": [
    [
     "Bernd T.",
     "Meyer"
    ],
    [
     "Birger",
     "Kollmeier"
    ]
   ],
   "title": "Optimization and evaluation of Gabor feature sets for ASR",
   "original": "i08_0906",
   "page_count": 4,
   "order": 234,
   "p1": "906",
   "pn": "909",
   "abstract": [
    "In order to enhance automatic speech recognition performance in adverse conditions, Gabor features motivated by physiological measurements in the primary auditory cortex were optimized and evaluated. In the Aurora 2 experimental setup such localized, spectro-temporal filters combined with a Tandem system yield robust performance with a feature set size of 30. Improved results can be obtained when using a Hanning window instead of a cut-off Gaussian envelope due to better modulation frequency characteristics. An analysis of complementarity of Gabor and MFCC features shows that errors could be reduced by 55% with a perfect classifier. In a real world scenario, a relative WER reduction of 15% compared to a competitive baseline is achieved by combining the feature types, indicating the potential of this class of physiologically motivated features.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-211"
  },
  "nguyen08b_interspeech": {
   "authors": [
    [
     "Binh Phu",
     "Nguyen"
    ],
    [
     "Takeshi",
     "Shibata"
    ],
    [
     "Masato",
     "Akagi"
    ]
   ],
   "title": "High-quality analysis/synthesis method based on temporal decomposition for speech modification",
   "original": "i08_0662",
   "page_count": 4,
   "order": 235,
   "p1": "662",
   "pn": "665",
   "abstract": [
    "The challenge of speech modification is to flexibly modify the speech without degrading speech quality. The conventional methods are limited by their inability to flexibly control speech signals in time and frequency domains. This causes degradation of the quality of modified speech. This paper proposes a high-quality analysis/synthesis method for speech modification. To control the temporal evolution, we use a speech analysis technique called temporal decomposition (TD), which decomposes a speech signal into event targets and event functions. The same event functions evaluated for the spectral parameters are also used to model the temporal evolution of the excitation parameters. The event functions describe the temporal evolution of the spectral and excitation parameters, and the event targets represent the \"ideal\" spectral parameters. To flexibly control speech signals in both time and frequency domains, we propose new methods to model the event functions and the event targets. The experimental results show that our proposed analysis/synthesis method produces highquality synthesized speech, and allows the flexibility to modify speech signals.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-212"
  },
  "gournay08_interspeech": {
   "authors": [
    [
     "Philippe",
     "Gournay"
    ]
   ],
   "title": "Improved frame loss recovery using closed-loop estimation of very low bit rate side information",
   "original": "i08_0666",
   "page_count": 4,
   "order": 236,
   "p1": "666",
   "pn": "669",
   "abstract": [
    "In CELP coders, the past excitation signal used to build the adaptive codebook is known to be the main source of error propagation when a frame is lost. This paper presents a novel resynchronization technique using very low bit rate side information to correct the past excitation signal after a frame erasure, the novelty being that the correction is computed in a closed loop fashion, based on the actual error introduced by the concealment. Subjective test results show that this approach is a promising area for future research on frame loss recovery.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-213"
  },
  "happel08_interspeech": {
   "authors": [
    [
     "Max F. K.",
     "Happel"
    ],
    [
     "Simon",
     "Müller"
    ],
    [
     "Jörn",
     "Anemüller"
    ],
    [
     "Frank W.",
     "Ohl"
    ]
   ],
   "title": "Predictability of STRFs in auditory cortex neurons depends on stimulus class",
   "original": "i08_0670",
   "page_count": 1,
   "order": 237,
   "p1": "670",
   "pn": "",
   "abstract": [
    "The goal of this study was to predict neuronal responses based on STRF-estimates for different stimulus sets of primary auditory cortex neurons of Mongolian gerbils. Here we review results from STRF estimations after stimulation with frequency modulated (FM) tones, Dynamic Moving Ripples (DMR), as well as, prediction of responses to logatomes stemming from the Oldenburg Logatome Corpus (OLLO) data set [1]. Our results suggest that it is not merely the similarity of the presented power spectrum of stimuli that leads to better STRF predictions. We will suggest new STRF-approaches to categorize neuronal responses to more natural stimuli.\n",
    ""
   ]
  },
  "mittal08_interspeech": {
   "authors": [
    [
     "Udar",
     "Mittal"
    ],
    [
     "James P.",
     "Ashley"
    ],
    [
     "Jonathan",
     "Gibbs"
    ]
   ],
   "title": "Higher layer coding of non-speech like signals using factorial pulse codebook",
   "original": "i08_0671",
   "page_count": 4,
   "order": 238,
   "p1": "671",
   "pn": "674",
   "abstract": [
    "A transform coding method for coding higher layers of a multilayer embedded speech and audio coding system using factorial pulse codebook is proposed. The proposed methods use frequency selective attenuation of lower layer output to reduce the spurious noise generated when speech model based coding method is used in lower layers for coding of the non-speech signals. The frequency selective attenuation along with the use of factorial pulse codebook makes the method suitable for coding non-speech like signals. A classifier for deciding whether a signal is speech like or non-speech like is also proposed. The proposed method is a part of an ITU embedded speech/audio coding standard (ITU-T G.EV-VBR). The formal listening tests confirm the benefits of using the proposed method for coding of music signals and speech signal having background music.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-214"
  },
  "ganapathy08_interspeech": {
   "authors": [
    [
     "Sriram",
     "Ganapathy"
    ],
    [
     "Petr",
     "Motlicek"
    ],
    [
     "Hynek",
     "Hermansky"
    ],
    [
     "Harinath",
     "Garudadri"
    ]
   ],
   "title": "Spectral noise shaping: improvements in speech/audio codec based on linear prediction in spectral domain",
   "original": "i08_0675",
   "page_count": 4,
   "order": 239,
   "p1": "675",
   "pn": "678",
   "abstract": [
    "Audio coding based on Frequency Domain Linear Prediction (FDLP) uses auto-regressive models to approximate Hilbert envelopes in frequency sub-bands. Although the basic technique achieves good coding efficiency, there is a need to improve the reconstructed signal quality for tonal signals with impulsive spectral content. For such signals, the quantization noise in the FDLP codec appears as frequency components not present in the input signal. In this paper, we propose a technique of Spectral Noise Shaping (SNS) for improving the quality of tonal signals by applying a Time Domain Linear Prediction (TDLP) filter prior to the FDLP processing. The inverse TDLP filter at the decoder shapes the quantization noise to reduce the artifacts. Application of the SNS technique to the FDLP codec improves the quality of the tonal signals without affecting the bit-rate. Performance evaluation is done with Perceptual Evaluation of Audio Quality (PEAQ) scores and with subjective listening tests.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-215"
  },
  "flax08_interspeech": {
   "authors": [
    [
     "Matthew R.",
     "Flax"
    ],
    [
     "W. Harvey",
     "Holmes"
    ]
   ],
   "title": "Introducing the compression wave cochlear amplifier",
   "original": "i08_0679",
   "page_count": 4,
   "order": 240,
   "p1": "679",
   "pn": "682",
   "abstract": [
    "The compression wave cochlear amplifier (CW-CA) model is introduced for the first time. This is the first cochlear amplifier model that is soundly based on the physiology of the inner ear and neural centres. The CW-CA, which is assumed to be driven by the basilar travelling wave (TW), includes outer hair cell (OHC)motility and neural signal transmissions between the hair cells and the central nervous system as key elements. The OHC motility sets up a pressure wave in the cochlear fluids, which is modulated by the neural feedback. The CW-CA is represented mathematically as a delay-differential equation (DDE). Even the simplest model based on this concept is capable of explaining a wide range of hearing phenomena, including various types of otoacoustic emissions, distortion products, two-tone suppression, amplifier gain and compression, frequency specificity, neural spontaneous rates and many other phenomena including various pathological conditions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-216"
  },
  "flax08b_interspeech": {
   "authors": [
    [
     "Matthew R.",
     "Flax"
    ],
    [
     "W. Harvey",
     "Holmes"
    ]
   ],
   "title": "Goldman-hodgkin-katz cochlear hair cell models - a foundation for nonlinear cochlear mechanics",
   "original": "i08_0683",
   "page_count": 4,
   "order": 241,
   "p1": "683",
   "pn": "686",
   "abstract": [
    "This theoretical contribution highlights the need for a nonlinear electrophysical hair cell model, since hair cell behaviour is what underlies important hearing phenomena, especially nonlinear cochlear mechanics. It is shown that a new nonlinear Goldman- Hodgkin-Katz (GHK) model of a cell membrane can lead to a more natural analysis of cell behaviour. In particular, instead of having to assume values for reversal potentials and internal ionic concentrations, it then becomes possible to calculate them from the boundary conditions, such as potentials and external ionic concentrations, which are usually known a priori. The new model also naturally includes nonlinear resistances, reversal potentials and membrane capacitance. The values of these nonlinear elements naturally adjust according to the known and derived potentials and ion concentrations.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-217"
  },
  "bao08_interspeech": {
   "authors": [
    [
     "Changchun",
     "Bao"
    ],
    [
     "Hai-ting",
     "Li"
    ],
    [
     "Ze-xin",
     "Liu"
    ],
    [
     "Rui",
     "Fan"
    ],
    [
     "Heng",
     "Zhu"
    ],
    [
     "Mao-shen",
     "Jia"
    ],
    [
     "Rui",
     "Li"
    ]
   ],
   "title": "A 8.32 kb/s embedded wideband speech coding candidate for ITU-t EV-VBR standardization",
   "original": "i08_0687",
   "page_count": 4,
   "order": 242,
   "p1": "687",
   "pn": "690",
   "abstract": [
    "This paper describes a 8.32 kb/s embedded speech codec submitted as a candidate for the ITU-T Embedded Variable Bit Rate (EV-VBR) standardization. The coder is built upon a 2-stage coding structure consisting of embedded ACELP coding from 8 to 16kb/s and embedded TCX coding at 24 and 32 kb/s. The coder is designed to support bandwidth scalability (narrow band and wide band signal) and bit-rate scalability (8.32 kb/s). The output bit-stream produced by the coder is scalable, consisting of core layer and four enhancement layers at 8 kb/s, 12 kb/s, 16 kb/s, 24 kb/s and 32 kb/s individually. ITU-T's test results showed that this coder has good performance compared with reference codec.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-218"
  },
  "kim08_interspeech": {
   "authors": [
    [
     "Jong Kyu",
     "Kim"
    ],
    [
     "Seung Seop",
     "Park"
    ],
    [
     "Chang Woo",
     "Han"
    ],
    [
     "Nam Soo",
     "Kim"
    ]
   ],
   "title": "Decision tree based frame mode selection for AMR-WB+",
   "original": "i08_0695",
   "page_count": 4,
   "order": 243,
   "p1": "695",
   "pn": "698",
   "abstract": [
    "In this paper, we propose a decision tree based coding mode selection method for the AMR-WB+ audio coder. In order to obtain improved performance with reduced computation, decision tree classifier is adopted with the closed loop mode selection results as the target classification labels. To secure the practical feasibility of this method, the size of the decision tree is controlled by pruning. Through an evaluation on a database covering both speech and music materials, the proposed method is found to increase the mode selection accuracy compared with the open loop mode selection module in the AMR-WB+.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-219"
  },
  "liu08b_interspeech": {
   "authors": [
    [
     "W. M.",
     "Liu"
    ],
    [
     "K. A.",
     "Jellyman"
    ],
    [
     "N. W. D.",
     "Evans"
    ],
    [
     "John S. D.",
     "Mason"
    ]
   ],
   "title": "Assessment of objective quality measures for speech intelligibility",
   "original": "i08_0699",
   "page_count": 4,
   "order": 244,
   "p1": "699",
   "pn": "702",
   "abstract": [
    "This paper assesses 9 prominent objective quality measures for their potential in intelligibility estimation. Degradation considered include additive noises and those introduced by coding and enhancement schemes, totalling 78 types. This paper is believed to be the first to conduct an assessment on such a large combination of quality measures and degradations allowing side-by-side analysis. Experimental results show that the sophisticated perceptual-based measures which are superior for quality estimation, do not necessarily correlate well with human intelligibility and, in fact, give poorer correlations when enhancement schemes are considered. Meanwhile, the weighted spectral slope (WSS) emerges to be the most promising approach among all measures considered, scoring the highest correlation in 5 out of the 6 test sets. Worth noting are the positive correlations obtained with WSS which range from 0.14 to 0.86, as opposed to those with PESQ from -0.58 to 0.74. Such findings put WSS, a relatively conventional measure, in a new light as a potential intelligibility assessor.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-220"
  },
  "scholz08_interspeech": {
   "authors": [
    [
     "Kirstin",
     "Scholz"
    ],
    [
     "Christine",
     "Kühnel"
    ],
    [
     "Marcel",
     "Waltermann"
    ],
    [
     "Sebastian",
     "Möller"
    ],
    [
     "Ulrich",
     "Heute"
    ]
   ],
   "title": "Assessment of the speech-quality dimension \"noisiness\" for the instrumental estimation and analysis of telephone-band speech quality",
   "original": "i08_0703",
   "page_count": 4,
   "order": 245,
   "p1": "703",
   "pn": "706",
   "abstract": [
    "The development of instrumental measures that do not only estimate the speech quality of modern telecommunication systems but also analyze it is a current issue in speech processing. Our work aims at an analytic quality measure for telephone-band speech that is based on the instrumental assessment of so-called quality dimensions. They describe different quality-relevant characteristics of speech signals and thus allow for a quality analysis. An overall-quality rating is obtained by a suitable combination of the dimension ratings. For telephone-band speech, three quality dimensions have been identified: \"directness/frequency content\", \"continuity\", and \"noisiness\". This paper presents an estimator for \"noisiness\" that is based on three parameters: the level and the color of additive noise contained in a speech signal as well as its amount of signal-correlated noise. The dimension estimates show a correlation > 0.95 with the results of auditory tests.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-221"
  },
  "gomez08_interspeech": {
   "authors": [
    [
     "Angel M.",
     "Gomez"
    ],
    [
     "Jose L.",
     "Carmona"
    ],
    [
     "Antonio M.",
     "Peinado"
    ],
    [
     "Victoria",
     "Sanchez"
    ],
    [
     "Jose A.",
     "Gonzalez"
    ]
   ],
   "title": "Intelligibility evaluation of Ramsey-derived interleavers for internet voice streaming with the iLBC codec",
   "original": "i08_0707",
   "page_count": 4,
   "order": 246,
   "p1": "707",
   "pn": "710",
   "abstract": [
    "This paper focuses on the application of a previously proposed interleaving, derived from the Ramsey convolutional class, in a voice streaming context with bursty packet losses. This kind of interleaving has already shown significant improvements in a distributed speech recognition context, in comparison with the widely used minimum latency block interleavers (MLBI). Here, the effectiveness of these interleavers is evaluated with an internet-oriented speech codec, such as iLBC. Since iLBC avoids error propagation due to lost frames and only uses the previously received frame to recover from those, Ramsey interleaving turns out especially suitable for this codec. In order to measure the performance of the system, the ITU PESQ algorithm is applied along with an intelligibility criterion based on the accuracy obtained through an automatic speech recognizer (ASR). In addition, an informal subjective test is carried out to corroborate the ASR scores. Results show that the proposed Ramsey-derived interleaving provides at least the same quality and better intelligibility than the MLBI ones when it is applied to iLBC codec.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-222"
  },
  "lyu08_interspeech": {
   "authors": [
    [
     "Dau-Cheng",
     "Lyu"
    ],
    [
     "Ren-Yuan",
     "Lyu"
    ]
   ],
   "title": "Language identification on code-switching utterances using multiple cues",
   "original": "i08_0711",
   "page_count": 4,
   "order": 247,
   "p1": "711",
   "pn": "714",
   "abstract": [
    "Code-switching speech is an utterance containing two or more languages. Usually, the switching linguistic unit is in clause or word levels. In this paper, a two-stage framework is proposed, containing a language identifier and then a speech recognizer, to evaluate on a Mandarin-Taiwanese code-switching utterance. In the language identifier, we use multiple cues including acoustic, prosodic and phonetic features. In order to integrate the cues to distinguish one language from another, we used a maximum a posteriori decision rule to connect an acoustic model, a duration model and a language model. In the experiments, we have achieved 34.5% (LID) and 17.7% (ASR) error rate reduction comparing with one stage LVCSR-based system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-223"
  },
  "tong08_interspeech": {
   "authors": [
    [
     "Rong",
     "Tong"
    ],
    [
     "Bin",
     "Ma"
    ],
    [
     "Haizhou",
     "Li"
    ],
    [
     "Eng Siong",
     "Chng"
    ]
   ],
   "title": "Target-oriented phone selection from universal phone set for spoken language recognition",
   "original": "i08_0715",
   "page_count": 4,
   "order": 248,
   "p1": "715",
   "pn": "718",
   "abstract": [
    "This paper studies target-oriented phone selection strategy for constructing phone tokenizers in the Parallel Phone Recognizers followed by Vector Space Model (PPR-VSM) paradigm of spoken language recognition. With this phone selection strategy, one derives a set of target-oriented phone tokenizers (TOPT), each having a subset of phones that have high discriminative ability for a target language. Two phone selection methods are proposed to derive such phone subsets from a phone recognizer. We show that the TOPTs derived from a universal phone recognizer (UPR) outperform those derived from language specific phone recognizers. The TOPT front-end derived from a UPR also consistently outperforms the UPR front-end without involving additional acoustic modeling. We achieve an equal error rates (EERs) of 1.33%, 1.75% and 2.80% on NIST 1996, 2003 and 2007 LRE databases respectively for 30 second closed-set tests by including multiple TOPTs in the PPR.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-224"
  },
  "torrescarrasquillo08_interspeech": {
   "authors": [
    [
     "Pedro A.",
     "Torres-Carrasquillo"
    ],
    [
     "Elliot",
     "Singer"
    ],
    [
     "W. M.",
     "Campbell"
    ],
    [
     "Terry",
     "Gleason"
    ],
    [
     "Alan",
     "McCree"
    ],
    [
     "Douglas A.",
     "Reynolds"
    ],
    [
     "Fred",
     "Richardson"
    ],
    [
     "Wade",
     "Shen"
    ],
    [
     "Douglas E.",
     "Sturim"
    ]
   ],
   "title": "The MITLL NIST LRE 2007 language recognition system",
   "original": "i08_0719",
   "page_count": 4,
   "order": 249,
   "p1": "719",
   "pn": "722",
   "abstract": [
    "This paper presents a description of the MIT Lincoln Laboratory language recognition system submitted to the NIST 2007 Language Recognition Evaluation. This system consists of a fusion of four core recognizers, two based on tokenization and two based on spectral similarity. Results for NIST's 14-language detection task are presented for both the closed-set and open-set tasks and for the 30, 10 and 3 second durations. On the 30 second 14-language closed set detection task, the system achieves a 1% equal error rate.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-225"
  },
  "torrescarrasquillo08b_interspeech": {
   "authors": [
    [
     "Pedro A.",
     "Torres-Carrasquillo"
    ],
    [
     "Douglas E.",
     "Sturim"
    ],
    [
     "Douglas A.",
     "Reynolds"
    ],
    [
     "Alan",
     "McCree"
    ]
   ],
   "title": "Eigen-channel compensation and discriminatively trained Gaussian mixture models for dialect and accent recognition",
   "original": "i08_0723",
   "page_count": 4,
   "order": 250,
   "p1": "723",
   "pn": "726",
   "abstract": [
    "This paper presents a series of dialect/accent identification results for three sets of dialects with discriminatively trained Gaussian mixture models and feature compensation using eigen-channel decomposition. The classification tasks evaluated in the paper include: 1) the Chinese language classes, 2) American and Indian accented English and 3) discrimination between three Arabic dialects. The first two tasks were evaluated on the 2007 NIST LRE corpus. The Arabic discrimination task was evaluated using data derived from the LDC Arabic set collected by Appen. Analysis is performed for the English accent problem studied and an approach to open set dialect scoring is introduced. The system resulted in equal error rates at or below 10% for each of the tasks studied.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-226"
  },
  "lopezmoreno08_interspeech": {
   "authors": [
    [
     "Ignacio",
     "Lopez-Moreno"
    ],
    [
     "Daniel",
     "Ramos"
    ],
    [
     "Joaquin",
     "Gonzalez-Rodriguez"
    ],
    [
     "Doroteo T.",
     "Toledano"
    ]
   ],
   "title": "Anchor-model fusion for language recognition",
   "original": "i08_0727",
   "page_count": 4,
   "order": 251,
   "p1": "727",
   "pn": "730",
   "abstract": [
    "State-of-the-art language recognition systems usually combine multiple acoustic and phonotactic subsystems. The outputs of those systems are usually fused in different ways but the score from a trial is always obtained from N scores from N subsystems. In this paper, a robust novel approach to subsystem fusion in language recognition is proposed based on the relative performance of each trial not just to the claimed model but to all available models. The proposed technique exploits the relative behavior of a given speech utterance over the cohort of anchor models from the different subsystems, resulting in the proposed anchor-model fusion. Experiments fusing seven phone-SVM subsystems submitted by the authors to NIST LRE 2007 assess the robustness to non-uniform data availability over rule-based and trained fusion schemes as linear kernel SVM, as well as significant improvements in performance both in average EER and Cavg as used in NIST LRE.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-227"
  },
  "yin08b_interspeech": {
   "authors": [
    [
     "Bo",
     "Yin"
    ],
    [
     "Tharmarajah",
     "Thiruvaran"
    ],
    [
     "Eliathamby",
     "Ambikairajah"
    ],
    [
     "Fang",
     "Chen"
    ]
   ],
   "title": "Introducing a FM based feature to hierarchical language identification",
   "original": "i08_0731",
   "page_count": 4,
   "order": 252,
   "p1": "731",
   "pn": "734",
   "abstract": [
    "Although relatively neglected in auditory analysis, phase information plays an important role in human auditory intelligibility. This paper investigates a Frequency Modulation (FM) based feature and its contribution to a Language Identification (LID) system, using a Hierarchical LID framework. FM components represent the phase information of a given signal in an AM-FM model. In this paper, we extract a FM-based feature using a technique which produces consistent and continuous FM components, and build a LID system on this feature with GMM based modeling. The performance is improved by combining this system with existing MFCC, Prosody based systems and a PRLM system. When compared to the baseline system without integrating a FM-based system, the proposed Hierarchical LID system shows improvements. Additionally, the proposed system outperforms the GMM fusion-based system integrating the same four primary systems, showing that the Hierarchical LID framework is more effective in integrating additional features.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-228"
  },
  "lei08_interspeech": {
   "authors": [
    [
     "Yun",
     "Lei"
    ],
    [
     "John H. L.",
     "Hansen"
    ]
   ],
   "title": "Dialect classification via discriminative training",
   "original": "i08_0735",
   "page_count": 4,
   "order": 253,
   "p1": "735",
   "pn": "738",
   "abstract": [
    "Variability in speech due to dialect is a major factor limiting speech system performance for speech recognition, spoken document retrieval, and dialog systems. In this study, we propose a novel discriminative algorithm to improve dialect classification for unsupervised spontaneous speech in Arabic. No transcripts are used for either training or testing, and all data are spontaneous speech. The Gaussian mixture model (GMM) is used as our baseline system for dialect classification. The major motivation is to remove confused/distractive regions from the dialect acoustic space, while emphasizing discriminative/sensitive information. The Kullback-Leibler divergence is used to find the most discriminative GMM mixtures (KLD-GMM), after which the confused acoustic GMM region is removed. The proposed algorithm is evaluated on three dialects of Arabic, with measurable improvement achieved (4.0%), over a generalized maximum likelihood estimation GMM baseline (MLE-GMM) system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-229"
  },
  "matejka08_interspeech": {
   "authors": [
    [
     "Pavel",
     "Matějka"
    ],
    [
     "Lukáš",
     "Burget"
    ],
    [
     "Ondřej",
     "Glembek"
    ],
    [
     "Petr",
     "Schwarz"
    ],
    [
     "Valiantsina",
     "Hubeika"
    ],
    [
     "Michal",
     "Fapšo"
    ],
    [
     "Tomáš",
     "Mikolov"
    ],
    [
     "Oldřich",
     "Plchot"
    ],
    [
     "Jan",
     "Černocký"
    ]
   ],
   "title": "BUT language recognition system for NIST 2007 evaluations",
   "original": "i08_0739",
   "page_count": 4,
   "order": 254,
   "p1": "739",
   "pn": "742",
   "abstract": [
    "This paper describes Brno University of Technology (BUT) system for 2007 NIST Language recognition (LRE) evaluation. The system is a fusion of 4 acoustic and 9 phonotactic sub-systems. We have investigated several new topics such as discriminatively trained language models in phonotactic systems, and eigen-channel adaptation in model and feature domain in acoustic systems. We also point out the importance of calibration and fusion. All results are presented on NIST 2007 LRE data.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-230"
  },
  "glembek08_interspeech": {
   "authors": [
    [
     "Ondřej",
     "Glembek"
    ],
    [
     "Pavel",
     "Matějka"
    ],
    [
     "Lukáš",
     "Burget"
    ],
    [
     "Tomáš",
     "Mikolov"
    ]
   ],
   "title": "Advances in phonotactic language recognition",
   "original": "i08_0743",
   "page_count": 4,
   "order": 255,
   "p1": "743",
   "pn": "746",
   "abstract": [
    "This paper summarizes recent advances in PRLM language recognition within the context of the NIST 2007 LR evaluations (LRE). We present a comparison of binary decision tree (BT) vs. N-gram models when adaptation from a universal (background) model (UBM) is used, we introduce multi-models - anchor-model-like approach to scoring, and we adopt the framework of intersession variation using factor analysis.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-231"
  },
  "mehrabani08_interspeech": {
   "authors": [
    [
     "Mahnoosh",
     "Mehrabani"
    ],
    [
     "John H. L.",
     "Hansen"
    ]
   ],
   "title": "Dialect separation assessment using log-likelihood score distributions",
   "original": "i08_0747",
   "page_count": 4,
   "order": 256,
   "p1": "747",
   "pn": "750",
   "abstract": [
    "Dialect differences within a given language represent major challenges for sustained speech system performance. For speech recognition, little if any knowledge exists on differences between dialects (e.g. vocabulary, grammar, prosody, etc.). Effective dialect classification can contribute to improved ASR, speaker ID, and spoken document retrieval. This study, presents an approach to establish a metric to estimate the separation between dialects, and to provide some sense of expected speech system performance. The proposed approach compares dialects based on their log-likelihood score distributions. From the score distributions, a numerical measure is obtained to assess the separation between resulting GMM dialect models. The proposed scheme is evaluated on a corpus of Arabic dialects. The sensitivity of the dialect separation score is also quantified based on controlled mixing of dialect data for the case of measuring dialect training data purity. The resulting scheme is shown to be effective in measuring dialect distance, and represents an important objective way of assessing dialect differences within a common language.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-232"
  },
  "alotaibi08_interspeech": {
   "authors": [
    [
     "Yousef A.",
     "Alotaibi"
    ],
    [
     "Khondaker",
     "Abdullah-Al-Mamun"
    ],
    [
     "Ghulam",
     "Muhammad"
    ]
   ],
   "title": "Study on unique pharyngeal and uvular consonants in foreign accented Arabic",
   "original": "i08_0751",
   "page_count": 4,
   "order": 257,
   "p1": "751",
   "pn": "754",
   "abstract": [
    "This paper investigates the unique pharyngeal and uvular consonants of Arabic from the automatic speech recognition (ASR) point of view. Comparisons of the recognition error rates for these phonemes are analyzed in five experiments that involve different combinations of native and non-native Arabic speakers. The most three confusing consonants for every investigated consonant are uncovered and discussed. Results confirm that these Arabic distinct consonants are a major source of difficulty for ASR. While the recognition rate for certain of these unique consonants such as /H/ can drop below 35% when uttered by non-native speakers, there are advantages to including non-native speakers in ASR. Regional differences in the pronunciation of Modern Standard Arabic by native Arabic speakers require attention of Arabic ASR research.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-233"
  },
  "bi08_interspeech": {
   "authors": [
    [
     "Fukun",
     "Bi"
    ],
    [
     "Jian",
     "Yang"
    ],
    [
     "Dan",
     "Xu"
    ]
   ],
   "title": "Automatic accent classification using ensemble methods",
   "original": "i08_0755",
   "page_count": 4,
   "order": 258,
   "p1": "755",
   "pn": "758",
   "abstract": [
    "Accent classification technologies directly influence the performance of the state-of-the-art speech recognition system. In this paper, we propose a novel scheme for accent classification, which uses decision-templates (DT) ensemble algorithm to combine base classifiers built on acoustic feature subsets. Different feature subsets can provide sufficient diversity among base classifiers, which is known as a necessary condition for improvement in ensemble performance. Compared with those methods of Majority Voting ensemble and Support Vector Machine, our ensemble scheme can achieve the highest performance. On the other hand, we investigate the possible reasons why ensemble systems can provide potential performance, in terms of diversity analysis. In our experiments, a native Mandarin speech corpus and a non-native multi-accent Mandarin speech corpus which contains three typical minorities' accents in Yunnan, China, are adopted.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-234"
  },
  "piat08_interspeech": {
   "authors": [
    [
     "Marina",
     "Piat"
    ],
    [
     "Dominique",
     "Fohr"
    ],
    [
     "Irina",
     "Illina"
    ]
   ],
   "title": "Foreign accent identification based on prosodic parameters",
   "original": "i08_0759",
   "page_count": 4,
   "order": 259,
   "p1": "759",
   "pn": "762",
   "abstract": [
    "In this paper we propose an automatic approach for foreign accent identification. Knowledge of the speaker's origin allows to adapt acoustic models for non-native speech recognition. In this study, we use a statistical approach based on prosodic parameters. This approach relies on the fact that prosody is different between languages, and has been done within the framework of the HIWIRE (Human Input that Works In Real Environments) European project. The corpus is composed of English sentences pronounced by French, Italian, Greek and Spanish speakers. Results obtained with duration and energy are promising for foreign accent identification: 67.1% correct L1 identification with duration and 68.6% with energy. These two parameters combined with MFCC achieve a 87.1% correct foreign accent identification rate.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-235"
  },
  "shen08_interspeech": {
   "authors": [
    [
     "Wade",
     "Shen"
    ],
    [
     "Nancy",
     "Chen"
    ],
    [
     "Douglas A.",
     "Reynolds"
    ]
   ],
   "title": "Dialect recognition using adapted phonetic models",
   "original": "i08_0763",
   "page_count": 4,
   "order": 260,
   "p1": "763",
   "pn": "766",
   "abstract": [
    "In this paper, we introduce a dialect recognition method that makes use of phonetic models adapted per dialect without phonetically labeled data. We show that this method can be implemented efficiently within an existing PRLM[1] system. We compare the performance of this system with other state-of-the-art dialect recognition methods (both acoustic and token-based) on the NIST LRE 2007 English and Mandarin dialect recognition tasks. Our experimental results indicate that this system can perform better than baseline GMM and adapted PRLM systems, and also results in consistent gains of 15.23% when combined with other systems.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-236"
  },
  "mccree08_interspeech": {
   "authors": [
    [
     "Alan",
     "McCree"
    ],
    [
     "Fred",
     "Richardson"
    ],
    [
     "Elliot",
     "Singer"
    ],
    [
     "Douglas A.",
     "Reynolds"
    ]
   ],
   "title": "Beyond frame independence: parametric modelling of time duration in speaker and language recognition",
   "original": "i08_0767",
   "page_count": 4,
   "order": 261,
   "p1": "767",
   "pn": "770",
   "abstract": [
    "In this work, we address the question of generating accurate likelihood estimates from multi-frame observations in speaker and language recognition. Using a simple theoretical model, we extend the basic assumption of independent frames to include two refinements: a local correlation model across neighboring frames, and a global uncertainty due to train/test channel mismatch. We present an algorithm for discriminative training of the resulting duration model based on logistic regression combined with a bisection search. We show that using this model we can achieve state-of-the-art performance for the NIST LRE07 task. Finally, we show that these more accurate class likelihood estimates can be combined to solve multiple problems using Bayes' rule, so that we can expand our single parametric back-end to replace all six separate back-ends used in our NIST LRE submission for both closed and open sets.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-237"
  },
  "dockendorf08_interspeech": {
   "authors": [
    [
     "Liz",
     "Dockendorf"
    ],
    [
     "Dalal",
     "Almubayei"
    ],
    [
     "Matthew",
     "Benton"
    ]
   ],
   "title": "Testing a large corpus of natural standard Arabic for rhythm class",
   "original": "i08_0771",
   "page_count": 1,
   "order": 262,
   "p1": "771",
   "pn": "",
   "abstract": [
    "Previous studies using acoustic correlates to measure speech rhythm have used small samples of audio and a limited number of speakers. Few have included standard Arabic in the analysis. This study uses Arabic news broadcast along with data output from an automatic speech recognizer time-aligned transcript to test over 50 minutes of speech by 46 speakers. The results show that Arabic, like English and Chinese news broadcasts, exhibits a range of individual speaker rhythm, but its average clearly distinguishes it from the other two on the rhythm continuum. A large number of speakers also makes the average more reliable.\n",
    ""
   ]
  },
  "benton08_interspeech": {
   "authors": [
    [
     "Matthew",
     "Benton"
    ],
    [
     "Liz",
     "Dockendorf"
    ]
   ],
   "title": "A comparison of two acoustic measurement approaches to the rhythm continuum of natural Chinese and English speech",
   "original": "i08_0772",
   "page_count": 4,
   "order": 263,
   "p1": "772",
   "pn": "775",
   "abstract": [
    "The recent interest in investigating the prosodic features of speech rhythm (speech timing patterns) using acoustic measurements instead of just perception has brought about several different metrics. Two of the main approaches are: (1) to compare vocalic segment (Vs) percentage against the standard deviation of consonantal segments (Cs) or (2) to compare the pairwise variability index (PVI) of normalized Vs against the raw PVI of Cs. This study compares and contrasts these two metrics using a large corpus of multi-speaker data of more naturally occurring speech from American English and Mandarin Chinese broadcast news.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-238"
  },
  "nariai08_interspeech": {
   "authors": [
    [
     "Tomoko",
     "Nariai"
    ],
    [
     "Kazuyo",
     "Tanaka"
    ]
   ],
   "title": "A study of pitch patterns of Japanese English analyzed via comparative linguistic features of English and Japanese",
   "original": "i08_0776",
   "page_count": 4,
   "order": 264,
   "p1": "776",
   "pn": "779",
   "abstract": [
    "Certain defects in utterances of a word or phrase occur in English as spoken by Japanese native subjects, referred to in this article as Japanese English. This study considers such prosodic feature patterns as one of the most common causes of deficiencies in Japanese English. Japanese English is linguistically supposed to have the phonetic characteristics of the native language, Japanese. This supposition leads to the hypothesis that pitch patterns of Japanese English can be interpreted from the point of view of comparative linguistic features of English and Japanese, and that Japanese English would have better (i.e. closer to English) prosodic patterns if its particular characteristics were removed. In this study, the hypothesis was acoustically examined by means of a synthesis-by-analysis system, STRAIGHT, and then tested by listening experiments. Results of the experiments indicate practical verification of the hypothesis.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-239"
  },
  "woehrling08_interspeech": {
   "authors": [
    [
     "Cécile",
     "Woehrling"
    ],
    [
     "Philippe",
     "Boula de Mareüil"
    ],
    [
     "Martine",
     "Adda-Decker"
    ],
    [
     "Lori",
     "Lamel"
    ]
   ],
   "title": "A corpus-based prosodic study of Alsatian, Belgian and Swiss French",
   "original": "i08_0780",
   "page_count": 4,
   "order": 265,
   "p1": "780",
   "pn": "783",
   "abstract": [
    "The object of this paper is a prosodic study of the French language as it is spoken in Alsace, Belgium and Switzerland, also compared with standard French through large corpora (over 100 hours) of scripted and spontaneous speech. The data were segmented into phones by automatic alignment; pitch values were extracted and averaged over segments. Two features are addressed: initial stress (through pitch and duration correlates) and penultimate lengthening. Different patterns enable us to distinguish the three varieties under investigation. Swiss speakers exhibit pitch rise and polysyllabic word onset lengthening in clitic.nonclitic sequences, while Alsatians tend to lengthen the initial vowel of nonclitic words. Belgians show prepausal penultimate lengthening whereas the Swiss tend to lengthen the last two prepausal vowels.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-240"
  },
  "nakamura08_interspeech": {
   "authors": [
    [
     "Mitsuhiro",
     "Nakamura"
    ]
   ],
   "title": "Prosodic position effects and function words in English: a pilot study",
   "original": "i08_0784",
   "page_count": 1,
   "order": 266,
   "p1": "784",
   "pn": "",
   "abstract": [
    "This study reports on the continuing investigation into the articulatory realization of function words in English. Using data from the Multichannel Articulatory (MOCHA) database, the consonant articulations in function and content words are examined as a function of position in an utterance. The preliminary results suggest that the word-class distinction is a vital factor in the interface between prosody and articulation.\n",
    ""
   ]
  },
  "ruiter08_interspeech": {
   "authors": [
    [
     "Laura E. de",
     "Ruiter"
    ]
   ],
   "title": "How useful are polynomials for analyzing intonation?",
   "original": "i08_0785",
   "page_count": 4,
   "order": 267,
   "p1": "785",
   "pn": "788",
   "abstract": [
    "This paper presents the first application of polynomial modeling as a means for validating phonological pitch accent labels to German data. It is compared to traditional phonetic analysis (measuring minima, maxima, alignment). The traditional method fares better in classification, but results are comparable in statistical accent pair testing. Robustness tests show that pitch correction is necessary in both cases. The approaches are discussed in terms of their practicability, applicability to other domains of research and interpretability of their results.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-241"
  },
  "chen08_interspeech": {
   "authors": [
    [
     "Qingcai",
     "Chen"
    ],
    [
     "Shusen",
     "Zhou"
    ],
    [
     "Dandan",
     "Wang"
    ],
    [
     "Xiaohong",
     "Yang"
    ]
   ],
   "title": "Adaptive filter based prosody modification approach",
   "original": "i08_0789",
   "page_count": 4,
   "order": 268,
   "p1": "789",
   "pn": "792",
   "abstract": [
    "This paper proposes an adaptive filter based prosody modification approach. It consists of three components, i.e., a frequency estimator, a pitch tracker and a prosody modifier. Firstly, the frequency estimator is applied to segment the speech signal into sentences and the maximum frequency level of each sentence is estimated. Then, the adaptive filter that is adapted according to the maximum frequency level of each sentence is constructed. Its output is applied to estimate the pitch of speech. The prosody of the speech, which includes its pitch and duration, is then modified according to estimated pitch and the given pitch modification target. Be compared with existing techniques, the advantage of the proposed approach is its capability of adapting with the frequency of given speech. This advantage makes the approach especially suitable for the large range pitch modification. Finally, the performance of the proposed approach is evaluated and is compared with three existing prosody modification approach and the result is quit encouraged.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-242"
  },
  "khine08_interspeech": {
   "authors": [
    [
     "Swe Zin Kalayar",
     "Khine"
    ],
    [
     "Tin Lay",
     "Nwe"
    ],
    [
     "Haizhou",
     "Li"
    ]
   ],
   "title": "Speech/laughter classification in meeting audio",
   "original": "i08_0793",
   "page_count": 4,
   "order": 269,
   "p1": "793",
   "pn": "796",
   "abstract": [
    "In this paper, harmonicity information is incorporated into acoustic features to detect laughter segments and speech segments. We implement our system using HMM (Hidden Markov Models) classifier trained on Pitch and Harmonic Frequency Scale based subband filters (PHFS). Harmonicity of the signal can be determined by variation of the pitch and harmonics. The cascaded subband filters are used to spread in pitch and harmonicity frequency scale to describe the harmonicity information. The pitch bandwidth of the first layer spans from 80 Hz to 300 Hz and the entire band spans 80 Hz-8 kHz. The experiments are conducted on ICSI meeting corpus (BMR and Bed). We achieve an average error rate of 0.84% for 'BMR' meeting and 3.64% for 'BED' meeting in segment level speech and laughter detection. The results show that the proposed Pitch and Harmonic Frequency Scale (PHFS) based feature is robust and effective.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-243"
  },
  "knox08_interspeech": {
   "authors": [
    [
     "Mary Tai",
     "Knox"
    ],
    [
     "Nelson",
     "Morgan"
    ],
    [
     "Nikki",
     "Mirghafori"
    ]
   ],
   "title": "Getting the last laugh: automatic laughter segmentation in meetings",
   "original": "i08_0797",
   "page_count": 4,
   "order": 270,
   "p1": "797",
   "pn": "800",
   "abstract": [
    "Our goal in this work was to develop an accurate method to identify laughter segments, ultimately for the purpose of speaker recognition. Our previous work used MLPs to perform frame level detection of laughter using short-term features, including MFCCs and pitch, and achieved a 7.9% EER on our test set. We improved upon our previous results by including high-level and long-term features, median filtering, and performing segmentation via a hybrid MLP/HMM system with Viterbi decoding. Upon including the long-term features and median filtering, our results improved to 5.4% EER on our test set and 2.7% EER on an equal-prior test set used by others. After attaining segmentation results by incorporating the hybrid MLP/HMM system and Viterbi decoding, we had a 78.5% precision rate and 85.3% recall rate on our test set. To our knowledge these are the best known laughter detection results on the ICSI Meeting Recorder Corpus to date.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-244"
  },
  "wrigley08_interspeech": {
   "authors": [
    [
     "Stuart N.",
     "Wrigley"
    ],
    [
     "Simon",
     "Tucker"
    ],
    [
     "Guy J.",
     "Brown"
    ],
    [
     "Steve",
     "Whittaker"
    ]
   ],
   "title": "The influence of audio presentation style on multitasking during teleconferences",
   "original": "i08_0801",
   "page_count": 4,
   "order": 271,
   "p1": "801",
   "pn": "804",
   "abstract": [
    "Teleconference participants often multitask: they work on a text-based 'foreground' task whilst listening in the 'background' for an item of interest to appear. Audio material should therefore be presented in a manner that has the smallest possible impact on the foreground task without affecting topic detection. Here, we ask whether dichotic or spatialised audio presentation of a meeting is less disruptive than the single-channel mixture of talkers normally used in teleconference audio. A number of talker location configurations are used, and we examine how these impact upon a text-based foreground task: finding all letter 'e' occurrences in a block of text. Additionally, we examine the effect of cueing the listener to direction or gender and record listener preferences for audio presentation style. Our results suggest that spatialised audio disrupts the foreground task less than single-channel audio when direction or gender is cued.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-245"
  },
  "vlasenko08_interspeech": {
   "authors": [
    [
     "Bogdan",
     "Vlasenko"
    ],
    [
     "Björn",
     "Schuller"
    ],
    [
     "Kinfe Tadesse",
     "Mengistu"
    ],
    [
     "Gerhard",
     "Rigoll"
    ],
    [
     "Andreas",
     "Wendemuth"
    ]
   ],
   "title": "Balancing spoken content adaptation and unit length in the recognition of emotion and interest",
   "original": "i08_0805",
   "page_count": 4,
   "order": 272,
   "p1": "805",
   "pn": "808",
   "abstract": [
    "Recognition and detection of non-lexical or paralinguistic cues from speech usually uses one general model per event (emotional state, level of interest). Commonly this model is trained independent of the phonetic structure. Given sufficient data, this approach seemingly works well enough. Yet, this paper addresses the question on which phonetic level there is the onset of emotions and level of interest. We therefore compare phoneme-, word- and sentence-level analysis for emotional sentence classification by use of a large prosodic, spectral, and voice quality feature space for SVM and MFCC for HMM/GMM. Experiments also take the necessity of ASR into account to select appropriate unit-models. In experiments on the well-known public EMO-DB database, and the SUSAS and AVIC spontaneous interest corpora, we found that the emotion recognition by sentence level analysis shows the best results. We discuss the implications of these types of analysis on the design of robust emotion and interest recognition of usable human-machine interfaces (HMI).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-246"
  },
  "krahmer08b_interspeech": {
   "authors": [
    [
     "Emiel",
     "Krahmer"
    ],
    [
     "Juliette",
     "Schaafsma"
    ],
    [
     "Marc",
     "Swerts"
    ],
    [
     "Ad",
     "Vingerhoets"
    ]
   ],
   "title": "Nonverbal responses to social inclusion and exclusion",
   "original": "i08_0809",
   "page_count": 4,
   "order": 273,
   "p1": "809",
   "pn": "812",
   "abstract": [
    "Applying an elaborate and novel experimental paradigm we collected non-verbal responses to social inclusion and exclusion. A judgement experiment revealed that it was possible to determine whether a person was included or excluded, based solely on non-verbal, behavioral cues, especially if the person was male. A detailed coding using the ECSI scale suggests that included speakers displayed more cues associated with Affiliation and Relaxation, whereas excluded speakers gave more evidence of Flight and Displacement.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-247"
  },
  "kitamura08_interspeech": {
   "authors": [
    [
     "Tatsuya",
     "Kitamura"
    ]
   ],
   "title": "Acoustic analysis of imitated voice produced by a professional impersonator",
   "original": "i08_0813",
   "page_count": 4,
   "order": 274,
   "p1": "813",
   "pn": "816",
   "abstract": [
    "We conducted a comparative study of a voice produced by a professional impersonator imitating a target speaker to explore the acoustical characteristics that the impersonator changes from his natural voice to imitate a target voice. Comparison of the pitch frequency of the target and imitated voices showed that the mean and dynamics of the pitch frequency of the imitated voice are changed so that they become closer to those of the target voice. The spectra of vowels uttered by the speakers are also similar in their shape and formant frequencies. In the imitated voice, the formant frequencies shift by up to 68% from those of the impersonator's natural voice. Moreover, the differences between the amplitudes of the first and second harmonics (H1-H2), a measure of the glottal source characteristics, of the target and imitated voices are small. These results clearly demonstrate that the impersonator controls vocal tract acoustic characteristics as well as those of the glottal source and pitch frequency to imitate the target voice.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-248"
  },
  "patil08_interspeech": {
   "authors": [
    [
     "Sanjay A.",
     "Patil"
    ],
    [
     "John H. L.",
     "Hansen"
    ]
   ],
   "title": "Detection of speech under physical stress: model development, sensor selection, and feature fusion",
   "original": "i08_0817",
   "page_count": 4,
   "order": 275,
   "p1": "817",
   "pn": "820",
   "abstract": [
    "Speech system scenarios can require the user to perform tasks which exert limitations on his speech production/physiology thereby causing speaker variability and reduced speech system performance. This is speech under stress, which represents a speech different from speech under neutral conditions. The stress can be physical, cognitive or noise induced (Lombard). In this study, the focus is on physical stress, with specific emphasis on: (i) number of speakers used for modeling, (ii) alternative audio sensors, and (iii) fusion based stress detection using a new audio corpus (UT-Scope). We used a GMM framework with our previously formulated TEO-CB-AutoEnv features for neutral/physical stress detection. Second, stress detection performance is investigated for both acoustic and non-acoustic (P-MIC) sensors. Evaluations show that effective stress models can be obtained with 12 speakers out of a random size of 1.42 subjects, with stress detection performance of 62.96% (for close-talking mic) and 66.36% (for P-MIC) respectively. The TEO-CB-AutoEnv model scores were fused with traditional MFCC based stress model scores using the Adaboost algorithm, resulting in an improvement in overall system performance of 9.43% (absolute, for close-talking mic) and 12.99% (absolute, for PMIC) respectively. These three advances allow for effective stress detection algorithm development with fewer training speakers and/or alternative sensors in combined feature domains.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-249"
  },
  "chen08b_interspeech": {
   "authors": [
    [
     "Langzhou",
     "Chen"
    ],
    [
     "Hisayoshi",
     "Nagae"
    ],
    [
     "Matt",
     "Stuttle"
    ]
   ],
   "title": "Improving Japanese language models using POS information",
   "original": "i08_0821",
   "page_count": 4,
   "order": 276,
   "p1": "821",
   "pn": "824",
   "abstract": [
    "In this paper, part-of-speech (POS) information is used to improve the performance of a Japanese language model (LM). The POS bigram is used to tackle the sparseness problem of the training data. Additionally, due to the characteristics of the Japanese language, part of the Japanese syntax information can be integrated into the POS bigram, through POS combination rules. Based on the Japanese syntax grammar, the POS combination rules determine if a POS pair is prohibited in Japanese language. The Japanese POS bigram table not only includes the POS pairs that occurred in the training corpus, but also includes all the prohibited POS pairs. The confusion in the search space can be reduced by explicitly modeling the prohibited POS pairs. In this work, a series of experiments have been carried out to investigate the impact of the POS bigram with prohibited POS pairs on the recognition search space. The framework of fast generation of the language model look-ahead (LMLA) probabilities based on POS bigram information is also presented in this paper. The experimental results showed that compared to the traditional word n-gram model, the LM with POS bigram information achieves significant improvement in both word accuracy and the speed of Japanese LVCSR system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-250"
  },
  "arsoy08_interspeech": {
   "authors": [
    [
     "Ebru",
     "Arısoy"
    ],
    [
     "Brian",
     "Roark"
    ],
    [
     "Izhak",
     "Shafran"
    ],
    [
     "Murat",
     "Saraçlar"
    ]
   ],
   "title": "Discriminative n-gram language modeling for Turkish",
   "original": "i08_0825",
   "page_count": 4,
   "order": 277,
   "p1": "825",
   "pn": "828",
   "abstract": [
    "In this paper Discriminative Language Models (DLMs) are applied to the Turkish Broadcast News transcription task. Turkish presents a challenge to Automatic Speech Recognition (ASR) systems due to its rich morphology. Therefore, in addition to word n-gram features, morphology based features like root n-grams and inflectional group n-grams are incorporated into DLMs in order to improve the language models. Various feature sets provide reductions in the word error rate (WER). Our best result is obtained with the inflectional group n-gram features. 1.0% absolute improvement is achieved over the baseline model and this improvement is statistically significant at p<0.001 as measured by the NIST MAPSSWE significance test.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-251"
  },
  "emami08_interspeech": {
   "authors": [
    [
     "Ahmad",
     "Emami"
    ],
    [
     "Imed",
     "Zitouni"
    ],
    [
     "Lidia",
     "Mangu"
    ]
   ],
   "title": "Rich morphology based n-gram language models for Arabic",
   "original": "i08_0829",
   "page_count": 4,
   "order": 278,
   "p1": "829",
   "pn": "832",
   "abstract": [
    "In this paper we investigate the use of rich morphology such as word segmentation, part-of-speech tagging and diacritic restoration to improve Arabic language modeling. We enrich the context by performing morphological analysis on the word history. We use neural network models to integrate this additional information, due to their ability to handle long and enriched dependencies. We experimented with models with increasing order of morphological features, starting with Arabic segmentation, and later adding part of speech labels as well as words with restored diacritics. Experiments on Arabic broadcast news and broadcast conversations data showed significant improvements in perplexity, reducing the baseline N-gram and the neural network N-gram model perplexities by 35% and 31% respectively.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-252"
  },
  "huang08b_interspeech": {
   "authors": [
    [
     "Songfang",
     "Huang"
    ],
    [
     "Steve",
     "Renals"
    ]
   ],
   "title": "Unsupervised language model adaptation based on topic and role information in multiparty meetings",
   "original": "i08_0833",
   "page_count": 4,
   "order": 279,
   "p1": "833",
   "pn": "836",
   "abstract": [
    "We continue our previous work on the modeling of topic and role information from multiparty meetings using a hierarchical Dirichlet process (HDP), in the context of language model adaptation. In this paper we focus on three problems: 1) an empirical analysis of the HDP as a nonparametric topic model; 2) the mismatch problem of vocabularies of the baseline n-gram model and the HDP; and 3) an automatic speech recognition experiment to further verify the effectiveness of our adaptation framework. Experiments on a large meeting corpus of more than 70 hours speech data show consistent and significant improvements in terms of word error rate for language model adaptation based on the topic and role information.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-253"
  },
  "liu08c_interspeech": {
   "authors": [
    [
     "X.",
     "Liu"
    ],
    [
     "M. J. F.",
     "Gales"
    ],
    [
     "P. C.",
     "Woodland"
    ]
   ],
   "title": "Context dependent language model adaptation",
   "original": "i08_0837",
   "page_count": 4,
   "order": 280,
   "p1": "837",
   "pn": "840",
   "abstract": [
    "Language models (LMs) are often constructed by building multiple component LMs that are combined using interpolation weights. By tuning these interpolation weights, using either perplexity or discriminative approaches, it is possible to adapt LMs to a particular task. In this work, improved LM adaptation is achieved by introducing context dependent interpolation weights. An important part of this new approach is obtaining robust estimation. Two schemes for this are described. The first is based on MAP estimation, where either global interpolation weights are used as priors, or context dependent interpolation priors obtained from the training data. The second scheme uses class based contexts to determine the interpolation weights. Both schemes are evaluated using unsupervised LM adaptation on a Mandarin broadcast transcription task. Consistent gains in perplexity using context dependent, rather than global, weights are observed as well as reductions in character error rate.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-254"
  },
  "hsu08_interspeech": {
   "authors": [
    [
     "Bo-June",
     "Hsu"
    ],
    [
     "James",
     "Glass"
    ]
   ],
   "title": "Iterative language model estimation: efficient data structure & algorithms",
   "original": "i08_0841",
   "page_count": 4,
   "order": 281,
   "p1": "841",
   "pn": "844",
   "abstract": [
    "Despite the availability of better performing techniques, most language models are trained using popular toolkits that do not support perplexity optimization. In this work, we present an efficient data structure and optimized algorithms specifically designed for iterative parameter tuning. With the resulting implementation, we demonstrate the feasibility and effectiveness of such iterative techniques in language model estimation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-255"
  },
  "ohta08_interspeech": {
   "authors": [
    [
     "Kengo",
     "Ohta"
    ],
    [
     "Masatoshi",
     "Tsuchiya"
    ],
    [
     "Seiichi",
     "Nakagawa"
    ]
   ],
   "title": "Evaluating spoken language model based on filler prediction model in speech recognition",
   "original": "i08_1558",
   "page_count": 4,
   "order": 282,
   "p1": "1558",
   "pn": "1561",
   "abstract": [
    "We propose a method that uses a filler prediction model for building a language model that includes fillers from a corpus without fillers. In our method, a filler prediction model is trained from a corpus that does not cover domain-relevant topics. It recovers fillers in inexact transcribed corpora in the target domain, and then a language model that includes fillers is built from the corpora. The results of an evaluation of the Japanese National Diet Record showed that a model using our method achieves higher recognition performance than conventional ones.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-256"
  },
  "duta08_interspeech": {
   "authors": [
    [
     "Nicolae",
     "Duta"
    ]
   ],
   "title": "Transcription-less call routing using unsupervised language model adaptation",
   "original": "i08_1562",
   "page_count": 4,
   "order": 283,
   "p1": "1562",
   "pn": "1565",
   "abstract": [
    "A key challenge when building call routing applications is the need for an extensive set of in-domain data that is manually transcribed and labeled, a process which is both expensive and time consuming. In this paper we analyze a Language Model training approach based on unsupervised self-adaptation which does not require any manual transcriptions of the in-domain audio data. We investigate the usefulness of several sources of language data for building bootstrapped LMs as well as an utterance duration dependent adaptation scheme which balances the required computational resources. Results on deployed call routing applications show that the routing accuracy obtained using the self-adapted LM is within 1.5% absolute of the accuracy of the system trained on manual transcriptions irrespective of the original bootstrapped LMs.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-257"
  },
  "pan08_interspeech": {
   "authors": [
    [
     "Zhen-Yu",
     "Pan"
    ],
    [
     "Hui",
     "Jiang"
    ]
   ],
   "title": "Large margin multinomial mixture model for text categorization",
   "original": "i08_1566",
   "page_count": 4,
   "order": 284,
   "p1": "1566",
   "pn": "1569",
   "abstract": [
    "In this paper, we present a novel discriminative training method for multinomial mixture models (MMM) in text categorization based on the principle of large margin. Under some approximation and relaxation conditions, large margin estimation (LME) of MMMs can be formulated as linear programming (LP) problems, which can be efficiently and reliably solved by many general optimization tools even for very large models. The text categorization experiments on the standard RCV1 text corpus show that the LME method of MMMs can largely improve classification accuracy over the traditional training method based on the EM algorithm. Comparing with the EM method, the proposed LME method can achieve over 20% relative error reduction on three independent test sets of RCV1.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-258"
  },
  "yeung08_interspeech": {
   "authors": [
    [
     "Yu Ting",
     "Yeung"
    ],
    [
     "Houwei",
     "Cao"
    ],
    [
     "N. H.",
     "Zheng"
    ],
    [
     "Tan",
     "Lee"
    ],
    [
     "P. C.",
     "Ching"
    ]
   ],
   "title": "Language modeling for speech recognition of spoken Cantonese",
   "original": "i08_1570",
   "page_count": 4,
   "order": 285,
   "p1": "1570",
   "pn": "1573",
   "abstract": [
    "This paper addresses the problem of language modeling for LVCSR of Cantonese spoken in daily communication. As a spoken dialect, Cantonese is not used in written documents and published materials. Thus it is difficult to collect sufficient amount of written Cantonese text data for the training of statistical language models. We propose to solve this problem by translating standard Chinese text, which is much easier to find, into written Cantonese. A rulebased method of translation is devised and implemented. Three different language models are trained from different types of text. They are evaluated in the task of LVCSR. Experimental results confirm that the translated text can well represent Cantonese spoken in formal occasions like broadcast news. For colloquial Cantonese, language model adaptation with a limited amount of colloquial Cantonese text data would be a practically feasible solution that leads to reasonable speech recognition performance.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-259"
  },
  "kobayashi08_interspeech": {
   "authors": [
    [
     "Akio",
     "Kobayashi"
    ],
    [
     "Takahiro",
     "Oku"
    ],
    [
     "Shinichi",
     "Homma"
    ],
    [
     "Shoei",
     "Sato"
    ],
    [
     "Toru",
     "Imai"
    ],
    [
     "Tohru",
     "Takagi"
    ]
   ],
   "title": "Discriminative rescoring based on minimization of word errors for transcribing broadcast news",
   "original": "i08_1574",
   "page_count": 4,
   "order": 286,
   "p1": "1574",
   "pn": "1577",
   "abstract": [
    "This paper describes a novel method of rescoring that reflects tendencies of errors in word hypotheses in speech recognition for transcribing broadcast news, including ill-trained spontaneous speech. The proposed rescoring assigns penalties to sentence hypotheses according to the recognition error tendencies in the training lattices themselves using a set of weighting factors for feature functions activated by a variety of linguistic contexts. Word hypotheses with low possibilities of correct words are penalized while those with high possibilities are rewarded by the weighting factors. We introduce two types of training techniques to obtain the factors. The first is based on conditional random fields (CRFs), and the second is based on the minimization of word errors, which explicitly reduces expected word errors. The results of transcribing Japanese broadcast news achieved a word error rate (WER) of 10.38%, which was a 6.06% reduction relative to conventional lattice rescoring.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-260"
  },
  "shi08b_interspeech": {
   "authors": [
    [
     "Qin",
     "Shi"
    ],
    [
     "Stephen M.",
     "Chu"
    ],
    [
     "Wen",
     "Liu"
    ],
    [
     "Hong-Kwang Jeff",
     "Kuo"
    ],
    [
     "Yi",
     "Liu"
    ],
    [
     "Yong",
     "Qin"
    ]
   ],
   "title": "Search and classification based language model adaptation",
   "original": "i08_1578",
   "page_count": 4,
   "order": 287,
   "p1": "1578",
   "pn": "1581",
   "abstract": [
    "Adaptation techniques in language modeling have shown growing potentials in improving speech recognition performance. For topic adaptation, a set of pre-defined topic-specific language models are typically used, and adaptation is achieved through adjusting the interpolation weights. However, mismatch between the test data and the pre-defined models inevitably exists and is left untreated in the static approach. Instead of tuning the parameters in the existing models, this paper describes a method that dynamically extracts relevant documents from training sources according to intermediate decoding hypotheses to build new targeted language models. Different from general search-based document collection, a new and effective ranking method is used here for candidate extraction. The targeted language models are interpolated with the static topic language models and a general language model, and used for lattice rescoring. The proposed adaptation technique is implemented in a state-of-the-art Mandarin broadcast transcription system, and evaluated on the GALE task. We show that static topic adaptation reduces the relative character error rate by 4.9%. It is further shown that the proposed dynamic adaptation technique attains an additional 10.3% reduction in error rate.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-261"
  },
  "huijbregts08_interspeech": {
   "authors": [
    [
     "Marijn",
     "Huijbregts"
    ],
    [
     "Roeland",
     "Ordelman"
    ],
    [
     "Franciska de",
     "Jong"
    ]
   ],
   "title": "Fast n-gram language model look-ahead for decoders with static pronunciation prefix trees",
   "original": "i08_1582",
   "page_count": 4,
   "order": 288,
   "p1": "1582",
   "pn": "1585",
   "abstract": [
    "Decoders that make use of token-passing restrict their search space by various types of token pruning. With use of the Language Model Look-Ahead (LMLA) technique it is possible to increase the number of tokens that can be pruned without loss of decoding precision. Unfortunately, for token passing decoders that use single static pronunciation prefix trees, full n-gram LMLA increases the needed number of language model probability calculations considerably. In this paper a method for applying full n-gram LMLA in a decoder with a single static pronunciation tree is introduced. The experiments show that this method improves the speed of the decoder without an increase of search errors.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-262"
  },
  "saykhum08_interspeech": {
   "authors": [
    [
     "Kwanchiva",
     "Saykhum"
    ],
    [
     "Vataya",
     "Boonpiam"
    ],
    [
     "Nattanun",
     "Thatphithakkul"
    ],
    [
     "Chai",
     "Wutiwiwatchai"
    ],
    [
     "Cholwich",
     "Natthee"
    ]
   ],
   "title": "Thai named-entity recognition using class-based language modeling on multiple-sized subword units",
   "original": "i08_1586",
   "page_count": 4,
   "order": 289,
   "p1": "1586",
   "pn": "1589",
   "abstract": [
    "This article investigates as an early work on speech recognition of Thai named-entities, which is a crucial out-of-vocabulary word problem in broadcast news transcription. Motivated by an analysis on Thai-name structure, a statistical class-based language model is applied on multiple-sized subword units with a constraint on subword positions. Subwords can be defined automatically by their statistics. The proposed model is evaluated on Thai person name recognition in broadcast news data. Based on the subword inventory built from a very large training set of Thai names, only 0.7% out-of-vocabulary subwords are found in the test set. The best configured system incorporating both syllable merging and subword clustering algorithms achieves an approximately 40% syllable accuracy with 25% of names fully discovered.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-263"
  },
  "schwarzler08_interspeech": {
   "authors": [
    [
     "S.",
     "Schwarzler"
    ],
    [
     "J.",
     "Geiger"
    ],
    [
     "J.",
     "Schenk"
    ],
    [
     "M.",
     "Al-Hames"
    ],
    [
     "B.",
     "Hornler"
    ],
    [
     "Günther",
     "Ruske"
    ],
    [
     "Gerhard",
     "Rigoll"
    ]
   ],
   "title": "Combining statistical and syntactical systems for spoken language understanding with graphical models",
   "original": "i08_1590",
   "page_count": 4,
   "order": 290,
   "p1": "1590",
   "pn": "1593",
   "abstract": [
    "There are two basic approaches for semantic processing in spoken language understanding: a rule based approach and a statistic approach. In this paper we combine both of them in a novel way by using statistical and syntactical dynamic bayesian networks (DBNs) together with Graphical Models (GMs) for spoken language understanding (SLU). GMs merge in a complex, mathematical way probability with graph theory. This results in four different setups which raise in their complexity. Comparing our results to a baseline system we achieve a F1-measure of 93:7% in word classes and 95:7% in concepts for our best setup in the ATIS-Task. This outperforms the baseline system relatively by 3:7% in word classes and by 8:2% in concepts. The experiments were performed with the graphical model toolkit (GMTK).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-264"
  },
  "sethy08_interspeech": {
   "authors": [
    [
     "Abhinav",
     "Sethy"
    ],
    [
     "Bhuvana",
     "Ramabhadran"
    ]
   ],
   "title": "Bag-of-word normalized n-gram models",
   "original": "i08_1594",
   "page_count": 4,
   "order": 291,
   "p1": "1594",
   "pn": "1597",
   "abstract": [
    "The Bag-Of-Word (BOW) model uses a fixed length vector of word counts to represent text. Although the model disregards word sequence information, it has been shown to be successful in capturing long range word-word correlations and topic information. In contrast, n-gram models have been shown to be an effective way to capture short term dependencies by modeling text as a Markovian sequence. In this paper, we propose a probabilistic framework to combine BOW models with n-gram models. In the proposed framework, we normalize the n-gram model to build a model for word sequences given the corresponding bag-of-words representation. By combining the two models, the proposed approach allows us to capture the latent topic information as well as local Markovian dependencies in text. Using the proposed model, we were able to achieve a 10% reduction in perplexity and a 2% reduction in WER (relative) over a state-of-the-art baseline for transcribing broadcast news in English.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-265"
  },
  "hahn08b_interspeech": {
   "authors": [
    [
     "Sangyun",
     "Hahn"
    ],
    [
     "Abhinav",
     "Sethy"
    ],
    [
     "Hong-Kwang Jeff",
     "Kuo"
    ],
    [
     "Bhuvana",
     "Ramabhadran"
    ]
   ],
   "title": "A study of unsupervised clustering techniques for language modeling",
   "original": "i08_1598",
   "page_count": 4,
   "order": 292,
   "p1": "1598",
   "pn": "1601",
   "abstract": [
    "There has been recent interest in clustering text data to build topic-specific language models for large vocabulary speech recognition. In this paper, we studied various unsupervised clustering algorithms on several corpora. First we compared the clustering methods with quality metrics such as entropy and purity. Of the techniques studied, two-phase bisecting K-means achieved good performance with relatively fast speed. Then we performed speech recognition experiments on English and Arabic systems using the automatically derived topic-based language models. We obtained modest word error rate improvements, comparable to previously published studies. A careful analysis of the correlation between word error rate and the distribution of misrecognized words, including an information-gain metric, is presented.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-266"
  },
  "martins08_interspeech": {
   "authors": [
    [
     "Ciro",
     "Martins"
    ],
    [
     "António",
     "Teixeira"
    ],
    [
     "João",
     "Neto"
    ]
   ],
   "title": "Automatic estimation of language model parameters for unseen words using morpho-syntactic contextual information",
   "original": "i08_1602",
   "page_count": 4,
   "order": 293,
   "p1": "1602",
   "pn": "1605",
   "abstract": [
    "Various information sources naturally contains new words that appear in a daily basis and which are not present in the vocabulary of the speech recognition system but are important for applications such as closed-captioning or information dissemination. To be recognized, those words need to be included in the vocabulary and the language model (LM) parameters updated. In this context, we propose a new method that allows including new words in the vocabulary even if no well suited training data is available, as is the case of archived documents, and without the need of LM retraining. It uses morpho-syntatic information about an in-domain corpus and part-of-speech word classes to define a new LM unigram distribution associated to the updated vocabulary. Experiments were carried out for a European Portuguese Broadcast News transcription system. Results showed a relative reduction of 4% in word error rate, with 78% of the occurrences of those newly included words being correctly recognized.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-267"
  },
  "ward08_interspeech": {
   "authors": [
    [
     "Nigel G.",
     "Ward"
    ],
    [
     "Alejandro",
     "Vega"
    ]
   ],
   "title": "Modeling the effects on time-into-utterance on word probabilities",
   "original": "i08_1606",
   "page_count": 4,
   "order": 294,
   "p1": "1606",
   "pn": "1609",
   "abstract": [
    "Most language models treat speech as simply sequences of words, ignoring the fact that words are also events in time. This paper reports an initial exploration of how word probabilities vary with time-into-utterance, and proposes a method for using this information to improve a language model. This is done by computing the ratio of the probability of the word at a specific time to its overall unigram probability, and using this ratio to adjust the n-gram probability. On casual dialogs from Switchboard this method gave a modest reduction in perplexity.-1609\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-268"
  },
  "wang08b_interspeech": {
   "authors": [
    [
     "Ye-Yi",
     "Wang"
    ],
    [
     "Xiao",
     "Li"
    ],
    [
     "Alex",
     "Acero"
    ]
   ],
   "title": "Inductive and example-based learning for text classification",
   "original": "i08_1610",
   "page_count": 4,
   "order": 295,
   "p1": "1610",
   "pn": "1613",
   "abstract": [
    "Text classification has been widely applied to many practical tasks. Inductive models trained from labeled data are the most commonly used technique. The basic assumption underlying an inductive model is that the training data are drawn from the same distribution as the test data. However, labeling such a training set is often expensive for practical applications. On the other hand, a large amount of labeled data, which have been drawn from a different distribution, is often available in the same application domain. It is thus very desirable to take advantage of these data even though there is a discrepancy between their underlying distribution and that of the test set. This paper compares three text classification algorithms applied in this scenario, including two inductive Maximum Entropy (MaxEnt) models, one flatly initialized and the other initialized with a term-frequency/inverse document frequency (Tf.Idf) weighted vector space model, and an examplebased learning algorithm, which assigns a class label to a text by learning from the labels assigned to the training data that are similar to the text. Experiment results show that example-based learning has achieved more than 5% improvement in precisions across almost all coverage levels.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-269"
  },
  "wilson08b_interspeech": {
   "authors": [
    [
     "Theresa",
     "Wilson"
    ],
    [
     "Stephan",
     "Raaijmakers"
    ]
   ],
   "title": "Comparing word, character, and phoneme n-grams for subjective utterance recognition",
   "original": "i08_1614",
   "page_count": 4,
   "order": 296,
   "p1": "1614",
   "pn": "1617",
   "abstract": [
    "In this paper, we compare the performance of classifiers trained using word n-grams, character n-grams, and phoneme n-grams for recognizing subjective utterances in multiparty conversation. We show that there is value in using very shallow linguistic representations, such as character n-grams, for recognizing subjective utterances, in particular, gains in the recall of subjective utterances.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-270"
  },
  "federico08_interspeech": {
   "authors": [
    [
     "Marcello",
     "Federico"
    ],
    [
     "Nicola",
     "Bertoldi"
    ],
    [
     "Mauro",
     "Cettolo"
    ]
   ],
   "title": "IRSTLM: an open source toolkit for handling large scale language models",
   "original": "i08_1618",
   "page_count": 4,
   "order": 297,
   "p1": "1618",
   "pn": "1621",
   "abstract": [
    "Research in speech recognition and machine translation is boosting the use of large scale n-gram language models. We present an open source toolkit that permits to efficiently handle language models with billions of n-grams on conventional machines. The IRSTLM toolkit supports distribution of n-gram collection and smoothing over a computer cluster, language model compression through probability quantization, lazy-loading of huge language models from disk. IRSTLM has been so far successfully deployed with the Moses toolkit for statistical machine translation and with the FBK-irst speech recognition system. Efficiency of the tool is reported on a speech transcription task of Italian political speeches using a language model of 1.1 billion four-grams.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-271"
  },
  "kajarekar08_interspeech": {
   "authors": [
    [
     "Sachin S.",
     "Kajarekar"
    ]
   ],
   "title": "Phone-based cepstral polynomial SVM system for speaker recognition",
   "original": "i08_0845",
   "page_count": 4,
   "order": 298,
   "p1": "845",
   "pn": "848",
   "abstract": [
    "We have been using a phone-based cepstral system with polynomial features in NIST evaluations for the past two years. This system uses three broad phone classes, three states per class, and third-order polynomial features obtained from MFCC features. In this paper, we present a complete analysis of the system. We start from a simpler system that does not use phones or states and show that the addition of phones gives a significant improvement. We show that adding state information does not provide improvement on its own but provides a significant improvement when used with phone classes. We complete the system by applying nuisance attribute projection (NAP) and score normalization. We show that splitting features after a joint NAP over all phone classes results in a significant improvement. Overall, we obtain about 25% performance improvement with polynomial features based on phones and states, and obtain a system with performance comparable to a state-of-the-art SVM system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-272"
  },
  "zhu08_interspeech": {
   "authors": [
    [
     "Donglai",
     "Zhu"
    ],
    [
     "Bin",
     "Ma"
    ],
    [
     "Haizhou",
     "Li"
    ]
   ],
   "title": "Using MAP estimation of feature transformation for speaker recognition",
   "original": "i08_0849",
   "page_count": 4,
   "order": 299,
   "p1": "849",
   "pn": "852",
   "abstract": [
    "We propose to use a new feature transformation (FT) function to construct supervectors of support vector machines for speaker recognition. Considering that estimation of bias vectors is more robust than that of transformation matrices, we define the FT function in a flexible form that transformation matrices and bias vectors are controlled by separate regression classes. Unlike the MLLR-based approach that needs a continuous speech recognition system, our FT function parameters are estimated based on a Gaussian mixture model (GMM). An iterative training procedure is used to achieve the maximum a posteriori estimation of the FT function parameters, which avoids the possible numerical problem caused by insufficient training data in the maximum likelihood estimation. Our approach is evaluated on the SRE2006 NIST evaluation and obtains better performance than a conventional SVM system based on GMM mean supervectors.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-273"
  },
  "vogt08_interspeech": {
   "authors": [
    [
     "Robbie",
     "Vogt"
    ],
    [
     "Brendan",
     "Baker"
    ],
    [
     "Sridha",
     "Sridharan"
    ]
   ],
   "title": "Factor analysis subspace estimation for speaker verification with short utterances",
   "original": "i08_0853",
   "page_count": 4,
   "order": 300,
   "p1": "853",
   "pn": "856",
   "abstract": [
    "Training the speaker and session subspaces is an integral problem in developing a joint factor analysis GMM speaker verification system. This work investigates and compares several alternative procedures for this task with a particular focus on training and testing with short utterances. Experiments show that better performance can be obtained when an independent rather than simultaneous optimisation of the two core variability subspaces is used. It is additionally shown that for verification trials on short utterances it is important for the session subspace to be trained with matched length utterances. Conversely, the speaker transform should always be trained with as much data as possible.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-274"
  },
  "mclaren08_interspeech": {
   "authors": [
    [
     "Mitchell",
     "McLaren"
    ],
    [
     "Driss",
     "Matrouf"
    ],
    [
     "Robbie",
     "Vogt"
    ],
    [
     "Jean-François",
     "Bonastre"
    ]
   ],
   "title": "Combining continuous progressive model adaptation and factor analysis for speaker verification",
   "original": "i08_0857",
   "page_count": 4,
   "order": 301,
   "p1": "857",
   "pn": "860",
   "abstract": [
    "This paper proposes a novel technique of incorporating factoranalysis- based inter-session variability (ISV) modelling in speaker verification systems that employ continuous progressive speaker model adaptation. Continuous model adaptation involves the use of all encountered trials in the adaptation process through the assignment of confidence measures. The proposed approach incorporates these confidence measures in the general statistics used in the ISV modelling process. Progressive SVM-based classification was integrated into the system through the utilisation of GMM mean supervectors. The proposed system demonstrated a gain of 50% over baseline results when trialled on the NIST 2005 SRE corpus. Adaptative score normalisation techniques were found to be beneficial to both GMM and SVM configurations alleviating the detrimental effects of score shift in progressive systems.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-275"
  },
  "hsieh08_interspeech": {
   "authors": [
    [
     "Chia-Hsin",
     "Hsieh"
    ],
    [
     "Chung-Hsien",
     "Wu"
    ],
    [
     "Han-Ping",
     "Shen"
    ]
   ],
   "title": "Adaptive decision tree-based phone cluster models for speaker clustering",
   "original": "i08_0861",
   "page_count": 4,
   "order": 302,
   "p1": "861",
   "pn": "864",
   "abstract": [
    "This study presents an approach to speaker clustering using adaptive decision tree-based phone cluster models (DT-PCMs). First, a large broadcast news database is used to train a set of phone models for universal speakers. The multi-space probability distributed-hidden Markov model (MSD-HMM) is adopted for phone modeling. Confusing phone models are merged into phone clusters. Next, for each state in the phone MSD-HMMs, a decision tree is constructed to store the contextual, phonetic, and speaker characteristics for data sharing over all speakers. For speaker clustering, each input speech segment is used to retrieve the Gaussian models from the DT-PCMs to construct the initial speaker-dependent phone cluster models. Finally, all the corresponding adapted speaker-dependent phone cluster models are used for speaker clustering via a cross-likelihood ratio measure. The experimental results show the DT-PCMs outperforms the conventional GMM-based approach.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-276"
  },
  "aronowitz08_interspeech": {
   "authors": [
    [
     "Hagai",
     "Aronowitz"
    ],
    [
     "Yosef A.",
     "Solewicz"
    ]
   ],
   "title": "Speaker recognition in two-wire test sessions",
   "original": "i08_0865",
   "page_count": 4,
   "order": 303,
   "p1": "865",
   "pn": "868",
   "abstract": [
    "This paper deals with the task of speaker recognition in four-wire training and two-wire testing conditions. Instead of performing blind speaker diarization before the recognition stage, we directly perform the recognition on the non-segmented (or imperfectly diarized) speech. We present an analysis of the problem with respect to three different speaker recognition systems and propose improved recognition techniques both in the frame domain and in the model domain. The proposed techniques reduce error rate significantly. Furthermore, the developed techniques may be also beneficial in conjunction with an imperfect blind diarization stage.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-277"
  },
  "bishop08_interspeech": {
   "authors": [
    [
     "Jason B.",
     "Bishop"
    ]
   ],
   "title": "The effect of position on the realization of second occurrence focus",
   "original": "i08_0869",
   "page_count": 4,
   "order": 304,
   "p1": "869",
   "pn": "872",
   "abstract": [
    "Previous authors have suggested that prenuclear second occurrence foci may be marked both by higher pitch and increased duration, unlike their post-nuclear counterparts which show only increased duration. Results are presented from a production study that finds no evidence of pitch prominence for prenuclear second occurrence foci. Given evidence from German, it appears that the realization of second occurrence focus may also be sensitive to position within the prenuclear domain.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-278"
  },
  "shue08_interspeech": {
   "authors": [
    [
     "Yen-Liang",
     "Shue"
    ],
    [
     "Stefanie",
     "Shattuck-Hufnagel"
    ],
    [
     "Markus",
     "Iseli"
    ],
    [
     "Sun-Ah",
     "Jun"
    ],
    [
     "Nanette",
     "Veilleux"
    ],
    [
     "Abeer",
     "Alwan"
    ]
   ],
   "title": "Effects of intonational phrase boundaries on pitch-accented syllables in american English",
   "original": "i08_0873",
   "page_count": 4,
   "order": 305,
   "p1": "873",
   "pn": "876",
   "abstract": [
    "Recent studies of the acoustic correlates of various prosodic elements in American English, such as prominence (in the form of phrase-level pitch accents and word-level lexical stress) and boundaries (in the form of boundary-marking tones), have begun to clarify the nature of the acoustic cues to different types and levels of these prosodic markers. This study focuses on the importance of controlling for context in such investigations, illustrating the effects of adjacent context by examining the cues to H. and L. pitch accent in early and late position in the Intonational Phrase, and how these cues vary when the accented syllable is followed immediately by boundary tones. Results show that F0 peaks for H. accents occur significantly earlier in words that also carry boundary tones, and that energy patterns are also affected; some effects on voice quality measures were also noted. Such findings highlight the caveat that the context of a particular prosodic target may significantly influence its acoustic correlates.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-279"
  },
  "walsh08_interspeech": {
   "authors": [
    [
     "Michael",
     "Walsh"
    ],
    [
     "Katrin",
     "Schweitzer"
    ],
    [
     "Bernd",
     "Möbius"
    ],
    [
     "Hinrich",
     "Schütze"
    ]
   ],
   "title": "Examining pitch-accent variability from an exemplar-theoretic perspective",
   "original": "i08_0877",
   "page_count": 4,
   "order": 306,
   "p1": "877",
   "pn": "880",
   "abstract": [
    "This paper presents an exemplar-theoretic account of pitch-accent variability. Results from two experiments are reported. The first experiment examines variability / similarity across a range of pitch-accent contour types which vary in frequency of occurrence. The results report similar behaviour across the frequency bins and are indicative of pitch-accent immunity from frequency effects. The second experiment, in order to establish the impact of lexical frequency on pitch-accent production, examines the similarity of pitch-accent contours linked to high and low frequency syllables. The results indicate further autonomy on the part of pitch-accents and offer possible evidence for post-lexical pitch-accent generation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-280"
  },
  "hakokari08_interspeech": {
   "authors": [
    [
     "Jussi",
     "Hakokari"
    ],
    [
     "Tuomo",
     "Saarni"
    ],
    [
     "Jouni",
     "Isoaho"
    ],
    [
     "Tapio",
     "Salakoski"
    ]
   ],
   "title": "Correlation of utterance length and segmental duration in Finnish is questionable",
   "original": "i08_0881",
   "page_count": 4,
   "order": 307,
   "p1": "881",
   "pn": "884",
   "abstract": [
    "This paper examines the way acoustic segmental duration correlates with utterance length in Finnish. It is commonly assumed that shorter utterances are characterized by greater segmental duration. However, that view has recently attracted some criticism. We conducted an explorative study on two linguistically uncontrolled Finnish-language speech corpora by examining segmental duration as a function of utterance length. We tested a hypothesis that the perceived differences in duration are in fact caused by short utterances containing a greater proportion of domain-edge effects, such as final lengthening. Pearson correlations were calculated for the timing information in different conditions. The results show that the weak correlation holds no more if domain edges are excluded from the material, suggesting there is no domain-span process at work.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-281"
  },
  "yuan08_interspeech": {
   "authors": [
    [
     "Jiahong",
     "Yuan"
    ],
    [
     "Stephen",
     "Isard"
    ],
    [
     "Mark",
     "Liberman"
    ]
   ],
   "title": "Different roles of pitch and duration in distinguishing word stress in English",
   "original": "i08_0885",
   "page_count": 1,
   "order": 308,
   "p1": "885",
   "pn": "",
   "abstract": [
    "Our study investigated the pitch and duration properties of word stress in a large speech corpus. We concluded that pitch and duration play different roles in distinguishing word-level stress classes. In the case of pitch, primary-stress vowels were different from secondary-stress and reduced vowels; in the case of duration, reduced vowels were different from the other two types of vowels.\n",
    ""
   ]
  },
  "oreilly08_interspeech": {
   "authors": [
    [
     "Maria",
     "O'Reilly"
    ],
    [
     "Ailbhe Ní",
     "Chasaide"
    ],
    [
     "Christer",
     "Gobl"
    ]
   ],
   "title": "Cross-dialect Irish prosody: linguistic constraints on Fujisaki modelling",
   "original": "i08_0886",
   "page_count": 4,
   "order": 309,
   "p1": "886",
   "pn": "889",
   "abstract": [
    "We describe here our approach to quantifying cross-dialect differences in Irish Gaelic, using the Fujisaki model. The basic principle is that the way in which the modelling is carried out respects a parallel linguistic (AM) analysis. The aims are: (1) to ensure that our modelling strategies permit a reliable cross-dialect comparison, (2) that the model-derived measurements can be related to meaningful linguistic dimensions and (3) that the analysis forms the basis for multi-dialect synthesis.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-282"
  },
  "nakayama08_interspeech": {
   "authors": [
    [
     "Masato",
     "Nakayama"
    ],
    [
     "Takanobu",
     "Nishiura"
    ],
    [
     "Yuki",
     "Denda"
    ],
    [
     "Norihide",
     "Kitaoka"
    ],
    [
     "Kazumasa",
     "Yamamoto"
    ],
    [
     "Takeshi",
     "Yamada"
    ],
    [
     "Satoru",
     "Tsuge"
    ],
    [
     "Chiyomi",
     "Miyajima"
    ],
    [
     "Masakiyo",
     "Fujimoto"
    ],
    [
     "Tetsuya",
     "Takiguchi"
    ],
    [
     "Satoshi",
     "Tamura"
    ],
    [
     "Tetsuji",
     "Ogawa"
    ],
    [
     "Shigeki",
     "Matsuda"
    ],
    [
     "Shingo",
     "Kuroiwa"
    ],
    [
     "Kazuya",
     "Takeda"
    ],
    [
     "Satoshi",
     "Nakamura"
    ]
   ],
   "title": "CENSREC-4: development of evaluation framework for distant-talking speech recognition under reverberant environments",
   "original": "i08_0968",
   "page_count": 4,
   "order": 310,
   "p1": "968",
   "pn": "971",
   "abstract": [
    "In this paper, we newly introduce a collection of databases and evaluation tools called CENSREC-4, which is an evaluation framework for distant-talking speech under hands-free conditions. Distant-talking speech recognition is crucial for a hands-free speech interface. Therefore, we measured room impulse responses to investigate reverberant speech recognition in various environments. The data contained in CENSREC-4 are connected digit utterances, as in CENSREC-1. Two subsets are included in the data: basic data sets and extra data sets. The basic data sets are used for the evaluation environment for the room impulse response-convolved speech data. The extra data sets consist of simulated and recorded data. An evaluation framework is only provided for the basic data sets as evaluation tools. The results of evaluation experiments proved that CENSREC-4 is an effective database for evaluating the new dereverberation method because the traditional dereverberation process had difficulty sufficiently improving the recognition performance.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-283"
  },
  "tsujikawa08_interspeech": {
   "authors": [
    [
     "Masanori",
     "Tsujikawa"
    ],
    [
     "Takayuki",
     "Arakawa"
    ],
    [
     "Ryosuke",
     "Isotani"
    ]
   ],
   "title": "In-car speech recognition using model-based wiener filter and multi-condition training",
   "original": "i08_0972",
   "page_count": 4,
   "order": 311,
   "p1": "972",
   "pn": "975",
   "abstract": [
    "This paper presents in-car speech recognition using a model-based Wiener filter (MBW) and multi-condition (MC) training. The MBW is a 2-step denoising algorithm based on both rough and precise estimation of speech signals. Correcting roughly estimated signals with a Gaussian mixture model (GMM) makes it possible to accurately denoise with little computational cost. In an evaluation of in-car speech recognition, training of both a GMM and a back-end hidden Markov model (HMM) was performed using both studio-recorded speech signals as well as those signals mixed with in-car noise signals that were recorded in real car environments. In-car speech signals for testing were recorded with a plurality of microphones in different car environments. With respect to word accuracy obtained with MC-trained HMM, it was confirmed that the MBW with MC-trained GMM outperformed the Noise Reduction in ETSI advanced front-end.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-284"
  },
  "kuhne08_interspeech": {
   "authors": [
    [
     "Marco",
     "Kühne"
    ],
    [
     "Roberto",
     "Togneri"
    ],
    [
     "Sven",
     "Nordholm"
    ]
   ],
   "title": "Adaptive beamforming and soft missing data decoding for robust speech recognition in reverberant environments",
   "original": "i08_0976",
   "page_count": 4,
   "order": 312,
   "p1": "976",
   "pn": "979",
   "abstract": [
    "This paper presents a novel approach to combine microphone array processing and robust speech recognition for reverberant multi-speaker environments. Spatial cues are extracted from a microphone array and automatically clustered to estimate localization masks in the time-frequency domain. The localization masks are then used to blindly design adaptive filters in order to enhance the source signals prior to missing data speech recognition. A novel evidence model better exploiting the information provided by the source separation stage is proposed. Recognition experiments demonstrate the effectiveness of the scheme when compared to traditional microphone array enhancement and a related binaural separation model.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-285"
  },
  "babaali08_interspeech": {
   "authors": [
    [
     "Bagher",
     "BabaAli"
    ],
    [
     "Hossein",
     "Sameti"
    ],
    [
     "Mehran",
     "Safayani"
    ]
   ],
   "title": "Spectral subtraction in likelihood-maximizing framework for robust speech recognition",
   "original": "i08_0980",
   "page_count": 4,
   "order": 313,
   "p1": "980",
   "pn": "983",
   "abstract": [
    "Spectral Subtraction (SS), as a speech enhancement technique, originally designed for improving quality of speech signal judged by human listeners. it usually improve the quality and intelligibility of speech signals, while the speech recognition systems need compensation techniques capable of reducing the mismatch between the noisy speech features and the clean models. This paper proposes a novel approach for solving this problem by considering the SS and the speech recognizer as two interconnected components, sharing the common goal of improved speech recognition accuracy. The experimental evaluations on a real recorded database and the TIMIT database show that the proposed method can achieve significant improvement in recognition rate across a wide range of the signal to noise ratios.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-286"
  },
  "ganapathy08b_interspeech": {
   "authors": [
    [
     "Sriram",
     "Ganapathy"
    ],
    [
     "Samuel",
     "Thomas"
    ],
    [
     "Hynek",
     "Hermansky"
    ]
   ],
   "title": "Front-end for far-field speech recognition based on frequency domain linear prediction",
   "original": "i08_0984",
   "page_count": 4,
   "order": 314,
   "p1": "984",
   "pn": "987",
   "abstract": [
    "Automatic Speech Recognition (ASR) systems usually fail when they encounter speech from far-field microphone in reverberant environments. This is due to the application of short-term feature extraction techniques which do not compensate for the artifacts introduced by long room impulse responses. In this paper, we propose a front-end, based on Frequency Domain Linear Prediction (FDLP), that tries to remove reverberation artifacts present in far-field speech. Long temporal segments of far-field speech are analyzed in narrow frequency sub-bands to extract FDLP envelopes and residual signals. Filtering the residual signals with gain normalized inverse FDLP filters result in a set of sub-band signals which are synthesized to reconstruct the signal back. ASR experiments on far-field speech data processed by the proposed front-end show significant improvements (relative reduction of 30% in word error rate) compared to other robust feature extraction techniques.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-287"
  },
  "park08_interspeech": {
   "authors": [
    [
     "Ji Hun",
     "Park"
    ],
    [
     "Jae Sam",
     "Yoon"
    ],
    [
     "Hong Kook",
     "Kim"
    ]
   ],
   "title": "Mask estimation incorporating time-frequency trajectories for a CASA-based ASR front-end",
   "original": "i08_0988",
   "page_count": 4,
   "order": 315,
   "p1": "988",
   "pn": "991",
   "abstract": [
    "In this paper, we propose a mask estimation method for a computational auditory scene analysis (CASA) based speech recognition front-end using speech obtained from two microphones. The proposed mask estimation method incorporates the observation that the mask information should be correlated over contiguous analysis time frames and adjacent frequency channels. To this end, two different hidden Markov models (HMMs), time HMM and frequency HMM, representing the time and frequency trajectories respectively, are trained using features such as the interaural time difference and the interaural level difference of two-channel signals. A mask for the given time-frequency bin is estimated by combining the likelihoods estimated from the two HMMs, and used to separate the desired speech from noisy speech. To show the effectiveness of the proposed mask estimation, we first measure the root mean square error between the ideal mask and that estimated by the proposed method. Then, we compare the performance of a speech recognition system using the proposed mask estimation method to those using conventional methods. Consequently, the proposed method provides an average word error rate reduction of 63.2% and 3.1% when compared with the Gaussian kernel-based and time HMM-based mask estimation methods, respectively.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-288"
  },
  "takahashi08_interspeech": {
   "authors": [
    [
     "Toru",
     "Takahashi"
    ],
    [
     "Shun'ichi",
     "Yamamoto"
    ],
    [
     "Kazuhiro",
     "Nakadai"
    ],
    [
     "Kazunori",
     "Komatani"
    ],
    [
     "Tetsuya",
     "Ogata"
    ],
    [
     "Hiroshi G.",
     "Okuno"
    ]
   ],
   "title": "Soft missing-feature mask generation for simultaneous speech recognition system in robots",
   "original": "i08_0992",
   "page_count": 4,
   "order": 316,
   "p1": "992",
   "pn": "995",
   "abstract": [
    "This paper addresses automatic soft missing-feature mask (MFM) generation based on a leak energy estimation for a simultaneous speech recognition system. An MFM is used as a weight for probability calculation in a recognition process. In a previous work, a threshold-base-zero-or-one function was applied to decide if spectral parameter can be reliable or not for each frequency bin. The function is extended into a weighted sigmoid function which has two free parameters. In addition, a contribution ratio of static features is introduced for the probability calculation in a recognition process which static and dynamic features are input. The ratio can be implemented as a part of soft mask. The average recognition rate based on a soft MFM improved by about 5% for all directions from a conventional system based on a hard MFM. Word recognition rates improved from 70 to 80% for peripheral talkers and from 93 to 97% for front speech when speakers were 90 degrees apart.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-289"
  },
  "wang08c_interspeech": {
   "authors": [
    [
     "Dong",
     "Wang"
    ],
    [
     "Ivan",
     "Himawan"
    ],
    [
     "Joe",
     "Frankel"
    ],
    [
     "Simon",
     "King"
    ]
   ],
   "title": "A posterior approach for microphone array based speech recognition",
   "original": "i08_0996",
   "page_count": 4,
   "order": 317,
   "p1": "996",
   "pn": "999",
   "abstract": [
    "Automatic speech recognition (ASR) is difficult in environments such as multiparty meetings because of adverse acoustic conditions: background noise, reverberation and cross-talk. Microphone arrays can increase ASR accuracy dramatically in such situations. However, most existing beamforming techniques use time-domain signal processing theory and are based on a geometric analysis of the relationship between sources and microphones. This limits their application, and leads to performance degradation when the geometric properties are unavailable, or heterogeneous channels are used. We present a new posterior-based approach for microphone array speech recognition. Instead of enhancing speech signals, we enhance posterior phone probabilities which are used in a tandem ANN-HMM system. Significant improvements were achieved over a single channel baseline. Combining beamforming and our method is significantly better than beamforming alone, especially in a moving speakers scenario.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-290"
  },
  "chiu08_interspeech": {
   "authors": [
    [
     "Yu-Hsiang Bosco",
     "Chiu"
    ],
    [
     "Richard M.",
     "Stern"
    ]
   ],
   "title": "Analysis of physiologically-motivated signal processing for robust speech recognition",
   "original": "i08_1000",
   "page_count": 4,
   "order": 318,
   "p1": "1000",
   "pn": "1003",
   "abstract": [
    "This paper discusses the relative impact that different stages of a popular auditory model have on improving the accuracy of automatic speech recognition in the presence of additive noise. Recognition accuracy is measured using the CMU SPHINX-III speech recognition system, and the DARPA Resource Management speech corpus for training and testing. It is shown that feature extraction based on auditory processing provides better performance in the presence of additive background noise than traditional MFCC processing and it is argued that an expansive nonlinearity in the auditory model contributes the most to noise robustness.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-291"
  },
  "sun08_interspeech": {
   "authors": [
    [
     "Liang-che",
     "Sun"
    ],
    [
     "Chang-wen",
     "Hsu"
    ],
    [
     "Lin-shan",
     "Lee"
    ]
   ],
   "title": "Evaluation of modulation spectrum equalization techniques for large vocabulary robust speech recognition",
   "original": "i08_1004",
   "page_count": 4,
   "order": 319,
   "p1": "1004",
   "pn": "1007",
   "abstract": [
    "Previous approaches for modulation spectrum equalization were evaluated only for the Aurora 2 small vocabulary task. We further apply these approaches on the Aurora 4 large vocabulary task. In the spectral histogram equalization (SHE) approach, we equalize the histogram of the modulation spectrum for each utterance to a reference histogram obtained from clean training data. In the magnitude ratio equalization (MRE) approach, we equalize the magnitude ratio of lower to higher frequency components on the modulation spectrum to a reference value also obtained from clean training data. Experimental test results indicate significant performance improvements using these approaches when cascaded with cepstral mean and variance normalization (CMVN). Cascading MRE with more advanced feature normalization approaches such as histogram equalization (HEQ) and higher-order cepstral moment normalization (HOCMN) yielded additional performance improvements.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-292"
  },
  "chen08c_interspeech": {
   "authors": [
    [
     "Yi",
     "Chen"
    ],
    [
     "Chia-yu",
     "Wan"
    ],
    [
     "Lin-shan",
     "Lee"
    ]
   ],
   "title": "Confusion-based entropy-weighted decoding for robust speech recognition",
   "original": "i08_1008",
   "page_count": 4,
   "order": 320,
   "p1": "1008",
   "pn": "1011",
   "abstract": [
    "An entropy-based feature parameter weighting scheme was proposed previously [1], in which the scores obtained from different feature parameters are weighted differently in the decoding process according to an entropy measure. In this paper, we propose a more delicate entropy measure for this purpose considering the inherent confusion among different acoustic classes. If a set of acoustic classes are easily confused, those feature parameters which can distinguish them should be emphasized. Extensive experiments with the Aurora 2 testing environment verified that this approach is equally useful for different types of features, and can be easily integrated with typical existing robust speech recognition approaches.\n",
    "",
    "",
    "Y. Chen, C.-Y. Wan, L.-S. Lee, \"Entropy-Based Feature Parameter Weighting for Robust Speech Recognition,\" ICASSP 2006.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-293"
  },
  "pettersen08_interspeech": {
   "authors": [
    [
     "Svein Gunnar",
     "Pettersen"
    ],
    [
     "Magne Hallstein",
     "Johnsen"
    ]
   ],
   "title": "Cepstral domain voice activity detection for improved noise modeling in MMSE feature enhancement for ASR",
   "original": "i08_1012",
   "page_count": 4,
   "order": 321,
   "p1": "1012",
   "pn": "1015",
   "abstract": [
    "In this paper we investigate the use of voice activity detection (VAD) for improving noise models used for cepstral domain minimum mean squared error (MMSE) filtering of noisy speech. Due to the popularity of MFCC features for speech recognition, it is useful to have VAD methods and MMSE filtering algorithms that both work in the MFCC domain. We propose a method for VAD based on the likelihood ratio test (LRT) that works directly on MFCC feature vectors. Detected noise-only frames are collected and used for creating a noise model which is then used for MMSE filtering. Finally, speech recognition is run using models trained in clean conditions. Experiments on AURORA2 show that our approach is successful in improving the noise model compared to the common approach of simply using the first few frames of each file for noise modeling, and that the proposed VAD method has performance comparable to a well-known LRT-based VAD algorithm that works in the DFT domain.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-294"
  },
  "molina08_interspeech": {
   "authors": [
    [
     "Carlos",
     "Molina"
    ],
    [
     "Nestor Becerra",
     "Yoma"
    ],
    [
     "Fernando",
     "Huenupan"
    ],
    [
     "Claudio",
     "Garreton"
    ]
   ],
   "title": "Unsupervised re-scoring of observation probability based on maximum entropy criterion by using confidence measure with telephone speech",
   "original": "i08_1016",
   "page_count": 4,
   "order": 322,
   "p1": "1016",
   "pn": "1019",
   "abstract": [
    "This paper describes a two-step Viterbi decoding based on reinforcement learning and information theory with telephone speech. The idea is to strength or weaken HMM's by using Bayes-based confidence measure (BBCM) and distances between models. If HMM's in the N-best list show a low BBCM, the second Viterbi decoding will prioritize the search on neighboring models according to their distances to the N-best HMM's. The current reinforcement learning mechanism is modeled as the linear combination of two metrics or information sources. Moreover, a criterion based on incremental conditional entropy maximization to optimize a linear combination of metrics or information sources is also presented. As shown here, the method requires only one adapting utterance and can lead to a reduction in WER as high as 10.9%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-295"
  },
  "liao08_interspeech": {
   "authors": [
    [
     "Yuan-Fu",
     "Liao"
    ],
    [
     "Chi-Hui",
     "Hsu"
    ],
    [
     "Chi-Min",
     "Yang"
    ],
    [
     "Jeng-Shien",
     "Lin"
    ],
    [
     "Sen-Chia",
     "Chang"
    ]
   ],
   "title": "Within-class feature normalization for robust speech recognition",
   "original": "i08_1020",
   "page_count": 4,
   "order": 323,
   "p1": "1020",
   "pn": "1023",
   "abstract": [
    "In this paper, a within-class feature normalization (WCFN) framework operating in transformed segment-level (instead of frame-level) super-vector space is proposed for robust speech recognition. In this framework, each segment hypothesis in a lattice is represented by a high dimensional super-vector and projected to a class-dependent lower-dimensional eigen-subspace to remove unwanted variability due to environment noise and speaker (different values of SNR, gender, types of noise and so on). The normalized super-vectors are verified by a bank of class detectors to further rescore the lattice. Experimental results on Aurora 2 multi-condition training task showed that the proposed WCFN approach achieved 7.45% average word error rate (WER). WCFN not only outperformed the multi-condition training baseline (Multi-Con., 13.72%) but also the blind ETSI advanced DSR front-end (ETSI-Adv., 8.65%), the histogram equalization (HEQ, 8.66%) and the non-blind reference model weighting (RMW, 7.29%) approaches.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-296"
  },
  "tan08_interspeech": {
   "authors": [
    [
     "Zheng-Hua",
     "Tan"
    ],
    [
     "Børge",
     "Lindberg"
    ]
   ],
   "title": "A posteriori SNR weighted energy based variable frame rate analysis for speech recognition",
   "original": "i08_1024",
   "page_count": 4,
   "order": 324,
   "p1": "1024",
   "pn": "1027",
   "abstract": [
    "This paper presents a variable frame rate (VFR) analysis method that uses an a posteriori signal-to-noise ratio (SNR) weighted energy distance for frame selection. The novelty of the method consists in the use of energy distance (instead of cepstral distance) to make it computationally efficient and the use of SNR weighting to emphasize the reliable regions in speech signals. The VFR method is applied to speech recognition in two scenarios. First, it is used for improving speech recognition performance in noisy environments. Secondly, the method is used for source coding in distributed speech recognition where the target bit rate is met by adjusting the frame rate, yielding a scalable coding scheme. Prior to recognition in the server, frames are repeated so that the original frame rate is restored. Very encouraging results are obtained for both noise robustness and source coding.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-297"
  },
  "wang08d_interspeech": {
   "authors": [
    [
     "Chieh-cheng",
     "Wang"
    ],
    [
     "Chi-an",
     "Pan"
    ],
    [
     "Jeih-weih",
     "Hung"
    ]
   ],
   "title": "Silence feature normalization for robust speech recognition in additive noise environments",
   "original": "i08_1028",
   "page_count": 4,
   "order": 325,
   "p1": "1028",
   "pn": "1031",
   "abstract": [
    "In this paper, we propose a simple yet very effective feature compensation scheme for two energy-related features, the logarithmic energy (logE) and the zeroth cepstral coefficient (c0), in order to improve their noise robustness. This compensation scheme, named silence feature normalization (SFN), uses the high-pass filtered features as the indicator for speech/non-speech classification, and then the features of non-speech frames are set to be small while those of speech frames are almost kept unchanged. In experiments conducted on the Aurora-2 database, SFN achieves a relative error reduction rate of nearly 50% from the baseline processing.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-298"
  },
  "wang08e_interspeech": {
   "authors": [
    [
     "L.",
     "Wang"
    ],
    [
     "Seiichi",
     "Nakagawa"
    ],
    [
     "Norihide",
     "Kitaoka"
    ]
   ],
   "title": "Blind dereverberation based on CMN and spectral subtraction by multi-channel LMS algorithm",
   "original": "i08_1032",
   "page_count": 4,
   "order": 326,
   "p1": "1032",
   "pn": "1035",
   "abstract": [
    "We proposed a blind dereverberation method based on spectral subtraction by Multi-Channel Least Mean Square (MCLMS) algorithm for distant-talking speech recognition in our previous study [1]. In this paper, we discuss the problems of the proposed method and present some solutions. In a distant-talking environment, the length of channel impulse response is longer than the short-term spectral analysis window. By treating the late reverberation as additive noise, a noise reduction technique based on spectral subtraction was proposed to estimate power spectrum of the clean speech using power spectra of the distorted speech and the unknown impulse responses. To estimate the power spectra of the impulse responses, a Variable Step-Size Unconstrained MCLMS (VSS-UMCLMS) algorithm for identifying the impulse responses in a time domain was extended to a frequency domain. To reduce the effect of the estimation error of channel impulse response, we normalize the early reverberation by CMN instead of the spectral subtraction used by the estimated impulse response in this paper. Furthermore, our proposed method is combined with a conventional delay-and-sum beamforming. We conducted the experiments on distorted speech signal simulated by convolving multi-channel impulse responses with clean speech. The modified proposed method achieved a relative error reduction rate of 22.7% from conventional CMN and 12.0% from the original proposed method, respectively. By combining the modified proposed method with the beamforming, a furthermore improvement (relative error reduction rate of 23.3%) was achieved.\n",
    "",
    "",
    "L. Wang, S. Nakagawa and N. Kitaoka, \"Blind dereverberation based on spectral subtraction by multi-channel LMS algorithm for distant-talking speech recognition,\" LangTech 2008, pp. 15-18, Feb. 2008.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-299"
  },
  "liao08b_interspeech": {
   "authors": [
    [
     "Yuan-Fu",
     "Liao"
    ],
    [
     "Hung-Hsiang",
     "Fang"
    ],
    [
     "Chi-Hui",
     "Hsu"
    ]
   ],
   "title": "Eigen-MLLR environment/speaker compensation for robust speech recognition",
   "original": "i08_1249",
   "page_count": 4,
   "order": 327,
   "p1": "1249",
   "pn": "1252",
   "abstract": [
    "In this paper an eigen-maximum likelihood linear regression (Eigen-MLLR) method is proposed to utilize a set of a priori noisy environment/speaker knowledge to online compensate the characteristics of unknown test environment/speaker. This idea is straightforward but is motivated from our recent findings that both the characteristics of different kinds of noisy environments and speakers could be simultaneously well organized in a PCA-constructed Eigen-MLLR subspace. Especially, the first three dimensions of the constructed Eigen-MLLR subspace are highly related to the SNR value, gender and type of noise. The proposed Eigen-MLLR was evaluated on Aurora 2 multi-condition training task. Experimental results showed that average word error rate (WER) of 6.14% was achieved. Moreover, Eigen-MLLR not only outperformed the multi-condition training baseline (Multi-Con., 13.72%) but also the blind ETSI advanced DSR front-end (ETSI-Adv., 8.65%), the histogram equalization (HEQ, 8.66%) and the non-blind reference model weighting (RMW, 7.29%) approaches.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-300"
  },
  "yu08b_interspeech": {
   "authors": [
    [
     "Dong",
     "Yu"
    ],
    [
     "Li",
     "Deng"
    ],
    [
     "Yifan",
     "Gong"
    ],
    [
     "Alex",
     "Acero"
    ]
   ],
   "title": "Parameter clustering and sharing in variable-parameter HMMs for noise robust speech recognition",
   "original": "i08_1253",
   "page_count": 4,
   "order": 328,
   "p1": "1253",
   "pn": "1256",
   "abstract": [
    "Recently we proposed a cubic-spline-based variable-parameter hidden Markov model (CS-VPHMM) whose mean and variance parameters vary according to some cubic spline functions of additional environment-dependent parameters. We have shown good properties of the CS-VPHMM and demonstrated on the Aurora-3 corpus that MCE-trained CS-VPHMM greatly outperforms the MCE-trained conventional HMM at the cost of increased total number of model parameters. In this paper, we propose to share spline functions across different Gaussian mixture components to reduce the total number of model parameters and develop a clustering algorithm to do so. We demonstrate the effectiveness of our parameter clustering and sharing algorithm for the CS-VPHMM on Aurora-3 corpus and show that proper parameter sharing can reduce the number of parameters from 4 times of that used in the conventional HMM to 1.13 times and still get 18% relative WER reduction over the MCE trained conventional HMM under the well-matched condition. Effective parameter sharing makes the CS-VPHMM an attractive model for noise robustness.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-301"
  },
  "du08b_interspeech": {
   "authors": [
    [
     "Jun",
     "Du"
    ],
    [
     "Qiang",
     "Huo"
    ]
   ],
   "title": "A feature compensation approach using high-order vector taylor series approximation of an explicit distortion model for noisy speech recognition",
   "original": "i08_1257",
   "page_count": 4,
   "order": 329,
   "p1": "1257",
   "pn": "1260",
   "abstract": [
    "This paper presents a new feature compensation approach to noisy speech recognition by using high-order vector Taylor series (HOVTS) approximation of an explicit model of environmental distortions. Formulations for maximum likelihood (ML) estimation of noise model parameters and minimum mean-squared error (MMSE) estimation of clean speech are derived. Experimental results on Aurora2 database demonstrate that the proposed approach achieves consistently significant improvement in recognition accuracy compared to traditional first-order VTS based feature compensation approach.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-302"
  },
  "cui08_interspeech": {
   "authors": [
    [
     "Xiaodong",
     "Cui"
    ],
    [
     "Mohamed",
     "Afify"
    ],
    [
     "Yuqing",
     "Gao"
    ]
   ],
   "title": "N-best based stochastic mapping on stereo HMM for noise robust speech recognition",
   "original": "i08_1261",
   "page_count": 4,
   "order": 330,
   "p1": "1261",
   "pn": "1264",
   "abstract": [
    "In this paper we present an extension of our previously proposed feature space stereo-based stochastic mapping (SSM). As distinct from an auxiliary stereo Gaussian mixture model in the front-end in our previous work, a stereo HMM model in the back-end is used. The basic idea, as in feature space SSM, is to form a joint space of the clean and noisy features, but to train a Gaussian mixture HMM in the new space. The MMSE estimation, which is the conditional expectation of the clean speech given the sequence of noisy observations, leads to clean speech predictors at the granularity of the Gaussian distributions in the HMM model. Because the Gaussians are not known during decoding, N-best hypotheses are employed. This results in a clean speech predictor which is a weighted (by posteriors) sum of the estimates from different Gaussian distributions. In experimental evaluation of the proposed method on the Aurora 2 database it gives better performance over the MST model, particularly, about 10%.20% relative improvement under unseen noise conditions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-303"
  },
  "tsao08_interspeech": {
   "authors": [
    [
     "Yu",
     "Tsao"
    ],
    [
     "Chin-Hui",
     "Lee"
    ]
   ],
   "title": "Improving the ensemble speaker and speaking environment modeling approach by enhancing the precision of the online estimation process",
   "original": "i08_1265",
   "page_count": 4,
   "order": 331,
   "p1": "1265",
   "pn": "1268",
   "abstract": [
    "In this paper, we study methods to enhance the precision of the online estimation process of a recently proposed approach, ensemble speaker and speaking environment modeling (ESSEM), and therefore improve its overall performance. The ESSEM approach consists of two integral phases, offline and online. In the offline phase, an ensemble environment configuration is prepared by a large collection of acoustic models. Each set of acoustic models represents a particular environment. In the online phase, with speech data from the testing condition, we estimate a mapping function and use it to generate a new set of acoustic models for that particular testing condition. In our previous study, we have discussed the issues of the offline process and proposed algorithms to refine the environment configuration. In this paper, we first study different online mapping structures and compare their performances on a same environment configuration. Next, we propose a multiple clustering matching algorithm to further improve the overall performance of ESSEM. We tested ESSEM and its extensions on the full evaluation set of the Aurora2 connected digit recognition task. When using our best offline environment configuration along with a properly specified online estimation method, the ESSEM approach can achieve an average word error rate (WER) of 4.77%, corresponding to a WER reduction of 13.43% (from 5.51% WER to 4.77% WER) over the baseline result.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-304"
  },
  "lu08_interspeech": {
   "authors": [
    [
     "Jianhua",
     "Lu"
    ],
    [
     "Ji",
     "Ming"
    ],
    [
     "Roger",
     "Woods"
    ]
   ],
   "title": "Combining noise compensation and missing-feature decoding for large vocabulary speech recognition in noise",
   "original": "i08_1269",
   "page_count": 4,
   "order": 332,
   "p1": "1269",
   "pn": "1272",
   "abstract": [
    "In this paper we propose a combination of noise compensation and missing-feature decoding for large-vocabulary speech recognition in noisy environments. Two approaches for noise compensation have been studied. These are noise training and vector Taylor series expansion, aiming to compensate white Gaussian noise at various levels. This is followed by subband missing-feature decoding to reduce the model/data mismatch. The proposed approach requires little knowledge about the noisy environments. The Aurora 4 corpus is used for the experiments. Better results are obtained by the new approach over a multi-condition baseline system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-305"
  },
  "pettersen08b_interspeech": {
   "authors": [
    [
     "Svein Gunnar",
     "Pettersen"
    ]
   ],
   "title": "Joint Bayesian predictive classification and parallel model combination with prior scaling for robust ASR",
   "original": "i08_1273",
   "page_count": 4,
   "order": 333,
   "p1": "1273",
   "pn": "1276",
   "abstract": [
    "This paper presents a model compensation approach based on Bayesian predictive classification (BPC). In order to obtain effective prior distributions for BPC, our approach uses parallel model combination (PMC) to set the prior mean, and a likelihood ratio to set a scaled frame-specific prior variance. Experiments on the Aurora 2 database show that the proposed approach results in improved average performance compared to PMC.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-306"
  },
  "kumar08_interspeech": {
   "authors": [
    [
     "Abhishek",
     "Kumar"
    ],
    [
     "John H. L.",
     "Hansen"
    ]
   ],
   "title": "Environment mismatch compensation using average eigenspace for speech recognition",
   "original": "i08_1277",
   "page_count": 4,
   "order": 334,
   "p1": "1277",
   "pn": "1280",
   "abstract": [
    "The performance of speech recognition systems is adversely affected by mismatch in training and testing environmental conditions. In addition to test data from noisy environments, there are scenarios where the training data itself is noisy. Speech enhancement techniques which solely focus on finding a clean speech estimate from the noisy signal are not effective here. Model adaptation techniques may also be ineffective due to the dynamic nature of the environment. In this paper, we propose a method for mismatch compensation between training and testing environments using the \"average eigenspace\" approach when the mismatch is non-stationary. There is no need for explicit adaptation data as the method works on incoming test data to find the compensatory transform. This method is different from traditional signal-noise subspace filtering techniques where the dimensionality of the clean signal space is assumed to be less than the noise space and noise affects all dimensions to the same extent. We evaluate this approach on two corpora which are collected from real car environments: CU-Move and UTDrive. Using Sphinx, a relative reduction of 40.50% is achieved in WER compared to the baseline system. The method also results in a reduction in the dimensionality of the feature vectors allowing for a more compact set of acoustic models in the phoneme space.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-307"
  },
  "povey08_interspeech": {
   "authors": [
    [
     "Daniel",
     "Povey"
    ],
    [
     "Brian",
     "Kingsbury"
    ]
   ],
   "title": "Monte Carlo model-space noise adaptation for speech recognition",
   "original": "i08_1281",
   "page_count": 4,
   "order": 335,
   "p1": "1281",
   "pn": "1284",
   "abstract": [
    "We describe a Monte Carlo method formodel-space noise adaptation of Gaussian mixture models (GMMs). This method combines a single-Gaussian noise model with the GMM speech model to produce an adapted model. It is similar to Parallel Model Combination or model-space Joint, except that it applies to spliced and projected MFCC features rather than to MFCC plus dynamic features. We demonstrate the necessity of re-estimating the noise using both the silence and speech frames rather than just estimating it from silence frames, and obtain improvements on a matched test set without added noise using a system that includes all standard adaptation techniques.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-308"
  },
  "ma08c_interspeech": {
   "authors": [
    [
     "Ning",
     "Ma"
    ],
    [
     "Phil",
     "Green"
    ]
   ],
   "title": "A 'speechiness' measure to improve speech decoding in the presence of other sound sources",
   "original": "i08_1285",
   "page_count": 4,
   "order": 336,
   "p1": "1285",
   "pn": "1288",
   "abstract": [
    "When speech is corrupted by other sound sources certain spectrotemporal regions will be dominated by speech energy and others by the noise. Listeners are able to exploit these cues to achieve robust speech perception in adverse conditions. Inspired by this perception process a 'speech fragment decoding' technique has shown promising robustness when handling multiple sound sources. This paper proposes an approach to estimating 'speechiness' - a degree of confidence that a spectro-temporal region is dominated by speech energy - using the modulation spectrogram. This additional knowledge is employed to steer the decoder towards selecting more reliable speech evidence in noise. Experiments show that the speechiness measure is capable of improving recognition accuracies in various noise conditions at 0 dB global signal-to-noise ratio.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-309"
  },
  "buera08_interspeech": {
   "authors": [
    [
     "Luis",
     "Buera"
    ],
    [
     "Antonio",
     "Miguel"
    ],
    [
     "Oscar",
     "Saz"
    ],
    [
     "Alfonso",
     "Ortega"
    ],
    [
     "Eduardo",
     "Lleida"
    ]
   ],
   "title": "Feature vector normalization with combined standard and throat microphones for robust ASR",
   "original": "i08_1289",
   "page_count": 4,
   "order": 337,
   "p1": "1289",
   "pn": "1292",
   "abstract": [
    "We propose on-line unsupervised compensation technique for robust speech recognition that combines standard and throat microphone feature vectors. The solution, called Multi-Environment Model-based LInear Normalization with Throat microphone information, MEMLINT, is an extension of MEMLIN formulation. Hence, standard microphone noisy space and throat microphone space are modelled as GMMs and a set of linear transformations are learnt from data associated to each pair of Gaussians (one for each GMM) using training stereo data. On the other hand, to compensate some kinds of degradation which are not considered in MEMLINT, we propose to use jointly an on-line unsupervised acoustic model adaptation method based on rotation transformations over an expanded HMM-state space (augMented stAte space acousTic dEcoder, MATE). Some experiments with an own recorded database were carried out, showing that the proposed approach significantly outperforms the single microphone approach.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-310"
  },
  "fukuda08_interspeech": {
   "authors": [
    [
     "Takashi",
     "Fukuda"
    ],
    [
     "Osamu",
     "Ichikawa"
    ],
    [
     "Masafumi",
     "Nishimura"
    ]
   ],
   "title": "Phone-duration-dependent long-term dynamic features for a stochastic model-based voice activity detection",
   "original": "i08_1293",
   "page_count": 4,
   "order": 338,
   "p1": "1293",
   "pn": "1296",
   "abstract": [
    "Accurate voice activity detection (VAD) is important for robust automatic speech recognition (ASR) systems. This paper proposes noise-robust VAD using long-term temporal information in speech. Long-term temporal information has been an ASR focus recently, but has not been investigated sufficiently for VAD. This paper describes an attempt to incorporate long-term temporal information into a feature parameter set by extracting conventional dynamic features from long-term cepstrum sequences. The proposed method includes the temporal contexts of phonemes by using long-term features and allows distinguishing between speech and non-speech intervals. The long-term features calculated over the average phoneme duration provide noise robustness. In an experiment on the Japanese digit corpus, the proposed method led to considerable improvements over conventional methods including the G.729 Annex B and the ETSI AFE-VAD under low SNR conditions, and had 71.1% error reduction on average as compared to the ETSI AFE-VAD.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-311"
  },
  "ijima08_interspeech": {
   "authors": [
    [
     "Yusuke",
     "Ijima"
    ],
    [
     "Makoto",
     "Tachibana"
    ],
    [
     "Takashi",
     "Nose"
    ],
    [
     "Takao",
     "Kobayashi"
    ]
   ],
   "title": "An on-line adaptation technique for emotional speech recognition using style estimation with multiple-regression HMM",
   "original": "i08_1297",
   "page_count": 4,
   "order": 339,
   "p1": "1297",
   "pn": "1300",
   "abstract": [
    "This paper describes a model adaptation technique for emotional speech recognition based on multiple-regression HMM (MR-HMM).We use a low-dimensional vector called style vector which corresponds the degree of expressivity of emotional speech as the explanatory variable of the regression. In the proposed technique, first, the value of the style vector for input speech is estimated. Then, using the estimated style vector, new mean vectors of the output distributions of HMM are adapted to the input style. The style vector is estimated every input utterance, and an on-line adaptation can be done in each utterance. We perform phoneme recognition experiments for professional narrators' acted speech and evaluate the performance by comparing with style-dependent and style-independent HMMs. Experimental results show the proposed technique reduced the error rates by 11% of the style-independent model.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-312"
  },
  "berkovitch08_interspeech": {
   "authors": [
    [
     "Michael",
     "Berkovitch"
    ],
    [
     "Ilan D.",
     "Shallom"
    ]
   ],
   "title": "HMM adaptation using statistical linear approximation for robust automatic speech recognition",
   "original": "i08_1301",
   "page_count": 4,
   "order": 340,
   "p1": "1301",
   "pn": "1304",
   "abstract": [
    "The lack of noise robustness is one of the main drawbacks of an Automatic Speech Recognition (ASR) system. A well trained ASR system can achieve high recognition rate on quiet laboratory conditions, but perform poorly in real life environments. In this paper we will present a noise robustness method which uses the clean speech Hidden Markov Models (HMM) and noise statistics, to create an approximation of the degraded speech HMM using the Statistical Linear Approximation (SLA). Experiments using the proposed methods had shown up to 87.7% word error rate improvement.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-313"
  },
  "rennie08_interspeech": {
   "authors": [
    [
     "Steven J.",
     "Rennie"
    ],
    [
     "Pierre L.",
     "Dognin"
    ]
   ],
   "title": "Beyond linear transforms: efficient non-linear dynamic adaptation for noise robust speech recognition",
   "original": "i08_1305",
   "page_count": 4,
   "order": 341,
   "p1": "1305",
   "pn": "1308",
   "abstract": [
    "In this paper, we present new theory and results that combine constrained Maximum Likelihood Linear Regression (MLLR), known as feature space MLLR (fMLLR), a state-of-the-art model adaptation technique, with Dynamic Noise Adaptation (DNA), a state-of-the-art noise adaptation algorithm. We explain how DNA implements a highly non-linear transform on speech model features, and why DNA is better suited for compensating for additive noise than fMLLR. Tests results are presented on the DNA + Aurora II framework, which is based upon a collection of challenging in-car noise recordings, as a function of SNR. The results demonstrate that DNA significantly outperforms block fMLLR on additive noise, and that DNA + fMLLR outperforms the ETSI advanced front-end (AFE) system + fMLLR by a significant margin (over 7% absolute).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-314"
  },
  "gomez08b_interspeech": {
   "authors": [
    [
     "Randy",
     "Gomez"
    ],
    [
     "Jani",
     "Even"
    ],
    [
     "Kiyohiro",
     "Shikano"
    ]
   ],
   "title": "Rapid unsupervised speaker adaptation robust in reverberant environment conditions",
   "original": "i08_1309",
   "page_count": 4,
   "order": 342,
   "p1": "1309",
   "pn": "1312",
   "abstract": [
    "We expand the conventional rapid adaptation based on N-closest speakers sufficient statistics (suff stat) to achieve robustness under reverberant conditions. We integrated our fast de-reverberation technique based on optimized multi-band spectral subtraction as pre-processing. This removes the late reflection components of the reverberant signal effectively and fast. Speakers' suff stat are then computed from the processed data and stored offline. The system only requires a single arbitrary utterance used to select the N-closest suff stat to update the model online. In this paper, we also investigate the effects in the acoustic subspace introduced by the channel (reverberation). Moreover, we compare the performance of the proposed expansion, with Speaker Adaptive Training (SAT), Constrained Maximum Likelihood Linear Regression (CMLLR) and evaluate in both artificial and actual reverberant data.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-315"
  },
  "li08d_interspeech": {
   "authors": [
    [
     "Jinyu",
     "Li"
    ],
    [
     "Chin-Hui",
     "Lee"
    ]
   ],
   "title": "On a generalization of margin-based discriminative training to robust speech recognition",
   "original": "i08_1992",
   "page_count": 4,
   "order": 343,
   "p1": "1992",
   "pn": "1995",
   "abstract": [
    "Recently, there have been intensive studies of margin-based learning for automatic speech recognition (ASR). It is our believe that by securing a margin from the decision boundaries to the training samples, a correct decision can still be made if the mismatches between testing and training samples are well within the tolerance region specified by the margin. This nice property should be effective for robust ASR, where the testing condition is different from those in training. In this paper, we report on experiment results with soft margin estimation (SME) on the Aurora2 task and show that SME is very effective under clean training with more than 50% relative word error reductions in the clean, 20db, and 15db testing conditions, and still gives a slight improvement over conventional multi-condition training approaches. This demonstrates that the margin in SME can equip recognizers with a nice generalization property under adverse conditions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-316"
  },
  "gales08_interspeech": {
   "authors": [
    [
     "M. J. F.",
     "Gales"
    ],
    [
     "C.",
     "Longworth"
    ]
   ],
   "title": "Discriminative classifiers with generative kernels for noise robust ASR",
   "original": "i08_1996",
   "page_count": 4,
   "order": 344,
   "p1": "1996",
   "pn": "1999",
   "abstract": [
    "Discriminative classifiers are a popular approach to solving classification problems. However one of the problems with these approaches, in particular kernel based classifiers such as Support Vector Machines (SVMs), is that they are hard to adapt to mismatches between the training and test data. This paper describes a scheme for overcoming this problem for speech recognition in noise. Generative kernels, defined using generative models, allow SVMs to handle sequence data. By compensating the generative models for the noise conditions noise-specific generative kernels can be obtained. These can be used to train a noise-independent SVM on a range of noise conditions, which can then be used with a test-set noise kernel for classification. Initial experiments using an idealised version of model-based compensation were run on the AURORA 2.0 continuous digit task. The proposed scheme yielded large gains in performance over the compensated models.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-317"
  },
  "dalen08_interspeech": {
   "authors": [
    [
     "R. C. van",
     "Dalen"
    ],
    [
     "M. J. F.",
     "Gales"
    ]
   ],
   "title": "Covariance modelling for noise-robust speech recognition",
   "original": "i08_2000",
   "page_count": 4,
   "order": 345,
   "p1": "2000",
   "pn": "2003",
   "abstract": [
    "Model compensation is a standard way of improving speech recognisers' robustness to noise. Most model compensation techniques produce diagonal covariances. However, this fails to handle changes in the feature correlations due to the noise. This paper presents a scheme that allows full covariance matrices to be estimated. One problem is that full covariance matrix estimation will be more sensitive to approximations, like those for dynamic parameters which are known to be crude. In this paper a linear transformation of a window of consecutive frames is used as the basis for dynamic parameter compensation. A second problem is that the resulting full covariance matrices slow down decoding. This is addressed by using predictive linear transforms that decorrelate the feature space, so that the decoder can then use diagonal covariance matrices. On a noise-corrupted Resource Management task, the proposed scheme outperformed the standard vts compensation scheme.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-318"
  },
  "chen08d_interspeech": {
   "authors": [
    [
     "Wei-Hau",
     "Chen"
    ],
    [
     "Shih-Hsiang",
     "Lin"
    ],
    [
     "Berlin",
     "Chen"
    ]
   ],
   "title": "Exploiting spatial-temporal feature distribution characteristics for robust speech recognition",
   "original": "i08_2004",
   "page_count": 4,
   "order": 346,
   "p1": "2004",
   "pn": "2007",
   "abstract": [
    "Noise robustness is one of the primary challenges facing most automatic speech recognition (ASR) systems. Quite several speech feature histogram equalization (HEQ) methods have been developed to compensate for nonlinear noise distortions. However, most of the current HEQ methods are merely performed in a dimension-wise manner and without taking into consideration the contextual relationships between consecutive speech frames. In this paper, we present a novel HEQ approach that exploits spatial-temporal feature distribution characteristics for speech feature normalization. All experiments were carried out on the Aurora-2 database and task. The performance of the presented approach is tested and verified by comparison with the other HEQ methods. The experiment results show that for clean-condition training, our method yields a significant word error rate reduction over the baseline system, and also considerably outperforms the other HEQ methods compared in this paper.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-319"
  },
  "fujimoto08_interspeech": {
   "authors": [
    [
     "Masakiyo",
     "Fujimoto"
    ],
    [
     "Kentaro",
     "Ishizuka"
    ],
    [
     "Tomohiro",
     "Nakatani"
    ]
   ],
   "title": "Study of integration of statistical model-based voice activity detection and noise suppression",
   "original": "i08_2008",
   "page_count": 4,
   "order": 347,
   "p1": "2008",
   "pn": "2011",
   "abstract": [
    "This paper addresses robust front-end processing for automatic speech recognition (ASR) in noisy environments. To recognize the corrupted speech accurately, it is necessary to employ robust methods against various types of interference. Usually, noise suppression (NS) is used for the front-end processing of ASR in noise. Voice activity detection (VAD) is also used for front-end processing to reduce the redundant non-speech period. VAD and NS are typically combined as series processing. However, VAD and NS should not be assumed to be a separate technique, because the output information of these methods be mutually beneficial. Thus, we investigate the integrated front-end processing of VAD and NS, which can utilize each others' input-output information. The evaluation is carried out by using a concatenated speech corpus, CENSREC-1-C. In the evaluation, the proposed method improves ASR accuracy compared with conventional series combination.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-320"
  },
  "li08e_interspeech": {
   "authors": [
    [
     "Weifeng",
     "Li"
    ],
    [
     "John",
     "Dines"
    ],
    [
     "Mathew Magimai",
     "Doss"
    ],
    [
     "Hervé",
     "Bourlard"
    ]
   ],
   "title": "Neural network based regression for robust overlapping speech recognition using microphone arrays",
   "original": "i08_2012",
   "page_count": 4,
   "order": 348,
   "p1": "2012",
   "pn": "2015",
   "abstract": [
    "This paper investigates a neural network based acoustic feature mapping to extract robust features for automatic speech recognition (ASR) of overlapping speech. The focus of this work is the novel investigation of additional sources of information to improve the effectiveness of the feature mapping. Specifically, we investigate two additional information sources. Firstly, we investigate the mapping of noisy, higher-order ASR features to clean, lower-order features, demonstrating that the redundancy in the higher order representation can be exploited in the case of overlapping speech. Secondly, we investigate the mapping of features from multiple sound sources, namely from the target and interfering speakers, once again resulting in significant improvements to ASR performance. In the latter case we liken out approach to post-filtering that is undertaken in conventional microphone array beamforming. We demonstrate the effectiveness of the proposed approach through extensive evaluations on the MONC corpus, which includes both non-overlapping single speaker and overlapping multi-speaker conditions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-321"
  },
  "pfitzinger08_interspeech": {
   "authors": [
    [
     "Hartmut R.",
     "Pfitzinger"
    ],
    [
     "Christian",
     "Kaernbach"
    ]
   ],
   "title": "Amplitude and amplitude variation of emotional speech",
   "original": "i08_1036",
   "page_count": 4,
   "order": 349,
   "p1": "1036",
   "pn": "1039",
   "abstract": [
    "The present study introduces a recording technique to maintain all dynamic information of even full-blown emotional speech, and investigates the effect of emotion class, speaker, and sentence type on the amplitude of speech. The results show that the factors emotion class and speaker are highly significant and that the former explains half of the variance while the latter explains only one ninth. Amplitude, as simple as it is, should therefore not be neglected or normalized in acoustic recordings and analyses of emotional speech.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-322"
  },
  "krishnamurthy08_interspeech": {
   "authors": [
    [
     "Nitish",
     "Krishnamurthy"
    ],
    [
     "Ayako",
     "Ikeno"
    ],
    [
     "John H. L.",
     "Hansen"
    ]
   ],
   "title": "Babble speech: acoustic and perceptual variability",
   "original": "i08_1040",
   "page_count": 4,
   "order": 350,
   "p1": "1040",
   "pn": "1043",
   "abstract": [
    "The presence of babble noise is one of the most difficult environments to sustain speech system performance. This study focuses on acoustic and perceptual analyses of babble. The acoustic variability of babble is analyzed as a function of the number of speakers in babble. The concept of acoustic volume is proposed and it is shown that the acoustic volume reduces as the number of speakers in babble increase. This framework is evaluated using over 40 hours of simulated babble from SWITCHBOARD corpus. It is observed that the acoustic volume does not change significantly when there are 4 or more speakers in babble. It is also seen that with an increase in the number of speakers in babble, there is an uneven spread of data in the acoustic space due to mixing of multi-speaker content. This study provides a framework to better understand the babble environment, enabling us to improve the formulation of reliable algorithms for robust speech systems.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-323"
  },
  "pantazis08_interspeech": {
   "authors": [
    [
     "Yannis",
     "Pantazis"
    ],
    [
     "Olivier",
     "Rosec"
    ],
    [
     "Yannis",
     "Stylianou"
    ]
   ],
   "title": "On the properties of a time-varying quasi-harmonic model of speech",
   "original": "i08_1044",
   "page_count": 4,
   "order": 351,
   "p1": "1044",
   "pn": "1047",
   "abstract": [
    "In this paper we present the properties of a parametric speech model based on a deterministic plus noise representation of speech initially suggested by Laroche et al. [1]. Aiming at a high resolution analysis of speech signals for voice quality control (transformation) and assessment, we focus on the deterministic representation and we reveal the properties of the model showing that such a representation is equivalent to a time-varying quasiharmonic representation of speech. Results show that the model is appropriate in estimating accurately linear amplitude modulations and modeling the inharmonicity of speech.\n",
    "",
    "",
    "J. Laroche Y. Stylianou and E. Moulines. HNM: A Simple, Effecient Harmonic plus Noise Model for Speech. In Workshop on Appl. of Signal Proc. to Audio and Acoustics (WASPAA), pages 169-172, New Paltz, NY, USA, Oct 1993.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-324"
  },
  "lu08b_interspeech": {
   "authors": [
    [
     "Wenliang",
     "Lu"
    ],
    [
     "D.",
     "Sen"
    ]
   ],
   "title": "Extraction and tracking of formant response jitter in the cochlea for objective prediction of SB/SF DAM attributes",
   "original": "i08_1048",
   "page_count": 4,
   "order": 352,
   "p1": "1048",
   "pn": "1051",
   "abstract": [
    "In this paper, we focus on the objective prediction of two of the foreground perceptual quality elements of the Diagnostic Acceptability Measure (DAM) - that of SB and SF - and show that they are correlated with statistical characteristics of features extracted from a physiologically motivated cochlear model response. The work complements earlier work where two other DAM quality elements, SH and SL, were predicted using the same cochlear model [1]. Novel methods of extracting salient features from the cochlear response as well as tracking their evolution are described. Finally, it is shown that the standard deviation of the features is highly correlated with the perception of 'fluttering' (SF) and 'babble' (SB) like distortions.\n",
    "",
    "",
    "D. Sen, \"Predicting foreground SH, SL and BNH DAM scores for multidimensionalobjective measure of speech quality,\" Acoustics, Speech, and Signal Processing, 2004. Proceedings. (ICASSP 04). IEEE International Conference on, vol. 1, pp. I-493-6 vol.1, 17-21 May 2004.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-325"
  },
  "messing08_interspeech": {
   "authors": [
    [
     "David P.",
     "Messing"
    ],
    [
     "Lorraine",
     "Delhorne"
    ],
    [
     "Ed",
     "Bruckert"
    ],
    [
     "Louis D.",
     "Braida"
    ],
    [
     "Oded",
     "Ghitza"
    ]
   ],
   "title": "Consonant discrimination of degraded speech using an efferent-inspired closed-loop cochlear model",
   "original": "i08_1052",
   "page_count": 4,
   "order": 353,
   "p1": "1052",
   "pn": "1055",
   "abstract": [
    "We present a model of auditory speech processing capable of predicting consonant confusions by normal hearing listeners, based on a phenomenological model of the Medial Olivocochlear efferent pathway. We then use this model to predict human error patterns of initial consonants in consonant-vowel-consonant words. In the process we demonstrate its potential for speech identification in noise. Our results produced performance that was robust to varying levels of additive noise and which was similar to human performance in discrimination of synthetically spoken consonants.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-326"
  },
  "tomar08_interspeech": {
   "authors": [
    [
     "Vikrant",
     "Tomar"
    ],
    [
     "Hemant A.",
     "Patil"
    ]
   ],
   "title": "On the development of variable length Teager energy operator (VTEO)",
   "original": "i08_1056",
   "page_count": 4,
   "order": 354,
   "p1": "1056",
   "pn": "1059",
   "abstract": [
    "Teager Energy Operator (TEO) proposed by Kaiser and Teager is based on a definition of energy required to generate the signal. TEO gives us the running estimate of energy as a function of amplitude and instantaneous frequency content of the signal. However, it considers three consecutive samples to calculate the energy estimate. In this paper, we suggests an alternative and generalized approach to TEO to calculate the instantaneous estimate of the energy where not only consecutive but other distant samples can also be incorporated in the calculation of running estimate of the energy and the number of samples taken to calculate energy can also be increased depending on our signal properties to better capture the energy content variations in the speech signal.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-327"
  },
  "qiao08_interspeech": {
   "authors": [
    [
     "Yu",
     "Qiao"
    ],
    [
     "Nobuaki",
     "Minematsu"
    ]
   ],
   "title": "Metric learning for unsupervised phoneme segmentation",
   "original": "i08_1060",
   "page_count": 4,
   "order": 355,
   "p1": "1060",
   "pn": "1063",
   "abstract": [
    "Unsupervised phoneme segmentation aims at dividing a speech stream into phonemes without using any prior knowledge of linguistic contents and acoustic models. In [1], we formulated this problem into an optimization framework, and developed an objective function, summation of squared error (SSE) based on the Euclidean distance of cepstral features. However, it is unknown whether or not Euclidean distance yields the best metric to estimate the goodness of segmentations. In this paper, we study how to learn a good metric to improve the performance of segmentation. We propose two criteria for learning metric: Minimum of Summation Variance (MSV) and Maximum of Discrimination Variance (MDV). The experimental results on TIMIT database indicate that the use of learning metric can achieve better segmentation performances. The best recall rate of this paper is 81.8% (20ms windows), compared to 77.5% of [1]. We also introduce an iterative algorithm to learn metric without using labeled data, which achieves similar results as those with labeled data.\n",
    "",
    "",
    "Y. Qiao, N. Shimomura, and N. Minematsu, \"Unsupervised Optimal Phoneme Segmentation: Objectives, Algorithm and Comparisons,\" Proc. ICASSP, pp. 885-888, 2008.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-328"
  },
  "kalinli08_interspeech": {
   "authors": [
    [
     "Ozlem",
     "Kalinli"
    ],
    [
     "Shrikanth S.",
     "Narayanan"
    ]
   ],
   "title": "Combining task-dependent information with auditory attention cues for prominence detection in speech",
   "original": "i08_1064",
   "page_count": 4,
   "order": 356,
   "p1": "1064",
   "pn": "1067",
   "abstract": [
    "Auditory attention is a highly complex mechanism that involves the process of low-level acoustic features of sound together with higher level cognitive rules. In this paper, a novel method that combines biologically inspired auditory attention cues with higher level lexical and syntactic information is proposed to model task-dependent influences on a given task. The feature maps are extracted from sound at multi-scales by mimicking the processing stages in the human auditory system, and converted to low-level auditory gist features. Then, the auditory attention model biases the gist features based on the task to maximize target detection. The top-down task-dependent influence of lexical and syntactic information is incorporated into the model using a probabilistic approach. The combined model is tested to detect prominent syllables in speech using the BU Radio News Corpus. The model achieves 88% prominence detection accuracy at syllable level, which is comparable to reported human performance on this task.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-329"
  },
  "zen08_interspeech": {
   "authors": [
    [
     "Heiga",
     "Zen"
    ],
    [
     "Yoshihiko",
     "Nankaku"
    ],
    [
     "Keiichi",
     "Tokuda"
    ]
   ],
   "title": "Probabilistic feature mapping based on trajectory HMMs",
   "original": "i08_1068",
   "page_count": 4,
   "order": 357,
   "p1": "1068",
   "pn": "1071",
   "abstract": [
    "This paper proposes a feature mapping algorithm based on the trajectory GMM or trajectory HMM. Although the GMM or HMM based feature mapping algorithm works effectively, its conversion quality sometimes degrades due to the inappropriate dynamic characteristics caused by the frame-by-frame conversion. While the use of dynamic features can alleviate this problem, it also introduces an inconsistency between training and mapping. The proposed algorithm can solve this inconsistency while keeping the benefits of the use of dynamic features, and offers an entire sequence-level transformation rather than the frame-by-frame conversion. Experimental results in voice conversion show that the proposed algorithm outperforms the conventional one both in objective and subjective tests.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-330"
  },
  "yutani08_interspeech": {
   "authors": [
    [
     "Kaori",
     "Yutani"
    ],
    [
     "Yosuke",
     "Uto"
    ],
    [
     "Yoshihiko",
     "Nankaku"
    ],
    [
     "Tomoki",
     "Toda"
    ],
    [
     "Keiichi",
     "Tokuda"
    ]
   ],
   "title": "Simultaneous conversion of duration and spectrum based on statistical models including time-sequence matching",
   "original": "i08_1072",
   "page_count": 4,
   "order": 358,
   "p1": "1072",
   "pn": "1075",
   "abstract": [
    "This paper describes a simultaneous conversion technique of duration and spectrum based on a statistical model including time-sequence matching. Conventional GMM-based approaches cannot perform spectral conversion taking account of speaking rate because it assumes one to one frame matching between source and target features. However, speaker characteristics may appear in speaking rates. In order to perform duration conversion, we attach duration models to statistical models including timesequence matching (DPGMM). Since DPGMM can represent two different length sequences directly, the conversion of spectrum and duration can be performed within an integrated framework. In the proposed technique, each mixture component of DPGMM has different duration transformation functions, therefore durations are converted nonlinearly and dependently on spectral information. In the subjective DMOS test, the proposed method is superior to the conventional method.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-331"
  },
  "muramatsu08_interspeech": {
   "authors": [
    [
     "Takashi",
     "Muramatsu"
    ],
    [
     "Yamato",
     "Ohtani"
    ],
    [
     "Tomoki",
     "Toda"
    ],
    [
     "Hiroshi",
     "Saruwatari"
    ],
    [
     "Kiyohiro",
     "Shikano"
    ]
   ],
   "title": "Low-delay voice conversion based on maximum likelihood estimation of spectral parameter trajectory",
   "original": "i08_1076",
   "page_count": 4,
   "order": 359,
   "p1": "1076",
   "pn": "1079",
   "abstract": [
    "As typical voice conversion methods, two spectral conversion processes have been proposed: 1) the frame-based conversion that converts spectral parameters frame by frame and 2) the trajectory-based conversion that converts all spectral parameters over an utterance simultaneously. The former process is capable of real-time conversion but it sometimes causes inappropriate spectral movements. On the other hand, the latter process provides the converted spectral parameters exhibiting proper dynamic characteristics but a batch process is inevitable. To achieve the real-time conversion process considering spectral dynamic characteristics, we propose a time-recursive conversion algorithm based on maximum likelihood estimation of spectral parameter trajectory. Experimental results show that the proposed method achieves the low-delay conversion process, e.g., only one frame delay, while keeping the conversion performance comparably high to that of the conventional trajectory-based conversion.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-332"
  },
  "ohtani08_interspeech": {
   "authors": [
    [
     "Yamato",
     "Ohtani"
    ],
    [
     "Tomoki",
     "Toda"
    ],
    [
     "Hiroshi",
     "Saruwatari"
    ],
    [
     "Kiyohiro",
     "Shikano"
    ]
   ],
   "title": "An improved one-to-many eigenvoice conversion system",
   "original": "i08_1080",
   "page_count": 4,
   "order": 360,
   "p1": "1080",
   "pn": "1083",
   "abstract": [
    "We have previously developed a one-to-many eigenvoice conversion (EVC) system enabling the conversion from a specific source speaker's voice into an arbitrary target speaker's voice. In this system, eigenvoice Gaussian mixture model (EV-GMM) is trained in advance with multiple parallel data sets composed of utterance pairs of the source and many pre-stored target speakers. The EV-GMM is effectively adapted to an arbitrary target speaker using a small amount of adaptation data. Although this system achieves the very flexible training of the conversion model, the quality of the converted speech is still not high enough. In order to alleviate this problem, we simultaneously apply the following promising techniques to the one-to-many EVC system: 1) STRAIGHT mixed excitation, 2) the conversion algorithm considering global variance, and 3) speaker adaptive training of the EV-GMM. Experimental results demonstrate that the proposed system causes remarkable improvements in the performance of EVC.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-333"
  },
  "uchimura08_interspeech": {
   "authors": [
    [
     "Yoshinori",
     "Uchimura"
    ],
    [
     "Hideki",
     "Banno"
    ],
    [
     "Fumitada",
     "Itakura"
    ],
    [
     "Hideki",
     "Kawahara"
    ]
   ],
   "title": "Study on manipulation method of voice quality based on the vocal tract area function",
   "original": "i08_1084",
   "page_count": 4,
   "order": 361,
   "p1": "1084",
   "pn": "1087",
   "abstract": [
    "This paper describes a new manipulation method of voice quality which is based on the STRAIGHT analysis-synthesis system. This method manipulates voice quality by changing the vocal tract area function calculated from the PARCOR coefficients. The PARCOR coefficients used in the proposed method is obtained from the auto-correlation function of the STRAIGHT spectrum. We have implemented a simple speech morphing system based on the proposed method. As a result, the morphed spectrum by the proposed method has natural spectral transition such as in human voice even if the formant frequencies of input two speech signals are very different, although the morphed spectrum by the conventional linear interpolation method has unnatural spectral transition.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-334"
  },
  "toth08_interspeech": {
   "authors": [
    [
     "Arthur",
     "Toth"
    ],
    [
     "Alan W.",
     "Black"
    ]
   ],
   "title": "Incorporating durational modification in voice transformation",
   "original": "i08_1088",
   "page_count": 4,
   "order": 362,
   "p1": "1088",
   "pn": "1091",
   "abstract": [
    "Voice transformation is the process of using a small amount of speech data from a target speaker to build a transformation model that can be used to generate arbitrary speech that sounds like the target speaker. One common current technique is building Gausian Mixture Models to map spectral aspects from source to target speakers. This paper proposes the use of duration models to improve the transformation models and output speech quality. Testing across seven target speakers shows a statistically significant improvement in a popular objective metric when duration modification is performed both during training and testing of a Gaussian Mixture Model mapping based voice transformation system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-335"
  },
  "dashiell08_interspeech": {
   "authors": [
    [
     "Amy",
     "Dashiell"
    ],
    [
     "Brian",
     "Hutchinson"
    ],
    [
     "Anna",
     "Margolis"
    ],
    [
     "Mari",
     "Ostendorf"
    ]
   ],
   "title": "Non-segmental duration feature extraction for prosodic classification",
   "original": "i08_1092",
   "page_count": 4,
   "order": 363,
   "p1": "1092",
   "pn": "1095",
   "abstract": [
    "This paper presents a set of novel duration features for detecting pitch accent and phrase boundaries, which depend on articulatory timing rather than segmental duration information. The features are computed from the detected syllable nuclei and boundaries, using peaks and valleys in an energy contour but also leveraging information from a simple HMM phone manner class recognizer to increase recall. In experiments on the hand-segmented TIMIT corpus, we obtain greater than 90% F-measure for vowel detection. In prosody detection experiments on the BU Radio News corpus, comparing to a segmental feature baseline, we obtain similar performance for pitch accent detection and slightly worse boundary detection from the new features without the need for phonetic alignments.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-336"
  },
  "zheng08_interspeech": {
   "authors": [
    [
     "Hong-Ying",
     "Zheng"
    ],
    [
     "William S.-Y.",
     "Wang"
    ]
   ],
   "title": "An ERP study on categorical perception of lexical tones and nonspeech pitches",
   "original": "i08_1096",
   "page_count": 1,
   "order": 364,
   "p1": "1096",
   "pn": "",
   "abstract": [
    "The study investigates the electrophysiological signature for Categorical Perception (CP) of lexical tones. Mandarin level and rising tones are the targets and nonspeech counterparts, using pure tones with the same pitch contours, are synthesized as controls. The results show that in lexical tone condition, across-category deviant elicits Mismatch Negativity (MMN) but within-category deviant does not. In pure tone condition, neither deviant elicits significant MMN. The result indicates CP of lexical tones can be observed by MMN signal. It also suggests that before listener does the overt decision, brain activates differently to lexical tones from nonspeech pitches.\n",
    ""
   ]
  },
  "otake08_interspeech": {
   "authors": [
    [
     "Takashi",
     "Otake"
    ],
    [
     "Marii",
     "Higuchi"
    ]
   ],
   "title": "The role of Japanese pitch accent in spoken-word recognition: evidence from middle-aged accentless dialect listeners",
   "original": "i08_1097",
   "page_count": 4,
   "order": 365,
   "p1": "1097",
   "pn": "1100",
   "abstract": [
    "This paper investigates the role of pitch accent information in spoken-word recognition in listeners in Fukushima, Japan, whose dialect is accentless. Previous research revealed that accentless listeners were less sensitive to pitch accent than Tokyo Japanese listeners. The present study asked whether middle-aged listeners' use of accent information would differ from that of young listeners. 40 middle-aged Fukushima listeners were presented with Tokyo Japanese materials used in the earlier study, employing a gating task. Results show that middle-aged Fukushima accentless listeners are even less sensitive to the pitch accent information than the younger accentless listeners. The findings suggest that the exploitation of pitch information by younger listeners reflects adaptation to the standard Tokyo Japanese dialect, e.g., via the media, whereas the older listeners are less influenced by the standard.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-337"
  },
  "wang08f_interspeech": {
   "authors": [
    [
     "Siwei",
     "Wang"
    ],
    [
     "Gina-Anne",
     "Levow"
    ]
   ],
   "title": "Mandarin Chinese tone nucleus detection with landmarks",
   "original": "i08_1101",
   "page_count": 4,
   "order": 366,
   "p1": "1101",
   "pn": "1104",
   "abstract": [
    "This paper discusses a new approach to improve tone recognition by modeling the tone nucleus with vowel landmark detection. The tone nucleus region is identified based on vowel landmark frames derived by an automatic landmark recognition system. In the corresponding tone recognition experiments, the best results with landmark-based tone nucleus regions outperform the best baseline system results by more than 6%. Moreover, in an exploratory experiment, the tone recognition accuracy using tone nucleus regions based only on vowel landmark evidence shows less than 2% degradation relative to the accuracy obtained using both landmark frames and force-aligned vowel boundary information. These findings further demonstrate the potential to perform tone recognition based on landmark detection alone, without full speech recognition or aligned transcriptions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-338"
  },
  "hu08b_interspeech": {
   "authors": [
    [
     "Weixiang",
     "Hu"
    ],
    [
     "Jin",
     "Jian"
    ],
    [
     "Aijun",
     "Li"
    ],
    [
     "Xia",
     "Wang"
    ]
   ],
   "title": "A comparative study on dissyllabic stress patterns of Mandarin and Cantonese",
   "original": "i08_1105",
   "page_count": 4,
   "order": 367,
   "p1": "1105",
   "pn": "1108",
   "abstract": [
    "This paper studied the similarity and dissimilarity for stress patterns between standard and various dialects of Mandarin, focusing on pitch and duration pattern comparison. We analyzed the distribution, pitch and duration patterns for stressed dissyllabic words in Cantonese, Cantonese-spoken Mandarin and Standard Mandarin. For isolated dissyllabic words, it was shown that there was a preference for a stress on the preceding word in Standard Mandarin while there was a stress on the posterior word in Cantonese-spoken Mandarin, and it is more easily lengthened in Cantonese-spoken Mandarin than in Standard Mandarin. For dissyllabic words in an utterance, there was a preference for stress on the preceding word in both Standard and Cantonese-spoken Mandarin but it is more easily lengthened in Standard Mandarin. Such results are due to the impact of the Cantonese dialect. At the same time, we investigated the neutral tone realization in Cantonese spoken Mandarin utterances. It was found that 36.6% of the neutral tones were wrongly realized in the other tones. It is an important reason for the difference of in stress patterns in Standard Mandarin and Cantonese-spoken Mandarin. It is necessary to compare acoustic cues according to the stress level. It is not enough for determining the similarities and dissimilarities for the rhythmic pattern between two dialects or between Mandarin and accented Mandarin according to the basic two stress pattern (stress-unstressed and unstressed-stressed.)\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-339"
  },
  "ho08_interspeech": {
   "authors": [
    [
     "Rerrario Shui-Ching",
     "Ho"
    ],
    [
     "Yoshinori",
     "Sagisaka"
    ]
   ],
   "title": "Three-sectional-staff characterization of Cantonese level tones",
   "original": "i08_1109",
   "page_count": 4,
   "order": 368,
   "p1": "1109",
   "pn": "1112",
   "abstract": [
    "We have introduced a new approach of acoustical characterization of the level tones of a tone language [1] and tested it with Cantonese [2]. To examine the validity of any perceptual account by analysing its correlation with acoustic data, our new approach requires it to be expressed in terms of the pitch distance ratios between the successive levels instead of representing the pitch levels directly by any system of notation. In this paper, we continued to apply the new approach to search for the existence of a general quantitative relationship governing the pitch distances between each Cantonese level tone. We gathered more recordings and modified the elicitation process of the pitch height of all the four level tones [2] by breaking down the single process for all tones into two sub-processes to avoid the problem of pitch reset.\n",
    "s Ho, R. S.-C., Ho, S.-F., Musical scales and Cantonese level tones, in Proc. 1st International Conference on Music Communication Science, Sydney, p. 64-67. /li>Ho, R. S.-C., Sagisaka, Y., F0 analysis of perceptual distance among Cantonese level tones, in Proc. INTERSPEECH 2007, Antwerp, Belgium, p.1038-1041. (ISCA Archive, http://www.isca-speech.org/archive/interspeech_2007)\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-340"
  },
  "zhu08b_interspeech": {
   "authors": [
    [
     "Xiaonong",
     "Zhu"
    ],
    [
     "Caicai",
     "Zhang"
    ]
   ],
   "title": "A seven-tone dialect in southern China with falling-rising-falling contour: a linguistic acoustic analysis",
   "original": "i08_1113",
   "page_count": 3,
   "order": 369,
   "p1": "1113",
   "pn": "1115",
   "abstract": [
    "This paper aims to give a quantitative description of the linguistic acoustic properties, i.e., fundamental frequency and duration, of a seven-tone dialect in Qiyang County (QY hereafter), south of Hunan Province, China. There are seven surface contrasting tones in the QY dialect, a branch of Old Xiang group. No acoustic analysis has been done in the QY dialect so far. What this paper concentrates on is the falling-rising-falling (FRF hereafter) pitch in two of the seven tones, which has been reported nowhere else. Many parameters which have potential influence on the pitch contour are considered so as to specify the factors which contribute to the forming of this FRF contour. Although emphasis is placed on the analysis of citation tones of monosyllables, evidence from disyllables will be provided as well to support the pitch contour found in monosyllables. But a further perception test should be conducted to decide if the FRF contour is linguistically significant.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-341"
  },
  "promon08_interspeech": {
   "authors": [
    [
     "Santitham",
     "Prom-on"
    ]
   ],
   "title": "Pitch target analysis of Thai tones using quantitative target approximation model and unsupervised clustering",
   "original": "i08_1116",
   "page_count": 4,
   "order": 370,
   "p1": "1116",
   "pn": "1119",
   "abstract": [
    "This paper presents the integration between the quantitative target approximation (qTA) model and the unsupervised clustering technique to study Thai tones. The qTA model simulates F0 production on the basis of articulation process. Parameters extracted from the F0 of Thai speech by analysis-and-synthesis method were further analyzed by K-means clustering. The number and form of pitch target were then identified by assessing the quality of clusters using silhouette scoring. The combination of qTA model and computational intelligent techniques make it possible to study pitch targets of tone function without prior hypothesis.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-342"
  },
  "so08b_interspeech": {
   "authors": [
    [
     "Connie K.",
     "So"
    ],
    [
     "Catherine T.",
     "Best"
    ]
   ],
   "title": "Do English speakers assimilate Mandarin tones to English prosodic categories?",
   "original": "i08_1120",
   "page_count": 1,
   "order": 371,
   "p1": "1120",
   "pn": "",
   "abstract": [
    "This study examined whether native English (NE) speakers perceive non-native tones of Mandarin in terms of their English intonational categories (Flat pitch, Question, Uncertainty, and Statement). The results indicated that NE listeners assimilated non-native tones to their native intonational categories, which share phonetic similarities with those of Mandarin tones. Thus, the assumption that assimilations of non-native prosodic categories (e.g., tones) to native prosodic categories - an extension from the Perceptual Assimilation Model (PAM) - is supported.\n",
    ""
   ]
  },
  "bundgaardnielsen08_interspeech": {
   "authors": [
    [
     "Rikke L.",
     "Bundgaard-Nielsen"
    ],
    [
     "Catherine T.",
     "Best"
    ],
    [
     "Michael D.",
     "Tyler"
    ],
    [
     "Christian",
     "Kroos"
    ]
   ],
   "title": "Evidence of a near-merger in western sydney australian English vowels",
   "original": "i08_1121",
   "page_count": 1,
   "order": 372,
   "p1": "1121",
   "pn": "",
   "abstract": [
    "Research on various dialects of English has demonstrated the existence of so-called near-mergers. The present study examines the identification and discrimination of the vowels of Western Sydney Australian English, for which no such mergers have been previously documented. We find evidence for perceptual confusion of /i/-/I@/ (in /hVb@/), despite significant acoustic differences in productions of these two vowels. This meets the defining features of a near-merger.\n",
    ""
   ]
  },
  "tabain08_interspeech": {
   "authors": [
    [
     "Marija",
     "Tabain"
    ],
    [
     "Kristine",
     "Rickard"
    ],
    [
     "Gavan",
     "Breen"
    ],
    [
     "Veronica",
     "Dobson"
    ]
   ],
   "title": "Central vowels in Arrernte: metrical prominence and pitch accent",
   "original": "i08_1122",
   "page_count": 1,
   "order": 373,
   "p1": "1122",
   "pn": "",
   "abstract": [
    "This paper presents duration and formant data for the central vowels /@/ (schwa) and /a/ in Arrernte, a central Australian language. Results show that /@/ has a shorter duration and higher position in the vowel space in metrically prominent syllables, whereas /a/ has a longer duration and no change in formant structure. By contrast, effects of pitch accent are minimal for both vowels.\n",
    ""
   ]
  },
  "ross08_interspeech": {
   "authors": [
    [
     "Bella",
     "Ross"
    ]
   ],
   "title": "Pausing and phrase length in two australian languages",
   "original": "i08_1123",
   "page_count": 1,
   "order": 374,
   "p1": "1123",
   "pn": "",
   "abstract": [
    "Pausing in speech allows a speaker to plan for the upcoming utterance, as well as to indicate to the listener finality of utterance. In this paper I examine pause patterning and IPU length in two typologically distinct Australian languages; Dalabon and Kayardild. Results show that Dalabon prefers considerably longer stretches of continuous speech than Kayardild. Preliminary findings reveal that the length of an IPU does not correlate to pause durations.\n",
    ""
   ]
  },
  "stevens08_interspeech": {
   "authors": [
    [
     "Mary",
     "Stevens"
    ],
    [
     "John",
     "Hajek"
    ]
   ],
   "title": "Positional effects on the characterization of ejectives in Waima'a",
   "original": "i08_1124",
   "page_count": 4,
   "order": 375,
   "p1": "1124",
   "pn": "1127",
   "abstract": [
    "This paper presents results from an ongoing investigation into stop consonants in Waima'a, focusing on the issue of tense v. lax ejectives. Sources tend to describe ejectives in a given language as either tense or lax; however ejectives in Waima'a, do not fit squarely into either category [4]. Here we compare ejectives in word-initial and word-medial contexts, to specifically address the role of word-position in the tense/lax distinction. Results show that word-position affects the duration of all stop types analyzed, i.e. unaspirated, post-aspirated, & ejective stops. Variability amongst the ejective tokens suggests that the notion of a tense/lax dichotomy should be replaced instead with that of a tense/lax continuum.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-343"
  },
  "starks08_interspeech": {
   "authors": [
    [
     "Donna",
     "Starks"
    ],
    [
     "Laura",
     "Thompson"
    ],
    [
     "Catherine I.",
     "Watson"
    ]
   ],
   "title": "A Niuean variant of New Zealand English?",
   "original": "i08_1128",
   "page_count": 1,
   "order": 376,
   "p1": "1128",
   "pn": "",
   "abstract": [
    "This paper provides an acoustic analysis of the vowels of five Niuean speakers of New Zealand English (NZE). The speakers are second-generation New Zealanders raised in a bilingual English/ Niuean environment. Analysis reveals differences between the vowel space of the Niuean New Zealanders and speakers of general NZE. Of particular note is the use of monophthongal variants of the HAY and HOE vowels. The paper ends with a discussion of whether these variants serve as ethnic identity markers in the New Zealand Niuean community.\n",
    ""
   ]
  },
  "ding08d_interspeech": {
   "authors": [
    [
     "Guo-Hong",
     "Ding"
    ]
   ],
   "title": "Phonetic confusion analysis and robust phone set generation for Shanghai-accented Mandarin speech recognition",
   "original": "i08_1129",
   "page_count": 4,
   "order": 377,
   "p1": "1129",
   "pn": "1132",
   "abstract": [
    "In this paper, accent issues are discussed for Shanghai-accented Mandarin speech recognition. The phonetic confusion is analyzed in detail based on the alignment between the surface form and the baseform transcriptions. Mutual information is used as the measure to extract the most confusing phoneme pairs. It was found that each phoneme in one pair can be easily misrecognized with the other. To remove the phonetic confusion, it is better to replace the two phonemes in one pair with a newly generated one. Consequentially new phone sets are derived. The phonetic confusion analysis and the experimental evaluation are performed on a Shanghai-accented Mandarin speech corpus. Experimental results show that compared to the canonical phone set, the generated one can reduce the substitution error greatly and achieve a 0.72% absolute Chinese character error rate (CER) reduction. When it is combined with pronunciation modeling, the absolute CER reduction is 1.58%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-344"
  },
  "yeung08b_interspeech": {
   "authors": [
    [
     "Yu Ting",
     "Yeung"
    ],
    [
     "Yao",
     "Qian"
    ],
    [
     "Tan",
     "Lee"
    ],
    [
     "Frank K.",
     "Soong"
    ]
   ],
   "title": "Prosody for Mandarin speech recognition: a comparative study of read and spontaneous speech",
   "original": "i08_1133",
   "page_count": 4,
   "order": 378,
   "p1": "1133",
   "pn": "1136",
   "abstract": [
    "In this paper, we present a comparative study between spontaneous speech and read Mandarin speech in the context of automatic speech recognition. We focus on analysis and modeling of prosodic features, based on a unique speech corpus that contains similar amounts of read and spontaneous speech data from the same group of speakers. Statistical analysis is carried out on tone contours and duration of syllable and sub-syllable units. Speech recognition experiments are performed to evaluate the effectiveness of different approaches to incorporate prosodic features into acoustic modeling. A key problem being addressed is how to deal with the unvoiced frames where F0 values are unavailable. We apply the technique of Multi-space distribution (MSD) to model partially continuous F0 contours. For spontaneous speech, the tonal-syllable error rate is reduced from the MFCC baseline of 64.8% to 59.4% with the MSD based prosody model. For read speech, the performance improves from 46.0% to 36.4%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-345"
  },
  "cheng08b_interspeech": {
   "authors": [
    [
     "Li-Wei",
     "Cheng"
    ],
    [
     "Lin-shan",
     "Lee"
    ]
   ],
   "title": "Improved large vocabulary Mandarin speech recognition by selectively using tone information with a two-stage prosodic model",
   "original": "i08_1137",
   "page_count": 4,
   "order": 379,
   "p1": "1137",
   "pn": "1140",
   "abstract": [
    "The incorporation of prosodic information in large vocabulary continuous speech recognition has attracted much attention in recent years, especially for a tonal language such as Mandarin Chinese. The tones of some syllables are very difficult to recognize correctly due to the very complicated prosodic behavior. Tone recognition errors inevitably degrade the recognition accuracy seriously. We propose a new approach by introducing an extra tone category of \"unknown.\" When the tone is difficult to recognize, the tone information will not be used. A two-stage prosodic model is developed for such a propose, and a 17.8% reduction in character error rate was achieved. Notably, this approach does not require speaker normalization for prosodic features.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-346"
  },
  "ru08_interspeech": {
   "authors": [
    [
     "Tingting",
     "Ru"
    ],
    [
     "Xiang",
     "Xie"
    ],
    [
     "Hui",
     "Yin"
    ],
    [
     "Jingming",
     "Kuang"
    ]
   ],
   "title": "Mandarin connected digits recognition for whispered speech",
   "original": "i08_1141",
   "page_count": 4,
   "order": 380,
   "p1": "1141",
   "pn": "1144",
   "abstract": [
    "In this paper, the acoustic characteristics and recognition of whispered speech are discussed. A Mandarin digits database is built both in normal speech and whispered speech. The collected speech materials of normal and whispered speech are analyzed to verify the characteristics and differences for the two kinds of speech. Cross recognition is carried out using normal and whispered speech as training data and testing data respectively, and the detailed recognition results are analyzed by using the confusion matrices. The results show that it's not suitable to recognize whispered speech using models trained by normal speech, and the word correct rate of the whispered speech is in close relation with its acoustic characteristics. Some possible solutions are also suggested.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-347"
  },
  "bao08b_interspeech": {
   "authors": [
    [
     "Changchun",
     "Bao"
    ],
    [
     "Weiqun",
     "Xu"
    ],
    [
     "Yonghong",
     "Yan"
    ]
   ],
   "title": "Recognizing named entities in spoken Chinese dialogues with a character-level maximum entropy tagger",
   "original": "i08_1145",
   "page_count": 4,
   "order": 381,
   "p1": "1145",
   "pn": "1148",
   "abstract": [
    "Named Entity Recognition (NER) is an important task in information extraction, where major attention has been paid to written texts of a news or academic paper (esp. biomedical) style. In this paper we report the first piece of work on NER in spoken Chinese dialogues, as a preliminary step for spoken language understanding. The NER task is taken as a sequential classification problem and solved with a character-level maximum entropy (maxent) tagger. Despite that spoken data seems noisier than written data, with a set of carefully selected features, the maxent tagger achieves an overall F1 score of 91.87 on our dialogue data.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-348"
  },
  "nguyen08c_interspeech": {
   "authors": [
    [
     "Hong Quang",
     "Nguyen"
    ],
    [
     "Pascal",
     "Nocera"
    ],
    [
     "Eric",
     "Castelli"
    ],
    [
     "Van Loan",
     "Trinh"
    ]
   ],
   "title": "A novel approach in continuous speech recognition for Vietnamese, an isolating tonal language",
   "original": "i08_1149",
   "page_count": 4,
   "order": 382,
   "p1": "1149",
   "pn": "1152",
   "abstract": [
    "This paper proposes a new approach for the integration of the Vietnamese language characteristics into a Large Vocabulary Continuous Speech Recognition System (LVCSR) which was built for some European languages. Firstly, a new module of tone recognition using Hidden Markov model was constructed. Secondly, several methods were applied to transform a text corpus of monosyllabic words into text corpus of polysyllabic words and a statistical language model of polysyllabic words was built by using the new text corpus. Finally, all the knowledge has been included in the LVCSR system so that this system can be adapted for Vietnamese. Experiments are made on the VNSPEECHCORPUS. The results show that the accuracy of Vietnamese recognition system was increased, 46% of relative reduction of the word error rate is obtained by using Vietnamese language characteristics.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-349"
  },
  "thomson08b_interspeech": {
   "authors": [
    [
     "B.",
     "Thomson"
    ],
    [
     "K.",
     "Yu"
    ],
    [
     "M.",
     "Gašić"
    ],
    [
     "S.",
     "Keizer"
    ],
    [
     "F.",
     "Mairesse"
    ],
    [
     "J.",
     "Schatzmann"
    ],
    [
     "Steve",
     "Young"
    ]
   ],
   "title": "Evaluating semantic-level confidence scores with multiple hypotheses",
   "original": "i08_1153",
   "page_count": 4,
   "order": 383,
   "p1": "1153",
   "pn": "1156",
   "abstract": [
    "In any dialogue manager, confidence scores play a central role in ensuring robust operation. Recently, dialogue managers have attempted to exploit N-best lists of alternatives for the semantics rather than the single most likely interpretation. Each alternative in the N-best list must have an associated confidence score and it is very useful to be able to evaluate the utility of these scored lists independent of the application in which they are used. This paper adapts several traditional metrics for confidence scoring to the context of the N-best semantic hypotheses output by a speech understanding system. An alternative metric, called the Item-level Cross Entropy (ICE), is proposed and is shown to have good theoretical and experimental characteristics. As an example of the use of the metrics, various simple methods for assigning confidences are discussed and evaluated. Of all the metrics tested only the ICE metric provided a consistent monotonic ranking of the various systems.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-350"
  },
  "zweig08_interspeech": {
   "authors": [
    [
     "Geoffrey",
     "Zweig"
    ],
    [
     "Dan",
     "Bohus"
    ],
    [
     "Xiao",
     "Li"
    ],
    [
     "Patrick",
     "Nguyen"
    ]
   ],
   "title": "Structured models for joint decoding of repeated utterances",
   "original": "i08_1157",
   "page_count": 4,
   "order": 384,
   "p1": "1157",
   "pn": "1160",
   "abstract": [
    "Due to speech recognition errors, repetition can be a frequent occurrence in voice-search applications. While a proper treatment of this phenomenon requires the joint modeling of two or more utterances simultaneously, currently deployed systems typically treat the utterances independently. In this paper, we analyze the structure of repetitions and find that in at least one commercial directory assistance application, repetitions follow simple structural transformations more than 70% of the time. We present preliminary results that suggest that significant gains are possible by explicitly modeling this structure in a joint decoding process.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-351"
  },
  "meurs08_interspeech": {
   "authors": [
    [
     "Marie-Jean",
     "Meurs"
    ],
    [
     "Fabrice",
     "Lefevre"
    ],
    [
     "Renato De",
     "Mori"
    ]
   ],
   "title": "A Bayesian approach to semantic composition for spoken language interpretation",
   "original": "i08_1161",
   "page_count": 4,
   "order": 385,
   "p1": "1161",
   "pn": "1164",
   "abstract": [
    "This paper introduces a stochastic interpretation process for composing semantic structures. This process, dedicated to spoken language interpretation, allows to derive semantic frame structures directly from word and basic concept sequences representing the users' utterances. First a two-step rule-based process has been used to provide a reference semantic frame annotation of the speech training data. Then, through a decoding stage, dynamic Bayesian networks are used to hypothesize frames with confidence scores from test data. The semantic frames used in this work have been derived from the Berkeley FrameNet paradigm. Experiments are reported on the Media corpus. Media is a French dialog corpus recorded using a Wizard of Oz system simulating a telephone server for tourist information and hotel booking. For all the data the manual transcriptions and annotations at the word and concept levels are available. In order to evaluate the robustness of the proposed approach tests are performed under 3 different conditions raising in difficulty wrt the errors in the word and concept sequence inputs: (i) according to whether they are manually transcribed and annotated, (ii) manually transcribed and enriched with concepts provided by an automatic annotation, (iii) fully automatically transcribed and annotated. From the experiment results it appears that the proposed probabilistic framework is able to carry out semantic frame annotation with a good reliability, comparable to a semi-manual rule-based approach.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-352"
  },
  "paek08_interspeech": {
   "authors": [
    [
     "Tim",
     "Paek"
    ],
    [
     "Yun-Cheng",
     "Ju"
    ]
   ],
   "title": "Accommodating explicit user expressions of uncertainty in voice search or something like that",
   "original": "i08_1165",
   "page_count": 4,
   "order": 386,
   "p1": "1165",
   "pn": "1168",
   "abstract": [
    "Voice search applications encourage users to \"just say what you want\" in order to obtain useful mobile content such as automated directory assistance (ADA). Unfortunately, when users only remember part of what they are looking for, they are forced to guess, even though what they know may be sufficient to retrieve the desired information. In this paper, we propose expanding the capabilities of voice search to allow users to explicitly express their uncertainties as part of their queries, and as such, to provide partial knowledge. Applied to ADA, we highlight the enhanced user experience uncertain expressions afford and delineate how we performed language modeling and information retrieval. We evaluate our approach by assessing its impact on overall ADA performance and by discussing the results of an experiment in which users generated both uncertain expressions as well as guesses for directory listings. Uncertain expressions reduced relative error rate by 31.8% compared to guessing.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-353"
  },
  "kim08b_interspeech": {
   "authors": [
    [
     "Dongho",
     "Kim"
    ],
    [
     "Hyeong Seop",
     "Sim"
    ],
    [
     "Kee-Eung",
     "Kim"
    ],
    [
     "Jin Hyung",
     "Kim"
    ],
    [
     "Hyunjeong",
     "Kim"
    ],
    [
     "Joo Won",
     "Sung"
    ]
   ],
   "title": "Effects of user modeling on POMDP-based dialogue systems",
   "original": "i08_1169",
   "page_count": 4,
   "order": 387,
   "p1": "1169",
   "pn": "1172",
   "abstract": [
    "Partially observable Markov decision processes (POMDPs) have gained significant interest in research on spoken dialogue systems, due to among many benefits its ability to naturally model the dialogue strategy selection problem under the unreliability in automated speech recognition. However, the POMDP approaches are essentially model-based, and as a result, the dialogue strategy computed from POMDP is subject to the correctness of the model. In this paper, we extend some of the previous user models for POMDPs, and evaluate the effects of user models on the dialogue strategy computed from POMDP.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-354"
  },
  "williams08b_interspeech": {
   "authors": [
    [
     "Jason D.",
     "Williams"
    ]
   ],
   "title": "The best of both worlds: unifying conventional dialog systems and POMDPs",
   "original": "i08_1173",
   "page_count": 4,
   "order": 388,
   "p1": "1173",
   "pn": "1176",
   "abstract": [
    "Partially observable Markov decision processes (POMDPs) and conventional design practices offer two very different but complementary approaches to building spoken dialog systems. Whereas conventional manual design readily incorporates business rules, domain knowledge, and contextually appropriate system language, POMDPs employ optimization to produce more detailed dialog plans and better robustness to speech recognition errors. In this paper we propose a novel method for integrating these two approaches, capturing both of their strengths. The POMDP and conventional dialog manager run in parallel; the conventional dialog manager nominates a set of one or more actions, and the POMDP chooses the optimal action. Experiments using a real dialog system confirm that this unified architecture yields better performance than using a conventional dialog manager alone, and also demonstrate an improvement in optimization speed and reliability vs. a pure POMDP.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-355"
  },
  "bundgaardnielsen08b_interspeech": {
   "authors": [
    [
     "Rikke L.",
     "Bundgaard-Nielsen"
    ],
    [
     "Catherine T.",
     "Best"
    ],
    [
     "Michael D.",
     "Tyler"
    ]
   ],
   "title": "The assimilation of L2 australian English vowels to L1 Japanese vowel categories: vocabulary size matters",
   "original": "i08_1177",
   "page_count": 1,
   "order": 389,
   "p1": "1177",
   "pn": "",
   "abstract": [
    "Theory strongly suggests that L2 perception is influenced by the L1. More recently, it has also been proposed that L2 vocabulary size may be related to the perception of non-native phones. Japanese (JP) learners of Australian English (AE) identified AE vowels using JP vowel categories and provided goodness-of-fit ratings. Results show systematic perceptual assimilations of L2 to L1 vowels and learners with a larger L2 vocabulary provided more consistent identifications.\n",
    ""
   ]
  },
  "ali08_interspeech": {
   "authors": [
    [
     "Azra N.",
     "Ali"
    ],
    [
     "Mohamed",
     "Lahrouchi"
    ],
    [
     "Michael",
     "Ingleby"
    ]
   ],
   "title": "Vowel epenthesis, acoustics and phonology patterns in Moroccan Arabic",
   "original": "i08_1178",
   "page_count": 4,
   "order": 390,
   "p1": "1178",
   "pn": "1181",
   "abstract": [
    "In Moroccan Arabic it is widely accepted that short vowels are mostly elided, resulting in consonant clusters and consonant geminates. In this paper we present evidence from our exploratory timing study that challenges this widely accepted principle. We work with minimal pairs of singleton consonants vs. geminates (e.g. /bka/ vs. /b@kka/) that reveals a presence of a vowel insertion between the clusters in word initial position in singleton cases. The length of the vowel insertion (epenthetic vowel) and silent pause of the stop consonant is greater than of a noise material. The epenthetic vowel is present in isolated words and in sentence context too. In this paper we also provide phonetic correlates in the minimal pairs - between epenthetic vowel and lexical vowel, between singleton and geminate consonants, and contrast these with other Arabic dialect phonetic timing studies.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-356"
  },
  "wang08g_interspeech": {
   "authors": [
    [
     "Gaowu",
     "Wang"
    ],
    [
     "Jianwu",
     "Dang"
    ],
    [
     "Jiangping",
     "Kong"
    ]
   ],
   "title": "Estimation of vocal tract area function for Mandarin vowel sequences using MRI",
   "original": "i08_1182",
   "page_count": 4,
   "order": 391,
   "p1": "1182",
   "pn": "1185",
   "abstract": [
    "To fully explore the dynamic properties of speech production and investigate the relation between vocal tract geometry and speech acoustics, estimation of vocal tract area functions from measurements of the sagittal plane is an important step. In this study, we investigated the relation between the measurements on two dimensional (2D) and three dimensional (3D) MRI data and used an alpha-beta model to describe this relation. As a result, a set of parameters were derived from 3D static MRI data, and applied to time-varying vocal tract widths derived from 2D MRI movies, to synthesize Mandarin vowel sequences. An acoustic evaluation comparing the natural and calculated formants shows that the alpha-beta model can represent dynamic states of articulatory movements of vowel sequences, as well as those of the sustained vowels.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-357"
  },
  "tsukada08_interspeech": {
   "authors": [
    [
     "Kimiko",
     "Tsukada"
    ],
    [
     "Thu T. A.",
     "Nguyen"
    ]
   ],
   "title": "The effect of first language (L1) dialects on the identification of Vietnamese word-final stops",
   "original": "i08_1186",
   "page_count": 4,
   "order": 392,
   "p1": "1186",
   "pn": "1189",
   "abstract": [
    "This study examined the extent to which speakers' first language (L1) dialect affects the identification of word-final stops in Vietnamese. Stops in the word-final position are unreleased in Vietnamese. Further, there is a /t/-/k/ merger in the Southern, but not the Northern dialect. We tested the hypothesis that the stop tokens produced in the Southern dialect are identified less accurately than those in the Northern dialect. The results showed that the speakers' dialect influenced the intelligibility of the final stops and the Northern dialect was more intelligible than the Southern dialect.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-358"
  },
  "antoniou08_interspeech": {
   "authors": [
    [
     "Mark",
     "Antoniou"
    ],
    [
     "Catherine T.",
     "Best"
    ],
    [
     "Michael D.",
     "Tyler"
    ]
   ],
   "title": "Perceptual evidence of modern Greek voiced stops as phonological categories",
   "original": "i08_1190",
   "page_count": 1,
   "order": 393,
   "p1": "1190",
   "pn": "",
   "abstract": [
    "The phonological status of voiced stops in Modern Greek (MG) remains unclear. Research shows that listeners typically discriminate native phonological contrasts without difficulty. We report MG listeners show excellent discrimination of MG bilabial [p]-[b] and coronal [t]-[d] stop voicing contrasts, significantly better than their discrimination of nonnative Australian English (AE) [ph]-[p] and [th]-[t]. We interpret the results as evidence for the phonological status of MG voiced stops /b, d/ and thus of MG stop voicing contrasts.\n",
    ""
   ]
  },
  "hazan08_interspeech": {
   "authors": [
    [
     "Valerie",
     "Hazan"
    ],
    [
     "Enid",
     "Li"
    ]
   ],
   "title": "The effect of auditory and visual degradation on audiovisual perception of native and non-native speakers",
   "original": "i08_1191",
   "page_count": 4,
   "order": 394,
   "p1": "1191",
   "pn": "1194",
   "abstract": [
    "This study investigated the effect of visual and auditory degradation on the weighting given by listeners to visual cues for tokens produced by native and non-native speakers. 'Ba', 'da' and 'ga' syllables uttered by English and Chinese speakers were presented in auditory (A), visual (V) and congruent/incongruent audiovisual (AV) conditions to English listeners. In some conditions, the visual channel was degraded via Gaussian blurring, the auditory channel by pink noise, or both channels were degraded. In A or V conditions, degradation reduced intelligibility. In AV conditions with one degraded channel, the weighting of the other channel increased, with little effect of speaker-language. However, in clear or combined-degradation conditions, listeners gave greater weight to visual cues for non-native speakers. The weighting of auditory and visual cues is therefore highly flexible and there is strong support for a 'foreign-language effect' in visual weighting.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-359"
  },
  "mixdorff08_interspeech": {
   "authors": [
    [
     "Hansjörg",
     "Mixdorff"
    ]
   ],
   "title": "Quantitative prosodic analysis of spontaneous speech",
   "original": "i08_1195",
   "page_count": 1,
   "order": 395,
   "p1": "1195",
   "pn": "",
   "abstract": [
    "This paper presents a study analyzing the prosody of spontaneous speech exemplified by the OSU Buckeye corpus of American English employing the Fujisaki model and syllable duration analysis. Results show, inter alia, that model parameters are much more strongly influenced by the higher level discourse structure than by their syntactic function. The study shows that modeling strategies can be employed in a similar fashion as for more formal reading style speech.\n",
    ""
   ]
  },
  "lindstrom08_interspeech": {
   "authors": [
    [
     "Anders",
     "Lindström"
    ],
    [
     "Jessica",
     "Villing"
    ],
    [
     "Staffan",
     "Larsson"
    ],
    [
     "Alexander",
     "Seward"
    ],
    [
     "Nina",
     "Åberg"
    ],
    [
     "Cecilia",
     "Holtelius"
    ]
   ],
   "title": "The effect of cognitive load on disfluencies during in-vehicle spoken dialogue",
   "original": "i08_1196",
   "page_count": 4,
   "order": 396,
   "p1": "1196",
   "pn": "1199",
   "abstract": [
    "In-vehicle spoken dialogue systems are gaining increased interest by the automotive industry. They enable the driver to perform secondary tasks (i.e. tasks not related to driving the vehicle) without having to take her eyes off the road or her hands from the steering wheel. Dialogue systems also enable the driver to speak in a natural way, without having to memorize commands or navigate through a menu structure. It is however crucial to take the cognitive load of the driver into consideration, in order to be able to adapt the dialogue system accordingly. This paper presents a user study where spoken dialogues between drivers and passengers have been analysed to find out how spontaneous speech is affected by driving and carrying out other activities that increase the cognitive load of the user. The results indicate systematic changes in specific disfluency rates as the cognitive load increases.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-360"
  },
  "tseng08_interspeech": {
   "authors": [
    [
     "Chiu-yu",
     "Tseng"
    ],
    [
     "Zhao-yu",
     "Su"
    ]
   ],
   "title": "Discourse prosody context - global F0 and tempo modulations",
   "original": "i08_1200",
   "page_count": 4,
   "order": 397,
   "p1": "1200",
   "pn": "1203",
   "abstract": [
    "The present study is a corpus analysis of discourse prosodic information using two different types of fluent continuous Mandarin speech. Global F0 heights and duration patterns of withinand between-paragraph phrases were compared by discourse positions. Results showed that overall phrase-level F0 height was paragraph-initial>-medial>-final while the tempo pattern was paragraph-initial<-medial<-final. All of the differences were statistically significant across speakers and speech materials. The results suggest that discourse prosody context provides information of discourse planning, within-paragraph phrase association and between-paragraph topic change. We argue that global discourse prosody context is a crucial factor of speech communication, and can be applied to discourse segmentation, TTS synthesis naturalness and language pedagogy.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-361"
  },
  "obin08_interspeech": {
   "authors": [
    [
     "Nicolas",
     "Obin"
    ],
    [
     "Anne",
     "Lacheret-Dujour"
    ],
    [
     "Christophe",
     "Veaux"
    ],
    [
     "Xavier",
     "Rodet"
    ],
    [
     "Anne-Catherine",
     "Simon"
    ]
   ],
   "title": "A method for automatic and dynamic estimation of discourse genre typology with prosodic features",
   "original": "i08_1204",
   "page_count": 4,
   "order": 398,
   "p1": "1204",
   "pn": "1207",
   "abstract": [
    "This paper presents a work-in-progress on the automatic analysis of discourse genre in non-elicited speech. The study is focused on the development of bottom-up methods for automatic validation of discourse typologies found in linguistic descriptions (prosodic, syntactic, pragmatic and/or contextual and situational cues). The linguistic classification examined here opposes five discourse genres +/- controlled. To test this a priori classification under prosodic criteria, we propose a method that provides an automatic and dynamic estimation of discourse genre typology i.e. of prosodic similarities between discourse genres. This is achieved in a two-step procedure: a set of discriminant prosodic patterns is estimated and then used to raise a typology of discourse genres based on prosodic similarity criterion. The discriminant analysis reveals that a small number of prosodic patterns is sufficient to discriminate the 5 discourse genres. The typological analysis reveals some multi-level caterogical oppositions on a continuous prosodic scale that can be interpreted in terms of +/- controlled speech.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-362"
  },
  "ishi08_interspeech": {
   "authors": [
    [
     "Carlos Toshinori",
     "Ishi"
    ],
    [
     "Hiroshi",
     "Ishiguro"
    ],
    [
     "Norihiro",
     "Hagita"
    ]
   ],
   "title": "The meanings carried by interjections in spontaneous speech",
   "original": "i08_1208",
   "page_count": 4,
   "order": 399,
   "p1": "1208",
   "pn": "1211",
   "abstract": [
    "This paper presents a qualitative analysis on the variations in the speaking style and in the meanings (paralinguistic information) carried by several interjections appearing in spontaneous speech. Analysis results show that part of the paralinguistic information carried by several interjections can be associated to their speaking styles. On the other hand, the effects of small differences in the phonetic content, on the possible meanings carried by the interjections were also analyzed. The non-modal voice qualities, such as whispery, harsh and pressed voices, were also shown to be important in the expression of emotions and attitudes.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-363"
  },
  "jones08_interspeech": {
   "authors": [
    [
     "Christian M.",
     "Jones"
    ],
    [
     "Andrew",
     "Deeming"
    ]
   ],
   "title": "Speech interaction with an emotional robotic dog",
   "original": "i08_1212",
   "page_count": 4,
   "order": 400,
   "p1": "1212",
   "pn": "1215",
   "abstract": [
    "The consumer electronics of the future will be aware of the emotion of their users in order to provide more natural, engaging, entertaining and productive experiences. The paper reports on the development and evaluation of an emotionally responsive robotic dog. The dog is able to recognise emotion in its user through acoustic emotion recognition, and respond through a series of actions. In particular this paper details the technology implementation of the acoustic emotion recognition and the integration into the robotic dog.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-364"
  },
  "ochi08_interspeech": {
   "authors": [
    [
     "Keiko",
     "Ochi"
    ],
    [
     "Keikichi",
     "Hirose"
    ],
    [
     "Nobuaki",
     "Minematsu"
    ]
   ],
   "title": "Control of prosodic focus in corpus-based generation of fundamental frequency based on the generation process model",
   "original": "i08_1216",
   "page_count": 1,
   "order": 401,
   "p1": "1216",
   "pn": "",
   "abstract": [
    "A method was developed for generating sentence F0 contours, when a focus is placed in one of bunsetsu of an utterance. The method is to predict differences in F0 model commands between with and without focus utterances, and applies them to the F0 model commands predicted beforehand by the baseline method. The validity of the method was proved by the experiment on F0 contour generation and speech synthesis.\n",
    ""
   ]
  },
  "godin08_interspeech": {
   "authors": [
    [
     "Keith W.",
     "Godin"
    ],
    [
     "John H. L.",
     "Hansen"
    ]
   ],
   "title": "Analysis and perception of speech under physical task stress",
   "original": "i08_1674",
   "page_count": 4,
   "order": 402,
   "p1": "1674",
   "pn": "1677",
   "abstract": [
    "It is known that speech under physical task stress degrades speech system performance. Therefore, an analysis of speech under physical task stress is performed across several parameters to identify acoustic correlates. Formal listener tests are also performed to determine the relationship between acoustic correlates and perception. To verify the statistical significance of all results, student-t statistical tests are applied. It was found that fundamental frequency decreases for many speakers, that utterance duration increases for some speakers and decreases for others, and that the glottal waveform is quantifiably different for many speakers. Perturbation of two speech features, fundamental frequency and the glottal waveform, is applied in listener tests to quantify the degree to which these features convey physical stress content in speech. Finally, the enhanced understanding of physical task stress speech provided here is discussed in the context of speech systems.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-365"
  },
  "lee08_interspeech": {
   "authors": [
    [
     "Chi-Chun",
     "Lee"
    ],
    [
     "Sungbok",
     "Lee"
    ],
    [
     "Shrikanth S.",
     "Narayanan"
    ]
   ],
   "title": "An analysis of multimodal cues of interruption in dyadic spoken interactions",
   "original": "i08_1678",
   "page_count": 4,
   "order": 403,
   "p1": "1678",
   "pn": "1681",
   "abstract": [
    "Interruptions are integral elements of natural spontaneous human interaction. Both competitive and cooperative interruption serve a distinct role in the flow of conversation. This paper analyzes their differences with features, change and activeness, employing audio, visual, and disfluency data. These features are able to capture differences between the two types of interruptions better than average feature values of any single modality. Also, discriminant analysis shows that the use of multimodal cues provides a 21% improvement in classification accuracy between the two types of interruptions relative to the baseline while any individual single modality cue does not provide significant improvement.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-366"
  },
  "mori08_interspeech": {
   "authors": [
    [
     "Hiroki",
     "Mori"
    ],
    [
     "Hideki",
     "Kasuya"
    ]
   ],
   "title": "Paralinguistic effects on turn-taking behavior in expressive conversation",
   "original": "i08_1682",
   "page_count": 1,
   "order": 404,
   "p1": "1682",
   "pn": "",
   "abstract": [
    "Speaker and paralinguistic properties of dialogue speech that affect the timing at turn changes are investigated by analyzing the UU Database for Paralinguistic Information Studies. The results showed a large variation among speakers and a strong interaction with partner in pause/overlap duration. In addition, perceived emotional states of utterances had significant effects on the pause/overlap duration.\n",
    ""
   ]
  },
  "yin08c_interspeech": {
   "authors": [
    [
     "Zhigang",
     "Yin"
    ],
    [
     "Aijun",
     "Li"
    ],
    [
     "Ziyu",
     "Xiong"
    ]
   ],
   "title": "Study on \"ng, a\" type of discourse markers in standard Chinese",
   "original": "i08_1683",
   "page_count": 4,
   "order": 405,
   "p1": "1683",
   "pn": "1686",
   "abstract": [
    "This paper attempts to give a detailed study on discourse markers of the \"../ng/&../a/\" type in Chinese spontaneous dialogues. Discourse markers (DMs) are considered to have coherent function to the context. It is treated as a kind of linguistic modality to express interactive information or a kind of nonlinguistic modality such as head and body gestures. The discourse markers have aroused many discussions from different aspects of grammar, semantics, cognition and pragmatics. However, this paper will focus on those DMs like \"/ng/\" and \"/a/\" from multi-functional and hierarchical points of view and investigate their phonetic relatives or patterns.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-367"
  },
  "moniz08_interspeech": {
   "authors": [
    [
     "Helena",
     "Moniz"
    ],
    [
     "Ana Isabel",
     "Mata"
    ],
    [
     "Isabel",
     "Trancoso"
    ],
    [
     "M. Ceu",
     "Viana"
    ]
   ],
   "title": "How can you use disfluencies and still sound as a good speaker?",
   "original": "i08_1687",
   "page_count": 1,
   "order": 406,
   "p1": "1687",
   "pn": "",
   "abstract": [
    "This paper explores the results of a previous experiment concerning listeners' ratings of different types of (dis)fluencies and extends the analysis of such phenomena to a corpus of university lectures. Results suggest that, although not all disfluency types are equally tolerated by listeners, such differences may be overridden by an adequate control of tonal scaling and pause length, at least.\n",
    ""
   ]
  },
  "strangert08_interspeech": {
   "authors": [
    [
     "Eva",
     "Strangert"
    ],
    [
     "Joakim",
     "Gustafson"
    ]
   ],
   "title": "What makes a good speaker? subject ratings, acoustic measurements and perceptual evaluations",
   "original": "i08_1688",
   "page_count": 4,
   "order": 407,
   "p1": "1688",
   "pn": "1691",
   "abstract": [
    "This paper deals with subjective qualities and acoustic-prosodic features contributing to the impression of a good speaker. Subjects rated a variety of samples of political speech on a number of subjective qualities and acoustic features were extracted from the speech samples. A perceptual evaluation was also conducted with manipulations of F0 dynamics, fluency and speech rate with the sample of the lowest rated speaker as a basis. Subjects' ranking revealed a clear preference for modified versions over the original with F0 dynamics - a wider range - being the most powerful cue.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-368"
  },
  "kousidis08_interspeech": {
   "authors": [
    [
     "Spyros",
     "Kousidis"
    ],
    [
     "David",
     "Dorran"
    ],
    [
     "Yi",
     "Wang"
    ],
    [
     "Brian",
     "Vaughan"
    ],
    [
     "Charlie",
     "Cullen"
    ],
    [
     "Dermot",
     "Campbell"
    ],
    [
     "Ciaran",
     "McDonnell"
    ],
    [
     "Eugene",
     "Coyle"
    ]
   ],
   "title": "Towards measuring continuous acoustic feature convergence in unconstrained spoken dialogues",
   "original": "i08_1692",
   "page_count": 4,
   "order": 408,
   "p1": "1692",
   "pn": "1695",
   "abstract": [
    "Acoustic/prosodic feature (a/p) convergence has been known to occur both in dialogues between humans, as well as in humancomputer interactions. Understanding the form and function of convergence is desirable for developing next generation conversational agents, as this will help increase speech recognition performance and naturalness of synthesized speech. Currently, the underlying mechanisms by which continuous and bi-directional convergence occurs are not well understood. In this study, a direct comparison between time-aligned frames shows significant similarity in acoustic feature variation between the two speakers. The method described (TAMA) constitutes a first step towards a quantitative analysis of a/p convergence.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-369"
  },
  "kawahara08b_interspeech": {
   "authors": [
    [
     "Tatsuya",
     "Kawahara"
    ],
    [
     "Masayoshi",
     "Toyokura"
    ],
    [
     "Teruhisa",
     "Misu"
    ],
    [
     "Chiori",
     "Hori"
    ]
   ],
   "title": "Detection of feeling through back-channels in spoken dialogue",
   "original": "i08_1696",
   "page_count": 1,
   "order": 409,
   "p1": "1696",
   "pn": "",
   "abstract": [
    "We investigate the usage of back-channel information in the information navigation dialogue between an expert guide and a user. By back-channel feedback, we mean the user's verbal short response, which expresses his state of the mind during the dialogue. Its prototypical lexical entries include \"hai\" in Japanese and \"yes\" or \"right\" in English, however, we do not count explicit affirmative responses as back-channels.\n",
    "Previously, there were several works[1, 2] which attempted to automatically generate back-channel responses for smooth communication between the user and the system. Recently, the back-channel information is included in the framework of dialogue act tagging in the game-playing dialogue[3] and meetings[4]. In the information navigation dialogue, in which an expert guide presents a list of recommendation spots, it is expected that the prosodic pattern of the back-channel conveys the para-linguistic information, that is, it suggests the positive/ negative feeling on the recommended candidate. We also presume that the human expert guide detects such feelings expressed via back-channels, and chooses to continue the explanation of the current topic if the user seems interested, or change the topic otherwise. Thus, we investigate the back-channel patterns observed in the Kyoto Tour Guide Dialog Corpus.\n",
    "s N.Ward. Using prosodic clues to decide when to produce backchannel utterances. In Proc. ICSLP, pages 1728-1731, 1996. N.Kitaoka, M.Takeuchi, R.Nishimura, and S.Nakagawa. Response timing detection using prosodic and linguistic information for human-friendly spoken dialog systems. J. Japanese Society for Artificial Intelligence, 20(3):220-228, 2005. A.Gravano, S.Benus, J.Hirschberg, S.Mitchell, and I.Vovsha. Classification of discourse functions of affirmative words in spoken dialogue. In Proc. INTERSPEECH, pages 1613-1616, 2007. F.Yang, G.Tur, and E.Shriberg. Exploiting dialog act tagging and prosodic information for action item identification. In Proc. IEEE-ICASSP, pages 4941-4944, 2008.\n",
    ""
   ]
  },
  "karafiat08_interspeech": {
   "authors": [
    [
     "Martin",
     "Karafiát"
    ],
    [
     "Lukáš",
     "Burget"
    ],
    [
     "Thomas",
     "Hain"
    ],
    [
     "Jan",
     "Černocký"
    ]
   ],
   "title": "Discrimininative training of narrow band - wide band adapted systems for meeting recognition",
   "original": "i08_1217",
   "page_count": 4,
   "order": 410,
   "p1": "1217",
   "pn": "1220",
   "abstract": [
    "The amount of training data has a crucial effect on the accuracy of HMM based meeting recognition systems. One of the largest collections of speech data is conversational telephone speech which was found to match speech in meetings well. However it is naturally recorded with limited bandwidth. In previous work we presented a scheme that allows to transform wide-band meeting data into the same space for improved model training. In this paper we focused on integration of discriminative adaptation into this scheme. This integration is not straightforward and we present the complexity of this process. The models are tested on the NIST RT'05 meeting evaluation where a relative reduction in word error rate of 5.6% against non-adapted meeting system was achieved.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-370"
  },
  "hahm08_interspeech": {
   "authors": [
    [
     "Seongjun",
     "Hahm"
    ],
    [
     "Akinori",
     "Ito"
    ],
    [
     "Shozo",
     "Makino"
    ],
    [
     "Motoyuki",
     "Suzuki"
    ]
   ],
   "title": "A fast speaker adaptation method using aspect model",
   "original": "i08_1221",
   "page_count": 4,
   "order": 411,
   "p1": "1221",
   "pn": "1224",
   "abstract": [
    "We propose a fast speaker adaptation method using an aspect model. The performance of speaker independent (SI) model is very sensitive to environments such as microphones, speakers, and noises. Speaker adaptation techniques try to obtain near speaker dependent (SD) performance with only small amounts of specific data and are often based on initial SI model. One of the most important purposes for adaptation algorithms is to modify a large number of parameters with only a small amount of adaptation data. The number of free parameters to be estimated from adaptation data can be reduced by using aspect model. In this paper, we introduce an aspect model into an acoustic model for rapid speaker adaptation. A formulation of probabilistic latent semantic analysis (PLSA) is extended to continuous density HMM. We carried out an isolated word recognition experiment on Korean database, and the results are compared to those of conventional expectation maximization (EM) algorithm, maximum a posteriori (MAP) and maximum likelihood linear regression (MLLR).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-371"
  },
  "su08_interspeech": {
   "authors": [
    [
     "Dan",
     "Su"
    ],
    [
     "Xihong",
     "Wu"
    ],
    [
     "Huisheng",
     "Chi"
    ]
   ],
   "title": "Probabilistic latent speaker training for large vocabulary speech recognition",
   "original": "i08_1225",
   "page_count": 4,
   "order": 412,
   "p1": "1225",
   "pn": "1228",
   "abstract": [
    "In this paper, we describe an improvement on probabilistic latent speaker analysis method and investigate the use of probabilistic latent speaker analysis for acoustic model training. By performing co-occurrence analysis between speaker and dominant components, speaker variation is dealt with based on different trajectories. Speech recognition experiment results show that our method, although with a general acoustic model and one-pass decoding, outperform the gender-dependent acoustic model with each gender is given for test set. Further experiment shows that the probabilistic latent speaker training method, although with no adaptation stage and no adaptation data, has outperformed the eigenMLLR adaptation method.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-372"
  },
  "tanji08_interspeech": {
   "authors": [
    [
     "Shutaro",
     "Tanji"
    ],
    [
     "Koichi",
     "Shinoda"
    ],
    [
     "Sadaoki",
     "Furui"
    ],
    [
     "Antonio",
     "Ortega"
    ]
   ],
   "title": "Improvement of eigenvoice-based speaker adaptation by parameter space clustering",
   "original": "i08_1229",
   "page_count": 4,
   "order": 413,
   "p1": "1229",
   "pn": "1232",
   "abstract": [
    "The segmental eigenvoice method has been proposed to provide rapid speaker adaptation with limited amounts of adaptation data. In this method, the speaker-vector space is clustered to several subspaces and PCA is applied to each of the resulting subspaces. In this paper, we propose two new techniques to improve the performance of this segmental eigenvoice approach. First, we propose a soft-clustering method in which each element in a speaker vector can be assigned to more than one cluster. Second, those elements far apart from any of the clusters are removed. Our experiments using the JNAS and S-JNAS databases show that the proposed method outperforms both the original eigenvoice and the segmental eigenvoice methods, e.g., 3.3% average improvement when only 10 utterances are used for adaptation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-373"
  },
  "sanand08_interspeech": {
   "authors": [
    [
     "D. R.",
     "Sanand"
    ],
    [
     "S.",
     "Umesh"
    ]
   ],
   "title": "Study of jacobian compensation using linear transformation of conventional MFCC for VTLN",
   "original": "i08_1233",
   "page_count": 4,
   "order": 414,
   "p1": "1233",
   "pn": "1236",
   "abstract": [
    "In this paper, we present a linear transformation (LT) to obtain warped features from unwarped features during vocal-tract length normalisation (VTLN). This LT between the warped and unwarped features is obtained within the conventional MFCC framework without any modification in the signal processing steps involved during the feature extraction stage. Further using the proposed LT, we study the effect of the Jacobian on the VTLN performance and show that it provides additional improvement in the recognition performance. The Jacobian of the proposed LT is simply the determinant of the LT matrix. Jacobian compensation is not done in conventional VTLN as the relation between warped and unwarped features is not known. We also study the effect of cepstral variance normalisation (CVN), which is often used as an approximation for Jacobian compensation in conventional VTLN. We show that the proposed Jacobian compensation gives better or comparable performance when compared to CVN.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-374"
  },
  "ting08_interspeech": {
   "authors": [
    [
     "Chuan-Wei",
     "Ting"
    ],
    [
     "Kuo-Yuan",
     "Lee"
    ],
    [
     "Jen-Tzung",
     "Chien"
    ]
   ],
   "title": "Adaptive HMM topology for speech recognition",
   "original": "i08_1237",
   "page_count": 4,
   "order": 415,
   "p1": "1237",
   "pn": "1240",
   "abstract": [
    "This paper presents an adaptive algorithm for compensating pronunciation variations in hidden Markov model (HMM) based speech recognition. The proposed method aims to adapt the HMM topology and the corresponding HMM parameters to meet the variations of speaker dialects. In adaptive HMM topology, two hypothesis test schemes are designed to detect whether a new speaking variation occurs in state/phone levels. The test statistics are approximated by the chi-square densities. A new HMM topology is automatically generated by a significance level. Simultaneously, the HMM parameters and their hyperparameters are updated by Bayesian learning of the newly-generated Markov models. The pronunciation variations are coped with by a dialect adaptive HMM topology. We develop the incremental algorithm for corrective training of HMM topology and parameters. Experiments on TIMIT database show that the proposed algorithm is substantially better than the standard HMM with comparable size of parameters.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-375"
  },
  "chen08e_interspeech": {
   "authors": [
    [
     "Liang-Yu",
     "Chen"
    ],
    [
     "Chun-Jen",
     "Lee"
    ],
    [
     "Jyh-Shing Roger",
     "Jang"
    ]
   ],
   "title": "Minimum phone error discriminative training for Mandarin Chinese speaker adaptation",
   "original": "i08_1241",
   "page_count": 4,
   "order": 416,
   "p1": "1241",
   "pn": "1244",
   "abstract": [
    "Speaker adaptation is an efficient way to model a new speaker from an existing speaker-independent model with limited speakerdependent data. In this paper, we investigate the use of discriminative training schemes based on the minimum phone error (MPE) criterion to improve a well-known speaker adaptation technique, a combination of transform-based adaptation and Bayesian adaptation. Furthermore, a new approach utilizing the statistics of the model-based regression tree for controlling the interpolation between maximum likelihood (ML) and MPE objective functions is also presented. Several comparative experiments were conducted on a continuous speech recognition task for Mandarin Chinese. Experimental results show that the proposed approach can further improve the performance of the original hybrid adaptation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-376"
  },
  "povey08b_interspeech": {
   "authors": [
    [
     "Daniel",
     "Povey"
    ],
    [
     "Hong-Kwang Jeff",
     "Kuo"
    ],
    [
     "Hagen",
     "Soltau"
    ]
   ],
   "title": "Fast speaker adaptive training for speech recognition",
   "original": "i08_1245",
   "page_count": 4,
   "order": 417,
   "p1": "1245",
   "pn": "1248",
   "abstract": [
    "In this paper we describe various fast and convenient implementations of Speaker Adaptive Training (SAT) for use in training when Maximum Likelihood Linear Regression (MLLR) is to be used in test time to adapt Gaussian means. The memory and disk requirements for most of these are similar to those for normal ML training; the computation in all cases is dominated by the need to compute the MLLR transforms. Commonly MLLR is combined with Constrained MLLR (CMLLR) which can be viewed as a feature space affine transform and has its own form of SAT (we will call this CMLLR-SAT); we experiment with combining the two forms of SAT. We find that even on top of CMLLR-SAT, MLLR-SAT gives improvements.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-377"
  },
  "raut08_interspeech": {
   "authors": [
    [
     "C. K.",
     "Raut"
    ],
    [
     "K.",
     "Yu"
    ],
    [
     "M. J. F.",
     "Gales"
    ]
   ],
   "title": "Adaptive training using discriminative mapping transforms",
   "original": "i08_1697",
   "page_count": 4,
   "order": 418,
   "p1": "1697",
   "pn": "1700",
   "abstract": [
    "Speaker adaptive training (SAT) is a useful technique for building speech recognition systems on non-homogeneous data. When combining SAT with discriminative training criteria, maximum likelihood (ML) transforms are often used for unsupervised adaptation tasks. This is because discriminatively estimated transforms are highly sensitive to errors in the supervision hypothesis. In this paper, speaker adaptive training based on discriminative mapping transforms (DMTs) is proposed. DMTs are speaker-independent discriminative transforms that are applied to ML-estimated speaker-specific transforms. As DMTs are estimated during training, they are not affected by errors in the supervision hypothesis. The proposed method was evaluated on an English conversational telephone speech task. It was found to significantly outperform the standard discriminative SAT schemes.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-378"
  },
  "loof08_interspeech": {
   "authors": [
    [
     "Jonas",
     "Loof"
    ],
    [
     "Christian",
     "Gollan"
    ],
    [
     "Hermann",
     "Ney"
    ]
   ],
   "title": "Speaker adaptive training using shift-MLLR",
   "original": "i08_1701",
   "page_count": 4,
   "order": 419,
   "p1": "1701",
   "pn": "1704",
   "abstract": [
    "In this paper a novel method for speaker adaptive training (SAT), based on Gaussian mean offset adaptation, so called Shift-MLLR, is presented. The method differs from previous SAT methods, where linear transformations of Gaussian means or features are utilized, in that only an offset vector is used for adaptation, but instead the number of regression classes is increased. This is shown to allow an efficient implementation. Furthermore, the use of word posterior confidence measures for Shift-MLLR is investigated, also in combination with the proposed SAT method. The presented methods are integrated into a state of the art speech recognition system, and performance is contrasted with Shift-MLLR without SAT, as well as with MLLR. Large and consistent improvements in word error rate are observed from the new SAT method, as well as from confidence based Shift-MLLR. The combination of the new speaker adaptive training method with confidence based estimation show consistent improvements.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-379"
  },
  "povey08c_interspeech": {
   "authors": [
    [
     "Daniel",
     "Povey"
    ],
    [
     "Hong-Kwang Jeff",
     "Kuo"
    ]
   ],
   "title": "XMLLR for improved speaker adaptation in speech recognition",
   "original": "i08_1705",
   "page_count": 4,
   "order": 420,
   "p1": "1705",
   "pn": "1708",
   "abstract": [
    "In this paper we describe a novel technique for adaptation of Gaussian means. The technique is related to Maximum Likelihood Linear Regression (MLLR), but we regress not on the mean itself but on a vector associated with each mean. These associated vectors are initialized by an ingenious technique based on eigen decomposition. As the only form of adaptation this technique outperforms MLLR, even with multiple regression classes and Speaker Adaptive Training (SAT). However, when combined with Constrained MLLR (CMLLR) and Vocal Tract Length Normalization (VTLN) the improvements disappear. The combination of two forms of SAT (CMLLR-SAT and MLLR-SAT) which we performed as a baseline is itself a useful result; we describe it more fully in a companion paper. XMLLR is an interesting approach which we hope may have utility in other contexts, for example in speaker identification.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-380"
  },
  "huang08c_interspeech": {
   "authors": [
    [
     "Jing",
     "Huang"
    ],
    [
     "Mark",
     "Epstein"
    ],
    [
     "Marco",
     "Matassoni"
    ]
   ],
   "title": "Effective acoustic adaptation for a distant-talking interactive TV system",
   "original": "i08_1709",
   "page_count": 4,
   "order": 421,
   "p1": "1709",
   "pn": "1712",
   "abstract": [
    "In this paper we have studied how to adapt a close-talking baseline acoustic model to a distant-talking application developed in an interactive TV dialogue system: distant-talking interfaces for control of interactive TV (DICIT) project. We have shown that in order to have effective adaptation from the out-of-domain data it is better to acquire that data in the same DICIT environment than using contaminated data. By measuring grammar error rate (GER) and action classification error rate (AER) in addition to word error rate (WER), we have shown the best way to adapt the baseline model using available out-of-domain adaptation data (TIMIT) and small amount of in-domain (DICIT) adaptation data. The best approach is to use cascading MAP adaptation. With less than 5 hours of out-of-domain data and 1 hour of in-domain data, the cascading MAP improves WER/GER/AER by 17%/18%/16% relative respectively over the baseline model. The experimental results show that in-domain adaptation data is definitely needed to improve GER and AER.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-381"
  },
  "akhil08_interspeech": {
   "authors": [
    [
     "P. T.",
     "Akhil"
    ],
    [
     "S. P.",
     "Rath"
    ],
    [
     "S.",
     "Umesh"
    ],
    [
     "D. R.",
     "Sanand"
    ]
   ],
   "title": "A computationally efficient approach to warp factor estimation in VTLN using EM algorithm and sufficient statistics",
   "original": "i08_1713",
   "page_count": 4,
   "order": 422,
   "p1": "1713",
   "pn": "1716",
   "abstract": [
    "In this paper, we develop a computationally efficient approach for warp factor estimation in Vocal Tract Length Normalization (VTLN). Recently we have shown that warped features can be obtained by a linear transformation of the unwarped features. Using the warp matrices we show that warp factor estimation can be efficiently performed in an EM framework. This can be done by collecting Sufficient Statistics by aligning the unwarped utterances only once. The likelihood of warped features, which are necessary for warp factor estimation, are computed by appropriately modifying the sufficient statistics using the warp matrices. We show using OGI, TIDIGITS and RM task that this approach has recognition performance that is comparable to conventional VTLN and yet is computationally more efficient.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-382"
  },
  "wang08h_interspeech": {
   "authors": [
    [
     "Shizhen",
     "Wang"
    ],
    [
     "Steven M.",
     "Lulich"
    ],
    [
     "Abeer",
     "Alwan"
    ]
   ],
   "title": "A reliable technique for detecting the second subglottal resonance and its use in cross-language speaker adaptation",
   "original": "i08_1717",
   "page_count": 4,
   "order": 423,
   "p1": "1717",
   "pn": "1720",
   "abstract": [
    "In previous work [1], we proposed a speaker adaptation technique based on the second subglottal resonance (Sg2), which showed good performance relative to vocal tract length normalization (VTLN). In this paper, we propose a more reliable algorithm for automatically estimating Sg2 from speech signals. The algorithm is calibrated on children's speech data collected simultaneously with accelerometer recordings from which Sg2 frequencies can be directly measured. To investigate whether Sg2 frequencies are independent of speech content and language, we perform a cross-language study with bilingual Spanish-English children. The study verifies that Sg2 is approximately constant for a given speaker and thus can be a good candidate for limited data speaker normalization and cross-language adaptation. We then present a cross-language speaker normalization method based on Sg2, which is computationally more efficient than maximum-likelihood based VTLN, and performs more robustly than VTLN.\n",
    "s S. Wang, A. Alwan and S. M. Lulich, \"Speaker normalization based on subglottal resonances,\" in Proc. ICASSP, pp. 4277-4280, 2008\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-383"
  },
  "fan08_interspeech": {
   "authors": [
    [
     "Xing",
     "Fan"
    ],
    [
     "John H. L.",
     "Hansen"
    ]
   ],
   "title": "Speaker identification for whispered speech based on frequency warping and score competition",
   "original": "i08_1313",
   "page_count": 4,
   "order": 424,
   "p1": "1313",
   "pn": "1316",
   "abstract": [
    "In certain situations, talkers will intentionally use whisper instead of neutral speech for the sake of privacy or confidentiality, which severely degrades the performance of speaker identification systems trained with only neutral speech. There are considerable differences in the spectral structure between whisper and neutral speech due to an absence of voice harmonic excitation. This study introduces a new feature based on frequency warping and score competition for the task of speaker identification for whisper. The proposed feature method is evaluated on a corpus of male speakers in both neutral and whisper. Closed set speaker ID results show an absolute 27% improvement in accuracy when compared with a traditional MFCC feature based system. The result confirms a viable approach to improving speaker ID performance between neutral and whisper speech condition.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-384"
  },
  "habib08_interspeech": {
   "authors": [
    [
     "Tania",
     "Habib"
    ],
    [
     "Lukas",
     "Ottowitz"
    ],
    [
     "Marián",
     "Képesi"
    ]
   ],
   "title": "Experimental evaluation of multi-band position-pitch estimation (m-popi) algorithm for multi-speaker localization",
   "original": "i08_1317",
   "page_count": 4,
   "order": 425,
   "p1": "1317",
   "pn": "1320",
   "abstract": [
    "This paper proposes an enhancement and evaluates the performance of the joint position and pitch estimation (PoPi) algorithm for speaker localization. A modification in the algorithm is introduced in order to improve the performance under high reverberation levels. The performance of the proposed method is evaluated by measuring the correct estimate of position at a frame level. This evaluation analyzes the algorithm performance in highly reverberant room by using real world recordings. A comparative analysis of the proposed method with the original, GCC-PHAT and SRP-PHAT methods is carried out. The results show that the proposed algorithm gives more consistent location estimates for multi-speaker scenarios.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-385"
  },
  "dhananjaya08_interspeech": {
   "authors": [
    [
     "N.",
     "Dhananjaya"
    ],
    [
     "S.",
     "Rajendran"
    ],
    [
     "B.",
     "Yegnanarayana"
    ]
   ],
   "title": "Features for automatic detection of voice bars in continuous speech",
   "original": "i08_1321",
   "page_count": 4,
   "order": 426,
   "p1": "1321",
   "pn": "1324",
   "abstract": [
    "In this paper we propose features for automatic detection of voice bar, which is an essential component of voiced stop consonants, in continuous speech. The acoustic-phonetic and production based knowledge such as, the presence of voicing, low strength of excitation compared to other voiced phones and a predominant low-frequency spectral energy, are mapped onto a set of acoustic features that can be automatically extracted from the signal. The usefulness of the proposed features in the detection of voice bars is studied using a knowledge-based as well as a neural network based approach. The performance of the proposed features and approaches is studied on phones from databases of two languages, namely English and Hindi.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-386"
  },
  "segura08_interspeech": {
   "authors": [
    [
     "Carlos",
     "Segura"
    ],
    [
     "Alberto",
     "Abad"
    ],
    [
     "Javier",
     "Hernando"
    ],
    [
     "Climent",
     "Nadeu"
    ]
   ],
   "title": "Speaker orientation estimation based on hybridation of GCC-PHAT and HLBR",
   "original": "i08_1325",
   "page_count": 4,
   "order": 427,
   "p1": "1325",
   "pn": "1328",
   "abstract": [
    "This paper presents a novel approach to speaker orientation estimation in a SmartRoom environment equipped with multiple microphones. The ratio between the high and low band energies (HLBR) received at each microphone has been shown in our previous work to be a potentially approach to estimate the direction of the voice produced by a speaker. In this work, for each microphone pair, a smoothed CPS phase is obtained by a proper windowing of the main peak of the cross-correlation sequence estimated with the GCC-PHAT method, and a HLBR is computed from the processed CPS. The proposed method keeps the computational simplicity of the HLBR algorithm while adding the robustness offered by the GCC-PHAT technique. Experimental preliminary results were conducted over a database recorded purposely in the UPC Smart room, and over the CLEAR head pose database. The proposed method performs consistently better than other state-of-the-art techniques with both databases.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-387"
  },
  "hou08_interspeech": {
   "authors": [
    [
     "Jun",
     "Hou"
    ],
    [
     "Lawrence R.",
     "Rabiner"
    ],
    [
     "Sorin",
     "Dusan"
    ]
   ],
   "title": "Parallel and hierarchical speech feature classification using frame and segment-based methods",
   "original": "i08_1329",
   "page_count": 4,
   "order": 428,
   "p1": "1329",
   "pn": "1332",
   "abstract": [
    "Phonemes in the English language can be represented using either parallel or hierarchical distinctive speech features. There have been a number of efforts to integrate multiple information sources but none of these efforts addressed the issue of combining multiple sets of articulatory/linguistic features with different organization topologies. In this study, we combine a frame-based parallel speech feature detection system and a segment-based hierarchical phoneme classification system with the goal of improving the overall phoneme classification performance. We describe a mathematical framework for a unified classification system in which frame-based parallel feature detection is incorporated into a segment-based hierarchical phoneme classification method. Experimental results show that the combined system provides improved classification performance for some phoneme classes.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-388"
  },
  "varadarajan08_interspeech": {
   "authors": [
    [
     "Balakrishnan",
     "Varadarajan"
    ],
    [
     "Sanjeev",
     "Khudanpur"
    ]
   ],
   "title": "Automatically learning speaker-independent acoustic subword units",
   "original": "i08_1333",
   "page_count": 4,
   "order": 429,
   "p1": "1333",
   "pn": "1336",
   "abstract": [
    "We investigate methods for unsupervised learning of sub-word acoustic units of a language directly from speech. Earlier we demonstrated that the states of a hidden Markov model \"grown\" using a novel modification of the maximum likelihood successive state splitting algorithm correspond very well with the phones of the language[1]. The correspondence between the Viterbi state sequence for unseen speech from the training speaker and the phone transcription of the speech is over 85%, and generalizes to a large extent (~ 61%) to speech from a different speaker. Furthermore, we are able to bridge more than half the gap between the speaker-dependent and cross-speaker correspondence of the automatically learned units to phones (~ 73% accuracy) by unsupervised adaptation via MLLR.\n",
    "",
    "",
    "1] B. Varadarajan, S. Khudanpur, and E. Dupoux, \"Unsupervised learning of acoustic sub-word units,\" in Proceedings of ACL-08: HLT, Short Papers. Columbus, Ohio: Association for Computational Linguistics, June 2008, pp. 165-168. [Online]. Available: http://www.aclweb.org/anthology/P/P08/P08-2042\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-389"
  },
  "abdulla08_interspeech": {
   "authors": [
    [
     "Waleed H.",
     "Abdulla"
    ],
    [
     "Yushi",
     "Zhang"
    ]
   ],
   "title": "Human-like ears versus two-microphone array, which works better for speaker identification?",
   "original": "i08_1337",
   "page_count": 4,
   "order": 430,
   "p1": "1337",
   "pn": "1340",
   "abstract": [
    "In this paper we try to answer with justifications the question posed in the title! We have used for this purpose a speech recording hardware; an acoustic artificial head, which accurately imitates human head, shoulder, and outer ears. It offers excellent level of realism and clarity in audio recording. Special speech corpuses are prepared under different noise conditions using the artificial head in normal office environment and anechoic chamber. Then the speech corpuses are used to evaluate the performance of a text-independent speaker identification system using two types of features. Identical corpuses using two-microphone array for recording are also prepared and used for assessing the performance of the same speaker identification system. The results show that using the artificial head for recording improves the identification rate under different noise conditions. This confirms that human ears and head structures have a role in improving human ability to recognize people from their voice.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-390"
  },
  "kobayashi08b_interspeech": {
   "authors": [
    [
     "Kenji",
     "Kobayashi"
    ],
    [
     "Mitsuhiro",
     "Somiya"
    ],
    [
     "Hiromitsu",
     "Nishizaki"
    ],
    [
     "Yoshihiro",
     "Sekiguchi"
    ]
   ],
   "title": "Is a speech recognizer useful for characteristic analysis of classroom lecture speech?",
   "original": "i08_1341",
   "page_count": 4,
   "order": 431,
   "p1": "1341",
   "pn": "1344",
   "abstract": [
    "This paper investigates whether a speech recognizer is useful for characteristic analysis of lecture speech or not. It is important to pay attention to how we speak in order to effectively convey the meaning of what we are trying to say to someone. We have examined how various acoustic features included in a lecture speech influence the understanding, and change the impression, of the speech, which is determined by performing tests involving trial subject. The research investigated the correlation acoustic features in speech and the impression rating of students who listened the classroom lecture speech. In this paper, especially, we try to use a speech recognizer which translated speech into transcriptions with acoustic likelihoods. Recognizer's outputs may have hidden potential for rating speech. As an opening the research, we clear the relationship between the recognition rate of the lecture speech and the impression of students (or acoustic features of the speech).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-391"
  },
  "golipour08_interspeech": {
   "authors": [
    [
     "Ladan",
     "Golipour"
    ],
    [
     "Douglas",
     "O'Shaughnessy"
    ]
   ],
   "title": "An intuitive class discriminability measure for feature selection in a speech recognition system",
   "original": "i08_1345",
   "page_count": 4,
   "order": 432,
   "p1": "1345",
   "pn": "1348",
   "abstract": [
    "In this paper we propose an intuitive method for feature selection and dimensionality reduction of a set of selected features. We analyze the feature set by applying an intuitive class discriminability measure. For this purpose, we use the proposed schemes for the \"editing\" of the training datasets. Editing algorithms basically smooth the decision boundaries and improve the performance of the classification with finite sample size. We propose that an intuitive measure for class separability or the overlap between classes can be the \"number of remaining points\" after editing the dataset using an algorithm with an idea similar to Wilson-editing algorithm over the total number of points. The algorithm that we aim to apply is a modification of the Wilson-Gabriel editing algorithm. We apply this method to compute the overlap between phonetic classes for two different feature sets, Mel Frequency Cepstral Coefficients (MFCCs), and Linear Prediction Coefficients (LPCs). These feature sets are commonly used in speech recognition systems. The proposed measure indicates a higher class discriminability for MFCC feature space comparing to LPC feature space.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-392"
  },
  "qiao08b_interspeech": {
   "authors": [
    [
     "Yu",
     "Qiao"
    ],
    [
     "Nobuaki",
     "Minematsu"
    ]
   ],
   "title": "f-divergence is a generalized invariant measure between distributions",
   "original": "i08_1349",
   "page_count": 4,
   "order": 433,
   "p1": "1349",
   "pn": "1352",
   "abstract": [
    "WedSe4.P3-10, Poster Finding measures (or features) invariant to inevitable variations caused by non-linguistical factors (transformations) is a fundamental yet important problem in speech recognition. Recently, Minematsu [1, 2] proved that Bhattacharyya distance (BD) between two distributions is invariant to invertible transforms on feature space, and develop an invariant structural representation of speech based on it. There is a question: which kind of measures can be invariant? In this paper, we prove that f -divergence yields a generalized family of invariant measures, and show that all the invariant measures have to be written in the forms of f -divergence. Many famous distances and divergences in information and statistics, such as Bhattacharyya distance (BD), KL-divergence, Hellinger distance, can be written into forms of f -divergence. As an application, we carried out experiments on recognizing the utterances of connected Japanese vowels. The experimental results show that BD and KL have the best performance among the measures compared.\n",
    "s N. Minematsu, \"Yet another acoustic representation of speech sounds,\" Proc. ICASSP, pp. 585-588, 2004. N. Minematsu, \"Mathematical Evidence of the Acoustic Universal Structure in Speech,\" Proc. ICASSP, pp. 889-892, 2005.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-393"
  },
  "giacobello08_interspeech": {
   "authors": [
    [
     "Daniele",
     "Giacobello"
    ],
    [
     "Mads Græsbøll",
     "Christensen"
    ],
    [
     "Joachim",
     "Dahl"
    ],
    [
     "Søren Holdt",
     "Jensen"
    ],
    [
     "Marc",
     "Moonen"
    ]
   ],
   "title": "Sparse linear predictors for speech processing",
   "original": "i08_1353",
   "page_count": 4,
   "order": 434,
   "p1": "1353",
   "pn": "1356",
   "abstract": [
    "This paper presents two new classes of linear prediction schemes. The first one is based on the concept of creating a sparse residual rather than a minimum variance one, which will allow a more efficient quantization; we will show that this works well in presence of voiced speech, where the excitation can be represented by an impulse train, and creates a sparser residual in the case of unvoiced speech. The second class aims at finding sparse prediction coefficients; interesting results can be seen applying it to the joint estimation of long-term and short-term predictors. The proposed estimators are all solutions to convex optimization problems, which can be solved efficiently and reliably using, e.g., interior-point methods.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-394"
  },
  "zhang08c_interspeech": {
   "authors": [
    [
     "J. X.",
     "Zhang"
    ],
    [
     "Mads Græsbøll",
     "Christensen"
    ],
    [
     "Joachim",
     "Dahl"
    ],
    [
     "Søren Holdt",
     "Jensen"
    ],
    [
     "Marc",
     "Moonen"
    ]
   ],
   "title": "Frequency-domain parameter estimations for binary masked signals",
   "original": "i08_1357",
   "page_count": 4,
   "order": 435,
   "p1": "1357",
   "pn": "1360",
   "abstract": [
    "We present an approach for the extraction of parameters of a damped complex exponential model from a spectrogram modified by a binary mask. The parameters are estimated by a frequency domain based methods using subspace techniques, where the core algorithm is F-ESPRIT. The sub-band defined by the binary mask provides a reduced number of DFT-samples for the parameter extractions, which results in a computational efficient scheme with high parameter estimation accuracy. The proposed synthesis system has synthesis performance comparable to the so-called LSEE-MSTFT. The estimated parameters can be used in many applications such as audio/speech coding, pitch estimation and pitch scale modification.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-395"
  },
  "saito08b_interspeech": {
   "authors": [
    [
     "Daisuke",
     "Saito"
    ],
    [
     "Nobuaki",
     "Minematsu"
    ],
    [
     "Keikichi",
     "Hirose"
    ]
   ],
   "title": "Decomposition of rotational distortion caused by VTL difference using eigenvalues of its transformation matrix",
   "original": "i08_1361",
   "page_count": 4,
   "order": 436,
   "p1": "1361",
   "pn": "1364",
   "abstract": [
    "In speech recognition studies, vocal tract length normalization (VTLN) techniques are widely used to cancel age- and genderdifference. In VTLN, the distortion is often modeled as a linear transform in a cepstrum space; .c= Ac. In our previous study, the geometrical properties of A were discussed and it was shown that the matrix can be approximated as rotation matrix. In this study, a new method of better approximating A is proposed. Using eigenvalues of A, its quasi-rotational distortion is factorized into multiple rotation operations and multiple magnification operations. Using this method, the intrinsic ambiguity of the rotation angle used in our previous study is resolved. Instead, multiple rotation angles are introduced to understand better what kind of geometrical distortions A induces to cepstrum vectors. Experiments show the validity of the new method and a new speech feature is also derived by the new method.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-396"
  },
  "maragakis08_interspeech": {
   "authors": [
    [
     "Michail G.",
     "Maragakis"
    ],
    [
     "Alexandros",
     "Potamianos"
    ]
   ],
   "title": "Region-based vocal tract length normalization for ASR",
   "original": "i08_1365",
   "page_count": 4,
   "order": 437,
   "p1": "1365",
   "pn": "1368",
   "abstract": [
    "In this paper, we propose a Region-based multi-parametric Vocal Tract Length Normalization (R-VTLN) algorithm for the problem of automatic speech recognition (ASR). The proposed algorithm extends the well-established mono-parametric utterance-based VTLN algorithm of Lee and Rose [1] by dividing the speech frames of a test utterance into regions and by warping independently the features corresponding to each region using a maximum likelihood criterion. We propose two algorithms for classifying frames into regions: an unsupervised clustering algorithm and an unsupervised algorithm assigning frames to regions based on phonetic-class labels obtained from a first recognition pass. We also investigate the ability of various mono-parametric and multi-parametric warping functions to reduce the spectral distance between two speakers, as a function of phone. R-VTLN is shown to significantly outperform mono-parametric VTLN in terms of word accuracy for the AURORA4 database.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-397"
  },
  "okamoto08_interspeech": {
   "authors": [
    [
     "Hideki",
     "Okamoto"
    ],
    [
     "Tomoko",
     "Matsui"
    ],
    [
     "Hiromichi",
     "Kawanami"
    ],
    [
     "Hiroshi",
     "Saruwatari"
    ],
    [
     "Kiyohiro",
     "Shikano"
    ]
   ],
   "title": "Speaker verification with non-audible murmur segments by combining global alignment kernel and penalized logistic regression machine",
   "original": "i08_1369",
   "page_count": 4,
   "order": 438,
   "p1": "1369",
   "pn": "1372",
   "abstract": [
    "We investigate a novel method for speaker verification with nonaudible murmur (NAM) segments. NAM is recorded using a special microphone placed on the neck and is hard for other people to hear. We have already reported a method based on a support vector machine (SVM) using NAM segments to use a keyword phrase effectively. To further exploit keyword-specific features, we introduce a global alignment (GA) kernel and penalized logistic regression machine (PLRM). In the experiments using NAM from 55 speakers, our method achieved an error reduction rate of roughly 60% compared with the SVM-based method using a polynomial kernel.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-398"
  },
  "lu08c_interspeech": {
   "authors": [
    [
     "Liang",
     "Lu"
    ],
    [
     "Yuan",
     "Dong"
    ],
    [
     "Xianyu",
     "Zhao"
    ],
    [
     "Jian",
     "Zhao"
    ],
    [
     "Chengyu",
     "Dong"
    ],
    [
     "Haila",
     "Wang"
    ]
   ],
   "title": "Analysis of subspace within-class covariance normalization for SVM-based speaker verification",
   "original": "i08_1373",
   "page_count": 4,
   "order": 439,
   "p1": "1373",
   "pn": "1376",
   "abstract": [
    "Nuisance attribute projection (NAP) and within-class covariance normalization (WCCN) are two effective techniques for intersession variability compensation in SVM based speaker verification systems. However, by normalizing or removing the nuisance subspace containing the session variability can not guarantee to enlarge the distance between speakers. In this paper, we investigated the probability of using linear discriminant analysis (LDA) for discriminative training. To cope with the small sample size problem which prevents us from using LDA directly, we adapted the subspace LDA approach, which first projects the whole feature space into a relatively low dimensional subspace by PCA, and then performs LDA in the subspace. By some modification, the subspace LDA can be degenerated into a kind of WCCN approach, which we called subspace WCCN. Experiments on NIST SRE tasks showed that, the subspace WCCN outperformed the conventional direct WCCN, especially in low dimensional feature space.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-399"
  },
  "zhao08b_interspeech": {
   "authors": [
    [
     "Xianyu",
     "Zhao"
    ],
    [
     "Yuan",
     "Dong"
    ],
    [
     "Jian",
     "Zhao"
    ],
    [
     "Liang",
     "Lu"
    ],
    [
     "Jiqing",
     "Liu"
    ],
    [
     "Haila",
     "Wang"
    ]
   ],
   "title": "Comparison of input and feature space nonlinear kernel nuisance attribute projections for speaker verification",
   "original": "i08_1377",
   "page_count": 4,
   "order": 440,
   "p1": "1377",
   "pn": "1380",
   "abstract": [
    "Nuisance attribute projection (NAP) was an effective method to reduce session variability in SVM-based speaker verification systems. As the expanded feature space of nonlinear kernels is usually high or infinite dimensional, it is difficult to find nuisance directions via conventional eigenvalue analysis and to do projection directly in the feature space. In this paper, two different approaches to nonlinear kernel NAP are investigated and compared. In one way, NAP projection is formulated in the expanded feature space and kernel PCA is employed to do kernel eigenvalue analysis. In the second approach, a gradient descent algorithm is proposed to find out projection over input variables. Experimental results on the 2006 NIST SRE corpus show that both kinds of NAP can reduce unwanted variability in nonlinear kernels to improve verification performance; and NAP performed in expanded feature space using kernel PCA obtains slightly better performance than NAP over input variables.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-400"
  },
  "longworth08_interspeech": {
   "authors": [
    [
     "C.",
     "Longworth"
    ],
    [
     "M. J. F.",
     "Gales"
    ]
   ],
   "title": "A generalised derivative kernel for speaker verification",
   "original": "i08_1381",
   "page_count": 4,
   "order": 441,
   "p1": "1381",
   "pn": "1384",
   "abstract": [
    "An important aspect of SVM-based speaker verification systems is the choice of dynamic kernel. For the GLDS kernel, a static kernel is used to map each observation into a higher order feature space. Features are then obtained by taking a simple average over all frames. Derivative kernels, such as the Fisher kernel, use a generative model as a principled way of extracting a fixed set of features from each utterance. However, the model and features are defined using the original observations. Here, a dynamic kernel is described that combines these two approaches. In general, it is not possible to explicitly train a model in the feature space associated with a static kernel. However, by using a suitable metric with approximate component posteriors, this form of dynamic kernel can be computed. This kernel generalises the GLDS and derivative kernel as special cases and is also closely related to parametric kernels such as the GMM-supervector kernel. Preliminary results using this kernel are presented on the 2002 NIST SRE dataset.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-401"
  },
  "ferrer08_interspeech": {
   "authors": [
    [
     "Luciana",
     "Ferrer"
    ]
   ],
   "title": "Modeling prior belief for speaker verification SVM systems",
   "original": "i08_1385",
   "page_count": 4,
   "order": 442,
   "p1": "1385",
   "pn": "1388",
   "abstract": [
    "Support vector machines (SVMs) can be interpreted as a maximum a posteriori (MAP) estimation of a model's parameters, for an appropriately chosen likelihood function. In the standard formulation for SVM classification and regression problems, the prior distribution on the weight vector is implicitly assumed to be a multidimensional Gaussian with zero mean and identity covariance matrix. In this paper we propose to relax the assumption that the covariance matrix is the identity matrix, allowing it to be a more general block diagonal matrix. In speaker verification, this covariance matrix can be estimated from held-out speakers. We show results on two speaker verification systems: a Maximum Likelihood Linear Regression (MLLR)-based system and a prosodic system. In both cases, the proposed prior model leads to more than 10% improvement in equal error rate (EER) with respect to results obtained using the standard prior assumptions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-402"
  },
  "charlet08_interspeech": {
   "authors": [
    [
     "Delphine",
     "Charlet"
    ],
    [
     "Xianyu",
     "Zhao"
    ],
    [
     "Yuan",
     "Dong"
    ]
   ],
   "title": "Convergence between SVM-based and distance-based paradigms for speaker recognition",
   "original": "i08_1389",
   "page_count": 4,
   "order": 443,
   "p1": "1389",
   "pn": "1392",
   "abstract": [
    "This paper investigates the convergence between SVM-based and distance-based classifiers in speaker recognition. It focuses on approaches in speaker recognition where a speech utterance is represented with a fixed dimension vector. We study various preprocessings to apply on the vectors before classification, and various choices of negative samples to train SVM. We prove that in one specific configuration, the SVM-based classifier and the distance-based classifier are strictly equivalent. Experiments on NIST2006 database, within the Anchor Models framework, show that this specific configuration gets very good performance.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-403"
  },
  "zhang08d_interspeech": {
   "authors": [
    [
     "Shi-Xiong",
     "Zhang"
    ],
    [
     "Man-Wai",
     "Mak"
    ]
   ],
   "title": "High-level speaker verification via articulatory-feature based sequence kernels and SVM",
   "original": "i08_1393",
   "page_count": 4,
   "order": 444,
   "p1": "1393",
   "pn": "1396",
   "abstract": [
    "Articulatory-feature based pronunciation models (AFCPMs) are capable of capturing the pronunciation variations among different speakers and are good for high-level speaker recognition. However, the likelihood-ratio scoring method of AFPCMs is based on a decision boundary created by training the target speaker model and universal background model (UBM) separately. Therefore, the method does not fully utilize the discriminative information available in the training data. To fully harness the discriminative information, this paper proposes training a support vector machine (SVM) for computing the verification scores. More precisely, the models of target speakers, individual background speakers, and claimants are converted to AF-supervectors, which form the inputs to an AF-based kernel of the SVM for computing verification scores. Results show that the proposed AF-kernel scoring is complementary to likelihood-ratio scoring, leading to better performance when the two scoring methods are combined. Further performance enhancement was also observed when the AF scores were combined with acoustic scores derived from a GMM-UBM system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-404"
  },
  "lee08b_interspeech": {
   "authors": [
    [
     "Kong-Aik",
     "Lee"
    ],
    [
     "Changhuai",
     "You"
    ],
    [
     "Haizhou",
     "Li"
    ],
    [
     "Tomi",
     "Kinnunen"
    ],
    [
     "Donglai",
     "Zhu"
    ]
   ],
   "title": "Characterizing speech utterances for speaker verification with sequence kernel SVM",
   "original": "i08_1397",
   "page_count": 4,
   "order": 445,
   "p1": "1397",
   "pn": "1400",
   "abstract": [
    "Support vector machine (SVM) equipped with sequence kernel has been proven to be a powerful technique for speaker verification. A number of sequence kernels have been recently proposed, each being motivated from different perspectives with diverse mathematical derivations. Analytical comparison of kernels becomes difficult. To facilitate such comparisons, we propose a generic structure showing how different levels of cues conveyed by speech utterances, ranging from low-level acoustic features to high-level speaker cues, are being characterized within a sequence kernel. We then identify the similarities and differences between the popular generalized linear discriminant sequence (GLDS) and GMM supervector kernels, as well as our own probabilistic sequence kernel (PSK). Furthermore, we enhance the PSK in terms of accuracy and computational complexity. The enhanced PSK gives competitive accuracy with the other two kernels. Fusing all the three kernels yields an EER of 4.83% on the 2006 NIST SRE core test.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-405"
  },
  "kenny08_interspeech": {
   "authors": [
    [
     "Patrick",
     "Kenny"
    ],
    [
     "Najim",
     "Dehak"
    ],
    [
     "Pierre",
     "Ouellet"
    ],
    [
     "Vishwa",
     "Gupta"
    ],
    [
     "Pierre",
     "Dumouchel"
    ]
   ],
   "title": "Development of the primary CRIM system for the NIST 2008 speaker recognition evaluation",
   "original": "i08_1401",
   "page_count": 4,
   "order": 446,
   "p1": "1401",
   "pn": "1404",
   "abstract": [
    "We describe how we modified the CRIM factor analysis speaker verification system to handle the new cross-channel conditions encountered in the 2008 NIST speaker recognition evaluation. Using the 2006 evaluation data for development, we obtained results on a broad spectrum of test conditions that are uniformly better than the best results that have been published in the literature.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-406"
  },
  "vogt08b_interspeech": {
   "authors": [
    [
     "Robbie",
     "Vogt"
    ],
    [
     "Sridha",
     "Sridharan"
    ],
    [
     "Michael",
     "Mason"
    ]
   ],
   "title": "Making confident speaker verification decisions with minimal speech",
   "original": "i08_1405",
   "page_count": 4,
   "order": 447,
   "p1": "1405",
   "pn": "1408",
   "abstract": [
    "Drastic reductions in the typical data requirements for producing confident decisions in an automatic speaker verification system are demonstrated through the application of a novel approach of score confidence interval estimation. The confidence estimation procedure is also extended to produce robust results with very limited and highly correlated frame scores. The early verification decision method evaluated on the 2005 NIST SRE protocol demonstrates that an average of 2.10 seconds of speech is sufficient to produce verification results approaching those achieved previously using an average of over 100 seconds of speech.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-407"
  },
  "luo08_interspeech": {
   "authors": [
    [
     "Jun",
     "Luo"
    ],
    [
     "Cheung-Chi",
     "Leung"
    ],
    [
     "Marc",
     "Ferràs"
    ],
    [
     "Claude",
     "Barras"
    ]
   ],
   "title": "Parallelized factor analysis and feature normalization for automatic speaker verification",
   "original": "i08_1409",
   "page_count": 4,
   "order": 448,
   "p1": "1409",
   "pn": "1412",
   "abstract": [
    "Factor analysis (FA) is one of the key advances presented in recent speaker verification evaluations. This technique is able to successfully remove session variability effects and it is currently used in many state-of-the-art automatic speaker verification systems. This paper addresses several practical issues in using an FA model in order to speed up model training and to achieve good performance. A parallelized training algorithm as well as maximum-likelihood estimation are proposed for fast training. The front-end feature normalization techniques are also investigated in the context of FA model. We demonstrate that factor analysis is very robust, and can be successfully applied to various kinds of feature normalization. Moreover, the proposed parallelized MLE implementation speeds up the training procedure from several days to several hours without sacrificing the performance.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-408"
  },
  "garciaromero08_interspeech": {
   "authors": [
    [
     "Daniel",
     "Garcia-Romero"
    ],
    [
     "Carol Y.",
     "Espy-Wilson"
    ]
   ],
   "title": "Intersession variability in speaker recognition: a behind the scene analysis",
   "original": "i08_1413",
   "page_count": 4,
   "order": 449,
   "p1": "1413",
   "pn": "1416",
   "abstract": [
    "The representation of a speaker's identity by means of Gaussian supervectors (GSV) is at the heart of most of the state-of-the-art recognition systems. In this paper we present a novel procedure for the visualization of GSV by which qualitative insight about the information being captured can be obtained. Based on this visualization approach, the Switchboard-I database (SWB-I) is used to study the relationship between a data-driven partition of the acoustic space and a knowledge based partition (i.e., broad phonetic classes). Moreover, the structure of an intersession variability subspace (IVS), computed from the SWB-I database, is analyzed by displaying the projection of a speaker's GSV into the set of eigenvectors with highest eigenvalues. This analysis reveals a strong presence of linguistic information in the IVS components with highest energy. Finally, after projecting away the information contained in the IVS from the speaker's GSV, a visualization of the resulting GSV provides information about the characteristic patterns of spectral allocation of energy of a speaker.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-409"
  },
  "ito08b_interspeech": {
   "authors": [
    [
     "Tatsuya",
     "Ito"
    ],
    [
     "Kei",
     "Hashimoto"
    ],
    [
     "Yoshihiko",
     "Nankaku"
    ],
    [
     "Akinobu",
     "Lee"
    ],
    [
     "Keiichi",
     "Tokuda"
    ]
   ],
   "title": "Speaker recognition based on variational Bayesian method",
   "original": "i08_1417",
   "page_count": 4,
   "order": 450,
   "p1": "1417",
   "pn": "1420",
   "abstract": [
    "This paper presents a speaker identification system based on Gaussian Mixture Models (GMM) using the variational Bayesian method. Maximum Likelihood (ML) and Maximum A Posterior (MAP) are well-known methods for estimating GMM parameters. However, the overtraining problem occurs with insufficient data due to a point estimate of model parameters. The Bayesian approach estimates a posterior distribution of model parameters and achieves a more robust prediction than ML and MAP approach. To solve complicated integral calculations in the Bayesian approach, the variational Bayesian method has been proposed and applied to many classification problems using latent variable models. However, the performance of the Bayesian approach has not been extensively investigated in large speaker identification tasks. The experimental results shows that the VB method improves the overtraining problem than the conventional ML and MAP methods.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-410"
  },
  "matrouf08_interspeech": {
   "authors": [
    [
     "Driss",
     "Matrouf"
    ],
    [
     "Jean-François",
     "Bonastre"
    ],
    [
     "Salah Eddine",
     "Mezaache"
    ]
   ],
   "title": "Factor analysis multi-session training constraint in session compensation for speaker verification",
   "original": "i08_1421",
   "page_count": 4,
   "order": 451,
   "p1": "1421",
   "pn": "1424",
   "abstract": [
    "For a few years now, the problem of session variability in text-independent automatic speaker verification is being tackled actively. A new paradigm based on a Latent Factor Analysis (LFA) model has been applied successfully for this task. However, using this approach, a large training corpus with several sessions per speaker is required. This constraint is hard to satisfy in many real applications. In this paper, we try to analyze if the LFA paradigm still holds even when the constraint of multiple sessions per speaker isn't satisfied. We propose to study two approaches. The first one consists in using the basic paradigm of the LFA model and the second one is founded on a new interpretation of the interaction between the session and the speaker. The experiments were carried out with NIST SRE 2005 and 2006 protocols. We show that even with only one session per speaker the gain obtained by LFA session compensation (with the two strategies) is still very important.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-411"
  },
  "liu08d_interspeech": {
   "authors": [
    [
     "Ying",
     "Liu"
    ],
    [
     "Martin J.",
     "Russell"
    ],
    [
     "Michael J.",
     "Carey"
    ]
   ],
   "title": "The role of 'delta' features in speaker verification",
   "original": "i08_1425",
   "page_count": 4,
   "order": 452,
   "p1": "1425",
   "pn": "1428",
   "abstract": [
    "Our previous experiments in Text-Dependent and -Independent Speaker Verification (TD-SV and TI-SV) using trajectory-based models, showed that non-stationary segments benefit TD-SV but not TI-SV, because in TI-SV maximum likelihood (ML) training results mainly in stationary segments. This result questions the role of non-stationary, 'delta' parameters in conventional GMM-based TI-SV. In this paper we develop and study a number of GMM-based TI-SV systems for Switchboard which use combinations of static and dynamic parameters. We show that in our segmental GMM and the AFRL GMM system, the trajectory slopes and deltas focus the verification process onto the stationary regions. In our GMM systems, however, the deltas are modelling some speech dynamics. The different functions of deltas may be due to different system settings and front-end processing (e.g. RASTA, speech noise detector). This indicates that the role of delta parameters in GMM-based speaker verification systems is more complex than simply \"modelling dynamics\". Our results also show that the superior performance obtained with front-end parameterizations which combine static and delta parameters only emerges after RASTA filtering; without RASTA filtering a 'delta-only' front-end performs best.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-412"
  },
  "lamel08_interspeech": {
   "authors": [
    [
     "Lori",
     "Lamel"
    ],
    [
     "Abdel.",
     "Messaoudi"
    ],
    [
     "Jean-Luc",
     "Gauvain"
    ]
   ],
   "title": "Investigating morphological decomposition for transcription of Arabic broadcast news and broadcast conversation data",
   "original": "i08_1429",
   "page_count": 4,
   "order": 453,
   "p1": "1429",
   "pn": "1432",
   "abstract": [
    "One of the challenges of Arabic speech recognition is to deal with the huge lexical variety. Morphological decomposition has been proposed to address this problem by increasing lexical coverage, thereby reducing errors that are due to words that are unknown to the system. In our previous attempts to develop an Arabic speech-to-text (STT) transcription system with morphological decomposition, an increase in word error rate of about 2% absolute was observed relative to a comparable word based system. Based on an error analysis and a comparison of our approach with that of other sites, two modifications were made. The first modification was to not decompose the most frequent words; and the second to not decompose the prefix 'Al' for words starting with a solar consonant since due to assimilation with the following consonant, deletion of the prefix was one of the most frequent errors. Comparable recognition performance was achieved using word-based and morphologically decomposed language models, and since the errors made by the systems are different, combining the two gave a performance gain.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-413"
  },
  "fousek08_interspeech": {
   "authors": [
    [
     "Petr",
     "Fousek"
    ],
    [
     "Lori",
     "Lamel"
    ],
    [
     "Jean-Luc",
     "Gauvain"
    ]
   ],
   "title": "Transcribing broadcast data using MLP features",
   "original": "i08_1433",
   "page_count": 4,
   "order": 454,
   "p1": "1433",
   "pn": "1436",
   "abstract": [
    "This paper describes incorporating discriminative features from a multi layer perceptron (MLP) into a state-of-the-art Arabic broadcast data transcription system based on cepstral features. The MLP features are based on a recently proposed Bottle-Neck architecture with long-term warped LP-TRAP speech representation at the input. It is shown that the previously reported improvements on a development Arabic transcription system carry through to a full system at a state-of-the-art level. SAT, CMLLR and MLLR adaptation techniques are shown to be useful for both MLP and combined features, though to a lesser degree than for PLPs. Without adaptation, MLP features obtain superior performance to cepstral features in all test conditions, and with adaptation both feature sets give comparable results. Combining the features, either by feature concatenation or system hypotheses, gives significant gains. Gains from MMI model training seem to be additive to the gain coming from discriminative MLP features.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-414"
  },
  "vergyri08_interspeech": {
   "authors": [
    [
     "D.",
     "Vergyri"
    ],
    [
     "A.",
     "Mandal"
    ],
    [
     "Wen",
     "Wang"
    ],
    [
     "Andreas",
     "Stolcke"
    ],
    [
     "Jing",
     "Zheng"
    ],
    [
     "Martin",
     "Graciarena"
    ],
    [
     "D.",
     "Rybach"
    ],
    [
     "Christian",
     "Gollan"
    ],
    [
     "Ralf",
     "Schlüter"
    ],
    [
     "Katrin",
     "Kirchhoff"
    ],
    [
     "A.",
     "Faria"
    ],
    [
     "Nelson",
     "Morgan"
    ]
   ],
   "title": "Development of the SRI/nightingale Arabic ASR system",
   "original": "i08_1437",
   "page_count": 4,
   "order": 455,
   "p1": "1437",
   "pn": "1440",
   "abstract": [
    "We describe the large vocabulary automatic speech recognition system developed for Modern Standard Arabic by the SRI/Nightingale team, and used for the 2007 GALE evaluation as part of the speech translation system. We show how system performance is affected by different development choices, ranging from text processing and lexicon to decoding system architecture design. Word error rate results are reported on broadcast news and conversational data from the GALE development and evaluation test sets.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-415"
  },
  "gollan08_interspeech": {
   "authors": [
    [
     "Christian",
     "Gollan"
    ],
    [
     "Hermann",
     "Ney"
    ]
   ],
   "title": "Towards automatic learning in LVCSR: rapid development of a Persian broadcast transcription system",
   "original": "i08_1441",
   "page_count": 4,
   "order": 456,
   "p1": "1441",
   "pn": "1444",
   "abstract": [
    "We present a new method for automatic learning and refining of pronunciations for large vocabulary continuous speech recognition which starts from a small amount of transcribed data and uses automatic transcription techniques for additional untranscribed speech data.\n",
    "The recognition performance of speech recognition systems usually depends on the available amount and quality of the transcribed training data. The creation of such data is a costly and tedious process and the approach presented here allows training with small amounts of annotated data.\n",
    "The model parameters of a statistical joint-multigram grapheme-tophoneme converter are iteratively estimated using small amounts of manual and relatively larger amounts of automatic transcriptions and thus the system improves itself in an unsupervised manner.\n",
    "Using the new approach, we create a Persian broadcast transcrip- tion system from less than five hours of transcribed speech and 52 hours of untranscribed audio data.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-416"
  },
  "hsiao08_interspeech": {
   "authors": [
    [
     "Roger",
     "Hsiao"
    ],
    [
     "Mark",
     "Fuhs"
    ],
    [
     "Yik-Cheung",
     "Tam"
    ],
    [
     "Qin",
     "Jin"
    ],
    [
     "Tanja",
     "Schultz"
    ]
   ],
   "title": "The CMU-interACT 2008 Mandarin transcription system",
   "original": "i08_1445",
   "page_count": 4,
   "order": 457,
   "p1": "1445",
   "pn": "1448",
   "abstract": [
    "We present our Mandarin BN/BC transcription system recently developed for the GALE07 evaluation. The system employs a 3-pass decoding strategy trained with over 1300 hours of quickly transcribed audio. We successfully apply discriminative training, dynamic unsupervised language model adaptation, and system combination techniques in our system. We furthermore achieve improvements by combining an Initial-Final system with a genre dependent phone system. On the GALE07 phase 2 retest evaluation, our system achieves a character error rate(CER) of 13.3% on dev07 test set and 13.5% on eval07 unsequestered test set. Our system also allows combination with other sites and in this paper, we investigate different system combination strategies which significantly improve the final recognition performance.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-417"
  },
  "deoras08_interspeech": {
   "authors": [
    [
     "Anoop",
     "Deoras"
    ],
    [
     "Jürgen",
     "Fritsch"
    ]
   ],
   "title": "Decoding-time prediction of non-verbalized punctuation",
   "original": "i08_1449",
   "page_count": 4,
   "order": 458,
   "p1": "1449",
   "pn": "1452",
   "abstract": [
    "This paper presents novel methods that integrate lexical prediction of non-verbalized punctuations with Viterbi decoding for Large Vocabulary Conversational Speech Recognition (LVCSR) in a single pass. We describe two different approaches - one based on a modified finite state machine representation of language models and one based on an extension of an LVCSR decoder. We discuss advantages over traditional punctuation prediction approaches based on post-processing of recognition hypotheses, including experimental evaluation of the proposed approach using a state-of-the-art LVCSR decoder. Experiments were performed on a medical documentation corpus and results demonstrate that the proposed methods yield improved punctuation prediction accuracy while at the same time reducing system complexity and memory requirements.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-418"
  },
  "helander08_interspeech": {
   "authors": [
    [
     "Elina",
     "Helander"
    ],
    [
     "Jan",
     "Schwarz"
    ],
    [
     "Jani",
     "Nurminen"
    ],
    [
     "Hanna",
     "Silen"
    ],
    [
     "Moncef",
     "Gabbouj"
    ]
   ],
   "title": "On the impact of alignment on voice conversion performance",
   "original": "i08_1453",
   "page_count": 4,
   "order": 459,
   "p1": "1453",
   "pn": "1456",
   "abstract": [
    "Most of the current voice conversion systems model the joint density of source and target features using a Gaussian mixture model. An inherent property of this approach is that the source and target features have to be properly aligned for the training. It is intuitively clear that the accuracy of the alignment has some effect on the conversion quality but this issue has not been thoroughly studied in the literature. Examples of alignment techniques include the usage of a speech recognizer with forced alignment or dynamic time warping (DTW). In this paper, we study the effect of alignment on voice conversion quality through extensive experiments and discuss issues that should be considered. The main outcome of the study is that alignment clearly matters but with simple voice activity detection, DTW and some constraints we can achieve the same quality as with hand-marked labels.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-419"
  },
  "pozo08_interspeech": {
   "authors": [
    [
     "Arantza del",
     "Pozo"
    ],
    [
     "Steve",
     "Young"
    ]
   ],
   "title": "The linear transformation of LF glottal waveforms for voice conversion",
   "original": "i08_1457",
   "page_count": 4,
   "order": 460,
   "p1": "1457",
   "pn": "1460",
   "abstract": [
    "Most Voice Conversion (VC) systems exploit source-filter decomposition based on linear prediction (LP) to transform spectral envelopes, incurring as a result various issues related to the oversimplification of the LP voice source model. Whilst residual prediction methods can mitigate this problem, they cannot be used to modify voice source quality. In this paper, a system which employs linear transformations to convert both the spectral envelope and the LF glottal waveform is presented. Its performance is shown to be comparable to that of a state-of-the-art VC implementation in terms of speaker identity conversion but its output has better quality. In addition, it is also capable of transforming the quality of the voice source.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-420"
  },
  "tani08_interspeech": {
   "authors": [
    [
     "Daisuke",
     "Tani"
    ],
    [
     "Tomoki",
     "Toda"
    ],
    [
     "Yamato",
     "Ohtani"
    ],
    [
     "Hiroshi",
     "Saruwatari"
    ],
    [
     "Kiyohiro",
     "Shikano"
    ]
   ],
   "title": "Maximum a posteriori adaptation for many-to-one eigenvoice conversion",
   "original": "i08_1461",
   "page_count": 4,
   "order": 461,
   "p1": "1461",
   "pn": "1463",
   "abstract": [
    "Many-to-one eigenvoice conversion (EVC) allows the conversion from an arbitrary speaker's voice into the pre-determined target speaker's voice. In this method, a canonical eigenvoice Gaussian mixture model is effectively adapted to any source speaker using only a few utterances as the adaptation data. In this paper, we propose a many-to-one EVC based on maximum a posteriori (MAP) adaptation for further improving the robustness of the adaptation process to the amount of adaptation data. Results of objective and subjective evaluations demonstrate that the proposed method is the most effective among the other conventional many-to-one VC methods when using any amount of adaptation data (e.g., from 300 ms to 16 utterances).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-421"
  },
  "tran08_interspeech": {
   "authors": [
    [
     "Viet-Anh",
     "Tran"
    ],
    [
     "Gérard",
     "Bailly"
    ],
    [
     "Hélène",
     "Loevenbruck"
    ],
    [
     "Christian",
     "Jutten"
    ]
   ],
   "title": "Improvement to a NAM captured whisper-to-speech system",
   "original": "i08_1465",
   "page_count": 4,
   "order": 462,
   "p1": "1465",
   "pn": "1468",
   "abstract": [
    "In this paper, new techniques to improve whisper-to-speech conversion are investigated, in the framework of silent speech telephone communication. A preliminary conversion method from Non-Audible Murmur (NAM) to modal speech, based on statistical mapping trained using aligned corpora has been proposed. Although it is a very promising technique, its performance is still insufficient due to the difficulties in estimating F0 from unvoiced speech. In this paper, two distinct modifications are proposed, in order to improve the naturalness of the synthesized speech. In the first modification, LDA (Linear Discriminant Analysis) is used instead of PCA (Principal Component Analysis) to reduce the dimensionality of the input spectral vectors. In addition, the influence of long-term variation of spectral information on pitch estimation is examined. The second modification is an attempt to integrate visual information as a complementary input to improve spectral estimation, F0 estimation and voicing decision.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-422"
  },
  "tran08b_interspeech": {
   "authors": [
    [
     "Huy Dat",
     "Tran"
    ],
    [
     "Haizhou",
     "Li"
    ]
   ],
   "title": "Speaker identification in noise mismatch conditions based on jump function Kolmogorov analysis in wavelet domain",
   "original": "i08_1469",
   "page_count": 4,
   "order": 463,
   "p1": "1469",
   "pn": "1472",
   "abstract": [
    "We present a novel method for speaker identification in noise mismatch conditions. The proposed method is based on Jump Function Kolmogorov (JFK), - a new stochastic instrument ., which is (a) additive so sum of signal and noise yields the sum of their JFKs; (b) sparse so signal and noise have better separable supports in JFK's representations. The separability of signal's and noise's representations is the main advantage of JFK to make this instrument more robust in the classification then the conventional probability density function (PDF). In the approach, we develop a speaker identification system based on JFK analysis in the wavelet domain, i.e. the JFKs are estimated in each subband to match the nearest from trained templates. The experimental results show that the proposed method is comparable to the conventional method under clean condition but significantly outperformed them under noise mismatch conditions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-423"
  },
  "scharenborg08_interspeech": {
   "authors": [
    [
     "Odette",
     "Scharenborg"
    ]
   ],
   "title": "Modelling fine-phonetic detail in a computational model of word recognition",
   "original": "i08_1473",
   "page_count": 4,
   "order": 464,
   "p1": "1473",
   "pn": "1476",
   "abstract": [
    "There is now considerable evidence that fine-grained acousticphonetic detail in the speech signal helps listeners to segment a speech signal into syllables and words. In this paper, we compare two computational models of word recognition on their ability to capture and use this fine-phonetic detail during speech recognition. One model, SpeM, is phoneme-based, whereas the other, newly developed Fine-Tracker, is based on articulatory features. Simulations dealt with modelling the ability of listeners to distinguish short words (e.g., 'ham') from the longer words in which they are embedded (e.g., 'hamster'). The simulations with Fine-Tracker showed that it was, like human listeners, able to distinguish between short words from the longer words in which they are embedded. This suggests that it is possible to extract this fine-phonetic detail from the speech signal and use it during word recognition.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-424"
  },
  "strik08_interspeech": {
   "authors": [
    [
     "Helmer",
     "Strik"
    ],
    [
     "Joost van",
     "Doremalen"
    ],
    [
     "Catia",
     "Cucchiarini"
    ]
   ],
   "title": "Pronunciation reduction: how it relates to speech style, gender, and age",
   "original": "i08_1477",
   "page_count": 4,
   "order": 465,
   "p1": "1477",
   "pn": "1480",
   "abstract": [
    "Many studies have shown that speech style affects pronunciation reduction, mixed results have been obtained for gender, and few results have been published regarding the relationship between age and reduction. In the present paper we investigate how pronunciation reduction is related to speech style, gender and age. Significant effects were found for speech style and age, while the effect of gender was not significant.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-425"
  },
  "yegnanarayana08_interspeech": {
   "authors": [
    [
     "B.",
     "Yegnanarayana"
    ],
    [
     "S.",
     "Rajendran"
    ],
    [
     "Hussien Seid",
     "Worku"
    ],
    [
     "N.",
     "Dhananjaya"
    ]
   ],
   "title": "Analysis of glottal stops in speech signals",
   "original": "i08_1481",
   "page_count": 4,
   "order": 466,
   "p1": "1481",
   "pn": "1484",
   "abstract": [
    "During production of glottal stops the glottal vibration has unequal cycles and is caused by laryngealization. While one can perceive the features of laryngealization in the speech, it is difficult to analyse the signal to detect these source features from the standard spectrum-based analysis methods. In this paper we propose methods to extract the voice source vibration characteristics, and show that in the region of glottal stop, the pitch periods will be irregular, and the crosscorrelation coefficient of the signal in successive pitch periods will be low. This analysis enable us to locate the regions of glottal stops in continuous speech, and also help us to study the characteristics of creaky voice.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-426"
  },
  "neiberg08_interspeech": {
   "authors": [
    [
     "Daniel",
     "Neiberg"
    ],
    [
     "G.",
     "Ananthakrishnan"
    ],
    [
     "Olov",
     "Engwall"
    ]
   ],
   "title": "The acoustic to articulation mapping: non-linear or non-unique?",
   "original": "i08_1485",
   "page_count": 4,
   "order": 467,
   "p1": "1485",
   "pn": "1488",
   "abstract": [
    "This paper studies the hypothesis that the acoustic-to-articulatory mapping is non-unique, statistically. The distributions of the acoustic and articulatory spaces are obtained by fitting the data into a Gaussian Mixture Model. The kurtosis is used to measure the non-Gaussianity of the distributions and the Bhattacharya distance is used to find the difference between distributions of the acoustic vectors producing non-unique articulator configurations. It is found that stop consonants and alveolar fricatives are generally not only non-linear but also non-unique, while dental fricatives are found to be highly non-linear but fairly unique. Two more investigations are also discussed: the first is on how well the best possible piecewise linear regression is likely to perform, the second is on whether the dynamic constraints improve the ability to predict different articulatory regions corresponding to the same region in the acoustic space.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-427"
  },
  "zhuang08_interspeech": {
   "authors": [
    [
     "Xiaodan",
     "Zhuang"
    ],
    [
     "Hosung",
     "Nam"
    ],
    [
     "Mark",
     "Hasegawa-Johnson"
    ],
    [
     "Louis M.",
     "Goldstein"
    ],
    [
     "Elliot",
     "Saltzman"
    ]
   ],
   "title": "The entropy of the articulatory phonological code: recognizing gestures from tract variables",
   "original": "i08_1489",
   "page_count": 4,
   "order": 468,
   "p1": "1489",
   "pn": "1492",
   "abstract": [
    "We propose an instantaneous \"gestural pattern vector\" to encode the instantaneous pattern of gesture activations across tract variables in the gestural score. The design of these gestural pattern vectors is the first step towards an automatic speech recognizer motivated by articulatory phonology, which is expected to be more invariant to speech coarticulation and reduction than conventional speech recognizers built with the sequence-of-phones assumption. We use a tandem model to recover the instantaneous gestural pattern vectors from tract variable time functions in local time windows, and achieve classification accuracy up to 84.5% for synthesized data from one speaker. Recognizing all gestural pattern vectors is equivalent to recognizing the ensemble of gestures. This result suggests that the proposed gestural pattern vector might be a viable unit in statistical models for speech recognition.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-428"
  },
  "ramos08_interspeech": {
   "authors": [
    [
     "Daniel",
     "Ramos"
    ],
    [
     "Joaquin",
     "Gonzalez-Rodriguez"
    ],
    [
     "Javier",
     "Gonzalez-Dominguez"
    ],
    [
     "Jose Juan",
     "Lucena-Molina"
    ]
   ],
   "title": "Addressing database mismatch in forensic speaker recognition with Ahumada III: a public real-casework database in Spanish",
   "original": "i08_1493",
   "page_count": 4,
   "order": 469,
   "p1": "1493",
   "pn": "1496",
   "abstract": [
    "This paper presents and describes Ahumada III, a speech database in Spanish collected from real forensic cases. In its current release, the database presents 61 male speakers recorded using the systems and procedures followed by Spanish Guardia Civil police force. The paper also explores the usefulness of such a corpus for facing the important problem of database mismatch in speaker recognition, understood as the difference between the database used for tuning a speaker recognition system and the data which the system will handle in operational conditions. This problem is typical in forensics, where variability in speech conditions may be extreme and difficult to model. Therefore, this work also presents a study evaluating the impact of such problem, for which a corpus quoted as NIST4M (NIST MultiMic MisMatch) has been constructed from NIST SRE 2006 data. NIST4M presents microphone data both in the enrolled models and in the test segments, allowing the generation of trials in a variety of strongly mismatching conditions. Database mismatch is simulated by eliminating some microphone channels of interest from the background data, and computing scores with speech from such microphones in unknown testing conditions as usually happens in forensic speaker recognition. Finally, we show how the incorporation of Ahumada III as background data is useful to face database mismatch in real-world forensic conditions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-429"
  },
  "thiruvaran08_interspeech": {
   "authors": [
    [
     "Tharmarajah",
     "Thiruvaran"
    ],
    [
     "Eliathamby",
     "Ambikairajah"
    ],
    [
     "Julien",
     "Epps"
    ]
   ],
   "title": "FM features for automatic forensic speaker recognition",
   "original": "i08_1497",
   "page_count": 4,
   "order": 470,
   "p1": "1497",
   "pn": "1500",
   "abstract": [
    "Frequency modulation (FM) information from the speech signal is herein proposed to complement the conventional amplitude based features for automatic forensic speaker recognition systems. In addition to presenting the AM-FM model of speech used to generate the proposed frequency modulation features, the significance of frequency modulation for speaker recognition is discussed. Evaluation results from an automatic forensic speaker recognition system combining FM and MFCC features are shown to out-perform those of a system employing MFCC features alone, in terms of all typical metrics, such as detection error trade-off curves, Tippett curves and applied probability of error curves.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-430"
  },
  "morrison08_interspeech": {
   "authors": [
    [
     "Geoffrey Stewart",
     "Morrison"
    ],
    [
     "Yuko",
     "Kinoshita"
    ]
   ],
   "title": "Automatic-type calibration of traditionally derived likelihood ratios: forensic analysis of australian English /o/ formant trajectories",
   "original": "i08_1501",
   "page_count": 4,
   "order": 471,
   "p1": "1501",
   "pn": "1504",
   "abstract": [
    "A traditional-style phonetic-acoustic forensic-speaker-recognition analysis was conducted on Australian English /o/ recordings. Different parametric curves were fitted to the formant trajectories of the vowel tokens, and cross-validated likelihood ratios were calculated using a single-stage generative multivariate kernel density formula. The outputs of different systems were compared using Cllr , a metric developed for automatic speaker recognition, and the cross-validated likelihood ratios were calibrated using a procedure developed for automatic speaker recognition. Calibration ameliorated some likelihood-ratio results which had offered strong support for a contrary-to-fact hypothesis.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-431"
  },
  "becker08_interspeech": {
   "authors": [
    [
     "Timo",
     "Becker"
    ],
    [
     "Michael",
     "Jessen"
    ],
    [
     "Catalin",
     "Grigoras"
    ]
   ],
   "title": "Forensic speaker verification using formant features and Gaussian mixture models",
   "original": "i08_1505",
   "page_count": 4,
   "order": 472,
   "p1": "1505",
   "pn": "1508",
   "abstract": [
    "A new method for speaker verification based on formant features is presented. A UBM-GMM verification system is applied to semi-automatically extracted formant features. Speaker-specific vocal tract configurations, including the speakers' variability, are incorporated in the speaker models. Speaker comparisons are expressed as likelihood ratios (the ratio of similarity to typicality). F1, F2 and F3 values all enable speakers to be distinguished with a low error rate. The corresponding bandwidths further lower the error rate.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-432"
  },
  "shriberg08b_interspeech": {
   "authors": [
    [
     "Elizabeth",
     "Shriberg"
    ],
    [
     "Andreas",
     "Stolcke"
    ]
   ],
   "title": "The case for automatic higher-level features in forensic speaker recognition",
   "original": "i08_1509",
   "page_count": 4,
   "order": 473,
   "p1": "1509",
   "pn": "1512",
   "abstract": [
    "Approaches from standard automatic speaker recognition, which rely on cepstral features, suffer the problem of lack of interpretability for forensic applications. But the growing practice of using \"higher-level\" features in automatic systems offers promise in this regard. We provide an overview of automatic higher-level systems and discuss potential advantages, as well as issues, for their use in the forensic context.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-433"
  },
  "lee08c_interspeech": {
   "authors": [
    [
     "Kye-Hwan",
     "Lee"
    ],
    [
     "Sang-Ick",
     "Kang"
    ],
    [
     "Ji-Hyun",
     "Song"
    ],
    [
     "Joon-Hyuk",
     "Chang"
    ]
   ],
   "title": "Group delay function for improved gender identification",
   "original": "i08_1513",
   "page_count": 4,
   "order": 474,
   "p1": "1513",
   "pn": "1516",
   "abstract": [
    "One of the key issues in practical speech recognition is to achieve robust gender identification. Most conventional gender identification approaches use relevant features derived from the magnitude spectrum. In this paper, we propose a novel gender identification method using a group delay function (GDF). Based on the statistical analysis of the GDF, it is found that the GDF is an effective feature for gender identification. The experimental results demonstrate that the proposed method gives significant improvement compared to conventional methods.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-434"
  },
  "razik08_interspeech": {
   "authors": [
    [
     "Joseph",
     "Razik"
    ],
    [
     "Odile",
     "Mella"
    ],
    [
     "Dominique",
     "Fohr"
    ],
    [
     "Jean-Paul",
     "Haton"
    ]
   ],
   "title": "Frame-synchronous and local confidence measures for on-the-fly automatic speech recognition",
   "original": "i08_1517",
   "page_count": 4,
   "order": 475,
   "p1": "1517",
   "pn": "1520",
   "abstract": [
    "This paper presents several new confidence measures with the major advantage that they can be evaluated as soon as possible without having to wait for the recognition process to be completed. We have defined two kinds of confidence measures. The first one can be computed synchronously with the frame processed by the engine and the second one with a slight delay.\n",
    "Such measures are useful for driving the recognition process by modifying the likelihood score or for validating recognised words in on-the-fly applications such as keyword spotting task and on-line automatic speech transcription for deaf people.\n",
    "The EER evaluation on a French broadcast news corpus shows a performance close to the batch version of these measures (23.0% against 22.0% of EER) with only 0.84s of data before and after the word to be analysed.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-435"
  },
  "thomas08_interspeech": {
   "authors": [
    [
     "Samuel",
     "Thomas"
    ],
    [
     "Sriram",
     "Ganapathy"
    ],
    [
     "Hynek",
     "Hermansky"
    ]
   ],
   "title": "Hilbert envelope based spectro-temporal features for phoneme recognition in telephone speech",
   "original": "i08_1521",
   "page_count": 4,
   "order": 476,
   "p1": "1521",
   "pn": "1524",
   "abstract": [
    "In this paper, we present a spectro-temporal feature extraction technique using sub-band Hilbert envelopes of relatively long segments of speech signal. Hilbert envelopes of the subbands are estimated using Frequency Domain Linear Prediction (FDLP). Spectral features are derived by integrating the subband Hilbert envelopes in short-term frames and the temporal features are formed by converting the FDLP envelopes into modulation frequency components. These are then combined at the phoneme posterior level and are used as the input features for a phoneme recognition system. In order to improve the robustness of the proposed features to telephone speech, the sub-band temporal envelopes are gain normalized prior to feature extraction. Phoneme recognition experiments on telephone speech in the HTIMIT database show significant performance improvements for the proposed features when compared to other robust feature techniques (average relative reduction of 11% in phoneme error rate).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-436"
  },
  "sangwan08_interspeech": {
   "authors": [
    [
     "Abhijeet",
     "Sangwan"
    ],
    [
     "Ayako",
     "Ikeno"
    ],
    [
     "John H. L.",
     "Hansen"
    ]
   ],
   "title": "Evidence of coarticulation in a phonological feature detection system",
   "original": "i08_1525",
   "page_count": 4,
   "order": 477,
   "p1": "1525",
   "pn": "1528",
   "abstract": [
    "In this study, we investigate the capability of phonological features (PFs) in capturing the fine variational structure in speech which arise due to natural phenomenon such as coarticulation. The PF theory provides a framework in which a far more richer description of speech is possible when compared to traditional phonetic representations. However, current approaches toward training PF detectors do not explicitly expose the statistical system to patterns of coarticulation. The analysis presented here shows that despite this handicap, our PF system still learns to capture these variants in speech. In fact, it is noted that the use of phone-based transcriptions to judge the performance of PF systems erroneously labels such variants as errors. Our result show that a large proportion of speech frames that are deemed errors by phone-transcriptions are actually coarticulated as is evidenced by their phonetic context. These findings offer important knowledge in analyzing and improving the utility of PFs in ASR (automatic speech recognition) for spontaneous conversational speech.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-437"
  },
  "huda08_interspeech": {
   "authors": [
    [
     "Mohammad Nurul",
     "Huda"
    ],
    [
     "Kouichi",
     "Katsurada"
    ],
    [
     "Tsuneo",
     "Nitta"
    ]
   ],
   "title": "Phoneme recognition based on hybrid neural networks with inhibition/enhancement of distinctive phonetic feature (DPF) trajectories",
   "original": "i08_1529",
   "page_count": 4,
   "order": 478,
   "p1": "1529",
   "pn": "1532",
   "abstract": [
    "In this paper, we introduce a novel distinctive phonetic feature (DPF) extraction method that incorporates inhibition/enhancement functionalities by discriminating the DPF dynamic patterns of trajectories relevant or not. The trajectories of each DPF show a convex pattern when the DPF is relevant and a concave one when irrelevant. The proposed algorithm enhances convex type patterns and inhibits concave type patterns. We implement the algorithm into a phoneme recognizer and evaluate it. The recognizer consists of two stages. The first stage extracts 45 dimensional DPF vectors from local features (LFs) of input speech using a hybrid neural network and incorporates an inhibition/enhancement network to obtain modified DPF patterns, and the second stage orthogonalizes the DPF vectors and then feeds them to an HMM-based classifier. The proposed phoneme recognizer significantly improves the phoneme recognition accuracy with fewer mixture components by resolving coarticulation effects.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-438"
  },
  "hu08c_interspeech": {
   "authors": [
    [
     "Hongbing",
     "Hu"
    ],
    [
     "Stephen A.",
     "Zahorian"
    ]
   ],
   "title": "A neural network based nonlinear feature transformation for speech recognition",
   "original": "i08_1533",
   "page_count": 4,
   "order": 479,
   "p1": "1533",
   "pn": "1536",
   "abstract": [
    "A neural network based feature dimensionality reduction for speech recognition is described for accurate phonetic speech recognition. In our previous work, a neural network based nonlinear principal component analysis (NLPCA) was proposed as a dimensionality reduction approach for speech features. It was shown that the reduced dimensionality features are very effective for representing data for vowel classification. In this paper, we extend this neural network based NLPCA approach for phonetic recognition using continuous speech. The reduced dimensionality features obtained with NLPCA are used as the features for HMM phone models. Experimental evaluation using the TIMIT database shows that recognition accuracies with NLPCA reduced dimensionality features are higher than recognition rates obtained with original features, especially when a small number of states and mixtures are used for HMM phonetic models.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-439"
  },
  "ramya08_interspeech": {
   "authors": [
    [
     "R.",
     "Ramya"
    ],
    [
     "Rajesh M.",
     "Hegde"
    ],
    [
     "Hema A.",
     "Murthy"
    ]
   ],
   "title": "Significance of group delay based acoustic features in the linguistic search space for robust speech recognition",
   "original": "i08_1537",
   "page_count": 4,
   "order": 480,
   "p1": "1537",
   "pn": "1540",
   "abstract": [
    "In this paper we discuss the complementarity of the group delay features with respect to other conventional acoustic features and also propose the use of such diverse information in the linguistic search space for robust speech recognition. A discriminability analysis is carried out on various classes of phonetic units. A class based phonetic unit analysis is conducted to compare the suitability of using different acoustic feature streams for recognition of different phonetic unit classes. The results of recognition for isolated phonemic or syllabic units, give the appropriate feature for each unit. We then turn to describe the significance of this diversity of information present in the various acoustic features and their integration into the linguistic search space for syllable based continuous speech recognition. A weighted average likelihood method is used here, which appropriately weights the relevant acoustic feature for each syllable in question during the Viterbi decoding process. This technique of integrating the complementarity of acoustic features into the linguistic search space gives reasonably reduced word error rates (WER) compared to conventional single or multi-stream acoustic features for experiments conducted on the TIMIT and the DBIL databases.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-440"
  },
  "abbasian08_interspeech": {
   "authors": [
    [
     "Houman",
     "Abbasian"
    ],
    [
     "Babak",
     "Nasersharif"
    ],
    [
     "Ahmad",
     "Akbari"
    ]
   ],
   "title": "Genetic programming based optimization of class-dependent PCA for extracting robust MFCC",
   "original": "i08_1541",
   "page_count": 4,
   "order": 481,
   "p1": "1541",
   "pn": "1544",
   "abstract": [
    "Principal component analysis (PCA) is commonly used in feature extraction. It projects the features in direction of maximum variance. This projection can be performed in a class-dependent or class-independent manner. In this paper, we propose to optimize class-dependent PCA transformation matrix for robust MFCC feature extraction using genetic programming. For this purpose, we first map logarithm of clean speech Mel filter bank energies (LMFE) in directions of maximum variability. We obtain the mapping functions using genetic programming. After this, we form class-dependent PCA transformation matrix based on mapped LMFE and use this matrix in place of DCT in MFCC feature extraction. The experimental results show that proposed method achieves to significant isolated word recognition rate on Aurora2 database.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-441"
  },
  "narayana08_interspeech": {
   "authors": [
    [
     "K. V. S.",
     "Narayana"
    ],
    [
     "T. V.",
     "Sreenivas"
    ]
   ],
   "title": "Comparison of AM-FM based features for robust speech recognition",
   "original": "i08_1545",
   "page_count": 4,
   "order": 482,
   "p1": "1545",
   "pn": "1548",
   "abstract": [
    "Effective feature extraction for robust speech recognition is a widely addressed topic and currently there is much effort to invoke non-stationary signal models instead of quasi-stationary signal models leading to standard features such as LPC or MFCC. Joint amplitude modulation and frequency modulation (AM-FM) is a classical non-parametric approach to non-stationary signal modeling and recently new feature sets for automatic speech recognition (ASR) have been derived based on a multi-band AM-FM representation of the signal. We consider several of these representations and compare their performances for robust speech recognition in noise, using the AURORA-2 database. We show that FEPSTRUM representation proposed is more effective than others. We also propose an improvement to FEPSTRUM based on the Teager energy operator (TEO) and show that it can selectively outperform even FEPSTRUM.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-442"
  },
  "frankel08_interspeech": {
   "authors": [
    [
     "Joe",
     "Frankel"
    ],
    [
     "Dong",
     "Wang"
    ],
    [
     "Simon",
     "King"
    ]
   ],
   "title": "Growing bottleneck features for tandem ASR",
   "original": "i08_1549",
   "page_count": 1,
   "order": 483,
   "p1": "1549",
   "pn": "",
   "abstract": [
    "We present a novel method for training bottleneck MLPs for use in tandem ASR. Experiments on meetings data show that this approach leads to improved performance compared with training MLPs from a random initialization.\n",
    ""
   ]
  },
  "karjigi08_interspeech": {
   "authors": [
    [
     "Veena",
     "Karjigi"
    ],
    [
     "Preeti",
     "Rao"
    ]
   ],
   "title": "Landmark based recognition of stops: acoustic attributes versus smoothed spectra",
   "original": "i08_1550",
   "page_count": 4,
   "order": 484,
   "p1": "1550",
   "pn": "1553",
   "abstract": [
    "Landmark based recognition of unvoiced word-initial stops is investigated. The relative effectiveness of acoustic-phonetic attributes versus more global spectral shape features is experimentally evaluated for four-way place classification of unvoiced, unaspirated stops. Various feature sets derived from the burst and vocalic transition regions of word initial consonants are compared via GMM based classification under speaker, gender, and vowel-context variability. While a set of acoustic attributes derived from the burst shows the best invariance to vowel context, it is found that global spectral shape features provide the most robust representation of the vocalic transition region by overcoming the problem of errors in explicit formant tracking. A combination of features from the burst and vocalic regions was superior to burst-only cues, but still far from the near perfect identification achieved in human perception.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-443"
  },
  "kogure08_interspeech": {
   "authors": [
    [
     "Satoru",
     "Kogure"
    ],
    [
     "Hiromitsu",
     "Nishizaki"
    ],
    [
     "Masatoshi",
     "Tsuchiya"
    ],
    [
     "Kazumasa",
     "Yamamoto"
    ],
    [
     "Shingo",
     "Togashi"
    ],
    [
     "Seiichi",
     "Nakagawa"
    ]
   ],
   "title": "Speech recognition performance of CJLC: corpus of Japanese lecture contents",
   "original": "i08_1554",
   "page_count": 4,
   "order": 485,
   "p1": "1554",
   "pn": "1557",
   "abstract": [
    "This paper discusses the speech recognition of Japanese classroom lecture speech. In particular, we mention the influences of microphone differences and the language model differences on the speech recognition performance of classroom lectures. First, we collected actual classroom lecture contents from several universities in Japan. In this paper, we recorded the lecture speech using lapel microphones because lapel microphones are more commonly used to record lectures. LVCSR is one of the essential technologies for adding tag information to such lecture speech. Next, therefore, we researched the influence of the differences between microphones used for recording lecture on speech recognition performance. Finally, seven types of language models that were trained using three types of corpora were compared on the basis of their ability to lecture speech.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-444"
  },
  "valente08_interspeech": {
   "authors": [
    [
     "Fabio",
     "Valente"
    ],
    [
     "Hynek",
     "Hermansky"
    ]
   ],
   "title": "On the combination of auditory and modulation frequency channels for ASR applications",
   "original": "i08_2242",
   "page_count": 4,
   "order": 486,
   "p1": "2242",
   "pn": "2245",
   "abstract": [
    "This paper investigates the combination of evidence coming from different frequency channels obtained filtering the speech signal at different auditory and modulation frequencies. In our previous work [1], we showed that combination of classifiers trained on different ranges of modulation frequencies is more effective if performed in sequential (hierarchical) fashion. In this work we verify that combination of classifiers trained on different ranges of auditory frequencies is more effective if performed in parallel fashion. Furthermore we propose an architecture based on neural networks for combining evidence coming from different auditorymodulation frequency sub-bands that takes advantages of previous findings. This reduces the final WER by 6.2% (from 45.8% to 39.6%) w.r.t the single classifier approach in a LVCSR task.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-445"
  },
  "tyagi08b_interspeech": {
   "authors": [
    [
     "Vivek",
     "Tyagi"
    ]
   ],
   "title": "Tandem processing of fepstrum features",
   "original": "i08_2246",
   "page_count": 4,
   "order": 487,
   "p1": "2246",
   "pn": "2249",
   "abstract": [
    "In our previous work [1, 2], we have introduced Fepstrum - an improved modulation spectrum estimation technique that overcomes certain theoretical as well as practical shortcomings in the previously published modulation spectrum related techniques[7, 8, 9]. In this paper, we provide further extensive ASR results using the Tandem processed Fepstrum features over the TIMIT corpus. The results are compared with TRAPS features derived from hierarchical and parallel structures of neural networks[3]. Unlike the multiple neural networks trained over multiple timefrequency patches or the frequency bands as in [3], we train a single neural network with the concatenated Fepstrum and MFCC features to derive Tandem(Fepstrum+MFCC) features. The resultant phoneme recognition accuracy of the concatenated Tandem(Fepstrum+MFCC)+MFCC feature is 76.5% on the TIMIT core test set and 77.6% on the complete test set making these one of the best reported results on the TIMIT continuous phoneme recognition task.\n",
    "s V. Tyagi, \"Fepstrum: An improved modualtion spectrum for ASR,\", In the Proc. of Interspeech 2007, Antwerp, Belgium. (ISCA Archive, http://www.isca-speech.org/archive/interspeech_2007)\n",
    "V. Tyagi and C. Wellekens, \"Fepstrum Representation of Speech Signal,\" In the Proc. of IEEE ASRU 2005, Cancun, Mexico.\n",
    "P. Schwarz, P. Matejka and J. Cernocky, \"Hierarchical structures of neural networks for phoneme recognition,\" In the Proc. of IEEE ICASSP 2006.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-446"
  },
  "chang08_interspeech": {
   "authors": [
    [
     "Shuo-Yiin",
     "Chang"
    ],
    [
     "Lin-shan",
     "Lee"
    ]
   ],
   "title": "Data-driven clustered hierarchical tandem system for LVCSR",
   "original": "i08_2250",
   "page_count": 4,
   "order": 488,
   "p1": "2250",
   "pn": "2253",
   "abstract": [
    "In tandem systems, the outputs of multi-layer perceptron (MLP) classifiers have been successfully used as features for HMM-based automatic speech recognition. In this paper, we propose a datadriven clustered hierarchical tandem system that yields improved performance on a large-vocabulary broadcast news transcription task. The complicated global learning for a large monolithic MLP classifier is divided into simpler tasks, in which hierarchical structures clustered based on the outputs of a monolithic MLP are used to alleviate phone confusion. The proposed approach yields error rate reductions of up to 16.4% over MFCC features alone.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-447"
  },
  "lee08d_interspeech": {
   "authors": [
    [
     "Hung-Shin",
     "Lee"
    ],
    [
     "Berlin",
     "Chen"
    ]
   ],
   "title": "Linear discriminant feature extraction using weighted classification confusion information",
   "original": "i08_2254",
   "page_count": 4,
   "order": 489,
   "p1": "2254",
   "pn": "2257",
   "abstract": [
    "Linear discriminant analysis (LDA) can be viewed as a two-stage procedure geometrically. The first stage conducts an orthogonal and whitening transformation of the variables. The second stage involves a principal component analysis (PCA) on the transformed class means, which is intended to maximize the class separability along the principal axes. In this paper, we demonstrate that the second stage does not necessarily guarantee better classification accuracy. Furthermore, we propose a generalization of LDA, weighted LDA (WLDA), by integrating the empirical classification confusion information between each class pair, such that the separability and the classification error rate can be taken into consideration simultaneously. WLDA can be efficiently solved by a lightweight eigen-decomposition and easily combined with other modifications to the LDA criterion. The experiment results show that WLDA can yield a relative character error reduction of 4.6% over LDA on the Mandarin LVCSR task.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-448"
  },
  "sanand08b_interspeech": {
   "authors": [
    [
     "D. R.",
     "Sanand"
    ],
    [
     "V.",
     "Balaji"
    ],
    [
     "Rani R.",
     "Sandhya"
    ],
    [
     "S.",
     "Umesh"
    ]
   ],
   "title": "Use of spectral centre of gravity for generating speaker invariant features for automatic speech recognition",
   "original": "i08_2258",
   "page_count": 4,
   "order": 490,
   "p1": "2258",
   "pn": "2261",
   "abstract": [
    "In this paper, we present an approach to generate speaker invariant features for automatic speech recognition (ASR) using the idea of spectral centre of gravity(CG). This is based on the observation that if two signals are delayed versions of one another, then their CG's also differ by the same amount. We exploit this idea to appropriately shift the mel warped log compressed spectra using the estimated CG to obtain speaker invariant features. The use of such speaker invariant or normalised features helps improve the recognition performance of speaker-independent ASR. We show that our proposed approach is computationally efficient when compared to a commonly used method of normalisation called Vocal Tract Length Normalisation (VTLN). We present normalisation results to show that the performance of our proposed approach is comparable to conventional VTLN and yet has the advantage of computational efficiency.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-449"
  },
  "fukuda08b_interspeech": {
   "authors": [
    [
     "Takashi",
     "Fukuda"
    ],
    [
     "Osamu",
     "Ichikawa"
    ],
    [
     "Masafumi",
     "Nishimura"
    ]
   ],
   "title": "Short- and long-term dynamic features for robust speech recognition",
   "original": "i08_2262",
   "page_count": 4,
   "order": 491,
   "p1": "2262",
   "pn": "2265",
   "abstract": [
    "The short-term temporal information in speech is widely used for automatic speech recognition (ASR) systems in the form of dynamic features. Long-term temporal information has also been focused on recently and is used to complement traditional short-term features (typically from 25 to 100 ms). There are several approaches to represent long-term temporal information in ASR systems. However, those systems use high-dimensional feature spaces to capture the long-term temporal information. This paper describes an attempt to incorporate long-term temporal information into a feature parameter set by combining conventional dynamic features extracted from both short- and long-term cepstrum sequences. The proposed method includes the temporal contexts of phonemes by using long-term features and the spectral variations within phonemes as short-term features. In an experiment on the realistic speech corpus CENSREC-2, the proposed method yielded higher performance than a standard feature parameter set with static mel-frequency cepstral coefficient (MFCCs) and their short-term dynamic features.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-450"
  },
  "kawahara08c_interspeech": {
   "authors": [
    [
     "Tatsuya",
     "Kawahara"
    ],
    [
     "Hisao",
     "Setoguchi"
    ],
    [
     "Katsuya",
     "Takanashi"
    ],
    [
     "Kentaro",
     "Ishizuka"
    ],
    [
     "Shoko",
     "Araki"
    ]
   ],
   "title": "Multi-modal recording, analysis and indexing of poster sessions",
   "original": "i08_1622",
   "page_count": 4,
   "order": 492,
   "p1": "1622",
   "pn": "1625",
   "abstract": [
    "A new project on multi-modal analysis of poster sessions is introduced. We have designed an environment dedicated to recording of poster conversations using multiple sensors, and collected a number of sessions, to which a variety of multi-modal information is annotated, including utterance units for individual speakers, backchannels, nodding, gazing, and pointing. Automatic speaker diarization, that is a combination of speech activity detection and speaker identification, is conducted using a set of distant microphones, and a reasonable performance is obtained. Then, we investigate automatic classification of conversation segments into two modes: presentation mode and question-answer mode. Preliminary experiments show that multi-modal features on nonverbal behaviors play a significant role in the indexing of this kind of conversations.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-451"
  },
  "matousek08_interspeech": {
   "authors": [
    [
     "Jindřich",
     "Matoušek"
    ],
    [
     "Jan",
     "Romportl"
    ]
   ],
   "title": "Automatic pitch-synchronous phonetic segmentation",
   "original": "i08_1626",
   "page_count": 4,
   "order": 493,
   "p1": "1626",
   "pn": "1629",
   "abstract": [
    "This paper deals with an HMM-based automatic phonetic segmentation (APS) system and proposes to increase its performance by employing a pitch-synchronous (PS) coding scheme. Such a coding scheme uses different frames of speech throughout voiced and unvoiced speech regions and enables thus better modelling of each individual phone. The PS coding scheme is shown to outperform the traditionally utilised pitch-asynchronous (PA) coding scheme for two corpora of Czech speech (one female and one male) both in the case of a base (not-refined) APS and in the case of a CART-refined APS. Better results were observed for each of the voicing-dependent boundary types (unvoiced-unvoiced, unvoiced-voiced, voiced-unvoiced and voiced-voiced).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-452"
  },
  "shen08b_interspeech": {
   "authors": [
    [
     "Wade",
     "Shen"
    ],
    [
     "Joseph",
     "Olive"
    ],
    [
     "Douglas",
     "Jones"
    ]
   ],
   "title": "Two protocols comparing human and machine phonetic recognition performance in conversational speech",
   "original": "i08_1630",
   "page_count": 4,
   "order": 494,
   "p1": "1630",
   "pn": "1633",
   "abstract": [
    "This paper describes two experimental protocols for direct comparison of human and machine phonetic discrimination performance in continuous speech. These protocols attempt to isolate phonetic discrimination while eliminating for language and segmentation biases. Results of two human experiments are described including comparisons with automatic phonetic recognition baselines. Our experiments suggest that in conversational telephone speech, human performance on these tasks exceeds that of machines by 15%. Furthermore, in a related controlled language model experiment, human subjects were better able to correctly predict words in conversational speech by 45%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-453"
  },
  "kato08b_interspeech": {
   "authors": [
    [
     "Tomoyuki",
     "Kato"
    ],
    [
     "Jun",
     "Okamoto"
    ],
    [
     "Makoto",
     "Shozakai"
    ]
   ],
   "title": "Analysis of drivers' speech in a car environment",
   "original": "i08_1634",
   "page_count": 4,
   "order": 495,
   "p1": "1634",
   "pn": "1637",
   "abstract": [
    "In order to accelerate the promotion of speech recognition systems to the public; understanding characteristics of speech in real environments is one of the most important issues. This paper reports variations of speech characteristics in a car environment. To analyze speech characteristics in the specific environment, a corpus, recorded carefully in terms of equality of utterances and conditions for whole set of speakers, is necessary. We created a new corpus named \"Drivers' Japanese Speech Corpus in a Car Environment (DJS-C)\": composed of utterances of words useful for the operation of in-vehicle information appliances. Analysis of the DJS-C corpus shows that differences in speech characteristics are diverse among drivers and change with driving conditions. Quantitative analysis and speech recognition experiments show that performance degrades due to Distance between Phonemes, Uniqueness of Speaker's Voice, and SNNR.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-454"
  },
  "schuppler08_interspeech": {
   "authors": [
    [
     "Barbara",
     "Schuppler"
    ],
    [
     "Mirjam",
     "Ernestus"
    ],
    [
     "Odette",
     "Scharenborg"
    ],
    [
     "Lou",
     "Boves"
    ]
   ],
   "title": "Preparing a corpus of dutch spontaneous dialogues for automatic phonetic analysis",
   "original": "i08_1638",
   "page_count": 4,
   "order": 496,
   "p1": "1638",
   "pn": "1641",
   "abstract": [
    "This paper presents the steps needed to make a corpus of Dutch spontaneous dialogues accessible for automatic phonetic research aimed at increasing our understanding of reduction phenomena and the role of fine phonetic detail. Since the corpus was not created with automatic processing in mind, it needed to be reshaped. The first part of this paper describes the actions needed for this reshaping in some detail. The second part reports the results of a preliminary analysis of the reduction phenomena in the corpus. For this purpose a phonemic transcription of the corpus was created by means of a forced alignment, first with a lexicon of canonical pronunciations and then with multiple pronunciation variants per word. In this study pronunciation variants were generated by applying a large set of phonetic processes that have been implicated in reduction to the canonical pronunciations of the words. This relatively straightforward procedure allows us to produce plausible pronunciation variants and to verify and extend the results of previous reduction studies reported in the literature.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-455"
  },
  "kotnik08_interspeech": {
   "authors": [
    [
     "Bojan",
     "Kotnik"
    ],
    [
     "Pierre",
     "Sendorek"
    ],
    [
     "Sergey",
     "Astrov"
    ],
    [
     "Turgay",
     "Koc"
    ],
    [
     "Tolga",
     "Ciloglu"
    ],
    [
     "Laura Docío",
     "Fernández"
    ],
    [
     "Eduardo Rodríguez",
     "Banga"
    ],
    [
     "Harald",
     "Höge"
    ],
    [
     "Zdravko",
     "Kačič"
    ]
   ],
   "title": "Evaluation of voice activity and voicing detection",
   "original": "i08_1642",
   "page_count": 4,
   "order": 497,
   "p1": "1642",
   "pn": "1645",
   "abstract": [
    "This paper describes the ECESS evaluation campaign of voice activity and voicing detection. Standard VAD classifies signal into speech and non-speech, we extend it to VAD+ so that it classifies a signal as a sequence of non-speech, voiced and unvoiced segments. The evaluation is performed on a portion of the Spanish SPEECON database with manually labeled segmentation. To avoid errors caused by the limited precision of manual labeling we introduce \"dead zones\" - tolerance intervals +-5 ms around label changes in the data set. In these tolerance intervals we don't evaluate the signal.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-456"
  },
  "draxler08_interspeech": {
   "authors": [
    [
     "Christoph",
     "Draxler"
    ],
    [
     "Klaus",
     "Jänsch"
    ]
   ],
   "title": "Wikispeech - a content management system for speech databases",
   "original": "i08_1646",
   "page_count": 4,
   "order": 498,
   "p1": "1646",
   "pn": "1649",
   "abstract": [
    "In this paper we describe WikiSpeech, a content management system for the web-based creation of speech databases for the development of spoken language technology and basic research. Its main features are full support for the typical recording, annotation and project administration workflow, easy editing of the speech content, plus a fully localizable user interface.\n",
    "For the creation of a new speech database, it is only necessary to open a new project within WikiSpeech, provide a link to any static project information pages and upload the prompt material to be presented to the speakers. Recordings and annotation are performed via the WWW in a platform independent manner on any Java compatible computer.\n",
    "WikiSpeech currently has been localized to four languages: German, English, Romanian and Russian, and it is now used for production recordings at the Bavarian Archive for Speech Signals in Munich, Germany.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-457"
  },
  "demenko08_interspeech": {
   "authors": [
    [
     "Grazyna",
     "Demenko"
    ],
    [
     "J.",
     "Bachan"
    ],
    [
     "Bernd",
     "Möbius"
    ],
    [
     "K.",
     "Klessa"
    ],
    [
     "M.",
     "Szymański"
    ],
    [
     "Stefan",
     "Grocholewski"
    ]
   ],
   "title": "Development and evaluation of Polish speech corpus for unit selection speech synthesis systems",
   "original": "i08_1650",
   "page_count": 4,
   "order": 499,
   "p1": "1650",
   "pn": "1653",
   "abstract": [
    "This paper presents the results of a set of experiments assessing the perceived quality of the Polish version of the BOSS unit selection synthesis system. The experiments aimed to evaluate the potential improvement of synthesis quality by three factors pertaining to corpus structure and coverage as well as levels of corpus annotation. The three factors affecting synthesis quality were (i) manual vs. automatic corpus annotation, (ii) coverage of CVC triphones in rich intonational patterns, and (iii) coverage of complex consonant clusters. Results indicate that a manual correction of automatic annotations enhances synthesis quality. Increased coverage of CVC sequences and consonant clusters also improved the perceived synthesis quality, but the effect was smaller than anticipated.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-458"
  },
  "pitrelli08_interspeech": {
   "authors": [
    [
     "John F.",
     "Pitrelli"
    ],
    [
     "Burn L.",
     "Lewis"
    ],
    [
     "Edward A.",
     "Epstein"
    ],
    [
     "Jerome L.",
     "Quinn"
    ],
    [
     "Ganesh",
     "Ramaswamy"
    ]
   ],
   "title": "A data format enabling interoperation of speech recognition, translation and information extraction engines: the GALE type system",
   "original": "i08_1654",
   "page_count": 4,
   "order": 500,
   "p1": "1654",
   "pn": "1657",
   "abstract": [
    "Live interoperation of several speech- and text-processing engines is key to tasks such as real-time cross-language story segmentation, topic clustering, and captioning of video. One requirement for interoperation is a common data format shared across engines, so that the output of one can be understood as the input of another. The GALE Type System has been created to serve this purpose for interoperating language-identification, speaker-recognition, speech-recognition, named-entity-detection, translation, story-segmentation, topic-clustering, summarization, and headline-generation engines in the context of Unstructured Information Management Architecture. GTS includes types designed to bridge across the domains of these engines, for example, linking the text-only domain of translation to the time-domain types needed for speech processing, and the monolingual domain of information-extraction engines to the cross-language types needed for translation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-459"
  },
  "li08f_interspeech": {
   "authors": [
    [
     "Wei",
     "Li"
    ],
    [
     "Qiang",
     "Huo"
    ]
   ],
   "title": "A rank-predicted pseudo-greedy approach to efficient text selection from large-scale corpus for maximum coverage of target units",
   "original": "i08_1658",
   "page_count": 4,
   "order": 501,
   "p1": "1658",
   "pn": "1661",
   "abstract": [
    "Selecting efficiently a minimum amount of text from a large-scale text corpus to achieve a maximum coverage of certain units is an important problem in spoken language processing area. In this paper, the above text selection problem is first formulated as a maximum coverage problem with a Knapsack constraint (MCK). An efficient rank-predicted pseudo-greedy approach is then proposed to solve this problem. Experiments on a Chinese text selection task are conducted to verify the efficiency of the proposed approach. Experimental results show that our approach can improve significantly the text selection speed yet without sacrificing the coverage score compared with traditional greedy approach.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-460"
  },
  "engelbrecht08_interspeech": {
   "authors": [
    [
     "Klaus-Peter",
     "Engelbrecht"
    ],
    [
     "Michael",
     "Kruppa"
    ],
    [
     "Sebastian",
     "Möller"
    ],
    [
     "Michael",
     "Quade"
    ]
   ],
   "title": "Memo workbench for semi-automated usability testing",
   "original": "i08_1662",
   "page_count": 4,
   "order": 502,
   "p1": "1662",
   "pn": "1665",
   "abstract": [
    "In this paper we present a new approach to the automation of usability evaluation for interactive systems. Design ideas or complete systems are modeled as a conditional state machine. Then, user interactions with the system are simulated on the basis of tasks, by first searching for possible solution paths and then generating deviations from these paths under consideration of user groups and system attributes. The approach has been implemented into a workbench which supports the modeling of the system and the evaluation of the simulations. We present first results for the reliability of the approach in modeling interactions with a spoken dialog system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-461"
  },
  "yamakawa08_interspeech": {
   "authors": [
    [
     "Kimiko",
     "Yamakawa"
    ],
    [
     "Tomoko",
     "Matsui"
    ],
    [
     "Shuichi",
     "Itahashi"
    ]
   ],
   "title": "MDS-based visualization method for multiple speech corpora",
   "original": "i08_1666",
   "page_count": 4,
   "order": 503,
   "p1": "1666",
   "pn": "1669",
   "abstract": [
    "The purpose of this study is to visualize the similarities between speech corpora. Speech data are indispensable for promoting speech research. A wide variety of speech corpora has recently been developed in many countries. Corpus diversification has given users many choices for corpus selection. In order for users to easily utilize these various corpora, we propose a new feature visualization method based on the corpus attribute. First, we listed eight attributes of the speech corpora. Then, we selected a few items for each attribute resulting in 58 items in all. Each item takes on a '1' or '0' depending on whether the corpus has the attribute or not. The set of corpus features is represented as a 58-dimensional vector. Then, the vectors are converted into a similarity matrix and analyzed using a multidimensional scaling method (MDS).We analyzed the speech corpora distributed by the Speech Resources Consortium (NII-SRC). The results showed that it is possible to visualize the similarities between multiple speech corpora using the proposed method. We also tested the effectiveness of the proposed method by analyzing six imaginary corpora having some specified attributes. This result will facilitate the idea of being able to search a specific corpus according to a user's needs.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-462"
  },
  "busso08b_interspeech": {
   "authors": [
    [
     "Carlos",
     "Busso"
    ],
    [
     "Shrikanth S.",
     "Narayanan"
    ]
   ],
   "title": "Scripted dialogs versus improvisation: lessons learned about emotional elicitation techniques from the IEMOCAP database",
   "original": "i08_1670",
   "page_count": 4,
   "order": 504,
   "p1": "1670",
   "pn": "1673",
   "abstract": [
    "Recording high quality data is an important step in the study of emotions. Given the inherent limitations and complexities of the current approaches to capture natural emotions collected in real-life scenarios, the use of professional actors appears to be a viable research methodology. To avoid stereotypical realization of the emotions, better elicitation techniques rooted in theatrical performance are needed. Based on the lessons learned from the collection of the IEMOCAP database, this paper analyzes the advantages and disadvantages of two of the most appealing elicitation approaches: improvisation, and scripted dialogs. These methods are studied in terms of the lexical content, disfluencies, facial activity and emotional content. The results indicate that spontaneous sessions have higher levels of disfluencies and overlapped speech. Also, the emotional content seems to be more intense than in scripted sessions, as revealed in subjective evaluations.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-463"
  },
  "deshmukh08_interspeech": {
   "authors": [
    [
     "Om D.",
     "Deshmukh"
    ],
    [
     "Sachindra",
     "Joshi"
    ],
    [
     "Ashish",
     "Verma"
    ]
   ],
   "title": "Automatic pronunciation evaluation and classification",
   "original": "i08_1721",
   "page_count": 4,
   "order": 505,
   "p1": "1721",
   "pn": "1724",
   "abstract": [
    "Pronunciation evaluation is an important module of every spoken language evaluation system. Automatic evaluation of quality of pronunciation that can mimic the performance of human assessors is a difficult task as human assessment accounts for several nuances of pronunciation including vowel substitutions and quality of consonants. This paper presents a novel approach that combines the knowledge of human assessment and the knowledge of the behaviour of automatic speech recognition systems to develop features for pronunciation evaluation. Instead of presenting the correlation of the proposed features with human assessment, the paper presents sentence-level classification accuracies which can directly be used in real-life applications. Inter-human and intrahuman agreements, which are indicative of human subjectivity, are also presented. The trends in confusions among humans scores and automatic scores are compared as the number of classification classes is varied.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-464"
  },
  "bolanos08b_interspeech": {
   "authors": [
    [
     "Daniel",
     "Bolanos"
    ],
    [
     "Wayne",
     "Ward"
    ],
    [
     "Barbara",
     "Wise"
    ],
    [
     "Sarel van",
     "Vuuren"
    ]
   ],
   "title": "Pronunciation error detection techniques for children's speech",
   "original": "i08_1725",
   "page_count": 4,
   "order": 506,
   "p1": "1725",
   "pn": "1728",
   "abstract": [
    "In this article we present a novel method for automatic pronunciation error detection of children's speech. A phone graph is generated from the audio segment and augmented if necessary with alignments of phonetic transcriptions of the word to score. This graph is used for extracting phone-level features using conventional HMM/GMM acoustic scores and Support Vector Machine (SVM) classifiers acting as probabilistic estimators. Finally an SVM is used to combine the phone-level features extracted for each word to produce a word-based pronunciation score. Experimental results show that the proposed method and features can be effectively used for pronunciation scoring. In particular the detection of mispronunciations is increased more than 22% with respect to the baseline.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-465"
  },
  "wang08i_interspeech": {
   "authors": [
    [
     "Lan",
     "Wang"
    ],
    [
     "Xin",
     "Feng"
    ],
    [
     "Helen M.",
     "Meng"
    ]
   ],
   "title": "Automatic generation and pruning of phonetic mispronunciations to support computer-aided pronunciation training",
   "original": "i08_1729",
   "page_count": 4,
   "order": 507,
   "p1": "1729",
   "pn": "1732",
   "abstract": [
    "This paper presents a mispronunciation detection system which uses automatic speech recognition to support computer-aided pronunciation training (CAPT). Our methodology extends a model pronunciation lexicon with possible phonetic mispronunciations that may appear in learners' speech. Generation of these pronunciation variants was previously achieved by means of phone-to-phone mapping rules derived from a cross-language phonological comparison between the primary language (L1, Cantonese) and secondary language (L2, American English). This rule-based generation process results in many implausible candidates of mispronunciation. We present a methodology that applies Viterbi decoding on learners' speech using an HMM-based recognizer and the fully extended pronunciation dictionary. Word boundaries are thus identified and all pronunciation variants are scored and ranked based on Viterbi scores. Pruning is applied to keep the N-best pronunciation variants which are deemed plausible candidates for mispronunciation detection. Experiments based on the speech recordings from 21 Cantonese learners of English shows that the agreement between automatic mispronunciation detection and human judges is over 86%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-466"
  },
  "li08g_interspeech": {
   "authors": [
    [
     "Xiaolong",
     "Li"
    ],
    [
     "Li",
     "Deng"
    ],
    [
     "Yun-Cheng",
     "Ju"
    ],
    [
     "Alex",
     "Acero"
    ]
   ],
   "title": "Automatic children's reading tutor on hand-held devices",
   "original": "i08_1733",
   "page_count": 4,
   "order": 508,
   "p1": "1733",
   "pn": "1736",
   "abstract": [
    "This paper presents an Automatic Reading Tutoring (ART) system using state-of-the-art speech recognition technologies aimed to improve children's oral reading ability. The features of this system include a compact and robust language model designed for detecting disfluencies in children's speech, low-footprint implementation, and built-in microphone array. Our system is targeting on hand-held devices to provide better accessibility, flexibility, and freedom for children's reading practice. The focus of this paper is on the current system's architecture, which has achieved real-time performance on two hand-held, small-form-factor devices (UMPC and Motion Tablet), with the same detection rate and false alarm rate as on desktop PCs. We also report the latest effort on a prototype system running on a PDA (Windows Mobile 6).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-467"
  },
  "wang08j_interspeech": {
   "authors": [
    [
     "Hongcui",
     "Wang"
    ],
    [
     "Tatsuya",
     "Kawahara"
    ]
   ],
   "title": "A Japanese CALL system based on dynamic question generation and error prediction for ASR",
   "original": "i08_1737",
   "page_count": 4,
   "order": 509,
   "p1": "1737",
   "pn": "1740",
   "abstract": [
    "We have developed a new CALL system to aid students learning Japanese as a second language. The system offers students the chance to practice the Japanese grammar and vocabulary, by creating their own sentences based on visual prompts, before receiving feedback on their mistakes. Questions are dynamically generated along with sentence patterns of the lesson point, to realize variety and flexibility of the lesson. Students can give their answers with either text input or speech input. To enhance speech recognition performance, a decision tree-based method is incorporated to predict possible errors made by non-native speakers for each generated sentence on the fly. Trials of the system have been conducted with a number of foreign students in our university, and positive feedbacks were obtained.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-468"
  },
  "black08_interspeech": {
   "authors": [
    [
     "Matthew",
     "Black"
    ],
    [
     "Joseph",
     "Tepperman"
    ],
    [
     "Sungbok",
     "Lee"
    ],
    [
     "Shrikanth S.",
     "Narayanan"
    ]
   ],
   "title": "Estimation of children's reading ability by fusion of automatic pronunciation verification and fluency detection",
   "original": "i08_2779",
   "page_count": 4,
   "order": 510,
   "p1": "2779",
   "pn": "2782",
   "abstract": [
    "Pronunciation verification of children's reading is a difficult task in itself, but automatic reading assessment software must also detect and evaluate other phenomena that influence human evaluators. Using an isolated word-reading task, we first show that humans use both pronunciation correctness (accuracy) and fluency information in their assessment of the reading ability of kindergarten to second grade children. Next, we used disfluency-specialized grammars and trained a Bayesian Network to automatically classify the fluency and accuracy of an utterance. Finally, we used these automatically determined scores to estimate evaluators' scores of children's reading ability with a 0.91 correlation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-469"
  },
  "black08b_interspeech": {
   "authors": [
    [
     "Matthew",
     "Black"
    ],
    [
     "Joseph",
     "Tepperman"
    ],
    [
     "Abe",
     "Kazemzadeh"
    ],
    [
     "Sungbok",
     "Lee"
    ],
    [
     "Shrikanth S.",
     "Narayanan"
    ]
   ],
   "title": "Pronunciation verification of English letter-sounds in preliterate children",
   "original": "i08_2783",
   "page_count": 4,
   "order": 511,
   "p1": "2783",
   "pn": "2786",
   "abstract": [
    "Correctly reading letter-sounds is an essential first step towards reading words and sentences. Pronunciation assessment of lettersounds is an important component of preliterate children's education, and automating this process can have several advantages. We propose a method to automatically verify the pronunciations of a letter-sound task administered to kindergarteners and first graders in realistic noisy classrooms. We compare different acoustic models, decoding grammars, and dictionaries to help differentiate between acceptable and unacceptable pronunciations. Our final system achieved 88.0 percent agreement (0.702 kappa agreement) with expert human evaluators, who agree themselves 94.9 percent of the time (0.886 kappa agreement).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-470"
  },
  "harrison08_interspeech": {
   "authors": [
    [
     "Alissa M.",
     "Harrison"
    ],
    [
     "Wing Yiu",
     "Lau"
    ],
    [
     "Helen M.",
     "Meng"
    ],
    [
     "Lan",
     "Wang"
    ]
   ],
   "title": "Improving mispronunciation detection and diagnosis of learners' speech with context-sensitive phonological rules based on language transfer",
   "original": "i08_2787",
   "page_count": 4,
   "order": 512,
   "p1": "2787",
   "pn": "2790",
   "abstract": [
    "This study demonstrates how knowledge of language transfer can enable a computer-assisted pronunciation teaching (CAPT) system to effectively detect and diagnose salient mispronunciations in second language learners' speech. Our approach uses a HMM-based speech recognizer with an extended pronunciation lexicon that includes both a model pronunciation for each word and common pronunciation variants of our target learners. The pronunciation variants in the extended pronunciation lexicon are generated based on language transfer theory (i.e knowledge from the first language is transferred to the second language). We find that a lexicon that characterizes language transfer using context-sensitive phonological rules can detect and diagnose errors better than a lexicon generated from context-insensitive rules. Furthermore, predicting errors from language transfer alone can approach the performance of a system where the lexicon is fully-informed of all possible pronunciation errors.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-471"
  },
  "cucchiarini08_interspeech": {
   "authors": [
    [
     "Catia",
     "Cucchiarini"
    ],
    [
     "Joost van",
     "Doremalen"
    ],
    [
     "Helmer",
     "Strik"
    ]
   ],
   "title": "DISCO: development and integration of speech technology into courseware for language learning",
   "original": "i08_2791",
   "page_count": 4,
   "order": 513,
   "p1": "2791",
   "pn": "2794",
   "abstract": [
    "Recent research has shown that a properly designed ASR-based CALL system (Dutch-CAPT) was capable of detecting pronunciation errors and of providing comprehensible feedback on pronunciation. Since pronunciation is not the only skill required for speaking a second language, we explored the possibility of extending the Dutch- CAPT approach to other aspects of speaking proficiency like morphology and syntax. In this paper we explain how a number of errors in morphology and syntax that are common in spoken Dutch L2 could be addressed in an ASR-based CALL system. Finally, we present our new project in which corrective feedback will be provided on all three aspects of spoken proficiency: pronunciation, morphology and syntax.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-472"
  },
  "samir08_interspeech": {
   "authors": [
    [
     "Abdurrahman",
     "Samir"
    ],
    [
     "Jacques",
     "Duchateau"
    ],
    [
     "Hugo",
     "Van hamme"
    ]
   ],
   "title": "Discriminative model combination and language model selection in a reading tutor for children",
   "original": "i08_2795",
   "page_count": 4,
   "order": 514,
   "p1": "2795",
   "pn": "2798",
   "abstract": [
    "In this paper, we suggest the use of general acoustic and language models to deal with the mismatch between the training and testing data of a reading tutor for children. The testing data consist of isolated real and non-existing (pseudo) words, while the training data consist of continuous readings of Dutch sentences. General acoustic (e.g. context independent) and language models (e.g. bigram phone language models) are proposed as they implicitly better model the hesitant nature of the testing data. Discriminative model combination (DMC) is modified to provide different weights for different phones and was utilized to combine the new models into the baseline system. Combination of general acoustic and language models into the baseline system using DMC significantly lowers the system phone error rate, by 3.5% relative to the baseline system for the non-existing (pseudo) words.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-473"
  },
  "pedersen08_interspeech": {
   "authors": [
    [
     "Jakob Schou",
     "Pedersen"
    ],
    [
     "Lars Bo",
     "Larsen"
    ],
    [
     "Børge",
     "Lindberg"
    ]
   ],
   "title": "Usability of ASR-based reading training for dyslexics",
   "original": "i08_2799",
   "page_count": 4,
   "order": 515,
   "p1": "2799",
   "pn": "2802",
   "abstract": [
    "As an initial step towards self-training of dyslexics, this paper presents the overall design- and usability evaluation results of a simulated reading training system for dyslexics that uses ASR and multi modal presentation techniques as core components. Based on an analysis of dyslexic reading behaviour as well as extensive usability evaluations involving actual dyslexic test subjects it is indicated that current ASR performance in fact suffices for dealing with dyslexic input to an automated training system when the special phenomena present in dyslexic speech are taken into account. Finally, explicit design guidelines for such a system are derived experimentally aiming at assuring a high level of perceived usability when used by dyslexic users.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-474"
  },
  "togashi08_interspeech": {
   "authors": [
    [
     "Shingo",
     "Togashi"
    ],
    [
     "Seiichi",
     "Nakagawa"
    ]
   ],
   "title": "A browsing system for classroom lecture speech",
   "original": "i08_2803",
   "page_count": 4,
   "order": 516,
   "p1": "2803",
   "pn": "2806",
   "abstract": [
    "Developing technologies to summarize and retrieve huge quantities of spoken documents, recorded during classroom lectures, for the purpose of e-Learning or self-learning are important. In this paper, we describe an adaptation method of a language model to recognize keywords in given slides. Next, we propose a summarization method for spoken classroom lectures using prosodic features and linguistic information. Last, we describe our \"Classroom Lecture Browsing System\", which enables more efficient access to lecture video by using the results of keyword-indexing and automatic summarization.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-475"
  },
  "luo08b_interspeech": {
   "authors": [
    [
     "Dean",
     "Luo"
    ],
    [
     "Naoya",
     "Shimomura"
    ],
    [
     "Nobuaki",
     "Minematsu"
    ],
    [
     "Yutaka",
     "Yamauchi"
    ],
    [
     "Keikichi",
     "Hirose"
    ]
   ],
   "title": "Automatic pronunciation evaluation of language learners' utterances generated through shadowing",
   "original": "i08_2807",
   "page_count": 4,
   "order": 517,
   "p1": "2807",
   "pn": "2810",
   "abstract": [
    "In foreign language learning, shadowing has been used as a method for improving speaking and listening ability. In this method, learners are required to repeat a presented native utterance as closely and quickly as possible. Since learners have to follow the speaking rate of the presented utterance, their pronunciation often becomes very inarticulate and unintelligible. These features of shadowing make it very difficult to build a reliable scoring system for shadowing productions. In this paper, two techniques are proposed and investigated for automatic scoring of shadowing productions. Experiments show that good correlations are found between automatic scores and TOEIC overall proficiency scores.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-476"
  },
  "chevalier08_interspeech": {
   "authors": [
    [
     "Sylvain",
     "Chevalier"
    ],
    [
     "Zhenhai",
     "Cao"
    ]
   ],
   "title": "Application and evaluation of speech technologies in language learning: experiments with the Saybot player",
   "original": "i08_2811",
   "page_count": 4,
   "order": 518,
   "p1": "2811",
   "pn": "2814",
   "abstract": [
    "In this paper we present Saybot Player, an application of speech technologies to help Chinese learners of English. It is a dialogue system based on a speech recognizer and a pronunciation scoring engine. We compare our approach with existing similar systems, propose metrics to evaluate such systems and give evaluation results obtained with our users in real learning situations.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-477"
  },
  "ge08_interspeech": {
   "authors": [
    [
     "Fengpei",
     "Ge"
    ],
    [
     "Fuping",
     "Pan"
    ],
    [
     "Changliang",
     "Liu"
    ],
    [
     "Bin",
     "Dong"
    ],
    [
     "Yonghong",
     "Yan"
    ]
   ],
   "title": "Forward optimal modeling of acoustic confusions in Mandarin CALL system",
   "original": "i08_2815",
   "page_count": 4,
   "order": 519,
   "p1": "2815",
   "pn": "2818",
   "abstract": [
    "Acoustic confusions degrade the accuracy of pronunciation assessment severely in Computer Assisted Language Learning (CALL) systems. This paper presents our recent study on optimal modeling of the acoustic confusions. We change the traditional mandarin syllable structure, which is composed of initial and final, to a novel phoneme structure. Several phoneme splitting strategies are investigated, and the question list used for building and merging decision tree is studied. The questions are special to each phoneme splitting strategy. Experiments show that the optimal phoneme splitting strategy outperforms the traditional initial-final structure in our CALL system, with relative 11.05% ASER improvement for nasal finals. This idea may be extended to improve the performance of automatic speech recognition (ASR).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-478"
  },
  "ito08c_interspeech": {
   "authors": [
    [
     "Akinori",
     "Ito"
    ],
    [
     "Ryohei",
     "Tsutsui"
    ],
    [
     "Shozo",
     "Makino"
    ],
    [
     "Motoyuki",
     "Suzuki"
    ]
   ],
   "title": "Recognition of English utterances with grammatical and lexical mistakes for dialogue-based CALL system",
   "original": "i08_2819",
   "page_count": 4,
   "order": 520,
   "p1": "2819",
   "pn": "2822",
   "abstract": [
    "Our goal is to develop a voice-interactive CALL system which enables language learners to practice words, phrases, and grammars interactively. Such a system must be able to recognize learner's utterances correctly. To enable the recognition of utterances containing grammatical mistakes, we used an n-gram language model trained from generated text. The proposed model achieved recognition performance similar to that of a language model based on a finite-state automaton and manual error rules. We then introduced two error correction techniques to improve recognition performance. One method used the Levenshtein distance between the target sentence and the recognized sentence. The other method used an error-corrective model based on POS n-gram features. The experimental results showed that both methods were able to improve recognition performance.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-479"
  },
  "kim08c_interspeech": {
   "authors": [
    [
     "Heejin",
     "Kim"
    ],
    [
     "Mark",
     "Hasegawa-Johnson"
    ],
    [
     "Adrienne",
     "Perlman"
    ],
    [
     "Jon",
     "Gunderson"
    ],
    [
     "Thomas S.",
     "Huang"
    ],
    [
     "Kenneth",
     "Watkin"
    ],
    [
     "Simone",
     "Frame"
    ]
   ],
   "title": "Dysarthric speech database for universal access research",
   "original": "i08_1741",
   "page_count": 4,
   "order": 521,
   "p1": "1741",
   "pn": "1744",
   "abstract": [
    "This paper describes a database of dysarthric speech produced by 19 speakers with cerebral palsy. Speech materials consist of 765 isolated words per speaker: 300 distinct uncommon words and 3 repetitions of digits, computer commands, radio alphabet and common words. Data is recorded through an 8-microphone array and one digital video camera. Our database provides a fundamental resource for automatic speech recognition development for people with neuromotor disability. Research on articulation errors in dysarthria will benefit clinical treatments and contribute to our knowledge of neuromotor mechanisms in speech production. Data files are available via secure ftp upon request.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-480"
  },
  "middag08_interspeech": {
   "authors": [
    [
     "Catherine",
     "Middag"
    ],
    [
     "Gwen Van",
     "Nuffelen"
    ],
    [
     "Jean-Pierre",
     "Martens"
    ],
    [
     "Marc De",
     "Bodt"
    ]
   ],
   "title": "Objective intelligibility assessment of pathological speakers",
   "original": "i08_1745",
   "page_count": 4,
   "order": 522,
   "p1": "1745",
   "pn": "1748",
   "abstract": [
    "Intelligibility is a primary measure for the assessment of pathological speech. Traditionally, it is measured using a perceptual test, which is by definition subjective in nature. Consequently, there is a great interest in reliable, automatic and therefore objective methods. This paper presents such a method that incorporates an automatic speech recognizer (ASR) for producing features that characterize the pronunciations of a speaker and an intelligibility prediction model (IPM) for converting these features into an intelligibility score. High correlations (about 0.90) between objective and perceptual scores are obtained with a system comprising two different speech recognizers: one with traditional acoustic models relating acoustical observations to triphone states and one using phonological features as an intermediate layer between the acoustical observations and the phonetic states.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-481"
  },
  "ma08d_interspeech": {
   "authors": [
    [
     "Joan K.-Y.",
     "Ma"
    ],
    [
     "Tara L.",
     "Whitehill"
    ]
   ],
   "title": "Quantitative analysis of intonation patterns produced by Cantonese speakers with Parkinson's disease: a preliminary study",
   "original": "i08_1749",
   "page_count": 4,
   "order": 523,
   "p1": "1749",
   "pn": "1752",
   "abstract": [
    "This preliminary study aimed to apply the command-response model to the analysis of disrupted intonation patterns in speakers with Parkinson's disease (PD). Three Cantonese PD speakers with mild prosodic impairment participated. The speech stimuli were 36 utterances (questions and statements). Productions were analyzed using the model parameters (base frequency, magnitude and onset time of each phrase command, and amplitude and duration of each tone command). The results showed that Cantonese PD speakers marked the question-statement contrast only by adding a question boundary tone command and contrasting the duration of the tone command at the final position, but not by differences in base frequency, as previously reported for non-dysarthric speakers. This study showed promising results for using the command-response model to analyze intonation patterns in Cantonese dysarthric speech. Additional modelling issues for this group of speakers are discussed.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-482"
  },
  "bruijn08_interspeech": {
   "authors": [
    [
     "Marieke de",
     "Bruijn"
    ],
    [
     "Irma Verdonck de",
     "Leeuw"
    ],
    [
     "Louis ten",
     "Bosch"
    ],
    [
     "Joop",
     "Kuik"
    ],
    [
     "Hugo",
     "Quene"
    ],
    [
     "Lou",
     "Boves"
    ],
    [
     "Hans",
     "Langendijk"
    ],
    [
     "Rene",
     "Leemans"
    ]
   ],
   "title": "Phonetic-acoustic and feature analyses by a neural network to assess speech quality in patients treated for head and neck cancer",
   "original": "i08_1753",
   "page_count": 4,
   "order": 524,
   "p1": "1753",
   "pn": "1756",
   "abstract": [
    "Subjective speech evaluation is the gold standard to assess speech quality of head and neck cancer patients. This study investigates if conventional acoustic-phonetic and novel feature analysis contribute to the development of a multidimensional speech assessment protocol. Speech recordings of 51 patients 6 months post-treatment and of 18 control speakers were subjectively evaluated for intelligibility, nasal resonance and articulation. Self-evaluation of speech problems was assessed by the EORTC QLQ-H&N35 speech subscale. Feature analysis was performed to assess objectively nasality in vowels /a,i,u/. Results revealed that size of the vowel triangle, pressure release of /k/ and nasality in /i/ predict best intelligibility, articulation and nasal resonance and differentiated best between patients and controls. Within patients, /k/ and /x/ differentiated tumour site and tumour classification. Various objective variables were related to speech problems as reported by patients.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-483"
  },
  "maier08_interspeech": {
   "authors": [
    [
     "Andreas",
     "Maier"
    ],
    [
     "Florian",
     "Hönig"
    ],
    [
     "Christian",
     "Hacker"
    ],
    [
     "Maria",
     "Schuster"
    ],
    [
     "Elmar",
     "Nöth"
    ]
   ],
   "title": "Automatic evaluation of characteristic speech disorders in children with cleft lip and palate",
   "original": "i08_1757",
   "page_count": 4,
   "order": 525,
   "p1": "1757",
   "pn": "1760",
   "abstract": [
    "This paper discusses the automatic evaluation of speech of children with cleft lip and palate (CLP). CLP speech shows special characteristics such as hypernasality, backing, and weakening of plosives. In total five criteria were subjectively assessed by an experienced speech expert on the phone level. This subjective evaluation was used as a gold standard to train a classification system. The automatic system achieves recognition results on frame, phone, and word level of up to 75.8% CL. On speaker level signiicant and high correlations between the subjective evaluation and the automatic system of up to 0.89 are obtained.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-484"
  },
  "morales08_interspeech": {
   "authors": [
    [
     "Omar Caballero",
     "Morales"
    ],
    [
     "Stephen",
     "Cox"
    ]
   ],
   "title": "Application of weighted finite-state transducers to improve recognition accuracy for dysarthric speech",
   "original": "i08_1761",
   "page_count": 4,
   "order": 526,
   "p1": "1761",
   "pn": "1764",
   "abstract": [
    "Standard speaker adaptation algorithms perform poorly on dysarthric speech because of the limited phonemic repertoire of dysarthric speakers. In a previous paper, we proposed the use of \"metamodels\" to correct dysarthric speech. Here, we report on an improved technique that makes use of a cascade of Weighted Finite-State Transducers (WFSTs) at the confusion-matrix, word and language levels. This approach outperforms both standard MLLR and metamodels.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-485"
  },
  "cooke08_interspeech": {
   "authors": [
    [
     "Martin",
     "Cooke"
    ],
    [
     "Odette",
     "Scharenborg"
    ]
   ],
   "title": "The interspeech 2008 consonant challenge",
   "original": "i08_1765",
   "page_count": 4,
   "order": 527,
   "p1": "1765",
   "pn": "1768",
   "abstract": [
    "Listeners outperform automatic speech recognition systems at every level, including the very basic level of consonant identification. What is not clear is where the human advantage originates. Does the fault lie in the acoustic representations of speech or in the recognizer architecture, or in a lack of compatibility between the two? Many insights can be gained by carrying out a detailed human-machine comparison. The purpose of the Interspeech 2008 Consonant Challenge is to promote focused comparisons on a task involving intervocalic consonant identification in noise, with all participants using the same training and test data. This paper describes the Challenge, listener results and baseline ASR performance.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-486"
  },
  "borgstrom08_interspeech": {
   "authors": [
    [
     "Bengt J.",
     "Borgström"
    ],
    [
     "Abeer",
     "Alwan"
    ]
   ],
   "title": "HMM-based estimation of unreliable spectral components for noise robust speech recognition",
   "original": "i08_1769",
   "page_count": 4,
   "order": 528,
   "p1": "1769",
   "pn": "1772",
   "abstract": [
    "This paper presents a novel approach for reconstructing unreliable spectral components, which utilizes HMM-based missing feature algorithms, and applies them to noise robust speech recognition. The proposed technique uses the forward-backward algorithm to estimate corrupt spectrographic data based on nearby reliable features, noisy observations, and on an underlying statistical model. The estimation process can be applied based on intra-channel information, intra-feature information, or a combination of both. The overall system is shown to provide vast improvements for the Consonant Challenge Database [1], for both MFCCs and PLP features, when using an oracle mask. Moreover, through downsampling of statistical models [2], the required complexity of the system is greatly reduced with negligible effects on results.\n",
    "s http://www.odettes.dds.nl/challengeIS08/index.html B. J. Borgström and A. Alwan, An Efficient Approximation of the Forward-Backward Algorithm to Deal With Packet Loss, With Applications to Remote Speech Recognition, Proc. of ICASSP 2008.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-487"
  },
  "yoon08_interspeech": {
   "authors": [
    [
     "Jae Sam",
     "Yoon"
    ],
    [
     "Ji Hun",
     "Park"
    ],
    [
     "Hong Kook",
     "Kim"
    ]
   ],
   "title": "Gammatone-domain model combination for consonant recognition in noisy environments",
   "original": "i08_1773",
   "page_count": 4,
   "order": 529,
   "p1": "1773",
   "pn": "1776",
   "abstract": [
    "In this paper, a gammatone-domain model combination method is proposed for consonant recognition in noisy environments. For this task, we first define a gammatone cepstral coefficient (GCC) as the cepstral representation of the averaged envelopes of a gammatone filtered signal. Then, we investigate a proper phonetic unit by comparing monophone, diphone, and triphone acoustic models, where it is determined from consonant recognition experiments that the diphone hidden Markov models (HMMs) provide the best performance. Next, a gammatone-domain model combination method is developed to combine the clean and noise models in the linear gammatone-envelope domain. We then evaluate the performance of the GCC-based feature and the proposed model combination on intervocalic English consonants (VCV) with 24 different consonants. It is experimentally shown that the GCC-based feature achieves a relatively higher recognition rate of 47.46% than the mel-frequency cepstral coefficients (MFCCs). Also, the model combination applied to the GCC-based diphone HMM system relatively increases the accuracy rate by 77.67% under the noisy conditions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-488"
  },
  "jancovic08_interspeech": {
   "authors": [
    [
     "Peter",
     "Jančovič"
    ],
    [
     "Munevver",
     "Kokuer"
    ]
   ],
   "title": "On the mask modeling and feature representation in the missing-feature ASR: evaluation on the Consonant Challenge",
   "original": "i08_1777",
   "page_count": 4,
   "order": 530,
   "p1": "1777",
   "pn": "1780",
   "abstract": [
    "In this paper we investigate an incorporation of mask modeling within the missing-feature HMM-based ASR and also explore on feature representation within this framework. The mask model is estimated for each HMM state and mixture by using a separate Viterbi-style training procedure. We explore an employment of the frequency-filtered features and their combination with the logarithm filter-bank energies. Experimental evaluation is performed on the Consonant Challenge corpus. The obtained results show significant improvements by incorporation of the proposed methods over the standard MFT-based ASR.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-489"
  },
  "garcialecumberri08_interspeech": {
   "authors": [
    [
     "M. Luisa",
     "Garcia Lecumberri"
    ],
    [
     "Martin",
     "Cooke"
    ],
    [
     "Francesco",
     "Cutugno"
    ],
    [
     "Mircea",
     "Giurgiu"
    ],
    [
     "Bernd T.",
     "Meyer"
    ],
    [
     "Odette",
     "Scharenborg"
    ],
    [
     "Wim van",
     "Dommelen"
    ],
    [
     "Jan",
     "Volin"
    ]
   ],
   "title": "The non-native consonant challenge for european languages",
   "original": "i08_1781",
   "page_count": 4,
   "order": 531,
   "p1": "1781",
   "pn": "1784",
   "abstract": [
    "This paper reports on a multilingual investigation into the effects of different masker types on native and non-native perception in a VCV consonant recognition task. Native listeners outperformed 7 other language groups, but all groups showed a similar ranking of maskers. Strong first language (L1) interference was observed, both from the sound system and from the L1 orthography. Universal acoustic-perceptual tendencies are also at work in both native and non-native sound identifications in noise. The effect of linguistic distance, however, was less clear: in large multilingual studies, listener variables may overpower other factors.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-490"
  },
  "gemmeke08_interspeech": {
   "authors": [
    [
     "J. F.",
     "Gemmeke"
    ],
    [
     "Bert",
     "Cranen"
    ]
   ],
   "title": "Noise reduction through compressed sensing",
   "original": "i08_1785",
   "page_count": 4,
   "order": 532,
   "p1": "1785",
   "pn": "1788",
   "abstract": [
    "We present an exemplar-based method for noise reduction using missing data imputation: A noise-corrupted word is sparsely represented in an over-complete basis of exemplar (clean) speech signals using only the uncorrupted time-frequency elements of the word. Prior to recognition the parts of the spectrogram dominated by noise are replaced by clean speech estimates obtained by projecting the sparse representation in the basis. Since at low SNRs individual frames may contain few, if any, uncorrupted coefficients, the method tries to exploit all reliable information that is available in a word-length time window. We study the effectiveness of this approach on the Interspeech 2008 Consonant Challenge (VCV) data as well as on AURORA-2 data. Using oracle masks, we obtain obtain accuracies of 36.44% on the VCV data. On AURORA-2 we obtain an accuracy of 91% at SNR -5 dB, compared to 61% using a conventional frame-based approach, clearly illustrating the great potential of the method.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-491"
  },
  "schuller08c_interspeech": {
   "authors": [
    [
     "Björn",
     "Schuller"
    ],
    [
     "Martin",
     "Wöllmer"
    ],
    [
     "Tobias",
     "Moosmayr"
    ],
    [
     "Gerhard",
     "Rigoll"
    ]
   ],
   "title": "Speech recognition in noisy environments using a switching linear dynamic model for feature enhancement",
   "original": "i08_1789",
   "page_count": 4,
   "order": 533,
   "p1": "1789",
   "pn": "1792",
   "abstract": [
    "The performance of automatic speech recognition systems strongly decreases whenever the speech signal is disturbed by background noise. We aim to improve noise robustness focusing on all major levels of speech recognition: feature extraction, feature enhancement, and speech modeling. Different auditory modeling concepts, speech enhancement techniques, training strategies, and model architectures are implemented in an in-car digit and spelling recognition task. We prove that joint speech and noise modeling with a global Switching Linear Dynamic Model (SLDM) capturing the dynamics of speech, and a Linear Dynamic Model (LDM) for noise, prevails over state-of-the-art speech enhancement techniques. Furthermore we show that the baseline recognizer of the Interspeech Consonant Challenge 2008 can be outperformed by SLDM feature enhancement for almost all of the noisy testsets.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-492"
  },
  "hodoshima08_interspeech": {
   "authors": [
    [
     "Nao",
     "Hodoshima"
    ],
    [
     "Wataru",
     "Yoshida"
    ],
    [
     "Takayuki",
     "Arai"
    ]
   ],
   "title": "Improving consonant identification in noise and reverberation by steady-state suppression as a preprocessing approach",
   "original": "i08_1793",
   "page_count": 4,
   "order": 534,
   "p1": "1793",
   "pn": "1796",
   "abstract": [
    "Noise (N), reverberation (R), and a combination of N and R (NR) differently degrade speech intelligibility. The current study aims to improve speech intelligibility in public spaces by processing speech signals through public address systems (a preprocessing approach). As a preprocessing approach, we proposed steady-state suppression, and it has improved consonant identification in R. The current study tests the effect of steady-state suppression in N, R, and NR at three signal to noise ratios and at reverberation time of 0.9 s. Results showed that steady-state suppression significantly improved consonant identification by 21 young people in NR and in R. Furthermore, steady-state suppression improved consonant identification more in NR than in R. The results indicate that steady-state suppression may be applicable to public spaces having N and R. The results also indicate that an integration of N and R improves the performance of a preprocessing approach in a certain range of N and R.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-493"
  },
  "lobdell08_interspeech": {
   "authors": [
    [
     "Bryce E.",
     "Lobdell"
    ],
    [
     "Mark",
     "Hasegawa-Johnson"
    ],
    [
     "Jont B.",
     "Allen"
    ]
   ],
   "title": "Human speech perception and feature extraction",
   "original": "i08_1797",
   "page_count": 4,
   "order": 535,
   "p1": "1797",
   "pn": "1800",
   "abstract": [
    "Speech perception experiments tell us a great deal about which factors affect human performance and behavior. In particular many experiments indicate that the signal-to-noise ratio spectrum is an important factor, indeed the signal-to-noise ratio spectrum is the basis of the Articulation Index, a standard measure of \"speech channel capacity.\" In this paper we compare speech recognition performance for features based on the Articulation Index with two alternatives typically used in speech recognition. The experimental conditions vary the spectrum and level of noise distorting the speech in the training and test set. The perceptually inspired features generally perform better when there is a mismatch between the training and test noise spectrum and level, but worse when the test and training noises match.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-494"
  },
  "tan08b_interspeech": {
   "authors": [
    [
     "Tien-Ping",
     "Tan"
    ],
    [
     "Laurent",
     "Besacier"
    ]
   ],
   "title": "Improving pronunciation modeling for non-native speech recognition",
   "original": "i08_1801",
   "page_count": 4,
   "order": 536,
   "p1": "1801",
   "pn": "1804",
   "abstract": [
    "In this paper, three different approaches to pronunciation modeling are investigated. Two existing pronunciation modeling approaches, namely the pronunciation dictionary and n-best rescoring approach are modified to work with little amount of non-native speech. We also propose a speaker clustering approach, which capable of grouping the speakers based on their pronunciation habits. Given some speech, the approach can also be used for pronunciation adaptation. This approach is called latent pronunciation analysis. The results show that conventional pronunciation dictionary perform slightly better than n-best list rescoring, while the latent pronunciation analysis has shown to be beneficial for speaker clustering, and it can produce nearly the same improvement as the pronunciation dictionary approach, without the need to know the origin of the speaker.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-495"
  },
  "aronowitz08b_interspeech": {
   "authors": [
    [
     "Hagai",
     "Aronowitz"
    ]
   ],
   "title": "Online vocabulary adaptation using contextual information and information retrieval",
   "original": "i08_1805",
   "page_count": 4,
   "order": 537,
   "p1": "1805",
   "pn": "1808",
   "abstract": [
    "This paper presents an algorithm for automatic online vocabulary adaptation based on contextual information and information retrieval. Experiments are presented on a transcription task of spoken annotations of business cards recorded by a hand-held device. Contextual information is used to trigger web search which is used to adapt the vocabulary for a given business card. Finally, the language model for the adapted vocabulary is modified by taking into account the relative value of each context information source. On the business card task, the proposed algorithm reduces 75% of the out-of-vocabulary rate and 16% of the word error rate.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-496"
  },
  "onishi08_interspeech": {
   "authors": [
    [
     "Yoshifumi",
     "Onishi"
    ]
   ],
   "title": "Lexicon expansion using pronunciation variations extracted on the basis of speaker-related deviation in recognition error statistics",
   "original": "i08_1809",
   "page_count": 4,
   "order": 538,
   "p1": "1809",
   "pn": "1812",
   "abstract": [
    "We propose a novel method for lexicon expansion using pronunciation variations extracted on the basis of speaker-related deviations in ASR error statistics. Two types of pronunciation variations were extracted: common pronunciation variations found with most speakers, and speaker-related pronunciation variations, identified on the basis of recognition error elements weighted by idf and tf-idf measures. Experimental results for CSJ show that entries added to the lexicon from speaker-related pronunciation variations were more effective than those generated on the basis of common pronunciation variations, some of which were superfluous.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-497"
  },
  "tepperman08_interspeech": {
   "authors": [
    [
     "Joseph",
     "Tepperman"
    ],
    [
     "Shrikanth S.",
     "Narayanan"
    ]
   ],
   "title": "Better nonnative intonation scores through prosodic theory",
   "original": "i08_1813",
   "page_count": 4,
   "order": 539,
   "p1": "1813",
   "pn": "1816",
   "abstract": [
    "Pronunciation scoring is one important task for software designed to give feedback to students practicing a second language. English intonation can convey information about a speaker's nativeness, so previous studies have proposed using intonation-based models to score nonnative pronunciation. One past approach trained models for a set of pronunciation scores using ad hoc features derived from the frequency contour. We use prosodic theory to train models for categorical intonation units, inspired by work in modeling tone languages. These HMM-based models offer 0.398 correlation between automatic and listener scores on the ISLE nonnative speech corpus, compared to the 0.156 baseline correlation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-498"
  },
  "garner08_interspeech": {
   "authors": [
    [
     "Philip N.",
     "Garner"
    ]
   ],
   "title": "Silence models in weighted finite-state transducers",
   "original": "i08_1817",
   "page_count": 4,
   "order": 540,
   "p1": "1817",
   "pn": "1820",
   "abstract": [
    "We investigate the effects of different silence modelling strategies in Weighted Finite-State Transducers for Automatic Speech Recognition. We show that the choice of silence models, and the way they are included in the transducer, can have a significant effect on the size of the resulting transducer; we present a means to prevent particularly large silence overheads. Our conclusions include that context-free silence modelling fits well with transducer based grammars, whereas modelling silence as a monophone and a context has larger overheads.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-499"
  },
  "sasada08_interspeech": {
   "authors": [
    [
     "Tetsuro",
     "Sasada"
    ],
    [
     "Shinsuke",
     "Mori"
    ],
    [
     "Tatsuya",
     "Kawahara"
    ]
   ],
   "title": "Extracting word-pronunciation pairs from comparable set of text and speech",
   "original": "i08_1821",
   "page_count": 4,
   "order": 541,
   "p1": "1821",
   "pn": "1824",
   "abstract": [
    "One of the problems in text-to-speech (TTS) systems and speech-totext (STT) systems is pronunciation estimation of unknown words. In this paper, we propose a method for extracting unknown words and their pronunciations from similar sets of Japanese text data and speech data. Out-of-vocabulary words are extracted from text with a stochastic model and pronunciations hypotheses are generated. These entries are verified by conducting automatic speech recognition on audio data. In this work, we use news articles and broadcast TV news covering similar topics. Most extracted pairs turned out to be correct according to a human judges. We also tested the TTS front-end enhanced with these entries on other web news articles, and observed an improvement in the pronunciation estimation accuracy of 9.2% (relative). The proposed method can be used to realize a spoken language processing system that acquires and updates its lexicon automatically.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-500"
  },
  "jin08_interspeech": {
   "authors": [
    [
     "Qin",
     "Jin"
    ],
    [
     "Tanja",
     "Schultz"
    ]
   ],
   "title": "Robust far-field speaker identification under mismatched conditions",
   "original": "i08_1893",
   "page_count": 4,
   "order": 542,
   "p1": "1893",
   "pn": "1896",
   "abstract": [
    "While speaker identification performance has improved dramatically over the past years, the presence of interfering noise and the variety of channel conditions pose a major obstacle. Particularly the mismatch between training and test condition leads to severe performance degradations. In this paper we investigate speaker identification based on data simultaneously recorded with multiple microphones in a far-field setup under different noise and reverberation conditions. Dramatic performance degradation is observed, especially when training and test conditions mismatch. To address this mismatch we apply our robust frame-based score competition approach in which we combine and compete models trained on multiple conditions. To further improve this approach we add simulated, i.e. artificially created training data on a variety of noise conditions for additional model training. Our experimental results show that the extended approach significantly improves speaker identification performance under adverse and mismatching conditions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-501"
  },
  "huang08d_interspeech": {
   "authors": [
    [
     "Chien-Lin",
     "Huang"
    ],
    [
     "Bin",
     "Ma"
    ],
    [
     "Chung-Hsien",
     "Wu"
    ],
    [
     "Brian",
     "Mak"
    ],
    [
     "Haizhou",
     "Li"
    ]
   ],
   "title": "Robust speaker verification using short-time frequency with long-time window and fusion of multi-resolutions",
   "original": "i08_1897",
   "page_count": 4,
   "order": 543,
   "p1": "1897",
   "pn": "1900",
   "abstract": [
    "This study presents a novel approach of feature analysis to speaker verification. There are two main contributions in this paper. First, the feature analysis of short-time frequency with long-time window (SFLW) is a compact feature for the efficiency of speaker verification. The purpose of SFLW is to take account of short-time frequency characteristics and long-time resolution at the same time. Secondly, the fusion of multi-resolutions is used for the effectiveness of robust speaker verification. The speaker verification system can be further improved using multi-resolution features. The experimental results indicate that the proposed approaches not only speed up the processing time but also improve the performance of speaker verification.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-502"
  },
  "kwon08_interspeech": {
   "authors": [
    [
     "C. H.",
     "Kwon"
    ],
    [
     "J. K.",
     "Choi"
    ],
    [
     "Eliathamby",
     "Ambikairajah"
    ]
   ],
   "title": "Performance improvement of text-independent speaker verification systems based on histogram enhancement in noisy environments",
   "original": "i08_1901",
   "page_count": 4,
   "order": 544,
   "p1": "1901",
   "pn": "1904",
   "abstract": [
    "In this paper a histogram enhancement technique is presented in order to improve the robustness of text-independent speaker verification systems. The technique transforms the features extracted from speech such that the contrast of their histogram is enhanced. Experiments showed significant improvements for this technique compared to standard techniques both in clean testing environments, and in the presence of additive noise.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-503"
  },
  "suh08_interspeech": {
   "authors": [
    [
     "Jun-Won",
     "Suh"
    ],
    [
     "Pongtep",
     "Angkititrakul"
    ],
    [
     "John H. L.",
     "Hansen"
    ]
   ],
   "title": "Filling acoustic holes through leveraged uncorellated GMMs for in-set/out-of-set speaker recognition",
   "original": "i08_1905",
   "page_count": 4,
   "order": 545,
   "p1": "1905",
   "pn": "1908",
   "abstract": [
    "In this study, the problem of in-set versus out-of-set speaker recognition for limited train/test data is addressed. Since enrollment data is so limited (5 sec), acoustic holes in the speaker phoneme space from training tokens will exist and must be filled. To achieve this, a cohort speaker selection process is developed that possess similar acoustic characteristics. The resulting GMM from common sentences are used to measure the speaker's acoustic similarity with the Kullback-Leibler (KL) distance. The likelihood ratio scores are employed to measure the speaker similarity when no common sentence structure exists. Gaussian components corresponding to the acoustic holes are harvested from the cohort model. Constructed using a phone recognition simulator with 65% accuracy, a comparison is made with the GMM employing common utterances with the TIMIT corpus. Finally, the combination of Gaussian components corresponding to acoustic holes and the common acoustic space are leveraged to improve overall system performance. The proposed acoustic hole filling algorithm is evaluated using speech from the TIMIT and FISHER corpora with the GMM-UBM as our baseline system. The proposed acoustic hole filling system is shown to improve performance by 25% and 13% over the baseline on TIMIT and FISHER. This advancement is a significant step forward in-set/out-of-set speaker recognition with limited train (5 sec) and test material (2.8 sec).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-504"
  },
  "kim08d_interspeech": {
   "authors": [
    [
     "Wooil",
     "Kim"
    ],
    [
     "John H. L.",
     "Hansen"
    ]
   ],
   "title": "Missing-feature method for speaker recognition in band-restricted conditions",
   "original": "i08_1909",
   "page_count": 4,
   "order": 546,
   "p1": "1909",
   "pn": "1912",
   "abstract": [
    "In this study, the missing-feature method is considered to address band-limited speech for speaker recognition. In an effort to mitigate possible degradation due to the general speaker independent model, a two-step reconstruction scheme is developed, where speaker class independent/dependent models are used separately. An advanced marginalization in the cepstral domain is proposed employing a high order extension method in order to address loss of model accuracy in the conventional method due to cepstrum truncation. To detect the cut-off regions from incoming speech, a blind mask estimation scheme is employed which uses a synthesized band-limited speech model. Experimental results on band-limited conditions indicate that our two-step reconstruction scheme with missing-feature processing is effective in improving in-set/out-of-set speaker recognition performance for band-limited speech, particularly in severely band-restricted conditions (i.e., 4.72% EER improvement in 2, 3, and 4kHz band-limited conditions over a conventional data-driven method). The improvement of the proposed marginalization method proves its effectiveness for acoustic model conversion by employing high order extension, showing 0.57% EER improvement over conventional marginalization.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-505"
  },
  "zhang08e_interspeech": {
   "authors": [
    [
     "Yushi",
     "Zhang"
    ],
    [
     "Waleed H.",
     "Abdulla"
    ]
   ],
   "title": "Robust speaker identification using cross-correlation GTF-ICA feature",
   "original": "i08_1913",
   "page_count": 4,
   "order": 547,
   "p1": "1913",
   "pn": "1916",
   "abstract": [
    "Robust feature for speaker identification in noisy environments is proposed. This method is inspired by the human binaural auditory system. A pair of microphones is used to replicate human ears in the processing. Cross-correlation processing is taken of the microphone outputs after Gammatone bandpass filtering, rectification and compression. ICA is then applied to the real cepstrum of the correlated waveform to extract the dominant components from each frequency band. The resulting feature emphases the difference in the statistical structures among speakers. Compared to the commonly used MFCC techniques, the proposed method is more robust to background noises and provides higher identification rate in real noisy environments for text-independent speaker identification systems. A specially prepared noisy speech corpus was used to gauge the performance of the proposed feature.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-506"
  },
  "amino08_interspeech": {
   "authors": [
    [
     "Kanae",
     "Amino"
    ],
    [
     "Takayuki",
     "Arai"
    ]
   ],
   "title": "Perceptual speaker identification using monosyllabic stimuli - effects of the nucleus vowels and speaker characteristics contained in nasals",
   "original": "i08_1917",
   "page_count": 4,
   "order": 548,
   "p1": "1917",
   "pn": "1920",
   "abstract": [
    "The goal of our research is to find out the acoustical correlates of human perception of speaker identity. In this study we investigated the effects of the stimulus contents on perceptual speaker identification. Forty-eight monosyllables were used as the stimuli for identifying four male speakers. The results showed that the syllables containing a coronal nasal yielded higher identification accuracies than the syllables without it, and the syllables with a back vowel gained significantly better scores than those with a front vowel. We also found speaker-dependent characteristics in the velar movements in articulation of nasal consonants.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-507"
  },
  "das08b_interspeech": {
   "authors": [
    [
     "Amitava",
     "Das"
    ],
    [
     "Gokul",
     "Chittaranjan"
    ]
   ],
   "title": "Text-dependent speaker recognition by efficient capture of speaker dynamics in compressed time-frequency representations of speech",
   "original": "i08_1921",
   "page_count": 4,
   "order": 549,
   "p1": "1921",
   "pn": "1924",
   "abstract": [
    "Prevalent speaker recognition methods use only spectral-envelope based features such as MFCC, ignoring the rich speaker identity information contained in the temporal-spectral dynamics of the entire speech signal. We propose a new feature called compressed spectral dynamics or CSD for speaker recognition based on a compressed time-frequency representations of spoken passwords which effectively captures the speaker identity. The fixed-dimension nature of the CSD allows classification to remain simple while keeping the discriminatory power of the 2D intermediate time-frequency representations. The proposed MSRI-CSD text-dependent speaker recognition method uses a simple nearest neighbor classifier and delivers performance competitive to conventional MFCC+DTW based speaker recognition methods at significantly lower complexity.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-508"
  },
  "das08c_interspeech": {
   "authors": [
    [
     "Amitava",
     "Das"
    ],
    [
     "Gokul",
     "Chittaranjan"
    ],
    [
     "Gopala K.",
     "Anumanchipalli"
    ]
   ],
   "title": "Usefulness of text-conditioning and a new database for text-dependent speaker recognition research",
   "original": "i08_1925",
   "page_count": 4,
   "order": 550,
   "p1": "1925",
   "pn": "1928",
   "abstract": [
    "Text Dependent (TD) Speaker Recognition systems assume that the password to be uttered by the speaker is known to the system. As the password is known, the system can apply a password-specific model capturing the speaker dynamics well. This enables TD systems to perform better than text-independent systems. We present a variation of the TD systems, called text-conditioning, in which the password is uniquely chosen by each user. This delivers a higher level of discrimination since the linguistic and phonetic differences of the passwords themselves are exploited in separating the speakers. As the database for such a study was not publicly available, we built an extensive database for speaker recognition having such text-conditioning property. The database is tested with various speaker recognition trials. The results indicate that for the design of a practical TD speaker-recognition system, \"text-conditioning\" does offer a significant edge.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-509"
  },
  "tsuge08_interspeech": {
   "authors": [
    [
     "Satoru",
     "Tsuge"
    ],
    [
     "Takashi",
     "Osanai"
    ],
    [
     "Hisanori",
     "Makinae"
    ],
    [
     "Toshiaki",
     "Kamada"
    ],
    [
     "Minoru",
     "Fukumi"
    ],
    [
     "Shingo",
     "Kuroiwa"
    ]
   ],
   "title": "Combination method of bone-conduction speech and air-conduction speech for speaker recognition",
   "original": "i08_1929",
   "page_count": 4,
   "order": 551,
   "p1": "1929",
   "pn": "1932",
   "abstract": [
    "Recently, some new sensors, such as bone-conductive microphones, throat microphones, and non-audible murmur (NAM) microphones, besides conventional condenser microphones have been developed for collecting speech data. Accordingly, some researchers began to study speaker and speech recognition using speech data collected by these new sensors. We focus on bone-conduction speech data collected by the bone-conductive microphone. This paper proposes a novel speaker identification method which combines \"bone-conduction speech\" and \"air-conduction speech\". The proposed method conducts speaker identification by integrating the similarity calculated by air-conduction speech model and similarity calculated by bone-conduction speech model. For evaluating the proposed method, we conduct the speaker identification experiment using part of a large bone-conduction speech corpus constructed by National Research Institute of Police Science, Japan (NRIPS). Experimental results show that the proposed method can reduce a identification error rate of air-conduction speech and bone-conduction speech. Especially, the proposed method achieves that the average error reduction rate from air-conduction speech to the proposed method is 35.8%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-510"
  },
  "toledano08_interspeech": {
   "authors": [
    [
     "Doroteo T.",
     "Toledano"
    ],
    [
     "Daniel",
     "Hernandez-Lopez"
    ],
    [
     "Cristina",
     "Esteve-Elizalde"
    ],
    [
     "Joaquin",
     "Gonzalez-Rodriguez"
    ],
    [
     "Ruben Fernandez",
     "Pozo"
    ],
    [
     "Luis Hernandez",
     "Gomez"
    ]
   ],
   "title": "MAP and sub-word level t-norm for text-dependent speaker recognition",
   "original": "i08_1933",
   "page_count": 4,
   "order": 552,
   "p1": "1933",
   "pn": "1936",
   "abstract": [
    "This paper presents improvements in text-dependent speaker recognition based on the use of Maximum A Posteriori (MAP) adaptation of Hidden Markov Models and the use of new sub-word level T-Normalization procedures. Results on the YOHO corpus show that the use of MAP adaptation provides a relative improvement of 22.6% in Equal Error Rate (EER) in comparison with Baum-Welch retraining and Maximum Likelihood Linear Regression (MLLR) adaptation. The newly proposed sub-word level T-Normalization procedures provide additional relative improvements, particularly for small cohorts, of up to 20% in EER in comparison with the normal utterance-level T-Normalization.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-511"
  },
  "zhang08f_interspeech": {
   "authors": [
    [
     "Cuiling",
     "Zhang"
    ],
    [
     "Geoffrey Stewart",
     "Morrison"
    ],
    [
     "Philip",
     "Rose"
    ]
   ],
   "title": "Forensic speaker recognition in Chinese: a multivariate likelihood ratio discrimination on /i/ and /y/",
   "original": "i08_1937",
   "page_count": 4,
   "order": 553,
   "p1": "1937",
   "pn": "1940",
   "abstract": [
    "A likelihood-ratio-based forensic speaker discrimination was conducted using the mean formant frequencies of Standard Chinese /i/ and /y/ tokens produced by 64 male speakers. The speech data were relatively forensically realistic in that they were relatively extemporaneous, were recorded over the telephone, and were from three non-contemporaneous recording sessions. A multivariate-kernel-density formula was used to calculate crossvalidated likelihood ratios comparing all possible same-speaker and different-speaker combinations across sessions. Results were comparable with those previously obtained with laboratory speech in other languages. In general, greater strength of evidence was obtained for recording sessions separated by one week than for recording sessions separated by one month.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-512"
  },
  "ishihara08b_interspeech": {
   "authors": [
    [
     "Shunichi",
     "Ishihara"
    ],
    [
     "Yuko",
     "Kinoshita"
    ]
   ],
   "title": "How many do we need? exploration of the population size effect on the performance of forensic speaker classification",
   "original": "i08_1941",
   "page_count": 4,
   "order": 554,
   "p1": "1941",
   "pn": "1944",
   "abstract": [
    "This paper investigates how change in population size affects the reliability of the likelihood ratio (LR)-based forensic speaker classification. Using features of the long term F0 distribution, we performed LR-based speaker classification and examined its performance with population sizes from 10 to 120. The results revealed that LRs could be heavily influenced by the population data. We discovered that the reliability of LR-based evaluation of the evidence was heavily compromised if the population data was limited to a small number of speakers.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-513"
  },
  "leung08_interspeech": {
   "authors": [
    [
     "Cheung-Chi",
     "Leung"
    ],
    [
     "Marc",
     "Ferras"
    ],
    [
     "Claude",
     "Barras"
    ],
    [
     "Jean-Luc",
     "Gauvain"
    ]
   ],
   "title": "Comparing prosodic models for speaker recognition",
   "original": "i08_1945",
   "page_count": 4,
   "order": 555,
   "p1": "1945",
   "pn": "1948",
   "abstract": [
    "Recently, speaker verification systems using different kinds of prosodic features have been proposed. Although it has been shown that most of these speaker verification systems can improve system performance using score-level fusion with state-of-the-art cepstral-based systems, a systematic comparison of the prosodic modelling algorithms used in these prosodic systems has not yet been performed. This motivated us to review the proposed prosodic modelling algorithms and compare them using a common experimental condition.\n",
    "These experiments explored different approaches in the sampling/ segmentation of prosodic contours and the selection of prosodic features. They show that simple prosodic systems with features extracted from fixed-size contour segments, without knowledge of phone/pseudo-syllable level information, still provide significant performance improvement when fused with a state-of-the-art cepstral-based system. Moreover, some prosodic systems are shown to be complementary to each other. Fusion of these systems with the cepstral-based system can provide further performance improvement on the speaker verification task.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-514"
  },
  "zieger08b_interspeech": {
   "authors": [
    [
     "Christian",
     "Zieger"
    ],
    [
     "Maurizio",
     "Omologo"
    ]
   ],
   "title": "Combination of clean and contaminated GMM/SVM for far-field text-independent speaker verification",
   "original": "i08_1949",
   "page_count": 4,
   "order": 556,
   "p1": "1949",
   "pn": "1952",
   "abstract": [
    "This paper addresses the problem of speaker verification under reverberant conditions, using only the signal acquired by a single distant microphone. The proposed system combines four different subsystems. Two of them are Gaussian Mixture Model (GMM) based and the other two are Support Vector Machine (SVM) based. The subsystems that use the same type of classifier differ in terms of models: one is trained with clean speech and the other is trained with noisy and reverberant speech obtained through the contamination of the clean data with the measured impulse responses of the room. The results show that the proposed system outperforms each single subsystem under matched or mismatched conditions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-515"
  },
  "braun08b_interspeech": {
   "authors": [
    [
     "Bettina",
     "Braun"
    ],
    [
     "Kristin",
     "Lemhofer"
    ],
    [
     "Anne",
     "Cutler"
    ]
   ],
   "title": "English word stress as produced by English and dutch speakers: the role of segmental and suprasegmental differences",
   "original": "i08_1953",
   "page_count": 1,
   "order": 557,
   "p1": "1953",
   "pn": "",
   "abstract": [
    "It has been claimed that Dutch listeners use suprasegmental cues (duration, spectral tilt) more than English listeners in distinguishing English word stress. We tested whether this asymmetry also holds in production, comparing the realization of English word stress by native English speakers and Dutch speakers. Results confirmed that English speakers centralize unstressed vowels more, while Dutch speakers of English make more use of suprasegmental differences.\n",
    ""
   ]
  },
  "reinisch08_interspeech": {
   "authors": [
    [
     "Eva",
     "Reinisch"
    ],
    [
     "Alexandra",
     "Jesse"
    ],
    [
     "James M.",
     "McQueen"
    ]
   ],
   "title": "The strength of stress-related lexical competition depends on the presence of first-syllable stress",
   "original": "i08_1954",
   "page_count": 1,
   "order": 558,
   "p1": "1954",
   "pn": "",
   "abstract": [
    "Dutch listeners' looks to printed words were tracked while they listened to instructions to click on one of them. When presented with targets from word pairs where the first two syllables were segmentally identical but differed in stress location, listeners used stress information to recognize the target before segmental information disambiguated the words. Furthermore, the amount of lexical competition was influenced by the presence or absence of word-initial stress.\n",
    ""
   ]
  },
  "ishikawa08_interspeech": {
   "authors": [
    [
     "Keiichi",
     "Ishikawa"
    ],
    [
     "Jun",
     "Nomura"
    ]
   ],
   "title": "Word stress placement by native speakers and Japanese learners of English",
   "original": "i08_1955",
   "page_count": 4,
   "order": 559,
   "p1": "1955",
   "pn": "1958",
   "abstract": [
    "This study investigated what factors are used by native speakers and Japanese learners of English when they determine the position of stress in producing and perceiving disyllabic English nonwords. Syllable structure (vowel length and number of consonants) and lexical class affected both groups, but in different ways. Phonotactic legality of intervocalic consonants influenced native speakers alone. The results are also discussed regarding the difference between production and perception tasks and the influence of Japanese syllables and lexical properties on learning English stress patterns.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-516"
  },
  "bunnell08_interspeech": {
   "authors": [
    [
     "H. Timothy",
     "Bunnell"
    ],
    [
     "Jason",
     "Lilley"
    ]
   ],
   "title": "Schwa variants in american English",
   "original": "i08_1959",
   "page_count": 4,
   "order": 560,
   "p1": "1959",
   "pn": "1962",
   "abstract": [
    "In this study, we examine the acoustic structure of schwa variants in a speech corpus intended for concatenative TTS. Our goals are two-fold. First, as a matter of academic interest, we seek to characterize schwa acoustics in a speech corpus designed to provide broad coverage of di- and tri-phone contexts. This characterization extends recent work on the distinction between the barred-i variant of schwa /1/ and the more central /@/ form of schwa [1]. Our second goal is to improve the concordance between our transcriptions as generated by the front-end of our TTS system, and the phonetic behavior of talkers who record corpora for concatenative TTS. Based on analysis of a single talker's corpus, our results support the claim in [1] that /1/ is more common than generally assumed. However, the claim that /1/ characterizes all stem-medial schwas is not well-supported by our data.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-517"
  },
  "yuan08b_interspeech": {
   "authors": [
    [
     "Jiahong",
     "Yuan"
    ]
   ],
   "title": "Covariations of English segmental durations across speakers",
   "original": "i08_1963",
   "page_count": 1,
   "order": 561,
   "p1": "1963",
   "pn": "",
   "abstract": [
    "This study investigated the ways in which segmental durations co-vary across speakers in a large speech corpus. We found that pauses were negatively correlated with other phones. Speakers who produced longer speech phones tended to have shorter pauses. The approximants had little correlation with the other phone classes. This suggests that the durations of the approximants are controlled by speaker-dependent mechanisms.\n",
    ""
   ]
  },
  "joto08_interspeech": {
   "authors": [
    [
     "Akiyo",
     "Joto"
    ]
   ],
   "title": "The intelligibility of the English vowel /ʌ/ produced by native speakers of Japanese and its relations to the acoustic characteristics",
   "original": "i08_1964",
   "page_count": 4,
   "order": 562,
   "p1": "1964",
   "pn": "1967",
   "abstract": [
    "This paper examined the intelligibility of the English mid-low central vowel /ʌ/ produced by 20 native Japanese speakers (JE /ʌ/), and the acoustic characteristics of JE /ʌ/ according to the different levels of intelligibility. Five native English speakers evaluated the intelligibility of JE /ʌ/ in the word \"but.\" F1 and F2 frequencies and the duration of JE /ʌ/ were measured and compared with those of /ʌ/ spoken by native English speakers and the Japanese vowel /a/. It was found that JE /ʌ/ with lower intelligibility was more often misheard for the low front vowel /æ/ by the American listeners and tended to have a longer duration and higher F1 and F2 frequencies. L1 transfer occurred in the pronunciation of /ʌ/ by the Japanese speakers, which resulted in the differences in the perception of JE /ʌ/ by the native English speakers.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-518"
  },
  "weiss08c_interspeech": {
   "authors": [
    [
     "Benjamin",
     "Weiss"
    ]
   ],
   "title": "Rate dependent spectral reduction for voiceless fricatives",
   "original": "i08_1968",
   "page_count": 1,
   "order": 563,
   "p1": "1968",
   "pn": "",
   "abstract": [
    "The relationship between local speaking rate and spectral reduction of voiceless fricatives is examined based on a corpus of spontaneous speech. The spectral balance of about 18k items is statistically analyzed with regard to consonant duration and local speaking rate. Both measures show comparable effects: For most of the treated allophones, significant within-speaker reduction occurs with higher rates. The speaking style of the material is constant and stress, part of speech, and position in the syllable was controlled. However, as the observed effects are small compared to differences between allophones, perceptional relevance is considered to be implausible.\n",
    ""
   ]
  },
  "ojala08_interspeech": {
   "authors": [
    [
     "Stina",
     "Ojala"
    ],
    [
     "Olli",
     "Aaltonen"
    ],
    [
     "Tapio",
     "Salakoski"
    ]
   ],
   "title": "Investigating perception of places of articulation in sign and speech",
   "original": "i08_1969",
   "page_count": 1,
   "order": 564,
   "p1": "1969",
   "pn": "",
   "abstract": [
    "Speech perception in field of phonetics is widely studied with behavioural testing based on the notion of categorical perception. Recently signed languages are also being studied within phonetic framework. The experimental design presented within this research project aims for analogous study design parameters for both speech and sign.\n",
    ""
   ]
  },
  "tyler08_interspeech": {
   "authors": [
    [
     "Michael D.",
     "Tyler"
    ],
    [
     "Catherine T.",
     "Best"
    ],
    [
     "Louis M.",
     "Goldstein"
    ],
    [
     "Mark",
     "Antoniou"
    ],
    [
     "Lidija",
     "Krebs-Lazendic"
    ]
   ],
   "title": "Six- and twelve-month-olds' discrimination of native versus non-native between- and within-organ fricative place contrasts",
   "original": "i08_1970",
   "page_count": 1,
   "order": 565,
   "p1": "1970",
   "pn": "",
   "abstract": [
    "Discrimination of native versus non-native between- and withinarticulatory organ fricative contrasts was examined in 6 and 12 month-old infants. 12 month-olds discriminated between- (tongue-tip vs. lips) but not within-organ place contrasts (two tongue tip constriction locations), but 6 month-olds only did so for the non-native between-organ contrast. The results support the Articulatory Organ Hypothesis that infants attend more to differences between active articulatory organs than to differences between specific gestures of a single organ (e.g., constriction location or degree).\n",
    ""
   ]
  },
  "lam08_interspeech": {
   "authors": [
    [
     "Christa",
     "Lam"
    ],
    [
     "Christine",
     "Kitamura"
    ]
   ],
   "title": "your baby can't hear you: how mothers talk to infants with simulated hearing loss",
   "original": "i08_1971",
   "page_count": 1,
   "order": 566,
   "p1": "1971",
   "pn": "",
   "abstract": [
    "In this study mothers were told \"your baby can't hear you,\" and their interactions recorded in either (i) a hearing condition in which infants could hear their mothers or (ii) a hearing loss condition in which infants could not hear their mothers. It was found that pitch levels of speech were similar in both conditions, but that there was vowel hyperarticulation in the hearing, but not the hearing loss condition, suggesting that mothers use of linguistic devices depends on their infants responsiveness.\n",
    ""
   ]
  },
  "klintfors08_interspeech": {
   "authors": [
    [
     "Eeva",
     "Klintfors"
    ],
    [
     "Ulla",
     "Sundberg"
    ],
    [
     "Francisco",
     "Lacerda"
    ],
    [
     "Ellen",
     "Marklund"
    ],
    [
     "Lisa",
     "Gustavsson"
    ],
    [
     "Ulla",
     "Bjursäter"
    ],
    [
     "Iris-Corinna",
     "Schwarz"
    ],
    [
     "Göran",
     "Söderlund"
    ]
   ],
   "title": "Development of communicative skills in 8- to 16-month-old children: a longitudinal study",
   "original": "i08_1972",
   "page_count": 4,
   "order": 567,
   "p1": "1972",
   "pn": "1975",
   "abstract": [
    "The aim of this study was to assess development of communicative skills in 8- to 16-month-old children. Information on 24 Swedish children's speech comprehension and production, as well as their utilization of communicative gestures was collected. A version of the MacArthur Communicative Development Inventory, the Swedish Early Communicative Development Inventory (SECDI), was used. The number of comprehended phrases, size of receptive and productive vocabularies, as well as the subjects' gesture score was estimated according to standardized scoring instructions. The study was performed longitudinally based on 71 completed forms. The children's performance was validated with existing norm-data collected from a large set of randomly selected children. The results showed an overall agreement with the norm-data. The performance of the subjects was though less stable and delayed about one month as compared to the norm-data. Adequacy of SECDI for screening language delay is discussed.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-519"
  },
  "gustavsson08_interspeech": {
   "authors": [
    [
     "Lisa",
     "Gustavsson"
    ],
    [
     "Francisco",
     "Lacerda"
    ]
   ],
   "title": "Vocal imitation in early language acquisition",
   "original": "i08_1976",
   "page_count": 4,
   "order": 568,
   "p1": "1976",
   "pn": "1979",
   "abstract": [
    "This paper presents a study of vocal imitation during the early stages of the language acquisition process. Utterances were extracted from recordings of adult-infant interactions in controlled but naturalistic experimental settings. For each recording session, utterances were used to create pairs of adult-infant samples that were presented to a panel of listeners, whose task was to judge whether the samples in a pair could be considered as imitations of each other or not. The results suggest an age-dependent hierarchy for the impact of different phonetic dimensions on imitation judgments and provide a basis for a quantitative model of vocal imitation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-520"
  },
  "rasanen08_interspeech": {
   "authors": [
    [
     "Okko",
     "Rasanen"
    ],
    [
     "Unto K.",
     "Laine"
    ],
    [
     "Toomas",
     "Altosaar"
    ]
   ],
   "title": "Computational language acquisition by statistical bottom-up processing",
   "original": "i08_1980",
   "page_count": 4,
   "order": 569,
   "p1": "1980",
   "pn": "1983",
   "abstract": [
    "Statistical learning of patterns from perceptual input is an increasingly central topic in cognitive processing including human language acquisition. We present an unsupervised computational method for statistical word learning by analysis of transitional probabilities of subsequent phone pairs. Results indicate that word differentiation is possible with this type of approach and are in line with previous behavioral findings.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-521"
  },
  "katagiri08_interspeech": {
   "authors": [
    [
     "Noriaki",
     "Katagiri"
    ],
    [
     "Goh",
     "Kawai"
    ]
   ],
   "title": "Lexical analyses of native and non-native English language instructor speech based on a six-month co-taught classroom video corpus",
   "original": "i08_1984",
   "page_count": 4,
   "order": 570,
   "p1": "1984",
   "pn": "1987",
   "abstract": [
    "We developed and analyzed a videotaped corpus of Japanese and English instructors who taught a high school English language class together over a 6-month period. Distributions of lexical items spoken by native and non-native speakers show they are (1) both concentrated within the 2000 most common words found in written language, (2) both skewed towards classroom context, but (3) unequal with regards to interrogative forms (such as what and why) and verbs (such as imperatives used in directing student behavior). These findings respectively suggest that (1) non-native instructors desiring to teach wholly in the target language should focus on common words, (2) students expect classroom context, and (3) native instructors favor content-oriented instruction that demands higher proficiency of students, while non-native instructors prefer step-wise instruction suited for learners across various levels. The last finding in particular may benefit the training of both kinds of instructors.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-522"
  },
  "masuda08_interspeech": {
   "authors": [
    [
     "Hinako",
     "Masuda"
    ],
    [
     "Takayuki",
     "Arai"
    ]
   ],
   "title": "Perception and production of consonant clusters in Japanese-English bilingual and Japanese monolingual speakers",
   "original": "i08_1988",
   "page_count": 4,
   "order": 571,
   "p1": "1988",
   "pn": "1991",
   "abstract": [
    "Previous research has revealed that Japanese native speakers are more likely to perceive an 'illusory vowel' within consonant clusters compared to French native speakers (Dupoux et al. 1998). The aim of this research is to investigate the differences of perception and production of consonant clusters in Japanese-English bilinguals and Japanese monolinguals. The stimuli of the two experiments consist of 36 pseudo-words that contain the two sequences, VCCV and VCVCV. Results of the perception and the production experiments on the two groups of participants revealed that bilinguals were more likely to achieve high scores in both experiments.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-523"
  },
  "moubayed08_interspeech": {
   "authors": [
    [
     "Samer Al",
     "Moubayed"
    ],
    [
     "Michael De",
     "Smet"
    ],
    [
     "Hugo",
     "Van hamme"
    ]
   ],
   "title": "Lip synchronization: from phone lattice to PCA eigen-projections using neural networks",
   "original": "i08_2016",
   "page_count": 4,
   "order": 572,
   "p1": "2016",
   "pn": "2019",
   "abstract": [
    "Lip synchronization is the process of generating natural lip movements from a speech signal. In this work we address the lip-sync problem using an automatic phone recognizer that generates a phone lattice carrying posterior probabilities. The acoustic feature vector contains the posterior probabilities of all the phones over a time window centered at the current time point. Hence this representation characterizes the phone recognition output including the confusion patterns caused by its limited accuracy. A 3D face model with varying texture is computed by analyzing a video recording of the speaker using a 3D morphable model. Training a neural network using 30 000 data vectors from an audiovisual recording in Dutch resulted in a very good simulation of the face on independent data sets of the same or of a different speaker.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-524"
  },
  "takahashi08b_interspeech": {
   "authors": [
    [
     "Ryoei",
     "Takahashi"
    ],
    [
     "Yasunori",
     "Ohishi"
    ],
    [
     "Norihide",
     "Kitaoka"
    ],
    [
     "Kazuya",
     "Takeda"
    ]
   ],
   "title": "Building and combining document and music spaces for music query-by-webpage system",
   "original": "i08_2020",
   "page_count": 4,
   "order": 573,
   "p1": "2020",
   "pn": "2023",
   "abstract": [
    "Building and combining document and music spaces of songs are discussed for a new music recommendation application, which uses commonly read texts such as Web log as query input. The most important application of this flexible recommendation system is its music query-by-Webpage, from which a song that appropriately matches Webpage is automatically played. The key idea of the proposed system is to train a linear transformation between document and music spaces so that query documents can be mapped onto a music space in which similarities based on acoustic characteristics is represented.\n",
    "The basic system has been trained using 2,650 pairs of song and review texts. Through experimental evaluations, we show the effectiveness of the system, which is three times better than the previous system. Web text as a training corpus and a bigram representation for the document vector are also investigated for the purpose of improving the system, and their effectiveness is also confirmed.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-525"
  },
  "wang08k_interspeech": {
   "authors": [
    [
     "Lei",
     "Wang"
    ],
    [
     "Shen",
     "Huang"
    ],
    [
     "Sheng",
     "Hu"
    ],
    [
     "Jiaen",
     "Liang"
    ],
    [
     "Bo",
     "Xu"
    ]
   ],
   "title": "Improving searching speed and accuracy of query by humming system based on three methods: feature fusion, candidates set reduction and multiple similarity measurement rescoring",
   "original": "i08_2024",
   "page_count": 4,
   "order": 574,
   "p1": "2024",
   "pn": "2027",
   "abstract": [
    "In this paper, we present three methods for improving the searching speed and accuracy for query by humming (QBH) system with large melody database. 1) At the feature level, to minimize the inevitable errors caused by a single pitch extractor, three different pitch extraction algorithms are fused together to gain more credible and robust pitch sequence. 2) To speed up the matching process, a candidate set reduction method is firstly adopted to filter out the unlikely candidates by faster but less precise methods; then a more accurate but slower strategy is executed on the survival candidate set to perform a finer match. 3) At the decision level, we utilize these scores generated during the filtering stage and fine-matching stage to fusing together to get more accurate result. The proposed system achieved mean reciprocal rank of 0.929 for the corpus used in MIREX2006 [1] while cost an average of 0.58 seconds for one query. The results reveal the advantage of our system on speed and accuracy comparing to other system participated in that contest.\n",
    "",
    "",
    "http://www.music-ir.org/mirex/2006/index.php/\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-526"
  },
  "hueber08_interspeech": {
   "authors": [
    [
     "Thomas",
     "Hueber"
    ],
    [
     "Gérard",
     "Chollet"
    ],
    [
     "Bruce",
     "Denby"
    ],
    [
     "Gérard",
     "Dreyfus"
    ],
    [
     "Maureen",
     "Stone"
    ]
   ],
   "title": "Towards a segmental vocoder driven by ultrasound and optical images of the tongue and lips",
   "original": "i08_2028",
   "page_count": 4,
   "order": 575,
   "p1": "2028",
   "pn": "2031",
   "abstract": [
    "This article presents a framework for a phonetic vocoder driven by ultrasound and optical images of the tongue and lips for a \"silent speech interface\" application. The system is built around an HMM-based visual phone recognition step which provides target phonetic sequences from a continuous visual observation stream. The phonetic target constrains the search for the optimal sequence of diphones that maximizes similarity to the input test data in visual space subject to a unit concatenation cost in the acoustic domain. The final speech waveform is generated using \"Harmonic plus Noise Model\" synthesis techniques. Experimental results are based on a one-hour continuous speech audiovisual database comprising ultrasound images of the tongue and both frontal and lateral view of the speaker's lips.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-527"
  },
  "hueber08b_interspeech": {
   "authors": [
    [
     "Thomas",
     "Hueber"
    ],
    [
     "Gérard",
     "Chollet"
    ],
    [
     "Bruce",
     "Denby"
    ],
    [
     "Gérard",
     "Dreyfus"
    ],
    [
     "Maureen",
     "Stone"
    ]
   ],
   "title": "Phone recognition from ultrasound and optical video sequences for a silent speech interface",
   "original": "i08_2032",
   "page_count": 4,
   "order": 576,
   "p1": "2032",
   "pn": "2035",
   "abstract": [
    "Latest results on continuous speech phone recognition from video observations of the tongue and lips are described in the context of an ultrasound-based silent speech interface. The study is based on a new 61-minute audiovisual database containing ultrasound sequences of the tongue as well as both frontal and lateral view of the speaker's lips. Phonetically balanced and exhibiting good diphone coverage, this database is designed both for recognition and corpus-based synthesis purposes. Acoustic waveforms are phonetically labeled, and visual sequences coded using PCA-based robust feature extraction techniques. Visual and acoustic observations of each phonetic class are modeled by continuous HMMs, allowing the performance of the visual phone recognizer to be compared to a traditional acoustic-based phone recognition experiment. The phone recognition confusion matrix is also discussed in detail.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-528"
  },
  "trmal08_interspeech": {
   "authors": [
    [
     "Jan",
     "Trmal"
    ],
    [
     "Marek",
     "Hrúz"
    ],
    [
     "Jan",
     "Zelinka"
    ],
    [
     "Pavel",
     "Campr"
    ],
    [
     "Luděk",
     "Müller"
    ],
    [
     "Luděk",
     "Müller"
    ]
   ],
   "title": "Feature space transforms for Czech sign-language recognition",
   "original": "i08_2036",
   "page_count": 4,
   "order": 577,
   "p1": "2036",
   "pn": "2039",
   "abstract": [
    "In this paper we describe a HMM-based sign language recognition (SLR) system for isolated signs. In the first part we describe the image parametrization method producing features used for recognition. Our goal was to find the best combination of a feature space dimension reduction method and an HMM structure.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-529"
  },
  "davis08_interspeech": {
   "authors": [
    [
     "Chris",
     "Davis"
    ],
    [
     "Jeesun",
     "Kim"
    ],
    [
     "Angelo",
     "Barbaro"
    ]
   ],
   "title": "Masked speech priming: no priming in dense neighbourhoods",
   "original": "i08_2040",
   "page_count": 4,
   "order": 578,
   "p1": "2040",
   "pn": "2043",
   "abstract": [
    "Priming from a masked presentation of a written stimulus has been a productive technique in exploring visual word recognition. The current study explored a method for masked speech priming recently introduced in the literature, in which a compressed spoken prime is embedded in masking stimuli and presented immediately prior to a target. This procedure was examined by first investigating the degree to which spoken stimuli could be compressed without significant data loss. Second, using this compression level, repetition and form priming was measured for the target words with High versus Low phonological neighbourhoods. The results indicated that robust masked speech priming occurred only for word targets that had few phonological neighbours. Third, the audibility of the masked prime was assessed and results indicated that participants were unable to reliability determine prime lexical status.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-530"
  },
  "ali08b_interspeech": {
   "authors": [
    [
     "Azra N.",
     "Ali"
    ]
   ],
   "title": "Integration of audiovisual speech and priming effects",
   "original": "i08_2044",
   "page_count": 4,
   "order": 579,
   "p1": "2044",
   "pn": "2047",
   "abstract": [
    "Humans report fusion /da/ when presented with audio /ba/ aligned with visual /ga/. Over the last three decades most researchers have neglected the velar McGurk fusion: audio /ba/ with visual /da/ eliciting /ga/ fusion, and audio /pa/ with visual /ta/ resulting in /ka/ fusion. Cathiard [1], claimed that these latter types of fusion are laboratory curiosities which do not occur embedded in French VCV syllables. We conduct two experiments; in the first experiment, incongruent segment is embedded in real English words and in the second experiment we use a priming approach to bias the perception towards either the audio channel or the visual or the expected fusion. Results show that velar fusion perceptions are not just a product of isolated nonsense syllables, but are robust percepts formed by integration of audio and visual channels. Thus, using both types of McGurk fusion has potential to be used as a probing tool for exploring the phonological organization of lexical entities. Referenc Cathiard, M., Schwartz, J-L. and Abry, C. Asking a Naive Question t the McGurk Effect: Why does Audio [B] give more [D] Percepts with Visual [G] than with Visual [D]? Auditory-Visual Speech Processing Proc. 138-142, 2001 (ISCA Archive, http://www.isca-speech.org/archive)\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-531"
  },
  "zevin08_interspeech": {
   "authors": [
    [
     "Jason D.",
     "Zevin"
    ],
    [
     "Thomas A.",
     "Farmer"
    ]
   ],
   "title": "Similarity between vowels influences response execution in word identification",
   "original": "i08_2048",
   "page_count": 4,
   "order": 580,
   "p1": "2048",
   "pn": "2051",
   "abstract": [
    "Speech categories vary in their similarity to one another, with consequences for phenomena ranging from patterns of confusability under noisy listening conditions to phonological processes described as operating on relatively abstract representations. Studies of inter-category similarity are typically conducted by studying error rates in noise, or by using metalinguistic similarity judgments. Here, we exploited the continuous and non-ballistic properties of computer mouse movements to investigate, in a fine-grained manner, the inter-category similarity of speech sounds. Participants listened to recordings of naturally produced words (\"pin,\" \"pen,\" and \"pan\") and used a computer mouse to select the matching stimulus from an array of two pictures. The same participants also performed a dissimilarity judgment task. Both tasks revealed evidence for graded effects of inter-category similarity, albeit in the context of strongly categorical classification. Multidimensional scaling of mean dissimilarity judgments revealed a very tight distribution of stimuli from the same category in perceptual space and, at a larger scale, differences between pairs of categories in their similarity to one another. Although accuracy in the word identification task was nearly perfect, arm-movement dynamics revealed attraction toward the alternative response, scaled in magnitude to the perceived similarity of the two categories.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-532"
  },
  "lentz08_interspeech": {
   "authors": [
    [
     "Tom",
     "Lentz"
    ]
   ],
   "title": "Phonotactically well-formed onset clusters as processing units in word recognition",
   "original": "i08_2052",
   "page_count": 4,
   "order": 581,
   "p1": "2052",
   "pn": "2055",
   "abstract": [
    "Phonotactic well-formedness has an effect on speech processing. This is likely due to an independent sub-lexical representation of phonotactics. Researching that knowledge requires isolating it from indirect effects. A prominent indirect effect comes via lexical neighbourhood. The better phonotactically a word is, the more neighbours it has, the harder it is to recognise it. The present study examined the sublexical effect for phonotactically good word onsets with auditory priming. Word recognition was facilitated for good clusters, in spite of the larger number of lexical competitors. Word recognition latency is corrected for the effect of lexical neighbourhood, additional effects have their origin in the processing differences of the auditory primes. We found that words with good phonotactic onsets are recognised quicker, but that (destructive) manipulation of the prime onset destroys the benefit of good onsets, and can even revert it.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-533"
  },
  "cutler08_interspeech": {
   "authors": [
    [
     "Anne",
     "Cutler"
    ],
    [
     "James M.",
     "McQueen"
    ],
    [
     "Sally",
     "Butterfield"
    ],
    [
     "Dennis",
     "Norris"
    ]
   ],
   "title": "Prelexically-driven perceptual retuning of phoneme boundaries",
   "original": "i08_2056",
   "page_count": 1,
   "order": 582,
   "p1": "2056",
   "pn": "",
   "abstract": [
    "Listeners heard an ambiguous /f-s/ in nonword contexts where only one of /f/ or /s/ was legal (e.g., frul/*srul or *fnud/snud). In later categorisation of a phonetic continuum from /f/ to /s/, their category boundaries had shifted; hearing -rul led to expanded /f/ categories, -nud expanded /s/. Thus phonotactic sequence information alone induces perceptual retuning of phoneme category boundaries; lexical access is not required.\n",
    ""
   ]
  },
  "cvejic08_interspeech": {
   "authors": [
    [
     "Erin",
     "Cvejic"
    ],
    [
     "Jeesun",
     "Kim"
    ],
    [
     "Chris",
     "Davis"
    ]
   ],
   "title": "Visual speech modifies the phoneme restoration effect",
   "original": "i08_2057",
   "page_count": 1,
   "order": 583,
   "p1": "2057",
   "pn": "",
   "abstract": [
    "The current study examined how seeing the talker (having visual speech information) affected phoneme restoration. For this purpose, six phonemes (/b/, /p/, /d/, /t/, /g/, /k/) in word contexts were used as target sounds in a 2IFC task in which participants were required to choose the white noise filled interval in which a target phoneme was present. The results showed that choosing the interval in which the phoneme was present was more difficult (i.e., more errors) in the auditory-visual than in the auditory-only presentation condition. The results were discussed in terms of two potential effects of visual speech, i.e., eliciting an illusion of speech or as a distracter to task performance.\n",
    ""
   ]
  },
  "cao08_interspeech": {
   "authors": [
    [
     "Chuan",
     "Cao"
    ],
    [
     "Ming",
     "Li"
    ],
    [
     "Jian",
     "Liu"
    ],
    [
     "Yonghong",
     "Yan"
    ]
   ],
   "title": "An objective singing evaluation approach by relating acoustic measurements to perceptual ratings",
   "original": "i08_2058",
   "page_count": 4,
   "order": 584,
   "p1": "2058",
   "pn": "2061",
   "abstract": [
    "This paper presents an objective singing quality evaluation approach based on a study of the relationship between acoustic measurements and perceptual ratings of singing voice quality. Individual perceptual criteria's contributions to the overall rating are also investigated to find significant evaluation terms. Experimental results show that critical perceptual criteria for singing evaluation have strong correlation with objective acoustic parameters, especially with combined acoustic features. And the proposed objective evaluation approach is tested to be comparable to human judges.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-534"
  },
  "gautierturbin08_interspeech": {
   "authors": [
    [
     "Valerie",
     "Gautier-Turbin"
    ],
    [
     "Laetitia",
     "Gros"
    ]
   ],
   "title": "On the perceived quality of noise reduced signals",
   "original": "i08_2062",
   "page_count": 4,
   "order": 585,
   "p1": "2062",
   "pn": "2065",
   "abstract": [
    "The subjective quality ratings of noisy speech enhanced by noise reduction algorithms should be obtained according to ITU-T Recommendation P.835, which is designed to evaluate the quality along three dimensions: signal distortion, annoyance due to background noise, and overall quality. In this paper, we investigate the impact of the scale used for the assessment of the background noise dimension and show that a degradation scale and a P.835 scale yield non negligible different results. The background noise ratings as well as the overall quality ratings are impacted, the degradation scale leading to more spread and discriminating results.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-535"
  },
  "murthy08_interspeech": {
   "authors": [
    [
     "Uma",
     "Murthy"
    ],
    [
     "John F.",
     "Pitrelli"
    ],
    [
     "Ganesh",
     "Ramaswamy"
    ],
    [
     "Martin",
     "Franz"
    ],
    [
     "Burn L.",
     "Lewis"
    ]
   ],
   "title": "A methodology and tool suite for evaluation of accuracy of interoperating statistical natural language processing engines",
   "original": "i08_2066",
   "page_count": 4,
   "order": 586,
   "p1": "2066",
   "pn": "2069",
   "abstract": [
    "Evaluation of accuracy of natural language processing (NLP) engines plays an important role in their development and improvement. Such evaluation usually takes place at a per-engine level. For example, there are evaluation methods for engines such as speech recognition, machine translation, story boundary detection, etc. Many real-world applications require combinations of these functions. This has become possible now with NLP engines attaining sufficient accuracy to be able to combine them for complex tasks. However, it is not evident how the accuracy of output of such aggregates of engines will be evaluated. We present an evaluation methodology to address this problem. The key contribution of our work is an extensible methodology that narrows down possible combinations of machine outputs and ground truths to be compared at various stages in an aggregate of interoperating engines. We also describe two example evaluation modules that we developed following this methodology.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-536"
  },
  "park08b_interspeech": {
   "authors": [
    [
     "Youngja",
     "Park"
    ],
    [
     "Siddharth",
     "Patwardhan"
    ],
    [
     "Karthik",
     "Visweswariah"
    ],
    [
     "Stephen C.",
     "Gates"
    ]
   ],
   "title": "An empirical analysis of word error rate and keyword error rate",
   "original": "i08_2070",
   "page_count": 4,
   "order": 587,
   "p1": "2070",
   "pn": "2073",
   "abstract": [
    "This paper studies the relationship between word error rate (WER) and keyword error rate (KER) in speech transcripts and their effect on the performance of speech analytics applications. Automatic speech recognition (ASR) systems are increasingly used as input for speech analytics, which raises the question of whether WER or KER is the more suitable performance metric for calibrating the ASR system. ASR systems are typically evaluated in terms of WER. Many speech analytics applications, however, rely on identifying keywords in the transcripts - thus their performance can be expected to be more sensitive to keyword errors than regular word errors.\n",
    "To study this question, we conduct a case study using an experimental data set comprising 100 calls to a contact center. We first automatically extract domain-specific words from the manual transcription and use this set of words to calculate keyword error rates in the following experiments. We then generate call transcripts with the IBM Attila speech recognition system, using different training for each repetition to generate transcripts with a range of word error rates. The transcripts are then processed with two speech analytics applications, call section segmentation and topic categorization. The results show similar WER and KER in high-accuracy transcripts, but KER increases more rapidly than WER as the accuracy of the transcription deteriorates. Neither speech analytics application showed significant sensitivity to the increase in KER for low-accuracy transcripts. Thus this case study did not identify a significant difference between using WER and KER.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-537"
  },
  "durin08_interspeech": {
   "authors": [
    [
     "Virginie",
     "Durin"
    ],
    [
     "Laetitia",
     "Gros"
    ]
   ],
   "title": "Measuring speech quality impact on tasks performance",
   "original": "i08_2074",
   "page_count": 4,
   "order": 588,
   "p1": "2074",
   "pn": "2077",
   "abstract": [
    "This paper deals with perceptual test methodologies to assess speech quality of telecommunication systems. Faced with the tremendous evolution in the telecommunications industry and with drawbacks of typical methodologies recommended by ITU-T, a new way to assess speech quality is investigated. It consists in measuring performance (i.e. reaction times and error rates) when subjects are performing two overlapped tasks involving degraded speech signals. This paper presents three tests based on theses two tasks. These tests are different in their protocols. Since results depends on the considered protocol, they are compared to define the most sensitive protocol to quality effect on performance.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-538"
  },
  "soronen08_interspeech": {
   "authors": [
    [
     "Hannu",
     "Soronen"
    ],
    [
     "Markku",
     "Turunen"
    ],
    [
     "Jaakko",
     "Hakulinen"
    ]
   ],
   "title": "Voice commands in home environment - a consumer survey",
   "original": "i08_2078",
   "page_count": 4,
   "order": 589,
   "p1": "2078",
   "pn": "2081",
   "abstract": [
    "We studied with telephone survey the opinions and ideas of 1004 Finns concerning domestic technology and controlling it with voice commands. There is distrust towards commanding your home environment with voice; the users doubt especially its functionality. Voice commands are regarded as unpleasant. The most positive conceived aspects of voice commands are the speed of control and the ability to free your hands for something else. Voice feedback, however, is really appreciated. Especially it is welcomed instead of alarm beeps and blinking lights. It is possible that users need first to get accustomed that a device speaks to them. Finally, our studies show major changes in user attitudes when they actually use speech applications.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-539"
  },
  "bouselmi08_interspeech": {
   "authors": [
    [
     "Ghazi",
     "Bouselmi"
    ],
    [
     "Jun",
     "Cai"
    ]
   ],
   "title": "Extended partial distance elimination and dynamic Gaussian selection for fast likelihood computation",
   "original": "i08_2082",
   "page_count": 4,
   "order": 590,
   "p1": "2082",
   "pn": "2085",
   "abstract": [
    "A new fast likelihood computation approach is proposed for HMM-based continuous speech recognition. This approach is an extension of the partial distance elimination (PDE) technique. Like PDE, the extended PDE (EPDE) approach aims at finding the most prominent Gaussian in a GMM for a given observation, and approximating the GMM's likelihood with the identified Gaussian. EPDE relies on a novel selection criterion in order to achieve greater time efficiency at the cost of slight degradation of recognition accuracy. This novel criterion has been combined with a dynamic Gaussian selection technique for greater recognition accuracy. Tests on TIMIT corpora shows a satisfying computation time saving of 7.3% at the same error level as PDE. Compared to a baseline, the methods we propose have also achieved a significant reduction in the number of computations of 71.5% at the same error level as PDE.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-540"
  },
  "driesen08_interspeech": {
   "authors": [
    [
     "Joris",
     "Driesen"
    ],
    [
     "Hugo",
     "Van hamme"
    ]
   ],
   "title": "Improving the multigram algorithm by using lattices as input",
   "original": "i08_2086",
   "page_count": 4,
   "order": 591,
   "p1": "2086",
   "pn": "2089",
   "abstract": [
    "The multigram algorithm is a statistical technique that can be used for extracting recurring patterns from a sequential input. When provided with a symbol sequence representing a speech signal, it is able to extract word-like patterns from it, despite the large amount of subsequences that can represent a single word. For this, it uses statistical information derived from the entire input. However, due to the abstraction of speech to symbols, much of the information originally present in the signal is no longer available to the algorithm.\n",
    "In this paper we propose a way of using a richer abstraction of the signal in the form of a lattice. Furthermore, a way of grounding recurring patterns to concepts in other modalities will be presented. Finally, the information learned by the algorithm using both kinds of input is tested in a recognition experiment. This will show that the use of lattices leads to a significant improvement in terms of recognition rate.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-541"
  },
  "tang08_interspeech": {
   "authors": [
    [
     "Min",
     "Tang"
    ],
    [
     "Philippe Di",
     "Cristo"
    ]
   ],
   "title": "Backward Viterbi beam search for utilizing dynamic task complexity information",
   "original": "i08_2090",
   "page_count": 4,
   "order": 592,
   "p1": "2090",
   "pn": "2093",
   "abstract": [
    "The backward Viterbi beam search has not received enough attentions other than being used in the second pass. The reason is that the speech recognition society has long ignored the concept of dynamic complexities of a speech recognition task which can help us to determine whether we should operate Viterbi decoding in forward or backward direction. We use the U.S. street address entry task as one example to show why the analysis of the dynamic task complexity information is important. We then describe the procedure to implement a backward Viterbi decoding system from an existing traditional ASR engine. We evaluated the backward Viterbi search using CMU's PocketSphinx. The experimental results of the backward Viterbi Search show significant performance improvement over the forward Viterbi decoding system on the U.S. street address entry task.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-542"
  },
  "bertoldi08_interspeech": {
   "authors": [
    [
     "Nicola",
     "Bertoldi"
    ],
    [
     "Marcello",
     "Federico"
    ],
    [
     "Daniele",
     "Falavigna"
    ],
    [
     "Matteo",
     "Gerosa"
    ]
   ],
   "title": "Fast speech decoding through phone confusion networks",
   "original": "i08_2094",
   "page_count": 4,
   "order": 593,
   "p1": "2094",
   "pn": "2097",
   "abstract": [
    "We present a two stage automatic speech recognition architecture suited for applications, such as spoken document retrieval, where large scale language models can be used and very low out-ofvocabulary rates need to be reached. The proposed system couples a weakly constrained phone-recognizer with a phone-to-word decoder that was originally developed for phrase-based statistical machine translation. The decoder permits to efficiently decode confusion networks in input, and to exploit large scale unpruned language models. Preliminary experiments are reported on the transcription of speeches of the Italian parliament. The use of phone confusion networks as interface between the two decoding steps permits to reduce the WER by 28%, thus making the system perform relatively close to a state-of-the-art baseline using a comparable language model.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-543"
  },
  "gu08_interspeech": {
   "authors": [
    [
     "Liang",
     "Gu"
    ],
    [
     "Jian",
     "Xue"
    ],
    [
     "Xiaodong",
     "Cui"
    ],
    [
     "Yuqing",
     "Gao"
    ]
   ],
   "title": "High-performance low-latency speech recognition via multi-layered feature streaming and fast Gaussian computation",
   "original": "i08_2098",
   "page_count": 4,
   "order": 594,
   "p1": "2098",
   "pn": "2101",
   "abstract": [
    "Highly accurate speech recognition with very low latency is a big challenge but also an important requirement for modern real-time speech recognition applications such as speech-to-speech translation. We attack this problem by proposing a highly effective and efficient streaming mode decoding scheme. A novel multi-layered feature streaming method is introduced to minimize truncation errors during streaming by optimizing look-ahead parameters. A set of speed-up algorithms are further proposed to speed up both Gaussian computation and graph search. Experiments show dramatic reduction in decoding latency using the proposed decoding scheme, with high recognition accuracy similar to utterance based decoding.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-544"
  },
  "bourke08_interspeech": {
   "authors": [
    [
     "Patrick J.",
     "Bourke"
    ],
    [
     "Rob A.",
     "Rutenbar"
    ]
   ],
   "title": "A low-power hardware search architecture for speech recognition",
   "original": "i08_2102",
   "page_count": 4,
   "order": 595,
   "p1": "2102",
   "pn": "2105",
   "abstract": [
    "High-performance speech recognition is extremely computationally expensive, limiting its use in the mobile domain. We therefore propose a low-power hardware speech recognition architecture for mobile applications, exploiting the orders-of-magnitude efficiency improvements dedicated hardware can offer. Our system is based on the Sphinx 3.0 software recognizer developed at Carnegie Mellon University, capable of large-vocabulary, speaker-independent, continuous, real-time speech recognition. We show through cycle-accurate simulation that our hardware, targeting the backend search stage of recognition, is capable of recognizing speech from a 5,000 word vocabulary 1.3 times faster than real-time, within an approximately 200mW power budget.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-545"
  },
  "mamou08_interspeech": {
   "authors": [
    [
     "Jonathan",
     "Mamou"
    ],
    [
     "Bhuvana",
     "Ramabhadran"
    ]
   ],
   "title": "Phonetic query expansion for spoken document retrieval",
   "original": "i08_2106",
   "page_count": 4,
   "order": 596,
   "p1": "2106",
   "pn": "2109",
   "abstract": [
    "We are interested in retrieving information from speech data using phonetic search. We show improvement by expanding the query phonetically using a joint maximum entropy N-gram model. The value of this approach is demonstrated on Broadcast News data from NIST 2006 Spoken Term Detection evaluation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-546"
  },
  "oonishi08_interspeech": {
   "authors": [
    [
     "Tasuku",
     "Oonishi"
    ],
    [
     "Paul R.",
     "Dixon"
    ],
    [
     "Koji",
     "Iwano"
    ],
    [
     "Sadaoki",
     "Furui"
    ]
   ],
   "title": "Implementation and evaluation of fast on-the-fly WFST composition algorithms",
   "original": "i08_2110",
   "page_count": 4,
   "order": 597,
   "p1": "2110",
   "pn": "2113",
   "abstract": [
    "When using Weighted Finite State Transducers (WFSTs) in speech recognition, on-the-fly composition approaches have been proposed as a method of reducing memory consumption and increasing flexibility during decoding. We have recently implemented several fast on-the-fly techniques, namely avoiding dead-end states, dynamic pushing and state sharing in our decoding engine. The goal of this paper is to provide a unified study of how the different on-the-fly techniques and online composition combinations effect speech recognition performance. The evaluations were performed on a large spontaneous speech recognition task and the results show that when using on-the-fly composition with a fully dynamically composed language model component the performance degrades substantially even when avoiding dead-end states. We then show in these cases the recognition performance can be dramatically improved with the addition of dynamic pushing and state sharing.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-547"
  },
  "takeda08_interspeech": {
   "authors": [
    [
     "Shoichi",
     "Takeda"
    ],
    [
     "Yuuri",
     "Yasuda"
    ],
    [
     "Risako",
     "Isobe"
    ],
    [
     "Shogo",
     "Kiryu"
    ],
    [
     "Makiko",
     "Tsuru"
    ]
   ],
   "title": "Analysis of voice-quality features of speech that expresses 'anger', 'joy', and 'sadness' uttered by radio actors and actresses",
   "original": "i08_2114",
   "page_count": 4,
   "order": 598,
   "p1": "2114",
   "pn": "2117",
   "abstract": [
    "This paper describes the analysis of the voice-quality features of \"anger\", \"joy\", and \"sadness\" depending on the degree of the emotion for expressions in Japanese speech. The degrees of emotion were \"neutral\", \"light\", \"medium\" and \"strong\". Among voice-quality features, we turned to the noise level of the glottalflow waveform. We adopted the AR model and measured the noise levels of the predictive residual signal of speech that expressed each emotion. To measure a relative noise level to the signal level, the \"noise-to-signal (N/S) ratio\" was introduced. The analysis results showed that the relative noise levels in the residual-waveform spectra were different, i.e., the N/S ratio of each emotion was larger in the order of \"anger\" > \"sadness\". \"neutral\" > \"joy\" by approximately 4 dB.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-548"
  },
  "badino08_interspeech": {
   "authors": [
    [
     "Leonardo",
     "Badino"
    ],
    [
     "Robert A. J.",
     "Clark"
    ],
    [
     "Volker",
     "Strom"
    ]
   ],
   "title": "Including pitch accent optionality in unit selection text-to-speech synthesis",
   "original": "i08_2118",
   "page_count": 4,
   "order": 599,
   "p1": "2118",
   "pn": "2121",
   "abstract": [
    "A significant variability in pitch accent placement is found when comparing the patterns of prosodic prominence realized by different English speakers reading the same sentences. In this paper we describe a simple approach to incorporate this variability to synthesize prosodic prominence in unit selection text-to-speech synthesis.\n",
    "The main motivation of our approach is that by taking into account the variability of accent placements we enlarge the set of prosodically acceptable speech units, thus increasing the chances of selecting a good quality sequence of units, both in prosodic and segmental terms.\n",
    "Results on a large scale perceptual test show the benefits of our approach and indicate directions for further improvements.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-549"
  },
  "inanoglu08_interspeech": {
   "authors": [
    [
     "Zeynep",
     "Inanoglu"
    ],
    [
     "Steve",
     "Young"
    ]
   ],
   "title": "Emotion conversion using F0 segment selection",
   "original": "i08_2122",
   "page_count": 4,
   "order": 600,
   "p1": "2122",
   "pn": "2125",
   "abstract": [
    "This paper describes F0 segment selection, a novel syllable-based F0 conversion method, which provides a concatenative framework to search for F0 segments in a modest corpus of emotional speech (.15 minutes of data). The method is compared with our earlier work on F0 generation using context-sensitive syllable HMMs. Both methods are complemented with a duration conversion module as well as GMM-based spectral conversion to form a unified emotion conversion framework in English. The system was evaluated using three target styles: surprise, anger and sadness. The results of an extensive perceptual test show that segment selection significantly outperforms the HMM-based method in terms of both emotion recognition rates and intonation quality ratings for surprise and anger. For conveying sadness both methods were effective.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-550"
  },
  "qian08_interspeech": {
   "authors": [
    [
     "Yao",
     "Qian"
    ],
    [
     "Hui",
     "Liang"
    ],
    [
     "Frank K.",
     "Soong"
    ]
   ],
   "title": "Generating natural F0 trajectory with additive trees",
   "original": "i08_2126",
   "page_count": 4,
   "order": 601,
   "p1": "2126",
   "pn": "2129",
   "abstract": [
    "In HMM-based TTS, while the segmental quality of synthesized speech is quite acceptable, intonation, especially at the sentence level, tends to be somewhat bland. The maximum likelihood (ML) criterion used in HMM training and parameter trajectory generation is partially responsible for the blandness. Additionally, the F0 trajectory thus generated has a smaller dynamic range than that of natural speech, and the synthesized speech does not sound lively. We propose to use multiple additive regression trees, a gradient-based, tree-boosting algorithm, for producing a more natural F0 trajectory. Multiple additive trees are trained in successive stages to minimize the error squares between natural and predicted F0 values. Additive tree modeling is integrated with MSD-HMM, which is an ideal model for characterizing the partially continuous (voiced/unvoiced) F0 contour. Experimental results in both Mandarin and English TTS trials show that the proposed approach can increase not only the dynamic range of generated F0 trajectory, but improve other objective (RMSE, correlation coefficient, voiced/unvoiced swapping errors) and subjective quality measures.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-551"
  },
  "boidin08_interspeech": {
   "authors": [
    [
     "Cédric",
     "Boidin"
    ],
    [
     "Olivier",
     "Boeffard"
    ]
   ],
   "title": "Generating intonation from a mixed CART-HMM model for speech synthesis",
   "original": "i08_2130",
   "page_count": 4,
   "order": 602,
   "p1": "2130",
   "pn": "2133",
   "abstract": [
    "This paper proposes two algorithms for generating intonation from a mixed CART-HMM intonation model for speech synthesis. Based either on a Viterbi search or on the Expectation-Maximization algorithm, the two generation algorithms are analyzed in terms of likelihood and F0 Root Mean Square Error. Listening tests are performed to subjectively evaluate the quality of the generated intonation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-552"
  },
  "aguero08_interspeech": {
   "authors": [
    [
     "Pablo Daniel",
     "Aguero"
    ],
    [
     "Antonio",
     "Bonafonte"
    ],
    [
     "Lu",
     "Yu"
    ],
    [
     "Juan Carlos",
     "Tulli"
    ]
   ],
   "title": "Intonation modeling of Mandarin Chinese using a superpositional approach",
   "original": "i08_2134",
   "page_count": 4,
   "order": 603,
   "p1": "2134",
   "pn": "2137",
   "abstract": [
    "The intonation model is an important component in text-to-speech systems to obtain natural and expressive speech synthesis. In this paper we propose a superpositional model for Mandarin Chinese. The intonation model is composed of the syllable and the phrase component. The parameters of the model are estimated using JEMA, a training approach with many advantages related to robustness and precision. Parameter estimation and model training are combined into a loop to progressively refine both the parameterization and the model. The high correlation (0.82) between synthetic and original contours in the test data show the suitability of this approach for modeling Mandarin. Furthermore, the high scores got in subjective evaluation (MOS=4.06) confirm the objective results.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-553"
  },
  "tang08b_interspeech": {
   "authors": [
    [
     "Hao",
     "Tang"
    ],
    [
     "Xi",
     "Zhou"
    ],
    [
     "Matthias",
     "Odisio"
    ],
    [
     "Mark",
     "Hasegawa-Johnson"
    ],
    [
     "Thomas S.",
     "Huang"
    ]
   ],
   "title": "Two-stage prosody prediction for emotional text-to-speech synthesis",
   "original": "i08_2138",
   "page_count": 4,
   "order": 604,
   "p1": "2138",
   "pn": "2141",
   "abstract": [
    "In this paper, we adopt a difference approach to prosody prediction for emotional text-to-speech synthesis, where the prosodic variations between emotional and neutral speech are decomposed into the global and local prosodic variations and predicted using a twostage model. The global prosodic variations are modeled by the means and standard deviations of the prosodic parameters, while the local prosodic variations are modeled by the classification and regression tree (CART) and dynamic programming. The proposed two-stage prosody prediction model has been successfully implemented as a prosodic module in a Festival-MBROLA architecture based emotional text-to-speech synthesis system, which is able to synthesize highly intelligible, natural and expressive speech.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-554"
  },
  "hu08d_interspeech": {
   "authors": [
    [
     "Yue-Ning",
     "Hu"
    ],
    [
     "Min",
     "Chu"
    ],
    [
     "Chao",
     "Huang"
    ],
    [
     "Yan-Ning",
     "Zhang"
    ]
   ],
   "title": "Prosody boundary detection through context-dependent position models",
   "original": "i08_2142",
   "page_count": 4,
   "order": 605,
   "p1": "2142",
   "pn": "2145",
   "abstract": [
    "In this paper, we propose to convert the prosody boundary detection task into a syllable position labeling task. In order to detect both prosodic word and prosodic phrase boundaries, 6 types of syllable positions are defined. For each position, contextdependent position models are trained from manually labeled data. These models are used to label syllable positions in unseen speech. Word and phrase boundaries are then easily derived from syllable position labels. The proposed approach is tested with a large scale single speaker database. The precision and recall for word boundary are 96.1% and 90.1%, respectively, and for phrase boundary are 83.7% and 80.5%, respectively. Results of a listening test shows that only 28% of word boundaries and 50% of phrase of boundaries detected automatically are critical error, implying only about 2.2% and 10% errors for word and phrase boundaries, respectively. The results are rather good, especially when it is considered that only acoustic features are used in this work.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-555"
  },
  "gao08_interspeech": {
   "authors": [
    [
     "Boyang",
     "Gao"
    ],
    [
     "Yao",
     "Qian"
    ],
    [
     "Zhizheng",
     "Wu"
    ],
    [
     "Frank K.",
     "Soong"
    ]
   ],
   "title": "Duration refinement by jointly optimizing state and longer unit likelihood",
   "original": "i08_2266",
   "page_count": 4,
   "order": 606,
   "p1": "2266",
   "pn": "2269",
   "abstract": [
    "We refine the duration model in HMM-based TTS by extending the work of Wu [1]. The model is refined by jointly maximizing the duration likelihoods of state, phone and syllable units. Both Gaussian and gamma distributions are employed. In synthesis, the state durations are generated by the same joint optimization procedure. By considering the duration of state and longer units jointly, the accumulation of errors in estimated state durations is regulated in the optimization procedure. Experiments on Mandarin and English databases show that the refined model yields more accurate duration predictions, compared with the baseline state duration model. The improvement of phone RMSEs are 2:2ms and 1:1ms or 11% and 5:6%, in English and Mandarin synthesis, respectively. The perceptual test on synthesized English and Mandarin speech further confirms that the refined duration model outperforms the baseline system.\n",
    "",
    "",
    "Y.Wu and R.Wang, \"HMM-Based Trainable Speech Synthesis for Chinese,\" Journal of Chinese Information Processing, vol. 20, pp. 75-81, 2006.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-556"
  },
  "thangthai08_interspeech": {
   "authors": [
    [
     "Ausdang",
     "Thangthai"
    ],
    [
     "Nattanun",
     "Thatphithakkul"
    ],
    [
     "Chai",
     "Wutiwiwatchai"
    ],
    [
     "Anocha",
     "Rugchatjaroen"
    ],
    [
     "Sittipong",
     "Saychum"
    ]
   ],
   "title": "T-tilt: a modified tilt model for F0 analysis and synthesis in tonal languages",
   "original": "i08_2270",
   "page_count": 4,
   "order": 607,
   "p1": "2270",
   "pn": "2273",
   "abstract": [
    "This paper proposes a modified Tilt model, called T-Tilt, for analyzing and synthesizing F0 contours in tonal languages. The Tilt model successfully designed for intonation modeling is extended to cover syllable-based F0 realization influenced strongly by the tonal context. Two modification approaches include adding a parameter indicating a F0 curve pattern and separating duration and amplitude controls inherent in the Tilt parameter for sake of flexibility. Evaluations are conducted by both an objective RMSE measure and a subjective MOS test on intelligibility and naturalness aspects. Applying to Thai and Mandarin Chinese continuous speech, the proposed model is proved to be very effective for F0 contour analysis. It rather requires extensive work on parameter synthesis although the synthesizing performance is comparable to those produced by other proposed models.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-557"
  },
  "latorre08_interspeech": {
   "authors": [
    [
     "Javier",
     "Latorre"
    ],
    [
     "Masami",
     "Akamine"
    ]
   ],
   "title": "Multilevel parametric-base F0 model for speech synthesis",
   "original": "i08_2274",
   "page_count": 4,
   "order": 608,
   "p1": "2274",
   "pn": "2277",
   "abstract": [
    "This paper proposes a new F0 model for speech synthesis based on the parameterization of the logF0 contour of the syllables. This parameterization consists of the N-order discrete cosine transform (DCT) plus some additional parameters such as the gradient of the syllable average pitch. A statistical model of the syllable pitch contour is then created by clustering the parameterized vectors with a decision tree. Similar statistical models are also created for other linguistic levels other than the syllable. For synthesis, the statistical model of each level is used to define a log-likelihood function for the input text. These functions are then weighted and added into a global log-likelihood function which is then maximized with respect to the DCT coefficients of the syllable model. The final logF0 contour is obtained from the inverse transformation of the syllable DCT coefficients. A subjective test showed a clear preference for the proposed model against our previous HMM-based baseline.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-558"
  },
  "adell08_interspeech": {
   "authors": [
    [
     "Jordi",
     "Adell"
    ],
    [
     "Antonio",
     "Bonafonte"
    ],
    [
     "David",
     "Escudero-Mancebo"
    ]
   ],
   "title": "On the generation of synthetic disfluent speech: local prosodic modifications caused by the insertion of editing terms",
   "original": "i08_2278",
   "page_count": 4,
   "order": 609,
   "p1": "2278",
   "pn": "2281",
   "abstract": [
    "Disfluent speech synthesis is necessary in some applications such as automatic film dubbing or spoken translation. This paper presents a model for the generation of synthetic disfluent speech based on inserting each element of a disfluency in a context where they can be considered fluent. Prosody obtained by the application of standard techniques on these new sentences is used for the synthesis of the disfluent sentence. In addition, local modifications are applied to segmental units adjacent to disfluency elements. Experiments evidence that duration follows this behavior, what supports the feasibility of the model.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-559"
  },
  "turk08_interspeech": {
   "authors": [
    [
     "Oytun",
     "Türk"
    ],
    [
     "Marc",
     "Schröder"
    ]
   ],
   "title": "A comparison of voice conversion methods for transforming voice quality in emotional speech synthesis",
   "original": "i08_2282",
   "page_count": 4,
   "order": 610,
   "p1": "2282",
   "pn": "2285",
   "abstract": [
    "This paper presents a comparison of methods for transforming voice quality in neutral synthetic speech to match cheerful, aggressive, and depressed expressive styles. Neutral speech is generated using the unit selection system in the MARY TTS platform and a large neutral database in German. The output is modified using voice conversion techniques to match the target expressive styles, the focus being on spectral envelope conversion for transforming the overall voice quality. Various improvements over the state-of-the-art weighted codebook mapping and GMM based voice conversion frameworks are employed resulting in three algorithms. Objective evaluation results show that all three methods result in comparable reduction in objective distance to target expressive TTS outputs whereas weighted frame mapping and GMM based transformations were perceived slightly better than the weighted codebook mapping outputs in generating the target expressive style in a listening test.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-560"
  },
  "tepperman08b_interspeech": {
   "authors": [
    [
     "Joseph",
     "Tepperman"
    ],
    [
     "Shrikanth S.",
     "Narayanan"
    ]
   ],
   "title": "Tree grammars as models of prosodic structure",
   "original": "i08_2286",
   "page_count": 4,
   "order": 611,
   "p1": "2286",
   "pn": "2289",
   "abstract": [
    "The common ToBI system of transcription assumes a sequential model of prosody. Many linguists argue for a tree structure explaining the synchronization and interaction among prosodic units. Could tree grammars, used previously in syntax-based language modeling, be used to model prosodic trees? We present a method of converting sequential transcripts into trees, and then demonstrate that modeling trees rather than sequences of prosodic tags results in lower perplexity as well as lower error rates when classifying pitch accents and boundaries on the Boston University Radio News Corpus. This finding could benefit areas like speech synthesis, speech understanding, and pronunciation evaluation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-561"
  },
  "meng08_interspeech": {
   "authors": [
    [
     "Sha",
     "Meng"
    ],
    [
     "Jian",
     "Shao"
    ],
    [
     "Roger Peng",
     "Yu"
    ],
    [
     "Jia",
     "Liu"
    ],
    [
     "Frank",
     "Seide"
    ]
   ],
   "title": "Addressing the out-of-vocabulary problem for large-scale Chinese spoken term detection",
   "original": "i08_2146",
   "page_count": 4,
   "order": 612,
   "p1": "2146",
   "pn": "2149",
   "abstract": [
    "While the Out-Of-Vocabulary (OOV) problem remains a challenge for English spoken term detection tasks, it is under-estimated for Chinese. This is because an Chinese OOV query term can still be matched as a sequence of Chinese characters, with each character itself being a word in the vocabulary. However, our experiments show that search accuracy levels differ significantly when a query is or is not in the vocabulary. In-Vocabulary (INV) queries outperform OOV queries for more than 20%. We examine this problem with a word-lattice-based spoken term detection task. We propose a two-stage method by first locating candidates by partial phonetic matching and then refining the matching score with word lattice rescoring. Experiments show that the proposed method achieves a 24.1% relative improvement for OOV queries on a large-scale Chinese spoken term detection task.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-562"
  },
  "shao08_interspeech": {
   "authors": [
    [
     "Jian",
     "Shao"
    ],
    [
     "Roger Peng",
     "Yu"
    ],
    [
     "Qingwei",
     "Zhao"
    ],
    [
     "Yonghong",
     "Yan"
    ],
    [
     "Frank",
     "Seide"
    ]
   ],
   "title": "Towards vocabulary-independent speech indexing for large-scale repositories",
   "original": "i08_2150",
   "page_count": 4,
   "order": 613,
   "p1": "2150",
   "pn": "2153",
   "abstract": [
    "The Out-Of-Vocabulary problem remains a challenge for wordlattice- based speech indexing. Sub-word-based approaches address this problem effectively for small-scale tasks, but suffer from poor precisions on large-scale databases due to lack of strong language model constraints. We propose a method for searching OOV queries with large-scale databases in two steps. First, result candidates are extracted from a sub-word-based system, ensuring a high recall. The candidates are then refined by word-lattice rescoring aiming at a high precision. Experiments on a 160-hours lecture set show that the proposed approach achieves a relative improvement of 8.7% over the sub-word-based baseline, and 19.7% for only single-word queries.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-563"
  },
  "morenodaniel08_interspeech": {
   "authors": [
    [
     "A.",
     "Moreno-Daniel"
    ],
    [
     "J.",
     "Wilpon"
    ],
    [
     "B.-H.",
     "Juang"
    ],
    [
     "S.",
     "Parthasarathy"
    ]
   ],
   "title": "Towards the integration of automatic speech recognition and information retrieval for spoken query processing",
   "original": "i08_2154",
   "page_count": 4,
   "order": 614,
   "p1": "2154",
   "pn": "2157",
   "abstract": [
    "Spoken query processing (SQP) is the task of fulfilling an information need, inferred from a spoken query, by listing a set of ranked relevant documents. The two main sources of uncertainty in SQP lay on the realization of the speech waveform and on the realization of the observed document. The proposed integration models these uncertainties under a single probabilistic framework. A case study on movie title retrieval by voice is presented to illustrate the proposed methodology. By allowing an ontology inlet, a 14% relative gain in the model convergence was achieved. An improved mean reciprocal rank and mean inclusion rate of the retrieval outcome was obtained using the proposed framework.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-564"
  },
  "turunen08b_interspeech": {
   "authors": [
    [
     "Ville T.",
     "Turunen"
    ]
   ],
   "title": "Reducing the effect of OOV query words by using morph-based spoken document retrieval",
   "original": "i08_2158",
   "page_count": 4,
   "order": 615,
   "p1": "2158",
   "pn": "2161",
   "abstract": [
    "Morph-based spoken document retrieval uses morpheme-like subword units for both language modeling and as index terms. Problems of out-of-vocabulary (OOV) words are avoided as the morph recognizer can recognize any word in speech as a sequence of subwords. The effect of previously unseen query words (i.e. words that are not in the language model training text) is analyzed for Finnish spoken document retrieval. The performance of the morph-based system is compared to a word-based approach. Language models with artificially high OOV query word rates are built and the results show that morph-based retrieval suffers significantly less from the OOV query words than word-based. Extracting alternative recognition candidates from confusion networks further improves the results, especially for morph-based retrieval.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-565"
  },
  "wu08d_interspeech": {
   "authors": [
    [
     "Meng-Sung",
     "Wu"
    ],
    [
     "Jen-Tzung",
     "Chien"
    ]
   ],
   "title": "Bayesian latent topic clustering model",
   "original": "i08_2162",
   "page_count": 4,
   "order": 616,
   "p1": "2162",
   "pn": "2165",
   "abstract": [
    "Document modeling is important for document retrieval and categorization. The probabilistic latent semantic analysis (PLSA) and latent Dirichlet allocation (LDA) are popular paradigms of document models where word/document correlations are inferred by latent topics. In PLSA and LDA, the unseen words and documents are not explicitly represented at the same time. Model generalization is constrained. This paper presents the Bayesian latent topic clustering (BLTC) model for document representation. The posterior distributions combined by Dirichlet priors and multinomial distributions are not only calculated in document level but also in word level. The modeling of unseen words and documents is tackled. An efficient variational inference method based on Gibbs sampling is presented to calculate the posterior probability of complex variables. In the experiments on TREC and Reuters-21578, the proposed BLTC performs better than PLSA and LDA in model perplexity and classification accuracy.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-566"
  },
  "akiba08_interspeech": {
   "authors": [
    [
     "Tomoyosi",
     "Akiba"
    ],
    [
     "Yusuke",
     "Yokota"
    ]
   ],
   "title": "Spoken document retrieval by translating recognition candidates into correct transcriptions",
   "original": "i08_2166",
   "page_count": 4,
   "order": 617,
   "p1": "2166",
   "pn": "2169",
   "abstract": [
    "This paper proposes an ad hoc retrieval method for spoken documents that uses a statistical translation technique. After transcribing the spoken documents by using a Large-Vocabulary Continuous Speech Recognition (LVCSR) decoder, a text-based ad hoc retrieval method can be directly applied to the transcribed documents. However, recognition errors will significantly degrade the retrieval performance. In particular, because words that are Out-Of-Vocabulary (OOV) for the recognition dictionary of the LVCSR decoder will not appear in the transcribed text, a query constructed from such words will never match any document in the target collection. To address such problems, the proposed method aims to fill the gap between the automatically transcribed text and the correctly transcribed text by using a statistical translation technique. Experimental evaluation shows that the proposed method performs better than the baseline ad hoc retrieval method using only the transcribed text, especially for retrieval tasks with relatively small target documents.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-567"
  },
  "drioli08_interspeech": {
   "authors": [
    [
     "Carlo",
     "Drioli"
    ],
    [
     "Piero",
     "Cosi"
    ]
   ],
   "title": "Audio indexing for an interactive Italian literature management system",
   "original": "i08_2170",
   "page_count": 1,
   "order": 618,
   "p1": "2170",
   "pn": "",
   "abstract": [
    "A generic audio-based multimedia indexing and retrieval framework is presented. This work is mainly finalized to the development of various digital archives and technologies focused on the creation of a Content Management System (CMS) for Italian Literature specialized on searching, reading and listening. The main general objectives, among others more specific, are: the promotion of reading and listening Italian literature, the spreading of Italian oral narrative art and the design of new interactive communication tools and interfaces.\n",
    ""
   ]
  },
  "terao08_interspeech": {
   "authors": [
    [
     "Makoto",
     "Terao"
    ],
    [
     "Takafumi",
     "Koshinaka"
    ],
    [
     "Shinichi",
     "Ando"
    ],
    [
     "Ryosuke",
     "Isotani"
    ],
    [
     "Akitoshi",
     "Okumura"
    ]
   ],
   "title": "Open-vocabulary spoken-document retrieval based on query expansion using related web documents",
   "original": "i08_2171",
   "page_count": 4,
   "order": 619,
   "p1": "2171",
   "pn": "2174",
   "abstract": [
    "This paper proposes a new method for open-vocabulary spokendocument retrieval based on query expansion using related Web documents. A large vocabulary continuous speech recognition (LVCSR) system first transcribes spoken documents into word sequences, which are then segmented into semantically cohesive units (i.e., stories) using a text segmentation technique. Given a text query word, Web documents containing the query word are first retrieved. Each retrieved Web document can be regarded as an expanded form of the original query word. Spoken documents relevant to the query word are then retrieved by searching for the stories with the LVCSR result similar to the previously obtained Web documents. Experimental results show that the proposed method is quite effective in retrieving spoken documents such as broadcast news programs with out-of-vocabulary (OOV) queries. In addition, the proposed method is also useful for ranking retrieval results with in-vocabulary (IV) queries.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-568"
  },
  "chaudhari08_interspeech": {
   "authors": [
    [
     "Upendra",
     "Chaudhari"
    ],
    [
     "Hong-Kwang Jeff",
     "Kuo"
    ],
    [
     "Brian",
     "Kingsbury"
    ]
   ],
   "title": "Discriminative graph training for ultra-fast low-footprint speech indexing",
   "original": "i08_2175",
   "page_count": 4,
   "order": 620,
   "p1": "2175",
   "pn": "2178",
   "abstract": [
    "We study low complexity models for audio search. The indexing and retrieval system consists of Automatic Speech Recognition (ASR), phone expansion, N-gram indexing and approximate match. In particular, the ASR system can vary tremendously in complexity ranging from a simple speaker-independent system to a fully speaker-adapted system. In this paper, we focus on a speakerindependent system with a small number of Gaussians. Such a system, with ASR followed by phone expansion, provides a good balance between speed and accuracy, allowing for the processing of large volumes of data and better retrieval performance than systems relying solely on phone recognition. Here we describe the use of discriminative training of a finite-state decoding graph for improving system accuracy while preserving speed of operation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-569"
  },
  "ju08_interspeech": {
   "authors": [
    [
     "Yun-Cheng",
     "Ju"
    ],
    [
     "Julian",
     "Odell"
    ]
   ],
   "title": "A language-modeling approach to inverse text normalization and data cleanup for multimodal voice search applications",
   "original": "i08_2179",
   "page_count": 4,
   "order": 621,
   "p1": "2179",
   "pn": "2182",
   "abstract": [
    "In this paper we address two related challenges in multimodal local search applications on mobile devices: first, correctly displaying the business names, and second, harvesting language model training data from an inconsistently labeled corpus. We investigate the impact of common text normalization and the quality of language model training corpus on the accuracy of displayed results. We propose a new language model framework that eliminates the need for explicit inverse text normalization. The same framework can be applied to sift through corrupted language model training data. Our new language model is 25% more accurate while 25% smaller in size.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-570"
  },
  "amaral08_interspeech": {
   "authors": [
    [
     "Rui",
     "Amaral"
    ],
    [
     "Isabel",
     "Trancoso"
    ]
   ],
   "title": "Topic segmentation and indexation in a media watch system",
   "original": "i08_2183",
   "page_count": 4,
   "order": 622,
   "p1": "2183",
   "pn": "2186",
   "abstract": [
    "The goal of this paper is the description of our current work in terms of topic segmentation and indexation and the comparison of their performance with the story boundaries and topics manually chosen by a professional media watch company. The segmentation module explores the typical structure of a broadcast news show, namely by cues provided by the audio pre-processing module, but an improved performance could be achieved by taking its contents into account. The topic indexation module was retrained for the media watch topics. The comparison showed how different criteria, together with manual labeling inconsistencies, may affect the performance.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-571"
  },
  "olsson08_interspeech": {
   "authors": [
    [
     "J. Scott",
     "Olsson"
    ]
   ],
   "title": "Vocabulary independent discriminative term frequency estimation",
   "original": "i08_2187",
   "page_count": 4,
   "order": 623,
   "p1": "2187",
   "pn": "2190",
   "abstract": [
    "We introduce a discriminative approach to vocabulary independent term frequency estimation. Using two separate corpora and recognition systems, we show that our model can perform significantly better than a previously established generative model at this task.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-572"
  },
  "lin08_interspeech": {
   "authors": [
    [
     "Hui",
     "Lin"
    ],
    [
     "Alex",
     "Stupakov"
    ],
    [
     "Jeff A.",
     "Bilmes"
    ]
   ],
   "title": "Spoken keyword spotting via multi-lattice alignment",
   "original": "i08_2191",
   "page_count": 4,
   "order": 624,
   "p1": "2191",
   "pn": "2194",
   "abstract": [
    "We propose a method for finding keywords in an audio database using a spoken query. Our method is based on performing a joint alignment between a phone lattice generated from a spoken utterance query and a second phone lattice representing a long utterance needing to be searched. We implement this joint alignment procedure in a graphical models framework. We evaluate our system on TIMIT as well as on the Switchboard conversational telephone speech (CTS) corpus. Our results show that a phone lattice representation of the spoken query achieves higher performance than using only the 1-best phone sequence representation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-573"
  },
  "iwata08_interspeech": {
   "authors": [
    [
     "Kenji",
     "Iwata"
    ],
    [
     "Koichi",
     "Shinoda"
    ],
    [
     "Sadaoki",
     "Furui"
    ]
   ],
   "title": "Robust spoken term detection using combination of phone-based and word-based recognition",
   "original": "i08_2195",
   "page_count": 4,
   "order": 625,
   "p1": "2195",
   "pn": "2198",
   "abstract": [
    "We propose a robust spoken term detection method against word recognition errors using a combination of phone-based and word-based recognition. Conventional methods based on similar frameworks are problematic because phone-based recognition produces a large number of insertion errors. In our method, different substitution penalties are assigned for phone pairs to reduce such errors. We evaluated our method using the corpus of spontaneous Japanese. When recall was fixed at 50%, precision improved to 4.4 points above detection using only word-based recognition. We also report here on the effectiveness of optimization of the combination weight for each keyword.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-574"
  },
  "dharo08_interspeech": {
   "authors": [
    [
     "Luis Fernando",
     "D'Haro"
    ],
    [
     "Ruben",
     "San-Segundo"
    ],
    [
     "Ricardo de",
     "Cordoba"
    ],
    [
     "Jan",
     "Bungeroth"
    ],
    [
     "Daniel",
     "Stein"
    ],
    [
     "Hermann",
     "Ney"
    ]
   ],
   "title": "Language model adaptation for a speech to sign language translation system using web frequencies and a MAP framework",
   "original": "i08_2199",
   "page_count": 4,
   "order": 626,
   "p1": "2199",
   "pn": "2202",
   "abstract": [
    "This paper presents a successful technique for creating a new language model (LM) that adapts the original target LM used by a machine translation (MT) system. This technique is especially useful for situations where there are very scarce resources for training the target side (Spanish Sign Language (LSE) in our case) in order to properly estimate the target LM, the Sign Language Model (SLM), used by the MT system. The technique uses information from the source language, Spanish in our task, and from the phrase-based translation matrix in order to create a new LM, estimated using web frequencies, which adapts the counts of the SLM through the Maximum A Posteriori method (MAP). The corpus consists of common used sentences spoken by an officer when assisting people in applying for, or renewing, the National Identification Document. The proposed technique allows relative reductions of 15.5% on perplexity and 2.7% on WER for translation, which are close to half the maximum performance obtainable when only the LM is optimized.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-575"
  },
  "beskow08b_interspeech": {
   "authors": [
    [
     "Jonas",
     "Beskow"
    ],
    [
     "Björn",
     "Granström"
    ],
    [
     "Peter",
     "Nordqvist"
    ],
    [
     "Samer",
     "Al Moubayed"
    ],
    [
     "Giampiero",
     "Salvi"
    ],
    [
     "Tobias",
     "Herzke"
    ],
    [
     "Arne",
     "Schulz"
    ]
   ],
   "title": "Hearing at home - communication support in home environments for hearing impaired persons",
   "original": "i08_2203",
   "page_count": 4,
   "order": 627,
   "p1": "2203",
   "pn": "2206",
   "abstract": [
    "The Hearing at Home (HaH) project focuses on the needs of hearingimpaired people in home environments. The project is researching and developing an innovative media-center solution for hearing support, with several integrated features that support perception of speech and audio, such as individual loudness amplification, noise reduction, audio classification and event detection, and the possibility to display an animated talking head providing real-time speechreading support. In this paper we provide a brief project overview and then describe some recent results related to the audio classifier and the talking head. As the talking head expects clean speech input, an audio classifier has been developed for the task of classifying audio signals as clean speech, speech in noise or other. The mean accuracy of the classifier was 82%. The talking head (based on technology from the SynFace project) has been adapted for German, and a small speech-in-noise intelligibility experiment was conducted where sentence recognition rates increased from 3% to 17% when the talking head was present.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-576"
  },
  "taft08_interspeech": {
   "authors": [
    [
     "Daniel A.",
     "Taft"
    ],
    [
     "David B.",
     "Grayden"
    ],
    [
     "Anthony N.",
     "Burkitt"
    ]
   ],
   "title": "Traveling wave based group delays for cochlear implant speech processing",
   "original": "i08_2207",
   "page_count": 1,
   "order": 628,
   "p1": "2207",
   "pn": "",
   "abstract": [
    "Cochlear implant speech processors seek to generate a neural response that mimics normal hearing. However, the cochlear phase response is generally discarded, together with other fine scale temporal aspects of sound. We sought to incorporate and compare a variety of cochlear traveling wave delays (i.e. group delays) in a clinical speech processing strategy. Traveling wave delays resulted in a significant improvement in the perception of speech in noise, a longstanding difficulty for cochlear implant patients.\n",
    ""
   ]
  },
  "smith08_interspeech": {
   "authors": [
    [
     "Damien J.",
     "Smith"
    ],
    [
     "Denis",
     "Burnham"
    ]
   ],
   "title": "Multimodal perception of Mandarin tone for cochlear implant users",
   "original": "i08_2208",
   "page_count": 1,
   "order": 629,
   "p1": "2208",
   "pn": "",
   "abstract": [
    "Due to limitations of cochlear implant (CI) technology in transmitting fundamental frequency (f0), there is poor perception of lexical tone by CI users. Here simulated cochlear implant audio is used to investigate the role of visual speech information in Mandarin tone discrimination. Results showed that visual information improved tone perception in both Mandarin and Australian English listeners, particularly in CI simulation conditions, and particularly for tone pairs where tonal distinctions are cued by non-f0 components, suggesting that tone discrimination in CI users can be trained.\n",
    ""
   ]
  },
  "nakamura08b_interspeech": {
   "authors": [
    [
     "Keigo",
     "Nakamura"
    ],
    [
     "Tomoki",
     "Toda"
    ],
    [
     "Yoshitaka",
     "Nakajima"
    ],
    [
     "Hiroshi",
     "Saruwatari"
    ],
    [
     "Kiyohiro",
     "Shikano"
    ]
   ],
   "title": "Evaluation of speaking-aid system with voice conversion for laryngectomees toward its use in practical environments",
   "original": "i08_2209",
   "page_count": 4,
   "order": 630,
   "p1": "2209",
   "pn": "2212",
   "abstract": [
    "In this paper, we evaluate our previously proposed speaking-aid system with voice conversion for laryngectomees. The proposed system employs a sound source unit generating extremely small signals to keep them from annoying other persons, and then it statistically converts articulated signals captured with a bodyattached microphone into natural speech. We have so far shown the effectiveness of the proposed system using speech data imitated by a non-laryngectomee, which have recorded in a sound proof room. In this paper, we further investigate 1) whether such small sound source signals cause the lack of auditory feedback under noisy environments and 2) whether the proposed system is effective for real laryngectomees. Experimental results demonstrate that 1) an explicit auditory feedback is useful to keep the speaker's articulation stable and 2) the voice conversion dramatically improves the naturalness of the laryngectomee's speech but it slightly degrades its intelligibility.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-577"
  },
  "mckechnie08_interspeech": {
   "authors": [
    [
     "Jacqueline",
     "McKechnie"
    ],
    [
     "Kirrie J.",
     "Ballard"
    ],
    [
     "Donald A.",
     "Robin"
    ],
    [
     "Adam",
     "Jacks"
    ],
    [
     "Sallyanne",
     "Palethorpe"
    ],
    [
     "Kristin M.",
     "Rosen"
    ]
   ],
   "title": "An acoustic typology of apraxic speech - toward reliable diagnosis",
   "original": "i08_2213",
   "page_count": 1,
   "order": 631,
   "p1": "2213",
   "pn": "",
   "abstract": [
    "Differential diagnosis of apraxia of speech (AOS) is complicated by frequently co-occurring speech and language disorders. Auditoryperceptual measures may not adequately differentiate between disorders. This study adapted an acoustic analysis protocol developed for Parkinson's disease (PD). Preliminary results are positive, showing differences between individuals with AOS, PD, and healthy controls (HC).\n",
    ""
   ]
  },
  "pouchoulin08_interspeech": {
   "authors": [
    [
     "G.",
     "Pouchoulin"
    ],
    [
     "C.",
     "Fredouille"
    ],
    [
     "Jean-François",
     "Bonastre"
    ],
    [
     "A.",
     "Ghio"
    ],
    [
     "A.",
     "Giovanni"
    ]
   ],
   "title": "Dysphonic voices and the 0-3000 hz frequency band",
   "original": "i08_2214",
   "page_count": 4,
   "order": 632,
   "p1": "2214",
   "pn": "2217",
   "abstract": [
    "Concerned with pathological voice assessment, this paper aims at characterizing dysphonia in the frequency domain for a better understanding of related phenomena while most of the studies have focused only on improving classification systems for diagnosis help purposes. Based on a first study which demonstrates that the low frequencies ([0.3000]Hz) are more relevant for dysphonia discrimination compared with higher frequencies, the authors propose in this paper to pursue by analyzing the impact of the restricted frequency band ([0.3000]Hz) on the dysphonic voice discrimination from a phonetical and perceptual point of views. A discussion around the frequency band limitation of telephone channel is also proposed.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-578"
  },
  "yin08d_interspeech": {
   "authors": [
    [
     "Shou-Chun",
     "Yin"
    ],
    [
     "Richard C.",
     "Rose"
    ],
    [
     "Oscar",
     "Saz"
    ],
    [
     "Eduardo",
     "Lleida"
    ]
   ],
   "title": "Verifying pronunciation accuracy from speakers with neuromuscular disorders",
   "original": "i08_2218",
   "page_count": 4,
   "order": 633,
   "p1": "2218",
   "pn": "2221",
   "abstract": [
    "This paper presents a study of confidence measure based techniques for detecting phoneme level mispronunciations in utterances from impaired children with neuromuscular disorders. Several different adaptation scenarios are investigated to determine the effects of mismatched speaker characteristics and mismatched task domain on the ability to verify the phoneme level pronunciations. These techniques are evaluated in the context of a speech corpus where utterances were elicited from children in interactive speech therapy sessions involving a multimodal game-like environment. Results are presented in terms of phone detection characteristics where, for example, equal error rates of as low as 16.2% were obtained for detecting instances where phonemes were deleted by impaired speakers.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-579"
  },
  "alpan08_interspeech": {
   "authors": [
    [
     "A.",
     "Alpan"
    ],
    [
     "Y.",
     "Maryn"
    ],
    [
     "F.",
     "Grenez"
    ],
    [
     "A.",
     "Kacha"
    ],
    [
     "J.",
     "Schoentgen"
    ]
   ],
   "title": "Multi-band and multi-cue analyses of disordered connected speech",
   "original": "i08_2222",
   "page_count": 4,
   "order": 634,
   "p1": "2222",
   "pn": "2225",
   "abstract": [
    "The objective is to analyze vocal dysperiodicities in connected speech produced by dysphonic speakers. The analysis involves a speech variogram-based method that enables tracking instantaneous vocal dysperiodicities. The dysperiodicity trace is summarized by means of the signal-to-dysperiodicity ratio, which has been shown to correlate strongly with the perceived degree of hoarseness of the speaker. Previously, this method has been evaluated on small corpora. In the study that is reported here the corpus has comprised 28 normophonic and 223 dysphonic speakers. This has enabled carrying out the analysis in multiple frequency bands and submitting the signal-to-dysperiodicity ratios per band to multi-variable linear regression analysis with a view to predicting the perceptual ratings of the disordered speech fragments. The analysis results are compared to the cepstral peak prominence, which is a cue that indirectly summarizes vocal dysperiodicities frame-wise via the size of the first rhamonic of the speech cepstrum. Results show that the signal-to-dysperiodicity ratios obtained for low-frequency bands up to 1500 Hz contribute most to the prediction of the perceptual scores. Also, combining the cepstral peak prominence with the low frequency-band signalto- dysperiodicity ratio increases their common correlation with perceptual scores to 0.8.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-580"
  },
  "carmichael08_interspeech": {
   "authors": [
    [
     "James",
     "Carmichael"
    ],
    [
     "Vincent",
     "Wan"
    ],
    [
     "Phil",
     "Green"
    ]
   ],
   "title": "Combining neural network and rule-based systems for dysarthria diagnosis",
   "original": "i08_2226",
   "page_count": 4,
   "order": 635,
   "p1": "2226",
   "pn": "2229",
   "abstract": [
    "This study reports on the development of a diagnostic expert system - incorporating a multilayer perceptron (MLP) - designed to identify any sub-type of dysarthria (loss of neuro-muscular control over the articulators) manifested by a patient undergoing a Frenchay Dysarthria Assessment (FDA) evaluation. If sufficient information is provided describing pathological features of the patient's speech, the rule-based classifier (RBC) can out-perform the MLP in terms of rendering a more accurate and consistent diagnosis. The combination MLP/RBC developed during this study realised an overall improvement in diagnostic accuracy of 9.3% (absolute) for a selection of dysarthric cases, representing a substantial improvement over the benchmark system which - unlike the MLP/RBC - cannot directly process acoustic data.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-581"
  },
  "darcy08b_interspeech": {
   "authors": [
    [
     "Shona",
     "D'Arcy"
    ],
    [
     "Viliam",
     "Rapcan"
    ],
    [
     "Nils",
     "Penard"
    ],
    [
     "Margaret E.",
     "Morris"
    ],
    [
     "Ian H.",
     "Robertson"
    ],
    [
     "Richard B.",
     "Reilly"
    ]
   ],
   "title": "Speech as a means of monitoring cognitive function of elderly speakers",
   "original": "i08_2230",
   "page_count": 4,
   "order": 636,
   "p1": "2230",
   "pn": "2233",
   "abstract": [
    "This study investigates the use of speech as an indicator of the onset of cognitive decline in the elderly. The analysis found features that correlate with the results of a clinical measure of cognitive function. Using a combination of temporal language features, such as pause and utterance duration, 76% classification accuracy was achieved. While no significant results were found for ASR experiments, vowel duration, on average, increased by 17% for subjects with cognitive impairment compared to those without. The results from this study introduce the concept of longitudinal studies into aging using speech as a window into cognitive function.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-582"
  },
  "matsumasa08_interspeech": {
   "authors": [
    [
     "Hironori",
     "Matsumasa"
    ],
    [
     "Tetsuya",
     "Takiguchi"
    ],
    [
     "Yasuo",
     "Ariki"
    ],
    [
     "Ichao",
     "Li"
    ],
    [
     "Toshitaka",
     "Nakabayashi"
    ]
   ],
   "title": "Integration of metamodel and acoustic model for speech recognition",
   "original": "i08_2234",
   "page_count": 4,
   "order": 637,
   "p1": "2234",
   "pn": "2237",
   "abstract": [
    "We investigated the speech recognition of a person with articulation disorders resulting from athetoid cerebral palsy. The articulation of the first speech tends to become unstable due to strain on speech-related muscles, and that causes degradation of speech recognition. Therefore, we proposed a robust feature extraction method based on PCA (Principal Component Analysis) instead of MFCC [1]. In this paper, we discuss our effort to integrate a Metamodel [2] and Acoustic model approach. Meta-model has a technique for incorporating a model of a speaker's confusion matrix into the ASR process in such a way as to increase recognition accuracy. Its effectiveness has been confirmed by word recognition experiments.\n",
    "s H. Matsumasa and T. Takiguchi and Y. Ariki and I. LI and T. Nakabayashi, \"PCA-Based Feature Extraction for Fluctuation in Speaking Style of Articulation Disorders,\" INTERSPEECH-2007, pp. 1150-1153, 2007 (ISCA Archive, http://www.isca-speech.org/archive/interspeech_2007)\n",
    "O. C. Morales and S. Cox..\"Modelling Confusion Matrices to Improve Speech Recognition Accuracy, with an Application to Dysarthric Speech,\" INTERSPEECH-2007, pp. 1565-1568, 2007. (ISCA Archive, http://www.isca-speech.org/archive/interspeech_2007)\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-583"
  },
  "fraga08_interspeech": {
   "authors": [
    [
     "Francisco J.",
     "Fraga"
    ],
    [
     "Leticia P. Costa S.",
     "Prates"
    ],
    [
     "Maria Cecilia M.",
     "Iorio"
    ],
    [
     "Maria Cecilia M.",
     "Iorio"
    ]
   ],
   "title": "Frequency compression/transposition of fricative consonants for the hearing impaired with high-frequency dead regions",
   "original": "i08_2238",
   "page_count": 4,
   "order": 638,
   "p1": "2238",
   "pn": "2241",
   "abstract": [
    "In this paper we present the first evaluation results of a carefully designed piecewise linear frequency compression curve, to improve the recognition of fricative consonants, for patients with high-frequency dead regions in the cochlea. Our original frequency compression/transposition algorithm takes into account the average short-time spectrum of the most frequent Brazilian Portuguese fricatives. A specific speech material, composed by twenty four different monosyllables spoken by eight speakers (four male and four female), was recorded for a consonant discrimination test. The algorithm was tested over ten normal hearing listeners with simulated dead regions above 1500 and 2000 Hz. One-way and two-way ANOVA results have shown a statistical significant improvement in the correct identification of fricatives in initial syllabic position.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-584"
  },
  "lee08e_interspeech": {
   "authors": [
    [
     "Sungbok",
     "Lee"
    ],
    [
     "Tsuneo",
     "Kato"
    ],
    [
     "Shrikanth S.",
     "Narayanan"
    ]
   ],
   "title": "Relation between geometry and kinematics of articulatory trajectory associated with emotional speech production",
   "original": "i08_2290",
   "page_count": 4,
   "order": 639,
   "p1": "2290",
   "pn": "2293",
   "abstract": [
    "We investigate whether articulatory movement trajectories follow the nonlinear invariant relationship between the tangential velocity and the curvature, i.e. the one-third power law. The power law holds for articulatory trajectories of phonetic rendering, although the exponent is in the range 0.35.0.42 when averaged across speakers and emotions but is relatively invariant under speaker/emotion differences. However, the velocity gain factor, a proportional constant in the power law, is sensitive to such variations. While the power law reflects some common articulatory movement characteristics for phonetic rendering across speakers and emotions, the prosodic variation/speaker idiosyncrasy are reflected in the velocity gain factor.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-585"
  },
  "carne08_interspeech": {
   "authors": [
    [
     "Michael J.",
     "Carne"
    ]
   ],
   "title": "Intrinsic consonantal F0 perturbation in 3-way VOT contrast and its implications for aspiration-conditioned tonal split: evidence from Vietnamese",
   "original": "i08_2294",
   "page_count": 4,
   "order": 640,
   "p1": "2294",
   "pn": "2297",
   "abstract": [
    "The nature of intrinsic consonantal F0 perturbations on tonal F0 in Vietnamese is investigated. Mean F0 and duration data for five unstopped tones of a female native speaker of Vietnamese, controlling for the effect of a three-way contrastive difference in VOT between alveolar stops, is presented. It is shown that [t], and [d] produce expected perturbations, but not [th], which evinces lower F0 at onset. The significance of this finding for differential tonal split conditioned by aspiration is discussed.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-586"
  },
  "fang08_interspeech": {
   "authors": [
    [
     "Qiang",
     "Fang"
    ],
    [
     "Satoru",
     "Fujita"
    ],
    [
     "Xugang",
     "Lu"
    ],
    [
     "Jianwu",
     "Dang"
    ]
   ],
   "title": "A model based investigation of activation patterns of the tongue muscles for vowel production",
   "original": "i08_2298",
   "page_count": 4,
   "order": 641,
   "p1": "2298",
   "pn": "2301",
   "abstract": [
    "Muscle activations in speech production are important for understanding speech control. To overcome the problems of previous methods, we proposed a physiological articulatory model based approach to explore the muscle activations in the production of the five sustained Japanese vowels through an optimization procedure which minimizes the morphological differences between the model simulations and MRI observations. The general findings were consistent with the observations obtained using EMG. In addition, we found that the Transversus and Verticalis actively participate in vowel production. This implies the proposed method is appropriate for estimating the muscle activation patterns during vowels production.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-587"
  },
  "garnier08_interspeech": {
   "authors": [
    [
     "Maeva",
     "Garnier"
    ],
    [
     "Joe",
     "Wolfe"
    ],
    [
     "Nathalie",
     "Henrich"
    ],
    [
     "John",
     "Smith"
    ]
   ],
   "title": "Interrelationship between vocal effort and vocal tract acoustics: a pilot study",
   "original": "i08_2302",
   "page_count": 4,
   "order": 642,
   "p1": "2302",
   "pn": "2305",
   "abstract": [
    "How and why do vocal tract resonances and articulation change when shouting? Vocal tract resonances R1 and R2, Open quotient, Fundamental frequency, voice intensity, larynx height, lip aperture and spreading were simultaneously recorded for a female native speaker of French, on 7 French vowels and for 3 different conditions of increasing vocal effort: \"normally\" (S1), when keeping a pitch constant (S2) and when keeping both pitch and articulatory position constant (S3). R1 depends not only on articulation, but also on open quotient and larynx height. Results suggest that raising R1 and hyper-articulating in loud speech may serve to tune R1 to the first or the second voice harmonic, and thus help in increasing voice intensity.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-588"
  },
  "qin08_interspeech": {
   "authors": [
    [
     "Chao",
     "Qin"
    ],
    [
     "Miguel A.",
     "Carreira-Perpinan"
    ],
    [
     "Korin",
     "Richmond"
    ],
    [
     "Alan",
     "Wrench"
    ],
    [
     "Steve",
     "Renals"
    ]
   ],
   "title": "Predicting tongue shapes from a few landmark locations",
   "original": "i08_2306",
   "page_count": 4,
   "order": 643,
   "p1": "2306",
   "pn": "2309",
   "abstract": [
    "We present a method for predicting the midsagittal tongue contour from the locations of a few landmarks (metal pellets) on the tongue surface, as used in articulatory databases such as MOCHA and the Wisconsin XRDB. Our method learns a mapping using ground-truth tongue contours derived from ultrasound data and drastically improves over spline interpolation. We also determine the optimal locations of the landmarks, and the number of landmarks required to achieve a desired prediction error: 3.4 landmarks are enough to achieve 0.3.0.2 mm error per point on the tongue.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-589"
  },
  "theobald08b_interspeech": {
   "authors": [
    [
     "Barry-John",
     "Theobald"
    ],
    [
     "Sascha",
     "Fagel"
    ],
    [
     "Gérard",
     "Bailly"
    ],
    [
     "Frédéric",
     "Elisei"
    ]
   ],
   "title": "LIPS2008: visual speech synthesis challenge",
   "original": "i08_2310",
   "page_count": 4,
   "order": 644,
   "p1": "2310",
   "pn": "2313",
   "abstract": [
    "In this paper we present an overview of LIPS2008: Visual Speech Synthesis Challenge. The aim of this challenge is to bring together researchers in the field of visual speech synthesis to firstly evaluate their systems within a common framework, and secondly to identify the needs of the wider community in terms of evaluation. In doing so we hope to better understand the differences between the various approaches and to identify the strengths/weaknesses of the competing approaches. In this paper we firstly motivate the need for the challenge, before describing the capture and preparation of the training data, the evaluation framework, and conclude with an outline of possible directions for standardising the evaluation of talking heads.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-590"
  },
  "hofer08_interspeech": {
   "authors": [
    [
     "Gregor",
     "Hofer"
    ],
    [
     "Junichi",
     "Yamagishi"
    ],
    [
     "Hiroshi",
     "Shimodaira"
    ]
   ],
   "title": "Speech-driven lip motion generation with a trajectory HMM",
   "original": "i08_2314",
   "page_count": 4,
   "order": 645,
   "p1": "2314",
   "pn": "2317",
   "abstract": [
    "Automatic speech animation remains a challenging problem that can be described as finding the optimal sequence of animation parameter configurations given some speech. In this paper we present a novel technique to automatically synthesise lip motion trajectories from a speech signal. The developed system predicts lip motion units from the speech signal and generates animation trajectories automatically employing a \"Trajectory Hidden Markov Model\". Using the MLE criterion, its parameter generation algorithm produces the optimal smooth motion trajectories that are used to drive control points on the lips directly. Additionally, experiments were carried out to find a suitable model unit that produces the most accurate results. Finally a perceptual evaluation was conducted, that showed that the developed motion units perform better than phonemes.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-591"
  },
  "bailly08_interspeech": {
   "authors": [
    [
     "Gérard",
     "Bailly"
    ],
    [
     "Oxana",
     "Govokhina"
    ],
    [
     "Gaspard",
     "Breton"
    ],
    [
     "Frédéric",
     "Elisei"
    ],
    [
     "Christophe",
     "Savariaux"
    ]
   ],
   "title": "A trainable trajectory formation model TD-HMM parameterized for the LIPS 2008 challenge",
   "original": "i08_2318",
   "page_count": 4,
   "order": 646,
   "p1": "2318",
   "pn": "2321",
   "abstract": [
    "We describe here the trainable trajectory formation model that will be used for the LIPS'2008 challenge organized at InterSpeech'2008. It predicts articulatory trajectories of a talking face from phonetic input. It basically uses HMM-based synthesis but asynchrony between acoustic and gestural boundaries - taking for example into account non audible anticipatory gestures - is handled by a phasing model that predicts the delays between the acoustic boundaries of allophones to be synthesized and the gestural boundaries of HMM triphones. The HMM triphones and the phasing model are trained simultaneously using an iterative analysis-synthesis loop. Convergence is obtained within a few iterations. Using different motion capture data, we demonstrate here that the phasing model improves significantly the prediction error and captures subtle context-dependent anticipatory phenomena.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-592"
  },
  "theobald08c_interspeech": {
   "authors": [
    [
     "Barry-John",
     "Theobald"
    ],
    [
     "Gavin",
     "Cawley"
    ],
    [
     "Andrew",
     "Bangham"
    ],
    [
     "Iain",
     "Matthews"
    ],
    [
     "Nicholas",
     "Wilkinson"
    ]
   ],
   "title": "Comparing text-driven and speech-driven visual speech synthesisers",
   "original": "i08_2322",
   "page_count": 1,
   "order": 647,
   "p1": "2322",
   "pn": "",
   "abstract": [
    "We present a comparison of a text-driven and a speech driven visual speech synthesiser. Both are trained using the same data and both use the same Active Appearance Model (AAM) to encode and re-synthesise visual speech. Objective quality, measured using correlation, suggests the performance of both approaches is close, but subjective opinion ranks the text-driven approach significantly higher.\n",
    ""
   ]
  },
  "zoric08_interspeech": {
   "authors": [
    [
     "Goranka",
     "Zoric"
    ],
    [
     "Aleksandra",
     "Cerekovic"
    ],
    [
     "Igor S.",
     "Pandzic"
    ]
   ],
   "title": "Automatic lip synchronization by speech signal analysis",
   "original": "i08_2323",
   "page_count": 1,
   "order": 648,
   "p1": "2323",
   "pn": "",
   "abstract": [
    "In this paper a system for the automatic lip synchronization of virtual 3D human based only on the speech input is described. The speech signal is classified into viseme classes using neural networks. Visual representation of phonemes, visemes, defined in MPEG-4 FA, is used for face synthesis.\n",
    ""
   ]
  },
  "fagel08_interspeech": {
   "authors": [
    [
     "Sascha",
     "Fagel"
    ]
   ],
   "title": "MASSY speaks English: adaptation and evaluation of a talking head",
   "original": "i08_2324",
   "page_count": 1,
   "order": 649,
   "p1": "2324",
   "pn": "",
   "abstract": [
    "This paper presents an extension of a talking head that was implemented to synthesize audiovisual speech utterances in German. A mapping of English to German phonemes and a subsequent refinement of the co-articulation model enables the system to generate English utterances. A modified rhyme test was carried out for evaluation in terms of intelligibility. Results show that the visualization increases the identification of isolated words from 27% to 50% when played with audio.\n",
    ""
   ]
  },
  "fagel08b_interspeech": {
   "authors": [
    [
     "Sascha",
     "Fagel"
    ],
    [
     "Frédéric",
     "Elisei"
    ],
    [
     "Gérard",
     "Bailly"
    ]
   ],
   "title": "From 3-d speaker cloning to text-to-audiovisual-speech",
   "original": "i08_2325",
   "page_count": 1,
   "order": 650,
   "p1": "2325",
   "pn": "",
   "abstract": [
    "Visible speech movements were motion captured and parameterized. Coarticulated targets were extracted from VCVs and modeled to generate arbitrary German utterances by target interpolation. The system was extended to synthesize English utterances by a mapping to German phonemes. An evaluation by means of a modified rhyme test reveals that the synthetic videos of isolated words increase the recognition scores from 27% to 47.5% when added to audio only presentation.\n",
    ""
   ]
  },
  "krnoul08_interspeech": {
   "authors": [
    [
     "Zdeněk",
     "Krňoul"
    ],
    [
     "Miloš",
     "Železný"
    ]
   ],
   "title": "A development of Czech talking head",
   "original": "i08_2326",
   "page_count": 4,
   "order": 651,
   "p1": "2326",
   "pn": "2329",
   "abstract": [
    "This paper presents a research on the Czech talking head system. It gives an overview of methods used for visual speech animation, parameterization of a human face and a tongue, necessary data sources and a synthesis method. A 3D animation model is used for a pseudo-muscular animation schema to create such animation of visual speech which is usable for a lipreading. An extension of animation schema is presented to reach more precise deformations mainly in a lip area. Furthermore, a problem of forming articulatory trajectories is formulated to solve labial coarticulation effects. It is used for the synthesis method based on a selection of articulatory targets and interpolation technique.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-593"
  },
  "liu08e_interspeech": {
   "authors": [
    [
     "Kang",
     "Liu"
    ],
    [
     "Joern",
     "Ostermann"
    ]
   ],
   "title": "Realistic facial animation system for interactive services",
   "original": "i08_2330",
   "page_count": 4,
   "order": 652,
   "p1": "2330",
   "pn": "2333",
   "abstract": [
    "This paper presents the optimization of parameters of talking head for web-based applications with a talking head, such as Newsreader and E-commerce, in which the realistic talking head initiates a conversation with users. Our talking head system includes two parts: analysis and synthesis. The audio-visual analysis part creates a face model of a recorded human subject, which is composed of a personalized 3D mask as well as a large database of mouth images and their related information. The synthesis part generates facial animation by concatenating appropriate mouth images from the database. A critical issue of the synthesis is the unit selection which selects these appropriate mouth images from the database such that they match the spoken words of the talking head. In order to achieve a realistic facial animation, the unit selection has to be optimized. Objective criteria are proposed in this paper and the Pareto optimization is used to train the unit selection. Subjective tests are carried out in our web-based evaluation system. Experimental results show that most people cannot distinguish our facial animations from real videos.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-594"
  },
  "yan08_interspeech": {
   "authors": [
    [
     "Juan",
     "Yan"
    ],
    [
     "Xiang",
     "Xie"
    ],
    [
     "Hao",
     "Hu"
    ]
   ],
   "title": "Speech-driven 3d facial animation for mobile entertainment",
   "original": "i08_2334",
   "page_count": 4,
   "order": 653,
   "p1": "2334",
   "pn": "2337",
   "abstract": [
    "This paper presents an entertainment-oriented application for mobile service, which generates customized speech-driven 3D facial animation and delivers to end-user by MMS (Multimedia Messaging Service). Some important methods of this application are discussed, including the 3D facial model based on 3 photos, the 3D facial animation driven by speech or text on-line and the video format transformer for most smart phones. The implementation shows the facial animation runs vividly and the system gets a positive feedback by subjects' evaluation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-595"
  },
  "wang08l_interspeech": {
   "authors": [
    [
     "Lijuan",
     "Wang"
    ],
    [
     "Xiaojun",
     "Qian"
    ],
    [
     "Lei",
     "Ma"
    ],
    [
     "Yao",
     "Qian"
    ],
    [
     "Yining",
     "Chen"
    ],
    [
     "Frank K.",
     "Soong"
    ]
   ],
   "title": "A real-time text to audio-visual speech synthesis system",
   "original": "i08_2338",
   "page_count": 4,
   "order": 654,
   "p1": "2338",
   "pn": "2341",
   "abstract": [
    "In addition to speech, visual information (e.g., facial expressions, head motions, and gestures) is an important part of human communication. It conveys, explicitly or implicitly, the intentions, the emotion states, and other paralinguistic information encoded in the speech chain. In this paper we present a multi-language, real-time text-to-audiovisual speech synthesis system, which automatically generates both audio and visual streams for a given text. While the audio stream is generated by our new HMM-based TTS engine, the visual stream is rendered by incorporating multiple animation channels, which control a cartoon figure parameterized in a 3D model simultaneously. The challenges in synthesizing, synchronizing, and integrating multiple-channel information sources are investigated and methods of generating natural, realistic animations are developed. The result of rendering all available or learned information is an expressive audio-visual synthesis module for user-friendly, human-machine communication applications.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-596"
  },
  "matusov08_interspeech": {
   "authors": [
    [
     "Evgeny",
     "Matusov"
    ],
    [
     "Björn",
     "Hoffmeister"
    ],
    [
     "Hermann",
     "Ney"
    ]
   ],
   "title": "Spoken language translation systems ************ ASR word lattice translation with exhaustive reordering is possible",
   "original": "i08_2342",
   "page_count": 4,
   "order": 655,
   "p1": "2342",
   "pn": "2345",
   "abstract": [
    "This paper shows how ASR word lattices can be translated even when exhaustive reordering is required for good translation quality. We propose a method for labeling lattice word hypotheses with position information derived from a confusion network (CN). This information is effectively used in the statistical phrase-based machine translation (MT) search to reduce its complexity, which makes even long-range reordering possible. The proposed method has the benefits of the CN-based MT without having its theoretical drawbacks.\n",
    "We compare our novel search with the search based on single-best recognition output and on confusion networks. We obtain significant improvements on two translation tasks over the single-best search and gain over the CN search on a task requiring heavy reordering.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-597"
  },
  "zheng08b_interspeech": {
   "authors": [
    [
     "Jing",
     "Zheng"
    ],
    [
     "Wen",
     "Wang"
    ],
    [
     "Necip Fazil",
     "Ayan"
    ]
   ],
   "title": "Development of SRI's translation systems for broadcast news and broadcast conversations",
   "original": "i08_2346",
   "page_count": 4,
   "order": 656,
   "p1": "2346",
   "pn": "2349",
   "abstract": [
    "We present our recent work on developing large-vocabulary Arabic-to-English and Chinese-to-English speech-to-text translation systems for the January 2008 Global Autonomous Language Exploitation (GALE) retest evaluation. Two audio genres were involved in the evaluation: broadcast news and broadcast conversation. Our system, following the hierarchical phrase-based translation approach, has a two-pass decoding strategy, with the first-pass integrated search generating 3000 unique n-best lists, which are then reranked by several different language models in the second pass.\n",
    "We emphasize our work on adapting the system, which was mostly trained on text data, to the speech genres, including number tokenization, punctuation compensation, and various optimization techniques. We present our results on several different tuning and testing data sets used for system development.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-598"
  },
  "sarikaya08_interspeech": {
   "authors": [
    [
     "Ruhi",
     "Sarikaya"
    ],
    [
     "Yonggang",
     "Deng"
    ],
    [
     "Mohamed",
     "Afify"
    ],
    [
     "Brian",
     "Kingsbury"
    ],
    [
     "Yuqing",
     "Gao"
    ]
   ],
   "title": "Machine translation in continuous space",
   "original": "i08_2350",
   "page_count": 4,
   "order": 657,
   "p1": "2350",
   "pn": "2353",
   "abstract": [
    "We present a different perspective on the machine translation problem that relies upon continuous-space probabilistic models for words and phrases. Within this perspective we propose a method called Tied-Mixture Machine Translation (TMMT) that uses a trainable parametric model employing Gaussian mixture probability density functions to represent word- and phrase-pairs. In the new perspective, machine translation is treated in the same way as acoustic modeling in speech recognition. This new treatment carries several potential advantages that may improve state-of-theart machine translation systems, including better generalization to unseen events; adaptation to new domains, languages, genres, and speakers via methods such as Maximum-Likelihood Linear Regression (MLLR); and improved discrimination through discriminative training methods such as Maximum Mutual Information Estimation (MMIE). Our goal in this paper, however, is to introduce the new approach and demonstrate its viability, leaving investigation of some of the potential advantages to future work. To this end, we report some preliminary experiments demonstrating the viability of the proposed method.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-599"
  },
  "lavecchia08_interspeech": {
   "authors": [
    [
     "Caroline",
     "Lavecchia"
    ],
    [
     "David",
     "Langlois"
    ],
    [
     "Kamel",
     "Smaïli"
    ]
   ],
   "title": "Discovering phrases in machine translation by simulated annealing",
   "original": "i08_2354",
   "page_count": 4,
   "order": 658,
   "p1": "2354",
   "pn": "2357",
   "abstract": [
    "In this paper, we propose a new phrase-based translation model based on inter-lingual triggers. The originality of our method is double. First we identify common source. Then we use inter-lingual triggers in order to retrieve their translations. Furthermore, we consider the way of extracting phrase translations as an optimization issue. For that we use simulated annealing algorithm to find out the best phrase translations among all those determined by inter-lingual triggers. The best phrases are those which improve the translation quality in terms of Bleu score. Tests are achieved on the proceedings of the European Parliament corpora. The training is made on a corpus containing 596K parallel sentences (French-English) and tests on a corpus of 1444 sentences. With only 8.1% of the identified source phrases occurring in the test corpus, our system overcomes the baseline model by almost 3 points.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-600"
  },
  "reddy08_interspeech": {
   "authors": [
    [
     "Aarthi",
     "Reddy"
    ],
    [
     "Richard C.",
     "Rose"
    ]
   ],
   "title": "Towards domain independence in machine aided human translation",
   "original": "i08_2358",
   "page_count": 4,
   "order": 659,
   "p1": "2358",
   "pn": "2361",
   "abstract": [
    "This paper presents an approach for integrating statistical machine translation and automatic speech recognition for machine aided human translation (MAHT). It is applied to the problem of improving ASR performance for a human translator dictating translations in a target language while reading from a source language document. The approach addresses the issues associated with task independent ASR including out of vocabulary words and mismatched language models. We show in this paper that by obtaining domain information from the document in the form of labelled named entities from the source language text the accuracy of the ASR system can be improved by 34.5%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-601"
  },
  "lane08_interspeech": {
   "authors": [
    [
     "Ian R.",
     "Lane"
    ],
    [
     "Alex",
     "Waibel"
    ]
   ],
   "title": "Class-based statistical machine translation for field maintainable speech-to-speech translation",
   "original": "i08_2362",
   "page_count": 4,
   "order": 660,
   "p1": "2362",
   "pn": "2365",
   "abstract": [
    "Current speech-to-speech translation systems lack any mechanism to handle out-of-vocabulary words that did not appear in the training data. To improve the usability of these systems we have developed a field maintainable speech-to-speech translation framework that enables users to add new words to the system while it is being used in the field. To realize such a framework, a novel class-based statistical machine translation framework is proposed, that applies class-based translation models and class n-gram language models during translation. To obtain consistent labelling of the parallel training corpora, on which these models are trained, we introduce a bilingual tagger that jointly labels both sides of the parallel corpora. On a Japanese-English evaluation system, the proposed framework significantly improved translation quality, obtaining a relative improvement in BLEU-score of 15% for both translation directions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-602"
  },
  "maskey08_interspeech": {
   "authors": [
    [
     "Sameer R.",
     "Maskey"
    ],
    [
     "Andrew",
     "Rosenberg"
    ],
    [
     "Julia",
     "Hirschberg"
    ]
   ],
   "title": "Intonational phrases for speech summarization",
   "original": "i08_2430",
   "page_count": 4,
   "order": 661,
   "p1": "2430",
   "pn": "2433",
   "abstract": [
    "Extractive speech summarization approaches select relevant segments of spoken documents and concatenate them to generate a summary. The extraction unit chosen, whether a sentence, syntactic constituent, or other segment, has a significant impact on the overall quality and fluency of the summary. Even though sentences tend to be the choice of most the extractive speech summarizers, in this paper, we present the results of an empirical study indicating that intonational phrases are better units of extraction for summarization. Our study compared four types of input segmentation: sentences, two pause-based segmentation, and intonational phrases (IP). We found that IPs are the best candidates for extractive summarization, improving over the second highest-performing approach, sentence-based summarization, by 8.2% F-measure.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-603"
  },
  "riedhammer08_interspeech": {
   "authors": [
    [
     "Korbinian",
     "Riedhammer"
    ],
    [
     "Dan",
     "Gillick"
    ],
    [
     "Benoit",
     "Favre"
    ],
    [
     "Dilek",
     "Hakkani-Tür"
    ]
   ],
   "title": "Packing the meeting summarization knapsack",
   "original": "i08_2434",
   "page_count": 4,
   "order": 662,
   "p1": "2434",
   "pn": "2437",
   "abstract": [
    "Despite considerable work in automatic meeting summarization over the last few years, comparing results remains difficult due to varied task conditions and evaluations. To address this issue, we present a method for determining the best possible extractive summary given an evaluation metric like ROUGE. Our oracle system is based on a knapsack-packing framework, and though NP-Hard, can be solved nearly optimally by a genetic algorithm. To frame new research results in a meaningful context, we suggest presenting our oracle results alongside two simple baselines. We show oracle and baseline results for a variety of evaluation scenarios that have recently appeared in this field.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-604"
  },
  "fujii08_interspeech": {
   "authors": [
    [
     "Yasuhisa",
     "Fujii"
    ],
    [
     "Kazumasa",
     "Yamamoto"
    ],
    [
     "Norihide",
     "Kitaoka"
    ],
    [
     "Seiichi",
     "Nakagawa"
    ]
   ],
   "title": "Class lecture summarization taking into account consecutiveness of important sentences",
   "original": "i08_2438",
   "page_count": 4,
   "order": 663,
   "p1": "2438",
   "pn": "2441",
   "abstract": [
    "This paper presents a novel sentence extraction framework that takes into account the consecutiveness of important sentences using a Support Vector Machine (SVM). Generally, most extractive summarizers do not take context information into account, but do take into account the redundancy over the entire summarization. However, there must exist relationships among the extracted sentences. Actually, we can observe these relationships as consecutiveness among the sentences. We deal with this consecutiveness by using dynamic and difference features to decide if a sentence needs to be extracted or not. Since important sentences tend to be extracted consecutively, we just used the decision made for the previous sentence as the dynamic feature. We used the differences between the current and previous feature values for the difference feature, since adjacent sentences in a block of important ones should have similar feature values to each other, where as, there should be a larger difference in the feature values between an important sentence and an unimportant one. We also present a way to ensure that no redundant summarization occurs. Experimental results on a Corpus of Japanese classroom Lecture Contents (CJLC) showed that the dynamic and difference features were complementary, and our proposed method outperformed traditional methods, which did not take context information into account.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-605"
  },
  "zhu08c_interspeech": {
   "authors": [
    [
     "Xiaodan",
     "Zhu"
    ],
    [
     "Xuming",
     "He"
    ],
    [
     "Cosmin",
     "Munteanu"
    ],
    [
     "Gerald",
     "Penn"
    ]
   ],
   "title": "Using latent Dirichlet allocation to incorporate domain knowledge for topic transition detection",
   "original": "i08_2443",
   "page_count": 3,
   "order": 664,
   "p1": "2443",
   "pn": "2445",
   "abstract": [
    "This paper studies automatic detection of topic transitions for recorded presentations. This can be achieved by matching slide content with presentation transcripts directly with some similarity metrics. Such literal matching, however, misses domain-specific knowledge and is sensitive to speech recognition errors. In this paper, we incorporate relevant written materials, e.g., textbooks for lectures, which convey semantic relationships, in particular domain-specific relationships, between words. To this end, we train latent Dirichlet allocation (LDA) models on these materials and measure the similarity between slides and transcripts in the acquired hidden-topic space. This similarity is then combined with literal matchings. Experiments show that the proposed approach reduces the errors in slide transition detection by 17.41% on manual transcripts and 27.37% on automatic transcripts.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-606"
  },
  "wang08m_interspeech": {
   "authors": [
    [
     "Wen",
     "Wang"
    ]
   ],
   "title": "Weakly supervised training for parsing Mandarin broadcast transcripts",
   "original": "i08_2446",
   "page_count": 4,
   "order": 665,
   "p1": "2446",
   "pn": "2449",
   "abstract": [
    "We present a systematic investigation of applying weakly supervised co-training approaches to improve parsing performance for parsing Mandarin broadcast news (BN) and broadcast conversation (BC) transcripts, by iteratively retraining two competitive Chinese parsers from a small set of treebanked data and a large set of unlabeled data. We compare co-training to self-training, and our results show that performance using co-training is significantly better than with self-training and both co-training and self-training with a small seed labeled corpus can improve parsing accuracy significantly over training on the mismatching newswire treebank. We also investigate a variety of example selection approaches for co-training and find that our proposed example selection approach based on maximizing training utility produces the best parsing accuracy. We also investigate Chinese parsing related issues including character-based parsing and word segmentation for parsing.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-607"
  },
  "plank08_interspeech": {
   "authors": [
    [
     "Barbara",
     "Plank"
    ],
    [
     "Khalil",
     "Sima'an"
    ]
   ],
   "title": "Parsing with subdomain instance weighting from raw corpora",
   "original": "i08_2540",
   "page_count": 0,
   "order": 666,
   "p1": "2540",
   "pn": "",
   "abstract": [
    "The treebanks that are used for training statistical parsers consist of hand-parsed sentences from a single source/domain like newspaper text. However, newspaper text concerns different subdomains of language use (e.g. finance, sports, politics, music), which implies that the statistics gathered by generative statistical parsers are averages over subdomain statistics. In this paper we explore a method, subdomain instance-weighting, that exploits raw subdomain corpora for introducing subdomain statistics into a state-of-the-art generative parser. We employ instance-weighting for creating an ensemble of subdomain specific versions of the parser, and explore methods for amalgamating their predictions. Our experiments show that subdomain statistics extracted from raw corpora can even improve the quality of the n-best lists of a formidable, state-of-the-art parser.\n",
    ""
   ]
  },
  "ohno08_interspeech": {
   "authors": [
    [
     "Tomohiro",
     "Ohno"
    ],
    [
     "Shigeki",
     "Matsubara"
    ],
    [
     "Hideki",
     "Kashioka"
    ],
    [
     "Yasuyoshi",
     "Inagaki"
    ]
   ],
   "title": "Dependency parsing of Japanese spoken monologue based on clause-starts detection",
   "original": "i08_2454",
   "page_count": 4,
   "order": 667,
   "p1": "2454",
   "pn": "2457",
   "abstract": [
    "A dependency parsing method based on sentence segmentation into clauses has been proposed and confirmed to be effective. In this method, dependency parsing is executed in two stages: at the clause level and the sentence level. However, since a sentence can not be segmented into complete clauses, in the past research, a unit sandwiched between two clause-end boundaries (clause boundary unit) was adopted as an approximate unit of the complete clause. There is the problem that the dependency structure of the clause boundary unit is not necessarily closed. This paper proposes a method for dependency parsing based on sentence segmentation into units which correspond to clauses and whose dependency structure is completely closed (clause fragment). Our method identifies such the unit by re-dividing a clause boundary unit at clause-start boundaries. As the results of the experiment show, we confirmed the improvement of the dependency parsing accuracy by utilizing the clause fragment as a parsing unit.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-608"
  },
  "gajjar08_interspeech": {
   "authors": [
    [
     "Mrugesh R.",
     "Gajjar"
    ],
    [
     "R.",
     "Govindarajan"
    ],
    [
     "T. V.",
     "Sreenivas"
    ]
   ],
   "title": "Online unsupervised pattern discovery in speech using parallelization",
   "original": "i08_2458",
   "page_count": 4,
   "order": 668,
   "p1": "2458",
   "pn": "2461",
   "abstract": [
    "Segmental dynamic time warping (DTW) has been demonstrated to be a useful technique for finding acoustic similarity scores between segments of two speech utterances. Due to its high computational requirements, it had to be computed in an offline manner, limiting the applications of the technique. In this paper, we present results of parallelization of this task by distributing the workload in either a static or dynamic way on an 8-processor cluster and discuss the trade-offs among different distribution schemes. We show that online unsupervised pattern discovery using segmental DTW is plausible with as low as 8 processors. This brings the task within reach of today's general purpose multi-core servers. We also show results on a 32-processor system, and discuss factors affecting scalability of our methods.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-609"
  },
  "melto08_interspeech": {
   "authors": [
    [
     "Aleksi",
     "Melto"
    ],
    [
     "Markku",
     "Turunen"
    ],
    [
     "Jaakko",
     "Hakulinen"
    ],
    [
     "Anssi",
     "Kainulainen"
    ],
    [
     "Tomi",
     "Heimonen"
    ]
   ],
   "title": "A comparison of input entry rates in a multimodal mobile application",
   "original": "i08_2462",
   "page_count": 4,
   "order": 669,
   "p1": "2462",
   "pn": "2465",
   "abstract": [
    "This paper presents results from a comparison of text and speech input methods on TravelMan, a multimodal route guidance application for mobile phones. TravelMan provides public transport information in Finland. The application includes a range of input methods, such as speech and predictive text inputs. In this paper we present results from the user evaluation focusing on entry rates of multi-tap text input, domain-specific predictive text input, and speech input. In addition to objective metrics, we present findings about user experiences related to different input methods.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-610"
  },
  "turunen08c_interspeech": {
   "authors": [
    [
     "Markku",
     "Turunen"
    ],
    [
     "Jaakko",
     "Hakulinen"
    ],
    [
     "Cameron",
     "Smith"
    ],
    [
     "Daniel",
     "Charlton"
    ],
    [
     "Li",
     "Zhang"
    ],
    [
     "Marc",
     "Cavazza"
    ]
   ],
   "title": "Physically embodied conversational agents as health and fitness companions",
   "original": "i08_2466",
   "page_count": 4,
   "order": 670,
   "p1": "2466",
   "pn": "2469",
   "abstract": [
    "We present a physical multimodal conversational Companion in the area of health and fitness. Conversational spoken dialogues using physical agents provide a potential interface for applications which are aimed at motivating and supporting users. Open source software called jNabServer, which enables spoken and multimodal interaction with Nabaztag/tag wireless rabbits, is presented together with other software architecture solutions applied in the development of the Companion. We also present how the Companion manages interaction with the combination of a dialogue manager and a cognitive model.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-611"
  },
  "metze08_interspeech": {
   "authors": [
    [
     "Florian",
     "Metze"
    ],
    [
     "Roman",
     "Englert"
    ],
    [
     "Udo",
     "Bub"
    ],
    [
     "Ingmar",
     "Kliche"
    ],
    [
     "Thomas",
     "Scheerbarth"
    ]
   ],
   "title": "User perception of multi-modal interfaces for mobile applications",
   "original": "i08_2470",
   "page_count": 4,
   "order": 671,
   "p1": "2470",
   "pn": "2473",
   "abstract": [
    "This paper presents a comparative study on the usability of a service presented in telephone, PC-based web interface, and mobile/ multi-modal variants. The goal is not to analyze individual strengths and weaknesses of the different modalities, but to understand the user's perception of the SUMI criteria (efficiency, affect/ likability, helpfulness, control, learnability), and the overall impression of a service with respect to the access variant tested. As multi-modality is often framed as a technology to make usage more \"intuitive\", we were particularly interested in the differences between experienced and novice users. To this end, we conducted a study with 80 participants and conclude that, while multi-modality is accepted by experienced users, it seems to be asking too much from novice users, particularly with respect to learnability and efficiency.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-612"
  },
  "nakano08_interspeech": {
   "authors": [
    [
     "Teppei",
     "Nakano"
    ],
    [
     "Tomoyuki",
     "Kumai"
    ],
    [
     "Tetsunori",
     "Kobayashi"
    ],
    [
     "Yasushi",
     "Ishikawa"
    ]
   ],
   "title": "Design and formulation for speech interface based on flexible shortcuts",
   "original": "i08_2474",
   "page_count": 4,
   "order": 672,
   "p1": "2474",
   "pn": "2477",
   "abstract": [
    "This paper proposes a new speech user interface for command selection called \"Flexible Shortcuts.\" With this approach, users can select any command using continuous keyword input, a voice input method that uses a series of keywords related to the command. The keywords are defined by a hierarchically structured command set, called the functional structure. Two types of interactions are designed to support user friendliness and effectiveness, namely, interaction for exploration and interaction for the resolution of ambiguity. A probabilistic formulation is also considered for this approach. An experiment is conducted to compare the proposed method with the conventional \"command and control\" interface in terms of efficiency and usability. Experimental results show that the proposed approach is superior to the conventional approach from both objective and subjective points of view.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-613"
  },
  "yin08e_interspeech": {
   "authors": [
    [
     "Bo",
     "Yin"
    ],
    [
     "Natalie",
     "Ruiz"
    ],
    [
     "Fang",
     "Chen"
    ],
    [
     "Eliathamby",
     "Ambikairajah"
    ]
   ],
   "title": "Exploring classification techniques in speech based cognitive load monitoring",
   "original": "i08_2478",
   "page_count": 4,
   "order": 673,
   "p1": "2478",
   "pn": "2481",
   "abstract": [
    "The ability to monitor cognitive load level in real time is extremely useful for preventing fatal operating errors or improving the efficiency of task execution. In top of the success of our previously proposed speech based cognitive load monitoring system, we explored alternative classification techniques in this paper, including simple linear kernel Support Vector Machine (SVM), hybrid SVM-GMM which accepts the likelihood scores from GMM as inputs for SVM, and a fusion approach which integrates GMM, SVM and SVM-GMM systems together. All systems are evaluated on the data collected from two different tasks - a reading comprehension and a Stroop test based task. SVM-GMM based system achieved the highest performance on both tasks and improved the accuracy of three cognitive load levels classification from 71.1% to 75.6% and 77.5% to 82.2%, respectively.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-614"
  },
  "okamoto08b_interspeech": {
   "authors": [
    [
     "Masayuki",
     "Okamoto"
    ],
    [
     "Naoki",
     "Iketani"
    ],
    [
     "Keisuke",
     "Nishimura"
    ],
    [
     "Masaaki",
     "Kikuchi"
    ],
    [
     "Kenta",
     "Cho"
    ],
    [
     "Masanori",
     "Hattori"
    ],
    [
     "Sougo",
     "Tsuboi"
    ]
   ],
   "title": "Finding two-level interpersonal context: proximity and conversation detection from personal audio feature data",
   "original": "i08_2482",
   "page_count": 4,
   "order": 674,
   "p1": "2482",
   "pn": "2485",
   "abstract": [
    "We propose a method to detect adhoc meeting based on crosscorrelation between audio feature data, which are collected from personal mobile terminals. This method can detect whether there is conversation between each pair of users without raw audio data. Through a two-day evaluation with eight users, we found our method could detect meeting contexts with 0.9 F-measures on average. We also introduce example applications such as a document search application in which detected meeting context is used as a file annotation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-615"
  },
  "gandhe08_interspeech": {
   "authors": [
    [
     "Sudeep",
     "Gandhe"
    ],
    [
     "David",
     "DeVault"
    ],
    [
     "Antonio",
     "Roque"
    ],
    [
     "Bilyana",
     "Martinovski"
    ],
    [
     "Ron",
     "Artstein"
    ],
    [
     "Anton",
     "Leuski"
    ],
    [
     "Jillian",
     "Gerten"
    ],
    [
     "David",
     "Traum"
    ]
   ],
   "title": "From domain specification to virtual humans: an integrated approach to authoring tactical questioning characters",
   "original": "i08_2486",
   "page_count": 4,
   "order": 675,
   "p1": "2486",
   "pn": "2489",
   "abstract": [
    "We present a new approach for rapidly developing dialogue capabilities for virtual humans. Starting from domain specification, an integrated authoring interface automatically generates dialogue acts with all possible contents. These dialogue acts are linked to example utterances in order to provide training data for natural language understanding and generation. The virtual human dialogue system contains a dialogue manager following the information-state approach, using finite-state machines and SCXML to manage local coherence, as well as explicit modeling of emotions and compliance level and a grounding component based on evidence of understanding. Using the authoring tools, we design and implement a version of the virtual human Hassan and compare to previous architectures for the character.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-616"
  },
  "rozak08_interspeech": {
   "authors": [
    [
     "Mike",
     "Rozak"
    ]
   ],
   "title": "Designing a massively multiplayer online role-playing game around text-to-speech",
   "original": "i08_2490",
   "page_count": 4,
   "order": 676,
   "p1": "2490",
   "pn": "2493",
   "abstract": [
    "CircumReality is an experimental massively multiplayer online role-playing game (MMORPG) that relies on text-to-speech for both narration and non-player character speech. A game-oriented text-to-speech engine differs significantly from a text-to-speech engine targeted at telephony or mobile devices. This paper discusses some of the differences, such memory and download-size requirements, voice-transformation requirements, voices with personality, licensing fees, and player-created voices.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-617"
  },
  "gao08b_interspeech": {
   "authors": [
    [
     "Jie",
     "Gao"
    ],
    [
     "Xiang",
     "Zhang"
    ],
    [
     "Qingwei",
     "Zhao"
    ],
    [
     "Yonghong",
     "Yan"
    ]
   ],
   "title": "Robust speaker change detection using Kernel-Gaussian model",
   "original": "i08_2494",
   "page_count": 4,
   "order": 677,
   "p1": "2494",
   "pn": "2497",
   "abstract": [
    "This paper introduces and evaluates a novel approach for unsupervised speaker change detection. In many unsupervised speaker change detection algorithms, each audio segment is typically modeled with a multivariate single Gaussian density, where it is assumed that the distribution of the speech features of the segment is Gaussian. However, this assumption is too strong in many cases. Therefore, this paper presents an alternative to the single Gaussian model: Gaussian model in reproducing kernel Hilbert space (RKHS) or Kernel-Gaussian model (KGM). KGM first projects speech features into RKHS via a nonlinear mapping. Then it models the features in RKHS with a Gaussian density. The mapping procedure enables KGM to capture nonlinear structure of speech features. An implementation of KGM is proposed and evaluated. Experiments on different datasets show that better results are achieved by KGM compared to the single Gaussian model.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-618"
  },
  "ntalampiras08_interspeech": {
   "authors": [
    [
     "Stavros",
     "Ntalampiras"
    ],
    [
     "Nikos",
     "Fakotakis"
    ]
   ],
   "title": "A comparative study in automatic recognition of broadcast audio",
   "original": "i08_2498",
   "page_count": 4,
   "order": 678,
   "p1": "2498",
   "pn": "2501",
   "abstract": [
    "This paper provides a thorough description of a methodology which leads to high accuracy as regards automatic analysis of broadcast audio. The main objective is to find a feature set for efficient speech/music discrimination while keeping the number of its dimensions as small as possible. Three groups of parameters based on Mel-scale filterbank, MPEG-7 standard and wavelet decomposition are examined in detail. We annotated on-line radio recordings characterized by great diversity, for building probabilistic models and testing four frameworks. The proposed approach utilizes wavelets and MPEG-7 ASP descriptor for modeling speech and music respectively, and results to 98.5% average recognition rate.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-619"
  },
  "tantibundhit08_interspeech": {
   "authors": [
    [
     "Charturong",
     "Tantibundhit"
    ],
    [
     "Gernot",
     "Kubin"
    ]
   ],
   "title": "Joint time-frequency segmentation for transient decomposition",
   "original": "i08_2502",
   "page_count": 4,
   "order": 679,
   "p1": "2502",
   "pn": "2505",
   "abstract": [
    "We propose a signal based joint time-frequency segmentation algorithm as an extension to Herley et al. (IEEE Trans. Image Processing, 1997). Our algorithm provides an unconstrained multiresolution analysis in time and frequency adapted to the characteristics of the signal, where the time-frequency segmentation of Herley et al. is modified to achieve a minimum entropy criterion. Experimental results on a synthetic signal, composed of a high frequency sinusoid and a single impulse, show that our algorithm outperforms several time-frequency representations such as the best basis wavelet packet, the best basis modified discrete cosine transform (MDCT), and the original Herley et al. algorithms. The application of our algorithm to the transient information in the signal provides good results and shows that the algorithm will be useful in speech decomposition problems.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-620"
  },
  "mitra08_interspeech": {
   "authors": [
    [
     "Vikramjit",
     "Mitra"
    ],
    [
     "Daniel",
     "Garcia-Romero"
    ],
    [
     "Carol Y.",
     "Espy-Wilson"
    ]
   ],
   "title": "Language and genre detection in audio content analysis",
   "original": "i08_2506",
   "page_count": 4,
   "order": 680,
   "p1": "2506",
   "pn": "2509",
   "abstract": [
    "This paper presents an audio genre detection framework that can be used for a multi-language audio corpus. Cepstral coefficients are considered and analyzed as the feature set for both a language dependent and language independent genre identification (GID) task. Language information is found to increase the overall detection accuracy on an average by at least 2.6% from its language independent counterpart. Mel-frequency cepstral coefficients have been widely used for Music Information Retrieval (MIR), however, the present study shows that Linear-frequency cepstral coefficients (LFCC) with a higher number of frequency bands can improve the detection accuracy. Two other GID architectures have also been considered, but the results show that the log-energy amplitudes from triangular linearly spaced filter banks and their deltas can offer average detection accuracy as high as 98.2%, when language information is taken into account.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-621"
  },
  "zhang08g_interspeech": {
   "authors": [
    [
     "Chi",
     "Zhang"
    ],
    [
     "John H. L.",
     "Hansen"
    ]
   ],
   "title": "An entropy based feature for whisper-island detection within audio streams",
   "original": "i08_2510",
   "page_count": 4,
   "order": 681,
   "p1": "2510",
   "pn": "2513",
   "abstract": [
    "Non-neutral speech, especially whispered speech, has strong negative impact on speech system performance. It is therefore necessary to detect whisper-islands embedded within neutral speech prior to subsequent processing steps. Detecting whisper-islands in speech audio streams can contribute to improved modeling, speech analysis, and understanding. Speech technology can also benefit by allowing for suppression/obscuring of sensitive data (names, credit card numbers, etc.) in audio archives, call centers, or for spoken document retrieval systems. This study focuses on detecting whisper-island from neutral speech within audio streams using a proposed new entropy-based feature. The new feature focused on effectively detecting vocal effort change points between whisper and neutral speech. Experimental results employing a multi-error score show that the new feature has superior performance over a previous method introduced in [2]. Overall, the detection accuracy of 97% (for male) and 96.7% (for female) indicate effective performance in whisper-island detection, and suggests a viable algorithm to assist speech and language technology when whisper is present.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-622"
  },
  "grasic08_interspeech": {
   "authors": [
    [
     "Matej",
     "Grašič"
    ],
    [
     "Marko",
     "Kos"
    ],
    [
     "Andrej",
     "Žgank"
    ],
    [
     "Zdravko",
     "Kačič"
    ]
   ],
   "title": "Two step speaker segmentation method using Bayesian information criterion and adapted Gaussian mixtures models",
   "original": "i08_2514",
   "page_count": 4,
   "order": 682,
   "p1": "2514",
   "pn": "2517",
   "abstract": [
    "This paper addresses the topic of online unsupervised speaker segmentation in a complex audio environment as it is present in the Broadcast News databases. A new two stage speaker change detection algorithm is proposed, which combines the Bayesian Information Criterion with an ABLS-SCD statistical framework where adapted Gaussian mixture models are used to achieve higher accuracy. To enhance the performance of the proposed method a sub-window dependent threshold selection strategy for the ABLS-SCD is introduced. Also an additional window selection strategy for the proposed method is presented. Experimental design and test evaluation were carried out on the Slovenian BNSI Broadcast News database.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-623"
  },
  "germesin08_interspeech": {
   "authors": [
    [
     "Sebastian",
     "Germesin"
    ],
    [
     "Tilman",
     "Becker"
    ],
    [
     "Peter",
     "Poller"
    ]
   ],
   "title": "Domain-specific classification methods for disfluency detection",
   "original": "i08_2518",
   "page_count": 4,
   "order": 683,
   "p1": "2518",
   "pn": "2521",
   "abstract": [
    "Speech disfluencies are very common in our everyday life and considerably affect NLP systems, which makes systems that can detect or even repair them highly desirable. Previous research achieved good results in the field of disfluency detection but only in subsets of the disfluency types. The aim of this study was to develop a technology that is able to cope with a broad field of disfluency types. A thorough investigation of our corpus led us to a detection design where basic rule-matching techniques are complemented with machine learning and N-gram based approaches. In this paper, we describe the different detection techniques, each specialized on its own disfluency domain and the results we gained.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-624"
  },
  "nwe08_interspeech": {
   "authors": [
    [
     "Tin Lay",
     "Nwe"
    ],
    [
     "Minghui",
     "Dong"
    ],
    [
     "Swe Zin Kalayar",
     "Khine"
    ],
    [
     "Haizhou",
     "Li"
    ]
   ],
   "title": "Multi-speaker meeting audio segmentation",
   "original": "i08_2522",
   "page_count": 4,
   "order": 684,
   "p1": "2522",
   "pn": "2525",
   "abstract": [
    "This paper presents segmentation of multi-speaker meeting audio into four different classes: local speech, crosstalk, overlapped speech and non-speech sounds. Firstly, Bayesian Information Criterion (BIC) segmentation method is used to pre-segment the meeting according to speaker changing points. Then, harmonicity information is integrated into acoustic features to differentiate speech from non-speech audio segments. We use cascaded subband filters spread in pitch and harmonic frequency scales to characterize the harmonicity information. Finally, total energy and multi-pitch tracking algorithm are used to classify speech segments into local speech, overlapped speech and crosstalk audio types. Experiments conducted on subset of ICSI meeting corpus shown promising results in classifying four audio types.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-625"
  },
  "maddage08_interspeech": {
   "authors": [
    [
     "Namunu C.",
     "Maddage"
    ],
    [
     "Haizhou",
     "Li"
    ]
   ],
   "title": "Rhythm based music segmentation and octave scale cepstral features for sung language recognition",
   "original": "i08_2526",
   "page_count": 4,
   "order": 685,
   "p1": "2526",
   "pn": "2529",
   "abstract": [
    "Sung language recognition relies on both effective feature extraction and acoustic modeling. In this paper, we study rhythm based music segmentation in which the frame size varies in proportion to inter-beat interval of the music, in contrast to fixed length segmentation (FIX) in spoken language recognition. We show that acoustic feature extracted from the BSS scheme outperforms that from FIX. We also compare the effectiveness of musically motivated acoustic features, Octave scale cepstral coefficients (OSCCs) with Log frequency cepstral coefficients. We adopt Gaussian mixture model for sung language classifier design. Experiments are conducted on a database of 400 popular songs sung in four languages, including English, Chinese, German and Indonesian, which show that OSCC feature outperforms other features. We achieve 64.9% of sung language identification accuracy with Gaussian mixture models trained on shifted-delta- cepstral OSCC acoustic features extracted via BSS.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-626"
  },
  "molla08_interspeech": {
   "authors": [
    [
     "Md. Khademul Islam",
     "Molla"
    ],
    [
     "Keikichi",
     "Hirose"
    ],
    [
     "Nobuaki",
     "Minematsu"
    ]
   ],
   "title": "Robust voiced/unvoiced speech classification using empirical mode decomposition and periodic correlation model",
   "original": "i08_2530",
   "page_count": 4,
   "order": 686,
   "p1": "2530",
   "pn": "2533",
   "abstract": [
    "This paper presents a method of voiced/unvoiced (V/Uv) classification of noisy speech signals. Empirical mode decomposition (EMD), a newly developed tool to analyze nonlinear and non-stationary signals is used to filter the additive noise with the speech signal. The normalized autocorrelation of the filtered speech signal is computed to enhance the periodicity if any. It is considered that the voiced speech signal is periodically correlated and the unvoiced signal is not. A statistical model of determining periodic correlation is used to differentiate voiced and unvoiced speech with low SNR. The experimental results show that the use of EMD improves the classification performance and the overall efficiency is noticeable as compared to other existing algorithms.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-627"
  },
  "wu08e_interspeech": {
   "authors": [
    [
     "Qiong",
     "Wu"
    ],
    [
     "Qin",
     "Yan"
    ],
    [
     "Jun",
     "Wang"
    ],
    [
     "Jun",
     "Hong"
    ]
   ],
   "title": "A combination of data mining method with decision trees building for speech/music discrimination",
   "original": "i08_2534",
   "page_count": 4,
   "order": 687,
   "p1": "2534",
   "pn": "2537",
   "abstract": [
    "Nowadays the applications in multimedia domain require that the Speech/Music classifier has many other merits in addition to the accuracy, such as short-time delay and low complexity. Here, we endeavor to form a Speech/Music classifier by using different data mining methods. The main work of this paper is to obtain such system by analyzing the inherent validity of diverse features extracted from the audio, combining them into two subsets, and building a hieratical structure of decision trees to maintain optimal performances. The classifier is evaluated by a set of 5-to-11-minutes 450 audio files of different types of speech and music, and outperforms AMR-WB+ by achieving 97.6% and 95.2% correct classification rate at the 10ms frame level in pure and high SNR (>=20dB) environment respectively. Besides, its complexity is lower than 1WMOPS which make it easily adapted to many scenarios.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-628"
  },
  "gupta08_interspeech": {
   "authors": [
    [
     "Vishwa",
     "Gupta"
    ],
    [
     "Gilles",
     "Boulianne"
    ],
    [
     "Patrick",
     "Kenny"
    ],
    [
     "Pierre",
     "Dumouchel"
    ]
   ],
   "title": "Advertisement detection in French broadcast news using acoustic repetition and Gaussian mixture models",
   "original": "i08_2538",
   "page_count": 4,
   "order": 688,
   "p1": "2538",
   "pn": "2541",
   "abstract": [
    "In this paper, we detect advertisements in French broadcast news by locating both repeated and non-repeated ads. The non-repeated ads are located by using Gaussian mixture models (GMMs) to discriminate between program and ad segments. The repeated ad detection stage first uses features generated by a symmetric KL2 metric to locate repeated 5-sec audio segments. These repeated segments are then verified and extended through a detailed matching algorithm that uses cepstral features. The proposed repeated advertisement detection algorithms detect repeated audio reliably, resulting in 33.2% advertisement detection error rate (AER). The 26.0% missed ads are due to ads not being repeated, while the 7.2% false alarms are due to short repeated segments in the program. Using GMMs to classify repeated segments as program or ad reduces the AER to 30.1%. To locate non-repeated ads in program segments, we divide the audio between these repeated ads into short segments, and classify each segment as a program or an ad using these GMMs. This reduces the AER from 30.1% to 13.0%. We improve the segment boundaries between programs and ads by Viterbi alignment. This re-alignment reduces the AER from 13.0% to 10.6% (96.7% recall and 93.0% precision). Overall, we detect 87% of the non-repeated ads.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-629"
  },
  "hazen08_interspeech": {
   "authors": [
    [
     "Timothy J.",
     "Hazen"
    ],
    [
     "Fred",
     "Richardson"
    ]
   ],
   "title": "A hybrid SVM/MCE training approach for vector space topic identification of spoken audio recordings",
   "original": "i08_2542",
   "page_count": 4,
   "order": 689,
   "p1": "2542",
   "pn": "2545",
   "abstract": [
    "The success of support vector machines (SVMs) for classification problems is often dependent on an appropriate normalization of the input feature space. This is particularly true in topic identification, where the relative contribution of the common but uninformative function words can overpower the contribution of the rare but informative content words in the SVM kernel function score if the feature space is not normalized properly. In this paper we apply the discriminative minimum classification error (MCE) training approach to the problem of learning an appropriate feature space normalization for use with an SVM classifier. Results are presented showing significant error rate reductions for an SVM-based system on a topic identification task using the Fisher corpus of audio recordings of human-human conversations.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-630"
  },
  "trancoso08_interspeech": {
   "authors": [
    [
     "Isabel",
     "Trancoso"
    ],
    [
     "Jose",
     "Portelo"
    ],
    [
     "Miguel",
     "Bugalho"
    ],
    [
     "João",
     "Neto"
    ],
    [
     "Antonio",
     "Serralheiro"
    ]
   ],
   "title": "Training audio events detectors with a sound effects corpus",
   "original": "i08_2546",
   "page_count": 4,
   "order": 690,
   "p1": "2546",
   "pn": "2549",
   "abstract": [
    "This paper describes the work done in the framework of the VIDIVIDEO European project in terms of audio event detection. Our first experiments concerned the detection of non-voice sounds, such as birds, machines, traffic, water and steps. Given the unavailability of a corpus labelled in terms of audio events, we used a relatively small sound effect corpus for training. Our initial experiments with one-against-all SVM classifiers for these 5 classes showed us the feasibility of using this type of data for training, thus avoiding the extremely morose task of manual labelling of a very high number of audio events. Preliminary integration experiments are quite promising.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-631"
  },
  "vipperla08_interspeech": {
   "authors": [
    [
     "Ravichander",
     "Vipperla"
    ],
    [
     "Steve",
     "Renals"
    ],
    [
     "Joe",
     "Frankel"
    ]
   ],
   "title": "Longitudinal study of ASR performance on ageing voices",
   "original": "i08_2550",
   "page_count": 4,
   "order": 691,
   "p1": "2550",
   "pn": "2553",
   "abstract": [
    "This paper presents the results of a longitudinal study of ASR performance on ageing voices. Experiments were conducted on the audio recordings of the proceedings of the Supreme Court Of The United States (SCOTUS). Results show that the Automatic Speech Recognition (ASR) Word Error Rates (WERs) for elderly voices are significantly higher than those of adult voices. The word error rate increases gradually as the age of the elderly speakers increase. Use of maximum likelihood linear regression (MLLR) based speaker adaptation on ageing voices improves theWER though the performance is still considerably lower compared to adult voices. Speaker adaptation however reduces the increase in WER with age during old age.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-632"
  },
  "vanhamme08_interspeech": {
   "authors": [
    [
     "Hugo",
     "Van hamme"
    ]
   ],
   "title": "HAC-models: a novel approach to continuous speech recognition",
   "original": "i08_2554",
   "page_count": 4,
   "order": 692,
   "p1": "2554",
   "pn": "2557",
   "abstract": [
    "In this paper, a bottom-up, activation-based paradigm for continuous speech recognition is described. Speech is described by co-occurrence statistics of acoustic events over an analysis window of variable length, leading to a vectorial representation of high but fixed dimension called \"Histogram of Acoustic Co-occurrence\" (HAC). During training, recurring acoustic patterns are discovered and associated to words through non-negative matrix factorisation. During testing, word activations are computed from the HAC-representation and their time of occurrence is estimated. Hence, words in a continuous utterance can be detected, ordered and located.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-633"
  },
  "mohapatra08_interspeech": {
   "authors": [
    [
     "Prateeti",
     "Mohapatra"
    ],
    [
     "Eric",
     "Fosler-Lussier"
    ]
   ],
   "title": "Investigations into phonological attribute classifier representations for CRF phone recognition",
   "original": "i08_2558",
   "page_count": 4,
   "order": 693,
   "p1": "2558",
   "pn": "2561",
   "abstract": [
    "Classifier combination has long been a staple for improving robustness of ASR systems; we present an experiment where introducing phonological feature scores from another lab's system [1] into our system gives a statistically significant improvement in Conditional Random Field-based TIMIT phone recognition, despite a standalone system based on their features performing significantly worse. The second part of the paper explores the reasons for this improvement by examining different representations of phonological attribute classifiers, in terms of what they are classifying (binary versus n-ary features) and representation of scoring functions. The analysis leads to the conclusions that while binary phonological feature estimates usually are worse than n-ary features, the combination of the two can be quite good if there are also differences in the feature definitions or training paradigm.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-634"
  },
  "subramanya08_interspeech": {
   "authors": [
    [
     "Amarnag",
     "Subramanya"
    ],
    [
     "Jeff A.",
     "Bilmes"
    ]
   ],
   "title": "Applications of virtual-evidence based speech recognizer training",
   "original": "i08_2562",
   "page_count": 4,
   "order": 694,
   "p1": "2562",
   "pn": "2565",
   "abstract": [
    "We present two applications of our previously proposed virtualevidence (VE) based speech recognizer training algorithm [1, 2]. The first relates to two-pass training where segmentations obtained during the first pass are used as VE to train the subsequent pass. We use the TIMIT phone and SVitchboard continuous speech recognition tasks to demonstrate the benefits of using VE based training in two-pass systems. The second application involves making use of functions that can incorporate prior domain knowledge to generate VE-scores. Here, in the case of TIMIT phone recognition, we show that using the proposed function to generate VE-scores results in about 6% relative error rate reduction over the baseline.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-635"
  },
  "doremalen08_interspeech": {
   "authors": [
    [
     "Joost van",
     "Doremalen"
    ],
    [
     "Lou",
     "Boves"
    ]
   ],
   "title": "Spoken digit recognition using a hierarchical temporal memory",
   "original": "i08_2566",
   "page_count": 4,
   "order": 695,
   "p1": "2566",
   "pn": "2569",
   "abstract": [
    "In this paper we explore the feasibility of the Memory-Prediction Theory, implemented in the form of a Hierarchical Temporal Memory (HTM), for automatic speech recognition. Up to now HTMs have almost exclusively been applied to image processing. However, the underlying theory can also be used as an approach to active perception of audio signals. Using the software platform under development by Numenta we implemented a system for isolated digit recognition, the speech recognition task that can be most easily cast in a form similar to image recognition. Our results show that the HTM approach holds promises for speech recognition. At the same time it is clear that the present implementation is not ideally suited for processing signals that encode information mainly in dynamic changes.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-636"
  },
  "bosch08_interspeech": {
   "authors": [
    [
     "Louis ten",
     "Bosch"
    ],
    [
     "Hugo",
     "Van hamme"
    ],
    [
     "Lou",
     "Boves"
    ]
   ],
   "title": "A computational model of language acquisition: focus on word discovery",
   "original": "i08_2570",
   "page_count": 4,
   "order": 696,
   "p1": "2570",
   "pn": "2573",
   "abstract": [
    "Young infants learn words by detecting patterns in the speech signal and by associating these patterns to stimuli presented by non-speech modalities (e.g vision). In this paper, we model this behaviour by designing and testing a computational model of word discovery. The model is able to build word-like representations on the basis of multimodal input data. The discovery of words (and word-like entities) takes place within a communicative loop between two protagonists, a 'carer' and the 'learner'. Experiments carried out on three different European languages (Finnish, Swedish, and Dutch) show that a robust word representation can be learned in using about 50 acoustic tokens (examples) of that word. The model is inspired by the memory structure that is assumed functional for human speech processing.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-637"
  },
  "kaushik08_interspeech": {
   "authors": [
    [
     "Lakshmish",
     "Kaushik"
    ],
    [
     "Douglas",
     "O'Shaughnessy"
    ]
   ],
   "title": "Voice activity detection using modified Wigner-ville distribution",
   "original": "i08_2574",
   "page_count": 4,
   "order": 697,
   "p1": "2574",
   "pn": "2577",
   "abstract": [
    "This paper introduces a new method of voice activity detection (VAD) using modified Wigner-Ville distribution. Modified Wigner-Ville distribution can track the speech regions efficiently in noisy environments. Fourier Transform (FT) and Discrete Cosine Transform (DCT) variants of Wigner-Ville distribution based, voice activity detection schemes are presented. The efficient time frequency spectral representation of Wigner-Ville is exploited for increasing the accuracy of VAD decision. The techniques are tested in three different standard noisy conditions (babble, gaussian, vehicle) with different levels of degradation. The proposed technique significantly outperforms the existing techniques in literature.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-638"
  },
  "chaitanya08_interspeech": {
   "authors": [
    [
     "Krishna",
     "Chaitanya"
    ],
    [
     "Rohit",
     "Sinha"
    ]
   ],
   "title": "Energy and entropy based switching algorithm for speech endpoint detection in varying SNR conditions",
   "original": "i08_2578",
   "page_count": 4,
   "order": 698,
   "p1": "2578",
   "pn": "2581",
   "abstract": [
    "In this work, we present an algorithm that switches between the energy and the entropy based voice activity detectors (VADs) to provide an improved performance under varying signal to noise ratio (SNR) conditions. The motivation for switching has come from the observed complementary behavior in the noise estimation performances of energy and entropy based voice activity detectors when evaluated over a range of -5 dB < SNR < 30 dB. At lower SNRs the entropy based voice activity detector outperformed the energy based one, whereas at higher SNRs interestingly the opposite trend was noted. The proposed online switching algorithm has a response time of 0.5 second and it achieves the optimal performance of either of the off-line VADs under fixed SNR conditions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-639"
  },
  "anemuller08_interspeech": {
   "authors": [
    [
     "Jörn",
     "Anemüller"
    ],
    [
     "Denny",
     "Schmidt"
    ],
    [
     "Jörg-Hendrik",
     "Bach"
    ]
   ],
   "title": "Detection of speech embedded in real acoustic background based on amplitude modulation spectrogram features",
   "original": "i08_2582",
   "page_count": 4,
   "order": 699,
   "p1": "2582",
   "pn": "2585",
   "abstract": [
    "A classification method is presented that detects the presence of speech embedded in a real acoustic background of non-speech sounds. Features used for classification are modulation components extracted by computation of the amplitude modulation spectrogram. Feature selection techniques and support vector classification are employed to identify modulation components most salient for the classification task and therefore considered as highly characteristic for speech. Results show that reliable detection of speech can be performed with less than 10 optimally selected modulation features, the most important ones are located in the modulation frequency range below 10 Hz. Detection of speech in a background of non-speech signals is performed with about 90% test-data accuracy at a signal-to-noise level of 0 dB. Compared to standard ITU G729.B voice activity detection, the proposed method results in increased true positive and reduced false positive rates induced by a real acoustic background.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-640"
  },
  "pham08_interspeech": {
   "authors": [
    [
     "Tuan Van",
     "Pham"
    ],
    [
     "Michael",
     "Stadtschnitzer"
    ],
    [
     "Franz",
     "Pernkopf"
    ],
    [
     "Gernot",
     "Kubin"
    ]
   ],
   "title": "Voice activity detection algorithms using subband power distance feature for noisy environments",
   "original": "i08_2586",
   "page_count": 4,
   "order": 700,
   "p1": "2586",
   "pn": "2589",
   "abstract": [
    "In this paper, we propose two robust voice activity detection (VAD) methods for adverse environments. A single subband power distance (SPD) feature is estimated from different wavelet subbands and further improved to be robust against noise. The first method is based on a neural network that operates on an input vector which consists of the SPD feature and its first and second derivatives. The second method is an adaptive threshold-based algorithm that employs only the single SPD feature. A statistical percentile filter based on long-term information is enhanced to estimate the noise threshold more adaptively. A performance evaluation and comparison is carried out for the proposed and state-of-the-art VAD algorithms on the TIMIT database which was artificially distorted by different additive noise types. The results show that the invented VAD methods are very robust to environmental noise and mostly outperform the standard VADs such as the ETSI AFE ES 202 050 and ITU-T G.729 B.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-641"
  },
  "muller08_interspeech": {
   "authors": [
    [
     "Christian",
     "Müller"
    ],
    [
     "Joan-Isaac",
     "Biel"
    ],
    [
     "Edward",
     "Kim"
    ],
    [
     "Daniel",
     "Rosario"
    ]
   ],
   "title": "Speech-overlapped acoustic event detection for automotive applications",
   "original": "i08_2590",
   "page_count": 4,
   "order": 701,
   "p1": "2590",
   "pn": "2593",
   "abstract": [
    "We present two approaches on acoustic event detection for speechenabled car applications: a generative GMM-UBM approach and a discriminative GMM-SVM supervector approach. The systems detect whether or not a certain acoustic event occurred while the built-in microphone of the car was active to record a spoken command, either before, while, or after the driver was speaking. These events can be music playing, phone ringing, a passenger different from the driver is talking, laughing, or coughing. The task is formally defined as a detection task along the lines of well established detection tasks such as speaker recognition or language recognition. Similarly, the evaluation procedure has been designed to resemble the respective official evaluation series performed by NIST (i.e. it was a blind 'one-shot' evaluation on a separately provided dataset). The performance of the system was calculated in terms of detection miss and false alarm probabilities (CMiss = CFA = 1, and PTarget = 0.5). The performance of the superior GMM-SVM system was 0.0345 for known test speakers and 0.1955 for novel test speakers. Frequency-filtered band energy coefficients (FFBE) outperformed MFCCS on that task. The results are promising and suggest further experiments on more data.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-642"
  },
  "temko08_interspeech": {
   "authors": [
    [
     "Andrey",
     "Temko"
    ],
    [
     "Climent",
     "Nadeu"
    ]
   ],
   "title": "Detection of acoustic events in interactive seminar data with temporal overlaps",
   "original": "i08_2594",
   "page_count": 4,
   "order": 702,
   "p1": "2594",
   "pn": "2597",
   "abstract": [
    "In Acoustic Event Detection (AED), both the identity of sounds and their position in time have to be obtained. In this paper, we first present our SVM-based system for AED in real-time conditions, along with the databases and metrics developed for the interna- tional evaluation campaign CLEAR 2007. In that evaluation, which was carried out with a real environment database that consists of interactive seminar recordings, the biggest encountered problem for AED was the presence of temporal overlaps, since they account for more than 70% of errors. In this paper we also report an initial attempt to deal with the overlap problem at the level of models. A two-step approach is proposed and it is tested firstly with artificially - overlapped acoustic data, and then with the above-mentioned seminar data.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-643"
  },
  "kim08e_interspeech": {
   "authors": [
    [
     "Chanwoo",
     "Kim"
    ],
    [
     "Richard M.",
     "Stern"
    ]
   ],
   "title": "Robust signal-to-noise ratio estimation based on waveform amplitude distribution analysis",
   "original": "i08_2598",
   "page_count": 4,
   "order": 703,
   "p1": "2598",
   "pn": "2601",
   "abstract": [
    "In this paper, we introduce a new algorithm for estimating the signal-to-noise ratio (SNR) of speech signals, called WADA-SNR (Waveform Amplitude Distribution Analysis). In this algorithm we assume that the amplitude distribution of clean speech can be approximated by the Gamma distribution with a shaping parameter of 0.4, and that an additive noise signal is Gaussian. Based on this assumption, we can estimate the SNR by examining the amplitude distribution of the noise-corrupted speech. We evaluate the performance of the WADA-SNR algorithm on databases corrupted by white noise, background music, and interfering speech. The WADA-SNR algorithm shows significantly less bias and less variability with respect to the type of noise compared to the standard NIST STNR algorithm. In addition, the algorithm is quite computationally efficient.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-644"
  },
  "stark08b_interspeech": {
   "authors": [
    [
     "Anthony P.",
     "Stark"
    ],
    [
     "Kuldip K.",
     "Paliwal"
    ]
   ],
   "title": "Speech analysis using instantaneous frequency deviation",
   "original": "i08_2602",
   "page_count": 4,
   "order": 704,
   "p1": "2602",
   "pn": "2605",
   "abstract": [
    "In this paper, our aim is to derive a phase spectrum representation computed via the short-time Fourier transform. Specifically, we are interested in developing a narrow-band speech representation - employing 20.40 ms analysis windows. Furthermore, this representation should be as physically meaningful as the magnitude spectrum. To achieve these ends, we concentrate on instantaneous frequency (IF) derived from the phase spectrum. In doing so, we introduce the IF deviation spectrum, and show that this spectrum exhibits pitch and formant structure similar to the magnitude spectrum. Lastly we demonstrate the advantages of the proposed IF deviation spectrum over the IF distribution spectrum proposed earlier in the literature.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-645"
  },
  "glaser08_interspeech": {
   "authors": [
    [
     "Claudius",
     "Gläser"
    ],
    [
     "Martin",
     "Heckmann"
    ],
    [
     "Frank",
     "Joublin"
    ],
    [
     "Christian",
     "Goerick"
    ]
   ],
   "title": "Auditory-based formant estimation in noise using a probabilistic framework",
   "original": "i08_2606",
   "page_count": 4,
   "order": 705,
   "p1": "2606",
   "pn": "2609",
   "abstract": [
    "We recently introduced a computationally efficient framework for tracking formants which combines a biologically inspired preprocessing for enhancing formants in spectrograms with a probabilistic framework for estimating formant trajectories. In contrast to previously published approaches our tracking scheme relies on the joint distribution of formants rather than using independent tracking instances for each formant separately. Therewith more precise formant estimates could be obtained. In this paper we will briefly review our algorithm and extend it by using more sophisticated models of the formants underlying dynamics. Furthermore, we will dwell on the robustness of our method for speech degraded by various types of noise. A comprehensive evaluation on a large publicly available database containing hand-labeled formant trajectories shows significant performance improvements in both clean and noisy speech compared to state of the art approaches.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-646"
  },
  "ramamurty08_interspeech": {
   "authors": [
    [
     "K. Sri",
     "Rama Murty"
    ],
    [
     "Saurav",
     "Khurana"
    ],
    [
     "Yogendra Umesh",
     "Itankar"
    ],
    [
     "M. R.",
     "Kesheorey"
    ],
    [
     "B.",
     "Yegnanarayana"
    ]
   ],
   "title": "Efficient representation of throat microphone speech",
   "original": "i08_2610",
   "page_count": 4,
   "order": 706,
   "p1": "2610",
   "pn": "2613",
   "abstract": [
    "The objective of this work is to represent the information in the speech signal picked up by a throat microphone (TM) in an efficient manner in terms of number of bits required. Since the TM signal is unaffected by ambient noise, it is possible to extract the required information effectively under different environmental conditions. A spectral mapping technique is proposed from the TM speech to normal microphone (NM) speech to improve the perceptual quality. The mapping is done using vector quantization of pairwise spectral feature vectors derived from each frame of TM and the corresponding NM speech signals. Once the codebook is formed, the spectral features from a TM signal are represented as a sequence of codebook indices. The sequence of codebook indices, the pitch contour and the energy contour derived from the TM signal are used to store/transmit the TM speech information efficiently. From the received sequence of codebook indices, the NM spectral vectors are retrieved due to pairwise vector quantization of the feature vectors. A synthetic residual signal is generated at the receiver from prestored residual templates by incorporating the pitch and the energy. The synthetic residual signal is used to excite the system corresponding to the NM spectral vectors to generate the speech signal.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-647"
  },
  "deshmukh08b_interspeech": {
   "authors": [
    [
     "Om D.",
     "Deshmukh"
    ],
    [
     "Ashish",
     "Verma"
    ]
   ],
   "title": "Acoustic-phonetic approach for automatic evaluation of spoken grammar",
   "original": "i08_2614",
   "page_count": 4,
   "order": 707,
   "p1": "2614",
   "pn": "2617",
   "abstract": [
    "Evaluating spoken grammar skills is an important component of evaluating the overall spoken English skills of a candidate. This paper presents a novel approach that incorporates the knowledge of acoustic-phonetics to improve the performance of an existing spoken grammar evaluation technique in a question-answer paradigm. A novel acoustic parameter, Onset Coherence, to make a one-pass distinction between the onsets of fricatives, stops and vowels is also proposed. The performance of the proposed approach is evaluated against human labeled data obtained from real-life spoken grammar assessment tests. The proposed approach of combining the knowledge from the acoustic-phonetic domain with the information obtained from the automatic speech recognition system leads to an improvement of 8.2% in utterance-level classification.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-648"
  },
  "cox08b_interspeech": {
   "authors": [
    [
     "Stephen",
     "Cox"
    ]
   ],
   "title": "On estimation of a speaker's confusion matrix from sparse data",
   "original": "i08_2618",
   "page_count": 4,
   "order": 708,
   "p1": "2618",
   "pn": "2621",
   "abstract": [
    "Confusion matrices have been widely used to increase the accuracy of speech recognisers, but usually a mean confusion matrix, averaged over many speakers, is used. However, analysis shows that confusion matrices for individual speakers vary considerably, and so there is benefit in obtaining estimates of confusion matrices for individual speakers. Unfortunately, there is rarely enough data to make reliable estimates. We present a technique for estimating the elements of a speaker's confusion matrix given only sparse data from the speaker. It utilizes non-negative matrix factorisation to find structure within confusion matrices, and this structure is exploited to make improved estimates. Results show that under certain conditions, this technique can give estimates that are as good as those obtained with twice the number of utterances available from the speaker.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-649"
  },
  "hazan08b_interspeech": {
   "authors": [
    [
     "Valerie",
     "Hazan"
    ]
   ],
   "title": "Talking heads and pronunciation training: a review",
   "original": "i08_2622",
   "page_count": 1,
   "order": 709,
   "p1": "2622",
   "pn": "",
   "abstract": [
    "This special session will include talks describing the use of talking heads in pronunciation training programs for second-language learners and clinical populations. This introductory talk will provide a brief review of the field.\n",
    ""
   ]
  },
  "massaro08_interspeech": {
   "authors": [
    [
     "Dominic W.",
     "Massaro"
    ],
    [
     "Stephanie",
     "Bigler"
    ],
    [
     "Trevor",
     "Chen"
    ],
    [
     "Marcus",
     "Perlman"
    ],
    [
     "Slim",
     "Ouni"
    ]
   ],
   "title": "Pronunciation training: the role of eye and ear",
   "original": "i08_2623",
   "page_count": 4,
   "order": 710,
   "p1": "2623",
   "pn": "2626",
   "abstract": [
    "For speech perception and production of a new language, we examined whether 1) they would be more easily learned by ear and eye relative to by ear alone, and 2) whether viewing the tongue, palate, and velum during production is more beneficial for learning than a standard frontal view of the speaker. In addition, we determine whether differences in learning under these conditions are due to enhanced receptive learning from additional visual information, or to more active learning motivated by the visual presentations. Test stimuli were two similar vowels in Mandarin and two similar stop consonants in Arabic, presented in different word contexts. Participants were tested with auditory speech and were either trained 1) unimodally with just auditory speech or bimodally with both auditory and visual speech, and 2) a standard frontal view versus an inside view of the vocal tract. The visual speech was generated by the appropriate multilingual versions of Baldi [1]. The results test the effectiveness of visible speech for learning a new language. Preliminary results indicate that visible speech can contribute positively to acquiring new speech distinctions and promoting active learning.\n",
    "",
    "",
    "Massaro, D. W. (1998). Perceiving talking faces: From speech perception to a behavioral principle. Cambridge, Massachusetts: MIT Press.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-650"
  },
  "wik08_interspeech": {
   "authors": [
    [
     "Preben",
     "Wik"
    ],
    [
     "Olov",
     "Engwall"
    ]
   ],
   "title": "Can visualization of internal articulators support speech perception?",
   "original": "i08_2627",
   "page_count": 4,
   "order": 711,
   "p1": "2627",
   "pn": "2630",
   "abstract": [
    "This paper describes the contribution to speech perception given by animations of intra-oral articulations. 18 subjects were asked to identify the words in acoustically degraded sentences in three different presentation modes: acoustic signal only, audiovisual with a front view of a synthetic face and an audiovisual with both front face view and a side view, where tongue movements were visible by making parts of the cheek transparent. The augmented reality side-view did not help subjects perform better overall than with the front view only, but it seems to have been beneficial for the perception of palatal plosives, liquids and rhotics, especially in clusters. The results indicate that it cannot be expected that intra-oral animations support speech perception in general, but that information on some articulatory features can be extracted. Animations of tongue movements have hence more potential for use in computer-assisted pronunciation and perception training than as a communication aid for the hearing-impaired.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-651"
  },
  "engwall08_interspeech": {
   "authors": [
    [
     "Olov",
     "Engwall"
    ]
   ],
   "title": "Can audio-visual instructions help learners improve their articulation? - an ultrasound study of short term changes",
   "original": "i08_2631",
   "page_count": 4,
   "order": 712,
   "p1": "2631",
   "pn": "2634",
   "abstract": [
    "This paper describes how seven French subjects change their pronunciation and articulation when practising Swedish words with a computer-animated virtual teacher. The teacher gives feedback on the user's pronunciation with audiovisual instructions suggesting how the articulation should be changed. A wizard-of-Oz set-up was used for the training session, in which a human listener choose the adequate pre-generated feedback based on the user's pronunciation. The subjects' change of the articulation was monitored during the practice session with a hand-held ultrasound probe. The perceptual analysis indicates that the subjects improved their pronunciation during the training and the ultrasound measurements suggest that the improvement was made by following the articulatory instructions given by the computer-animated teacher.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-652"
  },
  "badin08_interspeech": {
   "authors": [
    [
     "Pierre",
     "Badin"
    ],
    [
     "Yuliya",
     "Tarabalka"
    ],
    [
     "Frédéric",
     "Elisei"
    ],
    [
     "Gérard",
     "Bailly"
    ]
   ],
   "title": "Can you \"read tongue movements\"?",
   "original": "i08_2635",
   "page_count": 4,
   "order": 713,
   "p1": "2635",
   "pn": "2638",
   "abstract": [
    "Lip reading relies on visible articulators to ease audiovisual speech understanding. However, lips and face alone provide very incomplete phonetic information: the tongue, that is generally not entirely seen, carries an important part of the articulatory information not accessible through lip reading. The question was thus whether the direct and full vision of the tongue allows tongue reading. We have therefore generated a set of audiovisual VCV stimuli by controlling an audiovisual talking head that can display all speech articulators, including tongue, in an augmented speech mode, from articulators movements tracked on a speaker. These stimuli have been played to subjects in a series of audiovisual perception tests in various presentation conditions (audio signal alone, audiovisual signal with profile cutaway display with or without tongue, complete face), at various Signal-to-Noise Ratios. The results show a given implicit effect of tongue reading learning, a preference for the more ecological rendering of the complete face in comparison with the cutaway presentation, a predominance of lip reading over tongue reading, but the capability of tongue reading to take over when the audio signal is strongly degraded or absent. We conclude that these tongue reading capabilities could be used for applications in the domain of speech therapy for speech retarded children, perception and production rehabilitation of hearing impaired children, and pronunciation training for second language learners.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-653"
  },
  "kroger08_interspeech": {
   "authors": [
    [
     "Bernd J.",
     "Kröger"
    ],
    [
     "Verena",
     "Graf-Borttscheller"
    ],
    [
     "Anja",
     "Lowit"
    ]
   ],
   "title": "Two- and three-dimensional visual articulatory models for pronunciation training and for treatment of speech disorders",
   "original": "i08_2639",
   "page_count": 4,
   "order": 714,
   "p1": "2639",
   "pn": "2642",
   "abstract": [
    "Visual articulatory models can be used for visualizing vocal tract articulatory speech movements. This information may be helpful in pronunciation training or in therapy of speech disorders. For testing this hypothesis, speech recognition rates were quantified for mute animations of vocalic and consonantal speech movements generated by a 2D and a 3D visual articulatory model. The visually based speech sound recognition test (mimicry test) was performed by two groups of eight children (five to eight years old) matched in age and sex. The children were asked to mimic the visually produced mute speech movement animations for different speech sounds. Recognition rates stay significantly above chance but indicate no significant difference for each of the two models. Children older than 5 years are capable of interpreting vocal tract articulatory speech sound movements without any preparatory training in a speech adequate way. The complex 3D-display of vocal tract articulatory movements provides no significant advantage in comparison to the visually simpler 2D-midsagittal displays of vocal tract articulatory movements.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-654"
  },
  "fagel08c_interspeech": {
   "authors": [
    [
     "Sascha",
     "Fagel"
    ],
    [
     "Katja",
     "Madany"
    ]
   ],
   "title": "A 3-d virtual head as a tool for speech therapy for children",
   "original": "i08_2643",
   "page_count": 4,
   "order": 715,
   "p1": "2643",
   "pn": "2646",
   "abstract": [
    "A virtual talking head was adapted to show articulatory movements of the lips, jaw, tongue, and velum. A JavaScript program with a graphical user interface in HTML was developed for speech therapists to use the talking head as a tool in speech therapy. In an initial study children's productions of words containing the sounds /s/ and /z/ were recorded and evaluated before and after two short learning lessons with an experimenter using the virtual head to explain the correct (prototypic) pronunciation of these sounds. Results show that several children could significantly enhance their speech production of the /s,z/ sound.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-655"
  },
  "hofe08_interspeech": {
   "authors": [
    [
     "Robin",
     "Hofe"
    ],
    [
     "Roger K.",
     "Moore"
    ]
   ],
   "title": "Anton: an animatronic model of a human tongue and vocal tract",
   "original": "i08_2647",
   "page_count": 4,
   "order": 716,
   "p1": "2647",
   "pn": "2650",
   "abstract": [
    "This paper describes AnTon, the first animatronic model of a human tongue and vocal tract which is being developed using biomimetic design principles. The present model consists of movable tongue and jaw models that are connected to a fixed hyoid bone and a skull. AnTon's ability to reproduce speech gestures has been investigated using MRI scans of human subjects producing standard vowels. In the future, the animatronic vocal tract will be used to study the relation between speech variation and energy investment, specifically in the context of the Lombard reflex.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-656"
  },
  "arai08_interspeech": {
   "authors": [
    [
     "Takayuki",
     "Arai"
    ]
   ],
   "title": "Physical models of the human vocal tract with gel-type material",
   "original": "i08_2651",
   "page_count": 4,
   "order": 717,
   "p1": "2651",
   "pn": "2654",
   "abstract": [
    "Beginning in 2001, we have been developing models of the vocal tract to promote a more intuitive understanding of the theories for speech science for technical and non-technical students. In this paper, we compared and contrasted four newer models of the talking heads: our original model with a gel-type tongue, a similar model including teeth and palate, a third model with teeth and palate having a default low tongue height, and a fourth ultra-malleable model made completely of gel material. Results are discussed regarding their strengths and weaknesses for educational purposes and articulatory training in speech pathology and language learning.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-657"
  },
  "huang08e_interspeech": {
   "authors": [
    [
     "Chao",
     "Huang"
    ],
    [
     "Feng",
     "Zhang"
    ],
    [
     "Frank K.",
     "Soong"
    ],
    [
     "Min",
     "Chu"
    ]
   ],
   "title": "Mispronunciation detection for Mandarin Chinese",
   "original": "i08_2655",
   "page_count": 4,
   "order": 718,
   "p1": "2655",
   "pn": "2658",
   "abstract": [
    "In this paper, we propose several reliable weighting factors based on the speaker's proficiency level, which can be used to normalize the scaled log-posterior probability (SLPP) to further improve mispronunciation detection at syllable level for Mandarin Chinese. Experiments based on a database consisting of 8000 syllables, pronounced by 40 speakers with varied pronunciation proficiency, shows the very promising effectiveness of these normalization schemes by reducing FAR from 44.4% to 35.1% on average and greatly improving automatic mispronunciation detection (AMD) performance greatly. In addition, we have attempted to investigate and analyze underlying behavior of such normalization factors. Some modifications, extensions and possible applications of such factors in real usage cases are also discussed.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-658"
  },
  "wang08n_interspeech": {
   "authors": [
    [
     "Lijuan",
     "Wang"
    ],
    [
     "Tao",
     "Hu"
    ],
    [
     "Peng",
     "Liu"
    ],
    [
     "Frank K.",
     "Soong"
    ]
   ],
   "title": "Efficient handwriting correction of speech recognition errors with template constrained posterior (TCP)",
   "original": "i08_2659",
   "page_count": 4,
   "order": 719,
   "p1": "2659",
   "pn": "2662",
   "abstract": [
    "More mobile devices are starting to use automatic speech recognition for command or text input. However, correcting recognition errors in a small compact mobile device is usually inconvenient and it may take several finger operations on a small keypad to correct errors. In this paper, we propose a new multimodal input method and a novel confidence measure - template constrained posterior (TCP) to simplify the correction process. The method works by interactively integrating a handwriting recognizer with a speech recognizer. Information obtained in pen-based error marking, like error location, error type, etc., is fed back to the speech recognizer, and speech recognition errors are automatically corrected using the TCP confidence measure. Experimental results on Aurora2, Wall Street Journal, Switchboard, and two Chinese databases show that compared with speech recognition baseline, the proposed method achieves relative error reduction of 64.9%, 43.9%, 26.1%, 39.0%, 31.4%, respectively, after the auto correction.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-659"
  },
  "ejarque08_interspeech": {
   "authors": [
    [
     "Pascual",
     "Ejarque"
    ],
    [
     "Javier",
     "Hernando"
    ]
   ],
   "title": "Bi-Gaussian score equalization in an audio-visual SVM-based person verification system",
   "original": "i08_2663",
   "page_count": 4,
   "order": 720,
   "p1": "2663",
   "pn": "2666",
   "abstract": [
    "In multimodal fusion systems a normalization of the features or the scores is needed before the fusion process. In this work, in addition to the conventional methods, histogram equalization, which was recently introduced by the authors in multimodal systems, and Bi-Gaussian equalization, which takes into account the separate statistics of the genuine and impostor scores, and is introduced in this paper, are applied upon the scores in a multimodal SVM-based person verification system composed by prosodic, speech spectrum, and face information. Bi-Gaussian equalization has obtained the best results and outperform in more than a 23.25% the results obtained by Min-Max normalization.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-660"
  },
  "meltzner08_interspeech": {
   "authors": [
    [
     "Geoffrey S.",
     "Meltzner"
    ],
    [
     "Jason",
     "Sroka"
    ],
    [
     "James T.",
     "Heaton"
    ],
    [
     "L. Donald",
     "Gilmore"
    ],
    [
     "Glen",
     "Colby"
    ],
    [
     "Serge",
     "Roy"
    ],
    [
     "Nancy",
     "Chen"
    ],
    [
     "Carlo J. De",
     "Luca"
    ]
   ],
   "title": "Speech recognition for vocalized and subvocal modes of production using surface EMG signals from the neck and face",
   "original": "i08_2667",
   "page_count": 4,
   "order": 721,
   "p1": "2667",
   "pn": "2670",
   "abstract": [
    "We report automatic speech recognition accuracy for individual words using eleven surface electromyographic (sEMG) recording locations on the face and neck during three speaking modes: vocalized, mouthed, and mentally rehearsed. An HMM based recognition system was trained and tested on a 65 word vocabulary produced by 9 American English speakers in all three speaking modes. Our results indicate high sEMG-based recognition accuracy for the vocalized and mouthed speaking modes (mean rates of 92.1% and 86.7% respectively), but an inability to conduct recognition on mentally rehearsed speech due to a lack of sufficient sEMG activity.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-661"
  },
  "lewis08_interspeech": {
   "authors": [
    [
     "Trent W.",
     "Lewis"
    ],
    [
     "David M. W.",
     "Powers"
    ]
   ],
   "title": "Distinctive feature fusion for recognition of australian English consonants",
   "original": "i08_2671",
   "page_count": 4,
   "order": 722,
   "p1": "2671",
   "pn": "2674",
   "abstract": [
    "Audio-Visual Automatic Speech Recognition offers to make speech recognition possible in noisy environments. Early and late fusion approaches dominate the field but may ignore linguistically relevant features. Distinctive features offer an alternative unit for fusion and research has shown that this is feasible on subsets of phonemes [1]. This paper outlines two extended models, multiclass and binary, and results suggest that it is possible to achieve a 20dB gain over audio-only recognition in low SNR environments.\n",
    "",
    "",
    "T. Lewis and D. Powers, \"Distinctive feature fusion for improved audio-visual phoneme recognition,\" in The Eighth International Symposium on Signal Proocessing and Its Applications, A. Bouzerdoum and A. Beghdadi, Eds. Sydney, Australia: IEEE, 2005.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-662"
  },
  "watanabe08_interspeech": {
   "authors": [
    [
     "Yasushi",
     "Watanabe"
    ],
    [
     "Koichi",
     "Shinoda"
    ],
    [
     "Sadaoki",
     "Furui"
    ]
   ],
   "title": "Time-lag adaptation for semi-synchronous speech and pen input",
   "original": "i08_2675",
   "page_count": 4,
   "order": 723,
   "p1": "2675",
   "pn": "2678",
   "abstract": [
    "In a previous study, we developed an interface using semisynchronous speech and pen input. In this interface, a user speaks while writing, and the pen input complements the speech, enabling a higher recognition performance than with speech alone. When a user inputs speech and pen, there is a time lag between the two modes, and the lag differs among users. We propose a method for adapting to the different time lags of individual users. This method was evaluated in a Japanese continuous speech recognition task with three different pen-input interfaces including a QWERTY keyboard interface. The time-lag adaptation improved recognition accuracies by up to 0.5 point.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-663"
  },
  "lucey08_interspeech": {
   "authors": [
    [
     "Patrick",
     "Lucey"
    ],
    [
     "Sridha",
     "Sridharan"
    ],
    [
     "David",
     "Dean"
    ]
   ],
   "title": "Continuous pose-invariant lipreading",
   "original": "i08_2679",
   "page_count": 4,
   "order": 724,
   "p1": "2679",
   "pn": "2682",
   "abstract": [
    "In audio-visual automatic speech recognition (AVASR), no research to date has been conducted into the problem of recognising visual speech whilst the speaker is moving their head. In this paper, we extend our current system to deal with this task, which we entitle continuous pose-invariant lipreading. By developing an AVASR system which can deal with such a scenario, we believe we are making the system effectively \"real-world\" as it requires little cooperation from the user and as such can be used in a host of realistic applications (e.g. mobile phones, in-vehicles etc.). In this proof of concept paper, we show via our experiments on the CUAVE database, that recognising visual speech whilst a speaker is moving their head during the utterance is feasible.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-664"
  },
  "nouza08_interspeech": {
   "authors": [
    [
     "Jan",
     "Nouza"
    ],
    [
     "Jan",
     "Silovsky"
    ],
    [
     "Jindrich",
     "Zdansky"
    ],
    [
     "Petr",
     "Cerva"
    ],
    [
     "Martin",
     "Kroul"
    ],
    [
     "Josef",
     "Chaloupka"
    ]
   ],
   "title": "Czech-to-slovak adapted broadcast news transcription system",
   "original": "i08_2683",
   "page_count": 4,
   "order": 725,
   "p1": "2683",
   "pn": "2686",
   "abstract": [
    "The first broadcast news (BN) transcription system for Slovak is introduced. It employs the same modules as the system we developed earlier for Czech. We utilize similarity between the two languages in efficient lexicon building, in mapping Slovak specific (rarely occurring) phonemes onto Czech ones and in low-resource cross-lingual adaptation of acoustic model. The system uses 166K-word lexicon and on the Slovak part of European COST278 BN database achieves 23.6% WER (which is only 5% less than the original, long-term optimized Czech system). Similar results were achieved also on recently recorded data from four Slovak stations.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-665"
  },
  "lyu08b_interspeech": {
   "authors": [
    [
     "Dau-Cheng",
     "Lyu"
    ],
    [
     "Sabato Marco",
     "Siniscalchi"
    ],
    [
     "Tae-Yoon",
     "Kim"
    ],
    [
     "Chin-Hui",
     "Lee"
    ]
   ],
   "title": "Continuous phone recognition without target language training data",
   "original": "i08_2687",
   "page_count": 4,
   "order": 726,
   "p1": "2687",
   "pn": "2690",
   "abstract": [
    "Designing an automatic speech recognition system with little or no language-specific training data is a challenging research topic because collecting abundant speech training data is not always an easy job for all possible languages of interest. According to our previous studied detection-based paradigm, we used a set of 21 acoustic phonetic attributes shared by five languages to perform Japanese phone recognition without using any Japanese speech training data. In this paper, we address the key issue of designing attribute-to-phone mapping models by two techniques: (1) a phone-based background model for each of the speech attribute detector to improve attribute detection; and (2) a data-driven clustering algorithm to group attribute-to-phone mapping rules of known languages to predict such rules for target phones in an unseen language. We report on experimental results of continuous Japanese phone recognition with the OGI Multilingual Speech Corpus and show that the proposed approach indeed decreases the false rejection rate of attribute detection, and improves the phone recognition accuracy.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-666"
  },
  "white08_interspeech": {
   "authors": [
    [
     "Christopher M.",
     "White"
    ],
    [
     "Sanjeev",
     "Khudanpur"
    ],
    [
     "James K.",
     "Baker"
    ]
   ],
   "title": "An investigation of acoustic models for multilingual code-switching",
   "original": "i08_2691",
   "page_count": 4,
   "order": 727,
   "p1": "2691",
   "pn": "2694",
   "abstract": [
    "Multilingual speech processing continues to develop as speech technology spreads to heterogeneous clients and applications. We address a distinct problem of code-switching - the spontaneous but occasional use, within speech in one language (referred to as L1), of words, phrases, expressions or idioms from a second language (L2). We examine two alternatives for modeling the acoustics of such words: creation of L1 pronunciations for the out-of-language (OOL) words for use with L1 acoustic models, and retention of their L2 pronunciations for use with multilingual acoustic models. We test the hypothesis that the latter is a better acoustic model for OOL words. We develop a set of lexica in IPA form, a global phoneme inventory, and handle the problem of L2 word pronunciation by creating linguistically motivated pairwise mappings. We show that retention of L2 pronunciations with multilingual acoustic models better explains the observations when restricted to a forced alignment.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-667"
  },
  "toth08b_interspeech": {
   "authors": [
    [
     "Lászlá",
     "Tóth"
    ],
    [
     "Joe",
     "Frankel"
    ],
    [
     "Gábor",
     "Gosztolya"
    ],
    [
     "Simon",
     "King"
    ]
   ],
   "title": "Cross-lingual portability of MLP-based tandem features - a case study for English and Hungarian",
   "original": "i08_2695",
   "page_count": 4,
   "order": 728,
   "p1": "2695",
   "pn": "2698",
   "abstract": [
    "One promising approach for building ASR systems for lessresourced languages is cross-lingual adaptation. Tandem ASR is particularly well suited to such adaptation, as it includes two cascaded modelling steps: feature extraction using multi-layer perceptrons (MLPs), followed by modelling using a standard HMM. The language-specific tuning can be performed by adjusting the HMM only, leaving the MLP untouched.\n",
    "Here we examine the portability of feature extractor MLPs between an Indo-European (English) and a Finno-Ugric (Hungarian) language. We present experiments which use both conventional phone-posterior and articulatory feature (AF) detector MLPs, both trained on a much larger quantity of (English) data than the monolingual (Hungarian) system. We find that the cross-lingual configurations achieve similar performance to the monolingual system, and that, interestingly, the AF detectors lead to slightly worse performance, despite the expectation that they should be more language-independent than phone-based MLPs. However, the cross-lingual system outperforms all other configurations when the English phone MLP is adapted on the Hungarian data.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-668"
  },
  "zhao08c_interspeech": {
   "authors": [
    [
     "Xufang",
     "Zhao"
    ],
    [
     "Douglas",
     "O'Shaughnessy"
    ]
   ],
   "title": "Seed models combination and state level mappings of cross-lingual transfer for rapid HMM development: from English to Mandarin",
   "original": "i08_2699",
   "page_count": 4,
   "order": 729,
   "p1": "2699",
   "pn": "2702",
   "abstract": [
    "The practice of cross-lingual transfer of speech technology is of increasing concern as the demand for recognition systems in multiple languages grows. Previous work has proved that: firstly, if there is a big difference between the source language and the target language, cross-lingual adaptation may not outperform scratch training even if the training set is limited; secondly, cross-lingual seed models achieve lower word error rates than flat starts or random models. Based on these two points, this paper improved the approach of generating cross-lingual seed models through two ways: (1) using a combination of cross-lingual seed models and \"flat-start\" models, and (2) using phoneme mappings on an HMM state level in a new language to reduce mismatched coarticulation. All simulations are carried out with limited available training data, and the effectiveness of the approach was proved using English-Mandarin as a test case.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-669"
  },
  "bouselmi08b_interspeech": {
   "authors": [
    [
     "Ghazi",
     "Bouselmi"
    ],
    [
     "Dominique",
     "Fohr"
    ],
    [
     "Irina",
     "Illina"
    ]
   ],
   "title": "Multi-accent and accent-independent non-native speech recognition",
   "original": "i08_2703",
   "page_count": 4,
   "order": 730,
   "p1": "2703",
   "pn": "2706",
   "abstract": [
    "In this article we present a study of a multi-accent and accentindependent non-native speech recognition. We propose several approaches based on phonetic confusion and acoustic adaptation. The goal of this article is to investigate the feasibility of multi-accent non-native speech recognition without detecting the origin of the speaker. Tests on the HIWIRE corpus show that multiaccent pronunciation modeling and acoustic adaptation reduce the WER by up to 76% compared to results of canonical models of the target language. We also investigate accent-independent approaches in order to assess the robustness of the proposed methods to unseen foreign accents. Experiments show that our approaches correctly handle unseen accents and give up to 55% WER reduction, compared to the models of the target language. Finally, the proposed pronunciation modeling approach maintains the recognition accuracy on canonical native speech as assessed by our experiments on the TIMIT corpus.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-670"
  },
  "singla08_interspeech": {
   "authors": [
    [
     "Adish Kumar",
     "Singla"
    ],
    [
     "Dilek",
     "Hakkani-Tür"
    ]
   ],
   "title": "Cross-lingual sentence extraction for information distillation",
   "original": "i08_2707",
   "page_count": 4,
   "order": 731,
   "p1": "2707",
   "pn": "2710",
   "abstract": [
    "Information distillation aims to analyze and interpret large volumes of speech and text archives in multiple languages and produce structured information of interest to the user. In this work, we investigate cross-lingual information distillation, where non-English (source language) documents are searched for user queries that are in English (target language). We propose to perform distillation both on the original source language data and their English translations output by machine translation, and combine the two outputs. We experimentally show that combination approach results in 8% to 16% absolute (13% to 31% relative) F-measure improvement over the previous work.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-671"
  },
  "scanzio08_interspeech": {
   "authors": [
    [
     "Stefano",
     "Scanzio"
    ],
    [
     "Pietro",
     "Laface"
    ],
    [
     "Luciano",
     "Fissore"
    ],
    [
     "Roberto",
     "Gemello"
    ],
    [
     "Franco",
     "Mana"
    ]
   ],
   "title": "On the use of a multilingual neural network front-end",
   "original": "i08_2711",
   "page_count": 4,
   "order": 732,
   "p1": "2711",
   "pn": "2714",
   "abstract": [
    "This paper presents a front-end consisting of an Artificial Neural Network (ANN) architecture trained with multilingual corpora. The idea is to train an ANN front-end able to integrate the acoustic variations included in databases collected for different languages, through different channels, or even for specific tasks. This ANN front-end produces discriminant features that can be used as observation vectors for language or task dependent recognizers. The approach has been evaluated on three difficult tasks: recognition of non-native speaker sentences, training of a new language with a limited amount of speech data, and training of a model for car environment using a clean microphone corpus of the target language and data collected in car environment in another language.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-672"
  },
  "sim08_interspeech": {
   "authors": [
    [
     "Khe Chai",
     "Sim"
    ],
    [
     "Haizhou",
     "Li"
    ]
   ],
   "title": "Context-sensitive probabilistic phone mapping model for cross-lingual speech recognition",
   "original": "i08_2715",
   "page_count": 4,
   "order": 733,
   "p1": "2715",
   "pn": "2718",
   "abstract": [
    "This paper presents a probabilistic phone mapping model (PPM) that makes possible automatic speech recognition using a foreign phonetic system. We formulate the training of the phone mapping model in the framework of maximum likelihood estimation. The model can be learned automatically from the reference phonetic transcript and the phonetic transcript resulting from a foreign phonetic recogniser using the Expectation Maximisation algorithm. This paper also compares the use of temporal and spatial contexts to enchance the phone mapping performance. A decision tree clustering technique is used to tie unseen contexts for robustness. We evaluate the PPM method on cross-lingual phone and isolated word recognition tasks, using the Hungarian and Russian phone recognisers to recognise Czech speech. Consistent improvement is obtained by using context-dependent phone mapping.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-673"
  },
  "liu08f_interspeech": {
   "authors": [
    [
     "Chen",
     "Liu"
    ],
    [
     "Lynette",
     "Melnar"
    ]
   ],
   "title": "A non-acoustic approach to crosslingual speech recognition performance prediction",
   "original": "i08_2719",
   "page_count": 4,
   "order": 734,
   "p1": "2719",
   "pn": "2722",
   "abstract": [
    "Crosslingual acoustic modeling is an effective technique for building acoustic models in the absence of native training data. A small amount of native speech data is still needed for verifying the crosslingual models by running an actual recognition test. In some very stringent yet realistic situations, however, even the test data may not be available. We introduce an algorithm that objectively predicts the recognition performance of crosslingual acoustic models. This approach does not require conducting of actual speech recognition tests with target-language speech data; nor does it depend on any acoustic measurement techniques. The algorithm is based on a series of linguistic metrics characterizing the articulatory phonetic and phonological information of phonemes from both the target and source languages. It is useful both for validating crosslingual models for speech recognition applications, and for making database acquisition decisions that could prove very cost-beneficial.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-674"
  },
  "sridhar08_interspeech": {
   "authors": [
    [
     "Vivek Kumar Rangarajan",
     "Sridhar"
    ],
    [
     "Srinivas",
     "Bangalore"
    ],
    [
     "Shrikanth S.",
     "Narayanan"
    ]
   ],
   "title": "Factored translation models for enriching spoken language translation with prosody",
   "original": "i08_2723",
   "page_count": 4,
   "order": 735,
   "p1": "2723",
   "pn": "2726",
   "abstract": [
    "Key contextual information such as word prominence, emphasis, and contrast is typically ignored in speech-to-speech (S2S) translation due to the compartmentalized nature of the translation process. Conventional S2S systems rely on extracting prosody dependent cues from hypothesized (possibly erroneous) translation output using only words and syntax. In contrast, we propose the use of factored translation models to integrate the assignment and transfer of pitch accents (tonal prominence) during translation. We report experiments on 2 parallel corpora (Farsi-English and Japanese-English). The proposed factored translation models provide a relative improvement of 8.4% and 16.8% in pitch accent labeling accuracy over the post-processing approach for the two corpora respectively.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-675"
  },
  "schwenk08_interspeech": {
   "authors": [
    [
     "Holger",
     "Schwenk"
    ],
    [
     "Yannick",
     "Esteve"
    ]
   ],
   "title": "Data selection and smoothing in an open-source system for the 2008 NIST machine translation evaluation",
   "original": "i08_2727",
   "page_count": 4,
   "order": 736,
   "p1": "2727",
   "pn": "2730",
   "abstract": [
    "This paper gives a detailed description of a statistical machine translation system developed for the 2008 NIST open MT evaluation. The system is based on the open source toolkit Moses with extensions for language model rescoring in a second pass. Significant improvements were obtained with data selection methods for the language and translation model. An improvement of more than 1 point BLEU on the test set was achieved by a continuous space language model which performs the probability estimation with a neural network. The described system has achieved a very good ranking in the 2008 NIST open MT evaluation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-676"
  },
  "kathol08_interspeech": {
   "authors": [
    [
     "Andreas",
     "Kathol"
    ],
    [
     "Jing",
     "Zheng"
    ]
   ],
   "title": "Strategies for building a Farsi-English SMT system from limited resources",
   "original": "i08_2731",
   "page_count": 4,
   "order": 737,
   "p1": "2731",
   "pn": "2734",
   "abstract": [
    "One of the recent tasks for machine translation research has been development of translation capabilities in a time frame as short as 100 days. Such a task requires developers to consider what can be done with relatively small amounts of data in a small time frame. This inherently limits the type and complexity of the effort to be devoted to this task. In this paper we will focus on the kinds of improvements for a Farsi-to-English translation system achieved by means of algorithmic changes, adding raw, domain-unspecific resources, and unsupervised morphological segmentation. The cumulative effect of these measures has been an improvement in BLEU scores of about 25% relative on an internal test set.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-677"
  },
  "kolss08_interspeech": {
   "authors": [
    [
     "Muntsin",
     "Kolss"
    ],
    [
     "Stephan",
     "Vogel"
    ],
    [
     "Alex",
     "Waibel"
    ]
   ],
   "title": "Stream decoding for simultaneous spoken language translation",
   "original": "i08_2735",
   "page_count": 4,
   "order": 738,
   "p1": "2735",
   "pn": "2738",
   "abstract": [
    "In the typical speech translation system, the first-best speech recognizer hypothesis is segmented into sentence-like units which are then fed to the downstream machine translation component. The need for a sufficiently large context in this intermediate step and for the MT introduces delays which are undesirable in many application scenarios, such as real-time subtitling of foreign language broadcasts or simultaneous translation of speeches and lectures.\n",
    "In this paper, we propose a statistical machine translation decoder which processes a continuous input stream, such as that produced by a run-on speech recognizer. By decoupling decisions about the timing of translation output generation from any fixed input segmentation, this design can guarantee a maximum output lag for each input word while allowing for full word reordering within this time window.\n",
    "Experimental results show that this system achieves competitive translation performance with a minimum of translation-induced latency.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-678"
  },
  "ettelaie08_interspeech": {
   "authors": [
    [
     "Emil",
     "Ettelaie"
    ],
    [
     "Panayiotis G.",
     "Georgiou"
    ],
    [
     "Shrikanth S.",
     "Narayanan"
    ]
   ],
   "title": "Towards unsupervised training of the classifier-based speech translator",
   "original": "i08_2739",
   "page_count": 4,
   "order": 739,
   "p1": "2739",
   "pn": "2742",
   "abstract": [
    "Concept classification has been proven to be a useful translation method for speech-to-speech translation applications. However, preparing training data for classifier is a cumbersome task for human annotators. An unsupervised training method is introduced here that is based on utterance clustering. A technique to measure the distance between two utterances, based on the concepts they express, along with an appropriate clustering method has been adapted.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-679"
  },
  "pitrelli08b_interspeech": {
   "authors": [
    [
     "John F.",
     "Pitrelli"
    ],
    [
     "Burn L.",
     "Lewis"
    ],
    [
     "Edward A.",
     "Epstein"
    ],
    [
     "Martin",
     "Franz"
    ],
    [
     "Daniel",
     "Kiecza"
    ],
    [
     "Jerome L.",
     "Quinn"
    ],
    [
     "Ganesh",
     "Ramaswamy"
    ],
    [
     "Amit",
     "Srivastava"
    ],
    [
     "Paola",
     "Virga"
    ]
   ],
   "title": "Aggregating distributed STT, MT, and information extraction engines: the GALE interoperability-demo system",
   "original": "i08_2743",
   "page_count": 4,
   "order": 740,
   "p1": "2743",
   "pn": "2746",
   "abstract": [
    "Natural-language-processing engines are now attaining accuracy sufficient to begin combining them to perform more complex tasks. The GALE Interoperability Demo system consists of 12 engines including speech recognition, translation, and various information extraction engines, interoperated to make Arabic news video browsable as English text grouped and summarized by topic. Unstructured Information Management Architecture serves as the framework enabling remote pipelined and parallelized operation of these engines operating on their native computing platforms at their home sites.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-680"
  },
  "kazemzadeh08_interspeech": {
   "authors": [
    [
     "Abe",
     "Kazemzadeh"
    ],
    [
     "Sungbok",
     "Lee"
    ],
    [
     "Shrikanth S.",
     "Narayanan"
    ]
   ],
   "title": "An interval type-2 fuzzy logic system to translate between emotion-related vocabularies",
   "original": "i08_2747",
   "page_count": 4,
   "order": 741,
   "p1": "2747",
   "pn": "2750",
   "abstract": [
    "This paper describes a novel experiment that demonstrates the feasiblity of a fuzzy logic (FL) representation of emotion-related words used to translate between different emotional vocabularies. Type-2 fuzzy sets were encoded using input from web-based surveys that prompted users with emotional words and asked them to enter an interval using a double slider. The similarity of the encoded fuzzy sets was computed and it was shown that a reliable mapping can be made between a large vocabulary of emotional words and a smaller vocabulary of words naming seven emotion categories. Though the mapping results are comparable to Euclidian distance in the valence/ activation/dominance space, the FL representation has several benefits that are discussed.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-681"
  },
  "huang08f_interspeech": {
   "authors": [
    [
     "Ting",
     "Huang"
    ],
    [
     "Yingchun",
     "Yang"
    ]
   ],
   "title": "Applying pitch-dependent difference detection and modification to emotional speaker recognition",
   "original": "i08_2751",
   "page_count": 4,
   "order": 742,
   "p1": "2751",
   "pn": "2754",
   "abstract": [
    "Emotion is an internal source, which can cause the speaker recognition system performance degradation by inducing extra intraspeaker vocal variability. Several enhancements have been applied to speaker recognition system under emotional speech. However, these methods suffer from the limitation of requiring the emotional speech in training or the emotion state of the speaker in testing. This paper presents a novel approach based on the Pitch-dependent Difference Detection and Modification (PDDM) to overcome the limitation above. In this method, only the neutral speech is used to train the speaker models and the emotional state information is not needed in the testing. Experimental results on MASC show that this method enhances identification rate by 4.7% in the best case compared to the traditional speaker recognition.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-682"
  },
  "neiberg08b_interspeech": {
   "authors": [
    [
     "Daniel",
     "Neiberg"
    ],
    [
     "Kjell",
     "Elenius"
    ]
   ],
   "title": "Automatic recognition of anger in spontaneous speech",
   "original": "i08_2755",
   "page_count": 4,
   "order": 743,
   "p1": "2755",
   "pn": "2758",
   "abstract": [
    "Automatic detection of real life negative emotions in speech has been evaluated using Linear Discriminant Analysis, LDA, with \"classic\" emotion features and a classifier based on Gaussian Mixture Models, GMMs. The latter uses Mel-Frequency Cepstral Coefficients, MFCCs, from a filter bank covering the 300.3400 Hz region to capture spectral shape and formants, and another in the 20.600 Hz region to capture prosody. Both classifiers have been tested on an extensive corpus from Swedish voice controlled telephone services. The results indicate that it is possible to detect anger with reasonable accuracy (average recall 83%) in natural speech and that the GMM method performed better than the LDA one.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-683"
  },
  "nose08_interspeech": {
   "authors": [
    [
     "Takashi",
     "Nose"
    ],
    [
     "Yoichi",
     "Kato"
    ],
    [
     "Makoto",
     "Tachibana"
    ],
    [
     "Takao",
     "Kobayashi"
    ]
   ],
   "title": "An estimation technique of style expressiveness for emotional speech using model adaptation based on multiple-regression HSMM",
   "original": "i08_2759",
   "page_count": 4,
   "order": 744,
   "p1": "2759",
   "pn": "2762",
   "abstract": [
    "This paper describes a technique of estimating style expressiveness for an arbitrary speaker's emotional speech. In the proposed technique, the style expressiveness, representing how much the emotions and/or speaking styles affect the acoustic features, is estimated based on multiple-regression hidden semi-Markov model (MRHSMM). In the model training, we first train average voice model using multiple speakers' neutral style speech. Then, the speakerand style-adapted HSMMs are obtained based on linear transformation from the average voice model with a small amount of the target speaker's data. Finally, MRHSMM of the target speaker is obtained using the adapted models. For given input emotional speech, the style expressiveness is estimated based on maximum likelihood criterion. From the experimental results, we show that the estimated value gives good correspondence to the perceptual rating.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-684"
  },
  "ringeval08_interspeech": {
   "authors": [
    [
     "Fabien",
     "Ringeval"
    ],
    [
     "Mohamed",
     "Chetouani"
    ]
   ],
   "title": "A vowel based approach for acted emotion recognition",
   "original": "i08_2763",
   "page_count": 4,
   "order": 745,
   "p1": "2763",
   "pn": "2766",
   "abstract": [
    "This paper is devoted to the description of a new approach for emotion recognition. Our contribution is based on both the extraction and the characterization of phonemic units such as vowels and consonants, which are provided by a pseudo-phonetic speech segmentation phase combined with a vowel detector. Concerning the emotion recognition task, we explore acoustic and prosodic features from these pseudo-phonetic segments (vowels and consonants), and we compare this approach with traditional voiced and unvoiced segments. The classification is realized by the well-known k-nn classifier (k nearest neighbors) from two different emotional speech databases: Berlin (German) and Aholab (Basque).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-685"
  },
  "mcintyre08_interspeech": {
   "authors": [
    [
     "Gordon",
     "McIntyre"
    ],
    [
     "Roland",
     "Goecke"
    ]
   ],
   "title": "A composite framework for affective sensing",
   "original": "i08_2767",
   "page_count": 4,
   "order": 746,
   "p1": "2767",
   "pn": "2770",
   "abstract": [
    "A system capable of interpreting affect from a speaking face must recognise and fuse signals from multiple cues. Building such a system requires the integration of software components to perform tasks such as image registration, video segmentation, speech recognition and classification. Such software components tend to be idiosyncratic, purpose-built, and driven by scripts and textual configuration files. Integrating components to achieve the necessary degree of flexibility to perform full multimodal affective recognition is challenging. We discuss the key requirements and describe a system to perform multimodal affect sensing which integrates such software components and meets these requirements.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-686"
  },
  "shaukat08_interspeech": {
   "authors": [
    [
     "Arslan",
     "Shaukat"
    ],
    [
     "Ke",
     "Chen"
    ]
   ],
   "title": "Towards automatic emotional state categorization from speech signals",
   "original": "i08_2771",
   "page_count": 4,
   "order": 747,
   "p1": "2771",
   "pn": "2774",
   "abstract": [
    "This paper investigates the performance of automatic emotional state categorization from speech signals on the Serbian Emotional Speech Corpus, named GEES, against the corresponding human performance. We employ a multistage strategy along with sophisticated features used for automatic emotional state categorization. Our study is the first attempt to apply a machine learning technique to the GEES where the human performance was only available prior to our study. Our investigation indicates that the use of a multistage categorization strategy yields behaviors similar to what human perceives and the performance close to human being's.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-687"
  },
  "park08c_interspeech": {
   "authors": [
    [
     "Jeong-Sik",
     "Park"
    ],
    [
     "Ji-Hwan",
     "Kim"
    ],
    [
     "Sang-Min",
     "Yoon"
    ],
    [
     "Yung-Hwan",
     "Oh"
    ]
   ],
   "title": "Speaker-independent emotion recognition based on feature vector classification",
   "original": "i08_2775",
   "page_count": 4,
   "order": 748,
   "p1": "2775",
   "pn": "2778",
   "abstract": [
    "This paper proposes a new feature vector classification for speech emotion recognition. The conventional feature vector classification applied to speaker identification categorized feature vectors as overlapped and non-overlapped. This method discards all of the overlapped vectors in model training, while non-overlapped vectors are used to reconstruct corresponding speaker models. Although the conventional classification showed strong performance in speaker identification, it has limitations in constructing robust models when the number of overlapped vectors is significantly increased such as in emotion recognition. To overcome such a drawback, we propose a more sophisticated classification method which selects discriminative vectors among overlapped vectors and adds the vectors in model reconstruction. On experiments based on an LDC emotion corpus, our classification approach exhibited superior performance when compared to the conventional method.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-688"
  },
  "bresch08_interspeech": {
   "authors": [
    [
     "Erik",
     "Bresch"
    ],
    [
     "Daylen",
     "Riggs"
    ],
    [
     "Louis M.",
     "Goldstein"
    ],
    [
     "Dani",
     "Byrd"
    ],
    [
     "Sungbok",
     "Lee"
    ],
    [
     "Shrikanth S.",
     "Narayanan"
    ]
   ],
   "title": "An analysis of vocal tract shaping in English sibilant fricatives using real-time magnetic resonance imaging",
   "original": "i08_2823",
   "page_count": 4,
   "order": 749,
   "p1": "2823",
   "pn": "2826",
   "abstract": [
    "This study uses real-time MRI to investigate shaping aspects of two English sibilant fricatives. The purpose of this article is to 1) develop linguistically meaningful quantitative measurements based on vocal tract features that robustly capture the shaping aspects of the two fricatives, and 2) provide qualitative analyses of fricative shaping. Data was recorded in both midsagittal and coronal planes. The proposed three quantitative measures of this study provide robust results in categorizing shape. The qualitative analyses describe tongue shape in terms of grooving and doming and they support previous research.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-689"
  },
  "arai08b_interspeech": {
   "authors": [
    [
     "Takayuki",
     "Arai"
    ]
   ],
   "title": "Science workshop with sliding vocal-tract model",
   "original": "i08_2827",
   "page_count": 4,
   "order": 750,
   "p1": "2827",
   "pn": "2830",
   "abstract": [
    "In recent years, we have developed physical models of the vocal tract to promote education in the speech sciences for all ages of students. Beginning with our initial models based on Chiba and Kajiyama's measurements, we have gone on to present many other models to the public. Through science fairs which attract all ages, including elementary through college level students, we have sought to increase awareness of the importance of the speech sciences in the public mind. In this paper we described two science workshops we organized at the National Science Museum in 2006 and 2007 in Japan.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-690"
  },
  "bagou08_interspeech": {
   "authors": [
    [
     "Odile",
     "Bagou"
    ],
    [
     "Ulrich H.",
     "Frauenfelder"
    ]
   ],
   "title": "Segmentation cues in lexical identification and in lexical acquisition: same or different?",
   "original": "i08_2831",
   "page_count": 4,
   "order": 751,
   "p1": "2831",
   "pn": "2834",
   "abstract": [
    "This study investigates how French listeners exploit phonological and phonetic cues in segmenting continuous speech into words. We examined how these listeners integrate multiple sources of information not only in lexical identification, using the cross-modal priming paradigm, but also in the storage of new lexical representations, using an artificial language learning task. Results showed that the specific segmentation cues examined contributed differently to the performance of these two tasks. Syllable onsets, simultaneously cued by allophonic variations and phonotactics, played a predominant role in lexical identification while stress was only a \"last-resort\" segmentation cue. In contrast, rhythmic information, particularly primary stress, played a greater role in lexical acquisition than syllable onsets which were not used. These results suggest that segmentation cues contribute differently to these two processes and that caution is therefore required in relating results on lexical acquisition and lexical identification.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-691"
  },
  "kuijpers08_interspeech": {
   "authors": [
    [
     "Cecile",
     "Kuijpers"
    ],
    [
     "Louis ten",
     "Bosch"
    ]
   ],
   "title": "Phonological representations in poor readers",
   "original": "i08_2835",
   "page_count": 4,
   "order": 752,
   "p1": "2835",
   "pn": "2838",
   "abstract": [
    "According to the 'phonological deficit hypothesis', problems in reading and spelling in dyslexic persons are due to poor representations of sounds in long term memory. As a result, the acquisition of the grapheme-phoneme correspondence and word decoding is assumed to be difficult. In this paper we describe a perception experiment to find evidence for poor phonological representations in 9.13 year old pupils who perform poorly in reading. The experiment is a two-alternative forced choice task in which subjects had to identify an intervocalic consonant in clean and noisy conditions. The results support the phonological deficit hypothesis but do not provide unique cues for the precise manifestation of this deficit. Between normal and poor readers, significant differences have been found with respect to reaction times, but not to accuracy.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-692"
  },
  "buchaillard08_interspeech": {
   "authors": [
    [
     "Stephanie",
     "Buchaillard"
    ],
    [
     "Pascal",
     "Perrier"
    ],
    [
     "Yohan",
     "Payan"
    ]
   ],
   "title": "To what extent does tagged-MRI technique allow to infer tongue muscles' activation pattern? a modelling study",
   "original": "i08_2839",
   "page_count": 4,
   "order": 753,
   "p1": "2839",
   "pn": "2842",
   "abstract": [
    "'Tagged MRI' techniques have been used during the past years to predict which muscles are activated during the production tongue movements. Using this technique, inferences are based on the hypothesis that a significant strain of the anatomical region of a tongue muscle is evidence of voluntary muscle activation. In this paper, we propose to use a 3D finite-element biomechanical model of the oral cavity to study the relation between the distribution of the strains observed in the tongue body and the location of the tongue muscles activated either by the central nervous system or through reflex loops. Results showed in most cases a good correlation between the location of the tongue area that underwent high strains and the location of the muscle activated when studying single muscle activation. However, for movements involving combined muscle activations, a limited and even no correlation is measured. Hence, direct reading of Tagged MRI images would not allow inferring major tongue muscles activated in theses cases.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-693"
  },
  "aboutabit08_interspeech": {
   "authors": [
    [
     "Noureddine",
     "Aboutabit"
    ],
    [
     "Denis",
     "Beautemps"
    ],
    [
     "Olivier",
     "Mathieu"
    ],
    [
     "Laurent",
     "Besacier"
    ]
   ],
   "title": "Feature adaptation of hearing-impaired lip shapes: the vowel case in the cued speech context",
   "original": "i08_2843",
   "page_count": 4,
   "order": 754,
   "p1": "2843",
   "pn": "2846",
   "abstract": [
    "The phonetic translation of Cued Speech (CS) gestures needs to mix the manual CS information together with the lips, taking into account the desynchronization delay (Attina et al. [1], Aboutabit et al. [2]) between these two flows of information. This contribution focuses on the lip flow modeling in the case of French vowels. Previously, classification models have been developed for a professional normal-hearing CS speaker (Aboutabit et al., [3]). These models are used as a reference. In this study, we process the case of a deaf CS speaker and discuss the possibilities of classification. The best performance (92.8%) is obtained with the adaptation of the deaf data to the reference models.\n",
    "s Attina, V., Beautemps, D., Cathiard, M. A. and Odisio, M. \"A pilot study of temporal organization in cued speech production of French syllables: rules for Cued Speech synthesizer,\" Speech Communication, 44, pp. 197-214, 2004. Aboutabit, N., Beautemps, D. and Besacier, L., \"Hand and Lips desynchronization analysis in French Cued Speech: Automatic segmentation of Hand flow\". In Proc. of ICASSP, 2006. Aboutabit, N., Beautemps, D. and Besacier, L., \"Vowels classification from lips: the Cued Speech production case\". In Proceedings of ISSP06, 2006\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-694"
  },
  "veilleux08_interspeech": {
   "authors": [
    [
     "Nanette",
     "Veilleux"
    ],
    [
     "Stefanie",
     "Shattuck-Hufnagel"
    ]
   ],
   "title": "Automatic detection of the context of acoustic landmark deletion",
   "original": "i08_2847",
   "page_count": 4,
   "order": 755,
   "p1": "2847",
   "pn": "2850",
   "abstract": [
    "Earlier work has shown that the acoustic landmarks in speech [1,2], proposed to be important in lexical access, are largely preserved in spontaneous American English Speech [3]. Moreover, the loss of acoustic landmarks predicted from word's lexical representation is systematic and predictable. This study reports preliminary analysis of factors that govern whether a landmark will be realized as predicted, or will be changed or apparently omitted. A classification tree is designed to predict the deletion/change or preservation of a landmark in a corpus of spontaneous American English using contextual factors that include prosody, word structure, morphosyntactic categories and landmark type.\n",
    "s Stevens, K. Acoustic Phonetics. Cambridge, MA: MIT Press, 1998. Stevens, K.N. \"Toward a model for lexical access based on acoustic landmarks and distinctive features\". J.Acoust.Soc. Amer. 111, 1872-1891, 2002 The Robustness of Acoustic Landmarks in Spontaneous Speech\", ICPhS, Saarbrücken, 2007\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-695"
  },
  "ouni08_interspeech": {
   "authors": [
    [
     "Slim",
     "Ouni"
    ]
   ],
   "title": "Aspects of pharyngealized phonemes in Arabic using articulography",
   "original": "i08_2851",
   "page_count": 1,
   "order": 756,
   "p1": "2851",
   "pn": "",
   "abstract": [
    "In this paper, we present preliminary results of an articulatory study of pharyngealized phonemes in Arabic using an electromagnetic articulograph (AG500). We compared the articulations of pharyngealized phonemes to non-pharyngealized ones. The articulation of the tongue was tracked by four sensors glued onto the tongue. A corpus of several CVCVCVs was recorded using the AG500, labeled, and analyzed. The main finding of this work is that while the main articulation of the tongue is actually moving towards a dental position, the secondary articulation of backing of the tongue can be observed.\n",
    ""
   ]
  },
  "beach08_interspeech": {
   "authors": [
    [
     "Elizabeth",
     "Beach"
    ],
    [
     "Christine",
     "Kitamura"
    ],
    [
     "Harvey",
     "Dillon"
    ],
    [
     "Teresa",
     "Ching"
    ],
    [
     "Denis",
     "Burnham"
    ]
   ],
   "title": "The effect of spectral tilt on infants' discrimination of fricatives",
   "original": "i08_2852",
   "page_count": 1,
   "order": 757,
   "p1": "2852",
   "pn": "",
   "abstract": [
    "Research on infants' ability to discriminate speech sounds with different directions of spectral tilt can be used to optimise the frequency response of hearing aids being fitted to very young infants. In the current study, infants were presented with a fricative contrast with either a positive or negative spectral tilt, or in unmodified form. The results showed 6-month-olds were able to discriminate the contrast irrespective of the manipulation, while 9-month-olds could only discriminate the contrast in unmodified form.\n",
    ""
   ]
  },
  "knoll08_interspeech": {
   "authors": [
    [
     "Monja",
     "Knoll"
    ],
    [
     "Lisa",
     "Scharrer"
    ]
   ],
   "title": "look at the shark: evaluation of student produced standardized sentences of infant- and foreigner-directed speech",
   "original": "i08_2853",
   "page_count": 4,
   "order": 758,
   "p1": "2853",
   "pn": "2856",
   "abstract": [
    "We evaluated two speech research paradigms by comparing standardized sentences to 'free speech' (both produced by students without interaction partner). Samples of infant- (IDS), foreigner- (FDS) and adult-directed speech (ADS) were acoustically analyzed for F0 and hyperarticulation. No difference for hyperarticulation was found between IDS, FDS and ADS, which is in contrast to natural speech and imaginary 'free speech'. Differences in mean F0 approximated natural speech (IDS significantly higher than ADS and FDS), and represented an improvement compared to imaginary 'free speech'. The implications of these findings are discussed in the context of experimental speech research.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-696"
  },
  "panchapagesan08_interspeech": {
   "authors": [
    [
     "Sankaran",
     "Panchapagesan"
    ],
    [
     "Abeer",
     "Alwan"
    ]
   ],
   "title": "Vocal tract inversion by cepstral analysis-by-synthesis using chain matrices",
   "original": "i08_2857",
   "page_count": 4,
   "order": 759,
   "p1": "2857",
   "pn": "2860",
   "abstract": [
    "Acoustic-to-articulatory inversion for vowels is performed by cepstral analysis-by-synthesis, using chain-matrix calculation of vocal tract (VT) acoustics and the Maeda articulatory model. The derivative of the VT chain matrix with respect to the area function was calculated in a novel efficient manner, and used in the BFGS quasi- Newton method for optimizing a distance measure between input and synthesized cepstral features over the entire articulatory trajectory. The optimization is initialized by a fast search of an articulatory codebook with a bin structure in formant space and the cost function also includes regularization and continuity terms to obtain realistic inverted VT shapes and smooth articulatory trajectories. Inversion is evaluated on the three diphthongs /ai/, /oi/ and /au/ of two speakers, one male and one female, from the University of Wisconsin X-ray microbeam (XRMB) database, and good agreement was achieved between inverted midsagittal vocal tract outlines and measured XRMB tongue and lip pellet positions, with an average relative error of less than 3% in the first three formants.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-697"
  },
  "alku08_interspeech": {
   "authors": [
    [
     "Paavo",
     "Alku"
    ],
    [
     "Carlo",
     "Magi"
    ],
    [
     "Tom",
     "Bäckström"
    ]
   ],
   "title": "DC-constrained linear prediction for glottal inverse filtering",
   "original": "i08_2861",
   "page_count": 4,
   "order": 760,
   "p1": "2861",
   "pn": "2864",
   "abstract": [
    "Closed phase covariance (CP) analysis is a glottal inverse filtering method which estimates the vocal tract during the glottal closed phase. Since closed phase durations are typically short, the vocal tract computation with linear prediction is vulnerable to the covariance frame position. This study proposes a modified CP algorithm in which a DC-gain constraint is imposed in optimizing the linear predictive inverse model of the vocal tract. Results show that the algorithm improves the robustness of the CP analysis on the covariance frame position.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-698"
  },
  "alm08_interspeech": {
   "authors": [
    [
     "Magnus",
     "Alm"
    ],
    [
     "Dawn",
     "Behne"
    ]
   ],
   "title": "Voicing influences the saliency of place of articulation in audio-visual speech perception in babble",
   "original": "i08_2865",
   "page_count": 4,
   "order": 761,
   "p1": "2865",
   "pn": "2868",
   "abstract": [
    "Previous research has shown that voicing can influence the perception of consonant place of articulation (POA) in audio-visual (AV) speech perception, although findings are inconsistent and often differ with the use of background noise. The prediction in the current study was that the AV perception of voiced and voiceless stop consonant POA is influenced by the differences in spectral distribution between voiced and voiceless stops, a hypothesis not compatible with the direction of the voicing effect shifting with different types of noise. Fifteen young adults were tested using incongruent AV stimuli that differed in POA, in a voiced and a voiceless condition, applying the infrequently used babble noise as background. As predicted participants used the auditory modality to a greater extent identifying the POA of voiced stops compared to voiceless stops. The more distinct spectral distribution of the voiced stops may contribute to them being more easily identified auditorily on the POA dimension than the voiceless stops. The study extends previous research using white noise, by demonstrating a consistent pattern of results in babble noise.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-699"
  },
  "amano08_interspeech": {
   "authors": [
    [
     "Shigeaki",
     "Amano"
    ],
    [
     "Yukari",
     "Hirata"
    ]
   ],
   "title": "Correspondence of perception and production boundaries between single and geminate stops in Japanese",
   "original": "i08_2869",
   "page_count": 4,
   "order": 762,
   "p1": "2869",
   "pn": "2872",
   "abstract": [
    "To specify good variables for representing the perception and production boundaries between Japanese single and geminate stops at various speaking rates, regression analysis and discriminant analysis were conducted for perception data and production data, respectively. The regression function of the closure duration with the word duration with an intercept term predicted the perception boundary between a single and a geminate stop very well (R2=.892), and also discriminated the production of stops with a small error rate (8.73%). At the same time, a discriminant function with the same variables discriminated the production of stops with a small error rate (8.72%), and also predicted their perception boundary very well (R2=.815). In addition, the regression and discriminant functions were very similar. These results suggest that the closure and word durations with an intercept term are good variables for representing the perception and production boundaries of single and geminate stops irrespective of speaking rate, and that these two boundaries coincide with these variables.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-700"
  },
  "yip08_interspeech": {
   "authors": [
    [
     "Michael C. W.",
     "Yip"
    ]
   ],
   "title": "Inhibitory processes of Chinese spoken word recognition",
   "original": "i08_2873",
   "page_count": 4,
   "order": 763,
   "p1": "2873",
   "pn": "2876",
   "abstract": [
    "The present study was designed to examine the inhibitory processes of spoken word recognition of Chinese homophones during sentence comprehension, by using a cross-modal naming experiment. In this experiment, listeners were asked to name aloud a visual probe as fast as they could after hearing a sentence, which ended with a spoken Chinese homophone. Results confirm that preceding sentential context has exerted an early effect on disambiguating among different alternative meanings of the ambiguous word. Secondly, the contextually inappropriate meanings of the ambiguous word would be inhibited rapidly during sentence comprehension. Thirdly, the present results also demonstrated that the inhibitory mechanism could sustain to a longer duration following the occurrence of the ambiguous word.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2008-701"
  }
 },
 "sessions": [
  {
   "title": "Keynote Sessions",
   "papers": [
    "fujisaki08_interspeech",
    "alwan08_interspeech",
    "gonzalezrodriguez08_interspeech",
    "cassell08_interspeech"
   ]
  },
  {
   "title": "Segmentation and Classification",
   "papers": [
    "han08_interspeech",
    "benharush08_interspeech",
    "ikbal08_interspeech",
    "boakye08_interspeech",
    "nguyen08_interspeech",
    "vijayasenan08_interspeech"
   ]
  },
  {
   "title": "Speech Coding",
   "papers": [
    "ramasubramanian08_interspeech",
    "eksler08_interspeech",
    "xu08_interspeech",
    "noureldin08_interspeech",
    "garcia08_interspeech",
    "waltermann08_interspeech"
   ]
  },
  {
   "title": "Human Conversation and Communication",
   "papers": [
    "peters08_interspeech",
    "braun08_interspeech",
    "ishizaki08_interspeech",
    "ponbarry08_interspeech",
    "fernandez08_interspeech",
    "laskowski08_interspeech"
   ]
  },
  {
   "title": "OzPhon08 - Phonetics and Phonology of Australian Aboriginal Languages (Special Session)",
   "papers": [
    "fletcher08_interspeech",
    "loakes08_interspeech",
    "ingram08_interspeech",
    "pentland08_interspeech",
    "turpin08_interspeech"
   ]
  },
  {
   "title": "Acoustic Activity Detection, Pitch Tracking and Analysis",
   "papers": [
    "ishizuka08_interspeech",
    "kang08_interspeech",
    "ding08_interspeech",
    "shafiee08_interspeech",
    "zieger08_interspeech",
    "obuchi08_interspeech",
    "butko08_interspeech",
    "weiss08_interspeech",
    "petrick08_interspeech",
    "hussein08_interspeech",
    "ohishi08_interspeech",
    "vishnubhotla08_interspeech",
    "wohlmayr08_interspeech",
    "li08_interspeech",
    "martin08_interspeech"
   ]
  },
  {
   "title": "Single- and Multichannel Speech Enhancement I, II",
   "papers": [
    "malek08_interspeech",
    "ishibashi08_interspeech",
    "ramasubramanian08b_interspeech",
    "li08b_interspeech",
    "nandk08_interspeech",
    "ding08b_interspeech",
    "lyons08_interspeech",
    "so08_interspeech",
    "kundu08_interspeech",
    "das08_interspeech",
    "miyake08_interspeech",
    "alam08_interspeech",
    "wilson08_interspeech",
    "zou08_interspeech",
    "weiss08b_interspeech",
    "kumatani08_interspeech",
    "furuya08_interspeech",
    "rahmani08_interspeech",
    "tashev08_interspeech",
    "cheng08_interspeech",
    "cho08_interspeech",
    "zhang08_interspeech"
   ]
  },
  {
   "title": "Spoken Language Systems I, II",
   "papers": [
    "komatani08_interspeech",
    "katsumaru08_interspeech",
    "williams08_interspeech",
    "makalic08_interspeech",
    "cesari08_interspeech",
    "conrod08_interspeech",
    "ito08_interspeech",
    "hori08_interspeech",
    "yoshimi08_interspeech",
    "eskenazi08_interspeech",
    "batista08_interspeech",
    "paulik08_interspeech",
    "servan08_interspeech",
    "hoffmeister08_interspeech",
    "hahn08_interspeech",
    "takeuchi08_interspeech",
    "saruwatari08_interspeech",
    "stent08_interspeech",
    "herm08_interspeech",
    "gillick08_interspeech",
    "cevik08_interspeech",
    "camelin08_interspeech",
    "balakrishna08_interspeech",
    "thomson08_interspeech",
    "ikeda08_interspeech",
    "jan08_interspeech",
    "demuynck08_interspeech",
    "vacher08_interspeech",
    "turunen08_interspeech",
    "hain08_interspeech",
    "meinedo08_interspeech"
   ]
  },
  {
   "title": "Emotion and Expression I, II",
   "papers": [
    "suzuki08_interspeech",
    "boufaden08_interspeech",
    "millhouse08_interspeech",
    "kato08_interspeech",
    "begum08_interspeech",
    "busso08_interspeech",
    "krahmer08_interspeech",
    "schuller08_interspeech",
    "goudbeek08_interspeech",
    "truong08_interspeech",
    "arimoto08_interspeech",
    "shaikh08_interspeech",
    "yanushevskaya08_interspeech",
    "swerts08_interspeech"
   ]
  },
  {
   "title": "Automatic Speech Recognition: Acoustic Models I-III",
   "papers": [
    "li08c_interspeech",
    "heigold08_interspeech",
    "kanevsky08_interspeech",
    "liu08_interspeech",
    "yu08_interspeech",
    "droppo08_interspeech",
    "bell08_interspeech",
    "bell08b_interspeech",
    "mak08_interspeech",
    "willett08_interspeech",
    "saon08_interspeech",
    "bolanos08_interspeech",
    "aradilla08_interspeech",
    "shiota08_interspeech",
    "hashimoto08_interspeech",
    "ajmera08_interspeech",
    "shi08_interspeech",
    "hifny08_interspeech",
    "huang08_interspeech",
    "tyagi08_interspeech",
    "srinivasan08_interspeech",
    "cardinal08_interspeech",
    "zhang08b_interspeech",
    "schuller08b_interspeech",
    "ma08_interspeech",
    "sainath08_interspeech",
    "shinozaki08_interspeech",
    "matton08_interspeech",
    "siniscalchi08_interspeech",
    "abad08_interspeech",
    "mcdermott08_interspeech",
    "garau08_interspeech",
    "bartels08_interspeech",
    "hong08_interspeech",
    "pinto08_interspeech",
    "thambiratnam08_interspeech",
    "hu08_interspeech",
    "plahl08_interspeech",
    "vicsi08_interspeech"
   ]
  },
  {
   "title": "Accent and Language Identification",
   "papers": [
    "darcy08_interspeech",
    "castaldo08_interspeech",
    "hubeika08_interspeech",
    "wu08_interspeech",
    "campbell08_interspeech",
    "benzeghiba08_interspeech"
   ]
  },
  {
   "title": "Special Session: PANZE 2008 - Phonetics and Phonology of Australian and New Zealand English",
   "papers": [
    "watson08_interspeech",
    "cox08_interspeech",
    "price08_interspeech",
    "butcher08_interspeech",
    "mannell08_interspeech"
   ]
  },
  {
   "title": "Speaker Recognition and Diarisation",
   "papers": [
    "zajic08_interspeech",
    "dean08_interspeech",
    "markov08_interspeech",
    "mezaache08_interspeech",
    "larcher08_interspeech",
    "gerber08_interspeech",
    "chetty08_interspeech",
    "luque08_interspeech"
   ]
  },
  {
   "title": "Perception, Production, Discourse and Dialog",
   "papers": [
    "beskow08_interspeech",
    "hajek08_interspeech",
    "ma08b_interspeech",
    "leemann08_interspeech",
    "kroos08_interspeech",
    "kuratate08_interspeech",
    "kvale08_interspeech",
    "nishimura08_interspeech",
    "saarni08_interspeech",
    "tietze08_interspeech",
    "winterboer08_interspeech",
    "hattori08_interspeech",
    "sato08_interspeech"
   ]
  },
  {
   "title": "Single-Channel Speech Enhancement",
   "papers": [
    "stark08_interspeech",
    "faubel08_interspeech",
    "brouckxon08_interspeech",
    "mohammadi08_interspeech",
    "alam08b_interspeech",
    "du08_interspeech"
   ]
  },
  {
   "title": "Speech Synthesis Methods I, II",
   "papers": [
    "ling08_interspeech",
    "wu08b_interspeech",
    "yamagishi08_interspeech",
    "conkie08_interspeech",
    "ding08c_interspeech",
    "yanagisawa08_interspeech",
    "pollet08_interspeech",
    "cabral08_interspeech",
    "kominek08_interspeech",
    "saito08_interspeech",
    "tiomkin08_interspeech",
    "webster08_interspeech",
    "agiomyrgiannakis08_interspeech",
    "silen08_interspeech",
    "theobald08_interspeech",
    "cadic08_interspeech",
    "raghavendra08_interspeech",
    "king08_interspeech",
    "strom08_interspeech",
    "neubarth08_interspeech",
    "raitio08_interspeech",
    "sarkar08_interspeech",
    "rustullet08_interspeech"
   ]
  },
  {
   "title": "Speaking Style and Emotion Recognition",
   "papers": [
    "wollmer08_interspeech",
    "seppi08_interspeech",
    "he08_interspeech",
    "shriberg08_interspeech",
    "farrus08_interspeech",
    "sethu08_interspeech"
   ]
  },
  {
   "title": "Special Session: Cross-Linguistic and Developmental Issues in the Perception and Production of Lexical Tone",
   "papers": [
    "mattock08_interspeech",
    "krishnan08_interspeech",
    "ciocca08_interspeech",
    "xu08b_interspeech",
    "zerbian08_interspeech",
    "ishihara08_interspeech"
   ]
  },
  {
   "title": "Special Session: Auditory-Inspired Spectro-Temporal Features I, II",
   "papers": [
    "vinyals08_interspeech",
    "falk08_interspeech",
    "wu08c_interspeech",
    "kubo08_interspeech",
    "markaki08_interspeech",
    "kawahara08_interspeech",
    "yin08_interspeech",
    "petrick08b_interspeech",
    "sivaram08_interspeech",
    "heckmann08_interspeech",
    "zhao08_interspeech",
    "wang08_interspeech",
    "meyer08_interspeech"
   ]
  },
  {
   "title": "Speech Coding, Quality Measurement and Auditory Modelling",
   "papers": [
    "nguyen08b_interspeech",
    "gournay08_interspeech",
    "happel08_interspeech",
    "mittal08_interspeech",
    "ganapathy08_interspeech",
    "flax08_interspeech",
    "flax08b_interspeech",
    "bao08_interspeech",
    "kim08_interspeech",
    "liu08b_interspeech",
    "scholz08_interspeech",
    "gomez08_interspeech"
   ]
  },
  {
   "title": "Accent and Language Recognition",
   "papers": [
    "lyu08_interspeech",
    "tong08_interspeech",
    "torrescarrasquillo08_interspeech",
    "torrescarrasquillo08b_interspeech",
    "lopezmoreno08_interspeech",
    "yin08b_interspeech",
    "lei08_interspeech",
    "matejka08_interspeech",
    "glembek08_interspeech",
    "mehrabani08_interspeech",
    "alotaibi08_interspeech",
    "bi08_interspeech",
    "piat08_interspeech",
    "shen08_interspeech",
    "mccree08_interspeech"
   ]
  },
  {
   "title": "Prosody: Prosodic Structure, Paralinguistic, Non-linguistic and Other Cues",
   "papers": [
    "dockendorf08_interspeech",
    "benton08_interspeech",
    "nariai08_interspeech",
    "woehrling08_interspeech",
    "nakamura08_interspeech",
    "ruiter08_interspeech",
    "chen08_interspeech",
    "khine08_interspeech",
    "knox08_interspeech",
    "wrigley08_interspeech",
    "vlasenko08_interspeech",
    "krahmer08b_interspeech",
    "kitamura08_interspeech",
    "patil08_interspeech"
   ]
  },
  {
   "title": "Automatic Speech Recognition: Language Models I, II",
   "papers": [
    "chen08b_interspeech",
    "arsoy08_interspeech",
    "emami08_interspeech",
    "huang08b_interspeech",
    "liu08c_interspeech",
    "hsu08_interspeech",
    "ohta08_interspeech",
    "duta08_interspeech",
    "pan08_interspeech",
    "yeung08_interspeech",
    "kobayashi08_interspeech",
    "shi08b_interspeech",
    "huijbregts08_interspeech",
    "saykhum08_interspeech",
    "schwarzler08_interspeech",
    "sethy08_interspeech",
    "hahn08b_interspeech",
    "martins08_interspeech",
    "ward08_interspeech",
    "wang08b_interspeech",
    "wilson08b_interspeech",
    "federico08_interspeech"
   ]
  },
  {
   "title": "Speaker Identification and Verification",
   "papers": [
    "kajarekar08_interspeech",
    "zhu08_interspeech",
    "vogt08_interspeech",
    "mclaren08_interspeech",
    "hsieh08_interspeech",
    "aronowitz08_interspeech"
   ]
  },
  {
   "title": "Prosodic Structure and Processing",
   "papers": [
    "bishop08_interspeech",
    "shue08_interspeech",
    "walsh08_interspeech",
    "hakokari08_interspeech",
    "yuan08_interspeech",
    "oreilly08_interspeech"
   ]
  },
  {
   "title": "Robust Automatic Speech Recognition I-III",
   "papers": [
    "nakayama08_interspeech",
    "tsujikawa08_interspeech",
    "kuhne08_interspeech",
    "babaali08_interspeech",
    "ganapathy08b_interspeech",
    "park08_interspeech",
    "takahashi08_interspeech",
    "wang08c_interspeech",
    "chiu08_interspeech",
    "sun08_interspeech",
    "chen08c_interspeech",
    "pettersen08_interspeech",
    "molina08_interspeech",
    "liao08_interspeech",
    "tan08_interspeech",
    "wang08d_interspeech",
    "wang08e_interspeech",
    "liao08b_interspeech",
    "yu08b_interspeech",
    "du08b_interspeech",
    "cui08_interspeech",
    "tsao08_interspeech",
    "lu08_interspeech",
    "pettersen08b_interspeech",
    "kumar08_interspeech",
    "povey08_interspeech",
    "ma08c_interspeech",
    "buera08_interspeech",
    "fukuda08_interspeech",
    "ijima08_interspeech",
    "berkovitch08_interspeech",
    "rennie08_interspeech",
    "gomez08b_interspeech",
    "li08d_interspeech",
    "gales08_interspeech",
    "dalen08_interspeech",
    "chen08d_interspeech",
    "fujimoto08_interspeech",
    "li08e_interspeech"
   ]
  },
  {
   "title": "Speech Analysis and Processing, Voice Conversion and Modification",
   "papers": [
    "pfitzinger08_interspeech",
    "krishnamurthy08_interspeech",
    "pantazis08_interspeech",
    "lu08b_interspeech",
    "messing08_interspeech",
    "tomar08_interspeech",
    "qiao08_interspeech",
    "kalinli08_interspeech",
    "zen08_interspeech",
    "yutani08_interspeech",
    "muramatsu08_interspeech",
    "ohtani08_interspeech",
    "uchimura08_interspeech",
    "toth08_interspeech",
    "dashiell08_interspeech"
   ]
  },
  {
   "title": "Special Session: Tonality in Production and Perception, Language in Australia and New Zealand",
   "papers": [
    "zheng08_interspeech",
    "otake08_interspeech",
    "wang08f_interspeech",
    "hu08b_interspeech",
    "ho08_interspeech",
    "zhu08b_interspeech",
    "promon08_interspeech",
    "so08b_interspeech",
    "bundgaardnielsen08_interspeech",
    "tabain08_interspeech",
    "ross08_interspeech",
    "stevens08_interspeech",
    "starks08_interspeech"
   ]
  },
  {
   "title": "Automatic Speech Recognition: Tone Languages",
   "papers": [
    "ding08d_interspeech",
    "yeung08b_interspeech",
    "cheng08b_interspeech",
    "ru08_interspeech",
    "bao08b_interspeech",
    "nguyen08c_interspeech"
   ]
  },
  {
   "title": "Spoken Dialogue Systems",
   "papers": [
    "thomson08b_interspeech",
    "zweig08_interspeech",
    "meurs08_interspeech",
    "paek08_interspeech",
    "kim08b_interspeech",
    "williams08b_interspeech"
   ]
  },
  {
   "title": "Cross-Language and Language-Specific Phonetics",
   "papers": [
    "bundgaardnielsen08b_interspeech",
    "ali08_interspeech",
    "wang08g_interspeech",
    "tsukada08_interspeech",
    "antoniou08_interspeech",
    "hazan08_interspeech"
   ]
  },
  {
   "title": "Special Session: Prosody of Spontaneous Speech I, II",
   "papers": [
    "mixdorff08_interspeech",
    "lindstrom08_interspeech",
    "tseng08_interspeech",
    "obin08_interspeech",
    "ishi08_interspeech",
    "jones08_interspeech",
    "ochi08_interspeech",
    "godin08_interspeech",
    "lee08_interspeech",
    "mori08_interspeech",
    "yin08c_interspeech",
    "moniz08_interspeech",
    "strangert08_interspeech",
    "kousidis08_interspeech",
    "kawahara08b_interspeech"
   ]
  },
  {
   "title": "Automatic Speech Recognition: Adaptation I, II",
   "papers": [
    "karafiat08_interspeech",
    "hahm08_interspeech",
    "su08_interspeech",
    "tanji08_interspeech",
    "sanand08_interspeech",
    "ting08_interspeech",
    "chen08e_interspeech",
    "povey08b_interspeech",
    "raut08_interspeech",
    "loof08_interspeech",
    "povey08c_interspeech",
    "huang08c_interspeech",
    "akhil08_interspeech",
    "wang08h_interspeech"
   ]
  },
  {
   "title": "Features for Speech and Speaker Recognition",
   "papers": [
    "fan08_interspeech",
    "habib08_interspeech",
    "dhananjaya08_interspeech",
    "segura08_interspeech",
    "hou08_interspeech",
    "varadarajan08_interspeech",
    "abdulla08_interspeech",
    "kobayashi08b_interspeech",
    "golipour08_interspeech",
    "qiao08b_interspeech",
    "giacobello08_interspeech",
    "zhang08c_interspeech",
    "saito08b_interspeech",
    "maragakis08_interspeech"
   ]
  },
  {
   "title": "Speaker Recognition: Kernel-Based and Session Mismatch",
   "papers": [
    "okamoto08_interspeech",
    "lu08c_interspeech",
    "zhao08b_interspeech",
    "longworth08_interspeech",
    "ferrer08_interspeech",
    "charlet08_interspeech",
    "zhang08d_interspeech",
    "lee08b_interspeech",
    "kenny08_interspeech",
    "vogt08b_interspeech",
    "luo08_interspeech",
    "garciaromero08_interspeech",
    "ito08b_interspeech",
    "matrouf08_interspeech",
    "liu08d_interspeech"
   ]
  },
  {
   "title": "Broadcast Transcription Systems",
   "papers": [
    "lamel08_interspeech",
    "fousek08_interspeech",
    "vergyri08_interspeech",
    "gollan08_interspeech",
    "hsiao08_interspeech",
    "deoras08_interspeech"
   ]
  },
  {
   "title": "Voice Conversion and Modification",
   "papers": [
    "helander08_interspeech",
    "pozo08_interspeech",
    "tani08_interspeech",
    "tran08_interspeech",
    "tran08b_interspeech"
   ]
  },
  {
   "title": "Phonetics: General",
   "papers": [
    "scharenborg08_interspeech",
    "strik08_interspeech",
    "yegnanarayana08_interspeech",
    "neiberg08_interspeech",
    "zhuang08_interspeech"
   ]
  },
  {
   "title": "Special Session: Forensic Speaker Recognition - Traditional and Automatic Approaches",
   "papers": [
    "ramos08_interspeech",
    "thiruvaran08_interspeech",
    "morrison08_interspeech",
    "becker08_interspeech",
    "shriberg08b_interspeech"
   ]
  },
  {
   "title": "Automatic Speech Recognition: Features I, II",
   "papers": [
    "lee08c_interspeech",
    "razik08_interspeech",
    "thomas08_interspeech",
    "sangwan08_interspeech",
    "huda08_interspeech",
    "hu08c_interspeech",
    "ramya08_interspeech",
    "abbasian08_interspeech",
    "narayana08_interspeech",
    "frankel08_interspeech",
    "karjigi08_interspeech",
    "kogure08_interspeech",
    "valente08_interspeech",
    "tyagi08b_interspeech",
    "chang08_interspeech",
    "lee08d_interspeech",
    "sanand08b_interspeech",
    "fukuda08b_interspeech"
   ]
  },
  {
   "title": "Speech Resources and Technology Evaluation",
   "papers": [
    "kawahara08c_interspeech",
    "matousek08_interspeech",
    "shen08b_interspeech",
    "kato08b_interspeech",
    "schuppler08_interspeech",
    "kotnik08_interspeech",
    "draxler08_interspeech",
    "demenko08_interspeech",
    "pitrelli08_interspeech",
    "li08f_interspeech",
    "engelbrecht08_interspeech",
    "yamakawa08_interspeech",
    "busso08b_interspeech"
   ]
  },
  {
   "title": "Applications in Education and Learning I, II",
   "papers": [
    "deshmukh08_interspeech",
    "bolanos08b_interspeech",
    "wang08i_interspeech",
    "li08g_interspeech",
    "wang08j_interspeech",
    "black08_interspeech",
    "black08b_interspeech",
    "harrison08_interspeech",
    "cucchiarini08_interspeech",
    "samir08_interspeech",
    "pedersen08_interspeech",
    "togashi08_interspeech",
    "luo08b_interspeech",
    "chevalier08_interspeech",
    "ge08_interspeech",
    "ito08c_interspeech"
   ]
  },
  {
   "title": "Speech Pathologies",
   "papers": [
    "kim08c_interspeech",
    "middag08_interspeech",
    "ma08d_interspeech",
    "bruijn08_interspeech",
    "maier08_interspeech",
    "morales08_interspeech"
   ]
  },
  {
   "title": "Special Session: Consonant Challenge . Human-Machine Comparisons of Consonant Recognition in Noise",
   "papers": [
    "cooke08_interspeech",
    "borgstrom08_interspeech",
    "yoon08_interspeech",
    "jancovic08_interspeech",
    "garcialecumberri08_interspeech",
    "gemmeke08_interspeech",
    "schuller08c_interspeech",
    "hodoshima08_interspeech",
    "lobdell08_interspeech"
   ]
  },
  {
   "title": "Automatic Speech Recognition: Lexical and Prosodic Models",
   "papers": [
    "tan08b_interspeech",
    "aronowitz08b_interspeech",
    "onishi08_interspeech",
    "tepperman08_interspeech",
    "garner08_interspeech",
    "sasada08_interspeech"
   ]
  },
  {
   "title": "Speaker Recognition: Adverse Conditions and Forensics",
   "papers": [
    "jin08_interspeech",
    "huang08d_interspeech",
    "kwon08_interspeech",
    "suh08_interspeech",
    "kim08d_interspeech",
    "zhang08e_interspeech",
    "amino08_interspeech",
    "das08b_interspeech",
    "das08c_interspeech",
    "tsuge08_interspeech",
    "toledano08_interspeech",
    "zhang08f_interspeech",
    "ishihara08b_interspeech",
    "leung08_interspeech",
    "zieger08b_interspeech"
   ]
  },
  {
   "title": "Phonetics: Development, Learning, Cross-Language and Language-Specific",
   "papers": [
    "braun08b_interspeech",
    "reinisch08_interspeech",
    "ishikawa08_interspeech",
    "bunnell08_interspeech",
    "yuan08b_interspeech",
    "joto08_interspeech",
    "weiss08c_interspeech",
    "ojala08_interspeech",
    "tyler08_interspeech",
    "lam08_interspeech",
    "klintfors08_interspeech",
    "gustavsson08_interspeech",
    "rasanen08_interspeech",
    "katagiri08_interspeech",
    "masuda08_interspeech"
   ]
  },
  {
   "title": "Multimodal Signal Processing",
   "papers": [
    "moubayed08_interspeech",
    "takahashi08b_interspeech",
    "wang08k_interspeech",
    "hueber08_interspeech",
    "hueber08b_interspeech",
    "trmal08_interspeech"
   ]
  },
  {
   "title": "-Speech Perception",
   "papers": [
    "davis08_interspeech",
    "ali08b_interspeech",
    "zevin08_interspeech",
    "lentz08_interspeech",
    "cutler08_interspeech",
    "cvejic08_interspeech"
   ]
  },
  {
   "title": "Evaluation and Standardisation of Spoken-Language Technology",
   "papers": [
    "cao08_interspeech",
    "gautierturbin08_interspeech",
    "murthy08_interspeech",
    "park08b_interspeech",
    "durin08_interspeech",
    "soronen08_interspeech"
   ]
  },
  {
   "title": "Automatic Speech Recognition: Search Methods",
   "papers": [
    "bouselmi08_interspeech",
    "driesen08_interspeech",
    "tang08_interspeech",
    "bertoldi08_interspeech",
    "gu08_interspeech",
    "bourke08_interspeech",
    "mamou08_interspeech",
    "oonishi08_interspeech"
   ]
  },
  {
   "title": "Speech Synthesis: Prosody and Emotion I, II",
   "papers": [
    "takeda08_interspeech",
    "badino08_interspeech",
    "inanoglu08_interspeech",
    "qian08_interspeech",
    "boidin08_interspeech",
    "aguero08_interspeech",
    "tang08b_interspeech",
    "hu08d_interspeech",
    "gao08_interspeech",
    "thangthai08_interspeech",
    "latorre08_interspeech",
    "adell08_interspeech",
    "turk08_interspeech",
    "tepperman08b_interspeech"
   ]
  },
  {
   "title": "Language Information Retrieval Systems",
   "papers": [
    "meng08_interspeech",
    "shao08_interspeech",
    "morenodaniel08_interspeech",
    "turunen08b_interspeech",
    "wu08d_interspeech",
    "akiba08_interspeech",
    "drioli08_interspeech",
    "terao08_interspeech",
    "chaudhari08_interspeech",
    "ju08_interspeech",
    "amaral08_interspeech",
    "olsson08_interspeech",
    "lin08_interspeech",
    "iwata08_interspeech"
   ]
  },
  {
   "title": "Applications for the Aged and Handicapped",
   "papers": [
    "dharo08_interspeech",
    "beskow08b_interspeech",
    "taft08_interspeech",
    "smith08_interspeech",
    "nakamura08b_interspeech",
    "mckechnie08_interspeech",
    "pouchoulin08_interspeech",
    "yin08d_interspeech",
    "alpan08_interspeech",
    "carmichael08_interspeech",
    "darcy08b_interspeech",
    "matsumasa08_interspeech",
    "fraga08_interspeech"
   ]
  },
  {
   "title": "Human Speech Production",
   "papers": [
    "lee08e_interspeech",
    "carne08_interspeech",
    "fang08_interspeech",
    "garnier08_interspeech",
    "qin08_interspeech"
   ]
  },
  {
   "title": "Special Session: LIPS 2008 - Visual Speech Synthesis Challenge",
   "papers": [
    "theobald08b_interspeech",
    "hofer08_interspeech",
    "bailly08_interspeech",
    "theobald08c_interspeech",
    "zoric08_interspeech",
    "fagel08_interspeech",
    "fagel08b_interspeech",
    "krnoul08_interspeech",
    "liu08e_interspeech",
    "yan08_interspeech",
    "wang08l_interspeech",
    "matusov08_interspeech",
    "zheng08b_interspeech",
    "sarikaya08_interspeech",
    "lavecchia08_interspeech",
    "reddy08_interspeech",
    "lane08_interspeech"
   ]
  },
  {
   "title": "Spoken Language: Parsing and Summarisation",
   "papers": [
    "maskey08_interspeech",
    "riedhammer08_interspeech",
    "fujii08_interspeech",
    "zhu08c_interspeech",
    "wang08m_interspeech",
    "plank08_interspeech",
    "ohno08_interspeech",
    "gajjar08_interspeech"
   ]
  },
  {
   "title": "Multimodal Interfaces",
   "papers": [
    "melto08_interspeech",
    "turunen08c_interspeech",
    "metze08_interspeech",
    "nakano08_interspeech",
    "yin08e_interspeech",
    "okamoto08b_interspeech",
    "gandhe08_interspeech",
    "rozak08_interspeech"
   ]
  },
  {
   "title": "Speech, Music, Audio Segmentation and Classification",
   "papers": [
    "gao08b_interspeech",
    "ntalampiras08_interspeech",
    "tantibundhit08_interspeech",
    "mitra08_interspeech",
    "zhang08g_interspeech",
    "grasic08_interspeech",
    "germesin08_interspeech",
    "nwe08_interspeech",
    "maddage08_interspeech",
    "molla08_interspeech",
    "wu08e_interspeech",
    "gupta08_interspeech",
    "hazen08_interspeech",
    "trancoso08_interspeech"
   ]
  },
  {
   "title": "Automatic Speech Recognition: New Paradigms",
   "papers": [
    "vipperla08_interspeech",
    "vanhamme08_interspeech",
    "mohapatra08_interspeech",
    "subramanya08_interspeech",
    "doremalen08_interspeech",
    "bosch08_interspeech"
   ]
  },
  {
   "title": "Speech and Acoustic Activity Detection",
   "papers": [
    "kaushik08_interspeech",
    "chaitanya08_interspeech",
    "anemuller08_interspeech",
    "pham08_interspeech",
    "muller08_interspeech",
    "temko08_interspeech"
   ]
  },
  {
   "title": "Speech Analysis and Processing",
   "papers": [
    "kim08e_interspeech",
    "stark08b_interspeech",
    "glaser08_interspeech",
    "ramamurty08_interspeech",
    "deshmukh08b_interspeech",
    "cox08b_interspeech"
   ]
  },
  {
   "title": "Special Session: Talking Heads and Pronunciation Training",
   "papers": [
    "hazan08b_interspeech",
    "massaro08_interspeech",
    "wik08_interspeech",
    "engwall08_interspeech",
    "badin08_interspeech",
    "kroger08_interspeech",
    "fagel08c_interspeech",
    "hofe08_interspeech",
    "arai08_interspeech",
    "huang08e_interspeech"
   ]
  },
  {
   "title": "Multimodal Speech Processing",
   "papers": [
    "wang08n_interspeech",
    "ejarque08_interspeech",
    "meltzner08_interspeech",
    "lewis08_interspeech",
    "watanabe08_interspeech",
    "lucey08_interspeech"
   ]
  },
  {
   "title": "Cross-Lingual and Multilingual Automatic Speech Recognition, Speech Translation",
   "papers": [
    "nouza08_interspeech",
    "lyu08b_interspeech",
    "white08_interspeech",
    "toth08b_interspeech",
    "zhao08c_interspeech",
    "bouselmi08b_interspeech",
    "singla08_interspeech",
    "scanzio08_interspeech",
    "sim08_interspeech",
    "liu08f_interspeech",
    "sridhar08_interspeech",
    "schwenk08_interspeech",
    "kathol08_interspeech",
    "kolss08_interspeech",
    "ettelaie08_interspeech",
    "pitrelli08b_interspeech"
   ]
  },
  {
   "title": "Expression, Emotion and Personality Recognition",
   "papers": [
    "kazemzadeh08_interspeech",
    "huang08f_interspeech",
    "neiberg08b_interspeech",
    "nose08_interspeech",
    "ringeval08_interspeech",
    "mcintyre08_interspeech",
    "shaukat08_interspeech",
    "park08c_interspeech"
   ]
  },
  {
   "title": "Human Speech Production and Speech Perception",
   "papers": [
    "bresch08_interspeech",
    "arai08b_interspeech",
    "bagou08_interspeech",
    "kuijpers08_interspeech",
    "buchaillard08_interspeech",
    "aboutabit08_interspeech",
    "veilleux08_interspeech",
    "ouni08_interspeech",
    "beach08_interspeech",
    "knoll08_interspeech",
    "panchapagesan08_interspeech",
    "alku08_interspeech",
    "alm08_interspeech",
    "amano08_interspeech",
    "yip08_interspeech"
   ]
  }
 ],
 "doi": "10.21437/Interspeech.2008"
}