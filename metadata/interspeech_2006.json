{
 "title": "Interspeech 2006",
 "location": "Pittsburgh, PA, USA",
 "startDate": "17/9/2006",
 "endDate": "21/9/2006",
 "chair": "General Chair: Richard Stern",
 "conf": "Interspeech",
 "year": "2006",
 "name": "interspeech_2006",
 "series": "Interspeech",
 "SIG": "",
 "title1": "Interspeech 2006",
 "date": "17-21 September 2006",
 "booklet": "interspeech_2006.pdf",
 "papers": {
  "purver06_interspeech": {
   "authors": [
    [
     "Matthew",
     "Purver"
    ],
    [
     "Florin",
     "Ratiu"
    ],
    [
     "Lawrence",
     "Cavedon"
    ]
   ],
   "title": "Robust interpretation in dialogue by combining confidence scores with contextual features",
   "original": "i06_1314",
   "page_count": 4,
   "order": 1,
   "p1": "paper 1314-Mon1A1O.1",
   "pn": "",
   "abstract": [
    "We present an approach to dialogue management and interpretation that evaluates and selects amongst candidate dialogue moves based on features at multiple levels. Multiple interpretation methods can be combined, multiple speech recognition and parsing hypotheses tested, and multiple candidate dialogue moves considered to choose the highest scoring hypothesis overall. We integrate hypotheses generated from shallow slot-filling methods and from relatively deep parsing, using pragmatic information. We show that this gives more robust performance than using either approach alone, allowing n-best list reordering to correct errors in speech recognition or parsing.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-1"
  },
  "ye06_interspeech": {
   "authors": [
    [
     "Hui",
     "Ye"
    ],
    [
     "Steve",
     "Young"
    ]
   ],
   "title": "A clustering approach to semantic decoding",
   "original": "i06_1118",
   "page_count": 4,
   "order": 2,
   "p1": "paper 1118-Mon1A1O.2",
   "pn": "",
   "abstract": [
    "This paper presents a novel algorithm for semantic decoding in spoken language understanding systems. Unlike conventional semantic parsers which either use hand-crafted rules or statistical models trained from fully annotated data, the proposed approach uses an unsupervised sentence clustering technique called Y-clustering to automatically select a set of exemplar sentences from a training corpus. These exemplars are combined with simple sentence-level semantic annotations to form templates which are then used for semantic decoding. The performance of this approach was evaluated in the travel domain using the ATIS corpus. Training is fast and cheap, and the results are significantly better than those achieved using HMM-based or stack-based statistical parsers.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-2"
  },
  "misu06_interspeech": {
   "authors": [
    [
     "Teruhisa",
     "Misu"
    ],
    [
     "Tatsuya",
     "Kawahara"
    ]
   ],
   "title": "A bootstrapping approach for developing language model of new spoken dialogue systems by selecting web texts",
   "original": "i06_1167",
   "page_count": 4,
   "order": 3,
   "p1": "paper 1167-Mon1A1O.3",
   "pn": "",
   "abstract": [
    "This paper proposes a bootstrapping method of constructing statistical language models for new spoken dialogue systems by collecting and selecting sentences from the World Wide Web (WWW). To make effective search queries that cover the target domain in full detail, we exploit the document set described about the target domain as seeding data. An important issue is how to filter the retrieved Web pages, since all of the retrieved Web texts are not necessarily suitable as training data. We induct an existing dialogue corpus of different domain to prefer the texts of spoken style. The proposed method was evaluated on two different tasks of software support and sightseeing guidance, and significant reduction of the word error rate was achieved. We show that it is vital to incorporate the dialogue corpus, though not relevant to the target domain, in the text selection phase.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-3"
  },
  "horndasch06_interspeech": {
   "authors": [
    [
     "Axel",
     "Horndasch"
    ],
    [
     "Elmar",
     "Nöth"
    ],
    [
     "Anton",
     "Batliner"
    ],
    [
     "Volker",
     "Warnke"
    ]
   ],
   "title": "Phoneme-to-grapheme mapping for spoken inquiries to the semantic web",
   "original": "i06_1635",
   "page_count": 4,
   "order": 4,
   "p1": "paper 1635-Mon1A1O.4",
   "pn": "",
   "abstract": [
    "Automatic methods for grapheme-to-phoneme (G2P) and phoneme-to-grapheme (P2G) conversion have become very popular in recent years. Their performance has improved considerably, while at the same time these developments required less input from expert lexicographers. Continuing in this tradition we will present in this paper a data-driven, language-independent approach called MASSIVE with which it is possible to create efficient online modules for automatic symbol mapping. Our framework is solely based on statistical methods for training and run-time and has been optimized for P2G conversion in the context of spoken inquiries to the Semantic Web, an issue researched in the SmartWeb project. MASSIVE systems can be trained using a pronunciation lexicon, the output of a phone recognizer or any other suitable set of corresponding symbol strings. Successful tests have been performed on German and English data sets.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-4"
  },
  "weilhammer06_interspeech": {
   "authors": [
    [
     "Karl",
     "Weilhammer"
    ],
    [
     "Matthew N.",
     "Stuttle"
    ],
    [
     "Steve",
     "Young"
    ]
   ],
   "title": "Bootstrapping language models for dialogue systems",
   "original": "i06_1482",
   "page_count": 4,
   "order": 5,
   "p1": "paper 1482-Mon1A1O.5",
   "pn": "",
   "abstract": [
    "We report results on rapidly building language models for dialogue systems. Our base line is a recogniser using a grammar network. We show that we can almost halve the word error rate (WER) by combining language models generated from a simple task grammar with a standard speech corpus and data collected from the web using a sentence selection algorithm based on relative perplexity. This model compares very well to a language model using \"in-domain\" data from a Wizard Of Oz (WOZ) collection. We strongly advocate the use of statistical language models (SLMs) in speech recognisers for dialogue systems and show that costly WOZ data collections are not necessary to build SLMs.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-5"
  },
  "feng06_interspeech": {
   "authors": [
    [
     "Junlan",
     "Feng"
    ]
   ],
   "title": "Question answering with discriminative learning algorithms",
   "original": "i06_1642",
   "page_count": 4,
   "order": 6,
   "p1": "paper 1642-Mon1A1O.6",
   "pn": "",
   "abstract": [
    "In this paper, we describe a discriminative learning approach for question answering. Our training corpus consists of (2) million Frequently Asked Questions (FAQs) and their corresponding answers that we mined from the World Wide Web. This corpus is used to train the lexical and semantic association model between questions and answers. We evaluate our approach on two question answering tasks: 2003 Text Retrieval Conference Question Answering task, and finding answers to FAQs. In both cases, the proposed approach achieved significant improvements over the results for an information retrieval based question answering model.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-6"
  },
  "kenny06_interspeech": {
   "authors": [
    [
     "Patrick",
     "Kenny"
    ],
    [
     "Vishwa",
     "Gupta"
    ],
    [
     "G.",
     "Boulianne"
    ],
    [
     "Pierre",
     "Ouellet"
    ],
    [
     "Pierre",
     "Dumouchel"
    ]
   ],
   "title": "Feature normalization using smoothed mixture transformations",
   "original": "i06_1026",
   "page_count": 4,
   "order": 7,
   "p1": "paper 1026-Mon1A2O.1",
   "pn": "",
   "abstract": [
    "We propose a method for estimating the parameters of SPLICE-like transformations from individual utterances so that this type of transformation can be used to normalize acoustic feature vectors for speech recognition on an utterance-by-utterance basis in a similar manner to cepstral mean normalization. We report results on an in-house French language multi-speaker database collected while deploying an automatic closed-captioning system for live broadcast news. An unusual feature of this database is that there are very large amounts of training data for the individual speakers (typically several hours) so that it is very difficult to improve on multi-speaker modeling by using standard methods of speaker adaptation. We found that the proposed method of feature normalization is capable of achieving a 6% relative improvement over cepstral mean normalization on this task.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-7"
  },
  "hsieh06_interspeech": {
   "authors": [
    [
     "Chia-Hsin",
     "Hsieh"
    ],
    [
     "Chung-Hsien",
     "Wu"
    ],
    [
     "Jun-Yu",
     "Lin"
    ]
   ],
   "title": "Stochastic vector mapping-based feature enhancement using prior model and environment adaptation for noisy speech recognition",
   "original": "i06_1170",
   "page_count": 4,
   "order": 8,
   "p1": "paper 1170-Mon1A2O.2",
   "pn": "",
   "abstract": [
    "This paper presents an approach to feature enhancement for noisy speech recognition. Three prior models are introduced to characterize clean speech, noise and noisy speech respectively using sequential noise estimation based on noise-normalized stochastic vector mapping. Environment adaptation is also adopted to reduce the mismatch between training data and test data. For AURORA2 database, the experimental results indicate that a 0.77% digit accuracy improvement for multi-condition training and 0.29% digit accuracy improvement for clean speech training were achieved without stereo training data compared to the SPLICE-based approach with recursive noise estimation. For MAT-BN Mandarin broadcast news database, a (2).6% syllable accuracy improvement for anchor speech and 4.2% syllable accuracy improvement for field report speech were obtained compared to the MCE-based approach.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-8"
  },
  "nasersharif06_interspeech": {
   "authors": [
    [
     "Babak",
     "Nasersharif"
    ],
    [
     "Ahmad",
     "Akbari"
    ]
   ],
   "title": "A framework for robust MFCC feature extraction using SNR-dependent compression of enhanced mel filter bank energies",
   "original": "i06_1632",
   "page_count": 4,
   "order": 9,
   "p1": "paper 1632-Mon1A2O.3",
   "pn": "",
   "abstract": [
    "The Mel-frequency cepstral coefficients (MFCC) are most widely used and successful features for speech recognition. But, their performance degrades in presence of additive noise. In this paper, we propose a noise compensation method for Mel filter bank energies and so MFCC features. This compensation method includes two steps: Mel sub-band spectral subtraction and then compression of Mel-Sub-band energies. In the compression step, we propose a sub-band SNR-dependent compression function. We use this function instead of logarithm function in conventional MFCC feature extraction in presence of additive noise. Experimental results show that the proposed method significantly improves MFCC features performance in noisy conditions where it decreases word error rate about 70% in SNR value of 0 dB for different types of additive noise.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-9"
  },
  "faubel06_interspeech": {
   "authors": [
    [
     "Friedrich",
     "Faubel"
    ],
    [
     "Matthias",
     "Wölfel"
    ]
   ],
   "title": "Coupling particle filters with automatic speech recognition for speech feature enhancement",
   "original": "i06_1683",
   "page_count": 4,
   "order": 10,
   "p1": "paper 1683-Mon1A2O.4",
   "pn": "",
   "abstract": [
    "This paper addresses robust speech feature extraction in combination with statistical speech feature enhancement and couples the particle filter to the speech recognition hypotheses. To extract noise robust features the Fourier transformation is replaced by the warped and scaled minimum variance distortionless response spectral envelope. To enhance the features, particle filtering has been used. Further, we show that the robust extraction and statistical enhancement can be combined to good effect. One of the critical aspects in particle filter design is the particle weight calculation which is traditionally based on a general, time independent speech model approximated by a Gaussian mixture distribution. We replace this general, time independent speech model by time- and phoneme-specific models. The knowledge of the phonemes to be used is obtained by the hypothesis of a speech recognition system, therefore establishing a coupling between the particle filter and the speech recognition system which have been treated as independent components in the past.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-10"
  },
  "hsu06_interspeech": {
   "authors": [
    [
     "Chang-wen",
     "Hsu"
    ],
    [
     "Lin-shan",
     "Lee"
    ]
   ],
   "title": "Extension and further analysis of higher order cepstral moment normalization (HOCMN) for robust features in speech recognition",
   "original": "i06_1748",
   "page_count": 4,
   "order": 11,
   "p1": "paper 1748-Mon1A2O.5",
   "pn": "",
   "abstract": [
    "Cepstral normalization has been popularly used as a powerful approach to produce robust features for speech recognition. Good examples of approaches include the well known Cepstral Mean Subtraction (CMS) and Cepstral Mean and Variance Normalization (CMVN), in which either the first or both the first and the second moments of the Mel-frequency Cepstral Coefficients (MFCCs) are normalized [1, 2]. Such approaches were extended previously to Higher Order Cepstral Moment Normalization (HOCMN) for normalizing moments with orders much higher than two [3]. Here we further extend HOCMN to a more generalized form with the generalized moment with non-integer orders defined in this paper. Extensive experimental results based on a newly defined development set for AURORA 2.0 indicated that not only HOCMN for integer moment orders can perform significantly better than the well-known approach of Histogram Equalization (HEQ), but some further improvements can be consistently obtained for almost all SNR values with non-integer moment orders. The theoretical foundation behind the approaches proposed here which explains why HOCMN can perform well and how the statistical properties of the distributions of the MFCC parameters are adjusted during the normalization processes were also discussed.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-11"
  },
  "islam06_interspeech": {
   "authors": [
    [
     "Md. Babul",
     "Islam"
    ],
    [
     "Hiroshi",
     "Matsumoto"
    ],
    [
     "Kazumasa",
     "Yamamoto"
    ]
   ],
   "title": "An improved mel-wiener filter for mel-LPC based speech recognition",
   "original": "i06_1830",
   "page_count": 4,
   "order": 12,
   "p1": "paper 1830-Mon1A2O.6",
   "pn": "",
   "abstract": [
    "We previously proposed a Mel-Wiener filter to enhance Mel-LPC spectra in presence of additive noise. The proposed filter was estimated based on minimization of sum of square error on the linear frequency scale and efficiently implemented in the autocorrelation domain without denoising input speech. In the previously proposed system we segregated speech and noise using an energy based VAD and a very simple flooring technique were used for noise segment. In this present work, we improve the VAD using autoregressive (AR) model of noise and flooring technique as well. In addition, a lag window is applied to the estimated noise autocorrelation function to smooth the fine spectra of high order autocorrelation coefficients. As a result, substantial improvement is obtained over previous result.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-12"
  },
  "hurtado06_interspeech": {
   "authors": [
    [
     "Lluis F.",
     "Hurtado"
    ],
    [
     "David",
     "Griol"
    ],
    [
     "Encarna",
     "Segarra"
    ],
    [
     "Emilio",
     "Emilio"
    ],
    [
     "Sanchis",
     "Sanchis"
    ]
   ],
   "title": "A stochastic approach for dialog management based on neural networks",
   "original": "i06_1206",
   "page_count": 4,
   "order": 13,
   "p1": "paper 1206-Mon1A3O.1",
   "pn": "",
   "abstract": [
    "In this article, we present an approach for the construction of a stochastic dialog manager, in which the system answer is selected by means of a classification procedure. In particular, we use neural networks for the implementation of this classification process, which takes into account the data supplied by the user and the last system turn. The stochastic model is automatically learnt from training data which are labeled in terms of dialog acts. An important characteristic of this approach is the introduction of a partition in the space of sequences of dialog acts in order to deal with the scarcity of available training data. This system has been developed in the DIHANA project, whose goal is the design and development of a dialog system to access a railway information system using spontaneous speech in Spanish. An evaluation of this approach is also presented.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-13"
  },
  "rotaru06_interspeech": {
   "authors": [
    [
     "Mihai",
     "Rotaru"
    ],
    [
     "Diane J.",
     "Litman"
    ]
   ],
   "title": "Discourse structure and speech recognition problems",
   "original": "i06_1650",
   "page_count": 4,
   "order": 14,
   "p1": "paper 1650-Mon1A3O.2",
   "pn": "",
   "abstract": [
    "We study dependencies between discourse structure and speech recognition problems (SRP) in a corpus of speech-based computer tutoring dialogues. This analysis can inform us whether there are places in the discourse structure prone to more SRP. We automatically extract the discourse structure by taking advantage of how the tutoring information is encoded in our system. To quantify the discourse structure, we extract two features for each system turn: depth of the turn in the discourse structure and the type of transition from the previous turn to the current turn. The χ2 test is used to find significant dependencies. We find several interesting interactions which suggest that the discourse structure can play an important role in several dialogue related tasks: automatic detection of SRP and analyzing spoken dialogues systems with a large state space from limited amounts of available data.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-14"
  },
  "banerjee06_interspeech": {
   "authors": [
    [
     "Satanjeev",
     "Banerjee"
    ],
    [
     "Alexander I.",
     "Rudnicky"
    ]
   ],
   "title": "A texttiling based approach to topic boundary detection in meetings",
   "original": "i06_1827",
   "page_count": 4,
   "order": 15,
   "p1": "paper 1827-Mon1A3O.3",
   "pn": "",
   "abstract": [
    "Our goal is to automatically detect boundaries between discussions of different topics in meetings. Towards this end we adapt the TextTiling algorithm [1] to the context of meetings. Our features include not only the overlapped words between adjacent windows, but also overlaps in the amount of speech contributed by each meeting participant. We evaluate our algorithm by comparing the automatically detected boundaries with the true ones, and computing precision, recall and f-measure. We report average precision of 0.85 and recall of 0.59 when segmenting unseen test meetings. Error analysis of our results shows that although the basic idea of our algorithm is sound, it breaks down when participants stray from typical behavior (such as when they monopolize the conversation for too long).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-15"
  },
  "schulz06_interspeech": {
   "authors": [
    [
     "Stefan",
     "Schulz"
    ],
    [
     "Hilko",
     "Donker"
    ]
   ],
   "title": "An user-centered development of an intuitive dialog control for speech-controlled music selection in cars",
   "original": "i06_1855",
   "page_count": 4,
   "order": 16,
   "p1": "paper 1855-Mon1A3O.4",
   "pn": "",
   "abstract": [
    "During the last years speech dialog systems have proven valuable for controlling infotainment systems in cars. The increasing popularity of mobile MP3 players and their capability to store more and more MP3, suggest the development of a speech control function for car MP3 players. With technical advantages in hardware and speech recognition it is possible to focus on the development of an intuitive speech dialog, which ensures that the advantages of speech control can be utilized while disadvantages are minimal at the same time. This paper describes the user-centered development of such an intuitive dialog, its special challenges and the reached quality in usability and ease of use. Also the generalizability of the results is discussed.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-16"
  },
  "raux06_interspeech": {
   "authors": [
    [
     "Antoine",
     "Raux"
    ],
    [
     "Dan",
     "Bohus"
    ],
    [
     "Brian",
     "Langner"
    ],
    [
     "Alan W.",
     "Black"
    ],
    [
     "Maxine",
     "Eskenazi"
    ]
   ],
   "title": "Doing research on a deployed spoken dialogue system: one year of let's go! experience",
   "original": "i06_1794",
   "page_count": 4,
   "order": 17,
   "p1": "paper 1794-Mon1A3O.5",
   "pn": "",
   "abstract": [
    "This paper describes our work with Let's Go, a telephone-based bus schedule information system that has been in use by the Pittsburgh population since March 2005. Results from several studies show that while task success correlates strongly with speech recognition accuracy, other aspects of dialogue such as turn-taking, the set of error recovery strategies, and the initiative style also significantly impact system performance and user behavior.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-17"
  },
  "liscombe06_interspeech": {
   "authors": [
    [
     "Jackson",
     "Liscombe"
    ],
    [
     "Jennifer J.",
     "Venditti"
    ],
    [
     "Julia",
     "Hirschberg"
    ]
   ],
   "title": "Detecting question-bearing turns in spoken tutorial dialogues",
   "original": "i06_1491",
   "page_count": 4,
   "order": 18,
   "p1": "paper 1491-Mon1A3O.6",
   "pn": "",
   "abstract": [
    "Current speech-enabled Intelligent Tutoring Systems do not model student question behavior the way human tutors do, despite evidence indicating the importance of doing so. Our study examined a corpus of spoken tutorial dialogues collected for development of ITSpoke, an Intelligent Tutoring Spoken Dialogue System. The authors extracted prosodic, lexical, syntactic, and student and task dependent information from student turns. Results of running 5-fold cross validation machine learning experiments using AdaBoosted C4.5 decision trees show prediction of student question-bearing turns at a rate of 79.7%. The most useful features were prosodic, especially the pitch slope of the last 200 milliseconds of the student turn. Student pre-test score was the most-used feature. Findings indicate that using turn-based units is acceptable for incorporating question detection capability into practical Intelligent Tutoring Systems.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-18"
  },
  "srinivasan06_interspeech": {
   "authors": [
    [
     "Soundararajan",
     "Srinivasan"
    ],
    [
     "Yang",
     "Shao"
    ],
    [
     "Zhaozhang",
     "Jin"
    ],
    [
     "DeLiang",
     "Wang"
    ]
   ],
   "title": "A computational auditory scene analysis system for robust speech recognition",
   "original": "i06_1547",
   "page_count": 4,
   "order": 19,
   "p1": "paper 1547-Mon1WeS.1",
   "pn": "",
   "abstract": [
    "We present a computational auditory scene analysis system for separating and recognizing target speech in the presence of competing speech or noise. We estimate, in two stages, the ideal binary timefrequency (T-F) mask which retains the mixture in a local T-F unit if and only if the target is stronger than the interference within the unit. In the first stage, we use harmonicity to segregate the voiced portions of individual sources in each time frame based on multipitch tracking. Additionally, unvoiced portions are segmented based on an onset/offset analysis. In the second stage, speaker characteristics are used to group the T-F units across time frames. The resulting T-F masks are used in conjunction with missing-data methods for recognition. Systematic evaluations on a speech separation challenge task show significant improvement over the baseline performance.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-19"
  },
  "han06_interspeech": {
   "authors": [
    [
     "Runqiang",
     "Han"
    ],
    [
     "Pei",
     "Zhao"
    ],
    [
     "Qin",
     "Gao"
    ],
    [
     "Zhiping",
     "Zhang"
    ],
    [
     "Hao",
     "Wu"
    ],
    [
     "Xihong",
     "Wu"
    ]
   ],
   "title": "CASA based speech separation for robust speech recognition",
   "original": "i06_2068",
   "page_count": 4,
   "order": 20,
   "p1": "paper 2068-Mon1WeS.2",
   "pn": "",
   "abstract": [
    "This paper introduces a speech separation system as a front-end processing step for automatic speech recognition (ASR). It employs computational auditory scene analysis (CASA) to separate the target speech from the interference speech. Specifically, the mixed speech is preprocessed based on auditory peripheral model. Then a pitch tracking is conducted and the dominant pitch is used as a main cue to find the target speech. Next, the time frequency (TF) units are merged into many segments. These segments are then combined into streams via CASA initial grouping. A regrouping strategy is employed to refine these streams via amplitude modulate (AM) cues, which are finally organized by the speaker recognition techniques into corresponding speakers. Finally, the output streams are reconstructed to compensate the missing data in the abovementioned processing steps by a cluster based feature reconstruction. Experimental results of ASR show that at low TMR (<-6dB) the proposed method offers significantly higher recognition accuracy.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-20"
  },
  "every06_interspeech": {
   "authors": [
    [
     "Mark R.",
     "Every"
    ],
    [
     "Philip J.B.",
     "Jackson"
    ]
   ],
   "title": "Enhancement of harmonic content of speech based on a dynamic programming pitch tracking algorithm",
   "original": "i06_1637",
   "page_count": 4,
   "order": 21,
   "p1": "paper 1637-Mon1WeS.3",
   "pn": "",
   "abstract": [
    "For pitch tracking of a single speaker, a common requirement is to find the optimal path through a set of voiced or voiceless pitch estimates over a sequence of time frames. Dynamic programming (DP) algorithms have been applied before to this problem. Here, the pitch candidates are provided by a multi-channel autocorrelation-based estimator, and DP is extended to pitch tracking of multiple concurrent speakers. We use the resulting pitch information to enhance harmonic content in noisy speech and to obtain separations of target from interfering speech.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-21"
  },
  "barker06_interspeech": {
   "authors": [
    [
     "Jon",
     "Barker"
    ],
    [
     "André",
     "Coy"
    ],
    [
     "Ning",
     "Ma"
    ],
    [
     "Martin",
     "Cooke"
    ]
   ],
   "title": "Recent advances in speech fragment decoding techniques",
   "original": "i06_1479",
   "page_count": 4,
   "order": 22,
   "p1": "paper 1479-Mon1WeS.4",
   "pn": "",
   "abstract": [
    "This paper addresses the problem of recognising speech in the presence of a competing speaker. We employ a speech fragment decoding technique that treats segregation and recognition as coupled problems. Data-driven techniques are used to segment a spectro-temporal representation into a set of spectro-temporal fragments, such that each fragment is dominated by one or other of the speech sources. A speech fragment decoder is used which employs missing data techniques and clean speech models to simultaneously search for the set of fragments and the word sequence that best matches the target speaker model. The paper reports recent advances in this technique, and presents an evaluation based on artificially mixed speech utterances. The fragment decoder produces significantly lower error rates than a conventional recogniser, and mimics the pattern of human performance whereby performance increases as the target-masker ratio is reduced below -3 dB.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-22"
  },
  "virtanen06_interspeech": {
   "authors": [
    [
     "Tuomas",
     "Virtanen"
    ]
   ],
   "title": "Speech recognition using factorial hidden Markov models for separation in the feature space",
   "original": "i06_1850",
   "page_count": 4,
   "order": 23,
   "p1": "paper 1850-Mon1WeS.5",
   "pn": "",
   "abstract": [
    "This paper proposes an algorithm for the recognition and separation of speech signals in non-stationary noise, such as another speaker. We present a method to combine hidden Markov models (HMMs) trained for the speech and noise into a factorial HMM to model the mixture signal. Robustness is obtained by separating the speech and noise signals in a feature domain, which discards unnecessary information. We use mel-cepstral coefficients (MFCCs) as features, and estimate the distribution of mixture MFCCs from the distributions of the target speech and noise. A decoding algorithm is proposed for finding the state transition paths and estimating gains for the speech and noise from a mixture signal. Simulations were carried out using speech material where two speakers were mixed at various levels, and even for high noise level (9 dB above the speech level), the method produced relatively good (60% word recognition accuracy) results. Audio demonstrations are available at www.cs.tut.fi/~tuomasv.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-23"
  },
  "ming06_interspeech": {
   "authors": [
    [
     "Ji",
     "Ming"
    ],
    [
     "Timothy J.",
     "Hazen"
    ],
    [
     "James R.",
     "Glass"
    ]
   ],
   "title": "Combining missing-feature theory, speech enhancement and speaker-dependent/-independent modeling for speech separation",
   "original": "i06_1377",
   "page_count": 4,
   "order": 24,
   "p1": "paper 1377-Mon1WeS.6",
   "pn": "",
   "abstract": [
    "This paper considers the recognition of speech given in the form of two mixed sentences, spoken by the same talker or by two different talkers. The database published on the ICSLP2006 website for Two-Talker Speech Separation Challenge is used in the study. A system that recognizes and reconstructs both sentences from the given mixture is described. The system involves a combination of several different techniques, including a missing-feature approach for improving crosstalk/noise robustness, Wiener filtering for speech restoration, HMM-based speech reconstruction, and speakerdependent/- independent modeling for speaker/speech recognition. For clean speech recognition, the system obtained a word accuracy rate 96.7%. For the two-talker speech separation challenge task, the system obtained 81.4% at 6 dB TMR (target-to-masker ratio) and 34.1% at -9 dB TMR.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-24"
  },
  "kristjansson06_interspeech": {
   "authors": [
    [
     "T.",
     "Kristjansson"
    ],
    [
     "J.",
     "Hershey"
    ],
    [
     "P.",
     "Olsen"
    ],
    [
     "S.",
     "Rennie"
    ],
    [
     "Ramesh",
     "Gopinath"
    ]
   ],
   "title": "Super-human multi-talker speech recognition: the IBM 2006 speech separation challenge system",
   "original": "i06_1775",
   "page_count": 4,
   "order": 25,
   "p1": "paper 1775-Mon1WeS.7",
   "pn": "",
   "abstract": [
    "We describe a system for model based speech separation which achieves super-human recognition performance when two talkers speak at similar levels. The system can separate the speech of two speakers from a single channel recording with remarkable results. It incorporates a novel method for performing two-talker speaker identification and gain estimation. We extend the method of model based high resolution signal reconstruction to incorporate temporal dynamics. We report on two methods for introducing dynamics; the first uses dynamics in the acoustic model space, the second incorporates dynamics based on sentence grammar. The addition of temporal constraints leads to dramatic improvements in the separation performance. Once the signals have been separated they are then recognized using speaker dependent labeling.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-25"
  },
  "deshmukh06_interspeech": {
   "authors": [
    [
     "Om D.",
     "Deshmukh"
    ],
    [
     "Carol Y.",
     "Espy-Wilson"
    ]
   ],
   "title": "Modified phase opponency based solution to the speech separation challenge",
   "original": "i06_1936",
   "page_count": 4,
   "order": 26,
   "p1": "paper 1936-Mon1WeS.8",
   "pn": "",
   "abstract": [
    "In this work, we present a single-channel speech enhancement technique called the Modified Phase Opponency (MPO) model as a solution to the Speech Separation Challenge. The MPO model is based on a neural model for detection of tones-in-noise called the Phase Opponency (PO) model. Replacing the noisy speech signals by the corresponding MPO-processed signals increases the accuracy by 31% when the speech signals are corrupted by speech-shaped noise at 0 dB Signal-to-Noise Ratio (SNR). It is worth mentioning that the MPO enhancement scheme was developed using the noisy connected-digit Aurora database and was not tailored in any way to fit the Grid database used in this challenge. One of the salient features of the MPO-based speech enhancement scheme is that it does not need to estimate the noise characteristics, nor does it assume that the noise satisfies any statistical model.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-26"
  },
  "loof06_interspeech": {
   "authors": [
    [
     "J.",
     "Lööf"
    ],
    [
     "M.",
     "Bisani"
    ],
    [
     "Ch.",
     "Gollan"
    ],
    [
     "G.",
     "Heigold"
    ],
    [
     "Björn",
     "Hoffmeister"
    ],
    [
     "Ch.",
     "Plahl"
    ],
    [
     "Ralf",
     "Schlüter"
    ],
    [
     "Hermann",
     "Ney"
    ]
   ],
   "title": "The 2006 RWTH parliamentary speeches transcription system",
   "original": "i06_1545",
   "page_count": 4,
   "order": 27,
   "p1": "paper 1545-Mon1BuP.1",
   "pn": "",
   "abstract": [
    "In this work, the RWTH automatic speech recognition systems developed for the second TC-STAR evaluation campaign 2006 are presented. The systems were designed to transcribe parliamentary speeches taken from the European Parliament Plenary Sessions (EPPS) in European English and Spanish, as well as speeches from the Spanish Parliament. The RWTH systems apply a two pass search strategy with a fourgram one-pass decoder including a fast vocal tract length normalization variant as first pass. The systems further include several adaptation and normalization methods, minimum classification error trained models, and bayes risk minimization. For all relevant individual components contrastive results are presented on the EPPS Spanish and English data.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-27"
  },
  "bouselmi06_interspeech": {
   "authors": [
    [
     "G.",
     "Bouselmi"
    ],
    [
     "D.",
     "Fohr"
    ],
    [
     "I.",
     "Illina"
    ],
    [
     "Jean-Paul",
     "Haton"
    ]
   ],
   "title": "Multilingual non-native speech recognition using phonetic confusion-based acoustic model modification and graphemic constraints",
   "original": "i06_1569",
   "page_count": 4,
   "order": 28,
   "p1": "paper 1569-Mon1BuP.2",
   "pn": "",
   "abstract": [
    "In this paper we present an automated approach for non-native speech recognition. We introduce a new phonetic confusion concept that associates sequences of native language (NL) phones to spoken language (SL) phones. Phonetic confusion rules are automatically extracted from a non-native speech database for a given NL and SL using both NLs and SLs ASR systems. These rules are used to modify the acoustic models (HMMs) of SLs ASR by adding acoustic models of NLs phones according to these rules. As pronunciation errors that non-native speakers produce depend on the writing of the words, we have also used graphemic constraints in the phonetic confusion extraction process. In the lexicon, the phones in words pronunciations are linked to the corresponding graphemes (characters) of the word. In this way, the phonetic confusion is established between couples of (SL phones, graphemes) and sequences of NL phones. We evaluated our approach on French, Italian, Spanish and Greek non-native speech databases. The spoken language is English. The modified ASR system achieved significant improvements ranging from 20.3% to 43.2% (relative) in sentence error rate and from 26.6% to 50.0% in WER.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-28"
  },
  "chan06_interspeech": {
   "authors": [
    [
     "Joyce Y. C.",
     "Chan"
    ],
    [
     "P. C.",
     "Ching"
    ],
    [
     "Tan",
     "Lee"
    ],
    [
     "Houwei",
     "Cao"
    ]
   ],
   "title": "Automatic speech recognition of Cantonese-English code-mixing utterances",
   "original": "i06_1065",
   "page_count": 4,
   "order": 29,
   "p1": "paper 1065-Mon1BuP.3",
   "pn": "",
   "abstract": [
    "This paper describes our recent work on the development of a large vocabulary, speaker-independent, continuous speech recognition system for Cantonese-English code-mixing utterances. The details of both acoustic modeling and language modeling will be discussed. For acoustic modeling, Cantonese accents in English words are handled by applying cross-lingual acoustic units, as well as modifications in pronunciation dictionary. Statistic language models are built from a small amount of text data, as there are many limitations on data collection. Language boundary detection based on language identification algorithms is applied, and it offers a slight improvement to the overall accuracy. The recognition accuracy for Chinese characters and English lexicons in the code-mixing utterances is 56.37% and 52.99%, respectively.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-29"
  },
  "zimmerman06_interspeech": {
   "authors": [
    [
     "M.",
     "Zimmerman"
    ],
    [
     "Dilek",
     "Hakkani-Tür"
    ],
    [
     "J.",
     "Fung"
    ],
    [
     "N.",
     "Mirghafori"
    ],
    [
     "L.",
     "Gottlieb"
    ],
    [
     "Elizabeth",
     "Shriberg"
    ],
    [
     "Yang",
     "Liu"
    ]
   ],
   "title": "The ICSI+ multilingual sentence segmentation system",
   "original": "i06_1808",
   "page_count": 4,
   "order": 30,
   "p1": "paper 1808-Mon1BuP.4",
   "pn": "",
   "abstract": [
    "The ICSI+ multilingual sentence segmentation with results for English and Mandarin broadcast news automatic speech recognizer transcriptions represents a joint effort involving ICSI, SRI, and UT Dallas. Our approach is based on using hidden event language models for exploiting lexical information, and maximum entropy and boosting classifiers for exploiting lexical, as well as prosodic, speaker change and syntactic information. We demonstrate that the proposed methodology including pitch- and energy-related prosodic features performs significantly better than a baseline system that uses words and simple pause features only. Furthermore, the obtained improvements are consistent across both languages, and no language-specific adaptation of the methodology is necessary. The best results were achieved by combining hidden event language models with a boosting-based classifier that to our knowledge has not previously been applied for this task.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-30"
  },
  "cheng06_interspeech": {
   "authors": [
    [
     "Yan Ming",
     "Cheng"
    ],
    [
     "Changxue",
     "Ma"
    ],
    [
     "Lynette",
     "Melnar"
    ]
   ],
   "title": "Cross-language evaluation of voice-to-phoneme conversions for voice-tag application in embedded platforms",
   "original": "i06_1062",
   "page_count": 4,
   "order": 31,
   "p1": "paper 1062-Mon1BuP.5",
   "pn": "",
   "abstract": [
    "Previously, we proposed two voice-to-phoneme conversion algorithms for speaker-independent voice-tag creation specifically targeted at applications on embedded platforms, an environment sensitive to CPU and memory resource consumption [1]. These two algorithms (batch mode and sequential) were applied in a same-language context, i.e., both acoustic model training and voice-tag creation and application were performed on the same language.\n",
    "In this paper, we investigate the cross-language application of these two voice-to-phoneme conversion algorithms, where the acoustic models are trained on a particular source language while the voicetags are created and applied on a different target language. Here, both algorithms create phonetic representations of a voice-tag of a target language based on the speaker-independent acoustic models of a distinct source language. Our experiments show that recognition performances of these voice-tags vary depending on the source-target language pair, with the variation reflecting the predicted phonological similarity between the source and target languages. Among the most similar languages, performance nears that of the native-trained models and surpasses the native reference baseline.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-31"
  },
  "wang06_interspeech": {
   "authors": [
    [
     "Huanliang",
     "Wang"
    ],
    [
     "Yao",
     "Qian"
    ],
    [
     "Frank K.",
     "Soong"
    ],
    [
     "Jian-Lai",
     "Zhou"
    ],
    [
     "Jiqing",
     "Han"
    ]
   ],
   "title": "A multi-space distribution (MSD) approach to speech recognition of tonal languages",
   "original": "i06_1473",
   "page_count": 4,
   "order": 32,
   "p1": "paper 1473-Mon1BuP.6",
   "pn": "",
   "abstract": [
    "Tone plays an important role in recognizing spoken tonal languages like Chinese. However, the F0 contour discontinuity between voiced and unvoiced segments has traditionally been a bottleneck in modeling tone contour for automatic speech recognition and synthesis and various heuristic approaches were proposed to get around the problem. The Multi-Space Distribution (MSD) was proposed by Tokuda et.al. and applied to HMM-based speech synthesis, which models the two probability spaces, discrete for unvoiced region and continuous for voiced F0 contour, in a linearly weighted mixture. We extend the MSD to tone modeling for speech recognition applications. Specifically, modeling tones in speaker-independent, spoken Chinese is formulated and tested in a Mandarin speech database. The tone features and spectral features are further separated into two streams and stream-dependent models are built to cluster the two features into separated decision trees. The recognition results show that the ultimate performance of tonal syllable error rate can be improved from toneless baseline system to the MSD based stream-dependent system, 50.5% to 36.1% and 46.3% to 35.1%, for the two systems resulted from using two different phone sets. The absolute tonal syllable error rate improvement of the new approach is 5.5% and 6.1%, comparing with the conventional tone modeling.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-32"
  },
  "le06_interspeech": {
   "authors": [
    [
     "Viet Bac",
     "Le"
    ],
    [
     "Laurent",
     "Besacier"
    ]
   ],
   "title": "Comparison of acoustic modeling techniques for Vietnamese and Khmer ASR",
   "original": "i06_1662",
   "page_count": 4,
   "order": 33,
   "p1": "paper 1662-Mon1BuP.7",
   "pn": "",
   "abstract": [
    "This paper presents a comparison of some different acoustic modeling strategies for under-resourced languages. When only limited speech data are available for under-resourced languages, we propose some crosslingual acoustic modeling techniques. We apply and compare these techniques in Vietnamese ASR. Since there is no pronunciation dictionary for some under-resourced languages, we investigate grapheme-based acoustic modeling. Some initialization techniques for context independent modeling and some question generation techniques for context dependent modeling are applied and compared for Khmer ASR.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-33"
  },
  "liu06_interspeech": {
   "authors": [
    [
     "Yi",
     "Liu"
    ],
    [
     "Pascale",
     "Fung"
    ]
   ],
   "title": "Multi-accent Chinese speech recognition",
   "original": "i06_1887",
   "page_count": 4,
   "order": 34,
   "p1": "paper 1887-Mon1BuP.8",
   "pn": "",
   "abstract": [
    "Multiple accents are often present in spontaneous Chinese Mandarin speech as most Chinese have learned Mandarin as a second language. We propose a method to handle multiple accents as well as standard speech in a speaker-independent system by merging auxiliary accent decision trees with standard trees and reconstruct the acoustic model. In our proposed method, tree structures and shape are modified according to accent-specific data while the parameter set of the baseline model remains the same. The effectiveness of this approach is evaluated on Cantonese and Wu accented, as well as standard Mandarin speech. Our method yields a significant 4.4% and 3.3% absolute word error rate reduction without sacrificing the performance on standard Mandarin speech.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-34"
  },
  "ghorshi06_interspeech": {
   "authors": [
    [
     "Seyed",
     "Ghorshi"
    ],
    [
     "Saeed",
     "Vaseghi"
    ],
    [
     "Qin",
     "Yan"
    ]
   ],
   "title": "Comparative analysis of formants of British, american and australian accents",
   "original": "i06_1252",
   "page_count": 4,
   "order": 35,
   "p1": "paper 1252-Mon1BuP.9",
   "pn": "",
   "abstract": [
    "This paper compares and quantifies the differences between formants of speech across accents. The cross entropy information measure is used to compare the differences between the formants of the vowels of three major English accents namely British, American and Australian. An improved formant estimation method, based on a linear prediction (LP) model feature analysis and a hidden Markov model (HMM) of formants, is employed for estimation of formant trajectories of vowels and diphthongs. Comparative analysis of the formant space of the three accents indicates that these accents are mostly conveyed by the first two formants. The third and fourth formants exhibit some significant differences across accents for only a few phonemes most notably the variants of vowel r in the American (rhotic) accent compared to British (non-rhotic accent). The issue of speaker variability versus accent variability is examined by comparing the cross-entropies of speech models trained on different groups of speakers within and across the accents.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-35"
  },
  "liu06b_interspeech": {
   "authors": [
    [
     "Linquan",
     "Liu"
    ],
    [
     "Thomas Fang",
     "Zheng"
    ],
    [
     "Wenhu",
     "Wu"
    ]
   ],
   "title": "Automatic initial/final generation for dialectal Chinese speech recognition",
   "original": "i06_1051",
   "page_count": 4,
   "order": 36,
   "p1": "paper 1051-Mon1BuP.10",
   "pn": "",
   "abstract": [
    "Phonetic differences always exist between any Chinese dialect and standard Chinese (Putonghua). In this paper, a method, named automatic dialect-specific Initial/Final (IF) generation, is proposed to deal with the issue of phonemic difference which can automatically produce the dialect-specific units based on model distance measure. A dialect-specific decision tree regrowing method is also proposed to cope with the tri-IF expansion due to the introduction of dialect-specific IFs (DIFs). In combination with a certain adaptation technique, the proposed methods can achieve a syllable error rate (SER) reduction of 18.5% for Shanghai-accented Chinese compared with the Putonghua-based baseline while the use of the DIF set only can lead to an SER reduction of 5.5%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-36"
  },
  "sarikaya06_interspeech": {
   "authors": [
    [
     "Ruhi",
     "Sarikaya"
    ],
    [
     "Ossama",
     "Emam"
    ],
    [
     "Imed",
     "Zitouni"
    ],
    [
     "Yuqing",
     "Gao"
    ]
   ],
   "title": "Maximum entropy modeling for diacritization of Arabic text",
   "original": "i06_1418",
   "page_count": 4,
   "order": 37,
   "p1": "paper 1418-Mon1BuP.11",
   "pn": "",
   "abstract": [
    "We propose a novel modeling framework for automatic diacritization of Arabic text. The framework is based on Markov modeling where each grapheme is modeled as a state emitting a diacritic (or none) from the diacritic space. This space is exactly defined using 13 diacritics and a null-diacritic and covers all the diacritics used in any Arabic text. The state emission probabilities are estimated using maximum entropy (MaxEnt) models. The diacritization process is formulated as a search problem where the most likely diacritization realization is assigned to a given sentence. We also propose a diacritization parse tree (DPT) for Arabic that allows joint representation of diacritics, graphemes, words, word contexts, morphologically analyzed units, syntactic (parse tree), semantic (parse tree), part-of-speech tags and possibly other information sources. The features used to train MaxEnt models are obtained from the DPT. In our evaluation we obtained 7.8% diacritization error rate (DER) and 17.3% word diacritization error rate (WDER) on a dialectal Arabic data using the proposed framework.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-37"
  },
  "lihan06_interspeech": {
   "authors": [
    [
     "Slavomír",
     "Lihan"
    ],
    [
     "Jozef",
     "Juhár"
    ],
    [
     "Anton",
     "Cizmár"
    ]
   ],
   "title": "Comparison of Slovak and Czech speech recognition based on grapheme and phoneme acoustic models",
   "original": "i06_1462",
   "page_count": 4,
   "order": 38,
   "p1": "paper 1462-Mon1BuP.12",
   "pn": "",
   "abstract": [
    "Grapheme based mono-, cross- and bilingual speech recognition of Czech and Slovak is presented in the paper. The training and testing procedures follow the MASPER initiative that was formed as a part of the COST 278 Action. All experiments were performed using Czech and Slovak SpeechDat-E databases. Grapheme-based models gave equivalent recognition performance compared to phoneme-based models in monolingual as well as bilingual case. Moreover bilingual SK-CZ speech recognition is equivalent to monolingual recognition, which indicates the possibility to share Czech and Slovak speech data for training bilingual grapheme-based acoustic models usable for recognition of Slovak as well as Czech. Also the promising results confirmed the presumption, that languages with a close graphemeto- phoneme relation are well suited for grapheme-based speech recognition.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-38"
  },
  "jones06_interspeech": {
   "authors": [
    [
     "Rhys James",
     "Jones"
    ],
    [
     "Ambrose",
     "Choy"
    ],
    [
     "Briony",
     "Williams"
    ]
   ],
   "title": "Integrating Festival and Windows",
   "original": "i06_1380",
   "page_count": 4,
   "order": 39,
   "p1": "paper 1380-Mon1CaP.1",
   "pn": "",
   "abstract": [
    "Festival is a popular open-source development and execution environment for speech synthesis. It has been well-integrated within many environments, particularly Unix ones, but so far has not been easy to integrate natively into Windows. We present two solutions to this: an MSAPI interface, which allows Festival voices to work with a range of speech-enabled Windows applications, and SpeechServer, a client-server architecture which allows Festival to operate within a Flash (or other) application within a web browser. While the motivation for this work was to enable new Welsh diphone Festival voices to be used within screenreaders and other Windows programs, the MSAPI interface is now modularised, allowing it to work with any Festival voice.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-39"
  },
  "munteanu06_interspeech": {
   "authors": [
    [
     "Cosmin",
     "Munteanu"
    ],
    [
     "Gerald",
     "Penn"
    ],
    [
     "Ron",
     "Baecker"
    ],
    [
     "Elaine",
     "Toms"
    ],
    [
     "David",
     "James"
    ]
   ],
   "title": "Measuring the acceptable word error rate of machine-generated webcast transcripts",
   "original": "i06_1756",
   "page_count": 4,
   "order": 40,
   "p1": "paper 1756-Mon1CaP.2",
   "pn": "",
   "abstract": [
    "The increased availability of broadband connections has recently led to an increase in the use of Internet broadcasting (webcasting). Most webcasts are archived and accessed numerous times retrospectively. One of the hurdles users face when browsing and skimming through archives is the lack of text transcripts of the audio channel of the webcast archive. In this paper, we proposed a procedure for prototyping an Automatic Speech Recognition (ASR) system that generates realistic transcripts of any desired Word Error Rate (WER), thus overcoming the drawbacks of both prototype-based and Wizard of Oz simulations. We used such a system in a study where human subjects perform question-answering tasks using archives of webcast lectures, and showed that their performance and perception of transcript quality is linearly affected by WER, and that transcripts of WER equal or less than 25% would be acceptable for use in webcast archives.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-40"
  },
  "nagino06_interspeech": {
   "authors": [
    [
     "Goshu",
     "Nagino"
    ],
    [
     "Makoto",
     "Shozakai"
    ]
   ],
   "title": "Analyzing reusability of speech corpus based on statistical multidimensional scaling method",
   "original": "i06_1382",
   "page_count": 4,
   "order": 41,
   "p1": "paper 1382-Mon1CaP.3",
   "pn": "",
   "abstract": [
    "In order to develop a target speech recognition system with less cost of time and money, reusability of existing speech corpora is becoming one of the most important issues. This paper proposes a new technique of applying a statistical multidimensional scaling method to analyze the reusability of a speech corpus. In the experiment using six speech corpora, which contains isolated words and short sentences used in car navigation system, an effect of the proposed method is evaluated by a usual approach of cross task recognition. Furthermore, the relationship among those speech corpora is clearly shown by the proposed method.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-41"
  },
  "fitt06_interspeech": {
   "authors": [
    [
     "Susan",
     "Fitt"
    ],
    [
     "Korin",
     "Richmond"
    ]
   ],
   "title": "Redundancy and productivity in the speech technology lexicon - can we do better?",
   "original": "i06_1202",
   "page_count": 4,
   "order": 42,
   "p1": "paper 1202-1CaP.4",
   "pn": "",
   "abstract": [
    "Current lexica for speech technology typically contain much redundancy, while omitting useful information. A comparison with lexica in other media and for other purposes is instructive, as it highlights some features we may borrow for text-to-speech and speech recognition lexica.\n",
    "We describe some aspects of the new lexicon we are producing, Combilex, whose structure and implementation is specifically designed to reduce redundancy and improve the representation of productive elements of English. Most importantly, many English words are predictable derivations of baseforms, or compounds. Storing the lexicon as a combination of baseforms and derivational rules speeds up lexicon development, and improves coverage and maintainability.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-42"
  },
  "yamada06_interspeech": {
   "authors": [
    [
     "Takeshi",
     "Yamada"
    ],
    [
     "Masakazu",
     "Kumakura"
    ],
    [
     "Nobuhiko",
     "Kitawaki"
    ]
   ],
   "title": "Word intelligibility estimation of noise-reduced speech",
   "original": "i06_1443",
   "page_count": 4,
   "order": 43,
   "p1": "paper 1443-1CaP.5",
   "pn": "",
   "abstract": [
    "It is indispensable to establish an objective test methodology for noise-reduced speech. This paper proposes a new methodology which estimates word intelligibility of the noise-reduced speech from PESQ MOS (subjective MOS estimated by the PESQ). To evaluate the effectiveness of the proposed methodology, a word intelligibility test of the noise-reduced speech was performed by using four noise reduction algorithms and word lists which take word difficulty into account, and then the word intelligibility was estimated by the proposed methodology. The results confirmed that the word intelligibility can be estimated well from the PESQ MOS without distinguishing the noise reduction algorithms and the noise types.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-43"
  },
  "draxler06_interspeech": {
   "authors": [
    [
     "Christoph",
     "Draxler"
    ]
   ],
   "title": "Exploring the unknown - collecting 1000 speakers over the internet for the ph@ttsessionz database of adolescent speakers",
   "original": "i06_1217",
   "page_count": 4,
   "order": 44,
   "p1": "paper 1217-Mon1CaP.6",
   "pn": "",
   "abstract": [
    "The Ph@ttSessionz project will create a database of 1000 adolescent German speakers. The project employs a novel approach to collecting speech data: recordings are being performed via the WWW in more than 35 schools in Germany, and the data is immediately transferred to the BAS server in Munich. Using this approach, geographically distributed recordings in high bandwidth quality can be performed efficiently and reliably. The paper presents the infrastructure developed at BAS for WWW-based speech recordings, it discusses the strategies employed to get schools to participate in the project, and it presents preliminary analyses of the speech database.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-44"
  },
  "murphy06_interspeech": {
   "authors": [
    [
     "Timothy",
     "Murphy"
    ],
    [
     "Dorel",
     "Picovici"
    ],
    [
     "Abdulhussain E.",
     "Mahdi"
    ]
   ],
   "title": "A new single-ended measure for assessment of speech quality",
   "original": "i06_1538",
   "page_count": 4,
   "order": 45,
   "p1": "paper 1538-Mon1CaP.7",
   "pn": "",
   "abstract": [
    "This paper proposes a new non-intrusive measure for objective speech quality assessment in telephony applications and evaluates its performance. The measure is based on estimating perception-based objective auditory distances between voiced parts of the degraded speech under test and an appropriately formulated artificial reference model of clean speech signals. The reference model is extracted from one or many pre-formulated speech reference books. The reference books are formed by optimally clustering large number of parametric speech vectors extracted from a database of clean speech signals, using an efficient K dimensional tree structure. The measured auditory distances are then mapped into objective listening quality scores. Reported evaluation results show that the proposed measure offers sufficiently accurate and low-complexity assessment method of speech quality, making it suitable for real time applications.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-45"
  },
  "nichasaide06_interspeech": {
   "authors": [
    [
     "Ailbhe",
     "Ní Chasaide"
    ],
    [
     "John",
     "Wogan"
    ],
    [
     "Brian Ó",
     "Raghallaigh"
    ],
    [
     "Áine Ní",
     "Bhriain"
    ],
    [
     "Eric",
     "Zoerner"
    ],
    [
     "Harald",
     "Berthelsen"
    ],
    [
     "Christer",
     "Gobl"
    ]
   ],
   "title": "Speech technology for minority languages: the case of Irish (gaelic)",
   "original": "i06_1378",
   "page_count": 4,
   "order": 46,
   "p1": "paper 1378-Mon1CaP.8",
   "pn": "",
   "abstract": [
    "The development of speech technology could play an important role in the maintenance and preservation of minority languages, especially where the population of native speakers are dwindling. This paper outlines the efforts within the WISPR project, to develop annotated spoken corpora along with some of the prerequisites for the synthesis of Irish (Gaelic). It details the particular challenges that have confronted us as well as the strategies adopted to overcome them. It highlights the need for gearing our methodologies to these constraints and to maximise the reusability of resources. Our long-term goal is not only to develop these resources for Irish, but also, in parallel, to develop methodologies that will enable the technology to be flexible and suitable to the envisaged end users, e.g., more flexible kinds of synthesisers, with expressive capabilities and multiple voices, including childrens. It is therefore a major consideration to develop resources in such a way that they are in some sense independent of any single methodology (unit selection vs. other modalities for synthesis development).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-46"
  },
  "fraga06_interspeech": {
   "authors": [
    [
     "Francisco José",
     "Fraga"
    ],
    [
     "Carlos Alberto",
     "Ynoguti"
    ],
    [
     "André Godoi",
     "Chiovato"
    ]
   ],
   "title": "Further investigations on the relationship between objective measures of speech quality and speech recognition rates in noisy environments",
   "original": "i06_1877",
   "page_count": 4,
   "order": 47,
   "p1": "paper 1877-Mon1CaP.9",
   "pn": "",
   "abstract": [
    "The relationship between an objective measure of speech quality (PESQ) and the recognition rate of a given speech recognition system was already investigated by other researchers. In this paper, we present a further investigation on such a relationship. In our research, the speech recognition tests were performed on a wider class of signals and SNR. The experimental setup as well as the speech recognition systems now evaluated were based on the directions given by the Aurora project. Moreover, a new parametric modeling approach for the PESQ-MOS versus speech recognition rate curve, based on the logistic function, is proposed. This new modeling allows some meaningful interpretations of the parameters of the logistic function in terms of system robustness, and permits to make inferences in the regions outside the experimental measures. Furthermore, the PESQ versus SNR characteristic was used to group types of noise, leading to a much better fit of the logistic function over the data points.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-47"
  },
  "grancharov06_interspeech": {
   "authors": [
    [
     "Volodya",
     "Grancharov"
    ],
    [
     "David Y.",
     "Zhao"
    ],
    [
     "Jonas",
     "Lindblom"
    ],
    [
     "W. Bastiaan",
     "Kleijn"
    ]
   ],
   "title": "Non-intrusive speech quality assessment with low computational complexity",
   "original": "i06_1391",
   "page_count": 4,
   "order": 48,
   "p1": "paper 1391-Mon1CaP.10",
   "pn": "",
   "abstract": [
    "We describe an algorithm for monitoring subjective speech quality without access to the original signal that has very low computational and memory requirements. The features used in the proposed algorithm can be computed from commonly used speech-coding parameters. Reconstruction and perceptual transformation of the signal are not performed. The algorithm generates quality assessment ratings without explicit distortion modeling. The simulation results indicate that the proposed non-intrusive objective quality measure performs better than the ITU-T P.563 standard despite its very low computational complexity.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-48"
  },
  "liang06_interspeech": {
   "authors": [
    [
     "Min-Siong",
     "Liang"
    ],
    [
     "Ren-Yuan",
     "Lyu"
    ],
    [
     "Yuang-Chin",
     "Chiang"
    ]
   ],
   "title": "Using speech recognition technique for constructing a phonetically transcribed taiwanese (min-nan) text corpus",
   "original": "i06_1442",
   "page_count": 4,
   "order": 49,
   "p1": "paper 1442-Mon1CaP.11",
   "pn": "",
   "abstract": [
    "Collection of Taiwanese text corpus with phonetic transcription suffers from the problems of multiple pronunciation variation. By augmenting the text with speech, and using automatic speech recognition with a sausage searching net constructed from the multiple pronunciations of the text corresponding to its speech utterance, we are able to reduce the effort for phonetic transcription. By using the multiple pronunciation lexicon, the error rate of transcription 13.94% was achieved. Further improvement can be achieved by adapting the pronunciation lexicon with pronunciation variation (PV) rules derived from a manual corrected speech corpus. The PV rules can be categorized into two kinds: the knowledge-based and data-driven rules. By incorporating the PV rules, the error rate reduction 13.63% could be achieved. Although the technique was developed for Taiwanese speech, it could also be adapted easily to be applied in the other similar \"minority\" Chinese spoken languages.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-49"
  },
  "zgank06_interspeech": {
   "authors": [
    [
     "Andrej",
     "Zgank"
    ],
    [
     "Tomas",
     "Rotovnik"
    ],
    [
     "Matej",
     "Grasic"
    ],
    [
     "Marko",
     "Kos"
    ],
    [
     "Damjan",
     "Vlaj"
    ],
    [
     "Zdravko",
     "Kacic"
    ]
   ],
   "title": "Sloparl - slovenian parliamentary speech and text corpus for large vocabulary continuous speech recognition",
   "original": "i06_1493",
   "page_count": 4,
   "order": 50,
   "p1": "paper 1493-Mon1CaP.12",
   "pn": "",
   "abstract": [
    "This paper present a novel Slovenian language resource - SloParl database. It consists from debates acquired in the Slovenian Parliament. The main goal of the project was to cost-effectively collect a new Slovenian language resource that could be used to augment the available Slovenian speech corpora for developing a large vocabulary continuous speech recognition system. The SloParl speech corpus has a total length of 100 hours. The selected sessions between years 2000-2005 were incorporated in it. This speech corpus will be used for lightly supervised or unsupervised acoustic models training. In accordance with this, the accompanying transcriptions were prepared. The second part of the SloParl database is the text corpus, which covers text of all debates from period 1996-2005. It consists of 23M words. It will be used to create different types of speech recognisers language models. Comparison with other Slovenian language resources showed that SloParl database adds new aspects to the modelling of Slovenian language.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-50"
  },
  "toh06_interspeech": {
   "authors": [
    [
     "Siew Leng",
     "Toh"
    ],
    [
     "Fan",
     "Yang"
    ],
    [
     "Peter A.",
     "Heeman"
    ]
   ],
   "title": "An annotation scheme for agreement analysis",
   "original": "i06_1857",
   "page_count": 4,
   "order": 51,
   "p1": "paper 1857-Mon1CaP.13",
   "pn": "",
   "abstract": [
    "To accomplish a task that requires collaboration, people would first agree on a strategy and then together carry it out [1]. Our research interest lies in understanding how people explore different strategies and reach an agreement in conversation. We began by examining two-person dialogues in a very limited domain, in which we could just focus on the agreement process. In this paper, we describe an annotation scheme of coding the conversants behaviors of exploring possible strategies, suggesting and accepting the optimal one, and then maintaining it. We report the inter-coder reliability of the annotation scheme on three expert annotators and two non-experts.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-51"
  },
  "aoki06_interspeech": {
   "authors": [
    [
     "Hitoshi",
     "Aoki"
    ],
    [
     "Atsuko",
     "Kurashima"
    ],
    [
     "Akira",
     "Takahashi"
    ]
   ],
   "title": "Conversational quality estimation model for wideband IP-telephony services",
   "original": "i06_1036",
   "page_count": 4,
   "order": 52,
   "p1": "paper 1036-Tue2WeO.1",
   "pn": "",
   "abstract": [
    "As broadband and high-speed IP networks spread, IP-telephony services have become a popular speech communication application over IP networks. Recently, the speech quality of IP-telephony services has become close to that of conventional PSTN services. To provide better speech quality to users, speech communication with wider bandwidth (e.g., 7 kHz) is one of the most promising applications. To ensure desirable quality, we should design the quality before services start and manage it while they are being provided. To do this, an effective means for estimating users perceptions of speech quality is indispensable. This paper describes a model for estimating the conversational quality of wideband IP-telephony services from physical characteristics of terminals and networks. The proposed model takes into account the quality enhancement effect achieved by widening speech bandwidth and has the advantage that it can evaluate the quality of both wideband and telephone-band speech on the same scale. Based on subjective conversational quality evaluation experiments, we show that the proposed model can accurately estimate the subjective quality for wideband speech as well as for telephone-band speech.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-52"
  },
  "kilanski06_interspeech": {
   "authors": [
    [
     "Kelley",
     "Kilanski"
    ],
    [
     "Jonathan",
     "Malkin"
    ],
    [
     "Xiao",
     "Li"
    ],
    [
     "Richard",
     "Wright"
    ],
    [
     "Jeff A.",
     "Bilmes"
    ]
   ],
   "title": "The vocal joystick data collection effort and vowel corpus",
   "original": "i06_1885",
   "page_count": 4,
   "order": 53,
   "p1": "paper 1885-Tue2WeO.2",
   "pn": "",
   "abstract": [
    "Vocal Joystick is a mechanism that enables individuals with motor impairments to make use of vocal parameters to control objects on a computer screen (buttons, sliders, etc.) and ultimately will be used to control electro-mechanical instruments (e.g., robotic arms, wireless home automation devices). In an effort to train the VJ-system, speech data from the TIMIT speech corpus was initially used. However, due to problematic issues with co-articulation, we began a large data collection effort in a controlled environment that would not only address the problematic issues, but also yield a new vowel corpus that was representative of the utterances a user of the VJ-system would use. The data collection process evolved over the course of the effort as new parameters were added and as factors relating to the quality of the collected data in terms of the specified parameters were considered. The result of the data collection effort is a vowel corpus of approximately 11 hours of recorded data comprised of approximately 23500 sound files of the monophthongs and vowel combinations (e.g. diphthongs) chosen for the Vocal Joystick project varying along the parameters of duration, intensity and amplitude. This paper discusses how the data collection has evolved since its initiation and provides a brief summary of the resulting corpus.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-53"
  },
  "sityaev06_interspeech": {
   "authors": [
    [
     "Dmitry",
     "Sityaev"
    ],
    [
     "Katherine",
     "Knill"
    ],
    [
     "Tina",
     "Burrows"
    ]
   ],
   "title": "Comparison of the ITU-t p.85 standard to other methods for the evaluation of text-to-speech systems",
   "original": "i06_1233",
   "page_count": 4,
   "order": 54,
   "p1": "paper 1233-Tue2WeO.3",
   "pn": "",
   "abstract": [
    "Evaluation of TTS systems is essential to assess performance. The ITUT P.85 standard was introduced in 1994 to assess the overall quality of speech synthesis systems. However it has not been widely accepted or used. This paper compares the ITU test to more commonly used tests for intelligibility (semantically unpredictable sentences (SUS)) and naturalness (mean opinion score based). The aim of this research was to determine if the ITU test can provide a better performance measure and/or supplementary information to help evaluate TTS systems.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-54"
  },
  "heeman06_interspeech": {
   "authors": [
    [
     "Peter A.",
     "Heeman"
    ],
    [
     "Andy",
     "McMillin"
    ],
    [
     "J. Scott",
     "Yaruss"
    ]
   ],
   "title": "An annotation scheme for complex disfluencies",
   "original": "i06_1859",
   "page_count": 4,
   "order": 55,
   "p1": "paper 1859-Tue2WeO.4",
   "pn": "",
   "abstract": [
    "In this paper, we present an annotation scheme for disfluencies. Unlike previous schemes, this scheme allows complex disfluencies with multiple backtracking points to be annotated, which are common in stuttered speech. The scheme specifies each disfluency in terms of word-level annotations, thus making the scheme useful for building sophisticated language models of disfluencies. As determining the annotation codes is quite difficult, we have developed a pen and paper procedure in which the annotator lines up the words into rows and columns, from which it is straight-forward for the annotator to determine the annotation tags.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-55"
  },
  "bael06_interspeech": {
   "authors": [
    [
     "Christophe Van",
     "Bael"
    ],
    [
     "Lou",
     "Boves"
    ],
    [
     "Henk van den",
     "Heuvel"
    ],
    [
     "Helmer",
     "Strik"
    ]
   ],
   "title": "Automatic phonetic transcription of large speech corpora: a comparative study",
   "original": "i06_1173",
   "page_count": 4,
   "order": 56,
   "p1": "paper 1173-Tue2WeO.5",
   "pn": "",
   "abstract": [
    "This study investigates whether automatic transcription procedures can approximate manual phonetic transcriptions typically delivered with contemporary large speech corpora. We used ten automatic procedures to generate a broad phonetic transcription of well-prepared speech (read-aloud texts) and spontaneous speech (telephone dialogues). The resulting transcriptions were compared to manually verified phonetic transcriptions. We found that the quality of this type of transcription can be approximated by a fairly simple and cost-effective procedure.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-56"
  },
  "shi06_interspeech": {
   "authors": [
    [
     "Yongmei",
     "Shi"
    ],
    [
     "Lina",
     "Zhou"
    ]
   ],
   "title": "Examining knowledge sources for human error correction",
   "original": "i06_1530",
   "page_count": 4,
   "order": 57,
   "p1": "paper 1530-Tue2WeO.6",
   "pn": "",
   "abstract": [
    "A variety of knowledge sources have been employed by error correction mechanisms to improve the usability of speech recognition (SR) technology. However, little is known about the effect of knowledge sources on human error correction. Advancing our understanding of the role of knowledge sources in human error correction could improve the state of automatic error correction. We selected three knowledge sources, including alternative list, imperfect context, and perfect context, and compared their usefulness to human error correction via an empirical user study. The results showed that knowledge sources had significant impact on the performance of human error correction. In particular, perfect context was the best that could significantly reduce word error rate without increasing the processing time.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-57"
  },
  "chang06_interspeech": {
   "authors": [
    [
     "Joon-Hyuk",
     "Chang"
    ],
    [
     "Woohyung",
     "Lim"
    ],
    [
     "Nam Soo",
     "Kim"
    ]
   ],
   "title": "Signal modification incorporating perceptual weighting filter",
   "original": "i06_1495",
   "page_count": 4,
   "order": 58,
   "p1": "paper 1495-Mon1FoP.1",
   "pn": "",
   "abstract": [
    "In this paper, an improved preprocessor for low-bit-rate speech coding employing the perceptual weighting filter is proposed. Speech modification in the proposed approach is performed according to a criterion which makes a compromise between the modification and perceptual weighted quantization errors. For this, the perceptual weighting filter is expressed in terms of a transform domain matrix. The proposed approach is effective in enhancing the speech signal at coder-decoder (CODEC) output through a number of listening tests.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-58"
  },
  "nurminen06_interspeech": {
   "authors": [
    [
     "Jani",
     "Nurminen"
    ]
   ],
   "title": "Enhanced dynamic codebook reordering for advanced quantizer structures",
   "original": "i06_1560",
   "page_count": 4,
   "order": 59,
   "p1": "paper 1560-Mon1FoP.2",
   "pn": "",
   "abstract": [
    "Conventional dynamic codebook reordering can often significantly enhance the achievable compression efficiency in simple one-stage vector quantization. When applied to more advanced quantizer structures, such as multi-stage vector quantizers, the performance of the technique becomes worse. This paper describes in detail an enhanced approach for dynamic codebook reordering that improves the performance by taking into account the whole quantizer structure. The significant efficiency improvements provided by the proposed approach are demonstrated in practical experiments. Though the results presented in this paper relate to a speech storage system, the proposed approach can also be employed more widely in compression applications that keep the encoder and the decoder in synchrony.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-59"
  },
  "lee06_interspeech": {
   "authors": [
    [
     "Chang-Heon",
     "Lee"
    ],
    [
     "Sung-Kyo",
     "Jung"
    ],
    [
     "Thomas",
     "Eriksson"
    ],
    [
     "Won-Suk",
     "Jun"
    ],
    [
     "Hong-Goo",
     "Kang"
    ]
   ],
   "title": "An efficient segment-based speech compression technique for hand-held TTS systems",
   "original": "i06_1980",
   "page_count": 4,
   "order": 60,
   "p1": "paper 1980-Mon1FoP.3",
   "pn": "",
   "abstract": [
    "This paper proposes a novel segment-based speech coding algorithm to efficiently compress the database for concatenative text-to-speech (TTS) systems. To achieve a high compression ratio and meet the fundamental requirements of concatenative TTS synthesizers, i.e. partial segment decoding and random access capability, we adopt a modified analysis-by-synthesis scheme. The spectral coefficients are quantized by a length-based interpolation method and excitation signals are modeled with both non-predictive and predictive approaches. Considering that pitch pulse waveforms of a specific speaker show low intra-variation, the conventional adaptive codebook for pitch prediction is replaced by a speaker dependent pitch-pulse codebook. By applying the proposed algorithm to a hand-held Korean TTS system, we verify that the proposed coder provides a compression ratio of about 1/13, a low complexity of around 1.2 WMOPS, and random access capability.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-60"
  },
  "ramasubramanian06_interspeech": {
   "authors": [
    [
     "V.",
     "Ramasubramanian"
    ],
    [
     "D.",
     "Harish"
    ]
   ],
   "title": "An unified unit-selection framework for ultra low bit-rate speech coding",
   "original": "i06_2028",
   "page_count": 4,
   "order": 61,
   "p1": "paper 2028-Mon1FoP.4",
   "pn": "",
   "abstract": [
    "We propose a unified framework for segment quantization of speech at ultra low bit-rates of 150 bits/sec based on unit-selection principle using a modified one-pass dynamic programming algorithm. The algorithm handles both fixed- and variable- length units in a unified manner, thereby providing a generalization over two existing unit selection methods, which deal with single-frame and segmental units differently. The proposed algorithm performs unit-selection based quantization directly on the units of a continuous codebook, thereby not incurring any of the sub-optimalities of the existing segmental algorithm. Moreover, the existing single-frame algorithm becomes a special case of the proposed algorithm. Based on the rate-distortion performance on a multi-speaker database, we show that fixed length units of 6-8 frames perform significantly better than single-frame units and offer similar spectral distortions as variable-length phonetic units, thereby circumventing expensive segmentation and labeling of a continuous database for unit selection based low bit-rate coding.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-61"
  },
  "thyssen06_interspeech": {
   "authors": [
    [
     "Jes",
     "Thyssen"
    ],
    [
     "Juin-Hwey",
     "Chen"
    ]
   ],
   "title": "Efficient VQ techniques and general noise shaping in noise feedback coding",
   "original": "i06_1254",
   "page_count": 4,
   "order": 62,
   "p1": "paper 1254-Mon1FoP.5",
   "pn": "",
   "abstract": [
    "This paper describes new efficient Vector Quantization (VQ) techniques that enable low complexity implementations of VQ-based Noise Feedback Coding (NFC). These methods offer mathematical equivalence to higher complexity methods. Furthermore, the paper presents efficient codec structures for general noise shaping as used in BroadVoice 16 - a new SCTE (Society of Cable Telecommunications Engineers) and PacketCable speech coding standard for Cable Telephony.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-62"
  },
  "qian06_interspeech": {
   "authors": [
    [
     "Yasheng",
     "Qian"
    ],
    [
     "Wei-Shou",
     "Hsu"
    ],
    [
     "Peter",
     "Kabal"
    ]
   ],
   "title": "Classified comfort noise generation for efficient voice transmission",
   "original": "i06_1307",
   "page_count": 4,
   "order": 63,
   "p1": "paper 1307-Mon1FoP.6",
   "pn": "",
   "abstract": [
    "Comfort noise insertion during speech pause has been applied to Voice-over-IP and wireless networks for increasing bandwidth efficiency. We present two classified comfort noise generation (CCNG) schemes using Gaussian Mixture classifiers (GMM-C). Our first scheme employs a classified prototype background noise codebook with the prototype noise waveform chosen using a GMM-C. The second scheme utilizes a classified enhanced excitation codebook. The new CCNG algorithms provide better comfort noise during speech pauses and a smaller misclassification rate. We have retrofitted the scheme into existing speech transmission system, such as ITU-T G.711/Appendix II and G.723.1/Annex A. The perceived quality of a voice conversation of the novel system has been noticeably enhanced for car and babble noise. For the G.711 system, a large improvement is obtained for car noise while the largest amelioration is for babble noise in the G.723.1 system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-63"
  },
  "kovesi06_interspeech": {
   "authors": [
    [
     "Balázs",
     "Kövesi"
    ],
    [
     "Dominique",
     "Massaloux"
    ],
    [
     "David",
     "Virette"
    ],
    [
     "Julien",
     "Bensa"
    ]
   ],
   "title": "Integration of a CELP coder in the ARDOR universal sound codec",
   "original": "i06_1309",
   "page_count": 4,
   "order": 64,
   "p1": "paper 1309-Mon1FoP.7",
   "pn": "",
   "abstract": [
    "This paper describes the CELP coding module within the Adaptive Rate-Distortion Optimized sound codeR (ARDOR). The ARDOR codec combines coding techniques of different nature using a rate-distortion control mechanism, and is able to adapt to a large range of signal characteristics and system constraints. The implemented CELP codec is derived from the 3GPP AMR-WB codec. Adaptations were necessary to match the ARDOR structure constraints and several new features have been added to improve the codec performance in this context. Listening test results are given to illustrate the behavior of the final codec compared to state-of-the-art coders.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-64"
  },
  "chatterjee06_interspeech": {
   "authors": [
    [
     "Saikat",
     "Chatterjee"
    ],
    [
     "T. V.",
     "Sreenivas"
    ]
   ],
   "title": "Two stage transform vector quantization of LSFs for wideband speech coding",
   "original": "i06_1433",
   "page_count": 4,
   "order": 65,
   "p1": "paper 1433-Mon1FoP.8",
   "pn": "",
   "abstract": [
    "We investigate the use of a two stage transform vector quantizer (TSTVQ) for coding of line spectral frequency (LSF) parameters in wideband speech coding. The first stage quantizer of TSTVQ, provides better matching of source distribution and the second stage quantizer provides additional coding gain through using an individual cluster specific decorrelating transform and variance normalization. Further coding gain is shown to be achieved by exploiting the slow time-varying nature of speech spectra and thus using inter-frame cluster continuity (ICC) property in the first stage of TSTVQ method. The proposed method saves 3-4 bits and reduces the computational complexity by 58-66%, compared to the traditional split vector quantizer (SVQ), but at the expense of 1.5-2.5 times of memory.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-65"
  },
  "chatterjee06b_interspeech": {
   "authors": [
    [
     "Saikat",
     "Chatterjee"
    ],
    [
     "T. V.",
     "Sreenivas"
    ]
   ],
   "title": "Comparison of prediction based LSF quantization methods using split VQ",
   "original": "i06_1435",
   "page_count": 4,
   "order": 66,
   "p1": "paper 1435-Mon1FoP.9",
   "pn": "",
   "abstract": [
    "Further improvement in performance, to achieve near transparent quality LSF quantization, is shown to be possible by using a higher order two dimensional (2-D) prediction in the coefficient domain. The prediction is performed in a closed-loop manner so that the LSF reconstruction error is the same as the quantization error of the prediction residual. We show that an optimum 2-D predictor, exploiting both inter-frame and intra-frame correlations, performs better than existing predictive methods. Computationally efficient split vector quantization technique is used to implement the proposed 2-D prediction based method. We show further improvement in performance by using weighted Euclidean distance.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-66"
  },
  "hofbauer06_interspeech": {
   "authors": [
    [
     "Konrad",
     "Hofbauer"
    ],
    [
     "Gernot",
     "Kubin"
    ]
   ],
   "title": "High-rate data embedding in unvoiced speech",
   "original": "i06_1906",
   "page_count": 4,
   "order": 67,
   "p1": "paper 1906-Mon1FoP.10",
   "pn": "",
   "abstract": [
    "We propose a blind speech watermarking algorithm which allows high-rate embedding of digital side information into speech signals. We exploit the fact that the well-known LPC vocoder works very well for unvoiced speech. Using an auto-correlation based pitch tracking algorithm, a voiced/unvoiced segmentation is carried out. In the unvoiced segments, the linear prediction residual is replaced by a data sequence. This substitution does not cause perceptual degradation as long as the residuals power is matched. The signal is resynthesised using the unmodified LPC filter coefficients. The watermark is decoded by a linear prediction analysis of the received signal and the information is extracted from the sign of the residual. The watermark is nearly imperceptible and provides a channel capacity of up to 2000 bit/s in an 8 kHz-sampled speech signal.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-67"
  },
  "anderson06_interspeech": {
   "authors": [
    [
     "Kyle D.",
     "Anderson"
    ],
    [
     "Philippe",
     "Gournay"
    ]
   ],
   "title": "Pitch resynchronization while recovering from a late frame in a predictive speech decoder",
   "original": "i06_1029",
   "page_count": 4,
   "order": 68,
   "p1": "paper 1029-Mon1FoP.11",
   "pn": "",
   "abstract": [
    "The concealment procedure used by CELP speech decoders to regenerate lost frames introduces an error that propagates into the following frames. Within the context of voice transmission over packet networks, some packets arrive too late to be decoded and must also be concealed. Once they arrive however, those packets can be used to update the internal state of the decoder, which stops error propagation. Yet, care must be taken to ensure a smooth transition between the concealed frame and the following \"updated\" frame computed with properly updated internal states. During voiced or quasi-periodic segments, the pitch phase error that is generally introduced by the concealment procedure makes it difficult and detrimental to quality to use the traditional fade-in, fade-out approach. This paper presents a method to handle that pitch phase error. Specifically, the transition is done in such a way that the natural pitch periodicity of the speech signal is not broken.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-68"
  },
  "suhadi06_interspeech": {
   "authors": [
    [
     "Suhadi",
     "Suhadi"
    ],
    [
     "Sorel",
     "Stan"
    ],
    [
     "Tim",
     "Fingscheidt"
    ]
   ],
   "title": "A novel environment-dependent speech enhancement method with optimized memory footprint",
   "original": "i06_1181",
   "page_count": 4,
   "order": 69,
   "p1": "paper 1181-Mon2A1O.1",
   "pn": "",
   "abstract": [
    "Data-driven speech enhancement (Fingscheidt and Suhadi [1]) aims at improving speech quality for voice calls in a specific noise environment. The essence of the method are a set of frequency-dependent weighting rules, indexed by a priori and a posteriori SNRs, which are learned from clean speech and background noise training data. The weighting rules must be stored for each frequency bin separately and take up about 400 kBytes memory, which makes DSP implementations relatively expensive.\n",
    "In this paper we propose an alternative definition of the weighting rules which requires only 27 kBytes memory. That is 6.7% of the memory consumption of the original algorithm, with virtually no loss in performance measured in terms of speech distortion and noise attenuation. Our approach is to redefine the weighting rules on the Bark scale and store their parametric representation obtained by polynomial curve fitting.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-69"
  },
  "zavarehei06_interspeech": {
   "authors": [
    [
     "Esfandiar",
     "Zavarehei"
    ],
    [
     "Saeed",
     "Vaseghi"
    ],
    [
     "Qin",
     "Yan"
    ]
   ],
   "title": "Weighted codebook mapping for noisy speech enhancement using harmonic-noise model",
   "original": "i06_1244",
   "page_count": 4,
   "order": 70,
   "p1": "paper 1244-Mon2A1O.2",
   "pn": "",
   "abstract": [
    "Most noisy speech enhancement methods result in partial suppression and distortion of speech spectrum. At instances when the local signal-to-noise ratio at a frequency band is very low speech partials are often obliterated. In this paper a method for enhancement and restoration of noisy speech based on a harmonic-noise model (HNM) is introduced. A HNM imposes a temporal-spectral structure that may reduce processing artifacts. The restoration process is enhanced through incorporation of a prior HNM of clean speech stored in a pre-trained codebook. The restored speech is a SNR-dependent combination of the de-noised observation and the speech obtained from weighted codebook mapping. The additional improvements of speech quality resulting from the proposed method in comparison to conventional and modern speech enhancement systems are evaluated. The results show that the proposed method improves the quality of noisy speech and restores much of the information lost to noise.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-70"
  },
  "jensen06_interspeech": {
   "authors": [
    [
     "J.",
     "Jensen"
    ],
    [
     "R. C.",
     "Hendriks"
    ],
    [
     "J. S.",
     "Erkelens"
    ],
    [
     "R.",
     "Heusdens"
    ]
   ],
   "title": "MMSE estimation of complex-valued discrete Fourier coefficients with generalized gamma priors",
   "original": "i06_1277",
   "page_count": 4,
   "order": 71,
   "p1": "paper 1277-Mon2A1O.3",
   "pn": "",
   "abstract": [
    "We consider DFT based techniques for single-channel speech enhancement. Specifically, we derive minimum mean-square error estimators of clean speech DFT coefficients based on generalized gamma prior probability density functions. Our estimators contain as special cases the well-known Wiener estimator and the more recently derived estimators based on Laplacian and two-sided gamma priors. Simulation experiments with speech signals degraded by various additive noise sources verify that the estimator based on the two-sided gamma prior is close to optimal amongst all the estimators considered in this paper.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-71"
  },
  "subramanya06_interspeech": {
   "authors": [
    [
     "Amarnag",
     "Subramanya"
    ],
    [
     "Michael L.",
     "Seltzer"
    ],
    [
     "Alex",
     "Acero"
    ]
   ],
   "title": "Automatic removal of typed keystrokes from speech signals",
   "original": "i06_1324",
   "page_count": 4,
   "order": 72,
   "p1": "paper 1324-Mon2A1O.4",
   "pn": "",
   "abstract": [
    "Laptop computers are increasingly being used as recording devices to capture meetings, interviews, and lectures using the laptops local microphone. In these scenarios, the user frequently also uses the same laptop to make notes. Because of the close proximity of the laptops microphone to its keyboard, the captured speech signal is significantly corrupted by the impulsive sounds the users keystrokes generate. In this paper we propose an algorithm to automatically detect and remove keystrokes from a recorded speech signal. The detection and removal stages both operate by exploiting the natural correlations present in speech signals, but do so in different ways. The proposed algorithm is computationally efficient, requires no userspecific training or enrolment, and results in significantly enhanced speech. The proposed keystroke removal algorithm was evaluated through user listening tests and speech recognition experiments on speech recordings made in a realistic environment.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-72"
  },
  "rank06_interspeech": {
   "authors": [
    [
     "Erhard",
     "Rank"
    ],
    [
     "Gernot",
     "Kubin"
    ]
   ],
   "title": "Lattice LP filtering for noise reduction in speech signals",
   "original": "i06_1643",
   "page_count": 4,
   "order": 73,
   "p1": "paper 1643-Mon2A1O.5",
   "pn": "",
   "abstract": [
    "We present a simple yet effective algorithm for noise reduction of speech signals using a lattice LP filter. Based on previous investigations and a theoretical analysis of the lattice filter parameter estimation we introduce an improved parameter estimation algorithm that takes into account the non-stationary nature of speech and expected noise signals, yielding a good suppression of stationary and slowly timevarying noise. The algorithm has zero delay for the speech signal, promoting its application for telephony or hearing aids. No additional or explicit noise estimation algorithm is needed.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-73"
  },
  "deshmukh06b_interspeech": {
   "authors": [
    [
     "Om D.",
     "Deshmukh"
    ],
    [
     "Carol Y.",
     "Espy-Wilson"
    ]
   ],
   "title": "Speech enhancement using modified phase opponency model",
   "original": "i06_1699",
   "page_count": 4,
   "order": 74,
   "p1": "paper 1699-Mon2A1O.6",
   "pn": "",
   "abstract": [
    "We previously presented a single-channel speech enhancement technique called the Modified Phase Opponency (MPO) model. The MPO model is based on a neural model called the Phase Opponency (PO) model. The efficacy of the MPO model was demonstrated on speech signals corrupted by additive white noise. In the present work, we extend the MPO model to perform efficiently on speech signals corrupted by additive colored noise with time varying spectral characteristics and amplitude levels. The MPO enhancement scheme outperforms many of the statistical and signal-theoretic speech enhancement techniques when evaluated using three different objective quality measures on a subset of the Aurora database. The superiority of the MPO speech enhancement scheme in enhancing speech signals when the amplitude level and the spectral characteristics of the background noise are fluctuating is also demonstrated.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-74"
  },
  "jin06_interspeech": {
   "authors": [
    [
     "Wen",
     "Jin"
    ],
    [
     "Michael",
     "Scordilis"
    ]
   ],
   "title": "Single channel speech enhancement by frequency domain constrained optimization and temporal masking",
   "original": "i06_1027",
   "page_count": 4,
   "order": 75,
   "p1": "paper 1027-Tue3FoP.1",
   "pn": "",
   "abstract": [
    "A speech enhancement algorithm is proposed that exploits the masking properties of the human auditory system. The enhancement is formulated as a frequency domain constrained optimization problem. The noise components of the noisy speech are suppressed by a gain function subject to the constraint that both the signal distortion and residual noise should fall below the masking thresholds. Temporal as well as simultaneous masking effects are incorporated into the estimation of masking thresholds. The enhancement algorithm was tested with speech corrupted by white Gaussian and multitalker babble noise, respectively. Its performance was evaluated by ITU PESQ scores and segmental SNR. Experimental results indicate that the proposed gain function performs slightly but consistently better than a former perceptually motivated enhancement algorithm. Greater improvement is achieved by incorporating the temporal masking effects.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-75"
  },
  "shin06_interspeech": {
   "authors": [
    [
     "Jong Won",
     "Shin"
    ],
    [
     "Seung Yeol",
     "Lee"
    ],
    [
     "Hwan Sik",
     "Yun"
    ],
    [
     "Nam Soo",
     "Kim"
    ]
   ],
   "title": "Speech enhancement based on residual noise shaping",
   "original": "i06_1201",
   "page_count": 4,
   "order": 76,
   "p1": "paper 1201-Tue3FoP.2",
   "pn": "",
   "abstract": [
    "In this paper, we propose a novel approach to speech enhancement, which incorporates a new criterion based on residual noise shaping. In the proposed approach, our goal is to make the residual noise perceptually comfortable although the power of the residual noise is relatively high. In contrast to the conventional techniques, the proposed approach regulates not only the power of the signal distortion and residual noise but also the spectral shape of the residual noise. A predetermined comfort noise is provided as a target for the spectral shaping. Three different versions of enhancement algorithms adopting the proposed criterion are presented. Subjective listening test results show that the proposed algorithm outperforms the conventional spectral enhancement techniques which are based on soft decision and the noise suppression module implemented in IS-893 Selectable Mode Vocoder.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-76"
  },
  "pulakka06_interspeech": {
   "authors": [
    [
     "Hannu",
     "Pulakka"
    ],
    [
     "Laura",
     "Laaksonen"
    ],
    [
     "Paavo",
     "Alku"
    ]
   ],
   "title": "Quality improvement of telephone speech by artificial bandwidth expansion - listening tests in three languages",
   "original": "i06_1245",
   "page_count": 4,
   "order": 77,
   "p1": "paper 1245-Tue3FoP.3",
   "pn": "",
   "abstract": [
    "Quality and intelligibility of narrowband telephone speech can be improved by artificial bandwidth expansion (ABE), which expands the speech bandwidth using only information available in the narrowband speech signal. This paper describes an ABE method that generates a high-band expansion using spectral folding and then modifies the magnitude spectrum of the expansion band with spline curves. The performance of the ABE algorithm was evaluated by formal listening tests in three languages: American English, Russian, and Mandarin Chinese. The results of the listening tests indicate that ABE-processed speech was preferred to narrowband speech in all tested languages.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-77"
  },
  "shannon06_interspeech": {
   "authors": [
    [
     "Benjamin J.",
     "Shannon"
    ],
    [
     "Kuldip K.",
     "Paliwal"
    ]
   ],
   "title": "Role of phase estimation in speech enhancement",
   "original": "i06_1330",
   "page_count": 4,
   "order": 78,
   "p1": "paper 1330-Tue3FoP.4",
   "pn": "",
   "abstract": [
    "Typical speech enhancement algorithms that operate in the Fourier domain only modify the magnitude component. It is commonly understood that the phase component is perceptually unimportant, and thus, it is passed directly to the output.\n",
    "In recent intelligibility experiments, it has been reported that the Short-Time Fourier Transform (STFT) phase spectrum can provide significant intelligibility when estimated using a window function lower in dynamic range than the typical Hamming window. Motivated by this, we investigate the role of the window function for STFT phase estimation in relation to speech enhancement.\n",
    "Using a modified STFT Analysis-Modification-Synthesis (AMS) framework, we show that noise reduction can be achieved by modifying the window function used to estimate the STFT phase spectra. We demonstrate this through spectrogram plots and results from two objective speech quality measures.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-78"
  },
  "shannon06b_interspeech": {
   "authors": [
    [
     "Benjamin J.",
     "Shannon"
    ],
    [
     "Kuldip K.",
     "Paliwal"
    ],
    [
     "Climent",
     "Nadeu"
    ]
   ],
   "title": "Speech enhancement based on spectral estimation from higher-lag autocorrelation",
   "original": "i06_1331",
   "page_count": 4,
   "order": 79,
   "p1": "paper 1331-Tue3FoP.5",
   "pn": "",
   "abstract": [
    "In this paper, we propose a unique approach to enhance speech signals that have been corrupted by non-stationary noises. This approach is not based on a spectral subtraction algorithm, but on an algorithm that separates the speech signal and noise signal contributions in the autocorrelation domain. We call this technique the AR-HASE speech enhancement algorithm.\n",
    "In this initial study, we evaluate the performance of the new algorithm using the average PESQ score computed from 10 male utterances and 10 female utterances taken from the TIMIT database as a measure of speech quality. We test the algorithm using one broadband stationary noise and two non-stationary noises. We will show that the AR-HASE enhancement algorithm produces near transparent quality for clean speech, gives poor enhancement performance for broadband stationary noises, and gives significantly enhanced quality for the two nonstationary noises.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-79"
  },
  "krishnamurthy06_interspeech": {
   "authors": [
    [
     "Nitish",
     "Krishnamurthy"
    ],
    [
     "John H. L.",
     "Hansen"
    ]
   ],
   "title": "Noise update modeling for speech enhancement: when do we do enough?",
   "original": "i06_1396",
   "page_count": 4,
   "order": 80,
   "p1": "paper 1396-Tue3FoP.6",
   "pn": "",
   "abstract": [
    "In speech enhancement, it is generally assumed that if you can update your noise estimate on a frame-by-frame basis, you should achieve the highest level of enhancement performance. However, for many noise types and environmental conditions, it is not necessary to perform an update on a frame-by-frame basis to achieve superior performance if the noise structure does not change rapidly. For applications where compute/memory resources are limited, better overall speech performance could be achieved if a more reasonable update rate is estimated so that available compute/memory resources could be made available to the enhancement algorithm itself. In this study, we propose a framework to model the noise structure with the goal of determining the best update rate required to achieve a given quality for speech enhancement. Speech systems generally develop specialized solutions for noise which are unique to each application (i.e., recognition, speaker ID, enhancement etc.). Here we propose a model to predict the noise update rate required to achieve a given quality for enhancement. We evaluate the algorithm across a corpus of four noise types under different levels of degradation. The error between the mean observed and the mean predicted Itakuta-Saito (IS) values of quality are typically between 0.06 to 1.78 IS for our model selected noise frame update rate of 1 frame every 5 frames using the Log-MMSE enhancement scheme. Finally we consider mobile and resource limited applications where such a framework would be useful.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-80"
  },
  "shahina06_interspeech": {
   "authors": [
    [
     "A.",
     "Shahina"
    ],
    [
     "B.",
     "Yegnanarayana"
    ]
   ],
   "title": "Mapping neural networks for bandwidth extension of narrowband speech",
   "original": "i06_1840",
   "page_count": 4,
   "order": 81,
   "p1": "paper 1840-Tue3FoP.7",
   "pn": "",
   "abstract": [
    "This paper exploits the nonlinear mapping property of feed-forward neural networks for estimation of high frequency components (4-8kHz) of the speech signals from the band-limited (0-4kHz) signals. Cepstral coefficients are used to represent the feature vectors of each frame of data. This paper also proposes an approach that uses the autocorrelation method to derive the Linear Prediction (LP) coefficients from the estimated cepstral coefficients that are obtained from the mapping network. This method guarantees the stability of the LP synthesis filter. Informal listenings indicate the effectiveness of the proposed method for estimation of wideband frequency components of speech. The enhanced speech sounds similar to the original wideband speech. Also, it does not contain any distortion that may arise due to spectral discontinuities between adjacent frames.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-81"
  },
  "das06_interspeech": {
   "authors": [
    [
     "Amit",
     "Das"
    ],
    [
     "John H. L.",
     "Hansen"
    ]
   ],
   "title": "Decision directed constrained iterative speech enhancement",
   "original": "i06_1866",
   "page_count": 4,
   "order": 82,
   "p1": "paper 1866-Tue3FoP.8",
   "pn": "",
   "abstract": [
    "Earlier studies have shown that degradation due to environmental background noise is non-uniform across various phoneme classes of speech. In this study, we present an improved formulation of single channel constrained iterative speech enhancement (AutoLSP) that follows a rover based paradigm. The new approach overcomes some of the drawbacks observed earlier in the baseline AutoLSP system. First, it eliminates the sensitivity to proper determination of the terminating iteration. Second, it employs a phone level non-uniform enhancement approach which significantly improves perceptual quality of the overall utterance. Third, audible noise components are suppressed by incorporating an auditory masked threshold (AMT) framework. The proposed algorithm is evaluated using Itakura-Saito (IS) objective quality measure over four noise sources and two SNR levels. Comparative evaluations with other baseline systems (AutoLSP, log-MMSE) reveal that the new algorithm exhibits consistent quality improvement for each noise case over all phoneme classes in the TIMIT corpus. Reduction in IS distance over degraded speech is observed in the range of 35.09-46.88%. The Rover scheme outperforms AutoLSP and log-MMSE by 9.21% and 11.19% respectively using IS scores.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-82"
  },
  "murakami06_interspeech": {
   "authors": [
    [
     "Takahiro",
     "Murakami"
    ],
    [
     "Yoshihisa",
     "Ishida"
    ]
   ],
   "title": "Adaptive filtering for attenuating musical noise caused by spectral subtraction",
   "original": "i06_1919",
   "page_count": 4,
   "order": 83,
   "p1": "paper 1919-Tue3FoP.9",
   "pn": "",
   "abstract": [
    "A method of alleviating processing distortion caused by spectral subtraction is presented. It is well known that the spectral subtraction introduces annoying artifacts, which are referred to as undesirable musical noise, in the enhanced speech. The enhancement quality of the spectral subtraction quite depends on the performance of reducing the musical noise. Our approach exploits an adaptive filter in order to eliminate such distortion. In the method, the enhanced speech obtained by the spectral subtraction is used as a reference signal of the adaptive filter. The proposed method utilizes the characteristic difference between the musical noise and speech, i.e., the majority of the frequency components consisting the musical noise have the duration shorter than those of speech. Therefore, when the convergence speed of the adaptive filter is slower than the lifetime of the musical noise but faster than that of speech, only speech components can be tracked by the filter while the musical noise components are attenuated. Simulation results show that the proposed method can efficiently reduce the musical noise and the enhancement quality is improved in comparison with the conventional spectral subtraction.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-83"
  },
  "hu06_interspeech": {
   "authors": [
    [
     "Yi",
     "Hu"
    ],
    [
     "Philipos C.",
     "Loizou"
    ]
   ],
   "title": "Evaluation of objective measures for speech enhancement",
   "original": "i06_2007",
   "page_count": 4,
   "order": 84,
   "p1": "paper 2007-Tue3FoP.10",
   "pn": "",
   "abstract": [
    "In this paper, we evaluate the performance of several objective measures in terms of predicting the quality of noisy speech enhanced by noise suppression algorithms. The objective measures considered a wide range of distortions introduced by four types of real-world noise at two SNRs by four classes of speech enhancement algorithms: spectral subtractive, subspace, statistical-model based and Wiener algorithms. The subjective quality ratings were obtained using the ITU-T P.835 methodology designed to evaluate the speech quality along three dimensions: signal distortion, noise distortion and overall quality. This paper reports the correlations of five common objective measures with these three subjective measures. Improvements to the PESQ measure are reported along with new composite objective measures.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-84"
  },
  "song06_interspeech": {
   "authors": [
    [
     "Myung-Suk",
     "Song"
    ],
    [
     "Chang-Heon",
     "Lee"
    ],
    [
     "Hong-Goo",
     "Kang"
    ]
   ],
   "title": "Performance analysis of various single channel speech enhancement algorithms for automatic speech recognition",
   "original": "i06_2073",
   "page_count": 4,
   "order": 85,
   "p1": "paper 2073-Tue3FoP.11",
   "pn": "",
   "abstract": [
    "This paper analyzes the performance of various single channel speech enhancement systems when they are applied to automatic speech recognition (ASR) systems as a preprocessor. Until now the researches on speech enhancement algorithms have focused on improving the perceptual quality of speech signal. However, it has not been verified yet whether the improvements of the perceptual quality also increase the speech recognition rate. By investigating several enhancement modules designed for improving the perceptual quality, we analyze the relationship between a speech recognizer and speech enhancement systems. Simulation results show that the decision-directed method and speech absence probability (SAP) estimation proposed for improving perceptual quality influence adverse effects to the speech recognition performance.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-85"
  },
  "boulianne06_interspeech": {
   "authors": [
    [
     "G.",
     "Boulianne"
    ],
    [
     "J.-F.",
     "Beaumont"
    ],
    [
     "M.",
     "Boisvert"
    ],
    [
     "J.",
     "Brousseau"
    ],
    [
     "P.",
     "Cardinal"
    ],
    [
     "C.",
     "Chapdelaine"
    ],
    [
     "M.",
     "Comeau"
    ],
    [
     "Pierre",
     "Ouellet"
    ],
    [
     "F.",
     "Osterrath"
    ]
   ],
   "title": "Computer-assisted closed-captioning of live TV broadcasts in French",
   "original": "i06_1424",
   "page_count": 4,
   "order": 86,
   "p1": "paper 1424-Mon2A2O.1",
   "pn": "",
   "abstract": [
    "Growing needs for French closed-captioning of live TV broadcasts in Canada cannot be met only with stenography-based technology because of a chronic shortage of skilled stenographers. Using speech recognition for live closed-captioning, however, requires several specific problems to be solved, such as the need for low-latency real-time recognition, remote operation, automated model updates, and collaborative work. In this paper we describe our solutions to these problems and the implementation of a live captioning system based on the CRIM speech recognizer. We report results from field deployment in several projects. The oldest in operation has been broadcasting real-time closed-captions for more than 2 years.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-86"
  },
  "afify06_interspeech": {
   "authors": [
    [
     "Mohamed",
     "Afify"
    ],
    [
     "Ruhi",
     "Sarikaya"
    ],
    [
     "Hong-Kwang Jeff",
     "Kuo"
    ],
    [
     "Laurent",
     "Besacier"
    ],
    [
     "Yuqing",
     "Gao"
    ]
   ],
   "title": "On the use of morphological analysis for dialectal Arabic speech recognition",
   "original": "i06_1444",
   "page_count": 4,
   "order": 87,
   "p1": "paper 1444-Mon2A2O.2",
   "pn": "",
   "abstract": [
    "Arabic has a large number of affixes that can modify a stem to form words. In automatic speech recognition (ASR) this leads to a high outof- vocabulary (OOV) rate for typical lexicon size, and hence a potential increase in WER. This is even more pronounced for dialects of Arabic where additional affixes are often introduced and the available data is typically sparse. To address this problem we introduce a simple word decomposition algorithm which only requires a text corpus and a predefined list of affixes. Using this algorithm to create the lexicon for Iraqi Arabic ASR results in about 10% relative improvement in word error rate (WER). Also using the union of the segmented and unsegmented vocabularies and interpolating the corresponding language models results in further WER reduction. The net WER improvement is about 13% relative.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-87"
  },
  "trancoso06_interspeech": {
   "authors": [
    [
     "Isabel",
     "Trancoso"
    ],
    [
     "Ricardo",
     "Nunes"
    ],
    [
     "Luís",
     "Neves"
    ],
    [
     "Céu",
     "Viana"
    ],
    [
     "Helena",
     "Moniz"
    ],
    [
     "Diamantino",
     "Caseiro"
    ],
    [
     "Ana Isabel",
     "Mata"
    ]
   ],
   "title": "Recognition of classroom lectures in european portuguese",
   "original": "i06_1524",
   "page_count": 4,
   "order": 88,
   "p1": "paper 1524-Mon2A2O.3",
   "pn": "",
   "abstract": [
    "Classroom lectures may be very challenging for automatic speech recognizers, because the vocabulary may be very specific and the speaking style very spontaneous. Our first experiments using a recognizer trained for Broadcast News resulted in word error rates near 60%, clearly confirming the need for adaptation to the specific topic of the lectures, on one hand, and for better strategies for handling spontaneous speech. This paper describes our efforts in these two directions: the different domain adaptation steps that lowered the error rate to 45%, with very little transcribed adaptation material, and the exploratory study of spontaneous speech phenomena in European Portuguese, namely concerning filled pauses.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-88"
  },
  "pellegrini06_interspeech": {
   "authors": [
    [
     "Thomas",
     "Pellegrini"
    ],
    [
     "Lori",
     "Lamel"
    ]
   ],
   "title": "Investigating automatic decomposition for ASR in less represented languages",
   "original": "i06_1776",
   "page_count": 4,
   "order": 89,
   "p1": "paper 1776-Mon2A2O.4",
   "pn": "",
   "abstract": [
    "This paper addresses the use of an automatic decomposition method to reduce lexical variety and thereby improve speech recognition of less well-represented languages. The Amharic language has been selected for these experiments since only a small quantity of resources are available compared to well-covered languages. Inspired by the Harris algorithm, the method automatically generates plausible affixes, that combined with decompounding can reduce the size of the lexicon and the OOV rate. Recognition experiments are carried out for four different configurations (full-word and decompounded) and using supervised training with a corpus containing only two hours of manually transcribed data.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-89"
  },
  "nimaan06_interspeech": {
   "authors": [
    [
     "Abdillahi",
     "Nimaan"
    ],
    [
     "Pascal",
     "Nocéra"
    ],
    [
     "Jean-François",
     "Bonastre"
    ]
   ],
   "title": "Automatic transcription of Somali language",
   "original": "i06_1817",
   "page_count": 4,
   "order": 90,
   "p1": "paper 1817-Mon2A2O.5",
   "pn": "",
   "abstract": [
    "Most African countries follow an oral tradition system to transmit their cultural, scientific and historic heritage through generations. This ancestral knowledge accumulated during centuries is today threatened of disappearing. Automatic transcription and indexing tools seem potential solution to preserve it. This paper presents the first steps of automatic speech recognition (ASR) of Djibouti languages in order to index the Djibouti cultural heritage. This work is dedicated to process Somali language, which represents half of the targeted Djiboutian audio archives. We describe the principal characteristics of audio (10 hours) and textual (3M words) training corpora collected and the first ASR results of this language. Using the specificities of the Somali language, (words are composed of a concatenation of sub-words called \"roots\" in this paper), we improve the obtained results. We also discuss future ways of research like roots indexing of audio archives.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-90"
  },
  "cetin06_interspeech": {
   "authors": [
    [
     "Özgür",
     "Çetin"
    ],
    [
     "Elizabeth",
     "Shriberg"
    ]
   ],
   "title": "Analysis of overlaps in meetings by dialog factors, hot spots, speakers, and collection site: insights for automatic speech recognition",
   "original": "i06_1915",
   "page_count": 4,
   "order": 91,
   "p1": "paper 1915-Mon2A2O.6",
   "pn": "",
   "abstract": [
    "In previous work we found that automatic speech recognition (ASR) results on meetings show interesting patterns with respect to speaker overlaps, including a robust asymmetry in word error rates (WERs) before and after overlaps. The paradigm used allowed us to infer that these correlations are not due to crosstalk itself but to changes in how a person speaks around overlap regions. To better understand these ASR and perplexity results, we analyze speaker overlaps with respect to various factors, including collection site, speakers, dialog acts, and hot spots.\n",
    "We examine a total of 101 meetings from the ICSI meeting corpus and the NIST meeting transcription evaluations of the last four years. We find that overlaps tend to occur at high-perplexity regions in the foreground talkers speech. We also find that overlap regions tend to have higher perplexity than those in nonoverlaps, if trigrams or 4-grams are used, but unigram perplexity within overlaps is considerably lower than that of nonoverlaps. These appear to be robust findings, because they hold in general across meetings from different collection sites, even though meeting style and absolute rates of overlap vary by site. Further analyses of overlap with respect to speakers and meeting content reveal interesting relationships between overlap and dialog acts, as well as between overlap and \"hot spots\" (points of increased participant involvement). Finally, results from the ICSI meeting corpus show that individual speakers have widely varying rates of being overlapped.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-91"
  },
  "takeda06_interspeech": {
   "authors": [
    [
     "Ryu",
     "Takeda"
    ],
    [
     "Shun'ichi",
     "Yamamoto"
    ],
    [
     "Kazunori",
     "Komatani"
    ],
    [
     "Tetsuya",
     "Ogata"
    ],
    [
     "Hiroshi G.",
     "Okuno"
    ]
   ],
   "title": "Improving speech recognition of two simultaneous speech signals by integrating ICA BSS and automatic missing feature mask generation",
   "original": "i06_1729",
   "page_count": 4,
   "order": 92,
   "p1": "paper 1729-Thu1CaP.1",
   "pn": "",
   "abstract": [
    "Robot audition systems require capabilities for sound source separation and the recognition of separated sounds, since we hear a mixture of sounds in our daily lives, especially mixed of speech. We report a robot audition system with a pair of omni-directional microphones embedded in a humanoid that recognizes two simultaneous talkers. It first separates the sound sources by Independent Component Analysis (ICA) with the single-input multiple-output (SIMO) model. Then, spectral distortion in the separated sounds is then estimated to generate missing feature masks. Finally, the separated sounds are recognized by missing-feature theory (MFT) for Automatic Speech Recognition (ASR). The novel aspects of our system involve estimates of spectral distortion in the temporal-frequency domain in terms of feature vectors and based on estimates error in SIMO-ICA signals. The resulting system outperformed the baseline robot audition system by 7%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-92"
  },
  "kim06_interspeech": {
   "authors": [
    [
     "Wooil",
     "Kim"
    ],
    [
     "John H. L.",
     "Hansen"
    ]
   ],
   "title": "Missing-feature reconstruction for band-limited speech recognition in spoken document retrieval",
   "original": "i06_1826",
   "page_count": 4,
   "order": 93,
   "p1": "paper 1826-Thu1CaP.2",
   "pn": "",
   "abstract": [
    "In spoken document retrieval, it is necessary to support a variety of audio corpora from sources that have a range of conditions (e.g., channels, microphones, noise conditions, recording media, etc.). Varying band-limited speech represents one of the most challenging factors for robust speech recognition. The missing-feature reconstruction method shows the effectiveness in recognition of the speech corrupted by additive noise. However, it has a problem when applied to the band-limited speech reconstruction, since it assumes that the observations in the unreliable regions are always greater than the latent original clean speech. In this study, we propose to modify the current way to calculate the marginal probability for reconstruction into the computation depending only on the reliable components. To detect the cut-off regions from incoming speech, the blind mask estimation scheme is proposed, which employs the synthesized band-limited speech model without training database. Experimental results on Aurora 2.0 and actual band-limited speech (NGSW corpus) indicate that the proposed method is effective in improving recognition accuracy of the band-limited speech. Through combining with an adaptation method, 22.17% of relative improvement is obtained on NGSW.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-93"
  },
  "koo06_interspeech": {
   "authors": [
    [
     "Hahn",
     "Koo"
    ],
    [
     "Yan Ming",
     "Cheng"
    ]
   ],
   "title": "Incremental learning of MAP context-dependent edit operations for spoken phone number recognition in an embedded platform",
   "original": "i06_1032",
   "page_count": 4,
   "order": 94,
   "p1": "paper 1032-Thu1CaP.3",
   "pn": "",
   "abstract": [
    "Error-corrective post-processing (ECPP) has great potential to reduce speech recognition errors beyond that obtained by speech model improvement. ECPP approaches aim to learn error-corrective rules to directly reduce speech recognition errors. This paper presents our investigation into one such approach, incremental learning of maximum a posteriori (MAP) context-dependent edit operations. Limiting our dataset to spoken telephone number recognition output, we have evaluated this approach in an automotive environment using an embedded speech recognizer in a mobile device. We have found that a reduction of approximately 44¡«49% in speech recognition string errors can be achieved after learning.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-94"
  },
  "obuchi06_interspeech": {
   "authors": [
    [
     "Yasunari",
     "Obuchi"
    ],
    [
     "Nobuo",
     "Hataoka"
    ]
   ],
   "title": "Development and evaluation of speech database in automotive environments for practical speech recognition systems",
   "original": "i06_1168",
   "page_count": 4,
   "order": 95,
   "p1": "paper 1168-Thu1CaP.4",
   "pn": "",
   "abstract": [
    "Aiming at practical speech recognition systems, we are developing speech databases representing the situation in which the application is used, and evaluating various techniques using the database. Such methodology is expected to contribute to bridge the expectations of the developers and the reactions of the users. We start with the applications in automotive environments, or car navigation systems more precisely. During the data collection, special attention was paid to maintain the spontaneousness of the speaker, to cover failed utterances, and to use the hardware setup suitable for microphone array techniques. After the database is prepared, various techniques are evaluated. In some cases, oracle information is used to find the upper limit of the improvement of a specific module. In other cases, typical improving algorithms are tested. Recognition experiments using two separate decoders indicate that endpoint detection, feature normalization, speaker adaptation, and parallel decoding are promising fields. We also present some modifications of parallel decoding to reduce the computational cost and to realize practical applications.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-95"
  },
  "yu06_interspeech": {
   "authors": [
    [
     "Dong",
     "Yu"
    ],
    [
     "Yun-Cheng",
     "Ju"
    ],
    [
     "Alex",
     "Acero"
    ]
   ],
   "title": "An effective and efficient utterance verification technology using word n-gram filler models",
   "original": "i06_1408",
   "page_count": 4,
   "order": 96,
   "p1": "paper 1408-Thu1CaP.5",
   "pn": "",
   "abstract": [
    "In this paper we propose a novel, effective, and efficient utterance verification (UV) technology for access control in the interactive voice response (IVR) systems. The key of our approach is to construct a context-free grammar by using the secret answer to a question and a word N-gram based filler model. The N-gram filler provides rich alternatives to the secret answer and can potentially improve the accuracy of the UV task. It can also absorb carrier words used by callers and thus can improve the robustness. We also propose using a predictor based on the best alternative to calculate the confidence. We show detailed experimental results on a tough UV test set that contains 930 positive and 930 negative cases and discuss types of questions that are suitable for the UV task. We demonstrate that our approach can achieve a 2.14% equal error rate (EER) on average and 0.8% false accept rate if the false reject rate is 2.6% and above. This is a 49% EER reduction compared with the approaches using acoustic fillers, and a 72% EER reduction compared with the posterior probability based confidence measurement.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-96"
  },
  "gorriz06_interspeech": {
   "authors": [
    [
     "J. M.",
     "Górriz"
    ],
    [
     "Javier",
     "Ramírez"
    ],
    [
     "C. G.",
     "Puntonet"
    ],
    [
     "José C.",
     "Segura"
    ]
   ],
   "title": "An efficient bispectrum phase entropy-based algorithm for VAD",
   "original": "i06_1440",
   "page_count": 4,
   "order": 97,
   "p1": "paper 1440-Thu1CaP.6",
   "pn": "",
   "abstract": [
    "In this paper we propose a novel Voice Activity Detection (VAD) algorithm, based on the integrated bispectrum function (IBI), for improving Automated Speech Recognition (ASR) systems that work in noisy environments. In particular we use the combination of two features, IBI magnitude and IBI phase to formulate a robust and smoothed decision rule for speech/pause discrimination. The analysis performed on the new combined feature highlighted: i) the advantages of each individual feature, while compensating the drawback of each other, and ii) the higher ability for endpoint detection given by a lower variance of the decision function in pause/speech frames. The experiments conducted on the Spanish SpeechDat-Car database showed that the proposed algorithm outperforms ITU G.729, ETSI AMR1 and AMR2 and ETSI AFE standards as well as other recently reported VAD methods in speech/non-speech detection performance.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-97"
  },
  "cerva06_interspeech": {
   "authors": [
    [
     "Petr",
     "Cerva"
    ],
    [
     "Jan",
     "Nouza"
    ],
    [
     "Jan",
     "Silovsky"
    ]
   ],
   "title": "Two-step unsupervised speaker adaptation based on speaker and gender recognition and HMM combination",
   "original": "i06_1441",
   "page_count": 4,
   "order": 98,
   "p1": "paper 1441-Thu1CaP.7",
   "pn": "",
   "abstract": [
    "In this paper, we present a new strategy for unsupervised speaker adaptation. In our approach, the adaptation is performed in two steps for each test utterance. In the first online step, we utilize speaker and gender identification, a set of speaker dependent (SD) hidden Markov models (HMMs) and our own fast linear model combination approach to create a proper model for the first speech recognition pass. After that the recognized phonetic transcription of the utterance is used for maximum likelihood (ML) estimation of more accurate weights for the final model combination step. Our experimental results on different types of broadcast programs show that the proposed method is capable to reduce the word error rate (WER) relatively by more than 17%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-98"
  },
  "nakamura06_interspeech": {
   "authors": [
    [
     "Satoshi",
     "Nakamura"
    ],
    [
     "Masakiyo",
     "Fujimoto"
    ],
    [
     "Kazuya",
     "Takeda"
    ]
   ],
   "title": "CENSREC2: corpus and evaluation environments for in car continuous digit speech recognition",
   "original": "i06_1726",
   "page_count": 4,
   "order": 99,
   "p1": "paper 1726-1CaP.8",
   "pn": "",
   "abstract": [
    "This paper introduces a common database and an evaluation framework for connected digit speech recognition in real driving car environments, CENSREC-2, as an outcome of IPSJ-SIG SLP Noisy Speech Recognition Evaluation Working Group. Speech data of CENSREC-2 was collected using two microphones, a close-talking microphone and a hands-free microphone, under three car speeds and four car conditions. CENSREC-2 provides four evaluation environments which are designed using speech data collected in these car conditions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-99"
  },
  "chu06_interspeech": {
   "authors": [
    [
     "Cheng-Tao",
     "Chu"
    ],
    [
     "Yun-Hsuan",
     "Sung"
    ],
    [
     "Yuan",
     "Zhao"
    ],
    [
     "Daniel",
     "Jurafsky"
    ]
   ],
   "title": "Detection of word fragments in Mandarin telephone conversation",
   "original": "i06_1730",
   "page_count": 4,
   "order": 100,
   "p1": "paper 1730-Thu1CaP.9",
   "pn": "",
   "abstract": [
    "We describe preliminary work on the detection of word fragments in Mandarin conversational telephone speech. We extracted prosodic, voice quality, and lexical features, and trained Decision Tree and SVM classifiers. Previous research shows that glottalization features are instrumental in English fragment detection. However, we show that Mandarin fragments are quite different than English; 90% of Mandarin fragments are followed immediately by a repetition of the fragmentary word. These repetition fragments are not glottalized, and they have a very specific distribution; the 12 most frequent words (\"you\", \"I\", \"that\", \"have\", \"then\", etc.) cover 50% of the tokens of these fragments. Thus rather than glottalization, we found the most useful feature for Mandarin fragment detection was the identity of the neighboring character (word or morpheme). In an oracle experiment using the true (reference) neighboring words as well as prosodic and voice quality features, we achieved 80% accuracy in Mandarin fragment detection.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-100"
  },
  "huo06_interspeech": {
   "authors": [
    [
     "Qiang",
     "Huo"
    ],
    [
     "Wei",
     "Li"
    ]
   ],
   "title": "A DTW-based dissimilarity measure for left-to-right hidden Markov models and its application to word confusability analysis",
   "original": "i06_1745",
   "page_count": 4,
   "order": 101,
   "p1": "paper 1745-Thu1CaP.10",
   "pn": "",
   "abstract": [
    "We propose a dynamic time-warping (DTW) based distortion measure for measuring the dissimilarity between pairs of left-to-right continuous density hidden Markov models with state observation densities being mixture of Gaussians. The local distortion score required in DTW is defined as an approximate Kullback-Leibler divergence (KLD) between two Gaussian mixture models (GMMs). Several approximate KLDs are studied and compared for pairs of GMMs with different properties, and one of them is identified for being used in our DTWbased HMM dissimilarity measure. In an experiment of identifying automatically the subsets of confusable Putonghua (Mandarin Chinese) syllables, it is observed that the result based on the proposed HMM dissimilarity measure is highly consistent with the one based on syllable recognition confusion matrix obtained on a testing data set.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-101"
  },
  "gomez06_interspeech": {
   "authors": [
    [
     "Angel M.",
     "Gómez"
    ],
    [
     "Juan J.",
     "Ramos-Muñoz"
    ],
    [
     "Antonio M.",
     "Peinado"
    ],
    [
     "Victoria",
     "Sánchez"
    ]
   ],
   "title": "Multi-flow block interleaving applied to distributed speech recognition over IP networks",
   "original": "i06_1365",
   "page_count": 4,
   "order": 102,
   "p1": "paper 1365-Thu1CaP.11",
   "pn": "",
   "abstract": [
    "Interleaving has shown to be a useful technique to provide robust distributed speech recognition over IP networks. This is due to its ability to disperse consecutive losses. However, this ability is related to the delay introduced by the interleaver. In this work, we propose a novel multi-flow block interleaver which exploits the presence of several streams and allows to reduce the involved delay. Experimental results have shown that this interleaver approximates the performance of end-to-end interleavers but with a fraction of their delay. As disadvantage, this interleaver must be placed in a common node where more than one flow are available.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-102"
  },
  "lin06_interspeech": {
   "authors": [
    [
     "Edward C.",
     "Lin"
    ],
    [
     "Kai",
     "Yu"
    ],
    [
     "Rob A.",
     "Rutenbar"
    ],
    [
     "Tsuhan",
     "Chen"
    ]
   ],
   "title": "Moving speech recognition from software to silicon: the in silico vox project",
   "original": "i06_1942",
   "page_count": 4,
   "order": 103,
   "p1": "paper 1942-Thu1CaP.12",
   "pn": "",
   "abstract": [
    "To achieve much faster decoding, or much lower power consumption, we need to liberate speech recognition from the artificial constraints of its current software-only form, and move the essential computations directly into silicon. There are vast efficiencies waiting to be unlocked in this application - we need the proper architecture to do so. We report results from a first-generation hardware architecture simulated at bit-level, and a complete, working FPGA-based prototype. Simulation results show that rather modest hardware designs, running 10-20X slower than conventional processors, can already decode at 0.6 xRT, running the standard 5K Wall Street Journal benchmark.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-103"
  },
  "ma06_interspeech": {
   "authors": [
    [
     "Chengyuan",
     "Ma"
    ],
    [
     "Yu",
     "Tsao"
    ],
    [
     "Chin-Hui",
     "Lee"
    ]
   ],
   "title": "A study on detection based automatic speech recognition",
   "original": "i06_2053",
   "page_count": 4,
   "order": 104,
   "p1": "paper 2053-Thu1CaP.13",
   "pn": "",
   "abstract": [
    "We propose a new approach to automatic speech recognition based on word detection and knowledge-based verification. Given an utterance, we first design a collection of word detectors, one for each lexical item in the vocabulary. Some pruning strategies are used to eliminate unlikely word candidates. Then these detected words are combined into word strings. The proposed approach is different from the conventional maximum a posteriori decoding method, and it is a critical component in building a bottom-up, detection-based speech recognition system in which knowledge in acoustics, speech and language can easily be incorporated into pruning unlikely word hypotheses and rescoring. The proposed approach was evaluated on a connected digit task using phone models trained from the TIMIT corpus. When compared with state-of-the-art connected digit recognition algorithms, we found the proposed detection based framework works well even no digit samples were used for training the detectors and recognizers. ?Other knowledge based constraints, such as manner and place of articulation detectors, can be incorporated into this detection-based approach to improve the robustness and performance of the overall system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-104"
  },
  "chitturi06_interspeech": {
   "authors": [
    [
     "Rahul",
     "Chitturi"
    ],
    [
     "Mark",
     "Hasegawa-Johnson"
    ]
   ],
   "title": "Novel time domain multi-class SVMs for landmark detection",
   "original": "i06_1904",
   "page_count": 4,
   "order": 105,
   "p1": "paper 1904-Thu1CaP.14",
   "pn": "",
   "abstract": [
    "The training of precise speech recognition models depends on accurate segmentation of the phonemes in a training corpus. Segmentation is typically performed using HMMs, but recent speech recognition work suggests that the transient acoustic features characteristic of manner-class phoneme boundaries (landmarks) may be more precisely localized using acoustic classifiers specifically designed for the task of landmark detection. This paper makes an empirical exploration of new features which suit Landmark Detection and the application of Multi-class SVMs that are capable of improving the time alignment of phoneme boundaries proposed by Binary SVMs and HMM-based speech recognizer. On a standard benchmark data set (A database of Telugu ¡ª Official Indian Language, spoken by 75 million people), we achieve a new state-of-the-art performance, reducing RMS phone boundary alignment error from 32ms to 22ms.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-105"
  },
  "ananthakrishnan06_interspeech": {
   "authors": [
    [
     "Sankaranarayanan",
     "Ananthakrishnan"
    ],
    [
     "Shrikanth",
     "Narayanan"
    ]
   ],
   "title": "Combining acoustic, lexical, and syntactic evidence for automatic unsupervised prosody labeling",
   "original": "i06_1335",
   "page_count": 4,
   "order": 106,
   "p1": "paper 1335-Mon2A3O.1",
   "pn": "",
   "abstract": [
    "Automatic labeling of prosodic events in speech has potentially significant implications for spoken language processing applications, and has received much attention over the years, especially after the introduction of annotation standards such as ToBI. Current labeling techniques are based on supervised learning, relying on the availability of a corpus that is annotated with the prosodic labels of interest in order to train the system. However, creating such resources is an expensive and time-consuming task. In this paper, we examine an unsupervised labeling algorithm for accent (prominence) and prosodic phrase boundary detection at the linguistic syllable level, and evaluate their performance on an standard, manually annotated corpus. We obtain labeling accuracies of 77.8% and 88.5% for the accent and boundary labeling tasks, respectively. These figures compare well against previously reported performance levels for supervised labelers.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-106"
  },
  "rosenberg06_interspeech": {
   "authors": [
    [
     "Andrew",
     "Rosenberg"
    ],
    [
     "Julia",
     "Hirschberg"
    ]
   ],
   "title": "On the correlation between energy and pitch accent in read English speech",
   "original": "i06_1294",
   "page_count": 4,
   "order": 107,
   "p1": "paper 1294-Mon2A3O.2",
   "pn": "",
   "abstract": [
    "In this paper, we describe a set of experiments that examine the correlation between energy and pitch accent. We tested the discriminative power of the energy component of frequency subbands with a variety of frequencies and bandwidths on read speech spoken by four native speakers of Standard American English, using an analysis by classification approach. We found that the frequency region most robust to speaker differences is between 2 and 20 bark. Across all speakers, using only energy features we were able to predict pitch accent in read speech with accuracy of 81.9%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-107"
  },
  "hirose06_interspeech": {
   "authors": [
    [
     "Keikichi",
     "Hirose"
    ],
    [
     "Yasufumi",
     "Asano"
    ],
    [
     "Nobuaki",
     "Minematsu"
    ]
   ],
   "title": "Corpus-based generation of fundamental frequency contours using generation process model and considering emotional focuses",
   "original": "i06_1902",
   "page_count": 4,
   "order": 108,
   "p1": "paper 1902-Mon2A3O.3",
   "pn": "",
   "abstract": [
    "We formerly conducted emotional speech synthesis using our corpusbased method of generating fundamental frequency (F0) contours from text. The method predicts command values of F0 contour generation process model instead of directly predicting F0 value of each time frame. A better control of F0 contours was realized by taking the emotional level of each bunsetsu into account: adding information on which bunsetsu(s) the emotion is especially placed to the command predictor inputs. In the case of anger, F0 contours closer to the target contours are obtained by adding emotional levels. Speech synthesis was conducted by generating F0 contours in two ways: using commands predicted by taking emotional levels into account and those not. The result of perceptual experiment indicated that emotion was conveyed well by adding emotional levels.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-108"
  },
  "dubeda06_interspeech": {
   "authors": [
    [
     "Tomás",
     "Dubeda"
    ]
   ],
   "title": "Prosodic boundaries in Czech: an experiment based on delexicalized speech",
   "original": "i06_1056",
   "page_count": 4,
   "order": 109,
   "p1": "paper 1056-Mon2A3O.4",
   "pn": "",
   "abstract": [
    "The present experiment attempts to determine the role of prosody in the identification of stress unit boundaries in Czech, using three types of delexicalized stimuli evaluated by native speakers. After describing the relative contribution of intonation and duration to boundary perception, an analysis of misplaced boundaries is provided. Identified patterns concern especially the relationship between tonal structure and boundary salience, the order of preference between intonation and duration, and the tendency towards perceptual filling of accent lapses.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-109"
  },
  "yi06_interspeech": {
   "authors": [
    [
     "Lifu",
     "Yi"
    ],
    [
     "Jian",
     "Li"
    ],
    [
     "Xiaoyan",
     "Lou"
    ],
    [
     "Jie",
     "Hao"
    ]
   ],
   "title": "Totally data-driven intonation prediction model using a novel F0 contour parametric representation",
   "original": "i06_1465",
   "page_count": 4,
   "order": 110,
   "p1": "paper 1465-Mon2A3O.5",
   "pn": "",
   "abstract": [
    "This paper proposes a novel parametric representation of mandarin intonation based on orthogonal polynomial approximation. The polynomial is a simplified representation of Parallel Encoding and Target Approximation (PENTA) intonation model that includes a target component and an approximation component. We also propose predicting the polynomial parameters from linguistic and phonetic attributes by generalized linear models (GLM). The optimal attributes are automatically selected by stepwise regression method. Thus both model structures and model coefficients are optimized in a totally data-driven manner. In addition, speaking rate is introduced as a new attribute for prediction. When the method is applied to intonation prediction of Mandarin speech, it achieves F0 RMSE of 30.21 Hz and correlation coefficients of 0.85 in open test. Informal perceptual experiments show that the predicted intonation is quite appropriate and natural.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-110"
  },
  "dilley06_interspeech": {
   "authors": [
    [
     "Laura",
     "Dilley"
    ],
    [
     "Mara",
     "Breen"
    ],
    [
     "Marti",
     "Bolivar"
    ],
    [
     "John",
     "Kraemer"
    ],
    [
     "Edward",
     "Gibson"
    ]
   ],
   "title": "A comparison of inter-transcriber reliability for two systems of prosodic annotation: rap (rhythm and pitch) and toBI (tones and break indices)",
   "original": "i06_1619",
   "page_count": 4,
   "order": 111,
   "p1": "paper 1619-Mon2A3O.6",
   "pn": "",
   "abstract": [
    "Agreement was investigated among five labelers for the use of two prosodic annotation systems: the ToBI (Tones and Break Indices) system [1,2] and the RaP (Rhythm and Pitch) system [3]. Each system permits the labeling of pitch accents and two levels of phrasal boundaries; RaP also permits labeling of speech rhythm and distinguishes multiple levels of prominence on syllables. After training with computerized materials and getting expert feedback, coders applied each system to a corpus of read and spontaneous speech (36 minutes for ToBI and 19 for RaP). Inter-coder reliability was computed according to two metrics: transcriber-syllable-pairs and the kappa statistic. High agreement was obtained for both systems for pitch accent presence, pitch accent type, boundary presence, boundary type, and, for RaP, presence and strength of metrical prominences. Agreement levels for ToBI were similar to those of previous studies [4,5], indicating that participants were proficient coders. Moreover, the high level of agreement demonstrated for the RaP system indicates that RaP is a viable alternative to ToBI for prosodic labeling of large speech corpora.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-111"
  },
  "alphonso06_interspeech": {
   "authors": [
    [
     "Issac",
     "Alphonso"
    ],
    [
     "Shuangyu",
     "Chang"
    ]
   ],
   "title": "Saliency parsing for automated directory assistance",
   "original": "i06_1421",
   "page_count": 4,
   "order": 112,
   "p1": "paper 1421-Mon2WeO.1",
   "pn": "",
   "abstract": [
    "In a statistical language model based automated directory assistance system, extracting the salient information from the recognition output can significantly increase the accuracy of the backend listing database search. In this paper, we describe a Hidden Markov model (HMM) based saliency parser that was developed to accurately and efficiently identify salient words from the recognition output by modeling both the syntactic structure as well as the lexical distribution. The parser can be trained using a relatively small data set with coarse syntactic class labels, without the need for detailed syntactic knowledge or a treebank-like corpus. Experimental results on a research corpus of directory assistance utterances betoken the parsers importance within the automated system. The results demonstrate that the proposed saliency parser can significantly improve the overall automation rate without increasing the error rate.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-112"
  },
  "iwata06_interspeech": {
   "authors": [
    [
     "Kohei",
     "Iwata"
    ],
    [
     "Yoshiaki",
     "Itoh"
    ],
    [
     "Kazunori",
     "Kojima"
    ],
    [
     "Masaaki",
     "Ishigame"
    ],
    [
     "Kazuyo",
     "Tanaka"
    ],
    [
     "Shi-wook",
     "Lee"
    ]
   ],
   "title": "Open-vocabulary spoken document retrieval based on new subword models and subword phonetic similarity",
   "original": "i06_1342",
   "page_count": 4,
   "order": 113,
   "p1": "paper 1342-Mon2WeO.2",
   "pn": "",
   "abstract": [
    "A new type of video retrieval system is proposed that identifies a target video section by searching for a word passage submitted as a quoted speech or text query. The proposed system has two unique characteristics. The first characteristic is that it is based on subword models such as phonemes, syllables, and morphemes so the system is able to deal with any type of query, including new words and personal names. The second characteristic is that the system relies on acoustic similarity between subword models. Furthermore, new subword models were constructed for the retrieval system to improve performance. The new models were based on two concepts: contextdependent models and more sophisticated in the time axis than phone models. Through experimentation, the effectiveness and scope of the proposed spoken document retrieval system were confirmed, and suitable subword models for the proposed method discussed.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-113"
  },
  "li06_interspeech": {
   "authors": [
    [
     "Xiang",
     "Li"
    ],
    [
     "Ea-Ee",
     "Jan"
    ],
    [
     "Cheng",
     "Wu"
    ],
    [
     "David",
     "Lubensky"
    ]
   ],
   "title": "Improved topic classification over maximum entropy model using k-norm based new objectives",
   "original": "i06_2066",
   "page_count": 4,
   "order": 114,
   "p1": "paper 2066-Mon2WeO.3",
   "pn": "",
   "abstract": [
    "Maximum Entropy (MaxEnt) model has been proven to be a very effective approach in the topic classification task, where a specific topic from a pre-defined topic set will be assigned to each sentence. Although it is originally developed based on the motivation of maximizing the conditional probability entropy under certain constraints, MaxEnt model is indeed an exponential distribution model that maximizes the log-likelihood of the training data. This log-likelihood criterion bears similarity with the classification accuracy criterion, which is the ultimate performance measure of a topic classifier. But these two criterion still differ from each other, and their discrepancy consequently reduces the benefit of optimization in improving classification accuracy. In this paper we propose to use different objective functions, which are closer to the classification accuracy criterion, to replace the log-likelihood objective used in the MaxEnt model estimation process. Specifically, we propose a Summation-Log K-norm objective and a Summation K-norm objective. Our experiments conducted on two large volume topic classification dataset prove the effectiveness of our new objectives in improving topic classification performance on top of the state-of-art MaxEnt model.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-114"
  },
  "pan06_interspeech": {
   "authors": [
    [
     "Yi-cheng",
     "Pan"
    ],
    [
     "Jia-yu",
     "Chen"
    ],
    [
     "Yen-shin",
     "Lee"
    ],
    [
     "Yi-sheng",
     "Fu"
    ],
    [
     "Lin-shan",
     "Lee"
    ]
   ],
   "title": "Efficient interactive retrieval of spoken documents with key terms ranked by reinforcement learning",
   "original": "i06_1577",
   "page_count": 4,
   "order": 115,
   "p1": "paper 1577-Mon2WeO.4",
   "pn": "",
   "abstract": [
    "Unlike written documents, spoken documents are difficult to display on the screen; it is also difficult for users to browse these documents during retrieval. It has been proposed recently to use interactive multi-modal dialogues to help the user navigate through a spoken document archive to retrieve the desired documents. This interaction is based on a topic hierarchy constructed by the key terms extracted from the retrieved spoken documents. In this paper, the efficiency of the user interaction in such a system is further improved by a key term ranking algorithm using Reinforcement Learning with simulated users. Significant improvements in retrieval efficiency, which are relatively robust to the speech recognition errors, are observed in preliminary evaluations.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-115"
  },
  "sudoh06_interspeech": {
   "authors": [
    [
     "Katsuhito",
     "Sudoh"
    ],
    [
     "Hajime",
     "Tsukada"
    ],
    [
     "Hideki",
     "Isozaki"
    ]
   ],
   "title": "Discriminative named entity recognition of speech data using speech recognition confidence",
   "original": "i06_1153",
   "page_count": 4,
   "order": 116,
   "p1": "paper 1153-Mon2WeO.5",
   "pn": "",
   "abstract": [
    "This paper presents a method for the named entity recognition (NER) of speech data that uses automatic speech recognition (ASR) confidence as a feature that indicates whether each word is correctly recognized. An NER model is trained using ASR results with named entity (NE) labels to include an ASR confidence feature as well as corresponding transcriptions with NE labels. Experiments using support vector machines (SVMs) and speech data from Japanese newspaper articles show that the proposed method achieves higher F-measure in NER than a simple application of text-based NER to ASR results.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-116"
  },
  "turunen06_interspeech": {
   "authors": [
    [
     "Ville T.",
     "Turunen"
    ],
    [
     "Mikko",
     "Kurimo"
    ]
   ],
   "title": "Using latent semantic indexing for morph-based spoken document retrieval",
   "original": "i06_1220",
   "page_count": 4,
   "order": 117,
   "p1": "paper 1220-Mon2WeO.6",
   "pn": "",
   "abstract": [
    "Previously, phone-based and word-based approaches have been used for spoken document retrieval. The former suffers from high error rates and the latter from limited vocabulary of the recognizer. Our method relies on unlimited vocabulary continuous speech recognizer that uses morpheme-like units discovered in an unsupervised manner. The morpheme-like units, or \"morphs\" for short, have been successfully used also as index terms. One problem using morphs as index terms is that the segmentation does not always separate the same stem for different inflected forms of the same word. This resembles the problem of synonyms. In this paper, we apply latent semantic indexing to morph based retrieval. The idea is to project morphs that correspond to the same word, as well as other semantically related terms, to the same dimension. The results show clear improvements in Finnish spoken document retrieval performance.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-117"
  },
  "schluter06_interspeech": {
   "authors": [
    [
     "Ralf",
     "Schlüter"
    ],
    [
     "András",
     "Zolnay"
    ],
    [
     "Hermann",
     "Ney"
    ]
   ],
   "title": "Feature combination using linear discriminant analysis and its pitfalls",
   "original": "i06_1077",
   "page_count": 4,
   "order": 118,
   "p1": "paper 1077-Mon2BuP.1",
   "pn": "",
   "abstract": [
    "In this paper, Linear Discriminant Analysis (LDA) is investigated with respect to the combination of different acoustic features for automatic speech recognition. It is shown that the combination of acoustic features using LDA does not consistently lead to improvements in word error rate. A detailed analysis of the recognition results on the Verbmobil (VM II) and on the English portion of the European Parliament Plenary Sessions (EPPS) corpus is given. This includes an independent analysis of the effect of the dimension of the input to LDA, the effect of strongly correlated input features, as well as a detailed numerical analysis of the generalized eigenvalue problem underlying LDA. Relative improvements in word error rate of up to 5% were observed for LDA-based combination of multiple acoustic features.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-118"
  },
  "valente06_interspeech": {
   "authors": [
    [
     "Fabio",
     "Valente"
    ],
    [
     "Hynek",
     "Hermansky"
    ]
   ],
   "title": "Discriminant linear processing of time-frequency plane",
   "original": "i06_1300",
   "page_count": 4,
   "order": 119,
   "p1": "paper 1300-Mon2BuP.2",
   "pn": "",
   "abstract": [
    "Extending previous works done on considerably smaller data sets, the paper studies linear discriminant analysis of about 30 hours of phoneme-labeled speech data in the time-frequency domain. Analysis is carried both independently in time and frequency and jointly. Data driven spectral basis show similar frequency sensitivity as human hearing. LDA-derived temporal FIR filters are consistent with temporal lateral inhibition. Considerable improvement is obtained using first temporal discriminant.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-119"
  },
  "uraga06_interspeech": {
   "authors": [
    [
     "Esmeralda",
     "Uraga"
    ],
    [
     "Thomas",
     "Hain"
    ]
   ],
   "title": "Automatic speech recognition experiments with articulatory data",
   "original": "i06_1725",
   "page_count": 4,
   "order": 120,
   "p1": "paper 1725-Mon2BuP.3",
   "pn": "",
   "abstract": [
    "In this paper we investigate the use of articulatory data for speech recognition. Recordings of the articulatory movements originate from the MOCHA corpus, a database which contains speech, EGG, EMA and EPG recordings. It was found that in a Hidden Markov Model (HMM) based recognition framework careful processing of these signals can yield significantly better performance than that obtained by decoding of the acoustic signals. We present detailed results on the processing of the signals and the associated performance of monophone and triphone systems. Experimental evidence shows that acoustic-signal-to-word mappings and articulatory-signal-to-word mappings are equally complex. However, for the latter, evidence of short-comings of standard HMM based modelling is visible and should be addressed in future systems.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-120"
  },
  "stouten06_interspeech": {
   "authors": [
    [
     "Frederik",
     "Stouten"
    ],
    [
     "Jean-Pierre",
     "Martens"
    ]
   ],
   "title": "Speech recognition with phonological features: some issues to attend",
   "original": "i06_1081",
   "page_count": 4,
   "order": 121,
   "p1": "paper 1081-Mon2BuP.4",
   "pn": "",
   "abstract": [
    "It is often argued that acoustic-phonetic or articulatory features could be beneficial to automatic speech recognition because they provide a convenient interface between the acoustic and the linguistic level. Former research has shown that a combination of acoustic and articulatory information can lead to improved ASR. However there exists no purely articulatory driven ASR system that outperforms state-of-the-art systems driven by acoustic features. In this paper we propose a novel method for improving ASR on the basis of articulatory features. It is designed to take account of (1) the correlations between articulatory features and (2) the fact that not all articulatory features are relevant for the description of a certain phonetic unit. We also investigate to what extend an acoustic and an articulatory feature driven system make different errors.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-121"
  },
  "wolfel06_interspeech": {
   "authors": [
    [
     "Matthias",
     "Wölfel"
    ],
    [
     "Christian",
     "Fügen"
    ],
    [
     "Shajith",
     "Ikbal"
    ],
    [
     "John W.",
     "McDonough"
    ]
   ],
   "title": "Multi-source far-distance microphone selection and combination for automatic transcription of lectures",
   "original": "i06_1253",
   "page_count": 4,
   "order": 122,
   "p1": "paper 1253-Mon2BuP.5",
   "pn": "",
   "abstract": [
    "In this work, we present our progress in multi-source far field automatic speech-to-text transcription for lecture speech. In particular, we show how the best of several far field channels can be selected based on a signal-to-noise ratio criterion, and how the signals from multiple channels can be combined at either the waveform level using blind channel combination or at the hypothesis level using confusion network techniques to improve the accuracy of a far field lecture transcription system. Using the techniques described here, we ran a series of experiments on the test set used by the US National Institute of Standards and Technologies for the RT-05S evaluation. For the multiple distant microphones (MDM) task of RT-05S, our system achieved a word error rate of 38.5% which represents an improvement of over 13% absolute compared to the best reported results in the RT-05S evaluation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-122"
  },
  "breithaupt06_interspeech": {
   "authors": [
    [
     "Colin",
     "Breithaupt"
    ],
    [
     "Rainer",
     "Martin"
    ]
   ],
   "title": "Statistical analysis and performance of DFT domain noise reduction filters for robust speech recognition",
   "original": "i06_1537",
   "page_count": 4,
   "order": 123,
   "p1": "paper 1537-Mon2BuP.6",
   "pn": "",
   "abstract": [
    "Noise reduction frontends have been developed independently for speech communication and speech recognition purposes with the result that one and the same algorithm does not perform well in both application domains. In this paper we show that noise reduction filters based on the discrete Fourier transform (DFT) which are used in speech communication can also perform well in robust automatic speech recognition (ASR) experiments if some form of feature smoothing is applied.\n",
    "We analyse the statistics of the Mel frequency cepstral coefficients (MFCCs) that are used as speech features and describe the effects on recognition results if the mean and variance of these features change. It is shown that recognizers are more sensitive to an increase in variance of enhanced features than to errors in their mean values. We present a method that compensates for the increased variance of DFT-based noise reduction frontends by means of using prior knowledge and smoothing. We achieve high segmental SNR improvements as well as recognition results close to those of the Advanced Frontend (AFE) of the European Telecommunications Standards Institute (ETSI) for all noise types.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-123"
  },
  "garcia06_interspeech": {
   "authors": [
    [
     "L.",
     "García"
    ],
    [
     "José C.",
     "Segura"
    ],
    [
     "Carmen",
     "Benítez"
    ],
    [
     "Javier",
     "Ramírez"
    ],
    [
     "Ángel de la",
     "Torre"
    ]
   ],
   "title": "Normalization of the inter-frame information using smoothing filtering",
   "original": "i06_1687",
   "page_count": 4,
   "order": 124,
   "p1": "paper 1687-Mon2BuP.7",
   "pn": "",
   "abstract": [
    "A filter that introduces inter-frame information into the voice features set is proposed in this paper. The filter adds the autocorrelations of the cepstral coefficients to the set of characteristics used for training and recognition. Those autocorrelations should not depend on the environment conditions. Because they should only depend on the information to recognize, a normalization of that inter-frame information is convenient. The filter defined implements this normalization by transforming the autocorrelations into a normalized domain defined with clean adaptation data. This temporal processing of the features is added to the Histogram Equalization of the cepstral coefficients (HEQ) used to normalize the MFCCs. An analysis is done about the most effective domain (original MFCCS or equalized MFCCs) on which the temporal processing should be executed. Performance results for the proposed algorithm are presented for AURORA2 and AURORA4 databases.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-124"
  },
  "ghulam06_interspeech": {
   "authors": [
    [
     "Muhammad",
     "Ghulam"
    ],
    [
     "Junsei",
     "Horikawa"
    ],
    [
     "Tsuneo",
     "Nitta"
    ]
   ],
   "title": "Comparative study on contributions of pitch-synchronization and peak-amplitude towards robustness issue of ASR",
   "original": "i06_1781",
   "page_count": 4,
   "order": 125,
   "p1": "paper 1781-Mon2BuP.8",
   "pn": "",
   "abstract": [
    "We proposed previously a novel pitch-synchronous peak-amplitude (PS-PA) based feature extraction method, which achieved significant recognition accuracy for robust ASR [1]. It is well-known that an auditory neuron has pitch detection mechanism that can be useful for speech detection, and also peak-amplitudes in temporal pattern are robust to noise. In this paper, we conduct several experiments to find out relative contributions of pitch-synchronization (PS) and peakamplitudes (PA) on recognition accuracy of robust ASR. Experiments include methods with fixed and pitch-synchronous frame lengths, and that with traditional peak-amplitudes and pitch-synchronous peak-amplitudes. The experimental results show that both PS and PA have strong contributions towards robust ASR and the effect of PS is higher than that of PA.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-125"
  },
  "ariki06_interspeech": {
   "authors": [
    [
     "Yasuo",
     "Ariki"
    ],
    [
     "Shunsuke",
     "Kato"
    ],
    [
     "Tetsuya",
     "Takiguchi"
    ]
   ],
   "title": "Phoneme recognition based on fisher weight map to higher-order local auto-correlation",
   "original": "i06_1883",
   "page_count": 4,
   "order": 126,
   "p1": "paper 1883-Mon2BuP.9",
   "pn": "",
   "abstract": [
    "In this paper, we propose a new feature extraction method based on higher-order local auto-correlation (HLAC) and Fisher weight map (FWM). Widely used MFCC features lack temporal dynamics. To solve this problem, 35 types of local auto-correlation features are computed within two-dimensional local regions. These local features are accumulated over more global regions by weighting high scores on the discriminative areas where the typical features among all phonemes are well expressed. This score map is called Fisher weight map. We verified the effectiveness of the HLAC and FWM through vowel recognition and total phoneme recognition.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-126"
  },
  "boril06_interspeech": {
   "authors": [
    [
     "Hynek",
     "Boril"
    ],
    [
     "Petr",
     "Fousek"
    ],
    [
     "Petr",
     "Pollák"
    ]
   ],
   "title": "Data-driven design of front-end filter bank for Lombard speech recognition",
   "original": "i06_1803",
   "page_count": 4,
   "order": 127,
   "p1": "paper 1803-Mon2BuP.10",
   "pn": "",
   "abstract": [
    "Adverse environments not only corrupt speech signal by additive and convolutional noises, which can be successfully addressed by a number of suppression algorithms, but also affect the way how speech is produced. Speech production variations introduced by a speaker in reaction to a noisy background (Lombard effect) may result in a severe degradation of automatic speech recognition. This paper contributes to the solution of Lombard speech recognition issue by providing a robust filter bank for use in front-ends. It is shown that cepstral features derived from the proposed filter bank significantly outperform conventional cepstral features.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-127"
  },
  "ljolje06_interspeech": {
   "authors": [
    [
     "Andrej",
     "Ljolje"
    ]
   ],
   "title": "Optimization of class weights for LDA feature transformations",
   "original": "i06_2031",
   "page_count": 4,
   "order": 128,
   "p1": "paper 2031-Mon2BuP.11",
   "pn": "",
   "abstract": [
    "One popular feature type in speech recognition is based on linear transformations of sequences of cepstral feature vectors. In general the transformation is generated in two steps: first a transformation like linear discriminant analysis (LDA) or heteroscedastic linear discriminant analysis (HLDA) is used to maximize separation between classes and reduce the dimensionality, followed by a decorrelating transformation. Here we investigate the weighting of classes when using the LDA transformation. In particular we are concerned with the special status of silence, for which the data can be arbitrarily long, and which can be represented by more than one silence/noise model. The special case of our acoustic models for commercial applications, which consist of several sub-models for each type of application, like general English, digits, names, alphabet, etc., creates a conflict when using a transformation like LDA to improve the separability of states which correspond to the same phoneme, but used within a different type of task. We also evaluate replacing sample counts with error/accuracy counts and cross-task LDA transformation estimation. The results show that it is important to take these conditions into account and demonstrate accuracy/speed improvements when appropriate care is taken in computing the LDA transformations.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-128"
  },
  "pylkkonen06_interspeech": {
   "authors": [
    [
     "Janne",
     "Pylkkönen"
    ]
   ],
   "title": "LDA based feature estimation methods for LVCSR",
   "original": "i06_1213",
   "page_count": 4,
   "order": 129,
   "p1": "paper 1213-Mon2BuP.12",
   "pn": "",
   "abstract": [
    "Features that model temporal aspects of phonemes are important in speech recognition. One method is to use linear discriminant analysis (LDA) to find discriminative features from a spectro-temporal input formed by concatenating consecutive frames of short-time spectrum features. Others use e.g. neural networks to process longer span spectral segments to improve recognition accuracy. Still the most widely used method for including temporal cues is to augment the short-time spectral features with simple time derivatives. In this paper a new feature estimation method based on pairwise linear discriminants is presented. We compare it and some of its variants to traditional MFCC features and to LDA estimated features in a large vocabulary continuous speech recognition (LVCSR) task. The features obtained with the new estimation method show significant improvements in recognition accuracy over MFCC and LDA features.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-129"
  },
  "farahani06_interspeech": {
   "authors": [
    [
     "G.",
     "Farahani"
    ],
    [
     "S.M.",
     "Ahadi"
    ],
    [
     "M. Mehdi",
     "Homayounpour"
    ]
   ],
   "title": "Robust feature extraction based on spectral peaks of group delay and autocorrelation function and phase domain analysis",
   "original": "i06_1563",
   "page_count": 4,
   "order": 130,
   "p1": "paper 1563-Mon2BuP.13",
   "pn": "",
   "abstract": [
    "This paper presents a new robust feature set for noisy speech recognition in phase domain along with spectral peaks obtained from group delay and autocorrelation functions.\n",
    "The group delay domain is appropriate for formant tracking and autocorrelation domain is well-known for its pole preserving and noise separation properties. In this paper, we report on appending spectral peaks obtained in either group delay or autocorrelation domains to the feature vectors extracted originally in phase domain to create a new feature set.\n",
    "We tested our features on the Aurora 2 noisy isolated-word task and found that it led to improvements over other group delay-based and autocorrelation-based methods that use magnitude instead of phase for feature extraction.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-130"
  },
  "panchapagesan06_interspeech": {
   "authors": [
    [
     "Sankaran",
     "Panchapagesan"
    ]
   ],
   "title": "Frequency warping by linear transformation of standard MFCC",
   "original": "i06_1924",
   "page_count": 4,
   "order": 131,
   "p1": "paper 1924-Mon2BuP.14",
   "pn": "",
   "abstract": [
    "A novel linear transform (LT) is proposed for frequency warping (FW) with standard filterbank based MFCC features. Here, we use the idea of spectral interpolation of [9] to perform a continuous warping in the log filterbank output domain, and incorporate both interpolation and warping into a single warped IDCT matrix. The new transformation matrix is thus mathematically simpler than in [9], and no modification of standard MFCC feature extraction is required like the previous approach. In VTLN experiments with maximum likelihood score (MLS) estimation of the FW parameter, the new LT outperformed regular VTLN implemented by warping the Mel filterbank. In speaker adaptation experiments using the new LT to transform HMM means, the results were significantly better than MLLR for limited adaptation data and comparable to those in [8], while using the computationally simpler MLS FW estimation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-131"
  },
  "reyesherrera06_interspeech": {
   "authors": [
    [
     "Ana Lilia",
     "Reyes-Herrera"
    ],
    [
     "Luis",
     "Villaseñor-Pineda"
    ],
    [
     "Manuel",
     "Montes-y-Gómez"
    ]
   ],
   "title": "Automatic language identification using wavelets",
   "original": "i06_1998",
   "page_count": 4,
   "order": 132,
   "p1": "paper 1998-Mon2CaP.1",
   "pn": "",
   "abstract": [
    "Spoken language identification consists in recognizing a language based on a sample of speech from an unknown speaker. The traditional approach for this task mainly considers the phonothactic information of languages. However, for marginalized languages - languages with few speakers or oral languages without a fixed writing standard -, this information is practically not at hand and consequently the usual approach is not applicable. In this paper, we present a method that only considers the acoustic features of the speech signal and does not use any kind of linguistic information. This method applies a wavelet transform to extract the acoustic features of the speech signal. The experimental results on a pairwise discrimination task among nine languages demonstrated that this approach considerably outperforms other previous methods based on the sole use of acoustic features.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-132"
  },
  "bauer06_interspeech": {
   "authors": [
    [
     "Josef G.",
     "Bauer"
    ],
    [
     "Ekaterina",
     "Timoshenko"
    ]
   ],
   "title": "Minimum classification error training of hidden Markov models for acoustic language identification",
   "original": "i06_1981",
   "page_count": 4,
   "order": 133,
   "p1": "paper 1981-Mon2CaP.2",
   "pn": "",
   "abstract": [
    "The goal of acoustic Language Identification (LID) is to identify the language of spoken utterances. The described system is based on parallel Hidden Markov Model (HMM) phoneme recognizers. The standard approach for parameter learning of Hidden Markov Model parameters is Maximum Likelihood (ML) estimation which is not directly related to the classification error rate. Based on the Minimum Classification Error (MCE) parameter estimation scheme we introduce Minimum Language Identification Error (MLIDE) training that results in HMM model parameters (mean vectors) that give minimum classification error on the training data. Using a large telephone speech corpus with 7 languages achieve a language classification error rate of 4.7% which is a 40% reduction of error rate compared with a baseline system using ML trained HMMs. Even if the system trained on fixed network telephone speech is applied to mobile network speech data MLIDE can greatly improve the system performance.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-133"
  },
  "timoshenko06_interspeech": {
   "authors": [
    [
     "Ekaterina",
     "Timoshenko"
    ],
    [
     "Josef G.",
     "Bauer"
    ]
   ],
   "title": "Unsupervised adaptation for acoustic language identification",
   "original": "i06_1494",
   "page_count": 4,
   "order": 134,
   "p1": "paper 1494-Mon2CaP.3",
   "pn": "",
   "abstract": [
    "Our system for automatic language identification (LID) of spoken utterances is performed with language dependent parallel phoneme recognition (PPR) using Hidden Markov Model (HMM) phoneme recognizers and optional phoneme language models (LMs). Such a LID system for continuous speech requires many hours of orthographically transcribed data for training of language dependent HMMs and LMs as well as phonetic lexica for every considered language (supervised training). To avoid the time consuming process of obtaining the orthographically transcribed training material we propose an algorithm for automatic unsupervised adaptation that requires only raw audio data as training material covering the requested language and acoustic environment. The LID system was trained and evaluated using fixed and mobile network databases (DBs) from the SpeechDat II corpus. The baseline system - based on supervised training using fixed network databases and covering 4 languages - achieved a LID error rate of 6.7% for fixed data and 19.5% for mobile data. Using unsupervised adaptation of the HMMs trained on fixed network data the error rate for mobile DBs database mismatch is reduced to 10.6%. Exploring a situation when orthographically transcribed training data is not available at all multilingual HMMs were unsupervised adapted to fixed and mobile DBs and perform at 10.8% and 12.4% error rate respectively.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-134"
  },
  "basavaraja06_interspeech": {
   "authors": [
    [
     "S. V.",
     "Basavaraja"
    ],
    [
     "T. V.",
     "Sreenivas"
    ]
   ],
   "title": "Low complexity LID using pruned pattern tables of LZW",
   "original": "i06_1398",
   "page_count": 4,
   "order": 135,
   "p1": "paper 1398-Mon2CaP.4",
   "pn": "",
   "abstract": [
    "We present two discriminative language modelling techniques for Lempel-Ziv-Welch (LZW) based LID system. The previous approach to LID using LZW algorithm was to directly use the LZW pattern tables for language modelling. But, since the patterns in a language pattern table are shared by other language pattern tables, confusability prevailed in the LID task. For overcoming this, we present two pruning techniques (i) Language Specific (LS-LZW) - in which patterns common to more than one pattern table are removed. (ii) Length-Frequency product based (LF-LZW) - in which patterns having their length-frequency product below a threshold are removed. These approaches reduce the classification score (Compression Ratio [LZW-CR] or the weighted discriminant score [LZW-WDS]) for non native languages and increases the LID performance considerably. Also the memory and computational requirements of these techniques are much less compared to basic LZW techniques.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-135"
  },
  "yang06_interspeech": {
   "authors": [
    [
     "Xi",
     "Yang"
    ],
    [
     "Lu-Feng",
     "Zhai"
    ],
    [
     "Manhung",
     "Siu"
    ],
    [
     "Herbert",
     "Gish"
    ]
   ],
   "title": "Improved language identification using support vector machines for language modeling",
   "original": "i06_1450",
   "page_count": 4,
   "order": 136,
   "p1": "paper 1450-Mon2CaP.5",
   "pn": "",
   "abstract": [
    "Automatic language identification (LID) decisions are made based on scores of language models (LM). In our previous paper [1], we have shown that replacing n-gram LMs with SVMs significantly improved performance of both the PPRLM and GMM-tokenization-based LID systems when tested on the OGI-TS corpus. However, the relatively small corpus size may limit the general applicability of the findings. In this paper, we extend the SVM-based approach on the larger CallFriend corpus evaluated using the NIST 1996 and 2003 evaluation sets. With more data, we found that SVM is still better than n-gram models. In addition, back-end processing is useful with SVM scores in Call-Friend which differs from our observation in the OGI-TS corpus. By combining the SVM-based GMM and phonotactic systems, our LID system attains an ID error of 12.1% on NIST 2003 evaluation set which is more than 4% (25% relatively) better than the baseline n-gram system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-136"
  },
  "navratil06_interspeech": {
   "authors": [
    [
     "Jiri",
     "Navratil"
    ]
   ],
   "title": "Recent advances in phonotactic language recognition using binary-decision trees",
   "original": "i06_1338",
   "page_count": 4,
   "order": 137,
   "p1": "paper 1338-Mon2CaP.6",
   "pn": "",
   "abstract": [
    "Binary decision trees are an effective model structure in language recognition. This paper presents several related algorithmic steps to address data sparseness issues and computational complexity. In particular, a tree adaptation step, a recursive bottom-up smoothing step, and two variants of the Flip-Flop approximation algorithm are introduced to language detection and studied in the context of the NIST Language Recognition Evaluation task.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-137"
  },
  "lin06b_interspeech": {
   "authors": [
    [
     "Chi-Yueh",
     "Lin"
    ],
    [
     "Hsiao-Chuan",
     "Wang"
    ]
   ],
   "title": "Fusion of phonotactic and prosodic knowledge for language identification",
   "original": "i06_1166",
   "page_count": 4,
   "order": 138,
   "p1": "paper 1166-Mon2CaP.7",
   "pn": "",
   "abstract": [
    "Over the last few decades, language identification systems based on different kinds of linguistic knowledge had been studied by many researchers. Most of systems utilize one kind of linguistic knowledge only, i.e. phonotactic, phonetic repertoire, or prosody. It is possible to get the improvement by combining several linguistic knowledge. However, the combination of two systems based on different kinds of linguistic knowledge is not a trivial task. This paper presents a method where local identification results made by two individual systems, i.e. prosody-based and phonotactic-based systems, are fused in a Bayesian framework. Under this framework, local decisions, the associated false-alarm and miss probabilities are fused via Bayesian formulation to make the final decision. Experiments conducted on OGI-TS corpus demonstrate the effectiveness of this decision-level fusion strategy.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-138"
  },
  "li06b_interspeech": {
   "authors": [
    [
     "Haizhou",
     "Li"
    ],
    [
     "Bin",
     "Ma"
    ],
    [
     "Rong",
     "Tong"
    ]
   ],
   "title": "Vector-based spoken language recognition using output coding",
   "original": "i06_1155",
   "page_count": 4,
   "order": 139,
   "p1": "paper 1155-Mon2CaP.8",
   "pn": "",
   "abstract": [
    "The vector-based spoken language recognition approach converts a spoken utterance into a high dimensional vector, also known as a bagof- sounds vector, that consists of n-gram statistics of acoustic units. Dimensionality reduction would better prepare the bag-of-sounds vectors for classifier design. We propose projecting the bag-of-sounds vectors onto a low dimensional SVM output coding space, where each dimension represents a decision hyperplane between a pair of spoken languages. We also compare the performances of the output coding approach and the traditional low ranking approximation approach using latent semantic indexing (LSI) on the NIST 1996, 2003 and 2005 Language Recognition Evaluation (LRE) databases. The experiments show that the output coding approach consistently outperforms LSI with competitive results.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-139"
  },
  "guijarrubia06_interspeech": {
   "authors": [
    [
     "Victor G.",
     "Guijarrubia"
    ],
    [
     "M. Ines",
     "Torres"
    ]
   ],
   "title": "Basque-Spanish language identification using phone-based methods",
   "original": "i06_1892",
   "page_count": 4,
   "order": 140,
   "p1": "paper 1892-Mon2CaP.9",
   "pn": "",
   "abstract": [
    "This paper presents initial experiments in language identification for Spanish and Basque, which are both official languages in the Basque Country in the North of Spain. We focus on three methods based on Hidden Markov Models (HMMs): parallel phone decoding, with no phonotactic knowledge, phone decoder scored by a phonotactic model and single phone decoder scored by a phonotactic model, with phonotactic knowledge. Results for the three techniques are presented, along with others obtained using a neural network classifier. Significant accuracy is achieved when better phonotactic knowledge is used. The use of a neural network classifier results in a slightly improvement and, in overall, similar results are achieved for both languages, with accuracies around 98%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-140"
  },
  "ikeno06_interspeech": {
   "authors": [
    [
     "Ayako",
     "Ikeno"
    ],
    [
     "John H. L.",
     "Hansen"
    ]
   ],
   "title": "The role of prosody in the perception of US native English accents",
   "original": "i06_1437",
   "page_count": 4,
   "order": 141,
   "p1": "paper 1437-Mon2CaP.10",
   "pn": "",
   "abstract": [
    "A wide range of aspects are contained within the speech signal which provides information concerning a particular speaker's characteristics. Accent is a linguistic trait of speaker identity. It indicates the speaker's language and social background. The goal of this study is to provide perceptual assessment of accent variation in US native English. The main issue considered is how different components of prosody affect accent perception. This perceptual study employed an ASHA certified acoustic sound booth using 73 listeners (53 male, 20 female). The results from these perceptual experiments indicate the importance of prosody in combination with availability of utterance content via speech signal or transcripts. The trends also indicate that listeners' decisions are influenced by conceptual representation of prototypical accent characteristics, such as \"people from New York talk fast.\" These observations suggest that listeners use both bottom-up processing, based on the acoustic input, and top-town processing, based on their conceptual representation of prototypical accent characteristics. Those processes are multi-dimensional in that listeners use utterance content (e.g., meaning or comprehensibility) as well as accent characteristics in the acoustic input even though our experiment focuses on pronunciation features and does not include word selections that are dialect dependent. These findings contribute to a deeper understanding of the cognitive aspects of accent variation, and its applications for speech technology, such as accent classification for speaker identification or speech recognition.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-141"
  },
  "vierudimulescu06_interspeech": {
   "authors": [
    [
     "Bianca",
     "Vieru-Dimulescu"
    ],
    [
     "Philippe",
     "Boula de Mareüil"
    ]
   ],
   "title": "Perceptual identification and phonetic analysis of 6 foreign accents in French",
   "original": "i06_1251",
   "page_count": 4,
   "order": 142,
   "p1": "paper 1251-Mon2CaP.11",
   "pn": "",
   "abstract": [
    "A perceptual experiment was designed to determine to what extent naive French listeners are able to identify foreign accents in French: Arabic, English, German, Italian, Portuguese and Spanish. They succeed in recognizing the speaker's mother tongue in more than 50% of cases (while rating their degree of accentedness as average). They perform best with Arabic speakers and worst with Portuguese speakers. The Spanish/Italian and English/German accents are the most mistaken ones. Phonetic analyses were conducted; clustering and scaling techniques were applied to the results, and were related to the listeners' reactions that were recorded during the test. They support the idea that differences in the vowel realization (especially concerning the phoneme /y/) seem to outweigh rhythmic cues. Gaussian Mixture Selection and Data Selection for\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-142"
  },
  "huang06_interspeech": {
   "authors": [
    [
     "Rongqing",
     "Huang"
    ],
    [
     "John H. L.",
     "Hansen"
    ]
   ],
   "title": "Unsupervised Spanish dialect classification",
   "original": "i06_1242",
   "page_count": 4,
   "order": 143,
   "p1": "paper 1242-Mon2CaP.12",
   "pn": "",
   "abstract": [
    "Automatic dialect classification has gained interests in the field of speech research because it is important to characterize speaker traits and to estimate knowledge that could improve integrated speech technology (e.g., speech recognition, speaker recognition). This study addresses novel advances in unsupervised spontaneous Latin American Spanish dialect classification. The problem considers the case where no transcripts are available for train and test data, and speakers are talking spontaneously. A technique which aims to find the dialect dependence in the untranscribed audio by selecting the most discriminative Gaussian mixtures and selecting the most discriminative frames of speech is proposed. The Gaussian Mixture Model (GMM) based classifier is retrained after the dialect dependence information is identified. Both the MS-GMM (GMM trained with Mixture Selection) and FS-GMM (GMM trained with Frame Selection) classifiers improve dialect classification performance significantly. Using 122 speakers across three dialects of Spanish with 3.3 hours of speech, the relative error reduction is 30.4% and 26.1% respectively.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-143"
  },
  "gieselmann06_interspeech": {
   "authors": [
    [
     "Petra",
     "Gieselmann"
    ],
    [
     "Alex",
     "Waibel"
    ]
   ],
   "title": "Dynamic extension of a grammar-based dialogue system: constructing an all-recipes knowing robot",
   "original": "i06_1091",
   "page_count": 4,
   "order": 144,
   "p1": "paper 1091-Mon2FoP.1",
   "pn": "",
   "abstract": [
    "In the upcoming field of humanoid and human-friendly robots, the ability of the robot for simple, unconstrained and natural communication with its users is of central importance. The basis for appropriate actions of the robot is the correct understanding of the user utterances. To be able to cover all the entities a user might talk about, we enhanced our dialogue manager with an ability for dynamic vocabulary generation out of information found across the internet. As a test case, we chose an internet recipe database integrated in the dialogue manager of our household robot so that it can understand several thousand recipes and ingredients now.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-144"
  },
  "gruenstein06_interspeech": {
   "authors": [
    [
     "Alexander",
     "Gruenstein"
    ],
    [
     "Stephanie",
     "Seneff"
    ],
    [
     "Chao",
     "Wang"
    ]
   ],
   "title": "Scalable and portable web-based multimodal dialogue interaction with geographical databases",
   "original": "i06_1095",
   "page_count": 4,
   "order": 145,
   "p1": "paper 1095-Mon2FoP.2",
   "pn": "",
   "abstract": [
    "We describe work towards developing a scalable and portable framework for enabling map-based multimodal dialogue interaction over the web. Working in the context of a restaurant-guide system, we show how large information databases harvested from the web can be accommodated in our speech recognizer, parser, and web-based GUI. We compare two dynamic language modeling techniques, which calculate context-dependent weights for the large sets of proper nouns associated with geographical entities such as restaurants and streets. We show that the more fine-grained approach results in a 7.8% reduction in concept error rate.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-145"
  },
  "ackermann06_interspeech": {
   "authors": [
    [
     "Chantal",
     "Ackermann"
    ],
    [
     "Marion",
     "Libossek"
    ]
   ],
   "title": "System- versus user-initiative dialog strategy for driver information systems",
   "original": "i06_1172",
   "page_count": 4,
   "order": 146,
   "p1": "paper 1172-Mon2FoP.3",
   "pn": "",
   "abstract": [
    "In this paper we examine the advantages of a system-initiative approach for the voice user interface of a driver information system (DIS). The problems of user-initiative systems are the steep learning curve and the high demand on memory to recall the correct voice commands. This is especially true in a car environment where the main, and most important task is to drive the car. In a Wizard of Oz experiment, we compared this approach to one that uses a more system-initiative form of interaction. Furthermore, a context sensitive help prompt was included in the new system, instead of just a context sensitive list of commands. The results show that, for novice users, the error rate, the number and time of task completion, the mental workload, and the subjective joy of use (as measured by a semantic differential) are all better for the proposed system. Nevertheless, the possibility to use shortcuts remains. Thus, an expert user could still skip the supermenus and jump into the given submenu by saying the right voice command.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-146"
  },
  "krsmanovic06_interspeech": {
   "authors": [
    [
     "Filip",
     "Krsmanovic"
    ],
    [
     "Curtis",
     "Spencer"
    ],
    [
     "Daniel",
     "Jurafsky"
    ],
    [
     "Andrew Y.",
     "Ng"
    ]
   ],
   "title": "Have we met? MDP based speaker ID for robot dialogue",
   "original": "i06_1193",
   "page_count": 4,
   "order": 147,
   "p1": "paper 1193-Mon2FoP.4",
   "pn": "",
   "abstract": [
    "We present a novel approach to speaker identification in robot dialogue that allows a robot to recognize people during natural conversation and address them by name. Our STanford AI Robot (STAIR) dialogue system attempts to mirror the human speaker identification process. We model the robot dialogue problem as a Markov Decision Process (MDP) and apply a reinforcement learning algorithm to try to learn the optimal dialogue actions. The MDP model works in conjunction with a traditional statistical cluster based speaker identification subsystem. Our approach also addresses open-set speaker identification, dynamically adding new speaker profiles as well as continuously updating known profiles.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-147"
  },
  "son06_interspeech": {
   "authors": [
    [
     "Rob J. J. H. van",
     "Son"
    ],
    [
     "Wieneke",
     "Wesseling"
    ],
    [
     "Louis C. W.",
     "Pols"
    ]
   ],
   "title": "Prominent words as anchors for TRP projection",
   "original": "i06_1235",
   "page_count": 4,
   "order": 148,
   "p1": "paper 1235-Mon2FoP.5",
   "pn": "",
   "abstract": [
    "The effect of the position of the last accented word on the projection of TRPs was investigated with two RT experiments. Subjects were asked to respond with minimal responses to prerecorded dialogs and impoverished versions of these dialogs, containing either only intonation and pause information, hummed stimuli, or no periodic component at all, whispered stimuli. The distribution of these elicited response delays was comparable to that of natural turn switches. It is shown that the presence of non-prominent words before a TRP reduces the delays of elicited and natural responses alike, even in impoverished speech. This suggests that the presence of an prominent, informative, word starts the projection of a possible upcoming TRP. The availability of non-prominent, predictable, speech then allows listeners to improve their predictions of the exact timing of the TRP.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-148"
  },
  "cuayahuitl06_interspeech": {
   "authors": [
    [
     "Heriberto",
     "Cuayáhuitl"
    ],
    [
     "Steve",
     "Renals"
    ],
    [
     "Oliver",
     "Lemon"
    ],
    [
     "Hiroshi",
     "Shimodaira"
    ]
   ],
   "title": "Learning multi-goal dialogue strategies using reinforcement learning with reduced state-action spaces",
   "original": "i06_1282",
   "page_count": 4,
   "order": 149,
   "p1": "paper 1282-Mon2FoP.6",
   "pn": "",
   "abstract": [
    "Learning dialogue strategies using the reinforcement learning framework is problematic due to its expensive computational cost. In this paper we propose an algorithm that reduces a state-action space to one which includes only valid state-actions. We performed experiments on full and reduced spaces using three systems (with 5, 9 and 20 slots) in the travel domain using a simulated environment. The task was to learn multi-goal dialogue strategies optimizing single and multiple confirmations. Average results using strategies learnt on reduced spaces reveal the following benefits against full spaces: 1) less computer memory (94% reduction), 2) faster learning (93% faster convergence) and better performance (8.4% less time steps and 7.7% higher reward).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-149"
  },
  "mayer06_interspeech": {
   "authors": [
    [
     "Jörg",
     "Mayer"
    ],
    [
     "Ekaterina",
     "Jasinskaja"
    ],
    [
     "Ulrike",
     "Kölsch"
    ]
   ],
   "title": "Pitch range and pause duration as markers of discourse hierarchy: perception experiments",
   "original": "i06_1290",
   "page_count": 4,
   "order": 150,
   "p1": "paper 1290-Mon2FoP.7",
   "pn": "",
   "abstract": [
    "Discourse structure is reflected by a number of global prosodic parameters, like for example pause duration and pitch range. Discourse structure is also known to affect the accessibility/salience of antecedents of anaphoric expressions. Assuming these generalizations are correct, one can ask whether listeners use the information encoded in pauses and pitch range to resolve anaphoric references in ambiguous contexts. To examine this, we conducted a series of perception experiments with ambiguous discourses, where the pitch range of the sentences and the pause duration between sentences was manipulated. The results of our experiments corroborate our main research hypothesis that global prosodic parameters influence the resolution of anaphoric pronouns. The direction of the observed effect is clearly in accordance with the predictions of the existing theories of discourse anaphora and the current state of research on discourse prosody.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-150"
  },
  "roque06_interspeech": {
   "authors": [
    [
     "Antonio",
     "Roque"
    ],
    [
     "Anton",
     "Leuski"
    ],
    [
     "Vivek",
     "Rangarajan"
    ],
    [
     "Susan",
     "Robinson"
    ],
    [
     "Ashish",
     "Vaswani"
    ],
    [
     "Shrikanth",
     "Narayanan"
    ],
    [
     "David",
     "Traum"
    ]
   ],
   "title": "Radiobot-CFF: a spoken dialogue system for military training",
   "original": "i06_1828",
   "page_count": 4,
   "order": 151,
   "p1": "paper 1828-Mon2FoP.8",
   "pn": "",
   "abstract": [
    "We describe a spoken dialogue system which can engage in Call For Fire (CFF) radio dialogues to help train soldiers in proper procedures for requesting artillery fire missions. We describe the domain, an information-state dialogue manager with a novel system of interactive information components, and provide evaluation results.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-151"
  },
  "yamada06b_interspeech": {
   "authors": [
    [
     "Shinya",
     "Yamada"
    ],
    [
     "Toshihiko",
     "Itoh"
    ],
    [
     "Kenji",
     "Araki"
    ]
   ],
   "title": "Is voice quality enough? - study on how the situation and user²s awareness influence the utterance features",
   "original": "i06_1955",
   "page_count": 4,
   "order": 152,
   "p1": "paper 1955-Mon2FoP.9",
   "pn": "",
   "abstract": [
    "This paper presents the characteristic differences of linguistic and acoustic features observed in different spoken dialogue situations and with different dialogue partners: human-human vs. human-machine interactions. And it also presents influences of awareness of users on those characteristics. We compare the linguistic and acoustic features of the users speech to a spoken dialogue system and to a human operator in several goal setting and destination database searching tasks for a car navigation system. Because it is not clear enough whether different dialogue situations and different dialogue partners cause any differences of linguistic or acoustic features on ones utterances in a speech interface system, we have performed experiments in several dialogue situations[4]. However, in these experiments the conditions such as voice quality and awareness of users such as impressions on the partner and prejudices against a system have not been considered. And so we collected a set of spoken dialogues in new dialogue situations. To investigate influence of voice quality, we also prepare recorded voice for response of dialogue partners and compared the influences of voice (natural voice, synthetic voice and recorded voice). We also made users answer questionnaire before and after the experiments and investigated characteristic differences caused by awareness of users. Additionally, in order to confirm the usefulness of the results of all experiments, we actually applied acoustic features of users utterances and identified the utterances made to a system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-152"
  },
  "juhar06_interspeech": {
   "authors": [
    [
     "Jozef",
     "Juhár"
    ],
    [
     "Stanislav",
     "Ondas"
    ],
    [
     "Anton",
     "Cizmár"
    ],
    [
     "Milan",
     "Rusko"
    ],
    [
     "Gregor",
     "Rozinaj"
    ],
    [
     "Roman",
     "Jarina"
    ]
   ],
   "title": "Development of slovak GALAXY/voiceXML based spoken language dialogue system to retrieve information from the internet",
   "original": "i06_2056",
   "page_count": 4,
   "order": 153,
   "p1": "paper 2056-Mon2FoP.10",
   "pn": "",
   "abstract": [
    "In this paper we describe the research and development of the first Slovak spoken language dialogue system. The dialogue system is based on the DARPA Communicator architecture. The proposed system consists of the Galaxy hub and telephony, automatic speech recognition, text-to-speech, backend, transport and VoiceXML dialogue management modules. The SLDS enables multi-user interaction in the Slovak language. The functionality of the SLDS is demonstrated and tested via two pilot applications, \"Weather forecast for Slovakia\" and \"Timetable of Slovak Railways\". The required information is retrieved from Internet resources in multi-user mode through PSTN, ISDN, GSM and/or VoIP network.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-153"
  },
  "degerstedt06_interspeech": {
   "authors": [
    [
     "Lars",
     "Degerstedt"
    ],
    [
     "Arne",
     "Jönsson"
    ]
   ],
   "title": "LINTest: a development tool for testing dialogue systems",
   "original": "i06_1555",
   "page_count": 4,
   "order": 154,
   "p1": "paper 1555-Mon2FoP.11",
   "pn": "",
   "abstract": [
    "In this paper we present a development tool for testing dialogue systems. Testing software through the specification is important for software development in general and should be as automated as possible. For dialogue systems, the corpus can be seen as one part of the specification and the dialogue system should be tested on available corpora on each new build. The testing tool is inspired from work on agile software development methods, test driven development and unit testing, and can be used in two modes and during various phases of development.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-154"
  },
  "ito06_interspeech": {
   "authors": [
    [
     "Akinori",
     "Ito"
    ],
    [
     "Keisuke",
     "Shimada"
    ],
    [
     "Motoyuki",
     "Suzuki"
    ],
    [
     "Shozo",
     "Makino"
    ]
   ],
   "title": "A user simulator based on voiceXML for evaluation of spoken dialog systems",
   "original": "i06_1358",
   "page_count": 4,
   "order": 155,
   "p1": "paper 1358-Tue2A3O.1",
   "pn": "",
   "abstract": [
    "This paper describes a user simulator based on analysis of VoiceXML description. A user simulator is a method to evaluate a spoken dialog system without the use of human evaluators. The new feature of our simulator is that it uses a VoiceXML description that describes the dialog systems behavior. By using the VoiceXML description, the proposed simulator can be used for any dialog system that works with VoiceXML. We constructed a prototype of the user simulator and carried out an evaluation experiment. The experimental result showed that the dialog between the simulator and the dialog system had similar properties to that between human subjects and the dialog system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-155"
  },
  "jokinen06_interspeech": {
   "authors": [
    [
     "Kristiina",
     "Jokinen"
    ],
    [
     "Topi",
     "Hurtig"
    ]
   ],
   "title": "User expectations and real experience on a multimodal interactive system",
   "original": "i06_1815",
   "page_count": 4,
   "order": 156,
   "p1": "paper 1815-Tue2A3O.2",
   "pn": "",
   "abstract": [
    "We present evaluation results of a multimodal route navigation system that allows interaction using speech and tactile/visual modes. Various functional aspects of the system were studied, related especially to the IO-modalities and their use as means of communication. We compared the users expectations before the evaluation with their actual experience of the system, and found significant differences among various user groups.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-156"
  },
  "burkhardt06_interspeech": {
   "authors": [
    [
     "F.",
     "Burkhardt"
    ],
    [
     "J.",
     "Ajmera"
    ],
    [
     "Roman",
     "Englert"
    ],
    [
     "J.",
     "Stegmann"
    ],
    [
     "W.",
     "Burleson"
    ]
   ],
   "title": "Detecting anger in automated voice portal dialogs",
   "original": "i06_1977",
   "page_count": 4,
   "order": 157,
   "p1": "paper 1977-Tue2A3O.3",
   "pn": "",
   "abstract": [
    "Anger detection is a topic that is gaining more and more attention with voice portal carriers, as it can be useful for quality measurement and emotion-aware dialog strategies. In the context of a prototype voice portal we describe methods to search for training data, report on the performance of the prosodic classifier under real world conditions and explore the use of dialog information for anger prediction. The results show that, although significantly worse than under laboratory conditions, anger detection still works well above chance level and can be used to enhance real world voice-portal usability.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-157"
  },
  "turunen06b_interspeech": {
   "authors": [
    [
     "Markku",
     "Turunen"
    ],
    [
     "Jaakko",
     "Hakulinen"
    ],
    [
     "Anssi",
     "Kainulainen"
    ]
   ],
   "title": "Evaluation of a spoken dialogue system with usability tests and long-term pilot studies: similarities and differences",
   "original": "i06_1978",
   "page_count": 4,
   "order": 158,
   "p1": "paper 1978-Tue2A3O.4",
   "pn": "",
   "abstract": [
    "We present findings from the long-term study of a speech-based bus timetable system. After the deployment of the prototype system we have collected data from real usage for 30 months. In addition, we have conducted usability tests to get subjective ratings of the pilot system. The comparison of these evaluations shows that the results obtained with usability tests differ significantly from those gained from the real usage, and the data of the initial use differs significantly from the data collected after that. For example, the differences in help requests, interruptions, speech recognition rejections, silence timeouts, and repeat requests are highly significant, and in some cases, such as explicit quit requests, enormous (65% versus 3%).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-158"
  },
  "weng06_interspeech": {
   "authors": [
    [
     "Fuliang",
     "Weng"
    ],
    [
     "Sebastian",
     "Varges"
    ],
    [
     "Badri",
     "Raghunathan"
    ],
    [
     "Florin",
     "Ratiu"
    ],
    [
     "Heather",
     "Pon-Barry"
    ],
    [
     "Brian",
     "Lathrop"
    ],
    [
     "Qi",
     "Zhang"
    ],
    [
     "Harry",
     "Bratt"
    ],
    [
     "Tobias",
     "Scheideck"
    ],
    [
     "Kui",
     "Xu"
    ],
    [
     "Matthew",
     "Purver"
    ],
    [
     "Rohit",
     "Mishra"
    ],
    [
     "Annie",
     "Lien"
    ],
    [
     "M.",
     "Raya"
    ],
    [
     "S.",
     "Peters"
    ],
    [
     "Y.",
     "Meng"
    ],
    [
     "J.",
     "Russell"
    ],
    [
     "Lawrence",
     "Cavedon"
    ],
    [
     "Elizabeth",
     "Shriberg"
    ],
    [
     "H.",
     "Schmidt"
    ],
    [
     "R.",
     "Prieto"
    ]
   ],
   "title": "CHAT: a conversational helper for automotive tasks",
   "original": "i06_2020",
   "page_count": 4,
   "order": 159,
   "p1": "paper 2020-Tue2A3O.5",
   "pn": "",
   "abstract": [
    "Spoken dialogue interfaces, mostly command-and-control, become more visible in applications where attention needs to be shared with other tasks, such as driving a car. The deployment of the simple dialog systems, instead of more sophisticated ones, is partly because the computing platforms used for such tasks have been less powerful and partly because certain issues from these cognitively challenging tasks have not been well addressed even in the most advanced dialog systems. This paper reports the progress of our research effort in developing a robust, wide-coverage, and cognitive load-sensitive spoken dialog interface called CHAT: Conversational Helper for Automotive Tasks. Our research in the past few years has led to promising results, including high task completion rate, dialog efficiency, and improved user experience.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-159"
  },
  "georgila06_interspeech": {
   "authors": [
    [
     "Kallirroi",
     "Georgila"
    ],
    [
     "James",
     "Henderson"
    ],
    [
     "Oliver",
     "Lemon"
    ]
   ],
   "title": "User simulation for spoken dialogue systems: learning and evaluation",
   "original": "i06_2035",
   "page_count": 4,
   "order": 160,
   "p1": "paper 2035-Tue2A3O.6",
   "pn": "",
   "abstract": [
    "We propose the \"advanced\" n-grams as a new technique for simulating user behaviour in spoken dialogue systems, and we compare it with two methods used in our prior work, i.e. linear feature combination and \"normal\" n-grams. All methods operate on the intention level and can incorporate speech recognition and understanding errors. In the linear feature combination model user actions (lists of { speech act, task } pairs) are selected, based on features of the current dialogue state which encodes the whole history of the dialogue. The user simulation based on \"normal\" n-grams treats a dialogue as a sequence of lists of { speech act, task } pairs. Here the length of the history considered is restricted by the order of the n-gram. The \"advanced\" n-grams are a variation of the normal n-grams, where user actions are conditioned not only on speech acts and tasks but also on the current status of the tasks, i.e. whether the information needed by the application (in our case flight booking) has been provided and confirmed by the user. This captures elements of goal-directed user behaviour. All models were trained and evaluated on the COMMUNICATOR corpus, to which we added annotations for user actions and dialogue context. We then evaluate how closely the synthetic responses resemble the real user responses by comparing the user response generated by each user simulation model in a given dialogue context (taken from the annotated corpus) with the actual user response. We propose the expected accuracy, expected precision, and expected recall evaluation metrics as opposed to standard precision and recall used in prior work. We also discuss why they are more appropriate metrics for evaluating user simulation models compared to their standard counterparts. The advanced n-grams produce higher scores than the normal n-grams for small values of n, which proves their strength when little amount of data is available to train larger n-grams. The linear model produces the best expected accuracy but with respect to expected precision and expected recall it is outperformed by the large n-grams even though it is trained using more information. As a task-based evaluation, we also run each of the user simulation models against a system policy trained on the same corpus. Here the linear feature combination model outperforms the other methods and the advanced n-grams outperform the normal n-grams for all values of n, which again shows their potential. We also calculate the perplexity of the different user models.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-160"
  },
  "chao06_interspeech": {
   "authors": [
    [
     "Yi-Hsiang",
     "Chao"
    ],
    [
     "Wei-Ho",
     "Tsai"
    ],
    [
     "Hsin-Min",
     "Wang"
    ],
    [
     "Ruei-Chuan",
     "Chang"
    ]
   ],
   "title": "Improving the characterization of the alternative hypothesis via kernel discriminant analysis for likelihood ratio-based speaker verification",
   "original": "i06_1431",
   "page_count": 4,
   "order": 161,
   "p1": "paper 1431-Mon3A1O.1",
   "pn": "",
   "abstract": [
    "The performance of a likelihood ratio-based speaker verification system is highly dependent on modeling of the target speakers voice (the null hypothesis) and characterization of non-target speakers voices (the alternative hypothesis). To better characterize the ill-defined alternative hypothesis, this study proposes a new likelihood ratio measure based on a composite-structure Gaussian mixture model, the so-called GMM2. Motivated by the combined use of a variety of background models to represent the alternative hypothesis, GMM2 is designed with an inner set of mixture weights connected to the significance of each individual Gaussian density, and an outer set of mixture weights connected to the significance of each individual background model. Through the use of kernel discriminant analysis namely, Kernel Fisher Discriminant (KFD) or Support Vector Machine (SVM), GMM2 is trained in such a manner that the utterances of the null hypothesis can be optimally separated from those of the alternative hypothesis.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-161"
  },
  "lei06_interspeech": {
   "authors": [
    [
     "Zhenchun",
     "Lei"
    ],
    [
     "Yingchun",
     "Yang"
    ],
    [
     "Zhaohui",
     "Wu"
    ]
   ],
   "title": "A discriminative method for speaker verification using the difference information",
   "original": "i06_1952",
   "page_count": 4,
   "order": 162,
   "p1": "paper 1952-Mon3A1O.2",
   "pn": "",
   "abstract": [
    "In this paper, a discriminative method is proposed for speaker verification. An utterance can be mapped into a matrix by computing the difference to a codebook, and then expand the mapped matrix to a vector as the input of support vector machines for speaker verification. The Gaussian mixture model-based method is also constructed by utilizing its nature. The mapped vector indicates the utterances fitness to the codebook. Compared with the derivative operation in the famous fisher kernel the difference operation is used in our method. Experiments were run on the YOHO database in the text-independent case show that the new method is superior to the conventional GMM for speaker verification.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-162"
  },
  "scheffer06_interspeech": {
   "authors": [
    [
     "Nicolas",
     "Scheffer"
    ],
    [
     "Jean-François",
     "Bonastre"
    ]
   ],
   "title": "A multiclass framework for speaker verification within an acoustic event sequence system",
   "original": "i06_1574",
   "page_count": 4,
   "order": 163,
   "p1": "paper 1574-Mon3A1O.3",
   "pn": "",
   "abstract": [
    "Building acoustic events and their sequence analysis (AES system) is a method that proved its efficiency in [1]. Indeed, the methodology combines the power of the world model GMM, used in state-of-the-art speaker detection systems, for extracting speaker independent events with an analysis of these event sequences via tools usually used in so-called High Level Speaker Detection systems. The efficiency of this system has been validated at the last NIST evaluation campaign. This paper aims at proposing a new framework by applying an AES system on multiple classes, C-AES. The originality of this work is to consider that intraclass sequence analysis can bring more information than a global analysis on the whole speaker utterance. This paper also proposes a method to take into account the apriori knowledge of the classes within the scoring process. The results support the fact that intraclass information is discriminant for speaker verification, as a combination with a state-of-the-art GMM brings a 12% relative gain at the DCF.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-163"
  },
  "ma06b_interspeech": {
   "authors": [
    [
     "Bin",
     "Ma"
    ],
    [
     "Donglai",
     "Zhu"
    ],
    [
     "Rong",
     "Tong"
    ],
    [
     "Haizhou",
     "Li"
    ]
   ],
   "title": "Speaker cluster based GMM tokenization for speaker recognition",
   "original": "i06_1429",
   "page_count": 4,
   "order": 164,
   "p1": "paper 1429-Mon3A1O.4",
   "pn": "",
   "abstract": [
    "We present a speaker recognition system with multiple GMM tokenizers as the front-end, and vector space modeling as the back-end classifier. GMM tokenizer captures the acoustic and phonetic characteristics of a speaker from the speech without the need of phonetic transcription. To enhance the speaker characteristics coverage and provide more discriminative information, a speaker clustering algorithm is proposed to build multiple GMM tokenizers that are arranged in parallel. For an input utterance, each of the tokenizers outputs a token sequence, which is then represented by a vector of n-gram probabilities. Multiple vectors are concatenated to form a composite vector. Finally the Support Vector Machine (SVM) is used as the back-end classifier of the composite vectors. We use the 2002 NIST Speaker Recognition Evaluation (SRE) corpus for training GMM tokenizers and background modeling, and evaluate on the 2001 NIST SRE corpus.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-164"
  },
  "garreton06_interspeech": {
   "authors": [
    [
     "Claudio",
     "Garreton"
    ],
    [
     "Nestor Becerra",
     "Yoma"
    ],
    [
     "Carlos",
     "Molina"
    ],
    [
     "Fernando",
     "Huenupan"
    ]
   ],
   "title": "Intra-speaker variability compensation in speaker verification with limited enrolling data",
   "original": "i06_1425",
   "page_count": 4,
   "order": 165,
   "p1": "paper 1425-Mon3A1O.5",
   "pn": "",
   "abstract": [
    "In this paper a compensation method is proposed to address the problem of limited enrolling data in speaker verification. Instead of adapting the client HMM, the technique presented here modifies the verification speech signals by maximizing the a posteriori p.d.f. in order to optimize the reduction in intra-speaker variability. The proposed approach can lead to reductions of 38.9% and 61.8% in EER and in the integral below the false-acceptation / false-rejection ROC curve, respectively.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-165"
  },
  "chetty06_interspeech": {
   "authors": [
    [
     "Girija",
     "Chetty"
    ],
    [
     "Michael",
     "Wagner"
    ]
   ],
   "title": "Speaking faces for face-voice speaker identity verification",
   "original": "i06_2025",
   "page_count": 4,
   "order": 166,
   "p1": "paper 2025-Mon3A1O.6",
   "pn": "",
   "abstract": [
    "In this paper, we describe an approach for an animated speaking face synthesis and its application in modeling impostor/replay attack scenarios for face-voice based speaker verification systems. The speaking face reported here learns the spatiotemporal relationship between speech acoustics and MPEG4 compliant facial animation points. The influence of articulatory, perceptual, and prosodic acoustic features along with auditory context on prediction accuracy was examined. The results are indicative of vulnerability of audiovisual identity verification systems to impostor/replay attacks using synthetic faces. The level of vulnerability depends on several factors, such as the type of audiovisual features, the fusion techniques used for the audio and video features and their relative robustness. Also, the success of the synthetic impostor depends on the type of co-articulation models and acoustic features used for the audiovisual mapping of speaking face synthesis.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-166"
  },
  "prahallad06_interspeech": {
   "authors": [
    [
     "Kishore",
     "Prahallad"
    ],
    [
     "Varanasi",
     "Sudhakar"
    ],
    [
     "Veluru",
     "Ranganatham"
    ],
    [
     "Krishna M.",
     "Bharat"
    ],
    [
     "S. Roy",
     "Debashish"
    ]
   ],
   "title": "Significance of formants from difference spectrum for speaker identification",
   "original": "i06_1583",
   "page_count": 4,
   "order": 167,
   "p1": "paper 1583-Tue1CaP.1",
   "pn": "",
   "abstract": [
    "In this paper, we describe a prototype speaker identification system using auto-associative neural network (AANN) and formant features. Our experiments demonstrate that formants extracted from difference spectrum perform significantly better than formants extracted from normal spectrum for the task of speaker identification. We also demonstrate that formants from difference spectrum provide comparable speaker identification performance with that of features such as weighted linear predictive Cepstral coefficients and Mel-Frequency Cepstral coefficients. Finally, we combine the results of formant based system and linear predictive Cepstral coefficients based system to achieve 100% identification performance.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-167"
  },
  "zamalloa06_interspeech": {
   "authors": [
    [
     "Maider",
     "Zamalloa"
    ],
    [
     "Germán",
     "Bordel"
    ],
    [
     "Luis Javier",
     "Rodríguez"
    ],
    [
     "Mikel",
     "Penagarikano"
    ],
    [
     "Juan Pedro",
     "Uribe"
    ]
   ],
   "title": "Using genetic algorithms to weight acoustic features for speaker recognition",
   "original": "i06_1240",
   "page_count": 4,
   "order": 168,
   "p1": "paper 1240-Tue1CaP.2",
   "pn": "",
   "abstract": [
    "The Mel-Frequency Cepstral Coefficients (MFCC) are widely accepted as a suitable representation for speaker recognition applications. MFCC are usually augmented with dynamic features, leading to high dimensional representations. The issue arises of whether some of those features are redundant or dependent on other features. Probably, not all of them are equally relevant for speaker recognition. In this work, we explore the potential benefit of weighting acoustic features to improve speaker recognition accuracy. Genetic algorithms (GAs) are used to find the optimal set of weights for a 38-dimensional feature set. To evaluate each set of weights, recognition error is measured over a validation dataset. Naive speaker models are used, based on empirical distributions of vector quantizer labels. Weighting acoustic features yields 24.58% and 14.68% relative error reductions in two series of speaker recognition tests. These results provide evidence that further improvements in speaker recognition performance can be attained by weighting acoustic features. They also validate the use of GAs to search for an optimal set of feature weights.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-168"
  },
  "padilla06_interspeech": {
   "authors": [
    [
     "Michael T.",
     "Padilla"
    ],
    [
     "Thomas F.",
     "Quatieri"
    ],
    [
     "Douglas A.",
     "Reynolds"
    ]
   ],
   "title": "Missing feature theory with soft spectral subtraction for speaker verification",
   "original": "i06_1918",
   "page_count": 4,
   "order": 169,
   "p1": "paper 1918-Tue1CaP.3",
   "pn": "",
   "abstract": [
    "This paper considers the problem of training/testing mismatch in the context of speaker verification and, in particular, explores the application of missing feature theory in the case of additive white Gaussian noise corruption in testing. Missing feature theory allows for corrupted features to be removed from scoring, the initial step of which is the detection of these features. One method of detection, employing spectral subtraction, is studied in a controlled manner and it is shown that with missing feature compensation the resulting verification performance is improved as long as a minimum number of features remain. Finally, a blending of \"soft\" spectral subtraction for noise mitigation and missing feature compensation is presented. The resulting performance improves on the constituent techniques alone, reducing the equal error rate by about 15% over an SNR range of 5-25 dB.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-169"
  },
  "mary06_interspeech": {
   "authors": [
    [
     "Leena",
     "Mary"
    ],
    [
     "B.",
     "Yegnanarayana"
    ]
   ],
   "title": "Prosodic features for speaker verification",
   "original": "i06_1999",
   "page_count": 4,
   "order": 170,
   "p1": "paper 1999-Tue1CaP.4",
   "pn": "",
   "abstract": [
    "In this paper we study the effectiveness of prosodic features for speaker verification. We hypothesize that prosody is linked to linguistic units such as syllables and prosodic features can be better represented with reference to the syllabic sequence. For extracting prosodic features, speech is segmented into syllable-like regions using the knowledge of vowel onset points (VOP). We use a technique based on excitation source information to detect VOPs automatically. The location of VOPs serve as reference for extracting prosodic features directly from speech signal. Various parameters are used to represent the pitch and energy dynamics of the region between two consecutive VOPs. The effectiveness of the derived prosodic features for speaker verification is demonstrated on NIST SRE 2003 extended data. The complementary nature of prosodic features and spectral features help to improve the accuracy of the combined speaker verification system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-170"
  },
  "liu06c_interspeech": {
   "authors": [
    [
     "Ming",
     "Liu"
    ],
    [
     "Thomas S.",
     "Huang"
    ]
   ],
   "title": "Unsupervised learning of HMM topology for text-dependent speaker verification",
   "original": "i06_1302",
   "page_count": 4,
   "order": 171,
   "p1": "paper 1302-Tue1CaP.5",
   "pn": "",
   "abstract": [
    "Usually, text-dependent speaker verification can achieve better performance than text-independent system because of the constraint that the enrollment and testing utterance share the same phonetic content. However, the enrollment data for text-dependent system usually is very limited. Expectation Maximization(EM) training of HMM will suffer from noisy estimation because of limited enrollment. Adaptation is a popular solution in this scenario. The target model is formed by adapting the generic model based on limited speaker specific training data. Although the adaptation scheme can tolerate much less training data than direct EM method, the traditional method does not account the topology of HMM might be different for different speaker. The topology information further distinguish the target speaker from impostors. In this paper, we propose a unsupervised learning method to learn the topology of HMM for each speaker. The experimental results indicate that with learning the topology, the framework is more effective than traditional adaptation methods. In the pure acoustic matching experiments, the proposed method is the best system under extremely small amount enrollment data (1 training utterance) and moderate training data. That mainly due to explicitly including the label information in background modeling and discriminant capability of unsupervised learning of HMM topology.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-171"
  },
  "anguita06_interspeech": {
   "authors": [
    [
     "Jan",
     "Anguita"
    ],
    [
     "Javier",
     "Hernando"
    ]
   ],
   "title": "On the use of Jacobian adaptation in real speaker verification applications",
   "original": "i06_1604",
   "page_count": 4,
   "order": 172,
   "p1": "paper 1604-Tue1CaP.6",
   "pn": "",
   "abstract": [
    "Jacobian Adaptation (JA) of the acoustic models is a fast adaptation technique that has been successfully used in both speech and speaker recognition systems. This technique adapts the acoustic models on the basis of the difference between the testing and the training noise conditions. For this reason, a noise reference of both the training and the testing phase is needed. In previous works, the noise conditions have been commonly supposed to be known, or estimated from the first part of the signals or using manually obtained speech and non-speech labels. However, in a real application the noise conditions are not generally known, it is not sure that the first part of the signal does not contain speech and manual labels are not usually available. In this work we propose to obtain the noise references by using continuous noise estimation methods, which are more appropriate for real applications. Several noise estimation methods are compared and the obtained results show that these techniques are effective for JA.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-172"
  },
  "liu06d_interspeech": {
   "authors": [
    [
     "Ming",
     "Liu"
    ],
    [
     "Huazhong",
     "Ning"
    ],
    [
     "Thomas S.",
     "Huang"
    ],
    [
     "Zhengyou",
     "Zhang"
    ]
   ],
   "title": "A novel framework of text-independent speaker verification based on utterance transform and iterative cohort modeling",
   "original": "i06_2034",
   "page_count": 4,
   "order": 173,
   "p1": "paper 2034-Tue1CaP.7",
   "pn": "",
   "abstract": [
    "A novel framework for text-independent speaker verification is proposed. The framework is based on a new interpretation of Universal Background Model. The UBM in our framework actually defines a transform which maps the variable length observation into a fixed dimensional supervector (supervector space). Each speech utterance is then mapped into a point in this supervector space. The similarity measure in this vector space is progressively refined via an iterative cohort modeling scheme. The experiments on NIST 2002 corpus show the effectiveness of this new framework. Overall the EER drops from the baseline system (with T-Norm) 9.21% to final improved system (without T-Norm) 8.07%. The new framework can effectively reduce the data dependence in the final output score which is clearly indicated in the second sets of experiments. The EER after T-Norm of final system marginally increases by relatively 1.73% compared to the EER of baseline system drops 16.12% relatively after T-Norm. Also, the relative improvement of DCF after T-Norm is marginal for the final improved system (2.47%) compared to 33.68% in baseline system. It clear shows that the iterative cohort modeling effectively reduce the data dependence of the final scores, so that T-Norm will not further improve the system performance. Also, the performance of novel frame clearly increases as the iteration grows which suggest that the framework progressively refine the similarity measure on the supervector space with the iterative cohort modeling.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-173"
  },
  "prakash06_interspeech": {
   "authors": [
    [
     "Vinod",
     "Prakash"
    ],
    [
     "John H. L.",
     "Hansen"
    ]
   ],
   "title": "A cohort - UBM approach to mitigate data sparseness for in-set/out-of-set speaker recognition",
   "original": "i06_1847",
   "page_count": 4,
   "order": 174,
   "p1": "paper 1847-Tue1CaP.8",
   "pn": "",
   "abstract": [
    "In this study, the problem of identifying in-set versus out-of-set speakers is addressed. Here the emphasis is on low enrolment and test data durations, in a text-independent mode. In order to compensate for the limited enrolment data (5 sec), a method is proposed that utilizes data from speakers that are acoustically close to a particular in-set speaker. A speaker specific model is obtained by adaptation of a base model that is built using data from such speakers. The performance of the proposed algorithm is evaluated using the TIMIT database with an adapted GMM classifier (GMM-UBM) employed as the baseline system. Experimental results show a consistent increase in system performance, with a relative improvement ranging from 10.57-58.33% depending on inset speaker size and test data duration.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-174"
  },
  "varadarajan06_interspeech": {
   "authors": [
    [
     "Vaishnevi S.",
     "Varadarajan"
    ],
    [
     "John H. L.",
     "Hansen"
    ]
   ],
   "title": "Analysis of lombard effect under different types and levels of noise with application to in-set speaker ID systems",
   "original": "i06_1599",
   "page_count": 4,
   "order": 175,
   "p1": "paper 1599-Tue1CaP.9",
   "pn": "",
   "abstract": [
    "This paper presents an analysis of Lombard speech produced under different types and levels of noise. The speech used for the analysis forms a part of the UT-SCOPE database and consists of sentences from the well-known TIMIT corpus, spoken in the presence of highway, large crowd and pink noise. Differences are shown to exist in the speech characteristics under these varying noise types. The deterioration of the EER of an in-set speaker identification system trained on neutral and tested with Lombard speech is also illustrated. A clear demarcation between the effect of noise and Lombard effect on noise is also given by testing with noisy Lombard speech. The effect of test-token duration on system performance under the Lombard condition is addressed. It is seen that test duration has no effect on the EER under Lombard effect. The average EER for 3s test duration is 14.7, 28.3, 48.2, 51.3 for neutral clean, clean Lombard, noisy neutral and noisy Lombard respectively, and 7.2, 26.4, 45.8, 50.8 respectively for 12s test duration.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-175"
  },
  "mccree06_interspeech": {
   "authors": [
    [
     "Alan",
     "McCree"
    ]
   ],
   "title": "Reducing speech coding distortion for speaker identification",
   "original": "i06_1989",
   "page_count": 4,
   "order": 176,
   "p1": "paper 1989-Tue1CaP.10",
   "pn": "",
   "abstract": [
    "In this paper, we investigate the degradation of speaker identification performance due to speech coding algorithms used in digital telephone networks, cellular telephony, and voice over IP. By analyzing the difference between front-end feature vectors derived from coded and uncoded speech in terms of spectral distortion, we are able to quantify this coding degradation. This leads to two novel methods for distortion compensation: codebook and LPC compensation. Both are shown to significantly reduce front-end mismatch, with the second approach providing the most encouraging results. Full experiments using a GMMUBM speaker ID system confirm the usefulness of both the front-end distortion analysis and the LPC compensation technique.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-176"
  },
  "kato06_interspeech": {
   "authors": [
    [
     "Tsuneo",
     "Kato"
    ],
    [
     "Hisashi",
     "Kawai"
    ]
   ],
   "title": "A text-prompted distributed speaker verification system implemented on a cellular phone and a mobile terminal",
   "original": "i06_1896",
   "page_count": 4,
   "order": 177,
   "p1": "paper 1896-Tue1CaP.11",
   "pn": "",
   "abstract": [
    "For a practical application of biometrics authentication on cellular phones, a speaker verification system was implemented on a cellular phone and a PDA type mobile terminal. The system has the following three features: 1) a distributed system configuration for connectivity to internet services, 2) text-prompted speaker verification using connected digit patterns for robustness to imposture, and 3) incremental model update for preventing deterioration of accuracy. The system was examined in a two-month test, and effects of the incremental model update were evaluated with those data. Experimental results showed that the average equal error rate over this period was reduced from 8.6% to 4.4% by monthly update, to 2.9% by weekly update, and to 0.9% by daily update.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-177"
  },
  "vishnubhotla06_interspeech": {
   "authors": [
    [
     "Srikanth",
     "Vishnubhotla"
    ],
    [
     "Carol Y.",
     "Espy-Wilson"
    ]
   ],
   "title": "Automatic detection of irregular phonation in continuous speech",
   "original": "i06_1893",
   "page_count": 4,
   "order": 178,
   "p1": "paper 1893-Tue1CaP.12",
   "pn": "",
   "abstract": [
    "Voice quality is one of the most important source characteristics of a speakers speech production process, and creakiness is one of the variations of voice quality. This paper describes the development of an algorithm to automatically detect irregular phonation, including creakiness and other variations, in continuous running speech. The algorithm is an extension of the Aperiodicity, Periodicity and Pitch (APP) Detector. The algorithm has been run on 485 files of the TIMIT database, which contained 677 instances of irregular phonation. The test set comprised of 97 speakers, of which 57 were male and 40 were female. The algorithm has been found to give an accuracy of 86.7% on average, with performance being almost the same for both male and female speakers. Automatic detection of irregular phonation should help characterize speakers for speaker identification applications.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-178"
  },
  "ramasubramanian06b_interspeech": {
   "authors": [
    [
     "V.",
     "Ramasubramanian"
    ],
    [
     "Deepak",
     "Vijaywargiay"
    ],
    [
     "Kumar V.",
     "Praveen"
    ]
   ],
   "title": "Highly noise robust text-dependent speaker recognition based on hypothesized wiener filtering",
   "original": "i06_1474",
   "page_count": 4,
   "order": 179,
   "p1": "paper 1474-Wed1A1O.1",
   "pn": "",
   "abstract": [
    "We propose a hypothesized Wiener filtering (HWF) algorithm for noise robust variable-text text-dependent speaker-recognition. The proposed algorithm exploits an important feature of the text - dependent mode of operation of speaker-recognition, namely, the availability of the clean reference templates of the words of the password text which is supposed to be the text of the input noisy speech. The proposed HWF algorithm is set within the one-pass DP framework proposed by us recently for text-dependent speakerrecognition, which enables use of multiple-templates for each word in the password. We evaluate the proposed HWF algorithm for both speaker-identification and speaker-verification using the TIDIGITS database and show that the proposed HWF algorithm has very high recognition accuracies for both additive white-noise conditions and non-stationary color noise conditions (factory, chopper and babble noises), which are also the typical conditions where conventional spectral subtraction techniques perform poorly.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-179"
  },
  "fujihara06_interspeech": {
   "authors": [
    [
     "Hiromasa",
     "Fujihara"
    ],
    [
     "Tetsuro",
     "Kitahara"
    ],
    [
     "Masataka",
     "Goto"
    ],
    [
     "Kazunori",
     "Komatani"
    ],
    [
     "Tetsuya",
     "Ogata"
    ],
    [
     "Hiroshi G.",
     "Okuno"
    ]
   ],
   "title": "Speaker identification under noisy environments by using harmonic structure extraction and reliable frame weighting",
   "original": "i06_1525",
   "page_count": 4,
   "order": 180,
   "p1": "paper 1525-Wed1A1O.2",
   "pn": "",
   "abstract": [
    "We present methods for automatic speaker identification in noisy environments. To improve noise robustness of speaker identification, we developed two methods, the harmonic structure extraction method and the reliable frame weighting method. The harmonic structure extraction method enables the speaker of input speech signals to be identified after environmental noise has been reduced. This method first extracts harmonic components of the speech from the sound mixtures and then resynthesizes a clean speech signal by using a sinusoidal model driven by harmonic components. The reliable frame weighting method then determines how each frame of the resynthesized speech is reliable (i.e. little influenced by environmental noises) by using two Gaussian mixture models for the speech and noise. The speaker can be robustly identified by attaching importance to reliable frames. Experimental results with thirty speakers showed that our method was able to reduce the influences of environmental noise and achieved an error rate of 10.7%, while the error rate for a conventional method was 18.9%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-180"
  },
  "stergiou06_interspeech": {
   "authors": [
    [
     "Andreas",
     "Stergiou"
    ],
    [
     "Aristodemos",
     "Pnevmatikakis"
    ],
    [
     "Lazaros C.",
     "Polymenakos"
    ]
   ],
   "title": "Enhancing the performance of a GMM-based speaker identification system in a multi-microphone setup",
   "original": "i06_1608",
   "page_count": 4,
   "order": 181,
   "p1": "paper 1608-Wed1A1O.3",
   "pn": "",
   "abstract": [
    "In this paper the speaker identification system developed at Athens Information Technology is presented. It is based on the Gaussian Mixture modeling of the Mel-Frequency Cepstral Coefficients of speech. Starting from this basic algorithm, we describe and discuss two significant modifications that have resulted in performance enhancements, in terms of both processing speed and identification accuracy. We present the performance of our system in the recent CLEAR 2006 evaluation workshop and also discuss approaches to further improve our system by fusing decisions derived from a multitude of sensors in a multi-microphone setup.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-181"
  },
  "longworth06_interspeech": {
   "authors": [
    [
     "C.",
     "Longworth"
    ],
    [
     "M. J. F.",
     "Gales"
    ]
   ],
   "title": "Discriminative adaptation for speaker verification",
   "original": "i06_1553",
   "page_count": 4,
   "order": 182,
   "p1": "paper 1553-Wed1A1O.4",
   "pn": "",
   "abstract": [
    "Speaker verification is a binary classification task to determine whether a claimed speaker uttered a phrase. Current approaches to speaker verification tasks typically involve adapting a general speaker Universal Background Model (UBM), normally a Gaussian Mixture Model (GMM), to model a particular speaker. Verification is then performed by comparing the likelihoods from the speaker model to the UBM. Maximum A-Posteriori (MAP) is commonly used to adapt the UBM to a particular speaker. However speaker verification is a classification task. Thus, robust discriminative-based adaptation schemes should yield gains over the standard MAP approach. This paper describes and evaluates two discriminative approaches to speaker verification. The first is a discriminative version of MAP based on Maximum Mutual Information (MMI-MAP). The second is to use an augmented-GMM (A-GMM) as the speaker-specific model. The additional, augmented, parameters are discriminatively, and robustly, trained using a maximum margin estimation approach. The performance of these models is evaluated on the NIST 2002 SRE dataset. Though no gains were obtained using MMI-MAP, the A-GMM system gave an Equal Error Rate (EER) of 7.31%, a 30% relative reduction in EER compared to the best performing GMM system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-182"
  },
  "hatch06_interspeech": {
   "authors": [
    [
     "Andrew O.",
     "Hatch"
    ],
    [
     "Sachin",
     "Kajarekar"
    ],
    [
     "Andreas",
     "Stolcke"
    ]
   ],
   "title": "Within-class covariance normalization for SVM-based speaker recognition",
   "original": "i06_1874",
   "page_count": 4,
   "order": 183,
   "p1": "paper 1874-Wed1A1O.5",
   "pn": "",
   "abstract": [
    "This paper extends the within-class covariance normalization (WCCN) technique described in [1, 2] for training generalized linear kernels. We describe a practical procedure for applying WCCN to an SVM-based speaker recognition system where the input feature vectors reside in a high-dimensional space. Our approach involves using principal component analysis (PCA) to split the original feature space into two subspaces: a low-dimensional \"PCA space\" and a high-dimensional \"PCA-complement space.\" After performing WCCN in the PCA space, we concatenate the resulting feature vectors with a weighted version of their PCA-complements. When applied to a state-of-the-art MLLR-SVM speaker recognition system, this approach achieves improvements of up to 22% in EER and 28% in minimum decision cost function (DCF) over our previous baseline. We also achieve substantial improvements over an MLLR-SVM system that performs WCCN in the PCA space but discards the PCA-complement.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-183"
  },
  "espywilson06_interspeech": {
   "authors": [
    [
     "Carol Y.",
     "Espy-Wilson"
    ],
    [
     "Sandeep",
     "Manocha"
    ],
    [
     "Srikanth",
     "Vishnubhotla"
    ]
   ],
   "title": "A new set of features for text-independent speaker identification",
   "original": "i06_1880",
   "page_count": 4,
   "order": 184,
   "p1": "paper 1880-Wed1A1O.6",
   "pn": "",
   "abstract": [
    "The success of a speaker identification system depends largely on the set of features used to characterize speaker-specific information. In this paper, we discuss a small set of low-level acoustic parameters that capture information about the speakers source, vocal tract size and vocal tract shape. We demonstrate that the set of eight acoustic parameters has comparable performance to the standard sets of 26 or 39 MFCCs for the speaker identification task. Gaussian Mixture Models were used for constructing speaker models.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-184"
  },
  "ofoegbu06_interspeech": {
   "authors": [
    [
     "Uchechukwu O.",
     "Ofoegbu"
    ],
    [
     "Ananth N.",
     "Iyer"
    ],
    [
     "Robert E.",
     "Yantorno"
    ],
    [
     "Stanley J.",
     "Wenndt"
    ]
   ],
   "title": "Detection of a third speaker in telephone conversations",
   "original": "i06_1133",
   "page_count": 4,
   "order": 185,
   "p1": "paper 1133-Wed3CaP.1",
   "pn": "",
   "abstract": [
    "Differentiating speakers participating in telephone conversations is a challenging task in speech processing because only short consecutive utterances can be examined for each speaker. Research has shown that, given only brief utterances (1 second or less), humans can recognize speakers with an accuracy of about 54% on average. The task becomes even more challenging when no information about the speakers is known a priori. In this paper, a technique for determining whether there are two or three speakers participating in a telephone conversation is presented. This approach assumes no knowledge or information about any of the participating speakers. The technique is based on comparing short utterances within the conversation and deciding whether or not they belong to the same speaker. The applications of this research include 3-way call detection and speaker tracking, and could be extended to speaker change-point detection and indexing. The proposed method involves an elimination process in which speech segments matching two reference models are sequentially removed from the conversation. Models are formed using the mean vectors and covariance matrices of Linear Predictive Cepstral Coefficients of voiced segments in each conversation. Hotellings T2-Statistic is used to determine if two models belong to the same or to different speakers based on likelihood ratio testing. The relative amount of residual speech is observed after the elimination process to determine if a third speaker is present. The proposed technique yielded an equal error rate of 20% when tested on artificially simulated conversations from the HTIMIT database and 23% error rate when tested on actual telephone conversations.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-185"
  },
  "biatov06_interspeech": {
   "authors": [
    [
     "Konstantin",
     "Biatov"
    ],
    [
     "Joachim",
     "Köhler"
    ]
   ],
   "title": "Improvement speaker clustering using global similarity features",
   "original": "i06_1451",
   "page_count": 4,
   "order": 186,
   "p1": "paper 1451-Wed3CaP.2",
   "pn": "",
   "abstract": [
    "In this paper global similarity features that improve speaker clustering based on standard bottom-up clustering are proposed. The novelty of this approach lies in the fact that it exploits the hypothesis that audio segments belonging to the same speaker cluster should demonstrate similar global behavior, exhibit the same similarity and dissimilarity with all the other segments. Every segment is represented by a global similarity vector whose components are encoded by the distance between that segment and each of the other segments to be clustered. The distance between global similarity vectors is used for pre-selection of segment pairs having high global similarity for further merging. In this paper inter-segment distance for global similarity vectors based on Bayesian Information Criterion (BIC) and based on adapted cross likelihood ratio (CLR) are investigated. The evaluation, performed on radio programs, shows that the proposed approach represents an improvement in comparison with the baseline clustering.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-186"
  },
  "narayanaswamy06_interspeech": {
   "authors": [
    [
     "Balakrishnan",
     "Narayanaswamy"
    ],
    [
     "Rashmi",
     "Gangadharaiah"
    ],
    [
     "Richard M.",
     "Stern"
    ]
   ],
   "title": "Voting for two speaker segmentation",
   "original": "i06_1932",
   "page_count": 4,
   "order": 187,
   "p1": "paper 1932-Wed3CaP.3",
   "pn": "",
   "abstract": [
    "The process of locating the end points of each speakers voice in an audio file and then clustering segments based in speaker identity is called speaker segmentation. In this paper we present a method for two speaker segmentation, though it can be extended to more than two speakers. Most methods for speaker segmentation and clustering start with an initial computationally inexpensive speaker segmentation method, followed by a more accurate segment clustering. In this paper we describe a simple algorithm that improves the accuracy of the segment clustering while not increasing the computational complexity. Since the clustering is done iteratively, the improvement in each segment clustering step results in a significant overall increase in segmentation accuracy and cluster purity. We borrow ideas from speaker recognition to perform segment clustering by frame based voting. We look at each frame as an independent classifier deciding which speaker generated that segment. These classifiers are combined by voting to make a decision as to which segments should be clustered together. This simple change leads to 56.9% decrease in error rates on a segmentation task for the SWITCHBOARD corpus.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-187"
  },
  "preti06_interspeech": {
   "authors": [
    [
     "Alexandre",
     "Preti"
    ],
    [
     "Jean-François",
     "Bonastre"
    ]
   ],
   "title": "Unsupervised model adaptation for speaker verification",
   "original": "i06_1554",
   "page_count": 4,
   "order": 188,
   "p1": "paper 1554-Wed3CaP.4",
   "pn": "",
   "abstract": [
    "This paper deals with unsupervised model adaptation for speaker recognition. Two adaptation schemes are proposed, the first one is based on a test by test model adaptation and the second one proposes a batch mode, where the adaptation is performed using a set of tests before computing the decision score for each of them. The experiments are conducted thanks to the NIST SRE 2005 database. This paper shows clearly the interest of unsupervised model adaptation when enough test data is available (batch mode) and the intrinsic difficulty of an online (test by test) adaptation mode.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-188"
  },
  "zheng06_interspeech": {
   "authors": [
    [
     "Rong",
     "Zheng"
    ],
    [
     "Shuwu",
     "Zhang"
    ],
    [
     "Bo",
     "Xu"
    ]
   ],
   "title": "A quality measure method using Gaussian mixture models and divergence measure for speaker identification",
   "original": "i06_1328",
   "page_count": 4,
   "order": 189,
   "p1": "paper 1328-Wed3CaP.5",
   "pn": "",
   "abstract": [
    "Previous work has demonstrated the promise of frame-level quality measure methods to robust speaker recognition. This paper explores the issues involved in applying soft estimates to quality measures as weighting factors in score computation. A quality measure algorithm using Gaussian mixture density and Jensen divergence measure is presented for traditional GMM-UBM scoring mechanism. Derivation and validation of the quality measurement are reported in this paper. We investigate the usefulness of different feature processing, different GMM-based quality models and incorporation of divergence measure for quality estimation. Comparison experiments performed on the NIST1999 SRE corpus show the effectiveness of the proposed method.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-189"
  },
  "zhang06_interspeech": {
   "authors": [
    [
     "Yushi",
     "Zhang"
    ],
    [
     "Waleed H.",
     "Abdulla"
    ]
   ],
   "title": "Gammatone auditory filterbank and independent component analysis for speaker identification",
   "original": "i06_1354",
   "page_count": 4,
   "order": 190,
   "p1": "paper 1354-Wed3CaP.6",
   "pn": "",
   "abstract": [
    "Feature extraction is the key procedure when aiming at robust speaker identification. The most commonly used feature extraction techniques work successfully only in clean or matched environments. Accurate speaker identification is made difficult due to a number of factors, with handset/channel mismatch and environmental noise being the most prominent. This paper presents a novel technique which based on Gammatone filterbank (GTF) and independent component analysis (ICA). The presented method first relies on the Gammatone filterbank to emulate the human cochlea frequency resolution. By using ICA, it extracts the dominant components from these frequency banks. The extracted features emphasis the difference in the statistical structures among the speakers, which can model the distribution of the individuals. Compared to the commonly used techniques, such as linear predictive cepstral coefficients (LPCC), Mel-frequency cepstrum coefficients (MFCC) and perceptual linear predictive (PLP), the proposed method is more robust to additive noises and yields higher recognition rate in mismatch environments in a text-independent speaker identification system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-190"
  },
  "wu06_interspeech": {
   "authors": [
    [
     "Wei",
     "Wu"
    ],
    [
     "Thomas Fang",
     "Zheng"
    ],
    [
     "Ming-Xing",
     "Xu"
    ],
    [
     "Huan-Jun",
     "Bao"
    ]
   ],
   "title": "Study on speaker verification on emotional speech",
   "original": "i06_1124",
   "page_count": 4,
   "order": 191,
   "p1": "paper 1124-Wed3CaP.7",
   "pn": "",
   "abstract": [
    "Besides background noise, channel effect and speakers health condition, emotion is another factor which may influence the performance of a speaker verification system. In this paper, the performance of a GMM-UBM based speaker verification system on emotional speech is studied. It is found that speech with various emotions aggravates the verification performance. Two reasons for the performance aggravation are analyzed, they are mismatched emotions between the speaker models and the test utterances, and the articulating styles of certain emotions which create intense intra-speaker vocal variability. In response to the first reason, an emotion-dependent score normalization method is proposed, which is borrowed from the idea of Hnorm.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-191"
  },
  "farrus06_interspeech": {
   "authors": [
    [
     "M.",
     "Farrús"
    ],
    [
     "A.",
     "Garde"
    ],
    [
     "P.",
     "Ejarque"
    ],
    [
     "J.",
     "Luque"
    ],
    [
     "Javier",
     "Hernando"
    ]
   ],
   "title": "On the fusion of prosody, voice spectrum and face features for multimodal person verification",
   "original": "i06_1256",
   "page_count": 4,
   "order": 192,
   "p1": "paper 1256-Wed3CaP.8",
   "pn": "",
   "abstract": [
    "Multimodal person recognition systems normally use short-term spectral features as voice information. In this paper prosodic information is added to a system based on face and voice spectrum features. By using two fusion techniques, support vector machines and matcher weighting, different fusion strategies based on the fusion of monomodal scores in several steps are proposed. The performance of the system is clearly improved when the prosodic information is added and the best results are achieved when prosodic scores are previously fused and the resulting scores are fused again with spectral and facial scores. Speech and face scores have been obtained upon Switchboard-I and XM2VTS databases respectively.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-192"
  },
  "pruthi06_interspeech": {
   "authors": [
    [
     "Tarun",
     "Pruthi"
    ],
    [
     "Carol Y.",
     "Espy-Wilson"
    ]
   ],
   "title": "An MRI based study of the acoustic effects of sinus cavities and its application to speaker recognition",
   "original": "i06_1411",
   "page_count": 4,
   "order": 193,
   "p1": "paper 1411-Wed3CaP.9",
   "pn": "",
   "abstract": [
    "The goal of this paper is to explore the effects of changes in velar coupling area and oral cavity configuration on the poles and zeros introduced in the nasalized vowel and nasal consonant spectra due to the sphenoidal and maxillary sinuses. MRI data for the vocal tract and nasal tract of one speaker was used to simulate the spectra of the nasalized vowels /A, æ, i, u/, and nasal consonants /m, n/ with different coupling areas. It is shown that during nasalized vowels, the frequencies of both poles and zeros due to the sinuses change with a change in the velar coupling area or the vowel. It is also shown that during nasal consonants, the zero frequencies are constant, and the pole frequencies are more stable as compared to nasalized vowels. This study, therefore, corroborates the use of nasal consonant spectra for speaker recognition and raises doubts on the potential benefits of using nasalization during vowels for that purpose.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-193"
  },
  "kojima06_interspeech": {
   "authors": [
    [
     "Mariko",
     "Kojima"
    ],
    [
     "Tomoko",
     "Matsui"
    ],
    [
     "Hiromichi",
     "Kawanami"
    ],
    [
     "Hiroshi",
     "Saruwatari"
    ],
    [
     "Kiyohiro",
     "Shikano"
    ]
   ],
   "title": "Speaker verification with non-audible murmur segments",
   "original": "i06_1773",
   "page_count": 4,
   "order": 194,
   "p1": "paper 1773-Wed3CaP.10",
   "pn": "",
   "abstract": [
    "We propose a speaker verification method using non-audible murmur (NAM) segments, which are different from normal speech and hard for other people to catch them. To use NAM, we therefore take a text-dependent verification strategy in which each user utters her/his own keyword phrase and utilize not only speaker-specific but also keyword-specific acoustic information. We expect this strategy to yield a relatively high performance. NAM segments, which consist of multiple short-term feature vectors, are used as input vectors to capture keyword-specific acoustic information well. To handle segments with a large number of dimensions, we use the support vector machine (SVM). In experiments using NAM data of 19 male and 10 female speakers recorded in three different sessions, we achieved equal error rates of 0.04% (male) and 1.1% (female) when using 145-ms-long NAM segments. These rates are half or less those obtained with 25-ms-long input vectors.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-194"
  },
  "muller06_interspeech": {
   "authors": [
    [
     "Christian",
     "Müller"
    ]
   ],
   "title": "Automatic recognition of speakers² age and gender on the basis of empirical studies",
   "original": "i06_1031",
   "page_count": 4,
   "order": 195,
   "p1": "paper 1031-Wed3CaP.11",
   "pn": "",
   "abstract": [
    "This paper describes a system that exploits the paralinguistic information in the speech to estimate the speakers age and gender. Compared with previously published work, the so called AGENDER approach involves finer grained speaker classes and achieves a significantly higher classification accuracy. The introduction encompasses various application examples representing the actual AGENDER project context. Then hypotheses, method and a representative selection of results from extensive corpus analyses are presented, that build the empirical basis for the machine learning. Finally, the AGENDER approach on speaker classification is outlined, involving the comparison of different classification methods as well as evaluation results. The paper finishes with an outlook on extensions that are scheduled for the next project phase.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-195"
  },
  "fox06_interspeech": {
   "authors": [
    [
     "E. J. S.",
     "Fox"
    ],
    [
     "J. D.",
     "Roberts"
    ],
    [
     "M.",
     "Bennamoun"
    ]
   ],
   "title": "Text-independent speaker identification in birds",
   "original": "i06_1068",
   "page_count": 4,
   "order": 196,
   "p1": "paper 1068-Wed3CaP.12",
   "pn": "",
   "abstract": [
    "Speaker recognition is used to identify individual humans, but has rarely been applied to other species. To be applicable to the wide variety of bird species, text-independent speaker identification would be the most effective method. This is the first paper to report results of this technique in a species other than humans. Mel-frequency cepstral coefficients were extracted from recordings of three bird species and a multilayer perceptron was used as the classifier in each species. First, the song types used in training and testing were not controlled for, and these conditions gave an accuracy of 68-100%. Next the recordings of the wagtails and scrub-birds were split into their respective song types, a network was trained with one song type from each individual and tested with a different song type. With these purely text-independent conditions the accuracy was 71-96%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-196"
  },
  "potamitis06_interspeech": {
   "authors": [
    [
     "Ilyas",
     "Potamitis"
    ],
    [
     "Todor",
     "Ganchev"
    ],
    [
     "Nikos",
     "Fakotakis"
    ]
   ],
   "title": "Automatic acoustic identification of insects inspired by the speaker recognition paradigm",
   "original": "i06_1505",
   "page_count": 4,
   "order": 197,
   "p1": "paper 1505-Wed3CaP.13",
   "pn": "",
   "abstract": [
    "This work reports our research efforts towards developing efficient equipment for the automatic acoustic recognition of insects. In particular, we discuss the characteristics of the acoustic patterns of a target insect family, namely the cricket family. To address the recognition problem we apply a feature extraction methodology that has been inspired by well documented tactics of speech processing, which were adapted here to the specifics of the sound production mechanism of insects, in combination with state-of-the-art speaker identification technology. We apply this approach to a large and well documented database of families and subfamilies of cricket sounds, and we report results that exceed 99% recognition accuracy on the levels of family and subfamily, and 94% on the level of a specific insect out of 105 species. We deem this equipment will be of practical benefit for non-intrusive acoustic environmental monitoring applications as it is directly expandable to other insect species.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-197"
  },
  "siniscalchi06_interspeech": {
   "authors": [
    [
     "Sabato Marco",
     "Siniscalchi"
    ],
    [
     "Jinyu",
     "Li"
    ],
    [
     "Chin-Hui",
     "Lee"
    ]
   ],
   "title": "A study on lattice rescoring with knowledge scores for automatic speech recognition",
   "original": "i06_1319",
   "page_count": 4,
   "order": 198,
   "p1": "paper 1319-Mon3A2O.1",
   "pn": "",
   "abstract": [
    "We study lattice rescoring with knowledge scores for automatic speech recognition. Frame-based log likelihood ratio is adopted as a score measure of the goodness-of-fit between a speech segment and the knowledge sources. We evaluate our approach in two different applications: phone recognition, and connected digit continuous recognition. By incorporating knowledge scores obtained from 15 attribute detectors for place and manner of articulation, we reduced phone error rate from 40.52% to 35.16% using monophone models. The error rate can be further reduced to 33.42% for triphone models. The same lattice rescoring algorithm is extended to connected digit recognition using the TIDIGITS database, and without using any digit-specific training data. We observed the digit error rate can be effectively reduced to 4.03% from 4.54% which was obtained with the conventional Viterbi decoding algorithm with no knowledge scores.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-198"
  },
  "stuker06_interspeech": {
   "authors": [
    [
     "Sebastian",
     "Stüker"
    ],
    [
     "Christian",
     "Fügen"
    ],
    [
     "Susanne",
     "Burger"
    ],
    [
     "Matthias",
     "Wölfel"
    ]
   ],
   "title": "Cross-system adaptation and combination for continuous speech recognition: the influence of phoneme set and acoustic front-end",
   "original": "i06_1509",
   "page_count": 4,
   "order": 199,
   "p1": "paper 1509-Mon3A2O.2",
   "pn": "",
   "abstract": [
    "Cross-system adaptation and system combination methods, such as ROVER and confusion network combination, are known to lower the word error rate of speech recognition systems. They require the training of systems that are reasonably close in performance but at the same time produce output that differs in its errors. This provides complementary information which leads to performance improvements. In this paper we demonstrate the gains we have seen with cross-system adaptation and system combination on the English EPPS and RT0-05S lecture meeting task. We obtained the necessary varying systems by using different acoustic front-ends and phoneme sets on which our models are based. In a set of contrastive experiments we show the influence that the exchange of the components has on adaptation and system combination.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-199"
  },
  "breslin06_interspeech": {
   "authors": [
    [
     "C.",
     "Breslin"
    ],
    [
     "M. J. F.",
     "Gales"
    ]
   ],
   "title": "Generating complementary systems for speech recognition",
   "original": "i06_1541",
   "page_count": 4,
   "order": 200,
   "p1": "paper 1541-Mon3A2O.3",
   "pn": "",
   "abstract": [
    "Large Vocabulary Continuous Speech Recognition (LVCSR) systems often use a multi-pass recognition framework where the final output is obtained from a combination of multiple models. Previous systems within this framework have normally built a number of independently trained models, before performing multiple experiments to determine the optimal combination. For two models to give improvements upon combination, it is clear that they must be complementary, i.e. they must make different errors. While independently trained models often do give improvements when they are combined, it is not guaranteed that they will be complementary. This paper presents a new algorithm, Minimum Bayes Risk Leveraging (MBRL), for explicitly generating systems that are complementary to each other. This algorithm is based on Minimum Bayes Risk training, but within a boosting-like iterative framework. Experimental results are reported on a Broadcast News Mandarin task. These experiments show small but consistent gains when combining complementary systems using confusion network combination.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-200"
  },
  "zhang06b_interspeech": {
   "authors": [
    [
     "Rong",
     "Zhang"
    ],
    [
     "Alexander I.",
     "Rudnicky"
    ]
   ],
   "title": "Investigations of issues for using multiple acoustic models to improve continuous speech recognition",
   "original": "i06_1707",
   "page_count": 4,
   "order": 201,
   "p1": "paper 1707-Mon3A2O.4",
   "pn": "",
   "abstract": [
    "This paper investigates two important issues in constructing and combining ensembles of acoustic models for reducing recognition errors. First, we investigate the applicability of the AnyBoost algorithm for acoustic model training. AnyBoost is a generalized Boosting method that allows the use of an arbitrary loss function as the training criterion to construct ensemble of classifiers. We choose the MCE discriminative objective function for our experiments. Initial test results on a real-world meeting recognition corpus show that AnyBoost is a competitive alternate to the standard AdaBoost algorithm. Second, we investigate ROVER-based combination, focusing on the technique for selecting correct hypothesized words from aligned WTN. We propose a neural network based insertion detection and word scoring scheme for this. Our approach consistently outperforms the current voting technique used by ROVER in the experiments.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-201"
  },
  "chen06_interspeech": {
   "authors": [
    [
     "I-Fan",
     "Chen"
    ],
    [
     "Lin-shan",
     "Lee"
    ]
   ],
   "title": "A new framework for system combination based on integrated hypothesis space",
   "original": "i06_1728",
   "page_count": 4,
   "order": 202,
   "p1": "paper 1728-Mon3A2O.5",
   "pn": "",
   "abstract": [
    "In this paper, a new concept of integrated hypothesis space for large vocabulary continuous speech recognition (LVCSR) system combination is proposed. Unlike the conventional systems combination approaches such as ROVER, the hypothesis spaces are directly integrated here without string alignment. In this way the timing information for all word hypotheses is well preserved and the new framework is more flexible on rescoring approaches used. Four rescoring criteria on the integrated hypothesis space were further explored and experiments on Chinese broadcast news corpus indicated improved performance.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-202"
  },
  "hoffmeister06_interspeech": {
   "authors": [
    [
     "Björn",
     "Hoffmeister"
    ],
    [
     "Tobias",
     "Klein"
    ],
    [
     "Ralf",
     "Schlüter"
    ],
    [
     "Hermann",
     "Ney"
    ]
   ],
   "title": "Frame based system combination and a comparison with weighted ROVER and CNC",
   "original": "i06_1523",
   "page_count": 4,
   "order": 203,
   "p1": "paper 1523-Mon3A2O.6",
   "pn": "",
   "abstract": [
    "In this paper we present a novel ASR system combination technique able to combine systems producing word graphs of different structure and with different segmentations. The new method is based on the definition of a time frame-wise word error cost function in a minimum Bayes risk framework. In contrast to confusion network combination (CNC), it preserves both the word graph structure and the word boundaries.\n",
    "First experimental results are presented on the European Parliament Plenary Sessions (EPPS) task for European Spanish and British English. The new approach to system combination is compared to both ROVER and CNC. In addition, we also apply data-driven weighting schemes for all system combination approaches addressed in this work. For the experiments presented, a variety of internal systems as well as an additional external system were combined.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-203"
  },
  "yuan06_interspeech": {
   "authors": [
    [
     "Jiahong",
     "Yuan"
    ],
    [
     "Mark",
     "Liberman"
    ],
    [
     "Christopher",
     "Cieri"
    ]
   ],
   "title": "Towards an integrated understanding of speaking rate in conversation",
   "original": "i06_1795",
   "page_count": 4,
   "order": 204,
   "p1": "paper 1795-Mon3A3O.1",
   "pn": "",
   "abstract": [
    "We investigate factors that affect speaking rate in conversation, using large corpora of conversational telephone speech in English and Chinese. We find that speaking rate as a function of \"turn\" length rises rapidly for turns from one to seven words; remains level (when final words are included) or falls gradually (if final words are excluded) for turns of medium length; and rises slowly for longer turns. When talking with strangers or discussing certain topics, people tend to use longer turns but slower speech rates. In general older people have a slower speech, and males tend to speak slightly faster than females. Finally, we find that the effect of L1 (native language) on L2 (second language) speaking rate is L1 dependent.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-204"
  },
  "vu06_interspeech": {
   "authors": [
    [
     "Minh Quang",
     "Vu"
    ],
    [
     "Ðô Ðat",
     "Trân"
    ],
    [
     "Eric",
     "Castelli"
    ]
   ],
   "title": "Prosody of interrogative and affirmative sentences in vietnamese language: analysis and perceptive results",
   "original": "i06_1844",
   "page_count": 4,
   "order": 205,
   "p1": "paper 1844-Mon3A3O.2",
   "pn": "",
   "abstract": [
    "This paper presents a new study on the prosody of Vietnamese language. Sentence pairs containing one interrogative sentence and one affirmative sentence, which have the same tones and the same number of syllables to avoid the effects of lexical tones and of co-articulation, are recorded in order to analyze their prosody evolution. Comparisons allow us to characterize differences between interrogative and affirmative sentences at sentence prosody level. Our work is completed by a perceptual study on re-synthesized sound where all syllables of the sentence are replaced by the vowel /a/ to hide lexical meaning, while the prosody of the sentence is kept unchanged. Our goal is to see if sentence prosody carries any information about sentence nature characteristics, and then whether it enables listeners to classify sentence type (in this case interrogative and affirmative), despite the complex form of this prosody in tonal languages. The obtained results show that information on sentence type is present at the end of the second half of the last syllable and that about 70% of sentences are properly classified for female synthesis voice.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-205"
  },
  "venditti06_interspeech": {
   "authors": [
    [
     "Jennifer J.",
     "Venditti"
    ],
    [
     "Julia",
     "Hirschberg"
    ],
    [
     "Jackson",
     "Liscombe"
    ]
   ],
   "title": "Intonational cues to student questions in tutoring dialogs",
   "original": "i06_1407",
   "page_count": 4,
   "order": 206,
   "p1": "paper 1407-Mon3A3O.3",
   "pn": "",
   "abstract": [
    "Successful Intelligent Tutoring Systems (ITSs) must be able to recognize when their students are asking a question. They must identify question form as well as function in order to respond appropriately. Our study examines whether intonational features, specifically, F0 height and rise range, are useful cues to student question type in a corpus of 643 American English questions. Results show a quantitative effect of both form and function. In addition, among clarificationseeking questions, we observed differences based on the type of clarification being sought.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-206"
  },
  "krahmer06_interspeech": {
   "authors": [
    [
     "Emiel",
     "Krahmer"
    ],
    [
     "Marc",
     "Swerts"
    ]
   ],
   "title": "Testing the effect of audiovisual cues to prominence via a reaction-time experiment",
   "original": "i06_1288",
   "page_count": 4,
   "order": 207,
   "p1": "paper 1288-Mon3A3O.4",
   "pn": "",
   "abstract": [
    "This article discusses a perception experiment to investigate the relation between auditory and visual cues for marking prosodic prominence. The methodology makes use of a reaction-time experiment. For this experiment, recordings of a sentence with 3 accents were systematically manipulated in such a way that auditory and visual markers of prominence were either congruent (occurring on the same word) or incongruent (in that the auditory and the visual cues were positioned on different words). Subjects were instructed to indicate as fast as possible which word they perceived as the most prominent one. Classification results show first of all that subjects responses were much more dependent on auditory than on visual cues. In addition, however, we found that incongruent stimuli lead to slower reaction times than congruent stimuli, showing that visual cues do have an impact on the cognitive processing of prosodic prominence.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-207"
  },
  "gravano06_interspeech": {
   "authors": [
    [
     "Agustín",
     "Gravano"
    ],
    [
     "Julia",
     "Hirschberg"
    ]
   ],
   "title": "Effect of genre, speaker, and word class on the realization of given and new information",
   "original": "i06_1747",
   "page_count": 4,
   "order": 208,
   "p1": "paper 1747-Mon3A3O.5",
   "pn": "",
   "abstract": [
    "There is much evidence in the literature that speakers tend to deaccent discourse-given entities, while accenting new ones. However, speakers do not always follow this simple strategy and the causes for such variation are not yet well understood. In this paper, we describe several new forms of variability in the relationship between given/new information and accenting behavior, variation due to individual differences and to word class. We present results indicating that different speakers have different strategies for making new words prominent. We analyze two word-classes - nouns and verbs - in a corpus of spontaneous and read direction-giving monologues, and show that speakers use different combinations of pitch, intensity and inter-word pauses to distinguish between given and new information. Most interestingly, we find that in both genres all speakers tend to produce given verbs with higher intensity than new verbs.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-208"
  },
  "vainio06_interspeech": {
   "authors": [
    [
     "Martti",
     "Vainio"
    ],
    [
     "Juhani",
     "Järvikivi"
    ],
    [
     "Stefan",
     "Werner"
    ]
   ],
   "title": "Word order and tonal shape in the production of focus in short Finnish utterances",
   "original": "i06_1595",
   "page_count": 4,
   "order": 209,
   "p1": "paper 1595-Mon3A3O.6",
   "pn": "",
   "abstract": [
    "This paper presents results from a study on the production of Finnish prosody. The effect of word order and the tonal shape in the production of Finnish prosody was studied as produced by 8 native Finnish speakers. Predictions formulated with regard to results from an earlier study pertaining to the perception of prominence were tested. These predictions had to do with the tonal shape of the utterances in the form of a flat hat pattern and the effect of word order on the so called top-line declination within an adverbial phrase in the utterances. The results from the experiment give support to the following claims: the temporal domain of prosodic focus is the whole utterance, word order reversal from unmarked to marked has an effect on the production of prosody, and the production of the tonal aspects of focus in Finnish follows a basic flat hat pattern. That is the prominence of a word can be produced by an f0 rise or a fall, depending on the location of the word in an utterance. The basic accentual shape of a Finnish word is then not a pointed rise/fall hat shape as claimed before since it can vary depending on the syllable structure and the position within an utterance.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-209"
  },
  "kroger06_interspeech": {
   "authors": [
    [
     "Bernd J.",
     "Kröger"
    ],
    [
     "Peter",
     "Birkholz"
    ],
    [
     "Jim",
     "Kannampuzha"
    ],
    [
     "Christiane",
     "Neuschaefer-Rube"
    ]
   ],
   "title": "Modeling sensory-to-motor mappings using neural nets and a 3d articulatory speech synthesizer",
   "original": "i06_1192",
   "page_count": 4,
   "order": 210,
   "p1": "paper 1192-Mon3WeS.1",
   "pn": "",
   "abstract": [
    "A comprehensive neural model of speech motor control including a three dimensional articulatory speech synthesizer as a front-end device is described in detail in this paper. The training of the sensoryto- motor mappings - which can be interpreted as the prelinguistic phase of speech acquisition - is described in detail for quasi-static as well as for dynamic articulation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-210"
  },
  "fontecave06_interspeech": {
   "authors": [
    [
     "Julie",
     "Fontecave"
    ],
    [
     "Frédéric",
     "Berthommier"
    ]
   ],
   "title": "Semi-automatic extraction of vocal tract movements from cineradiographic data",
   "original": "i06_1439",
   "page_count": 4,
   "order": 211,
   "p1": "paper 1439-Mon3WeS.2",
   "pn": "",
   "abstract": [
    "Since high speed X-ray films still provide the best dynamic view of the entire vocal tract, large existing databases have been preserved and are available for the speech research community. We propose a new technique for facilitating the extraction of the vocal tract shape and the movements of the articulators from complete sequences of these databases. The method was first developed for the extraction of the tongue movements in \"Wioland\". It has been adapted to a sequence of the ATR database, Laval43 (provided by Rochette). The method, based on the retromarking algorithm, combines the human expertise applied for marking a small number of key images, and the automatic processing of the video data. It has been extended to other articulators (lips, velum) in order to obtain the shape and the sections of the complete vocal tract. Quantitative evaluations of the estimate error and a comparison with Thimm and Luettin (1999) are achieved.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-211"
  },
  "jou06_interspeech": {
   "authors": [
    [
     "Szu-Chen",
     "Jou"
    ],
    [
     "Tanja",
     "Schultz"
    ],
    [
     "Matthias",
     "Walliczek"
    ],
    [
     "Florian",
     "Kraft"
    ],
    [
     "Alex",
     "Waibel"
    ]
   ],
   "title": "Towards continuous speech recognition using surface electromyography",
   "original": "i06_1592",
   "page_count": 4,
   "order": 212,
   "p1": "paper 1592-Mon3WeS.3",
   "pn": "",
   "abstract": [
    "We present our research on continuous speech recognition of the surface electromyographic signals that are generated by the human articulatory muscles. Previous research on electromyographic speech recognition was limited to isolated word recognition because it was very difficult to train phoneme-based acoustic models for the electromyographic speech recognizer. In this paper, we demonstrate how to train the phoneme-based acoustic models with carefully designed electromyographic feature extraction methods. By decomposing the signal into different feature space, we successfully keep the useful information while reducing the noise. Additionally, we also model the anticipatory effect of the electromyographic signals compared to the speech signal. With a 108-word decoding vocabulary, the experimental results show that the word error rate improves from 86.8% to 32.0% by using our novel feature extraction methods.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-212"
  },
  "richmond06_interspeech": {
   "authors": [
    [
     "Korin",
     "Richmond"
    ]
   ],
   "title": "A trajectory mixture density network for the acoustic-articulatory inversion mapping",
   "original": "i06_1790",
   "page_count": 4,
   "order": 213,
   "p1": "paper 1790-Mon3WeS.4",
   "pn": "",
   "abstract": [
    "This paper proposes a trajectory model which is based on a mixture density network trained with target features augmented with dynamic features together with an algorithm for estimating maximum likelihood trajectories which respects constraints between the static and derived dynamic features. This model was evaluated on an inversion mapping task. We found the introduction of the trajectory model successfully reduced root mean square error by up to 7.5%, as well as increasing correlation scores.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-213"
  },
  "metze06_interspeech": {
   "authors": [
    [
     "Florian",
     "Metze"
    ]
   ],
   "title": "Articulatory features for \"meeting\" speech recognition",
   "original": "i06_1891",
   "page_count": 4,
   "order": 214,
   "p1": "paper 1891-Mon3WeS.5",
   "pn": "",
   "abstract": [
    "\"Meeting\" speech, for example from the RT-04S task, contains a mixture of different speaking styles that leads to word error rates higher than 25% even when close-talking microphones are being used. The problem is even more serious, as word error rates are particularly high when speakers use a clear speaking mode, for example because they want to stress an important point. Previous work showed that an approach that combines standard phone-based acoustic models with models detecting the presence or absence of \"Articulatory Features\" such as \"Rounded\" or \"Voiced\" can improve ASR performance particularly for these cases. This paper presents a discriminative approach to automatically computing from training or adaptation data the feature stream weights needed for the above approach, therefore presenting a framework for integrating articulatory features into existing automatic speech recognition systems. We find a 7% relative improvements on top of our best RT-04S system using discriminative adaptation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-214"
  },
  "krnoul06_interspeech": {
   "authors": [
    [
     "Zdenek",
     "Krnoul"
    ],
    [
     "Milos",
     "Zelezný"
    ],
    [
     "Ludek",
     "Müller"
    ],
    [
     "Jakub",
     "Kanis"
    ]
   ],
   "title": "Training of coarticulation models using dominance functions and visual unit selection methods for audio-visual speech synthesis",
   "original": "i06_1905",
   "page_count": 4,
   "order": 215,
   "p1": "paper 1905-Mon3WeS.6",
   "pn": "",
   "abstract": [
    "This paper presents results of training of coarticulation models for Czech audio-visual speech synthesis. Two approaches for solution of coarticulation in audio-visual speech synthesis were used, coarticulation based on dominance functions and visual unit selection. For both approaches, coarticulation models were trained. Models for unit selection approach were trained by visually clustered data. These data were obtained using decision tree algorithm. Outputs of audio-visual speech synthesis for both approaches were assessed and compared objectively.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-215"
  },
  "zhang06c_interspeech": {
   "authors": [
    [
     "Le",
     "Zhang"
    ],
    [
     "Steve",
     "Renals"
    ]
   ],
   "title": "Phone recognition analysis for trajectory HMM",
   "original": "i06_1203",
   "page_count": 4,
   "order": 216,
   "p1": "paper 1203-Mon3BuP.1",
   "pn": "",
   "abstract": [
    "The trajectory HMM has been shown to be useful for model-based speech synthesis where a smoothed trajectory is generated using temporal constraints imposed by dynamic features. To evaluate the performance of such model on an ASR task, we present a trajectory decoder based on tree search with delayed path merging. Experiment on a speaker-dependent phone recognition task using the MOCHA-TIMIT database shows that the MLE-trained trajectory model, while retaining attractive properties of being a proper generative model, tends to favour over-smoothed trajectory among competing hypothesises, and does not perform better than a conventional HMM. We use this to build an argument that models giving better fit on training data may suffer a reduction of discrimination by being too faithful to training data. This partially explains why alternative acoustic models that try to explicitly model temporal constraints do not achieve significant improvements in ASR.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-216"
  },
  "keshet06_interspeech": {
   "authors": [
    [
     "Joseph",
     "Keshet"
    ],
    [
     "Shai",
     "Shalev-Shwartz"
    ],
    [
     "Samy",
     "Bengio"
    ],
    [
     "Yoram",
     "Singer"
    ],
    [
     "Dan",
     "Chazan"
    ]
   ],
   "title": "Discriminative kernel-based phoneme sequence recognition",
   "original": "i06_1284",
   "page_count": 4,
   "order": 217,
   "p1": "paper 1284-Mon3BuP.2",
   "pn": "",
   "abstract": [
    "We describe a new method for phoneme sequence recognition given a speech utterance, which is not based on the HMM. In contrast to HMMbased approaches, our method uses a discriminative kernel-based training procedure in which the learning process is tailored to the goal of minimizing the Levenshtein distance between the predicted phoneme sequence and the correct sequence. The phoneme sequence predictor is devised by mapping the speech utterance along with a proposed phoneme sequence to a vector-space endowed with an inner-product that is realized by a Mercer kernel. Building on large margin techniques for predicting whole sequences, we are able to devise a learning algorithm which distills to separating the correct phoneme sequence from all other sequences. We describe an iterative algorithm for learning the phoneme sequence recognizer and further describe an efficient implementation of it. We present initial encouraging experimental results with the TIMIT and compare the proposed method to an HMM-based approach.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-217"
  },
  "morris06_interspeech": {
   "authors": [
    [
     "Jeremy",
     "Morris"
    ],
    [
     "Eric",
     "Fosler-Lussier"
    ]
   ],
   "title": "Combining phonetic attributes using conditional random fields",
   "original": "i06_1287",
   "page_count": 4,
   "order": 218,
   "p1": "paper 1287-Mon3BuP.3",
   "pn": "",
   "abstract": [
    "A Conditional Random Field is a mathematical model for sequences that is similar in many ways to a Hidden Markov Model, but is discriminative rather than generative in nature. In this paper, we explore the application of the CRF model to ASR processing of discriminative phonetic features by building a system that performs first-pass phonetic recognition using discriminatively trained phonetic features. With this system, we show that this CRF model achieves an accuracy level in a phone recognition task that is superior to a similarly trained HMM model.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-218"
  },
  "nagarajan06_interspeech": {
   "authors": [
    [
     "T.",
     "Nagarajan"
    ],
    [
     "Douglas",
     "O'Shaughnessy"
    ]
   ],
   "title": "Discriminative MLE training using a product of Gaussian likelihoods",
   "original": "i06_1292",
   "page_count": 4,
   "order": 219,
   "p1": "paper 1292-Mon3BuP.4",
   "pn": "",
   "abstract": [
    "In this paper, we describe a discriminative technique to determine an optimal HMM topology for the each of the models in a continuous speech recognition system such that the word error rate (WER) is minimized. In conventional model selection techniques such as Bayesian information criterion (BIC), the model complexity is determined without considering the other classes in a system. In our work, an optimal model topology is selected by considering how well a given model can discriminate examples of other classes from its own. By doing so, the estimated model parameters indirectly make sure that class separability is increased. In an earlier work [1], we have proposed this technique and experiments were carried out on an E-set. Presently, we extend it for building a syllable-based continuous speech recognition system. Preliminary experiments carried out on the TIMIT corpus show that a considerable reduction in WER can be achieved using the proposed technique over the BIC-based technique for model selection.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-219"
  },
  "li06c_interspeech": {
   "authors": [
    [
     "Hao-Zheng",
     "Li"
    ],
    [
     "Douglas",
     "O'Shaughnessy"
    ]
   ],
   "title": "State-level variable modeling for phoneme classification",
   "original": "i06_1332",
   "page_count": 4,
   "order": 220,
   "p1": "paper 1332-Mon3BuP.5",
   "pn": "",
   "abstract": [
    "In HMM-based pattern recognition, the structure of the HMM is often predetermined according to some prior knowledge. In the recognition process, we usually make our judgment based on the maximum likelihood of the HMM, without considering the time-varying property of state-level variables, which unfortunately may lead to incorrect results. In this paper, we analyze the property of state-level variables in the HMM and show it is possible to significantly enhance the performance of speech recognition systems when using the state-level variable time-varying property. We propose four methods to model state-level variable trajectories and then test them on a phoneme classification task on the TIMIT speech corpus, 11.95% error rate reduction is achieved and some empirical conclusions are drawn.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-220"
  },
  "li06d_interspeech": {
   "authors": [
    [
     "Xiaolong",
     "Li"
    ],
    [
     "Li",
     "Deng"
    ],
    [
     "Dong",
     "Yu"
    ],
    [
     "Alex",
     "Acero"
    ]
   ],
   "title": "A time-synchronous phonetic decoder for a long-contextual-Span hidden trajectory model",
   "original": "i06_1409",
   "page_count": 4,
   "order": 221,
   "p1": "paper 1409-Mon3BuP.6",
   "pn": "",
   "abstract": [
    "A novel time-synchronous decoder, designed specifically for a Hidden Trajectory Model (HTM) whose likelihood score computation depends on long-span phonetic contexts, is presented. HTM is a recently developed acoustic model aimed to capture the underlying dynamic structure of speech coarticulation and reduction using a compact set of parameters. The long-span nature of the HTM had posed a great technical challenge for developing efficient search algorithms for full evaluation of the model. Taking on the challenge, the decoding algorithm is developed to deal effectively with the exponentially increased search space by HTM-specific techniques for hypothesis representation, word-ending recombination, and hypothesis pruning. Experimental results obtained on the TIMIT phonetic recognition task are reported, extending our earlier HTM evaluation paradigms based on N-best and A. lattice rescoring.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-221"
  },
  "casar06_interspeech": {
   "authors": [
    [
     "Marta",
     "Casar"
    ],
    [
     "Jose A. R.",
     "Fonollosa"
    ]
   ],
   "title": "Analysis of HMM temporal evolution for automatic speech recognition and utterance verification",
   "original": "i06_1586",
   "page_count": 4,
   "order": 222,
   "p1": "paper 1586-Mon3BuP.7",
   "pn": "",
   "abstract": [
    "This paper proposes a double layer speech recognition and utterance verification system based on the analysis of the temporal evolution of HMM¡¯s state scores. For the lower layer, it uses standard HMM-based acoustic modeling, followed by a Viterbi grammar-free decoding step which provides us with the state scores of the acoustic models. In the second layer, these state scores are added to the regular set of acoustic parameters, building a new set of expanded HMMs. Using this expanded set of HMMs for speech recognition a significant improvement in performance is achieved. Next, we will use this new architecture for utterance verification in a ¡°second opinion¡± framework. We will consign to the second layer evaluating the reliability of decoding using the acoustic models from the first layer. An outstanding improvement in performance versus a baseline verification algorithm has been achieved.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-222"
  },
  "tang06_interspeech": {
   "authors": [
    [
     "Min",
     "Tang"
    ],
    [
     "Aravind",
     "Ganapathiraju"
    ]
   ],
   "title": "Improvements to bucket box intersection algorithm for fast GMM computation in embedded speech recognition systems",
   "original": "i06_1678",
   "page_count": 4,
   "order": 223,
   "p1": "paper 1678-Mon3BuP.8",
   "pn": "",
   "abstract": [
    "Real-time performance is a very important goal for embedded speech recognition systems, where the evaluation of likelihoods for Gaussian mixture models (GMM) usually dominates the computation of a continuous density hidden Markov model (CDHMM) based system. The Bucket Box Intersection (BBI) algorithm is an optimization technique that uses a K-Dimensional binary tree to speed up the score computation of GMM without significantly hurting the recognition accuracy. In this paper, we propose three improvements to the traditional BBI algorithm. First, we define the optimal dividing hyper-plane as the plane that generates minimum expected number of mixture evaluations instead of the median hyper-plane. The size of BBI tree is reduced largely because of that. Second, we refine the location of dividing plane as the one that has the same Mahalanobis distance to the closest dividing mixture pair, instead of the boundary of Gaussian box. By doing this, we are able to improve recognition accuracy. Finally, we introduce the dividing planes which run across two dimensions to boost the range of dividing plane candidates and thus bring more speed-ups. We evaluated these techniques using Conversays speech engine CASSI in 2 different domains. The experimental results of new BBI algorithm show significant performance improvement over traditional BBI algorithm. Compared to the baseline system with no BBI algorithm implementation, we were able to speed up Gaussian computations by 50% with a less than 5% relative increase in word error rate.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-223"
  },
  "markov06_interspeech": {
   "authors": [
    [
     "Konstantin",
     "Markov"
    ],
    [
     "Satoshi",
     "Nakamura"
    ]
   ],
   "title": "Forward-backwards training of hybrid HMM/BN acoustic models",
   "original": "i06_1838",
   "page_count": 4,
   "order": 224,
   "p1": "paper 1838-Mon3BuP.9",
   "pn": "",
   "abstract": [
    "In this paper, we describe an application of the Forward-Backwards (FB) algorithm for maximum likelihood training of hybrid HMM/Bayesian Network (BN) acoustic models. Previously, HMM/BN parameter estimation was based on a Viterbi training algorithm that requires two passes over the training data: one for BN learning and one for updating HMM transition probabilities. In this work, we first analyze the F-B training for a conventional HMM and show that the state PDF parameter estimation is analogous to weighted-data classifier training. The gamma variable of the Forward-Backwards algorithm plays the role of the data weight. From this perspective, it is straightforward to apply F-B-based training to the HMM/BN models since the BN learning algorithm allows training with weighted data. Experiments on accented speech (American, British and Australian English) show that F-B training outperforms the previous Viterbi learning approach and that the HMM/BN model achieved better performance than the conventional HMM.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-224"
  },
  "gehrig06_interspeech": {
   "authors": [
    [
     "Dirk",
     "Gehrig"
    ],
    [
     "Thomas",
     "Schaaf"
    ]
   ],
   "title": "A comparative study of Gaussian selection methods in large vocabulary continuous speech recognition",
   "original": "i06_1954",
   "page_count": 4,
   "order": 225,
   "p1": "paper 1954-Mon3BuP.10",
   "pn": "",
   "abstract": [
    "Gaussian mixture models are the most popular probability density used in automatic speech recognition. During decoding, often many Gaussians are evaluated. Only a small number of Gaussians contributes significantly to probability. Several promising methods to select relevant Gaussians are known. These methods have different properties in terms of required memory, overhead and quality of selected Gaussians. Projection search, bucket box intersection, and Gaussian clustering are investigated in a broadcast news system with focus on adaptation (MLLR).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-225"
  },
  "suk06_interspeech": {
   "authors": [
    [
     "Soo-Young",
     "Suk"
    ],
    [
     "Seong-Jun",
     "Hahm"
    ],
    [
     "Ho-Youl",
     "Jung"
    ],
    [
     "Hyun-Yeol",
     "Chung"
    ]
   ],
   "title": "A successive state and mixture splitting for optimizing the size of models in speech recognition",
   "original": "i06_2022",
   "page_count": 4,
   "order": 226,
   "p1": "paper 2022-Mon3BuP.11",
   "pn": "",
   "abstract": [
    "A Successive State and Mixture Splitting (SSMS) algorithm for optimizing the size of models used in speech recognition for small size of mobile devices is proposed in this paper. The proposed algorithm employs essentially Continuous Hidden Markov Model (CHMM) structure and this CHMM consists of variable parameter topology in order to minimize the number of model parameters and to reduce recognition time. SSMS splits the Gaussian Output Probability Density Distribution (GOPDD) for variable parameter context independent model. Unlike the Successive State Splitting generating context dependent model, the algorithm constructs context independent model with suitable number of states and mixtures for each recognition units by automatic splitting of GOPDD in time and mixture domain. The recognition results showed that the proposed SSMS could reduce the total number of Gaussian up to 40.0% compared with the fixed parameter models at the same performance in speech recognition.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-226"
  },
  "ion06_interspeech": {
   "authors": [
    [
     "Valentin",
     "Ion"
    ],
    [
     "Reinhold",
     "Haeb-Umbach"
    ]
   ],
   "title": "Improved source modeling and predictive classification for channel robust speech recognition",
   "original": "i06_1083",
   "page_count": 4,
   "order": 227,
   "p1": "paper 1083-Mon3BuP.12",
   "pn": "",
   "abstract": [
    "The accuracy of distributed speech recognition has been shown to be very sensitive to errors occurring during transmission. One reason for this is that the classifier, usually trained under error free conditions, is unable to cope with the mismatch between an error free and error prone channel. In this paper we present a novel decision rule for classification which is able to account for channel errors. To achieve this, the classical Bayesian speech recognition approach has been reformulated for the server side, where the observation is known only to the extent, as is given by its a posteriori density function. We present a method to estimate the a posteriori density which is based on a Markov model of the source, which captures correlations of both static and dynamic features. A practical implementation is given, accompanied by experimental results for distributed speech recognition over an IP-network.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-227"
  },
  "kuhne06_interspeech": {
   "authors": [
    [
     "Marco",
     "Kühne"
    ],
    [
     "Roberto",
     "Togneri"
    ]
   ],
   "title": "Automatic English stop consonants classification using wavelet analysis and hidden Markov models",
   "original": "i06_1174",
   "page_count": 4,
   "order": 228,
   "p1": "paper 1174-Mon3CaP.1",
   "pn": "",
   "abstract": [
    "This paper compares wavelet and STFT analysis for a speaker-independent stop classification task using the TIMIT database. In the designed experiment the HMM classifier had to assign each test token to one of the following stop classes [d,g,b,t,k,p,dx]. On 6332 stops the wavelet features obtained an overall accuracy of 86% which corresponds to a 14% relative error reduction compared to the STFT baseline system. Furthermore an analysis of the HMM misclassifications revealed that voiced stops were highly confused with their voiceless unaspirated counterparts.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-228"
  },
  "wu06b_interspeech": {
   "authors": [
    [
     "Tingyao",
     "Wu"
    ],
    [
     "Dirk Van",
     "Compernolle"
    ],
    [
     "Jacques",
     "Duchateau"
    ],
    [
     "Hugo",
     "Van hamme"
    ]
   ],
   "title": "Single frame selection for phoneme classification",
   "original": "i06_1247",
   "page_count": 4,
   "order": 229,
   "p1": "paper 1247-Mon3CaP.2",
   "pn": "",
   "abstract": [
    "Our former study [1] has shown that maximum likelihood (ML) based frame selection, which selects reliable frames from a high resolution along the time axis, helps to improve the discrimination between phonemes. In this paper, we present our recent research on single frame selection for a phoneme classification task. A new single selection, which only selects one frame for one state in an Hidden Markov Model (HMM), is proposed. The new technique takes likelihoods of frames and their positions in a phoneme segment into account at the same time, and selects very few frames to represent the spectral evolution of the phoneme. Furthermore, we also show that for a low model complexity, a phoneme model trained by selected frames is more discriminative than a model using all frames.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-229"
  },
  "dusan06_interspeech": {
   "authors": [
    [
     "Sorin",
     "Dusan"
    ],
    [
     "Lawrence",
     "Rabiner"
    ]
   ],
   "title": "On the relation between maximum spectral transition positions and phone boundaries",
   "original": "i06_1317",
   "page_count": 4,
   "order": 230,
   "p1": "paper 1317-Mon3CaP.3",
   "pn": "",
   "abstract": [
    "Earlier research has shown that the maximum spectral transition positions are related with the perceptual critical points that contain the most important information for consonant and syllable perception. This paper presents a quantitative analysis of the relation, in time, between the maximum spectral transition positions and the phone boundaries in fluent read speech. This analysis is based on the training part of the TIMIT American English database which contains both phone boundaries and labels manually-determined by a group of experts. The results of this analysis show that there is a significant correlation between the maximum spectral transition positions and the manually selected phone boundaries. This, in turn, suggests that there is an important relation between the commonly accepted phone boundaries and the perceptual critical points.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-230"
  },
  "yingthawornsuk06_interspeech": {
   "authors": [
    [
     "T.",
     "Yingthawornsuk"
    ],
    [
     "H. Kaymaz",
     "Keskinpala"
    ],
    [
     "D.",
     "France"
    ],
    [
     "D. M.",
     "Wilkes"
    ],
    [
     "R. G.",
     "Shiavi"
    ],
    [
     "R. M.",
     "Salomon"
    ]
   ],
   "title": "Objective estimation of suicidal risk using vocal output characteristics",
   "original": "i06_1321",
   "page_count": 4,
   "order": 231,
   "p1": "paper 1321-Mon3CaP.4",
   "pn": "",
   "abstract": [
    "Vocal output characteristics of speech have previously been identified as possible cues to the assessment of suicide risk, and there is evidence that certain vocal parameters may be used as suicidal discriminators. The acoustic properties of male speech samples comprised of individuals carrying diagnoses of depression, suicide risk, and remission were analyzed and statistically compared.\n",
    "The male sample contained 10 high-risk suicidal patients, 13 depressed patients, and 9 remitted patients. Acoustic analyses of voiced power distribution were performed on speech samples extracted from audio recordings collected from the patients during clinical interviews. Features derived from the power spectral densities were found to be powerful discriminators of class membership in both studies of interview and reading passage sessions. The results support theories that identify psychomotor disturbances as central elements in depression and suicidality.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-231"
  },
  "didiot06_interspeech": {
   "authors": [
    [
     "E.",
     "Didiot"
    ],
    [
     "I.",
     "Illina"
    ],
    [
     "O.",
     "Mella"
    ],
    [
     "D.",
     "Fohr"
    ],
    [
     "Jean-Paul",
     "Haton"
    ]
   ],
   "title": "A wavelet-based parameterization for speech/music segmentation",
   "original": "i06_1361",
   "page_count": 4,
   "order": 232,
   "p1": "paper 1361-Mon3CaP.5",
   "pn": "",
   "abstract": [
    "The problem of speech/music discrimination is a challenging research problem which significantly impacts Automatic Speech Recognition (ASR) performance. This paper proposes new features for the Speech/Music discrimination task. We propose to use a decomposition of the audio signal based on wavelets, which allows a good analysis of non stationary signal like speech or music. We compute different energy types in each frequency band obtained from wavelet decomposition. Two class/non-class classifiers are used : one for speech/non-speech, one for music/non-music. On the different test corpora, the proposed wavelet approach gives better results than the MFCC one. For instance, we have a significant relative improvements of the error rate of 58.0% on the \"Scheirer\" corpus for the speech/music discrimination task.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-232"
  },
  "nagino06b_interspeech": {
   "authors": [
    [
     "Goshu",
     "Nagino"
    ],
    [
     "Makoto",
     "Shozakai"
    ]
   ],
   "title": "Distance measure between Gaussian distributions for discriminating speaking styles",
   "original": "i06_1383",
   "page_count": 4,
   "order": 233,
   "p1": "paper 1383-Mon3CaP.6",
   "pn": "",
   "abstract": [
    "Discriminating speaking styles is an important issue in speech recognition, speaker recognition and speaker segmentation. This paper compares distance measures between Gaussian distributions for discriminating speaking styles. The Mahalanobis distance, the Bhattacharyya distance and the Kullback-Leibler divergence, which are in common use for a definition as a distance measure between Gaussian distributions, are evaluated in terms of an accuracy to discriminate speaking styles. In this paper, the accuracy is judged on a visualized map, where speaking style speech corpora are mapped onto twodimensional space by utilizing a multidimensional scaling method. It is shown that speaking style clusters appear clearly grouped on the visualized map obtained by the Bhattacharyya distance and the Kullback-Leibler divergence. In addition, the visualized map corresponds to speech recognition performance, and the Kullback-Leibler shows higher sensitivity to recognition performance.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-233"
  },
  "pernkopf06_interspeech": {
   "authors": [
    [
     "Franz",
     "Pernkopf"
    ],
    [
     "Tuan Van",
     "Pham"
    ]
   ],
   "title": "Bayesian networks for phonetic classification using time-scale features",
   "original": "i06_1532",
   "page_count": 4,
   "order": 234,
   "p1": "paper 1532-Mon3CaP.7",
   "pn": "",
   "abstract": [
    "We present a phonetic classification approach based on Bayesian networks using time-scale features which are extracted from the discrete Wavelet transform. We apply Bayesian networks using discriminative and generative parameter and/or structure learning for classifying the speech frames into silence, voiced, unvoiced, mixed sounds, and two more categories, voiced closure and release of plosives. Gender dependent/independent experiments have been performed on the TIMIT database. The experiments show that (i) our time-scale features mostly outperform standard MFCC features, (ii) discriminative learning of Bayesian networks is superior to the generative approach.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-234"
  },
  "beringer06_interspeech": {
   "authors": [
    [
     "Nicole",
     "Beringer"
    ]
   ],
   "title": "Fast and effective retraining on contrastive vocal characteristics with bidirectional long short-term memory nets",
   "original": "i06_1602",
   "page_count": 4,
   "order": 235,
   "p1": "paper 1602-Mon3CaP.8",
   "pn": "",
   "abstract": [
    "We apply Long Short-Term Memory (LSTM) recurrent neural networks to a large corpus of unprompted speech - the German part of the VERBMOBIL corpus. By training first on a fraction of the data, then retraining on another fraction, we both reduce time costs and significantly improve recognition rates. Contrastive retraining on the initial vowel cluster fraction of the data according to the Psycho- Computational Model of Sound Acquisition (PCMSA) shows higher frame by frame correctness due to more sparseness and the articulatory position of the sounds. For comparison we show recognition rates of Hidden Markov Models (HMMs) on the same corpus, and provide a promising extrapolation for HMM-LSTM hybrids.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-235"
  },
  "ma06c_interspeech": {
   "authors": [
    [
     "Ning",
     "Ma"
    ],
    [
     "Phil",
     "Green"
    ],
    [
     "André",
     "Coy"
    ]
   ],
   "title": "Exploiting dendritic autocorrelogram structure to identify spectro-temporal regions dominated by a single sound source",
   "original": "i06_1639",
   "page_count": 4,
   "order": 236,
   "p1": "paper 1639-Mon3CaP.9",
   "pn": "",
   "abstract": [
    "Autocorrelograms exhibit tree-like structures whose spines are located at a delay of 1/F0. This paper exploits the dendritic autocorrelogram structure for the identification of spectro-temporal regions dominated by a single periodic sound source in monaural acoustic mixtures. Each frame of the mixture is first segmented into different sound sources in the autocorrelogram domain. Local pitch estimates are formed for each source and used as a cue for temporal integration. A confidence score is computed for each time-frequency pixel in the grouped regions to determine its probability of belonging to the group. The system is evaluated using simultaneous speech in a coherence measuring experiment and also employed within an ASR system where it produces improved results for the Interspeech 2006 Speech Separation Challenge.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-236"
  },
  "leelaphattarakij06_interspeech": {
   "authors": [
    [
     "Pairote",
     "Leelaphattarakij"
    ],
    [
     "Proadpran",
     "Punyabukkana"
    ],
    [
     "Atiwong",
     "Suchato"
    ]
   ],
   "title": "Locating phone boundaries from acoustic discontinuities using a two-staged approach",
   "original": "i06_1734",
   "page_count": 4,
   "order": 237,
   "p1": "paper 1734-Mon3CaP.10",
   "pn": "",
   "abstract": [
    "Ability to automatically align phonetic transcriptions with their associated acoustic signal is crucial to the development of computer-assisted speech training system where, it is frequently needed to locate phone boundaries from a known transcription in the signal. In this paper, an attempt to locate phone boundaries in the case where only the numbers of boundaries are known is described. Phone boundaries are hypothesized based solely on acoustic discontinuities without knowing exact transcriptions. This allows speech segmentation to be performed without large number of in-domain speech data for training. The boundary identification is done in two stages. Candidates for possible boundaries are selected in the first stage via local maxima of spectral changes. Dynamic programming is used to search for the best locations of phone boundaries from the candidate list. Allowing at most 20 ms. deviation from the actual boundaries, approximately 75% accuracy is achieved on a Thai continuous speech corpus.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-237"
  },
  "fu06_interspeech": {
   "authors": [
    [
     "Qiang",
     "Fu"
    ],
    [
     "Biing-Hwang",
     "Juang"
    ]
   ],
   "title": "Investigation on rescoring using minimum verification error (MVE) detectors",
   "original": "i06_1761",
   "page_count": 4,
   "order": 238,
   "p1": "paper 1761-Mon3CaP.11",
   "pn": "",
   "abstract": [
    "Discriminative training, especially Minimum Verification Error( MVE)method plays an important role in the detection-based ASR. Recently, discriminative training also has been shown to be effective in large vocabulary continuous speech recognition [1]. In this paper, we propose a rescoring framework to show the improvement by fusing MVE-trained detectors with a conventional recognizer. The recognizer performs regular Viterbi decoding, generating possible recognition candidates with corresponding likelihood in a fashion of either N-best lists or word graphs. Detectors trained under MVE criterion form and conduct hypothesis testing for all test tokens to accomplish additional scores. A number of linear or non-linear rescoring methods are then presented to combine these two groups of scores. The experiments were conducted on the TIMIT database, and the results indicates that combining based on word graphs outperforms the one on N-best lists in the final accuracy. This rescoring framework explores possible ways to combine other independent knowledge sources with a conventional recognizer. Further more, it can guide the future research of the pure detection-based ASR techniques.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-238"
  },
  "fu06b_interspeech": {
   "authors": [
    [
     "Qiang",
     "Fu"
    ],
    [
     "Antonio",
     "Moreno-Daniel"
    ],
    [
     "Biing-Hwang",
     "Juang"
    ],
    [
     "Jian-Lai",
     "Zhou"
    ],
    [
     "Frank K.",
     "Soong"
    ]
   ],
   "title": "Generalization of the minimum classification error (MCE) training based on maximizing generalized posterior probability (GPP)",
   "original": "i06_1780",
   "page_count": 4,
   "order": 239,
   "p1": "paper 1780-Mon3CaP.12",
   "pn": "",
   "abstract": [
    "In this paper, we generalize the training error definitions for minimum classification error (MCE) training and investigate their impact on recognition performance. Starting the conventional MCE method, we discuss with three issues in regard to training error definition, which may affect the recognizer performance and need to be extensively studied. We focus our discussions on the first two aspects in this paper. We re-visit the fact that the objective function in MCE training can be formulated into an equivalent form for maximizing the \"posterior probability\" of the corresponding training units. Based on the framework of the generalized posterior probability (GPP) [1], we design experiments to demonstrate effects about different training units and different constraints on segmentation boundaries for the MCE training. We also provide a performance analysis to illustrate our generalization for both phone recognition and word recognition tasks based on the wall street journal (WSJ0) [2, 3] database.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-239"
  },
  "carlin06_interspeech": {
   "authors": [
    [
     "Michael A.",
     "Carlin"
    ],
    [
     "Brett Y.",
     "Smolenski"
    ],
    [
     "Stanley J.",
     "Wenndt"
    ]
   ],
   "title": "Unsupervised detection of whispered speech in the presence of normal phonation",
   "original": "i06_1990",
   "page_count": 4,
   "order": 240,
   "p1": "paper 1990-Mon3CaP.13",
   "pn": "",
   "abstract": [
    "The results of an investigation into unsupervised detection of whispered speech segments in the presence of normally phonated speech are presented. The Whispered Speech Detection system presented here extracts features which exploit both waveform energy and periodicity. Unsupervised classification of these features was performed to identify and label long segments (approx. 2-2.5 seconds) of whispered speech which is typically an indication of criminal activity over telephone networks, for instance, in a correctional facility environment. Experiments indicate that it is possible to automatically detect long segments of whispering in the presence of normally phonated speech; testing of the algorithm presented in this paper yields promising results in correct identification of whispered speech segments.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-240"
  },
  "anguera06_interspeech": {
   "authors": [
    [
     "Xavier",
     "Anguera"
    ],
    [
     "Chuck",
     "Wooters"
    ],
    [
     "Javier",
     "Hernando"
    ]
   ],
   "title": "Friends and enemies: a novel initialization for speaker diarization",
   "original": "i06_1661",
   "page_count": 4,
   "order": 241,
   "p1": "paper 1661-Mon3CaP.14",
   "pn": "",
   "abstract": [
    "The task of speaker diarization consists of answering the question \"Who spoke when?\". The most commonly used approach to speaker diarization is agglomerative clustering of multiple initial clusters. Even though the initial clustering is greatly modified by iterative cluster merging and possibly multiple resegmentations of the data, the initialization algorithm is a key module for system performance and robustness. In this paper we present a novel approach that obtains a desired initial number of clusters in three steps. It first computes possible speaker change points via a standard technique based on the Bayesian information criterion (BIC). It then classifies the resulting segments into \"friend\" and \"enemy\" groups to finally creates an initial set of clusters for the system. We test this algorithm with the dataset used in the RT05s evaluation, where we show a 13% Diarization error rate relative improvement and a 2.5% absolute cluster purity improvement with respect to our previous algorithm.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-241"
  },
  "surana06_interspeech": {
   "authors": [
    [
     "Kushan",
     "Surana"
    ],
    [
     "Janet",
     "Slifka"
    ]
   ],
   "title": "Acoustic cues for the classification of regular and irregular phonation",
   "original": "i06_1755",
   "page_count": 4,
   "order": 242,
   "p1": "paper 1755-Mon3FoP.1",
   "pn": "",
   "abstract": [
    "Irregular phonation serves an important communicative function. It can be a cue to linguistic contrasts, and often serves as a marker for word and utterance boundaries. Automatic methods for classification and detection of regions of irregular phonation can be used to improve analyses of occurrences of irregular phonation and support technologies such as speech recognition and synthesis. This study proposes a set of acoustic cues from both the temporal and frequency domains - fundamental frequency, normalized RMS amplitude, smoothed-energy-difference amplitude and shift-difference amplitude -for separation of regions of regular and irregular phonation. Tokens from the TIMIT database are classified using support vector machines, trained on 114 different speakers and tested with 37 different speakers. Both genders are well represented in the data set and the tokens occur in various contexts within the utterance. In the test set, 292 of 320 irregular tokens (recognition rate of 91.25%), and 4105 of 4320 regular tokens (recognition rate of 95.02%) are correctly identified.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-242"
  },
  "nitisaroj06_interspeech": {
   "authors": [
    [
     "Rattima",
     "Nitisaroj"
    ]
   ],
   "title": "Realizations and representations of Thai tones in monomoraic syllables",
   "original": "i06_1381",
   "page_count": 4,
   "order": 243,
   "p1": "paper 1381-Mon3FoP.2",
   "pn": "",
   "abstract": [
    "This study investigates the phonetic realizations of Thai tones in monomoraic syllables produced with three different speeds. The results do not support previous claim that tones in monomoraic syllables are neutralized to mid, and lend support to a phonological representation in which the mora is associated to either H or no tone at all. In normal speech, a /H/ vs. /Ø/ opposition is found. In slow speech, a /H/ vs. /L/ distinction is found. Past research used the contrastive tonal status observed in slow speech as evidence to posit the underlying high and low tones for monomoraic syllables. In the present study, it is argued that the moraic association of the phonological low tone takes place as a result of glottal stop insertion.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-243"
  },
  "jacobi06_interspeech": {
   "authors": [
    [
     "Irene",
     "Jacobi"
    ],
    [
     "Louis C. W.",
     "Pols"
    ],
    [
     "Jan",
     "Stroop"
    ]
   ],
   "title": "Measuring and comparing vowel qualities in a Dutch spontaneous speech corpus",
   "original": "i06_1215",
   "page_count": 4,
   "order": 244,
   "p1": "paper 1215-Mon3FoP.3",
   "pn": "",
   "abstract": [
    "Recent studies of spoken Standard Dutch support an ongoing change in the phonetic quality of the diphthong /EI/ [1, 2]. However, there is a need for broader analyses and larger data sets. Here, we took Dutch vowel variants of 44 speakers from a spoken Dutch speech corpus, the CGN [3]. The vowels were measured and compared on the basis of 15.000 vowel segments, consisting of productions of /EI/, /Au/, /2y/, /o:/, and /e:/, as well as the anchor vowels /a/, /i/, /u/. It was our aim to analyze changes in vowel quality dependent on the speakers sociological backgrounds and ages, and to deal with the variable recording qualities of the corpus. All vowels were taken from spontaneously uttered sentences and were analyzed automatically by means of a principal component analysis (PCA) on the vowels bark-filtered spectra, as well as by formant analysis.\n",
    "Recalculating spectral positions in the principal components (pcs) plane displayed the spectral interaction of the first formants in the pc1-pc2 plane, and explained the better separability of the vowels compared to the F1-F2 plane, as well as the high correlation of the first three formants with pc1 and pc2. The first pcs turned out to be rather insensitive to sex-differences, but they were sensitive to the signal-to-noise ratio of the speech data. Variable recording qualities manifested themselves in speaker-specific location and size of the vowel spaces. Good signal-to-noise ratios could be transformed to poorer signals by increasing the lowest possible dB values per filter. Having analyzed the influence of noise on our data, we could normalize the data by taking each speakers /a-i-u/ positions and the focal point as references for better inter-speaker comparison.\n",
    "The results clearly show different vowel quality patterns dependent on the speakers education and age, and indicate a progress of quality changes with as parameters the lowering and the degree of diphthongization of the long vowels and diphthongs.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-244"
  },
  "li06e_interspeech": {
   "authors": [
    [
     "Aijun",
     "Li"
    ],
    [
     "Qiang",
     "Fang"
    ],
    [
     "Ziyu",
     "Xiong"
    ]
   ],
   "title": "Phonetic research on accented Chinese in three dialectal regions: Shanghai, Wuhan and Xiamen",
   "original": "i06_1143",
   "page_count": 4,
   "order": 245,
   "p1": "paper 1143-Mon3FoP.4",
   "pn": "",
   "abstract": [
    "There are 10 major dialects in China. Most people in dialectal regions are bilingual speakers, i.e. native dialect and Mandarin. Although lots of people can speak Mandarin, they speak it with different accents (called regional accented Chinese in this paper) depending on how well they grasp the language. In this study, we categorize the regional accented Chinese into 3 levels of accents according to phonetic annotation and subjective evaluation on a regional accented speech corpus of three regions: Shanghai, Wuhan and Xiamen. Three accent evaluation methods, namely segmental annotation, clustering on phonetic annotation and subjective evaluation, are compared based on phonetic error rates. The results show that objective evaluation score based on segmental pronunciation is higher than subjective evaluation for the same speaker. This implies that supra-segmental features play an important role in rating accent degree and segmental features alone are not enough for objective evaluation. In accent level criterion, the errors from prosodic and segmental aspects are not equal and the percentage of these two parts are various for different regional speakers. The result is helpful for machine evaluation, L2 teaching and acquisition.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-245"
  },
  "zhang06d_interspeech": {
   "authors": [
    [
     "Chi",
     "Zhang"
    ],
    [
     "Ji",
     "Wu"
    ],
    [
     "Xi",
     "Xiao"
    ],
    [
     "Zuoying",
     "Wang"
    ]
   ],
   "title": "Pronunciation variation modeling for Mandarin with accent",
   "original": "i06_1849",
   "page_count": 4,
   "order": 246,
   "p1": "paper 1849-Mon3FoP.5",
   "pn": "",
   "abstract": [
    "In order to solve the problem of the performance decrease when state-of-art automatic speech recognition (ASR) system facing accent speech, we propose the Pronunciation Variation Model (PVM). Two approaches are proposed to construct the PVM in this paper. 6.38% and 7.78% relative error rate reduction is achieved for Shanghai and Wuhan accent mandarin, respectively. The experiment on these two typical accent mandarin shows it is a possible way to deal with accent speech.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-246"
  },
  "nielsen06_interspeech": {
   "authors": [
    [
     "Kuniko Y.",
     "Nielsen"
    ]
   ],
   "title": "Specificity and generalizability of spontaneous phonetic imitation",
   "original": "i06_1326",
   "page_count": 4,
   "order": 247,
   "p1": "paper 1326-Mon3FoP.6",
   "pn": "",
   "abstract": [
    "The imitation paradigm [1, 2], in which subjects speech is compared before and after they are exposed to target speech, has shown that subjects shift their production in the direction of the target, indicating the use of episodic traces in speech perception as well as the close tie between speech perception and production. By using this paradigm, the current study aims to investigate the psychological reality of three levels of linguistic unit (i.e., word, phoneme, and feature). An experiment was designed to test whether spontaneous phonetic imitation can be generalized from words across (a) new words which share the same initial phoneme, and (b) new words with a new phoneme falling in the same natural class (sharing a feature); and also whether word-level specificity can be obtained through physical measurements instead of perceptual assessments. The feature manipulated in the experiment was aspiration, or [+spread glottis], on the phonemes /p/ and /k/.\n",
    "The results showed that subjects produced significantly longer VOTs after they were exposed to target speech with longer VOTs, replicating [2] in a non-shadowing paradigm. Furthermore, the modeled feature (increased aspiration) was generalized to new instances of /p/ (i.e., in new words) as well as to the new segment /k/. At the same time, subjects post-exposure VOT was significantly longer for those items that were in the target speech than items which they had not previously heard. These results, taken together, indicate that speakers possess both sub-phonemic and word-level representations.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-247"
  },
  "bael06b_interspeech": {
   "authors": [
    [
     "Christophe Van",
     "Bael"
    ],
    [
     "Hans van",
     "Halteren"
    ]
   ],
   "title": "On the sufficiency of automatic phonetic transcriptions for pronunciation variation research",
   "original": "i06_1265",
   "page_count": 4,
   "order": 248,
   "p1": "paper 1265-Mon3FoP.7",
   "pn": "",
   "abstract": [
    "We investigated whether automatic phonetic transcriptions (APTs) can replace manually verified phonetic transcriptions (MPTs) in a large corpus-based study on pronunciation variation. To this end, we compared the performance of both transcription types in a classification experiment aimed at establishing the direct influence of a particular situational setting on pronunciation variation. We trained classifiers on the speech processes extracted from the alignments of an APT and an MPT with a canonical transcription. We tested whether the classifiers were equally good at verifying whether unknown transcriptions represent read speech or telephone dialogues, and whether the same speech processes were identified to distinguish between transcriptions of the two situational settings. Our results not only show that similar distinguishing speech processes were identified; our APT-based classifier yielded better classification accuracy than the MPT-based classifier whilst using fewer classification features.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-248"
  },
  "kazemzadeh06_interspeech": {
   "authors": [
    [
     "Abe",
     "Kazemzadeh"
    ],
    [
     "Joseph",
     "Tepperman"
    ],
    [
     "Jorge",
     "Silva"
    ],
    [
     "Hong",
     "You"
    ],
    [
     "Sungbok",
     "Lee"
    ],
    [
     "Abeer",
     "Alwan"
    ],
    [
     "Shrikanth",
     "Narayanan"
    ]
   ],
   "title": "Automatic detection of voice onset time contrasts for use in pronunciation assessment",
   "original": "i06_1884",
   "page_count": 4,
   "order": 249,
   "p1": "paper 1884-Mon3FoP.8",
   "pn": "",
   "abstract": [
    "This study examines methods for recognizing different classes of phones from accented speech based on voice onset time (VOT). These methods are tested on data from the Tball corpus of Los Angeles-area elementary school children [1]. The methods proposed and tested are: 1) to train models based on standard English VOT contrasts and then extract the VOT characteristics of the phones by measuring the duration of phone-level and sub-phone-level alignments, 2) to train phone models with explicit aspiration, and 3) to train different models for different phoneme classes of VOT times. Error rates of 23-53% for different phone classes are reported for the first method, 5-57% for the second method, and 0-36% for the third. The results show that different methods work better on different phone classes. We interpret these results in relation to past research on VOT, explain possible uses for these findings, and propose directions for future research.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-249"
  },
  "hirano06_interspeech": {
   "authors": [
    [
     "Hiroko",
     "Hirano"
    ],
    [
     "Goh",
     "Kawai"
    ],
    [
     "Keikichi",
     "Hirose"
    ],
    [
     "Nobuaki",
     "Minematsu"
    ]
   ],
   "title": "Unfilled pauses in Japanese sentences read aloud by non-native learners",
   "original": "i06_1871",
   "page_count": 4,
   "order": 250,
   "p1": "paper 1871-Mon3FoP.9",
   "pn": "",
   "abstract": [
    "Perception experiments suggest that natives judge non-native unfilled pauses as indiscriminate and indecisive. Multiple regression analyses of unfilled pauses indicate a connection between syntactic structure and pause location and duration. Native speakers uniformly pause at large syntactic breaks with marked duration, whereas non-natives unfilled pauses are spread over various locations, possibly reflecting limited syntactic planning. Our method might be used to synthesize appropriate unfilled pauses in text-to-speech systems, and to train pausing behavior in automated pronunciation learning systems for nonnative learners.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-250"
  },
  "hamabe06_interspeech": {
   "authors": [
    [
     "Ryoji",
     "Hamabe"
    ],
    [
     "Kiyotaka",
     "Uchimoto"
    ],
    [
     "Tatsuya",
     "Kawahara"
    ],
    [
     "Hitoshi",
     "Isahara"
    ]
   ],
   "title": "Detection of quotations and inserted clauses and its application to dependency structure analysis in spontaneous Japanese",
   "original": "i06_1151",
   "page_count": 4,
   "order": 251,
   "p1": "paper 1151-Mon3FoP.10",
   "pn": "",
   "abstract": [
    "Japanese dependency structure is usually represented by relationships between phrasal units called bunsetsus. One of the biggest problems with dependency structure analysis in spontaneous speech is that clause boundaries are ambiguous. This paper describes a method for detecting the boundaries of quotations and inserted clauses and that for improving the dependency accuracy by applying the detected boundaries to dependency structure analysis. The quotations and inserted clauses are determined by using an SVM-based text chunking method that considers information on morphemes, pauses, fillers, etc. The information on automatically analyzed dependency structure is also used to detect the beginning of the clauses. Our evaluation experiment using Corpus of Spontaneous Japanese (CSJ) showed that the automatically estimated boundaries of quotations and inserted clauses helped to improve the accuracy of dependency structure analysis.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-251"
  },
  "tseng06_interspeech": {
   "authors": [
    [
     "Chun-Han",
     "Tseng"
    ],
    [
     "Chia-Ping",
     "Chen"
    ]
   ],
   "title": "Chinese input method based on reduced Mandarin phonetic alphabet",
   "original": "i06_1944",
   "page_count": 4,
   "order": 252,
   "p1": "paper 1944-Mon3FoP.11",
   "pn": "",
   "abstract": [
    "In this paper we study the problem of simplifying Chinese input method and making it suitable for use with mobile devices. To see the feasibility of aggressively reducing the number of keystrokes per Chinese character, we compare three input modes: character-based, syllable-based and first-symbol-based. Specifically, we use these linguistic units as token types and compare the perplexities. With the language model trained by data based on the ASBC corpus, the perplexity of the data set we collect from on-line chat and instant messages is 102.6 for character-based model, 67.7 for syllable-based model and 16.3 for first-symbol-based model. Arguing from the relation between the perplexity and the number of \"typical\" sentences of a language model, our conclusion is that on average there are 6 to 7 characters per first-symbol in natural Chinese language.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-252"
  },
  "suzuki06_interspeech": {
   "authors": [
    [
     "Yoshimi",
     "Suzuki"
    ],
    [
     "Fumiyo",
     "Fukumoto"
    ]
   ],
   "title": "Thesaurus expansion using similar word pairs from patent documents",
   "original": "i06_1920",
   "page_count": 4,
   "order": 253,
   "p1": "paper 1920-Mon3FoP.12",
   "pn": "",
   "abstract": [
    "In both written and spoken languages, we sometimes use different words in order to describe the same meaning. For instance, we use \"constraint\" (seigen) and \"restriction\" (seiyaku) as the same meaning. This makes text classification and text summarization difficult. In order to deal with this problem, dictionaries especially thesauri are used. However, in technical paper and patent documents, a lot of new words which are not given in the dictionary. In this paper, we propose a method to accurately extract words which are semantically similar to each other. Using this method, we extracted similar word pairs from patent documents. We also expand a thesaurus using the extracted similar words.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-253"
  },
  "schone06_interspeech": {
   "authors": [
    [
     "Patrick",
     "Schone"
    ]
   ],
   "title": "Low-resource autodiacritization of abjads for speech keyword search",
   "original": "i06_1412",
   "page_count": 4,
   "order": 254,
   "p1": "paper 1412-Mon3FoP.13",
   "pn": "",
   "abstract": [
    "Keyword search in speech requires retrieval systems to know the pronunciation of keywords. Many languages of the world are either largely alphabetic or have pronouncing dictionaries so that deducing pronunciations at run-time is manageable. There are many under-resourced languages, though, with writing systems where only some of the vowels are represented in the orthography (i.e., \"abjads\"). The absence of vowels makes direct mapping of abjads to pronunciation non-trivial. We describe an automatic system for inferring pronunciations from abjadic languages which seamlessly integrates into an existing contextsensitive pronunciation generator that serves a language-universal keyword search system. We also identify Web resources and system performance for each of five abjadic languages: Arabic, Farsi, Hebrew, Pashto, and Urdu. We show that almost effortlessly, the system can learn new rules which increase pronunciation accuracies by as much as 31.2% relative.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-254"
  },
  "hertz06_interspeech": {
   "authors": [
    [
     "Susan R.",
     "Hertz"
    ]
   ],
   "title": "A model of the regularities underlying speaker variation: evidence from hybrid synthesis",
   "original": "i06_1286",
   "page_count": 4,
   "order": 255,
   "p1": "paper 1286-Tue3A3O.1",
   "pn": "",
   "abstract": [
    "This paper presents the framework of a speech model, tentatively called the \"hybrid model,\" which offers an explanation of how listeners can identify phonemes in an incoming speech signal despite the vast amount of cross-speaker and contextual variation. Fundamental to the model are two basic speech units into which listeners process the incoming speech stream: acoustic consonant clusters and acoustic nuclei. Acoustic nuclei are responsible for speaker identity, but acoustic consonant clusters are more generic and can even be substituted across speakers with negligible impact on speech quality. The paper focuses on acoustic consonant clusters, showing that much of the variability in them is perceptually irrelevant, and how the hybrid model accounts for listeners ability to parse them into phonemes. The paper supports the model as applied to English by drawing on experiments in hybrid synthesis, a technique in which speech is produced by splicing together segments from different speakers [1].\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-255"
  },
  "speyer06_interspeech": {
   "authors": [
    [
     "Augustin",
     "Speyer"
    ]
   ],
   "title": "Pauses as a tool to ensure rhythmic wellformedness",
   "original": "i06_1406",
   "page_count": 3,
   "order": 256,
   "p1": "paper 1406-Tue3A3O.2",
   "pn": "",
   "abstract": [
    "Rhythmic wellformedness on the level of syllables and words and the mechanisms which are employed to ensure it are well known and researched on. The level of sentential accent is less often in the focus of such studies. In this paper I argue that rhythmic wellformedness plays an equally important role on the sentential level and that the preferred strategy to ensure it is the insertion of a pause (or lengthening of an already present pause), rather than the classical strategies of stress shift and destressing. This has to do with the role that focal accent plays for the semantic interpretation of an utterance. A reading experiment showed clearly that in clash cases the distance between the words in clash is increased by a pause; as we get the same effect in words in which a number of unstressed syllables intervenes between the clashing focal accents, it is clear that it is not simply stress clash resolution but resolution of a clash on a higher level of representation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-256"
  },
  "watanabe06_interspeech": {
   "authors": [
    [
     "Michiko",
     "Watanabe"
    ],
    [
     "Yasuharu",
     "Den"
    ],
    [
     "Keikichi",
     "Hirose"
    ],
    [
     "Shusaku",
     "Miwa"
    ],
    [
     "Nobuaki",
     "Minematsu"
    ]
   ],
   "title": "Factors affecting speakers² choice of fillers in Japanese presentations",
   "original": "i06_1498",
   "page_count": 4,
   "order": 257,
   "p1": "paper 1498-Tue3A3O.3",
   "pn": "",
   "abstract": [
    "Disfluencies are intrinsic in spontaneous speech. Although it is known that there is a wide range of frequencies and types of disfluencies among speech, little is known about factors affecting speakers choice of disfluency types. We first conducted a correspondence analysis using ratios of seven types of fillers, and other disfluencies, in 174 presentations and quantified the data. We conducted a cluster analysis using 4 dimension scores from the correspondence analysis, and extracted five filler-type groups. We then examined frequent types of presentations (formal or casual) and speaker attributes (gender and age) in each group. The results indicate that speakers choice of filler types is affected by speech levels, speakers gender and age, and that relevant factors differ depending on the type of fillers.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-257"
  },
  "davel06_interspeech": {
   "authors": [
    [
     "Marelie",
     "Davel"
    ],
    [
     "Etienne",
     "Barnard"
    ]
   ],
   "title": "Developing consistent pronunciation models for phonemic variants",
   "original": "i06_1760",
   "page_count": 4,
   "order": 258,
   "p1": "paper 1760-Tue3A3O.4",
   "pn": "",
   "abstract": [
    "Pronunciation lexicons often contain pronunciation variants. This can create two problems: It can be difficult to define these variants in an internally consistent way and it can also be difficult to extract generalised grapheme-to-phoneme rule sets from a lexicon containing variants. In this paper we address both these issues by creating pseudo-phonemes associated with sets of generation restriction rules to model those pronunciations that are consistently realised as two or more variants. By pre-processing and post-processing the lexicon appropriately, grapheme-to-phoneme algorithms that were not able to deal with pronunciation variants previously can now be extended to incorporate variants easily, without requiring changes to the standard algorithms. We evaluate the effectiveness of this approach using the Default&Refine rule extraction algorithm, and apply the method to both the English Oxford Advanced Learners Dictionary (OALD) and the Flemish FONILEX pronunciation lexicon. We find that the approach generalises to different languages, is able to model phonemic variation accurately and is able to identify inconsistent variants in pre-existing lexicons.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-258"
  },
  "lee06b_interspeech": {
   "authors": [
    [
     "Jinsik",
     "Lee"
    ],
    [
     "Seungwon",
     "Kim"
    ],
    [
     "Gary Geunbae",
     "Lee"
    ]
   ],
   "title": "Grapheme-to-phoneme conversion using automatically extracted associative rules for Korean TTS system",
   "original": "i06_1405",
   "page_count": 4,
   "order": 259,
   "p1": "paper 1405-Tue3A3O.5",
   "pn": "",
   "abstract": [
    "In this paper, we describe a method for automatically extracting grapheme-to-phoneme conversion rules directly from the transcription of speech synthesis database and introduce a weighted score and jamo similarity to overcome the rule application difficulties. We make a structured rule tree by rule pruning and rule association, and can eliminate most of the rules with almost no decrease of the performance. Our system achieves over 99.5 percent of phoneme-level accuracy and this performance is easily achievable even with the small amount of training data.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-259"
  },
  "charoenpornsawat06_interspeech": {
   "authors": [
    [
     "Paisarn",
     "Charoenpornsawat"
    ],
    [
     "Tanja",
     "Schultz"
    ]
   ],
   "title": "Example-based grapheme-to-phoneme conversion for Thai",
   "original": "i06_1782",
   "page_count": 4,
   "order": 260,
   "p1": "paper 1782-Tue3A3O.6",
   "pn": "",
   "abstract": [
    "Several characteristics of the Thai writing system make Thai graphemeto- phoneme (G2P) conversion very challenging. In this paper, we propose an Example-Based Grapheme-to-Phoneme conversion approach. It generates the pronunciation of a word by selecting, modifying and combining pronunciations from syllables from training corpus. The best system achieves 80.99% word accuracy and 94.19% phone accuracy which significantly outperform previous approaches for Thai.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-260"
  },
  "riesa06_interspeech": {
   "authors": [
    [
     "Jason",
     "Riesa"
    ],
    [
     "Behrang",
     "Mohit"
    ],
    [
     "Kevin",
     "Knight"
    ],
    [
     "Daniel",
     "Marcu"
    ]
   ],
   "title": "Building an English-iraqi Arabic machine translation system for spoken utterances with limited resources",
   "original": "i06_2012",
   "page_count": 4,
   "order": 261,
   "p1": "paper 2012-Tue1A1O.1",
   "pn": "",
   "abstract": [
    "This paper presents an English-Iraqi Arabic speech-to-speech statistical machine translation system using limited resources. In it, we explore the constraints involved, how we endeavored to mitigate such problems as a non-standard orthography and a highly inflected grammar, and discuss leveraging existing plentiful resources for Modern Standard Arabic to assist in this task. These combined techniques yield a reduction in unknown words at translation time by over 40% and a +3.65 increase in BLEU score over a previous state-of-the-art system using the same parallel training corpus of spoken utterances.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-261"
  },
  "maskey06_interspeech": {
   "authors": [
    [
     "Sameer",
     "Maskey"
    ],
    [
     "Bowen",
     "Zhou"
    ],
    [
     "Yuqing",
     "Gao"
    ]
   ],
   "title": "A phrase-level machine translation approach for disfluency detection using weighted finite state transducers",
   "original": "i06_1886",
   "page_count": 4,
   "order": 262,
   "p1": "paper 1886-Tue1A1O.2",
   "pn": "",
   "abstract": [
    "We propose a novel algorithm to detect disfluency in speech by reformulating the problem as phrase-level statistical machine translation using weighted finite state transducers. We approach the task as translation of noisy speech to clean speech. We simplify our translation framework such that it does not require fertility and alignment models. We tested our model on the Switchboard disfluency-annotated corpus. Using an optimized decoder that is developed for phrasebased translation at IBM, we are able to detect repeats, repairs and filled pauses for more than a thousand sentences in less than a second with encouraging results.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-262"
  },
  "lee06c_interspeech": {
   "authors": [
    [
     "Jonghoon",
     "Lee"
    ],
    [
     "Donghyeon",
     "Lee"
    ],
    [
     "Gary Geunbae",
     "Lee"
    ]
   ],
   "title": "Improving phrase-based Korean-English statistical machine translation",
   "original": "i06_1371",
   "page_count": 4,
   "order": 263,
   "p1": "paper 1371-Tue1A1O.3",
   "pn": "",
   "abstract": [
    "In this paper, we describe several techniques to improve Korean- English statistical machine translation. We have built a phrase-based statistical machine translation system in a travel domain. On the baseline phrase-based system, several techniques are applied to improve the translation quality. Each technique can be applied or removed easily since the techniques are part of the preprocessing method or corpus processing method. Our experiments show that most of the techniques were successful except reordering the word sequence. The combination of the successful techniques has significantly improved the translation quality.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-263"
  },
  "stallard06_interspeech": {
   "authors": [
    [
     "David",
     "Stallard"
    ],
    [
     "Fred",
     "Choi"
    ],
    [
     "Kriste",
     "Krstovski"
    ],
    [
     "Prem",
     "Natarajan"
    ],
    [
     "Rohit",
     "Prasad"
    ],
    [
     "Shirin",
     "Saleem"
    ]
   ],
   "title": "A hybrid phrase-based/statistical speech translation system",
   "original": "i06_1732",
   "page_count": 4,
   "order": 264,
   "p1": "paper 1732-Tue1A1O.4",
   "pn": "",
   "abstract": [
    "Spoken communication across a language barrier is of increasing importance in both civilian and military applications. In this paper, we present a system for task-directed 2-way communication between speakers of English and Iraqi colloquial Arabic. The application domain of the system is force protection. The system supports translingual dialogue in areas that include municipal services surveys, detainee screening, and descriptions of people, houses, vehicles, etc. N-gram speech recognition is used to recognize both English and Arabic speech. The system uses a combination of a pre-recorded questions and statistical machine translation with speech synthesis to translate the recognition output.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-264"
  },
  "wang06b_interspeech": {
   "authors": [
    [
     "Chao",
     "Wang"
    ],
    [
     "Stephanie",
     "Seneff"
    ]
   ],
   "title": "High-quality speech translation in the flight domain",
   "original": "i06_1135",
   "page_count": 4,
   "order": 265,
   "p1": "paper 1135-Tue1A1O.5",
   "pn": "",
   "abstract": [
    "Portability is an important issue to the viability of a domain-specific translation approach. This paper describes an English to Chinese translation system for flight-domain queries, utilizing an interlingua translation framework that has been successfully applied in the weather domain. Portability of various components is tested, and new technologies to handle parse ambiguities and ill-formed inputs are developed to enhance the translation framework. Evaluation of translation quality is conducted manually on a set of 432 unseen flight-domain utterances, which are translated into Chinese using a formal method and a new robust back-off method in tandem. We achieved 96.7% sentence accuracy with a rejection rate of 7.6% on manual transcripts, and 89.1% accuracy with an 8.6% rejection rate on speech input. A game for language learning using the translation capability is currently under development.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-265"
  },
  "hsiao06_interspeech": {
   "authors": [
    [
     "Roger",
     "Hsiao"
    ],
    [
     "Ashish",
     "Venugopal"
    ],
    [
     "Thilo",
     "Köhler"
    ],
    [
     "Ying",
     "Zhang"
    ],
    [
     "Paisarn",
     "Charoenpornsawat"
    ],
    [
     "Andreas",
     "Zollmann"
    ],
    [
     "Stephan",
     "Vogel"
    ],
    [
     "Alan W.",
     "Black"
    ],
    [
     "Tanja",
     "Schultz"
    ],
    [
     "Alex",
     "Waibel"
    ]
   ],
   "title": "Optimizing components for handheld two-way speech translation for an English-iraqi Arabic system",
   "original": "i06_1712",
   "page_count": 4,
   "order": 266,
   "p1": "paper 1712-Tue1A1O.6",
   "pn": "",
   "abstract": [
    "This paper described our handheld two-way speech translation system for English and Iraqi. The focus is on developing a field usable handheld device for speech-to-speech translation. The computation and memory limitations on the handheld impose critical constraints on the ASR, SMT, and TTS components. In this paper we discuss our approaches to optimize these components for the handheld device and present performance numbers from the evaluations that were an integral part of the project. Since one major aspect of the TransTac program is to build fieldable systems, we spent significant effort on developing an intuitive interface that minimizes the training time for users but also provides useful information such as back translations for translation quality feedback.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-266"
  },
  "sehr06_interspeech": {
   "authors": [
    [
     "Armin",
     "Sehr"
    ],
    [
     "Marcus",
     "Zeller"
    ],
    [
     "Walter",
     "Kellermann"
    ]
   ],
   "title": "Distant-talking continuous speech recognition based on a novel reverberation model in the feature domain",
   "original": "i06_1111",
   "page_count": 4,
   "order": 267,
   "p1": "paper 1111-Tue1A2O.1",
   "pn": "",
   "abstract": [
    "A novel approach for automatic speech recognition in highly reverberant environments, proposed in [1] for isolated word recognition, is extended to continuous speech recognition (CSR) in this paper. The approach is based on a combined acoustic model consisting of a network of clean speech HMMs and a reverberation model. Because the grammatical information and the information about the acoustic environment are strictly separated in the combined model, a high degree of flexibility for adapting the system to new tasks and new environments is attained. We show that virtually all known CSR search algorithms can be used for decoding the proposed combined model if a few extensions are added. In a simulation of a connected digit recognition task, the proposed method achieves more than 40% reduction of the word error rate compared to a conventional HMM-based system trained on reverberant speech, at the cost of an increased decoding complexity.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-267"
  },
  "lei06b_interspeech": {
   "authors": [
    [
     "Xin",
     "Lei"
    ],
    [
     "Jon",
     "Hamaker"
    ],
    [
     "Xiaodong",
     "He"
    ]
   ],
   "title": "Robust feature space adaptation for telephony speech recognition",
   "original": "i06_1743",
   "page_count": 4,
   "order": 268,
   "p1": "paper 1743-Tue1A2O.2",
   "pn": "",
   "abstract": [
    "Speaker adaptation is critical for modern speech recognition systems. Due to the computational and multi-channel model sharing considerations, the use of model adaptation techniques is limited in telephony speech recognition systems. On the other hand, feature space adaptation methods such as feature space maximum likelihood linear regression (fMLLR) are efficient approaches suitable for telephony systems. In this work, we first describe techniques for efficient implementation of online fMLLR adaptation. Then feature space maximum a posteriori linear regression (fMAPLR) is proposed to incorporate prior knowledge for the feature transform estimation and improve the robustness of the conventional fMLLR approach. Experiments on telephony data indicate that fMAPLR is significantly more robust than fMLLR, and outperforms fMLLR especially when the adaptation data is very limited.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-268"
  },
  "thatphithakkul06_interspeech": {
   "authors": [
    [
     "Nattanun",
     "Thatphithakkul"
    ],
    [
     "Boontee",
     "Kruatrachue"
    ],
    [
     "Chai",
     "Wutiwiwatchai"
    ],
    [
     "Sanparith",
     "Marukatat"
    ],
    [
     "Vataya",
     "Boonpiam"
    ]
   ],
   "title": "A simulated-data adaptation technique for robust speech recognition",
   "original": "i06_1157",
   "page_count": 4,
   "order": 269,
   "p1": "paper 1157-Tue1A2O.3",
   "pn": "",
   "abstract": [
    "This paper proposes an efficient acoustic model adaptation method based on the use of simulated-data in maximum likelihood linear regression (MLLR) adaptation for robust speech recognition. Online MLLR adaptation is an unsupervised process which requires an input speech with phone labels transcribed automatically. Instead of using only the input signal in adaptation, our proposed simulated data method increases the size of adaptation data by adding noise portions extracted from the input speech to a set of pre-recorded clean speech, whose correct transcriptions are known. Various configurations of the proposed method are explored. Evaluations are performed with both additive and real noisy speech. The experimental results show that the proposed system achieves higher recognition rate than the system using only the input speech in adaptation and the system using a multi-conditioned acoustic model.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-269"
  },
  "hirsch06_interspeech": {
   "authors": [
    [
     "Hans-Günter",
     "Hirsch"
    ],
    [
     "Harald",
     "Finster"
    ]
   ],
   "title": "A new HMM adaptation approach for the case of a hands-free speech input in reverberant rooms",
   "original": "i06_1175",
   "page_count": 4,
   "order": 270,
   "p1": "paper 1175-Tue1A2O.4",
   "pn": "",
   "abstract": [
    "A new method is presented for adapting the HMMs of a speech recognition system to the condition of a hands-free speech input in a room environment. The reverberation in a room usually has a bad effect on the performance of a recognition system. Reverberation causes an artificial extension of acoustic excitations what gets visible as so called reverberation tail when looking at the envelope of the short-term energy over the whole frequency range or in subbands. The approach is based on the assumption that the acoustic excitation of a speech segment, as modeled by an HMM state, will be seen as attenuated versions at successive HMM states. Adding this attenuated excitations in the spectral domain at each HMM state leads to a considerable improvement of the recognition performance. Furthermore a new approach is presented to adapt the Delta parameters that are usually taken as additional acoustic features. The efficiency of both new techniques has been proved by some experiments on isolated and connected word recognition with the TIDigits speech data base.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-270"
  },
  "tsao06_interspeech": {
   "authors": [
    [
     "Yu",
     "Tsao"
    ],
    [
     "Chin-Hui",
     "Lee"
    ]
   ],
   "title": "A vector space approach to environment modeling for robust speech recognition",
   "original": "i06_1617",
   "page_count": 4,
   "order": 271,
   "p1": "paper 1617-Tue1A2O.5",
   "pn": "",
   "abstract": [
    "We propose a vector space approach to characterizing environments for robust speech recognition. We represent a given environment by a super-vector formed by concatenating all the mean vectors of the Gaussian mixture components of the state observation densities of all hidden Markov models trained in the particular environment. New environment super-vectors can now be obtained either by an interpolation method with a collection of super-vectors trained from many real or simulated environments or by a transformation performed on an anchor super-vector for a specific environment, such as a clean condition. At a 5dB signal-to-noise (SNR) level, both interpolationand transformation-based approaches achieve a significant error rate reduction of close to 47% from a baseline system with cepstral mean subtraction (CMS) with only two adaptation utterances. When incorporating N-best information to perform unsupervised adaptation at 5dB SNR with the same two utterances, we achieve a relative error reduction of about 40%, close to that achieved in the supervised mode.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-271"
  },
  "chien06_interspeech": {
   "authors": [
    [
     "Jen-Tzung",
     "Chien"
    ],
    [
     "Chuan-Wei",
     "Ting"
    ]
   ],
   "title": "Subspace modeling and selection for noisy speech recognition",
   "original": "i06_1333",
   "page_count": 4,
   "order": 272,
   "p1": "paper 1333-Tue1A2O.6",
   "pn": "",
   "abstract": [
    "This paper presents a new subspace modeling and selection approach for noisy speech recognition. In subspace modeling, we develop factor analysis (FA) for representing noisy speech. FA is a data generation model where the common factors are extracted with factor loading matrix and specific factors. We bridge the connection of FA to signal subspace (SS) approach. Interestingly, FA partitions noisy speech space into a principal subspace containing speech and noise and a minor subspace containing residual speech and residual noise. To estimate clean speech, we minimize the energies of speech distortion in principal subspace as well as minor subspace. More importantly, in subspace selection, we explore optimal subspace partition via solving hypothesis test problems. We test the equivalence of eigenvalues in minor subspace so as to determine subspace dimension. To fulfill FA spirit, we further examine the hypothesis of uncorrelated residual speech. Optimal solutions are realized through likelihood ratio test with the approximated chi-square distributions as test statistics. Subspace partition is performed according to the confidence towards rejecting null hypotheses. In the experiments on Aurora2 database, FA outperforms SS in subspace modeling. New selection algorithms effectively determine subspace dimension for noisy speech recognition.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-272"
  },
  "schuller06_interspeech": {
   "authors": [
    [
     "Björn",
     "Schuller"
    ],
    [
     "Niels",
     "Köhler"
    ],
    [
     "Ronald",
     "Müller"
    ],
    [
     "Gerhard",
     "Rigoll"
    ]
   ],
   "title": "Recognition of interest in human conversational speech",
   "original": "i06_1621",
   "page_count": 4,
   "order": 273,
   "p1": "paper 1621-Tue1A3O.1",
   "pn": "",
   "abstract": [
    "Recognition of interest of a speaker within a human dialog bears great potential in many commercial applications. Within this work we therefore introduce an approach that analyses acoustic and linguistic cues of a spoken utterance. A systematic generation of more than 5k hi-level features basing on prosodic and spectral feature contours by means of descriptive statistical analysis and subsequent feature space optimization is used to find relevant acoustic attributes. For linguistic information integration a bag-of-words representation is used relying on a speech recognizers output. One main aspect is the database of more than 2k spontaneous sub-speaker turns recorded and annotated for this analysis. Several influence factors as microphone distance and ASR versus annotation of spoken content are discussed. Overall remarkable performance of a running prototype can be reported discriminating between three levels of interest.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-273"
  },
  "ai06_interspeech": {
   "authors": [
    [
     "Hua",
     "Ai"
    ],
    [
     "Diane J.",
     "Litman"
    ],
    [
     "Kate",
     "Forbes-Riley"
    ],
    [
     "Mihai",
     "Rotaru"
    ],
    [
     "Joel",
     "Tetreault"
    ],
    [
     "Amruta",
     "Purandare"
    ]
   ],
   "title": "Using system and user performance features to improve emotion detection in spoken tutoring dialogs",
   "original": "i06_1682",
   "page_count": 4,
   "order": 274,
   "p1": "paper 1682-Tue1A3O.2",
   "pn": "",
   "abstract": [
    "In this study, we incorporate automatically obtained system/user performance features into machine learning experiments to detect student emotion in computer tutoring dialogs. Our results show a relative improvement of 2.7% on classification accuracy and 8.08% on Kappa over using standard lexical, prosodic, sequential, and identification features. This level of improvement is comparable to the performance improvement shown in previous studies by applying dialog acts or lexical-/prosodic-/discourse-level contextual features.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-274"
  },
  "devillers06_interspeech": {
   "authors": [
    [
     "Laurence",
     "Devillers"
    ],
    [
     "Laurence",
     "Vidrascu"
    ]
   ],
   "title": "Real-life emotions detection with lexical and paralinguistic cues on human-human call center dialogs",
   "original": "i06_1636",
   "page_count": 4,
   "order": 275,
   "p1": "paper 1636-Tue1A3O.3",
   "pn": "",
   "abstract": [
    "The emotion detection work reported here is part of a larger study aiming to model user behavior in real interactions. We already studied emotions in a real-life corpus with human-human dialogs on a financial task. We now make use of another corpus of real agent-caller spoken dialogs from a medical emergency call center in which emotion manifestations are much more complex, and extreme emotions are common. Our global aims are to define appropriate verbal labels for annotating real-life emotions, to annotate the dialogs, to validate the presence of emotions via perceptual tests and to find robust cues for emotion detection. Annotations have been done by two experts with twenty verbal classes organized in eight macro-classes. We retained for experiments in this paper four macro classes: Relief, Anger, Fear and Sadness. The relevant cues for detecting natural emotions are paralinguistic and linguistic. Two studies are reported in this paper: the first investigates automatic emotion detection using linguistic information, whereas the second investigates emotion detection with paralinguistic cues. On the medical corpus, preliminary experiments using lexical cues detect about 78% of the four labels showing very good detection for Relief (about 90%) and Fear (about 86%) emotions. Experiments using paralinguistic cues show about 60% of good detection, Fear being best detected.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-275"
  },
  "wilting06_interspeech": {
   "authors": [
    [
     "Janneke",
     "Wilting"
    ],
    [
     "Emiel",
     "Krahmer"
    ],
    [
     "Marc",
     "Swerts"
    ]
   ],
   "title": "Real vs. acted emotional speech",
   "original": "i06_1093",
   "page_count": 4,
   "order": 276,
   "p1": "paper 1093-Tue1A3O.4",
   "pn": "",
   "abstract": [
    "Even though the use of actors is a popular method for researching the expression of emotion, little is known about the relation between acted and real emotions. To shed some light on this, we set up a novel experiment, based on the Velten mood induction procedure, during which participants have to utter pre-defined sentences with a strong emotional content. In one group of participants, real positive or negative emotions were induced, while another group was instructed to act positive or negative while uttering Velten sentences. Results of a mood questionnaire revealed that participants in the real emotion condition, indeed felt positive or negative, depending on whether they read positive or negative sentences, while participants in the acted emotion condition felt neutral afterwards. In a second, perception experiment, it was found that acted emotions (especially negative ones) were perceived more strongly than the real emotions. This suggests that actors do not feel the acted emotion, and may engage in overacting, which casts doubt on the usefulness of actors as a way to study real emotions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-276"
  },
  "neiberg06_interspeech": {
   "authors": [
    [
     "Daniel",
     "Neiberg"
    ],
    [
     "Kjell",
     "Elenius"
    ],
    [
     "Kornel",
     "Laskowski"
    ]
   ],
   "title": "Emotion recognition in spontaneous speech using GMMs",
   "original": "i06_1581",
   "page_count": 4,
   "order": 277,
   "p1": "paper 1581-Tue1A3O.5",
   "pn": "",
   "abstract": [
    "Automatic detection of emotions has been evaluated using standard Mel-frequency Cepstral Coefficients, MFCCs, and a variant, MFCC-low, calculated between 20 and 300 Hz, in order to model pitch. Also plain pitch features have been used. These acoustic features have all been modeled by Gaussian mixture models, GMMs, on the frame level. The method has been tested on two different corpora and languages; Swedish voice controlled telephone services and English meetings. The results indicate that using GMMs on the frame level is a feasible technique for emotion classification. The two MFCC methods have similar performance, and MFCC-low outperforms the pitch features. Combining the three classifiers significantly improves performance.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-277"
  },
  "enos06_interspeech": {
   "authors": [
    [
     "Frank",
     "Enos"
    ],
    [
     "Stefan",
     "Benus"
    ],
    [
     "Robin L.",
     "Cautin"
    ],
    [
     "Martin",
     "Graciarena"
    ],
    [
     "Julia",
     "Hirschberg"
    ],
    [
     "Elizabeth",
     "Shriberg"
    ]
   ],
   "title": "Personality factors in human deception detection: comparing human to machine performance",
   "original": "i06_1664",
   "page_count": 4,
   "order": 278,
   "p1": "paper 1664-Tue1A3O.6",
   "pn": "",
   "abstract": [
    "Previous studies of human performance in deception detection have found that humans generally are quite poor at this task, comparing unfavorably even to the performance of automated procedures. However, different scenarios and speakers may be harder or easier to judge. In this paper we compare human to machine performance detecting deception on a single corpus, the Columbia-SRI-Colorado Corpus of deceptive speech. On average, our human judges scored worse than chance - and worse than current best machine learning performance on this corpus. However, not all judges scored poorly. Based on personality tests given before the task, we find that several personality factors appear to correlate with the ability of a judge to detect deception in speech.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-278"
  },
  "cleuren06_interspeech": {
   "authors": [
    [
     "Leen",
     "Cleuren"
    ],
    [
     "Jacques",
     "Duchateau"
    ],
    [
     "Alain",
     "Sips"
    ],
    [
     "Pol",
     "Ghesquière"
    ],
    [
     "Hugo",
     "Van hamme"
    ]
   ],
   "title": "Developing an automatic assessment tool for children²s oral reading",
   "original": "i06_1113",
   "page_count": 4,
   "order": 279,
   "p1": "paper 1113-Tue1WeS.1",
   "pn": "",
   "abstract": [
    "Automation of oral reading assessment and of feedback in a reading tutor is a very challenging task. This paper describes our research aiming at developing such automated systems. First topic is the recording and annotation of CHOREC, the Flemish database of childrens oral reading we develop in order to characterize oral reading processes statistically. Next, we propose a classification of both oral reading strategies and errors, which provides the basis of the envisaged assessment and feedback. Finally, experimental results show that our two-layered recognition system is able to provide high reading miscue detection rates, while only few correctly read words are erroneously tagged as miscue.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-279"
  },
  "waple06_interspeech": {
   "authors": [
    [
     "Christopher",
     "Waple"
    ],
    [
     "Yasushi",
     "Tsubota"
    ],
    [
     "Masatake",
     "Dantsuji"
    ],
    [
     "Tatsuya",
     "Kawahara"
    ]
   ],
   "title": "Prototyping a call system for students of Japanese using dynamic diagram generation and interactive hints",
   "original": "i06_1171",
   "page_count": 4,
   "order": 280,
   "p1": "paper 1171-Tue1WeS.2",
   "pn": "",
   "abstract": [
    "In this paper we discuss the concept and design of a new CALL (Computer Assisted Language Learning) system being developed to aid students learning Japanese as a second language. The system is being designed to allow students to create and speak their own sentences based on visual prompts, before receiving feedback on their mistakes. The students may choose to receive guidance in order to complete each task, selecting the level of help that best suits their needs. Having described the concept of the system and its design, we discuss some tests recently carried out using a prototype of the system, summarize the results obtained, and give some thought as to the significance of these results on the future development of the system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-280"
  },
  "massaro06_interspeech": {
   "authors": [
    [
     "Dominic W.",
     "Massaro"
    ],
    [
     "Ying",
     "Liu"
    ],
    [
     "Trevor H.",
     "Chen"
    ],
    [
     "Charles",
     "Perfetti"
    ]
   ],
   "title": "A multilingual embodied conversational agent for tutoring speech and language learning",
   "original": "i06_1313",
   "page_count": 4,
   "order": 281,
   "p1": "paper 1313-Tue1WeS.3",
   "pn": "",
   "abstract": [
    "Speech and language science and technology evolved under the assumption that speech was a solely auditory event. However, a burgeoning record of research findings reveals that our perception and understanding are influenced by a speakers face and accompanying gestures, as well as the actual sound of the speech. Perceivers expertly use these multiple sources of information to identify and interpret the language input. Given the value of face-to-face interaction, our persistent goal has been to develop, evaluate, and apply animated agents to produce realistic and accurate speech. Baldi is an accurate three-dimensional animated talking head appropriately aligned with either synthesized or natural speech. Baldi has a realistic tongue and palate, which can be shown by making his skin transparent. Based on this research and technology, we have implemented computer-assisted speech and language tutors for children with language challenges and for all persons learning a second language. Our language-training program utilizes Baldi (or his likeness) as the conversational agent, who guides students through a variety of exercises designed to teach vocabulary and grammar, to improve speech articulation, and to develop linguistic and phonological awareness. We have also implemented multilingual agents, using a client/server architecture system. This system has been used to develop Bao, a Mandarin talker, which has been used in an initial training study for college students learning Mandarin as a new language. The results address the potential for using visible speech technology and pedagogy in language learning of both similar segments in the two languages and new speech segments in the new language. Although visible speech did not facilitate pronunciation learning relative to just auditory speech, we expect that a more prolonged training period would show an advantage of visible speech. Some of the advantages of the Baldi pedagogy and technology include the popularity and proven effectiveness of computers and embodied conversational agents, the perpetual availability of the program, and individualized instruction. The science and technology of Baldi holds great promise in language learning, dialog, human-machine interaction, education, and edutainment.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-281"
  },
  "heilman06_interspeech": {
   "authors": [
    [
     "Michael",
     "Heilman"
    ],
    [
     "Kevyn",
     "Collins-Thompson"
    ],
    [
     "Jamie",
     "Callan"
    ],
    [
     "Maxine",
     "Eskenazi"
    ]
   ],
   "title": "Classroom success of an intelligent tutoring system for lexical practice and reading comprehension",
   "original": "i06_1325",
   "page_count": 4,
   "order": 282,
   "p1": "paper 1325-Tue1WeS.4",
   "pn": "",
   "abstract": [
    "We present an intelligent tutoring system called REAP that provides reader-specific lexical practice for improved reading comprehension. REAP offers individualized practice to students by presenting authentic and appropriate reading materials selected automatically from the web. We encountered a number of challenges that must be met in order for the system to be effective in a classroom setting. These include general challenges for a system that uses authentic materials, as well as more specific challenges that arise from integrating the system with pre-existing classroom curricula. We discuss how these challenges were met, and present evidence that REAP has gained acceptance into the classroom at the English Language Institute at the University of Pittsburgh.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-282"
  },
  "petersen06_interspeech": {
   "authors": [
    [
     "Sarah E.",
     "Petersen"
    ],
    [
     "Mari",
     "Ostendorf"
    ]
   ],
   "title": "Assessing the reading level of web pages",
   "original": "i06_1610",
   "page_count": 4,
   "order": 283,
   "p1": "paper 1610-Tue1WeS.5",
   "pn": "",
   "abstract": [
    "Reading is an important part of educational development. However, finding appropriate reading material for all students can be difficult and time consuming for teachers. Our goal is to automate the task of assessing the reading level of text to enable teachers to more effectively take advantage of the large amounts of text available today on the World Wide Web. Reading level assessment tools already exist for clean corpora such as books and magazine articles. This paper presents extensions of a particular set of tools to handle web pages returned by a standard search engine, including a step that pre-filters web pages to eliminate \"junk\" pages with little or no text. Results of applying the reading level detectors to web pages are manually evaluated by elementary school teachers, the intended audience for these tools. The tools work well for grades 4 and 5, with room for improvement in grades 2 and 3.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-283"
  },
  "mostow06_interspeech": {
   "authors": [
    [
     "Jack",
     "Mostow"
    ]
   ],
   "title": "Is ASR accurate enough for automated reading tutors, and how can we tell?",
   "original": "i06_1796",
   "page_count": 4,
   "order": 284,
   "p1": "paper 1796-Tue1WeS.6",
   "pn": "",
   "abstract": [
    "We discuss pros and cons of several ways to evaluate ASR accuracy in automated tutors that listen to students read aloud. Whether ASR is accurate enough for a particular reading tutor function depends on what ASR-based judgment it requires, the visibility of that judgment to students and teachers, and the amount of input speech on which it is based. How to tell depends on the purpose, criterion, and space of the evaluation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-284"
  },
  "tsurutani06_interspeech": {
   "authors": [
    [
     "Chiharu",
     "Tsurutani"
    ],
    [
     "Yutaka",
     "Yamauchi"
    ],
    [
     "Nobuaki",
     "Minematsu"
    ],
    [
     "Dean",
     "Luo"
    ],
    [
     "Kazutaka",
     "Maruyama"
    ],
    [
     "Keikichi",
     "Hirose"
    ]
   ],
   "title": "Development of a program for self assessment of Japanese pronunciation by English learners",
   "original": "i06_1805",
   "page_count": 4,
   "order": 285,
   "p1": "paper 1805-Tue1WeS.7",
   "pn": "",
   "abstract": [
    "A program for self assessment of Japanese pronunciation by Englishspeaking learners was developed using a language model built with input from a language teacher in collaboration with speech engineers. This collaboration enhanced the programs capacity for accurate assessment and provides practical support to users by linking evaluation with feedback, and an editorial function of error patterns. The program drew positive responses from participants in a trial run. This paper discusses the development of our language model, the function and evaluation of this self assessment program.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-285"
  },
  "tepperman06_interspeech": {
   "authors": [
    [
     "Joseph",
     "Tepperman"
    ],
    [
     "Jorge",
     "Silva"
    ],
    [
     "Abe",
     "Kazemzadeh"
    ],
    [
     "Hong",
     "You"
    ],
    [
     "Sungbok",
     "Lee"
    ],
    [
     "Abeer",
     "Alwan"
    ],
    [
     "Shrikanth",
     "Narayanan"
    ]
   ],
   "title": "Pronunciation verification of children²s speech for automatic literacy assessment",
   "original": "i06_1814",
   "page_count": 4,
   "order": 286,
   "p1": "paper 1814-Tue1WeS.8",
   "pn": "",
   "abstract": [
    "Arguably the most important part of automatically assessing a new readers literacy is in verifying his pronunciation of read-aloud target words. But the pronunciation evaluation task is especially difficult in children, non-native speakers, and pre-literates. Traditional likelihood ratio thresholding methods do not generalize easily, and even expert human evaluators do not always agree on what constitutes an acceptable pronunciation. We propose new recognition- and alignment-based features in a decision tree classification framework, along with the use of prior linguistic information and human perceptual evaluations. Our classification methods demonstrate a 91% agreement with the voted results of 20 human evaluators who agree among themselves 85% of the time.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-286"
  },
  "abdou06_interspeech": {
   "authors": [
    [
     "Sherif Mahdy",
     "Abdou"
    ],
    [
     "Salah Eldeen",
     "Hamid"
    ],
    [
     "Mohsen",
     "Rashwan"
    ],
    [
     "Abdurrahman",
     "Samir"
    ],
    [
     "Ossama",
     "Abdel-Hamid"
    ],
    [
     "Mostafa",
     "Shahin"
    ],
    [
     "Waleed",
     "Nazih"
    ]
   ],
   "title": "Computer aided pronunciation learning system using speech recognition techniques",
   "original": "i06_1888",
   "page_count": 4,
   "order": 287,
   "p1": "paper 1888-Tue1WeS.9",
   "pn": "",
   "abstract": [
    "This paper describes a speech-enabled Computer Aided Pronunciation Learning (CAPL) system HAFSS. This system was developed for teaching Arabic pronunciations to non-native speakers. A challenging application of HAFSS is teaching the correct recitation of the holy Quran. HAFSS uses a state of the art speech recognizer to detect errors in user recitation. To increase accuracy of the speech recognizer, only probable pronunciation variants, that cover all common types of recitation errors, are examined by the speech decoder. A module for the automatic generation of pronunciation hypotheses is built as a component of the system. A phoneme duration classification algorithm is implemented to detect recitation errors related to phoneme durations. The decision reached by the recognizer is accompanied by a confidence score to reduce effect of misleading system feedbacks to unpredictable speech inputs. Performance evaluation using a data set that includes 6.6% wrong speech segments showed that the system correctly identified the error in 62.4% of pronunciation errors, reported \"Repeat Request\" for 22.4% of the errors and made false acceptance of 14.9% of total errors.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-287"
  },
  "lobdell06_interspeech": {
   "authors": [
    [
     "Bryce",
     "Lobdell"
    ],
    [
     "Jont B.",
     "Allen"
    ]
   ],
   "title": "An information theoretic tool for investigating speech perception",
   "original": "i06_1209",
   "page_count": 4,
   "order": 288,
   "p1": "paper 1209-Tue1BuP.1",
   "pn": "",
   "abstract": [
    "A method for investigating human speech perception that combines information about human perception of speech, auditory modeling, signal detection theory, and information theory is described. For that purpose, a model for detection of signals in the auditory nerve is developed and used to analyze speech sounds. Examples are given that combine detectability information with P(heard = h |spoken = s) to access information about \"cues\" to some some speech sounds. These example agree qualitatively with previous results about perception of these sounds. Refinements and prospects for this approach are discussed in light of the examples.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-288"
  },
  "morrison06_interspeech": {
   "authors": [
    [
     "Geoffrey Stewart",
     "Morrison"
    ]
   ],
   "title": "An adaptive sampling procedure for speech perception experiments",
   "original": "i06_1147",
   "page_count": 4,
   "order": 289,
   "p1": "paper 1147-Tue1BuP.2",
   "pn": "",
   "abstract": [
    "Synthetic speech perception experiments may make use of several acoustic dimensions in order to adequately model listeners perception; however, the number of stimuli increases exponentially as dimensions are added. A relatively large number of identification responses per stimulus are needed in the vicinity of category boundaries in order to model the boundaries with reasonable accuracy. Fewer responses per stimulus are needed to model portions of the stimulus space where a single response category predominates. Rather than collecting the same number of responses for each stimulus, an experiment can therefore be shortened via adaptive sampling. An adaptive sampling procedure is described. After an initial pass through the stimuli, the procedure uses a logistic regression model to select stimuli to resample in subsequent rounds. Results of simulations indicated that the number of trials in the experiment could be reduced by a third without substantially affecting the results.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-289"
  },
  "viswanathan06_interspeech": {
   "authors": [
    [
     "Navin",
     "Viswanathan"
    ],
    [
     "James S.",
     "Magnuson"
    ],
    [
     "Carol A.",
     "Fowler"
    ]
   ],
   "title": "Disentangling gestural and auditory contrast accounts of compensation for coarticulation",
   "original": "i06_2045",
   "page_count": 4,
   "order": 290,
   "p1": "paper 2045-Tue1BuP.3",
   "pn": "",
   "abstract": [
    "Compensation for coarticulation (CfC), a context effect in which the articulatory characteristics of one segment influence the perception of a neighboring segment [1], has been a matter of considerable debate between proponents of gestural [2] and auditory theories of speech perception [3]. We set out to distinguish the two accounts by using nonnative liquids (Tamil with American English listeners) that have distinct articulatory and acoustic characteristics from the native phoneme categories to which they are assimilated. We report three experiments that show that the auditory contrast account of CfC cannot explain compensatory effects with our non-native stimuli. We argue that these context effects reflect perceptual compensation for coarticulation, as predicted on a gestural account, but discuss problems for both theories.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-290"
  },
  "yip06_interspeech": {
   "authors": [
    [
     "Michael C. W.",
     "Yip"
    ]
   ],
   "title": "The role of positional probability in the segmentation of Cantonese speech",
   "original": "i06_1034",
   "page_count": 4,
   "order": 291,
   "p1": "paper 1034-Tue1BuP.4",
   "pn": "",
   "abstract": [
    "The present paper examines the question of whether native Cantonese listeners make use of probabilistic phonotactics information of words in the segmentation process of Cantonese continuous speech. Because some sounds appear more frequently at the beginning or ending of Cantonese syllables than the others, these kinds of probabilistic information of syllables may be likely to cue the locations of possible syllable boundaries in Cantonese continuous speech. A syllable-spotting experiment was conducted and the results indicated that native Cantonese listeners indeed made use of the positional probabilities of a syllable¡¯s onset but not for the case of a syllable¡¯s final in the segmentation process. Along with my previous study [1], I argue that probabilistic phonotactics is one useful source of information in Cantonese speech segmentation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-291"
  },
  "haque06_interspeech": {
   "authors": [
    [
     "Shahina",
     "Haque"
    ],
    [
     "Tomio",
     "Takara"
    ]
   ],
   "title": "Nasality perception of vowels in different language background",
   "original": "i06_1108",
   "page_count": 4,
   "order": 292,
   "p1": "paper 1108-Tue1BuP.5",
   "pn": "",
   "abstract": [
    "Nasality is a distinctive feature of Bangla vowels. In this paper, we describe our study on nasality perception of Bangla vowels by Bangla and Japanese listeners. We discuss an interesting result that Japanese listeners perceive most Bangla nasal /¡­i/ as Japanese non-nasal /u/. As the amount of nasalization of /¡­i/ was increased synthetically, perception of this vowel change was found to be more categorical for Japanese listeners than that of Bangla listeners¡¯ vowel perception.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-292"
  },
  "hodoshima06_interspeech": {
   "authors": [
    [
     "Nao",
     "Hodoshima"
    ],
    [
     "Dawn",
     "Behne"
    ],
    [
     "Takayuki",
     "Arai"
    ]
   ],
   "title": "Steady-state suppression in reverberation: a comparison of native and nonnative speech perception",
   "original": "i06_1819",
   "page_count": 4,
   "order": 293,
   "p1": "paper 1819-Tue1BuP.6",
   "pn": "",
   "abstract": [
    "This study investigated whether the steady-state suppression method proposed by Arai et al. (2001, 2002) improved consonant identification for nonnative listeners in reverberation. It also compared the effect of steady-state suppression on consonant identification by native and nonnative listeners in reverberation. We used steady-state suppression as a preprocessing technique which processes speech signals before they are radiated from loudspeakers in order to reduce the amount of overlap-masking. Participants were 24 native English (native listeners) and 24 Japanese speakers (nonnative listeners), both with normal hearing. A diotic Modified Rhyme Test was conducted with and without steady-state suppression for reverberation times of 0.4, 0.7 and 1.1 s and a non-reverberant condition. The results showed that native listeners performed better than nonnative listeners, and that the mean percentage of correct answers in initial consonants was higher than in final consonants. The results also showed that processed and unprocessed speech was comparable for word initial and final consonants. These findings indicate that parameters of steady-state suppression would need adjustment to accommodate speech materials and reverberant conditions. They also suggest that the difficulties that nonnative listeners have might not be due to the actual acoustic-phonetic information from the signal.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-293"
  },
  "joto06_interspeech": {
   "authors": [
    [
     "Akiyo",
     "Joto"
    ]
   ],
   "title": "Effect of dynamic information of formants on discrimination of English vowels in consonantal contexts by Japanese listeners",
   "original": "i06_1926",
   "page_count": 4,
   "order": 294,
   "p1": "paper 1926-Tue1BuP.7",
   "pn": "",
   "abstract": [
    "This study examined how differently native speakers of Japanese discriminated between the American English vowels /e/ and /ae/ in /CVp/ syllables with 20 different initial consonants, and how the differing discrimination was related to the formant changes throughout the vowels in comparison with the Japanese vowels /e/ and /a/. A perceptual test and formant analyses of the English and Japanese vowels were conducted. The results showed that there were significant differences in discrimination across the consonantal contexts: the discrimination of /e/ was significantly poorer when the initial consonant was /dg/, /g/, /ch/, /m/ or /th/ (voiceless), and that of /ae/, when it was /sh/, /h/ or /t/. It was found that the poorer discrimination was more related to the smaller formant ratios (F2/F1) and the higher F1 frequency in the latter part of /e/, and to the larger formant ratios and the lower F1 frequency in the latter part of /ae/. The changing of formant patterns throughout the two English vowels could be attributed to the poorer discrimination in the particular consonantal contexts.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-294"
  },
  "wang06c_interspeech": {
   "authors": [
    [
     "Yue",
     "Wang"
    ],
    [
     "Dawn",
     "Behne"
    ],
    [
     "Haisheng",
     "Jiang"
    ],
    [
     "Chad",
     "Danyluck"
    ]
   ],
   "title": "Native and nonnative audio-visual perception of English fricatives in quiet and cafe-noise backgrounds",
   "original": "i06_1798",
   "page_count": 4,
   "order": 295,
   "p1": "paper 1798-Tue1BuP.8",
   "pn": "",
   "abstract": [
    "This study examines audio-visual perception of second-language (L2) speech, with the goal of investigating the extent to which the auditory and visual input modalities are integrated in processing unfamiliar L2 speech. Native (Canadian English) and nonnative (Mandarin) perceivers responses were collected for a set of fricative-initial syllables presented with a quiet and a cafe-noise background, and presented in four ways: congruent audio-visual (AVc), incongruent audio-visual (AVi), audioonly (A) and visual-only (V). Results show that for both native groups, performance was better in the AVc condition than A or V condition; and better in quiet than in cafe-noise background. A comparison of the native and nonnative performance revealed that Mandarin participants showed (1) poorer identification of the L2 interdental fricatives, (2) a greater degree of reliance on visual information, even when auditory information was available, and (3) a higher percentage of McGurk responses with the incongruent AV speech. These findings indicate that although nonnatives were able to use visual information, they failed to adopt the visual cues that are linguistically characteristic of the L2 sounds, suggesting a language-specific AV processing pattern. However, similarities between the two native groups are also indicative of possible perceptual universals involved. Together they point to an integrated network in speech processing across modalities.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-295"
  },
  "grawunder06_interspeech": {
   "authors": [
    [
     "Sven",
     "Grawunder"
    ],
    [
     "Ines",
     "Bose"
    ],
    [
     "Birgit",
     "Hertha"
    ],
    [
     "Franziska",
     "Trauselt"
    ],
    [
     "Lutz Christian",
     "Anders"
    ]
   ],
   "title": "Perceptive and acoustic measurement of average speaking pitch of female and male speakers in German radio news",
   "original": "i06_1966",
   "page_count": 4,
   "order": 296,
   "p1": "paper 1966-Tue1BuP.9",
   "pn": "",
   "abstract": [
    "The average pitch of 68 news broadcasters (34 female / 34 male speakers) was evaluated by 6 expert listeners. Additionally, the average fundamental frequency for all samples was analyzed by means of a series of standard pitch detection algorithms. The results show a strong correlation of acoustic mean and auditory median values for male voices, whereas the auditory mean values female voices are slightly higher than their acoustic counterparts. Vice versa interlistener reliability is higher for the evaluation of female speakers, whereas listeners report more difficulties in rating for male speaking voices.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-296"
  },
  "assmann06_interspeech": {
   "authors": [
    [
     "Peter F.",
     "Assmann"
    ],
    [
     "Sophia",
     "Dembling"
    ],
    [
     "Terrance M.",
     "Nearey"
    ]
   ],
   "title": "Effects of frequency shifts on perceived naturalness and gender information in speech",
   "original": "i06_1710",
   "page_count": 4,
   "order": 297,
   "p1": "paper 1710-Tue1BuP.10",
   "pn": "",
   "abstract": [
    "In natural speech, there is a moderate correlation between the fundamental frequency and formant frequencies across talkers. The present study used a high-quality vocoder to manipulate these properties and determine their contribution to perceived naturalness and voice gender. The stimuli were re-synthesized sentences spoken by two adult males and two adult females. Scale factors were chosen for each sentence and for each talker to produce frequency-shifted versions with a specified mean fundamental frequency (F0) ranging from 60 Hz to 450 Hz in 10 steps, paired with 10 steps in geometric mean formant frequencies ranging from 850 Hz to 2500 Hz. Listeners judged frequencyshifted sentences as more natural when F0 and formant frequencies followed the co-variation of F0 and formant frequencies in natural voices. Sentences with low F0s and low formant frequencies were perceived as masculine, while sentences with high F0 and high formant frequencies were assigned high ratings of femininity. Sentences with \"mismatched\" F0 and formant frequencies were assigned ratings near the midpoint of the range, indicating gender ambiguity. Frequency-shifted sentences derived from male talkers received consistently higher ratings of masculinity than those derived from females and vice versa, even when assigned scale factors appropriate for the opposite gender, indicating that factors other than F0 and mean formant frequencies contribute to perceived gender.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-297"
  },
  "tohyama06_interspeech": {
   "authors": [
    [
     "Hitomi",
     "Tohyama"
    ],
    [
     "Shigeki",
     "Matsubara"
    ]
   ],
   "title": "Influence of pause length on listeners² impressions in simultaneous interpretation",
   "original": "i06_1959",
   "page_count": 4,
   "order": 298,
   "p1": "paper 1959-Tue1BuP.11",
   "pn": "",
   "abstract": [
    "We have been attempting to realize simultaneous machine interpretation. However, determining the interpreting utterance timing is as difficult as determining translation units. This remains a major concern for the development of such a speech translation system. It is also crucial for the systems users that the speech generated by the system is clear and easy to listen to. In this paper, we focus attention on the pauses that partly characterize simultaneous interpreters utterances. We attempt to analyze the results of an experiment conducted using 31 subjects on the relationship between listener-friendliness and the length of pauses in speech, using the CIAIR simultaneous interpretation database as the data source. The results generated some knowledge about listener impressions of simultaneous interpretation, which will be helpful for realizing simultaneous machine interpretation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-298"
  },
  "schwarz06_interspeech": {
   "authors": [
    [
     "Iris-Corinna",
     "Schwarz"
    ],
    [
     "Denis",
     "Burnham"
    ]
   ],
   "title": "New measures to chart toddlers² speech perception and language development: a test of the lexical restructuring hypothesis",
   "original": "i06_1864",
   "page_count": 4,
   "order": 299,
   "p1": "paper 1864-Tue1BuP.12",
   "pn": "",
   "abstract": [
    "Language acquisition factors at work in toddlers between 2 1/2 and 3 years of age were investigated in the first longitudinal study of this kind. New age-appropriate tasks were devised to measure the development of vocabulary size; articulation accuracy; sensitivity to the phonemic features of, in this case, Australian English; and the degree of specialisation towards the native tongue, as measured by language-specific speech perception; LSSP, with 45 Australian English learning toddlers (18 male, 27 female) at 30, 33, and 36 months of age. Results indicated (i) that nearly all measures improved linearly over age; (ii) that there were significant correlations between articulation ability and vocabulary size; and (iii) that, in confirmation of the lexical restructuring hypothesis, vocabulary size is significantly predicted by the broad range of native language abilities under the rubric of Phoneme Sensitivity, but not by the more specific measure of LSSP.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-299"
  },
  "torre06_interspeech": {
   "authors": [
    [
     "Ángel de la",
     "Torre"
    ],
    [
     "Cristina",
     "Roldán"
    ],
    [
     "Manuel",
     "Sainz"
    ]
   ],
   "title": "Perception of fundamental frequency in cochlear implant patients",
   "original": "i06_1477",
   "page_count": 4,
   "order": 300,
   "p1": "paper 1477-Tue1BuP.13",
   "pn": "",
   "abstract": [
    "Fundamental frequency is important in speech perception, since it provides prosodic information to emphasize key words and contributes to speaker identification. Additionally, fundamental frequency contains phonetic value in tonal languages. In the case of hearing impaired patients who have received a cochlear implant, the perception of the fundamental frequency is affected by the technical limitations associated to the procedure of auditory nerve stimulation. In this paper we analyze the possibilities and limitations of cochlear implant systems with respect to perception of the fundamental frequency.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-300"
  },
  "creel06_interspeech": {
   "authors": [
    [
     "Sarah C.",
     "Creel"
    ],
    [
     "Delphine",
     "Dahan"
    ],
    [
     "Daniel",
     "Swingley"
    ]
   ],
   "title": "Effects of featural similarity and overlap position on lexical confusions and overt similarity judgments",
   "original": "i06_1298",
   "page_count": 4,
   "order": 301,
   "p1": "paper 1298-Wed1A3O.1",
   "pn": "",
   "abstract": [
    "Spoken word recognition involves selecting one word out of many competing candidate word-forms. However, there is disagreement as to what candidates actually compete with each other given a particular signal, though similarity is commonly invoked. We investigate whether the word similarity literature, drawing from explicit similarity judgments of word forms, can speak to this current state of confusion. Many word similarity models emphasize the primacy of featural overlap. With a single set of stimuli, we find that featural overlap and overlap position influence a non-time-pressured lexical learning task as well as explicit similarity judgments. The implications for appropriate metrics of similarity in word recognition and models of word similarity are discussed.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-301"
  },
  "mixdorff06_interspeech": {
   "authors": [
    [
     "Hansjörg",
     "Mixdorff"
    ],
    [
     "Yu",
     "Hu"
    ]
   ],
   "title": "Word structure and tone perception in Mandarin",
   "original": "i06_1609",
   "page_count": 4,
   "order": 302,
   "p1": "paper 1609-Wed1A3O.2",
   "pn": "",
   "abstract": [
    "This paper presents results concerning the relationship between word structure in terms of number of syllables and tonal realization in Mandarin. It examines whether the fact that a word (in our context a prosodic word) is more complex implies certain tonal reductions. Our hypothesis is that a monosyllabic word will be uttered more carefully than a polysyllabic word due to the potentially larger number of possibly confusable words. We also examine whether the total number of syllables in a word has an effect, creating more tonal reductions in longer than in shorter words. A database of Mandarin originally designed for concatenative speech synthesis and segmented into prosodic words was statistically analyzed regarding the occurrences of syllable/tone combinations in prosodic words of varying length. 10 sets of syllables were selected comprising all four tones of Mandarin and occurring as monosyllabic words as well as in varying positions in two- to five-syllable prosodic words. The target syllables were then extracted from their original context and presented to native speakers of Mandarin who had to decide which tone they perceived. The results of the perception test indicate, inter alia, that perception of syllables taken from polysyllables indeed is more error prone than that of monosyllabic words. The number of syllables in a word, however, has only a weak influence. Furthermore, reductions mostly appear for syllables in certain locations in a word and are related with underlying syllables' durations.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-302"
  },
  "woehrling06_interspeech": {
   "authors": [
    [
     "Cecile",
     "Woehrling"
    ],
    [
     "Philippe",
     "Boula de Mareüil"
    ]
   ],
   "title": "Identification of regional accents in French: perception and categorization",
   "original": "i06_1261",
   "page_count": 4,
   "order": 303,
   "p1": "paper 1261-Wed1A3O.3",
   "pn": "",
   "abstract": [
    "This article is dedicated to the perceptual identification of French varieties by listeners from the surroundings of Paris and Marseilles. It is based on the geographical localization of about forty speakers from 6 Francophone regions: Normandy, Vendee, Romand Switzerland, Languedoc and the Basque Country. Contrary to the speech type (read or spontaneous speech) and the listeners' region of origin, the speakers' degree of accentedness has a major effect and interacts with the speakers' age. The origin of the oldest speakers (who have the strongest accent according to Paris listeners' judgments) is better recognized than the origin of the youngest speakers. However confusions are frequent among the Southern varieties. On the whole, three accents can be distinguished (by clustering and multidimensional scaling techniques): Northern French, Southern French and Romand Swiss.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-303"
  },
  "phatak06_interspeech": {
   "authors": [
    [
     "Sandeep",
     "Phatak"
    ],
    [
     "Jont B.",
     "Allen"
    ]
   ],
   "title": "Consonant and vowel confusions in speech-weighted noise",
   "original": "i06_1061",
   "page_count": 4,
   "order": 304,
   "p1": "paper 1061-Wed1A3O.4",
   "pn": "",
   "abstract": [
    "Confusion analysis for closed-set recognition of 64 consonant-vowel (CV) sounds, spoken by 18 talkers, is presented. In presence of speech-weighted noise the confusions patterns of the CV syllables categorize into three sets, depending on the consonant. The consonant confusions correlate closely with the high frequency spectra of the consonants while the vowel confusions correlate with the vowel durations and the second formant frequencies.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-304"
  },
  "broersma06_interspeech": {
   "authors": [
    [
     "Mirjam",
     "Broersma"
    ]
   ],
   "title": "Accident - execute: increased activation in nonnative listening",
   "original": "i06_1511",
   "page_count": 4,
   "order": 305,
   "p1": "paper 1511-Wed1A3O.5",
   "pn": "",
   "abstract": [
    "Dutch and English listeners' perception of English words with partially overlapping onsets (e.g., accident . execute) was investigated. Partially overlapping words remained active longer for nonnative listeners, causing an increase of lexical competition in nonnative compared with native listening.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-305"
  },
  "scholz06_interspeech": {
   "authors": [
    [
     "Kirstin",
     "Scholz"
    ],
    [
     "Marcel",
     "Waltermann"
    ],
    [
     "Lu",
     "Huo"
    ],
    [
     "Alexander",
     "Raake"
    ],
    [
     "Sebastian",
     "Möller"
    ],
    [
     "Ulrich",
     "Heute"
    ]
   ],
   "title": "Estimation of the quality dimension \"directness/frequency content\" for the instrumental assessment of speech quality",
   "original": "i06_1219",
   "page_count": 4,
   "order": 306,
   "p1": "paper 1219-Wed1A3O.6",
   "pn": "",
   "abstract": [
    "The presented work aims at an instrumental method for the assessment of speech-quality of modern telecommunication networks that features two functions: the prediction of a system's overall speechquality as well as the analysis of the overall speech-quality. The introduced method is based on the estimation of quality dimensions, each describing a particular characteristic relevant for the quality of speech signals. For narrowband telephone-speech three relevant dimensions have been identified: \"directness/ frequency content\" , \"continuity\", and \"noisiness\". The estimates of the single dimensions allow for an analysis of speech quality while together forming a model for the overall quality. This paper presents a method for the estimation of the quality dimension \"directness/frequency content\". The estimator is based on two parameters considered suitable to measure \"directness/ frequency content\" of a speech signal y(k) that has been transmitted over a telecommunication system. The estimates DF of this dimension show correlations of ρ > 0.93 with the results of corresponding auditory tests.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-306"
  },
  "pluymaekers06_interspeech": {
   "authors": [
    [
     "Mark",
     "Pluymaekers"
    ],
    [
     "Mirjam",
     "Ernestus"
    ],
    [
     "R. Harald",
     "Baayen"
    ]
   ],
   "title": "Effects of word frequency on the acoustic durations of affixes",
   "original": "i06_1241",
   "page_count": 4,
   "order": 307,
   "p1": "paper 1241-Tue1FoP.1",
   "pn": "",
   "abstract": [
    "This study investigates whether the acoustic durations of derivational affixes in Dutch are affected by the frequency of the word they occur in. In a word naming experiment, subjects were presented with a large number of words containing one of the affixes ge-, ver-, ont, or -lijk. Their responses were recorded on DAT tapes, and the durations of the affixes were measured using Automatic Speech Recognition technology. To investigate whether frequency also affected durations when speech rate was high, the presentation rate of the stimuli was varied. The results show that a higher frequency of the word as a whole led to shorter acoustic realizations for all affixes. Furthermore, affixes became shorter as the presentation rate of the stimuli increased. There was no interaction between word frequency and presentation rate, suggesting that the frequency effect also applies in situations in which the speed of articulation is very high.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-307"
  },
  "niu06_interspeech": {
   "authors": [
    [
     "Xiaochuan",
     "Niu"
    ],
    [
     "Alexander B.",
     "Kain"
    ],
    [
     "Jan P. H. van",
     "Santen"
    ]
   ],
   "title": "A noninvasive, low-cost device to study the velopharyngeal port during speech and some preliminary results",
   "original": "i06_1829",
   "page_count": 4,
   "order": 308,
   "p1": "paper 1829-Tue1FoP.2",
   "pn": "",
   "abstract": [
    "It is desirable to monitor the status of the velopharyngeal port during speech. This paper reports on the design and usage of a noninvasive device that measures the static nasal airflow from the nose during speech. The signal can be recorded directly from a generic sound card. Neither does the usage of this device interfere with the articulatory process of speech, nor does it introduce distortions to the simultaneously recorded acoustic signal. We successfully tested the device in an experiment to analyze the velopharyngeal status during normal speech.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-308"
  },
  "aboutabit06_interspeech": {
   "authors": [
    [
     "Noureddine",
     "Aboutabit"
    ],
    [
     "Denis",
     "Beautemps"
    ],
    [
     "Laurent",
     "Besacier"
    ]
   ],
   "title": "Characterization of cued speech vowels from the inner lip contour",
   "original": "i06_1515",
   "page_count": 4,
   "order": 309,
   "p1": "paper 1515-Tue1FoP.3",
   "pn": "",
   "abstract": [
    "Cued Speech (CS) is a manual code that complements lip-reading to enhance speech perception from visual input. The phonetic translation of CS gestures needs to combine the manual CS information with information from the lips, taking into account the desynchronization delay (Attina et al. [1], Aboutabit et al. [2]) between these two flows of information. This paper focuses on the analysis of the lip flow for vowels in French Cued Speech. The vocalic lip targets are defined automatically at the instant of minimum velocity of the inner lip contour area parameter, constrained by the corresponding acoustic labeling. We discuss in particular the possibility of discriminating the vowels with geometric lip parameters using the values at the instant of vocalic targets when associated to a Cued Speech hand position.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-309"
  },
  "gobl06_interspeech": {
   "authors": [
    [
     "Christer",
     "Gobl"
    ]
   ],
   "title": "Modelling aspiration noise during phonation using the LF voice source model",
   "original": "i06_1718",
   "page_count": 4,
   "order": 310,
   "p1": "paper 1718-Tue1FoP.4",
   "pn": "",
   "abstract": [
    "This paper presents a technique for modelling the aspiration noise produced during phonation. The method employs the widely used LF voice source model, which is here extended to include a turbulence noise source for the generation of aspiration. Drawing on speech production theory and on empirical data, the overall amplitude level as well as the within-pulse modulation of the noise are determined by the specific shape of the LF model waveform. The main advantage of this approach is that it simplifies the control of the glottal source, as the temporal dynamics of the variation in noise level need not be explicitly set, but is generated automatically. Informal listening tests also suggest that the proposed modelling technique may contribute to the naturalness of synthesised speech, particularly with regard to breathy voicing.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-310"
  },
  "wei06_interspeech": {
   "authors": [
    [
     "Jianguo",
     "Wei"
    ],
    [
     "Xugang",
     "Lu"
    ],
    [
     "Jianwu",
     "Dang"
    ]
   ],
   "title": "A simulation based parameter optimization for a coarticulation model",
   "original": "i06_1772",
   "page_count": 4,
   "order": 311,
   "p1": "paper 1772-Tue1FoP.5",
   "pn": "",
   "abstract": [
    "A coarticulation model, namely carrier model, has been proposed previously by Dang et al. to improve the performance of a physiological articulatory model based speech synthesizer. The carrier model offers a good framework to account for coarticulation in the planning stage, while its parameters need to be refined for improving the performance of the model. This study is to refine the parameters of the carrier model and estimate typical phonetic targets by minimizing the differences between model simulations and observations. A simulation based optimization framework is proposed for this purpose. The framework consists of two layers: obtaining planned targets in a low layer; estimating phonetic targets and optimizing the parameters in a high layer. A direct search method was applied to the low layer due to the non-analytic nature of the articulation model, while the high layer adopts bilevel optimization strategy to decompose the complicated problem into a set of subproblems. A general evaluation was conducted by combining the refined carrier model and the learned phonetic targets together using the physiological articulatory model and the average error between observations and simulations was 0.15 cm over 103 VCV combinations on the jaw, tongue tip and tongue dorsum.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-311"
  },
  "kacha06_interspeech": {
   "authors": [
    [
     "A.",
     "Kacha"
    ],
    [
     "Francis",
     "Grenez"
    ],
    [
     "Jean",
     "Schoentgen"
    ]
   ],
   "title": "Multivariate analysis of frame-based acoustic cues of dysperiodicities in connected speech",
   "original": "i06_1388",
   "page_count": 4,
   "order": 312,
   "p1": "paper 1388-Tue1FoP.6",
   "pn": "",
   "abstract": [
    "Generalized variogram is used to extract vocal dysperiocities in disordered speech produced by dysphonic speakers. Both signal and dysperiodicity are passed through a filter bank and a segmental signalto- dysperiodicity ratio is defined in each frequency band. Multivariate analysis is carried out to summarize the degree of perceived hoarseness. The predictor variables are the segmental signal-todysperiodicity ratios in the different bands. It is shown that high correlations are achieved by linear regression analysis of the segmental signal-to-dysperiodicity ratios in different non-overlapping frequency bands.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-312"
  },
  "kovacs06_interspeech": {
   "authors": [
    [
     "Tom",
     "Kovacs"
    ],
    [
     "Donald S.",
     "Finan"
    ]
   ],
   "title": "Effects of midline tongue piercing on spectral centroid frequencies of sibilants",
   "original": "i06_1310",
   "page_count": 4,
   "order": 313,
   "p1": "paper 1310-Tue1FoP.7",
   "pn": "",
   "abstract": [
    "Compensatory speech motor patterns occur in response to sensory changes in the vocal tract to facilitate natural sounding speech. The purpose of this project was to investigate speech compensation to midline lingual piercing. In the first experiment, two groups of female speakers (one with tongue piercing) produced two sets of speech samples. Tongue piercing barbells were kept in place as baseline for the first set, and removed for the second. No differences between subject groups were found for /s/ spectral centroid frequencies for the baseline set. A progressive change of /s/ centroid frequencies after barbell removal was observed, indicating rapid compensation to the lingual perturbation. In a second experiment speech samples were recorded from three subjects immediately prior to and following tongue piercing. Changes in centroid frequencies of /s/ and /S/ were dependent on the speaker. It is likely that physiological factors contributed to altered speech observed following piercing.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-313"
  },
  "vijayalakshmi06_interspeech": {
   "authors": [
    [
     "P.",
     "Vijayalakshmi"
    ],
    [
     "M. R.",
     "Reddy"
    ],
    [
     "Douglas",
     "O’Shaughnessy"
    ]
   ],
   "title": "Assessment of articulatory sub-systems of dysarthric speech using an isolated-style phoneme recognition system",
   "original": "i06_1281",
   "page_count": 4,
   "order": 314,
   "p1": "paper 1281-Tue1FoP.8",
   "pn": "",
   "abstract": [
    "In this work, the variation in the acoustic realizations of phonemes between normal and dysarthric speech is utilized for the assessment of articulatory sub-systems of dysarthric speech. Using a speech recognition system for the assessment of dysarthric speech is well established either by considering the continuous speech as a sequence of phonemes, or by considering isolated words. These systems will provide information about the intelligibility alone. However, the problems associated with the subsystems of speech can be well captured when a dysarthric speaker is asked to speak continuously and the resultant speech is analyzed at phoneme-level in isolation. Considering this aspect, in our work, an isolated-style triphone-based phoneme recognition system is developed for analyzing continuous speech. Acoustic variations of the phonemes based on place of articulation alone provides information associated with the malfunctioning of articulatory sub-system and are correlated with the Frenchay dysarthric assessment (FDA) scores. The correlation error between our system and FDA scores is found to be only 9%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-314"
  },
  "finan06_interspeech": {
   "authors": [
    [
     "Donald S.",
     "Finan"
    ],
    [
     "Carol A.",
     "Boliek"
    ]
   ],
   "title": "Respiratory/laryngeal interactions during sustained vowel production in children",
   "original": "i06_1833",
   "page_count": 4,
   "order": 315,
   "p1": "paper 1833-Tue1FoP.9",
   "pn": "",
   "abstract": [
    "Like many other rhythmic movements, respiratory behaviors arise from a central pattern generator (CPG). Modulation of the respiratory CPG function is requisite to generate the air pressures and flows essential for speech production. The emergence of voluntary respiratory system control and the coordination of respiratory and laryngeal structures likely parallels and contributes to the development of speech. Perturbation experiments have proven to be fruitful in exploring neural control of many CPG-modulated behaviors. Little is known about the development of respiratory control and the coordination between respiratory and laryngeal systems, especially for complex behaviors such as speech production. The purpose of this study was to investigate the interaction between laryngeal and respiratory responses to mechanical respiratory perturbation in children.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-315"
  },
  "bunnell06_interspeech": {
   "authors": [
    [
     "H. Timothy",
     "Bunnell"
    ],
    [
     "James B.",
     "Polikoff"
    ]
   ],
   "title": "Acoustic characterization of children with speech delay",
   "original": "i06_2057",
   "page_count": 4,
   "order": 316,
   "p1": "paper 2057-Tue1FoP.10",
   "pn": "",
   "abstract": [
    "Over the past two decades, significant advances have been made in speech analysis and speech pattern recognition techniques, however, the penetration of these advances (notably in pattern recognition techniques) into the speech disorders research arena has lagged, and penetration into the clinic is virtually non-existent. Here we examine one approach to adapting and extending speech recognition technology based on Hidden Markov Modeling (HMM) to an analysis of speech from children with speech disorders of unknown origin. Specifically, we examine the use of normal-speech trained HMMs to identify acoustically defined categories of segmental distortions, and use those categories to characterize differences among a group of children with developmental speech delays.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-316"
  },
  "saz06_interspeech": {
   "authors": [
    [
     "Oscar",
     "Saz"
    ],
    [
     "Antonio",
     "Miguel"
    ],
    [
     "Eduardo",
     "Lleida"
    ],
    [
     "Alfonso",
     "Ortega"
    ],
    [
     "Luis",
     "Buera"
    ]
   ],
   "title": "Study of time and frequency variability in pathological speech and error reduction methods for automatic speech recognition",
   "original": "i06_1266",
   "page_count": 4,
   "order": 317,
   "p1": "paper 1266-Tue1FoP.11",
   "pn": "",
   "abstract": [
    "In this work, we study the variations in the time and frequency domains inside a Spanish language corpus of speakers with non-pathological and pathological speech. We show how pathological speech has a greater variability in the duration of the words than non-pathological speech, while in the frequency domain we show that the vowels confusability increases by a 18%. The baseline experiments in Automatic Speech Recognition (ASR) with this corpus demonstrate that this variability causes a loss in the performance of ASR systems. To reduce the impact of time and frequency variability we use a recent Vocal Tract Length Normalization (VTLN) system: MATE (augMented stAte space acousTic modEl), as a way of improving the performance of ASR systems when dealing with speakers who suffer any kind of speech pathology. Experiments with MATE show a 17.04% and 11.19% WER reduction by using frequency and time MATE respectively.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-317"
  },
  "iseli06_interspeech": {
   "authors": [
    [
     "Markus",
     "Iseli"
    ],
    [
     "Yen-Liang",
     "Shue"
    ],
    [
     "Melissa A.",
     "Epstein"
    ],
    [
     "Patricia",
     "Keating"
    ],
    [
     "Jody",
     "Kreiman"
    ],
    [
     "Abeer",
     "Alwan"
    ]
   ],
   "title": "Voice source correlates of prosodic features in american English: a pilot study",
   "original": "i06_1933",
   "page_count": 4,
   "order": 318,
   "p1": "paper 1933-Thu1A3O.1",
   "pn": "",
   "abstract": [
    "In this paper, we examine the dependencies of voice source parameters F0(fundamental frequency), Ee(maximal glottal flow change), RK(glottal symmetry/skew), LIN(value related to source spectral tilt) and H.1 - H.2 (difference of formant-corrected magnitudes of the first two source spectral harmonics) on prosodic features such as pitch accents, stress, and sentence type and the interdependencies of some of these measures. A small, carefully designed corpus containing a sentence in different prosodic configurations was used in this study. Statistical analysis was performed using two-way ANOVAs to test for the voice source parameter dependencies. Results show that F0 is positively correlated with Ee and LIN, and negatively correlated with H.1 - H.2 . Stressed syllables showed lower values of RK and H.1 - H.2 compared to stressless syllables. The effect of pitch accent can be seen as a combination of its F0, and stress. Phrase-final syllables for interrogative sentences yielded a higher F0 and lower RK and H.1 - H.2 compared to declarative sentences. It was found that it is important to differentiate between tones when analyzing prosodic features that involve tones, such as pitch accent and probably boundary.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-318"
  },
  "bosch06_interspeech": {
   "authors": [
    [
     "Louis ten",
     "Bosch"
    ],
    [
     "R. Harald",
     "Baayen"
    ],
    [
     "Mirjam",
     "Ernestus"
    ]
   ],
   "title": "On speech variation and word type differentiation by articulatory feature representations",
   "original": "i06_1923",
   "page_count": 4,
   "order": 319,
   "p1": "paper 1923-Thu1A3O.2",
   "pn": "",
   "abstract": [
    "This paper describes ongoing research aiming at the description of variation in speech as represented by asynchronous articulatory features. We will first illustrate how distances in the articulatory feature space can be used for event detection along speech trajectories in this space. The temporal structure imposed by the cosine distance in articulatory feature space coincides to a large extent with the manual segmentation on phone level. The analysis also indicates that the articulatory feature representation provides better such alignments than the MFCC representation does. Secondly, we will present first results that indicate that articulatory features can be used to probe for acoustic differences in the onsets of Dutch singulars and plurals.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-319"
  },
  "lee06d_interspeech": {
   "authors": [
    [
     "Sungbok",
     "Lee"
    ],
    [
     "Erik",
     "Bresch"
    ],
    [
     "Jason",
     "Adams"
    ],
    [
     "Abe",
     "Kazemzadeh"
    ],
    [
     "Shrikanth",
     "Narayanan"
    ]
   ],
   "title": "A study of emotional speech articulation using a fast magnetic resonance imaging technique",
   "original": "i06_1792",
   "page_count": 4,
   "order": 320,
   "p1": "paper 1792-Thu1A3O.3",
   "pn": "",
   "abstract": [
    "A recently developed fast MR imaging system is utilized for a study of emotional speech production. Speech utterances and corresponding mid-sagittal vocal tract images are simultaneously acquired by the MRI system. Neutral, angry, sad and happy emotions are simulated by a male American English speaker. The MRI system and analysis results are described in this report. In general articulation is found to be more active in terms of the rate of vocal tract shaping and the ranges of spectral parameter values in emotional speech. It is confirmed that angry speech is characterized by wider and faster vocal tract shaping. Moreover, angry speech shows the more prominent usage of the pharyngeal region than any other emotions. It is also observed that the average vocal tract length above the false vocal folds varies as a function of emotion and that happy speech exhibit relatively shorter length than other emotions. It is likely that this is due to the elevation of the larynx and that may facilitate the higher pitch and larger pitch range manipulation to encode happy emotional quality by the speaker.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-320"
  },
  "kjellstrom06_interspeech": {
   "authors": [
    [
     "Hedvig",
     "Kjellström"
    ],
    [
     "Olov",
     "Engwall"
    ],
    [
     "Olle",
     "Bälter"
    ]
   ],
   "title": "Reconstructing tongue movements from audio and video",
   "original": "i06_1071",
   "page_count": 4,
   "order": 321,
   "p1": "paper 1071-Thu1A3O.4",
   "pn": "",
   "abstract": [
    "This paper presents an approach to articulatory inversion using audio and video of the user¡¯s face, requiring no special markers. The video is stabilized with respect to the face, and the mouth region cropped out. The mouth image is projected into a learned independent component subspace to obtain a low-dimensional representation of the mouth appearance. The inversion problem is treated as one of regression; a non-linear regressor using relevance vector machines is trained with a dataset of simultaneous images of a subject¡¯s face, acoustic features and positions of magnetic coils glued to the subjects¡¯s tongue. The results show the benefit of using both cues for inversion. We envisage the inversion method to be part of a pronunciation training system with articulatory feedback.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-321"
  },
  "feng06b_interspeech": {
   "authors": [
    [
     "Gang",
     "Feng"
    ],
    [
     "Cyril",
     "Kotenkoff"
    ]
   ],
   "title": "New considerations for vowel nasalization based on separate mouth-nose recording",
   "original": "i06_1096",
   "page_count": 4,
   "order": 322,
   "p1": "paper 1096-Thu1A3O.5",
   "pn": "",
   "abstract": [
    "In this paper an experimental setting is described which allows to separately record the speech signals emitted from the mouth and the nostrils. A series of transitions from an oral configuration to the nasopharyngeal configuration is then recorded with a speaker capable to control the movement of his velum while minimizing that of other articulators. The analysis of the recorded signals in the light of simulations with an acoustic model clearly shows that the formant evolution pattern observed in the mouth output is essentially caused by a modification of the oral tract shape due to the velum lowering and the pharyngeal constriction, while the connection with the nasal tract can be neglected in first approximation. On the other hand, the velum movement controls rather the nose output amplitude than its spectral pattern.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-322"
  },
  "garnier06_interspeech": {
   "authors": [
    [
     "Maeva",
     "Garnier"
    ],
    [
     "Lucie",
     "Bailly"
    ],
    [
     "Marion",
     "Dohen"
    ],
    [
     "Pauline",
     "Welby"
    ],
    [
     "Helene",
     "Loevenbruck"
    ]
   ],
   "title": "An acoustic and articulatory study of Lombard speech: global effects on the utterance",
   "original": "i06_1862",
   "page_count": 4,
   "order": 323,
   "p1": "paper 1862-Thu1A3O.6",
   "pn": "",
   "abstract": [
    "This study aims at characterizing the acoustic and articulatory modifications that occur in speech in noisy environments, and at examining them as compensatory strategies. Audio, EGG and video signals were recorded for a female native speaker of French. The corpus consisted of short sentences with a subject-verb-object (SVO) structure. The sentences were recorded in three conditions: silence, 85dB white noise, and 85dB cocktail party noise. Labial parameters were extracted from the video data. The analyses enabled us to examine the effect of the type of noise and to show that hyper-articulation concerns lip aperture and spreading rather than lip pinching. The analysis of the relationship between acoustic and articulatory parameters shows that this speaker especially adapts to noise not only by talking louder or increasing vowel recognition cues but also by increasing spectral emergence.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-323"
  },
  "cnockaert06_interspeech": {
   "authors": [
    [
     "Laurence",
     "Cnockaert"
    ],
    [
     "Jean",
     "Schoentgen"
    ],
    [
     "Pascal",
     "Auzou"
    ],
    [
     "Canan",
     "Ozsancak"
    ],
    [
     "Francis",
     "Grenez"
    ]
   ],
   "title": "Tracking of involuntary formant frequency variations and application to parkinsonian speech",
   "original": "i06_1043",
   "page_count": 4,
   "order": 324,
   "p1": "paper 1043-Tue2A1O.1",
   "pn": "",
   "abstract": [
    "The objective of this paper is to present a formant frequency estimation method, developed with a view to track small variations due to involuntary vocal tract movement. The formant frequency estimation is based on the instantaneous frequencies obtained by means of a complex wavelet transform and is synchronised with the glottal cycle. Results for synthetic speech signals show the precision of the formant frequency estimation method. Results are presented for speech signals from both normophonic speakers and speakers with Parkinson's disease.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-324"
  },
  "weruaga06_interspeech": {
   "authors": [
    [
     "Luis",
     "Weruaga"
    ],
    [
     "Amar",
     "Al-Khayat"
    ]
   ],
   "title": "All-pole model estimation of vocal tract on the frequency domain",
   "original": "i06_1188",
   "page_count": 4,
   "order": 325,
   "p1": "paper 1188-Tue2A1O.2",
   "pn": "",
   "abstract": [
    "Probably the most powerful method for speech analysis is the linear prediction analysis, or LPC analysis, one of its main characteristics being the estimation of time-domain related parameters from time-domain samples. This paper proposes a novel speech analysis framework for estimating the spectral poles directly from spectral samples in voiced speech utterances. The method can be described in plain words as the task of fitting the spectral envelope of an all-pole model directly on the log energy of the harmonics. This problem is addressed with an analysis-by-synthesis mechanism supported on a Newton-Raphson algorithm of fast convergence. The proposed method differs clearly from previous approaches commonly used in Harmonic or Sinusoidal Coding. Comparative results on synthetic signals show the excellent performance of the novel analysis technique.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-325"
  },
  "darch06_interspeech": {
   "authors": [
    [
     "Jonathan",
     "Darch"
    ],
    [
     "Ben",
     "Milner"
    ]
   ],
   "title": "HMM-based MAP prediction of voiced and unvoiced formant frequencies from noisy MFCC vectors",
   "original": "i06_1540",
   "page_count": 4,
   "order": 326,
   "p1": "paper 1540-Tue2A1O.3",
   "pn": "",
   "abstract": [
    "This paper describes how formant frequencies of voiced and unvoiced speech can be predicted from mel-frequency cepstral coefficients (MFCC) vectors using maximum a posteriori (MAP) estimation within a hidden Markov model (HMM) framework. Gaussian mixture models (GMMs) are used to model the local joint density of MFCCs and formant frequencies. More localised prediction is achieved by modelling speech using voiced, unvoiced and non-speech GMMs for every state of each model of a set of HMMs. To predict formant frequencies from a MFCC vector, first a prediction of the speech class (voiced, unvoiced or non-speech) is made. Formant frequencies are predicted from voiced and unvoiced speech using a MAP estimation made using the state-specific GMMs. This 'eHMM-GMM' prediction of speech class and formant frequencies was evaluated on a male 5000 word unconstrained large vocabulary speaker-independent database.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-326"
  },
  "anand06_interspeech": {
   "authors": [
    [
     "Joseph M.",
     "Anand"
    ],
    [
     "S.",
     "Guruprasad"
    ],
    [
     "B.",
     "Yegnanarayana"
    ]
   ],
   "title": "Extracting formants from short segments of speech using group delay functions",
   "original": "i06_1848",
   "page_count": 4,
   "order": 327,
   "p1": "paper 1848-Tue2A1O.4",
   "pn": "",
   "abstract": [
    "Speech is a non-stationary signal, with the shape of the vocal tract changing over several pitch periods, and also within the open and closed glottis phases. The effect of these changes is reflected in the locations of the formants which correspond to the resonant frequencies of the vocal tract. To observe these changes, the analysis window should be small enough (relative to a pitch period), and appropriately anchored. A non-model based method is proposed in this paper to accurately determine formants from short segments (less than a pitch period) of speech signals. It makes use of high resolution properties of group delay function to estimate formants from segments of duration less than a pitch period. The main advantage of this method is its lack of dependence on the parameters of a model. Analysis segments are synchronised with instants of glottal closure, to increase the robustness of formant extraction. Since continuity or additional acoustic-phonetic knowledge are not used, this method is fairly reliable and robust.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-327"
  },
  "ozbek06_interspeech": {
   "authors": [
    [
     "I. Yücel",
     "Özbek"
    ],
    [
     "Mübeccel",
     "Demirekler"
    ]
   ],
   "title": "Tracking of visible vocal tract resonances (VVTR) based on kalman filtering",
   "original": "i06_2029",
   "page_count": 4,
   "order": 328,
   "p1": "paper 2029-Tue2A1O.5",
   "pn": "",
   "abstract": [
    "This paper analyzes vocal tract resonance (VTR) frequency trajectories and their relationship to formants from a new point of view. Considering abrupt/continuous changes in the physical geometry of vocal tract, VTR may change in number, suddenly change their positions or may leak to some regions where they usually do not exist. We define the visible VTR (VVTR) as VTR that can be seen from the spectrogram. So we propose an algorithm, based on Kalman filtering, that can handle all these changes in VVTR. The suggested properties of VVTR trajectories and the performance of the algorithm are demonstrated on several examples.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-328"
  },
  "chaari06_interspeech": {
   "authors": [
    [
     "Salma",
     "Chaari"
    ],
    [
     "Kais",
     "Ouni"
    ],
    [
     "Noureddine",
     "Ellouze"
    ]
   ],
   "title": "Wavelet ridge track interpretation in terms of formants",
   "original": "i06_2030",
   "page_count": 4,
   "order": 329,
   "p1": "paper 2030-Tue2A1O.6",
   "pn": "",
   "abstract": [
    "This paper proposes two new approaches for formant tracking using Fourier and wavelet ridges. The speech signal is decomposed into Time-Frequency representations issued from windowed Fourier transform and wavelet transform. Formant tracking is achieved by exploring ridges from time-frequency representation and imposing continuity constraints on formant trajectories. These approaches are validated by their application on synthesized vowels. The formant tracking shows a reliable estimation in comparison with the values given in synthesis. In the same way, the two analyses are applied on natural voiced speech. All the results are compared to two traditional methods using some type of LPC analysis. Fourier ridge detection analysis is shown to be a useful tool for formant tracking compared to traditional methods.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-329"
  },
  "kurimo06_interspeech": {
   "authors": [
    [
     "Mikko",
     "Kurimo"
    ],
    [
     "Mathias",
     "Creutz"
    ],
    [
     "Matti",
     "Varjokallio"
    ],
    [
     "Ebru",
     "Arsoy"
    ],
    [
     "Murat",
     "Saraclar"
    ]
   ],
   "title": "Unsupervised segmentation of words into morphemes - morpho challenge 2005 application to automatic speech recognition",
   "original": "i06_1512",
   "page_count": 4,
   "order": 330,
   "p1": "paper 1512-Tue2A2O.1",
   "pn": "",
   "abstract": [
    "Within the EU Network of Excellence PASCAL, a challenge was organized to design a statistical machine learning algorithm that segments words into the smallest meaning-bearing units of language, morphemes. Ideally, these are basic vocabulary units suitable for different tasks, such as speech and text understanding, machine translation, information retrieval, and statistical language modeling. Twelve research groups participated in the challenge and had submitted segmentation results obtained by their algorithms. In this paper, we evaluate the application of these segmentation algorithms to large vocabulary speech recognition using statistical n-gram language models based on the proposed word segments instead of entire words. Experiments were done for two agglutinative and morphologically rich languages: Finnish and Turkish. We also investigate combining various segmentations to improve the performance of the recognizer.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-330"
  },
  "arsoy06_interspeech": {
   "authors": [
    [
     "Ebru",
     "Arsoy"
    ],
    [
     "Murat",
     "Saraclar"
    ]
   ],
   "title": "Lattice extension and rescoring based approaches for LVCSR of Turkish",
   "original": "i06_1622",
   "page_count": 4,
   "order": 331,
   "p1": "paper 1622-Tue2A2O.2",
   "pn": "",
   "abstract": [
    "In this paper, we present some techniques to solve the problems of Turkish Large Vocabulary Continuous Speech Recognition (LVCSR). Its agglutinative nature makes Turkish a challenging language in terms of speech recognition since it is impossible to include all possible words in the recognition lexicon. Therefore, data-driven sub-word recognition units, in addition to words, are used in a newspaper content transcription task. We obtain Word Error Rates (WER) of 38.8% for the baseline word model and 33.9% for the baseline sub-word model. In addition, some new methods are investigated. Baseline lattice outputs of each model are rescored with the root and root-class language models for words and first-sub-word language model for sub-words. The word-root interpolation achieves 0.5% decrease in the WER. Other two approaches fail due to the non-robust estimates over the baseline models. Moreover, we have tried dynamic vocabulary extension techniques to handle the Out-of-Vocabulary (OOV) problem in the word model and to remove non-word items in the sub-word model. Applying this method to the 50K baseline word model, in the best situation, we obtain an error rate of 36.2%. In average, the lexicon size of this method is around 188K. However, the error rate is approximately same as the 120K lexicon recognizer. For sub-words, 1.1% absolute improvement is achieved with the vocabulary extension technique giving us our best result.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-331"
  },
  "kobus06_interspeech": {
   "authors": [
    [
     "Catherine",
     "Kobus"
    ],
    [
     "Geraldine",
     "Damnati"
    ],
    [
     "Lionel",
     "Delphin-Poulat"
    ],
    [
     "Renato De",
     "Mori"
    ]
   ],
   "title": "Exploiting semantic relations for a spoken language understanding application",
   "original": "i06_1269",
   "page_count": 4,
   "order": 332,
   "p1": "paper 1269-Tue2A2O.3",
   "pn": "",
   "abstract": [
    "This article proposes a new confidence measure estimated for concept hypotheses provided by a semantic language model used in the context of a dialog application. This confidence measure is based upon the ontology and more precisely, upon the semantic relations between concepts. It aims at measuring how high a concept hypothesis is related to the other hypotheses of an utterance. The semantic relation confidence measure is evaluated alone, and in combination with a classical acoustic confidence measure. The two measures are also used as parameters of a decision tree. It is shown that the two confidence measures are complementary and yield good performance in terms of cross entropy relative reduction.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-332"
  },
  "akita06_interspeech": {
   "authors": [
    [
     "Yuya",
     "Akita"
    ],
    [
     "Masahiro",
     "Saikou"
    ],
    [
     "Hiroaki",
     "Nanjo"
    ],
    [
     "Tatsuya",
     "Kawahara"
    ]
   ],
   "title": "Sentence boundary detection of spontaneous Japanese using statistical language model and support vector machines",
   "original": "i06_1370",
   "page_count": 4,
   "order": 333,
   "p1": "paper 1370-Tue2A2O.4",
   "pn": "",
   "abstract": [
    "This paper presents two different approaches utilizing statistical language model (SLM) and support vector machines (SVM) for sentence boundary detection of spontaneous Japanese. In the SLM-based approach, linguistic likelihoods and occurrence of pause are used to determine sentence boundaries. To suppress false alarms, heuristic patterns of end-of-sentence expressions are also incorporated. On the other hand, SVM is adopted to realize robust classification against a wide variety of expressions and speech recognition errors. Detection is performed by an SVM-based text chunker using lexical and pause information as features. We evaluated these approaches on manual and automatic transcription of spontaneous lectures and speeches, and achieved F-measures of 0.85 and 0.78, respectively.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-333"
  },
  "virpioja06_interspeech": {
   "authors": [
    [
     "Sami",
     "Virpioja"
    ],
    [
     "Mikko",
     "Kurimo"
    ]
   ],
   "title": "Compact n-gram models by incremental growing and clustering of histories",
   "original": "i06_1231",
   "page_count": 4,
   "order": 334,
   "p1": "paper 1231-Tue2A2O.5",
   "pn": "",
   "abstract": [
    "This work concerns building n-gram language models that are suitable for large vocabulary speech recognition in devices that have a restricted amount of memory and space available. Our target language is Finnish, and in order to evade the problems of its rich morphology, we use sub-word units, morphs, as model units instead of the words. In the proposed model we apply incremental growing and clustering of the morph n-gram histories. By selecting the histories using maximum a posteriori estimation, and clustering them with information radius measure, we obtain a clustered varigram model. We show that for restricted model sizes this model gives better cross-entropy and speech recognition results than the conventional n-gram models, and also better recognition results than non-clustered varigram models built with another recently introduced method.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-334"
  },
  "camelin06_interspeech": {
   "authors": [
    [
     "Nathalie",
     "Camelin"
    ],
    [
     "Geraldine",
     "Damnati"
    ],
    [
     "Frederic",
     "Bechet"
    ],
    [
     "Renato De",
     "Mori"
    ]
   ],
   "title": "Opinion mining in a telephone survey corpus",
   "original": "i06_1417",
   "page_count": 4,
   "order": 335,
   "p1": "paper 1417-Tue2A2O.6",
   "pn": "",
   "abstract": [
    "Telephone surveys are often used by Customer Services to evaluate their clients' satisfaction and to improve their services. Large amounts of data are collected to observe the evolution of customers' opinions. Within this context, the automatization of the process of these databases becomes a crucial issue. This paper addresses the automatic analysis of audio messages where customers are asked to give their opinion over several dimensions about a Customer Service. Interpretation methods that integrate automatically and manually acquired knowledge are proposed. Experimental results, done on a database collected from a deployed Customer Service in real conditions with real customers are given.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-335"
  },
  "peinado06_interspeech": {
   "authors": [
    [
     "Antonio M.",
     "Peinado"
    ],
    [
     "Angel M.",
     "Gómez"
    ],
    [
     "Victoria",
     "Sánchez"
    ],
    [
     "José L.",
     "Pérez-Córdoba"
    ],
    [
     "Antonio J.",
     "Rubio"
    ]
   ],
   "title": "An integrated solution for error concealment in DSR systems over wireless channels",
   "original": "i06_1273",
   "page_count": 4,
   "order": 336,
   "p1": "paper 1273-Tue2BuP.1",
   "pn": "",
   "abstract": [
    "Distributed Speech Recognition involves the development of techniques to conceal the degradations that the transmission channel introduces in the speech features. This work proposes a lowcomplexity high-accuracy error concealment technique compatible with the DSR ETSI standards. This is achieved by combining three different techniques: fast MMSE estimation, Viterbi decoding with soft-data and subvector-based error detection. We also propose a method to extend this Viterbi decoding to dynamic features. The experimental results show the effectiveness of our proposal.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-336"
  },
  "gomez06b_interspeech": {
   "authors": [
    [
     "Angel M.",
     "Gómez"
    ],
    [
     "Antonio M.",
     "Peinado"
    ],
    [
     "Victoria",
     "Sánchez"
    ],
    [
     "José L.",
     "Carmona"
    ],
    [
     "Antonio J.",
     "Rubio"
    ]
   ],
   "title": "Interleaving and MMSE estimation with VQ replicas for distributed speech recognition over lossy packet networks",
   "original": "i06_1279",
   "page_count": 4,
   "order": 337,
   "p1": "paper 1279-Tue2BuP.2",
   "pn": "",
   "abstract": [
    "In this work we evaluate the performance of MMSE estimation with a media-specific FEC based on VQ replicas in comparison with MAP estimation and interleaving, both operating in a DSR system over a loss-prone packet switched network. Both schemes combine a sender-driven with a receiver-based technique and, as we show, clearly outperform the standard Aurora mitigation. However, as independent techniques, interleaving and FEC codes could be jointly applied. Although this would provide better results, a direct combination of FECs and interleaving involves a sum of the delays of both operations. In this work, we introduce a double stream-based strategy that avoids this sum of delays.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-337"
  },
  "chen06b_interspeech": {
   "authors": [
    [
     "Gang",
     "Chen"
    ],
    [
     "Hesham",
     "Tolba"
    ],
    [
     "Douglas",
     "O’Shaughnessy"
    ]
   ],
   "title": "Noise-robust speech recognition of conversational telephone speech",
   "original": "i06_1304",
   "page_count": 4,
   "order": 338,
   "p1": "paper 1304-Tue2BuP.3",
   "pn": "",
   "abstract": [
    "Over the past several years, the primary focus of investigation for speech recognition has been over the telephone or IP network. Recently more and more IP telephony has been extensively used. This paper describes the performance of a speech recognizer on noisy speech transmitted over an H.323 IP telephony network, where the minimum mean-square error log spectra amplitude (MMSE-LSA) method [1,2] is used to reduce the mismatch between training and deployment condition in order to achieve robust speech recognition. In the H.323 network environment, the sources of distortion to the speech are packet loss and additive noise. In this work, we evaluate the impact of packet losses on speech recognition performance first, and then explore the effects of uncorrelated additive noise on the performance. To explore how additive acoustic noise affects the speech recognition performance, seven types of noise sources are selected for use in our experiments. Finally, the experimental results indicate that the MMSE-LSA enhancement method apparently increased robustness for some types of additive noise under certain packet loss rates over the H.323 telephone network.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-338"
  },
  "kuroiwa06_interspeech": {
   "authors": [
    [
     "Shingo",
     "Kuroiwa"
    ],
    [
     "Satoru",
     "Tsuge"
    ],
    [
     "Fuji",
     "Ren"
    ]
   ],
   "title": "Lost speech reconstruction method using speech recognition based on missing feature theory and HMM-based speech synthesis",
   "original": "i06_1347",
   "page_count": 4,
   "order": 339,
   "p1": "paper 1347-Tue2BuP.4",
   "pn": "",
   "abstract": [
    "In recent years, IP telephone service has spread rapidly. However, an unavoidable problem of IP telephone service is deterioration of speech due to packet loss, which often occurs on wireless networks. To overcome this problem, we propose a novel lost speech reconstruction method using speech recognition based on Missing Feature Theory and HMM-based speech synthesis. The proposed method uses linguistic information and can deal with the lack of syllable units which conventional methods are unable to handle. We conducted subjective and objective evaluation experiments under speaker independent conditions. These results showed the effectiveness of the proposed method. Although there is a processing delay in the proposed method, we believe that this method will open up new applications for speech recognition and speech synthesis technology.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-339"
  },
  "selouani06_interspeech": {
   "authors": [
    [
     "Sid-Ahmed",
     "Selouani"
    ],
    [
     "Douglas",
     "O’Shaughnessy"
    ]
   ],
   "title": "Speaker adaptation using evolutionary-based linear transform",
   "original": "i06_1368",
   "page_count": 4,
   "order": 340,
   "p1": "paper 1368-Tue2BuP.5",
   "pn": "",
   "abstract": [
    "This paper presents a technique to adapt HMMs to new speakers by using Genetic Algorithms (GAs) in unsupervised mode. The implementation requirements of GAs, such as genetic operators and objective function, have been chosen in order to give more reliability to a global linear transformation matrix. By implementing a survival of the fittest strategy, the proposed GA-MLLR approach allows to maintain and manipulate a population of a wide range of solutions. Experiments have been performed on ARPA-RM and TIMIT databases using a triphones HMM-based system. Results show that from a new speaker, significant decrease of word error rate which can reach 6% for a particular speaker, has been achieved by the evolutionary approach, compared to the conventional MLLR-based adaptation method.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-340"
  },
  "wang06d_interspeech": {
   "authors": [
    [
     "Jingying",
     "Wang"
    ],
    [
     "Zuoying",
     "Wang"
    ]
   ],
   "title": "A speaker adaptation algorithm using principal curves in noisy environments",
   "original": "i06_1374",
   "page_count": 4,
   "order": 341,
   "p1": "paper 1374-Tue2BuP.6",
   "pn": "",
   "abstract": [
    "A new speaker adaptation method of speech recognition is proposed in this paper utilizing principal curves algorithm. The key feature of this method is the construction of a transformation function based on the correlation information between observations of different acoustic states. This is an important a priori information crucial to improving systems recognition performance. Herein the relationships between the statistics information required choosing the best reconstruction of an audio speech pattern and the codebook state parameters of the new algorithm are described, and then the method is applied to a large database of continuous speech. Experiment results on large vocabulary continuous speech recognition database showed that this new method is superior to MLLR adaptation approach in noisy cases, demonstrating that the principal curves speaker adaptation algorithm successfully exploits the correlation information and improve robustness.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-341"
  },
  "clarke06_interspeech": {
   "authors": [
    [
     "Constance",
     "Clarke"
    ],
    [
     "Daniel",
     "Jurafsky"
    ]
   ],
   "title": "Limitations of MLLR adaptation with Spanish-accented English: an error analysis",
   "original": "i06_1611",
   "page_count": 4,
   "order": 342,
   "p1": "paper 1611-Tue2BuP.7",
   "pn": "",
   "abstract": [
    "We studied the effect of MLLR adaptation with Spanish-accented English to understand the strengths and weaknesses of MLLR with unseen foreign accents. We trained a global MLLR transform on 10 adaptation sentences per speaker, giving a 3.4% absolute decrease in phone error rate. We then studied the pattern of improvements across phones and phone classes. Phones that improved the least tended to be those that do not exist in Spanish. Results suggest the poorer performance is related to increased insertion and substituter rates during the adaptation phase, as well as greater acoustic variability.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-342"
  },
  "liao06_interspeech": {
   "authors": [
    [
     "H.",
     "Liao"
    ],
    [
     "M. J. F.",
     "Gales"
    ]
   ],
   "title": "Issues with uncertainty decoding for noise robust speech recognition",
   "original": "i06_1627",
   "page_count": 4,
   "order": 343,
   "p1": "paper 1627-Tue2BuP.8",
   "pn": "",
   "abstract": [
    "Recently there has been interest in uncertainty decoding for robust speech recognition. Here the uncertainty associated with the observation in noise is propagated to the recogniser. By using appropriate approximations for this uncertainty, it is possible to obtain efficient implementations during decoding. The aim of these schemes is to obtain performance which is close to that of a model-based compensated system, without the computational cost. Unfortunately, in low SNR there is a fundamental issue with front-end uncertainty decoding where the model means and variances are updated according to the features. This is described in detail using the Joint and SPLICE with uncertainty forms, but is not limited to these two techniques. A solution for the Joint scheme is presented along with the implicit approach used in SPLICE with uncertainty. In addition, a model-based Joint uncertainty scheme is described, which is more efficient and powerful than the front-end schemes, and being model-based not affected by this problem. This issue is illustrated using the AURORA 2.0 database with these various systems.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-343"
  },
  "xu06_interspeech": {
   "authors": [
    [
     "Haitian",
     "Xu"
    ],
    [
     "Luca",
     "Rigazio"
    ],
    [
     "David",
     "Kryze"
    ]
   ],
   "title": "Vector taylor series based joint uncertainty decoding",
   "original": "i06_1688",
   "page_count": 4,
   "order": 344,
   "p1": "paper 1688-Tue2BuP.9",
   "pn": "",
   "abstract": [
    "Joint uncertainty decoding has recently achieved promising results by using front-end uncertainty in the back-end in a mathematically consistent framework. One drawback of the method is that it relies on stereo-data or numerical algorithms, such as DPMC, which have high computational complexity and are difficult to deploy in real applications. We propose a Vector Taylor Series (VTS) approach to joint uncertainty decoding which provides a closed-form solution to the key problem of estimating the clean/noisy speech cross-covariance matrix. Our solution does not require stereo-data or numerical integration. We also propose a new strategy to deal with the cross-covariance matrix singularity. Experiments on Aurora2 show that VTS-based joint uncertainty decoding has similar accuracy compared to DPMC-based joint uncertainty decoding while being at least three times faster. Finally, VTS-based joint uncertainty decoding provided more than 2% absolute improvement when combined with our new strategy for cross-covariance singularity.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-344"
  },
  "huo06b_interspeech": {
   "authors": [
    [
     "Qiang",
     "Huo"
    ],
    [
     "Donglai",
     "Zhu"
    ]
   ],
   "title": "A maximum likelihood training approach to irrelevant variability compensation based on piecewise linear transformations",
   "original": "i06_1740",
   "page_count": 4,
   "order": 345,
   "p1": "paper 1740-Tue2BuP.10",
   "pn": "",
   "abstract": [
    "In our previous works, a maximum likelihood training approach was developed based on the concept of stochastic vector mapping (SVM) that performs a frame-dependent bias removal to compensate for environmental variabilities in both training and recognition stages. Its effectiveness was confirmed by evaluation experiments on Aurora2 and Aurora3 databases. In this paper, we present an extended ML formulation to entertain some new SVM functions that are piecewise linear transformations and are more flexible than the frame-dependent bias removal. Evaluation results on Finnish Aurora3 database show that in comparison with the performance of a baseline system based on ML-trained CDHMMs without feature compensation, the previous and the new SVM-based feature compensation approaches achieve a relative word error rate reduction of 15.7% and 26.1% respectively for well-matched condition.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-345"
  },
  "mandal06_interspeech": {
   "authors": [
    [
     "Arindam",
     "Mandal"
    ],
    [
     "Mari",
     "Ostendorf"
    ],
    [
     "Andreas",
     "Stolcke"
    ]
   ],
   "title": "Speaker clustered regression-class trees for MLLR adaptation",
   "original": "i06_1763",
   "page_count": 4,
   "order": 346,
   "p1": "paper 1763-Tue2BuP.11",
   "pn": "",
   "abstract": [
    "A speaker clustering algorithm is presented that is based on an eigenspace representation of Maximum Likelihood Linear Regression (MLLR) transformations and is used for training cluster-dependent regression-class trees for MLLR adaptation. It is shown that significant automatic speech recognition (ASR) system performance gains are possible by choosing the best regression-class tree structure for individual speakers. To take advantage of the potential gains, an algorithm for combining the MLLR mean transformations from cluster-specific trees is described that effectively results in a soft regression-class tree. In conversational speech recognition, only small overall improvements are obtained, but the number of speakers that have performance degradation due to adaptation is reduced by over 70%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-346"
  },
  "tan06_interspeech": {
   "authors": [
    [
     "Zheng-Hua",
     "Tan"
    ],
    [
     "Paul",
     "Dalsgaard"
    ],
    [
     "Børge",
     "Lindberg"
    ]
   ],
   "title": "Robust speech recognition over mobile networks using combined weighted viterbi decoding and subvector based error concealment",
   "original": "i06_1825",
   "page_count": 4,
   "order": 347,
   "p1": "paper 1825-Tue2BuP.12",
   "pn": "",
   "abstract": [
    "Robustness against transmission errors is one of the primary barriers to the widespread application of automatic speech recognition (ASR) in mobile communications. We have previously proposed a subvector based error concealment (EC) method that conducts error detection and mitigation in the feature-domain at the subvector level. This paper presents a weighted Viterbi decoding (WVD) algorithm that works in the model domain for counteracting unreliable features generated by the subvector based EC. The reliability of each feature is estimated during the process of subvector based EC and is used by the WVD for modifying the observation probability of the feature. Recognition experiments are conducted on the Aurora 2 database corrupted by GSM error pattern EP3. Combining the WVD and the subvector EC achieves 70% and 24% performance improvement as compared to the ETSI-DSR standard and the subvector based EC, respectively.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-347"
  },
  "zen06_interspeech": {
   "authors": [
    [
     "Heiga",
     "Zen"
    ],
    [
     "Yoshihiko",
     "Nankaku"
    ],
    [
     "Keiichi",
     "Tokuda"
    ],
    [
     "Tadashi",
     "Kitamura"
    ]
   ],
   "title": "Speaker adaptation of trajectory HMMs using feature-space MLLR",
   "original": "i06_1958",
   "page_count": 4,
   "order": 348,
   "p1": "paper 1958-Tue2BuP.13",
   "pn": "",
   "abstract": [
    "Recently, a trajectory model, derived from the hidden Markov model (HMM) by imposing explicit relationships between static and dynamic features, has been proposed. The derived model, named trajectory HMM, can alleviate two limitations of the HMM: constant statistics within a state and conditional independence assumption of state output probabilities. In the present paper, a speaker adaptation algorithm for the trajectory HMM based on feature-space Maximum Likelihood Linear Regression (fMLLR) is derived and evaluated. Results of a simple continuous speech recognition experiment shows that adapting trajectory HMMs using the derived adaptation algorithm improves the speech recognition performance.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-348"
  },
  "povey06_interspeech": {
   "authors": [
    [
     "Daniel",
     "Povey"
    ],
    [
     "George",
     "Saon"
    ]
   ],
   "title": "Feature and model space speaker adaptation with full covariance Gaussians",
   "original": "i06_2050",
   "page_count": 4,
   "order": 349,
   "p1": "paper 2050-Tue2BuP.14",
   "pn": "",
   "abstract": [
    "Full covariance models can give better results for speech recognition than diagonal models, yet they introduce complications for standard speaker adaptation techniques such as MLLR and fMLLR. Here we introduce efficient update methods to train adaptation matrices for the full covariance case. We also experiment with a simplified technique in which we pretend that the full covariance Gaussians are diagonal and obtain adaptation matrices under that assumption. We show that this approximate method works almost as well as the exact method.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-349"
  },
  "gispert06_interspeech": {
   "authors": [
    [
     "Adrià de",
     "Gispert"
    ],
    [
     "José B.",
     "Mariño"
    ]
   ],
   "title": "Linguistic tuple segmentation in n-gram-based statistical machine translation",
   "original": "i06_1049",
   "page_count": 4,
   "order": 350,
   "p1": "paper 1049-Tue2CaP.1",
   "pn": "",
   "abstract": [
    "Ngram-based Statistical Machine Translation relies on a standard Ngram language model of tuples to estimate the translation process. In training, this translation model requires a segmentation of each parallel sentence, which involves taking a hard decision on tuple segmentation when a word is not linked during word alignment. This is especially critical when this word appears in the target language, as this hard decision is compulsory.\n",
    "In this paper we present a thorough study of this situation, comparing for the first time each of the proposed techniques in two independent tasks, namely English-Spanish European Parliament Proceedings large-vocabulary task and Arabic-English Basic Travel Expressions small-data task. In the face of this comparison, we present a novel segmentation technique which incorporates linguistic information. Results obtained in both tasks outperform all previous techniques.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-350"
  },
  "oba06_interspeech": {
   "authors": [
    [
     "Takanobu",
     "Oba"
    ],
    [
     "Takaaki",
     "Hori"
    ],
    [
     "Atsushi",
     "Nakamura"
    ]
   ],
   "title": "Sentence boundary detection using sequential dependency analysis combined with CRF-based chunking",
   "original": "i06_1657",
   "page_count": 4,
   "order": 351,
   "p1": "paper 1657-Tue2CaP.2",
   "pn": "",
   "abstract": [
    "In spoken language, sentence boundaries are much less explicit than in written language. Since conventional natural language processing (NLP) techniques are generally designed assuming the sentence boundaries are already given, it is crucial to detect the boundaries accurately for applying such NLP techniques to spoken language. Classification frameworks, such as Support Vector Machines (SVMs) and Conditional Random Fields (CRFs), can be used to detect the boundaries. With these methods, the sentence boundaries are determined based on local sentence-end-like word sequences around the boundaries. However, the methods do not evaluate whether or not each block determined by the boundaries is appropriate as a sentence. We have proposed sequential dependency analysis (SDA), which extracts the dependency structure of unsegmented word sequences with a subsidiary mechanism of sentence boundary detection. In this paper, we extend SDA by combining it with CRFs to reflect both the properties of local word sequences and the appropriateness as a sentence. In this way we achieve more accurate sentence boundary detection. The experimental result shows that our proposed method provides better detection accuracy than that obtained with SVMs or CRFs alone. Our method can also work sequentially because it is based on the SDA framework and can be used for on-line spoken applications.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-351"
  },
  "bangalore06_interspeech": {
   "authors": [
    [
     "Srinivas",
     "Bangalore"
    ],
    [
     "Patrick",
     "Haffner"
    ],
    [
     "Stephan",
     "Kanthak"
    ]
   ],
   "title": "Sequence classification for machine translation",
   "original": "i06_1722",
   "page_count": 4,
   "order": 352,
   "p1": "paper 1722-Tue2CaP.3",
   "pn": "",
   "abstract": [
    "Discriminatively trained classification techniques have been shown to out-perform generative techniques on many speech and natural language processing problems. However, most of the research in machine translation has been based on generative modeling techniques. The application of classification techniques to machine translation requires scaling classifiers to deal with very large label sets (the vocabulary of the target language). In this paper, we present a method to scale classifiers to very large label sets and apply it to train classifiers for machine translation. We contrast this approach to a generatively trained machine translation model represented as a weighted finite-state transducer. We show translation accuracy results on spoken language corpora in English to Spanish and English to Japanese translation tasks.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-352"
  },
  "itoh06_interspeech": {
   "authors": [
    [
     "Yoshiaki",
     "Itoh"
    ],
    [
     "Takayuki",
     "Otake"
    ],
    [
     "Kohei",
     "Iwata"
    ],
    [
     "Kazunori",
     "Kojima"
    ],
    [
     "Masaaki",
     "Ishigame"
    ],
    [
     "Kazuyo",
     "Tanaka"
    ],
    [
     "Shi-wook",
     "Lee"
    ]
   ],
   "title": "Two-stage vocabulary-free spoken document retrieval - subword identification and re-recognition of the identified sections",
   "original": "i06_1865",
   "page_count": 4,
   "order": 353,
   "p1": "paper 1865-Tue2CaP.4",
   "pn": "",
   "abstract": [
    "A query word for retrieval systems is liable to be a special term not included in a speech recognizer dictionary. Spoken document retrieval (SDR) systems must therefore be vocabulary-free to deal with arbitrary query words. This paper proposes a new method for vocabulary-free spoken document retrieval. The method exploits two-stage tactics. First, when a query word is submitted, the query word is transformed to a subword sequence according to conversion rules. The subword sequence is searched for spoken documents previously transcribed to a subword sequence by subword recognition. The identified sections are extracted according to the distance between the subword sequences of the query and the identified sections. Second, each identified section is re-recognized using a grammar that includes the query subword sequence. Retrieval experiments were conducted with an actual TV program and the results demonstrated that the proposed method improved SDR performance without long delays in retrieval.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-353"
  },
  "surdeanu06_interspeech": {
   "authors": [
    [
     "Mihai",
     "Surdeanu"
    ],
    [
     "David",
     "Dominguez-Sal"
    ],
    [
     "Pere R.",
     "Comas"
    ]
   ],
   "title": "Design and performance analysis of a factoid question answering system for spontaneous speech transcriptions",
   "original": "i06_1046",
   "page_count": 4,
   "order": 354,
   "p1": "paper 1046-Tue2CaP.5",
   "pn": "",
   "abstract": [
    "This paper introduces a QA designed from scratch to handle speech transcriptions. The systems strength is achieved by analyzing the speech transcriptions with a mix of IR-oriented methodologies and a small number of robust NLP components. We evaluate the system on transcriptions of spontaneous speech from several 1-hour-long seminars and presentations and show that the system obtains encouraging performance.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-354"
  },
  "takezawa06_interspeech": {
   "authors": [
    [
     "Toshiyuki",
     "Takezawa"
    ],
    [
     "Tohru",
     "Shimizu"
    ]
   ],
   "title": "Performance improvement of dialog speech translation by rejecting unreliable utterances",
   "original": "i06_1100",
   "page_count": 4,
   "order": 355,
   "p1": "paper 1100-Tue2CaP.6",
   "pn": "",
   "abstract": [
    "We discuss how to measure the reliability of recognized utterances based on a confidence measure, and applied it to a dialog speech translation system. In this study, we employ generalized word posterior probability (GWPP), a confidence measure for verifying recognized words, and expand it to measure the reliability of recognized utterances. We confirmed the performance improvement by applying the rejection technique to a dialog speech translation system from Japanese to English. We conducted two kinds of performance evaluation. One is a ranking evaluation of translation output by human evaluators. The other is to measure the machine output against human results by a paired-comparison method. Both of them yield significant improvements.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-355"
  },
  "ettelaie06_interspeech": {
   "authors": [
    [
     "Emil",
     "Ettelaie"
    ],
    [
     "Panayiotis G.",
     "Georgiou"
    ],
    [
     "Shrikanth",
     "Narayanan"
    ]
   ],
   "title": "Cross-lingual dialog model for speech to speech translation",
   "original": "i06_1858",
   "page_count": 4,
   "order": 356,
   "p1": "paper 1858-Tue2CaP.7",
   "pn": "",
   "abstract": [
    "Speech understanding through concept classification offers a possible way of machine translation in speech-to-speech translation systems and can be used in conjunction with conventional statistical machine translation. While correct concept classification offers the promise of obtaining well-formed target language speech output, the approach does not scale well to large number of concepts. Importantly, it is also critical to know when to accept or reject the classifier. We formulate the speech classification as a MAP estimation problem to derive the understanding model and improve its performance by incorporating dialog context information. Specifically, for a two-way speech translation system, a classification scheme is derived here that utilizes context information from both sides of the conversation through an n-gram dialog model. The method was evaluated using data from an English-Farsi trans-lingual doctor-patient dialog system and its classification and rejection accuracies were compared to those of a baseline system with an understanding model only. The benefit of incorporating context with the proposed dialog model provided a modest improvement in classification accuracy (about 5% relative error reduction) and a significant improvement in the rejection accuracy (up to 31.4% relative reduction in error).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-356"
  },
  "akbacak06_interspeech": {
   "authors": [
    [
     "Murat",
     "Akbacak"
    ],
    [
     "John H. L.",
     "Hansen"
    ]
   ],
   "title": "A robust fusion method for multilingual spoken document retrieval systems employing tiered resources",
   "original": "i06_1835",
   "page_count": 4,
   "order": 357,
   "p1": "paper 1835-Tue2CaP.8",
   "pn": "",
   "abstract": [
    "In this study, we present two novel fusion approaches to merge subword and word based retrieval methods within a multilingual spoken document retrieval (SDR) system. Considering the fact that more than 6000 languages are spoken in the world today, resources (e.g., text and audio data, pronunciation lexicon) needed to develop Automatic Speech Recognition (ASR) systems for such a range of languages (accordingly the performances of these ASR systems) can be considered within a tiered structure. Even for resource-rich languages, some applications (e.g., historical digital archives) contain acoustical/lexical variations among time which presents challenges to build effective up-to-date audio indexing and retrieval systems. Within this concept, we focus on creating robust multilingual SDR systems employing both word-based and subword-based retrieval methods. Our proposed algorithms employ an OOV-word detection module to generate hybrid transcripts/ lattices. In our Dynamic Fusion (DF) approach, hybrid transcripts/lattices are used to assign dynamic fusion weights to each subsystem. In our Hybrid Fusion (HF) approach, queries are searched through hybrid lattices. We evaluated our proposed algorithms in a proper name retrieval task within the Spanish Broadcast News domain, and spoken document retrieval task using our historical speech archive NGSW corpus [1], where the proposed algorithms yield improvements over traditional fusion methods.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-357"
  },
  "zhu06_interspeech": {
   "authors": [
    [
     "Weizhong",
     "Zhu"
    ],
    [
     "Bowen",
     "Zhou"
    ],
    [
     "Charles",
     "Prosser"
    ],
    [
     "Pavel",
     "Krbec"
    ],
    [
     "Yuqing",
     "Gao"
    ]
   ],
   "title": "Recent advances of IBM’s handheld speech translation system",
   "original": "i06_1590",
   "page_count": 4,
   "order": 358,
   "p1": "paper 1590-Tue2CaP.9",
   "pn": "",
   "abstract": [
    "Recent Advances in the processing capabilities of Personal Digital Assistants (PDAs) have enabled the implementation of an end-to-end speech translation system on these devices. We have presented a bidirectional speech-to-speech (English and Chinese) translation system, which is hosted on a PDA running embedded Linux. Our Multilingual Automatic Speech-to-Speech Translator (MASTOR) system includes an HMM-based large vocabulary continuous speech recognizer using statistical n-grams, a translation module, and a multi-language speech synthesis system. This paper describes our recent efforts to develop a bi-directional English and colloquial Arabic speech-to-speech system on a device running the popular Windows-CE operating system. We have created completely new translation modules and introduced a fast adaptation scheme for a new speaker or environment. In addition, the componentization of system modules speeds up the development of a new speech-to-speech system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-358"
  },
  "stenchikova06_interspeech": {
   "authors": [
    [
     "Svetlana",
     "Stenchikova"
    ],
    [
     "Dilek",
     "Hakkani-Tür"
    ],
    [
     "Gokhan",
     "Tur"
    ]
   ],
   "title": "QASR: question answering using semantic roles for speech interface",
   "original": "i06_2054",
   "page_count": 4,
   "order": 359,
   "p1": "paper 2054-Tue2CaP.10",
   "pn": "",
   "abstract": [
    "In this paper, we evaluate a semantic role labeling approach to the extraction of answers in the open domain question answering task. We show that this technique especially improves the system performance when answers are communicated to the user by voice. Semantic role labeling identifies predicates and semantic argument phrases in a sentence. With this information we are able to analyze and extract structure from both questions and candidate sentences, which helps us identify more relevant and precise answers in a long list of candidate sentences. When searching for an answer to a question, we match the missing argument in the question to the semantic parses of the candidate answers. This technique significantly improves the accuracy of the question answering system and results in more concise and grammatical answers, which is essential for enabling voice interfaces to question answering systems. In this paper we apply our approach to factoid questions containing predicates; however, this technique can be also useful in answering more complex questions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-359"
  },
  "maas06_interspeech": {
   "authors": [
    [
     "Jan F.",
     "Maas"
    ],
    [
     "Britta",
     "Wrede"
    ],
    [
     "Gerhard",
     "Sagerer"
    ]
   ],
   "title": "Towards a multimodal topic tracking system for a mobile robot",
   "original": "i06_1121",
   "page_count": 4,
   "order": 360,
   "p1": "paper 1121-Tue2CaP.11",
   "pn": "",
   "abstract": [
    "Topics in situated and task oriented communication depend heavily on the given, often changing environment, making the detection of predetermined topics in many cases useless. Detection of non-predefined topics can enhance Human-Robot-Interaction (HRI) in a variety of ways, though.\n",
    "In this paper we propose a way to dynamically determine topics during Human-Robot-Communication using well established techniques such as Latent Semantic Analysis (LSA). The procedure is based on multimodal cues, supporting the view that topics are not simply a property of spoken or written language, but of multimodal situated communication. An online version of the topic detection system has been developed and is currently being tested on our mobile robot BIRON. To demonstrate the feasibility of our approach, we present the results of an evaluation of our system on the BITT corpus.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-360"
  },
  "kaiser06_interspeech": {
   "authors": [
    [
     "Edward C.",
     "Kaiser"
    ],
    [
     "Paulo",
     "Barthelmess"
    ]
   ],
   "title": "Edge-splitting in a cumulative multimodal system, for a no-wait temporal threshold on information fusion, combined with an under-specified display",
   "original": "i06_2016",
   "page_count": 4,
   "order": 361,
   "p1": "paper 2016-Tue2CaP.12",
   "pn": "",
   "abstract": [
    "Predicting the end of user input turns in a multimodal system can be complex. User interactions vary across a spectrum from single, unimodal inputs to multimodal combinations delivered either simultaneously or sequentially. Early multimodal systems used a fixed duration temporal threshold to determine how long to wait for the next input before processing and integration. Several recent studies have proposed using dynamic or adaptive temporal thresholds to predict turn segmentation and thus achieve faster system response times. We introduce an approach that requires no temporal threshold. First we contrast current multimodal command interfaces to a new class of cumulativeobservant multimodal systems that we introduce. Within that new system class we show how our technique of edge-splitting combined with our strategy for under-specified, no-wait, visual feedback resolves parsing problems that underlie turn segmentation errors. Test results show a 46.2% significant reduction in multimodal recognition errors, compared to not using these techniques.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-361"
  },
  "hui06_interspeech": {
   "authors": [
    [
     "Pui-Yu",
     "Hui"
    ],
    [
     "Helen M.",
     "Meng"
    ]
   ],
   "title": "Joint interpretation of input speech and pen gestures for multimodal human-computer interaction",
   "original": "i06_1834",
   "page_count": 4,
   "order": 362,
   "p1": "paper 1834-Tue2CaP.13",
   "pn": "",
   "abstract": [
    "This paper describes out initial work in semantic interpretation of multimodal user input that consist of speech and pen gestures. We have designed and collected a multimodal corpus of over a thousand navigational inquiries around the Beijing area. We devised a processing sequence for extracting spoken references from the speech input (perfect transcripts) and interpreting each reference by generating a hypothesis list of possible semantics (i.e. locations). We also devised a processing sequence for interpreting pen gestures (pointing, circling and strokes) and generating a hypothesis list for every gesture. Partial interpretations from individual modalities are combined using Viterbi alignment, which enforces the constraints of temporal order and semantic compatibility constraints in its cost functions to generate an integrated interpretation across modalities for overall input. This approach can correctly interpret over 97% of the 322 multimodal inquiries in our test set.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-362"
  },
  "cournapeau06_interspeech": {
   "authors": [
    [
     "David",
     "Cournapeau"
    ],
    [
     "Tatsuya",
     "Kawahara"
    ],
    [
     "Kenji",
     "Mase"
    ],
    [
     "Tomoji",
     "Toriyama"
    ]
   ],
   "title": "Voice activity detector based on enhanced cumulant of LPC residual and on-line EM algorithm",
   "original": "i06_1375",
   "page_count": 4,
   "order": 363,
   "p1": "paper 1375-Tue3A1O.1",
   "pn": "",
   "abstract": [
    "This paper addresses the problem of segmenting audio data recorded with embedded devices for the purpose of intelligent sensing in the context of multi-modal interactions. We propose a real-time method for robust speech detection in natural, noisy environments. It is based on a fusion of high order statistics of the LPC residual and autocorrelation, and adopts an on-line version of Expectation Maximization algorithm for the classification. Experimental evaluations show that the proposed method provides better detection performance under different types of natural noises, working robustly against other voices in the context of multi-speaker interactive situations. As the proposed method is based on features which have a low computational cost, and has a small latency, it is suitable for real-time tracking applications.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-363"
  },
  "hugginsdaines06_interspeech": {
   "authors": [
    [
     "David",
     "Huggins-Daines"
    ],
    [
     "Alexander I.",
     "Rudnicky"
    ]
   ],
   "title": "A constrained baum-welch algorithm for improved phoneme segmentation and efficient training",
   "original": "i06_1580",
   "page_count": 4,
   "order": 364,
   "p1": "paper 1580-Tue3A1O.2",
   "pn": "",
   "abstract": [
    "We describe an extension to the Baum-Welch algorithm for training Hidden Markov Models that uses explicit phoneme segmentation to constrain the forward and backward lattice. The HMMs trained with this algorithm can be shown to improve the accuracy of automatic phoneme segmentation. In addition, this algorithm is significantly more computationally efficient than the full Baum-Welch algorithm, while producing models that achieve equivalent accuracy on a standard phoneme recognition task.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-364"
  },
  "valente06b_interspeech": {
   "authors": [
    [
     "Fabio",
     "Valente"
    ]
   ],
   "title": "Infinite models for speaker clustering",
   "original": "i06_1329",
   "page_count": 4,
   "order": 365,
   "p1": "paper 1329-Tue3A1O.3",
   "pn": "",
   "abstract": [
    "In this paper we propose the use of infinite models for the clustering of speakers. Speaker segmentation is obtained trough a Dirichlet Process Mixture (DPM) model which can be interpreted as a flexible model with an infinite a priori number of components. Learning is based on a Variational Bayesian approximation of the infinite sequence. DPM model is compared with fixed prior systems learned by ML/BIC, MAP/BIC and a Variational Bayesian method. Experiments are run on a speaker clustering task on the NIST-96 Broadcast News database.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-365"
  },
  "dines06_interspeech": {
   "authors": [
    [
     "John",
     "Dines"
    ],
    [
     "Jithendra",
     "Vepa"
    ],
    [
     "Thomas",
     "Hain"
    ]
   ],
   "title": "The segmentation of multi-channel meeting recordings for automatic speech recognition",
   "original": "i06_1548",
   "page_count": 4,
   "order": 366,
   "p1": "paper 1548-Tue3A1O.4",
   "pn": "",
   "abstract": [
    "One major research challenge in the domain of the analysis of meeting room data is the automatic transcription of what is spoken during meetings, a task which has gained considerable attention within the ASR research community through the NIST rich transcription evaluations conducted over the last three years. One of the major difficulties in carrying out automatic speech recognition (ASR) on this data is dealing with the challenging recording environment, which has instigated the development of novel audio pre-processing approaches. In this paper we present a system for the automatic segmentation of multiple-channel individual headset microphone (IHM) meeting recordings for automatic speech recognition. The system relies on an MLP classifier trained from several meeting room corpora to identify speech/non-speech segments of the recordings. We give a detailed analysis of the segmentation performance for a number of system configurations, with our best system achieving ASR performance on automatically generated segments within 1.3% (3.7% relative) of a manual segmentation of the data.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-366"
  },
  "kuo06_interspeech": {
   "authors": [
    [
     "Jen-Wei",
     "Kuo"
    ],
    [
     "Hsin-Min",
     "Wang"
    ]
   ],
   "title": "Minimum boundary error training for automatic phonetic segmentation",
   "original": "i06_1497",
   "page_count": 4,
   "order": 367,
   "p1": "paper 1497-Tue3A1O.5",
   "pn": "",
   "abstract": [
    "Annotated speech corpora are indispensable to various areas of speech research. In this paper, we present a novel discriminative training approach for HMM-based automatic phonetic segmentation. The objective of the proposed minimum boundary error (MBE) discriminative training approach is to minimize the expected boundary errors over a set of phonetic alignments represented as a phonetic lattice. This approach is inspired by the recently proposed minimum phone error (MPE) training algorithm for automatic speech recognition. To evaluate the MBE training approach, we conducted automatic phonetic segmentation experiments on the TIMIT acoustic-phonetic continuous speech corpus. The MBE-trained HMMs can identify 79.75% of human-labeled phone boundaries within a tolerance of 10 ms, compared to 71.23% identified by the conventional ML-trained HMMs. Moreover, by using the MBE-trained HMMs, only 7.89% of automatically labeled phone boundaries have errors larger than 20 ms.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-367"
  },
  "schuler06_interspeech": {
   "authors": [
    [
     "William",
     "Schuler"
    ],
    [
     "Tim",
     "Miller"
    ],
    [
     "Stephen",
     "Wu"
    ],
    [
     "Andrew",
     "Exley"
    ]
   ],
   "title": "Dynamic evidence models in a DBN phone recognizer",
   "original": "i06_1770",
   "page_count": 4,
   "order": 368,
   "p1": "paper 1770-Tue3A1O.6",
   "pn": "",
   "abstract": [
    "This paper describes an implementation of a discriminative acoustical model - a Conditional Random Field (CRF) - within a Dynamic Bayes Net (DBN) formulation of a Hierarchic Hidden Markov Model (HHMM) phone recognizer. This CRF-DBN topology accounts for phone transition dynamics in conditional probability distributions over random variables associated with observed evidence, and therefore has less need for hidden variable states corresponding to transitions between phones, leaving more hypothesis space available for modeling higherlevel linguistic phenomena such syntax and semantics. The model also has the interesting property that it explicitly represents likely formant trajectories and formant targets of modeled phones in its random variable distributions, making it more linguistically transparent than models based on traditional HMMs with conditionally independent evidence variables. Results on the standard TIMIT phone recognition task show this CRF evidence model, even with a relatively simple first-order feature set, is competitive with standard HMMs and DBN variants using static Gaussian mixture models on MFCC features.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-368"
  },
  "ramabhadran06_interspeech": {
   "authors": [
    [
     "B.",
     "Ramabhadran"
    ],
    [
     "Olivier",
     "Siohan"
    ],
    [
     "L.",
     "Mangu"
    ],
    [
     "G.",
     "Zweig"
    ],
    [
     "M.",
     "Westphal"
    ],
    [
     "H.",
     "Schulz"
    ],
    [
     "A.",
     "Soneiro"
    ]
   ],
   "title": "The IBM 2006 speech transcription system for european parliamentary speeches",
   "original": "i06_2027",
   "page_count": 4,
   "order": 369,
   "p1": "paper 2027-Tue3A2O.1",
   "pn": "",
   "abstract": [
    "TC-STAR is an European Union funded speech to speech translation project to transcribe, translate and synthesize European Parliamentary Plenary Speeches (EPPS). This paper describes IBMs English and Spanish speech recognition systems submitted to the TC-STAR 2006 Evaluation. The technical advances in this submission include two different algorithms for automatic segmentation and speaker clustering of the input audio; a system architecture that is based on cross-adaptation across these two segmentation schemes and system combination through generation of an ensemble of systems using randomized decision tree state-tying; automatic punctuation of the speech recognition output; and the incorporation of an additional 35 hours of in-domain EPPS acoustic training data. These advances reduced the error rate by 30% relative over the best-performing system in the TC-STAR 2005 Evaluation on the 2006 English development test set, and produced one of the best performing systems on the 2006 evaluation in English with a word error rate of 8.3%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-369"
  },
  "fugen06_interspeech": {
   "authors": [
    [
     "Christian",
     "Fügen"
    ],
    [
     "Matthias",
     "Wölfel"
    ],
    [
     "John W.",
     "McDonough"
    ],
    [
     "Shajith",
     "Ikbal"
    ],
    [
     "Florian",
     "Kraft"
    ],
    [
     "Kornel",
     "Laskowski"
    ],
    [
     "Mari",
     "Ostendorf"
    ],
    [
     "Sebastian",
     "Stüker"
    ],
    [
     "Kenichi",
     "Kumatani"
    ]
   ],
   "title": "Advances in lecture recognition: the ISL RT-06s evaluation system",
   "original": "i06_1415",
   "page_count": 4,
   "order": 370,
   "p1": "paper 1415-Tue3A2O.2",
   "pn": "",
   "abstract": [
    "This paper describes the 2006 lecture recognition system developed at the Interactive Systems Laboratories (ISL), for individual headmicrophone (IHM), single distant microphone (SDM), and multiple distant microphones (MDM) conditions. It was evaluated in RT-06S rich transcription meeting evaluation sponsored by the US National Institute of Standards and Technologies (NIST). We describe the principal differences between our current system and those submitted in previous years, namely, improved acoustic and language models, cross adaptation between systems with different front-ends and phoneme sets, and the use of various automatic speech segmentation algorithms. Our system achieved word error rates of 38.5% (53.4%) and 22.9% (32.2%), respectively, on the MDM and IHM conditions of the RT-05S (RT-06S) lecture evaluation set.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-370"
  },
  "hwang06_interspeech": {
   "authors": [
    [
     "Mei-Yuh",
     "Hwang"
    ],
    [
     "Xin",
     "Lei"
    ],
    [
     "Wen",
     "Wang"
    ],
    [
     "Takahiro",
     "Shinozaki"
    ]
   ],
   "title": "Investigation on Mandarin broadcast news speech recognition",
   "original": "i06_1916",
   "page_count": 4,
   "order": 371,
   "p1": "paper 1916-Tue3A2O.3",
   "pn": "",
   "abstract": [
    "This paper describes our efforts in building a competitive Mandarin broadcast news speech recognizer. We successfully incorporated the most popular speech technologies into our system. More importantly, we present two novel algorithms in smoothing pitch features and segmenting Chinese characters into word units. Additionally, we propose to borrow the principle of pointwise mutual information for creating a Chinese word lexicon automatically. Our final system achieved 6.0% character error rate (CER) on dev04 and 16.0% on eval04, with simpler acoustic models, less training data, and simpler decoding architecture compared with other state-of-the-art systems, yet was equally competitive.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-371"
  },
  "lei06c_interspeech": {
   "authors": [
    [
     "Xin",
     "Lei"
    ],
    [
     "Manhung",
     "Siu"
    ],
    [
     "Mei-Yuh",
     "Hwang"
    ],
    [
     "Mari",
     "Ostendorf"
    ],
    [
     "Tan",
     "Lee"
    ]
   ],
   "title": "Improved tone modeling for Mandarin broadcast news speech recognition",
   "original": "i06_1752",
   "page_count": 4,
   "order": 372,
   "p1": "paper 1752-Tue3A2O.4",
   "pn": "",
   "abstract": [
    "Tone has a crucial role in Mandarin speech in distinguishing ambiguous words. Most state-of-the-art Mandarin automatic speech recognition systems adopt embedded tone modeling, where tonal acoustic units are used and F0 features are appended to the spectral feature vector. In this paper, we combine the embedded approach (using improved F0 smoothing) with explicit tone modeling in rescoring the output lattices. Oracle experiments indicate 32% relative improvement can be achieved by rescoring with perfect tone information. Recognition experiments on Mandarin broadcast news show that, even with an accuracy of only 70%, the explicit tone classifier offers complementary knowledge and improves performance significantly. Through the combination of tone modeling techniques, the character error rate on the CTV test set can be improved from 13.0% to 11.5%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-372"
  },
  "huang06b_interspeech": {
   "authors": [
    [
     "Jui-Ting",
     "Huang"
    ],
    [
     "Lin-shan",
     "Lee"
    ]
   ],
   "title": "Prosodic modeling in large vocabulary Mandarin speech recognition",
   "original": "i06_1546",
   "page_count": 4,
   "order": 373,
   "p1": "paper 1546-Tue3A2O.5",
   "pn": "",
   "abstract": [
    "The issue of incorporating prosodic information into speech recognition processes has emerged in recent years. In this work we present a complete framework for Mandarin speech recognition with prosodic modeling considering two-level hierarchical prosodic information for Mandarin Chinese. We developed a GMM-based, a decision-tree-based, and a hybrid approach. The best improvements in character recognition accuracy were obtained by the decision-tree-based prosodic models. This approach does NOT require a training corpus labeled with prosodic features, and works reasonably for a large-scale multispeaker task.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-373"
  },
  "sun06_interspeech": {
   "authors": [
    [
     "Ying",
     "Sun"
    ],
    [
     "Daniel",
     "Willett"
    ],
    [
     "Raymond",
     "Brueckner"
    ],
    [
     "Rainer",
     "Gruhn"
    ],
    [
     "Dirk",
     "Bühler"
    ]
   ],
   "title": "Experiments on Chinese speech recognition with tonal models and pitch estimation using the Mandarin speecon data",
   "original": "i06_1452",
   "page_count": 4,
   "order": 374,
   "p1": "paper 1452-Tue3A2O.6",
   "pn": "",
   "abstract": [
    "Automatic speech recognition of a tonal and syllabic language such as Chinese Mandarin poses new challenges but also offers new opportunities. We present approaches and experimental results concerning the choice of base units for acoustic modeling, pitch estimation and how to integrate pitch estimates into the modeling framework. The experimental evaluations are carried out both on rather clean headset data and on noisy and reverberant distant talking speech data. Results show that tonal base units offer a word error rate reduction of more than 30% compared to toneless base units. This holds for both phoneme models and initial-final models. The integration of pitch as an additional feature stream yields another remarkable improvement of more than 20% over the best tonal baseline system. In a two-stream modeling approach, the pitch stream distributions can be strongly tied such that the overall model size increases only very moderately.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-374"
  },
  "beskow06_interspeech": {
   "authors": [
    [
     "Jonas",
     "Beskow"
    ],
    [
     "Björn",
     "Granström"
    ],
    [
     "David",
     "House"
    ]
   ],
   "title": "Visual correlates to prominence in several expressive modes",
   "original": "i06_1922",
   "page_count": 4,
   "order": 375,
   "p1": "paper 1922-Tue3WeO.1",
   "pn": "",
   "abstract": [
    "In this paper, we present measurements of visual, facial parameters obtained from a speech corpus consisting of short, read utterances in which focal accent was systematically varied. The utterances were recorded in a variety of expressive modes including certain, confirming, questioning, uncertain, happy, angry and neutral. Results showed that in all expressive modes, words with focal accent are accompanied by a greater variation of the facial parameters than are words in non-focal positions. Moreover, interesting differences between the expressions in terms of different parameters were found.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-375"
  },
  "barkhuysen06_interspeech": {
   "authors": [
    [
     "Pashiera",
     "Barkhuysen"
    ],
    [
     "Emiel",
     "Krahmer"
    ],
    [
     "Marc",
     "Swerts"
    ]
   ],
   "title": "How auditory and visual prosody is used in end-of-utterance detection",
   "original": "i06_1238",
   "page_count": 4,
   "order": 376,
   "p1": "paper 1238-Tue3WeO.2",
   "pn": "",
   "abstract": [
    "In this paper, we describe a series of perception studies using visual and auditory cues to end-of-utterance. Fragments were taken from a recorded interview session, consisting of the parts in which speakers provided answers. Final and non-final parts of these fragments were used, varying in length. The subjects had to assess whether the speaker had finished his or her turn, based upon these fragments. The fragments were presented in 3 modalities: either a bimodal presentation mode (both auditory and visually), or in only the auditory or the visual mode. Results show that the audio-visual condition evoked the highest proportion of correct classifications and the auditory condition the lowest. Thus, the combination of modalities clearly works best. Also, non-final fragments are classified better than final ones, and longer fragments are classified better than short ones. It furthermore appears that these factors are different for different modalities: longer fragments are better classified in the auditory modality, while for short fragments the visual modality works better. This suggests that people may make more use of global cues in the auditory modality, while for the visual modality local cues are sufficient.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-376"
  },
  "swerts06_interspeech": {
   "authors": [
    [
     "Marc",
     "Swerts"
    ],
    [
     "Emiel",
     "Krahmer"
    ]
   ],
   "title": "The importance of different facial areas for signalling visual prominence",
   "original": "i06_1289",
   "page_count": 4,
   "order": 377,
   "p1": "paper 1289-Tue3WeO.3",
   "pn": "",
   "abstract": [
    "This article discusses the processing of facial markers of prominence in spoken utterances. In particular, it investigates which area of a speakers face contains the strongest cues to prominence, using stimuli with the entire face visible or versions in which participants could only see the upper or lower half, or the right or left part of the face. To compensate for potential ceiling effects, subjects were positioned at a distance of either 50cm, 250cm or 380cm from the screen which displayed the film fragments. The task of the subjects was to indicate for each stimulus which word they perceived as the most prominent one. Results show that, while prominence detection becomes more difficult at longer distances, the upper facial area has stronger cue value for prominence detection than the bottom part, and that the left part of the face is more important than the right part. Results of mirror-images of the original fragments show that this latter result is due both to a speaker and an observer effect.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-377"
  },
  "chaloupka06_interspeech": {
   "authors": [
    [
     "Josef",
     "Chaloupka"
    ]
   ],
   "title": "Visual speech segmentation and speaker recognition for transcription of TV news",
   "original": "i06_1485",
   "page_count": 4,
   "order": 378,
   "p1": "paper 1485-Tue3WeO.4",
   "pn": "",
   "abstract": [
    "This paper is about a method for visual segmentation of TV news. The TV news shows are segmented according to the visual stream from the video TV recordings in this method. Human faces are found in the single visual segments with the help of the fast algorithm for face detection. The found faces are compared with the visual GMMs, that have been trained from the video picture of the single broadcasters (anchors) from the TV news. The single visual segments, where the faces of the broadcasters have been found and recognized, have been compared with the acoustic segments from the acoustic segmentation. The speaker adapted HMMs have been used for speech recognition of these acoustic segments. The recognition rate is better for the use of this speaker-adapted HMMs compared to the use of the speaker independent HMMs. It is possible to use the methods for the speaker identification and verification from the acoustic signal in the acoustic segments. The results from the visual speaker identification will be better for smaller number of speakers and for the use of the video recordings of TV news with a lot of noise in the acoustic signal.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-378"
  },
  "cortes06_interspeech": {
   "authors": [
    [
     "G.",
     "Cortés"
    ],
    [
     "L.",
     "García"
    ],
    [
     "Carmen",
     "Benítez"
    ],
    [
     "José C.",
     "Segura"
    ]
   ],
   "title": "HMM-based continuous sign language recognition using a fast optical flow parameterization of visual information",
   "original": "i06_1543",
   "page_count": 4,
   "order": 379,
   "p1": "paper 1543-Tue3WeO.5",
   "pn": "",
   "abstract": [
    "This paper presents a preliminary study of an optical flow-based parameterization of visual information in a sign language recognition system using Hidden Markov Models (HMM). Current feature extraction processes need initialization, tracking and segmentation stages in order to describe signer gestures. Our aim is to develop a single and fast technique to reduce computational complexity which doesnt require these stages and is able to work in mobile devices with limited hardware resources. The Moving Block Distance (MBD) parameterization is an interesting first approach for this purpose, proved by two signers under a static background constraint. A lexicon of 33 basic word units (signemes) was used to build the data set containing phrases with a variable number of words. Continuous recognition results achieve more than 99% accuracy in close test.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-379"
  },
  "shao06_interspeech": {
   "authors": [
    [
     "Xu",
     "Shao"
    ],
    [
     "Jon",
     "Barker"
    ]
   ],
   "title": "Audio-visual speech recognition in the presence of a competing speaker",
   "original": "i06_1589",
   "page_count": 4,
   "order": 380,
   "p1": "paper 1589-Tue3WeO.6",
   "pn": "",
   "abstract": [
    "This paper examines the problem of estimating stream weights for a multistream audio-visual speech recogniser in the context of a simultaneous speaker task. The task is challenging because signal-tonoise ratio (SNR) cannot be readily inferred from the acoustics alone. The method proposed employs artificial neural networks (ANNs) to estimate the SNR from HMM state-likelihoods. SNR is converted to stream weight using a mapping optimised on development data. The method produces an audio-visual recognition performance better than that of both the audio-only and the video-only baselines across a wide range of SNRs. The performance using SNR estimates based on audio state-likelihoods is compared to that obtained using both audio and visual likelihoods. Although the audio-visual SNR estimator outperforms the audio-only SNR estimator, the recognition performance benefit is small. Ideas for making fuller use of the visual information are discussed.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-380"
  },
  "strom06_interspeech": {
   "authors": [
    [
     "Volker",
     "Strom"
    ],
    [
     "Robert A. J.",
     "Clark"
    ],
    [
     "Simon",
     "King"
    ]
   ],
   "title": "Expressive prosody for unit-selection speech synthesis",
   "original": "i06_1522",
   "page_count": 4,
   "order": 381,
   "p1": "paper 1522-Tue3BuP.1",
   "pn": "",
   "abstract": [
    "Current unit selection speech synthesis voices cannot produce emphasis or interrogative contours because of a lack of the necessary prosodic variation in the recorded speech database. A method of recording script design is proposed which addresses this shortcoming. Appropriate components were added to the target cost function of the Festival Multisyn engine, and a perceptual evaluation showed a clear preference over the baseline system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-381"
  },
  "carlson06_interspeech": {
   "authors": [
    [
     "Rolf",
     "Carlson"
    ],
    [
     "Kjell",
     "Gustafson"
    ],
    [
     "Eva",
     "Strangert"
    ]
   ],
   "title": "Cues for hesitation in speech synthesis",
   "original": "i06_1516",
   "page_count": 4,
   "order": 382,
   "p1": "paper 1516-Tue3BuP.2",
   "pn": "",
   "abstract": [
    "The current study investigates acoustic correlates to perceived hesitation based on previous work showing that pause duration and final lengthening both contribute to the perception of hesitation. It is the total duration increase that is the valid cue rather than the contribution by either factor. The present experiment using speech synthesis was designed to evaluate F0 slope and presence vs. absence of creaky voice before the inserted hesitation in addition to durational cues. The manipulations occurred in two syntactic positions, within a phrase and between two phrases, respectively. The results showed that in addition to durational increase, variation of both F0 slope and creaky voice had perceptual effects, although to a much lesser degree. The results have a bearing on efforts to model spontaneous speech including disfluencies, to be explored, for example, in spoken dialogue systems.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-382"
  },
  "alias06_interspeech": {
   "authors": [
    [
     "Francesc",
     "Alías"
    ],
    [
     "Joan Claudi",
     "Socoró"
    ],
    [
     "Xavier",
     "Sevillano"
    ],
    [
     "Ignasi",
     "Iriondo"
    ],
    [
     "Xavier",
     "Gonzalvo"
    ]
   ],
   "title": "Multi-domain text-to-speech synthesis by automatic text classification",
   "original": "i06_1579",
   "page_count": 4,
   "order": 383,
   "p1": "paper 1579-Tue3BuP.3",
   "pn": "",
   "abstract": [
    "This paper describes a multi-domain text-to-speech (MD-TTS) synthesis strategy for generating speech among different domains and so increasing the flexibility of high quality TTS systems. To that effect, the MD-TTS introduces a flexible TTS architecture that includes an automatic domain classification module, which allows MD-TTS systems to be implemented by different synthesis strategies and speech corpus typologies. In this work, the performance of a corpus-based MD-TTS system is subjectively validated by means of several perceptual tests.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-383"
  },
  "yi06b_interspeech": {
   "authors": [
    [
     "Lifu",
     "Yi"
    ],
    [
     "Jian",
     "Li"
    ],
    [
     "Xiaoyan",
     "Lou"
    ],
    [
     "Jie",
     "Hao"
    ]
   ],
   "title": "Phrase break prediction using logistic generalized linear model",
   "original": "i06_1468",
   "page_count": 4,
   "order": 384,
   "p1": "paper 1468-Tue3BuP.4",
   "pn": "",
   "abstract": [
    "In this paper we propose a novel phrase break prediction model for Mandarin speech synthesis. It is generalized linear models (GLM) with stepwise regression solution. We assume phrase break obeys Bernoulli distribution and then model phrase break probability by Logistic GLM. The attribute set is automatically selected by stepwise regression, which is a totally data-driven method. We also introduce speaking rate as a new attribute for prediction. The proposed method is applied to 2,150 utterances of the Mandarin speech corpus, and it achieves 5.4% higher performances than CART method in open test. The method can be extended to include more linguistic and prosodic attributes and it is very compact for application.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-384"
  },
  "clark06_interspeech": {
   "authors": [
    [
     "Robert A. J.",
     "Clark"
    ],
    [
     "Simon",
     "King"
    ]
   ],
   "title": "Joint prosodic and segmental unit selection speech synthesis",
   "original": "i06_1262",
   "page_count": 4,
   "order": 385,
   "p1": "paper 1262-Tue3BuP.5",
   "pn": "",
   "abstract": [
    "We describe a unit selection technique for text-to-speech synthesis which jointly searches the space of possible diphone sequences and the space of possible prosodic unit sequences in order to produce synthetic speech with more natural prosody. We demonstrates that this search, although currently computationally expensive, can achieve improved intonation compared to a baseline in which only the space of possible diphone sequences is searched. We discuss ways in which the search could be made sufficiently efficient for use in a real-time system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-385"
  },
  "kim06b_interspeech": {
   "authors": [
    [
     "Yeon-Jun",
     "Kim"
    ],
    [
     "Ann K.",
     "Syrdal"
    ],
    [
     "Alistair",
     "Conkie"
    ],
    [
     "Mark C.",
     "Beutnagel"
    ]
   ],
   "title": "Phonetically enriched labeling in unit selection TTS synthesis",
   "original": "i06_2055",
   "page_count": 4,
   "order": 386,
   "p1": "paper 2055-Tue3BuP.6",
   "pn": "",
   "abstract": [
    "Unit selection techniques have improved the quality of text-to-speech (TTS) synthesis. However, mistakes which had been less noticeable previously in poorer quality synthetic speech become very noticeable in more natural-sounding synthetic speech. Many problems appear to be caused by mismatches between phones requested by the TTS front-end and phones selected from the labeled speech inventory. Given the input text and the added information predicted by the TTS front-end, finding the optimal units from a speech inventory database still remains a challenge in unit selection TTS synthesis. Consonants in American English affect intelligibility of speech synthesis and they are realized differently depending on their position in the syllable. Pre-vocalic plosives must have a release burst before the vowel begins while post-vocalic consonants may or may not be released. When a post-vocalic consonant is chosen to synthesize a pre-vocalic consonant, it may cause problems such as missing consonants, consonant confusion or word-boundary confusion. In this paper, a new phone labeling method which differentiates pre-vocalic and post-vocalic consonants is proposed. The proposed phone labeling method leads unit selection to choose contextually accurate phone units and minimizes unit selection errors caused by lack of specification in TTS front-end transcriptions and phone labels in the speech inventory. In a listening test the TTS voices labeled with the pre-vocalic / post-vocalic distinction were rated significantly higher (+0.33) compared to reference voices that did not use this distinction.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-386"
  },
  "bellegarda06_interspeech": {
   "authors": [
    [
     "Jerome R.",
     "Bellegarda"
    ]
   ],
   "title": "Further developments in LSM-based boundary training for unit selection TTS",
   "original": "i06_1142",
   "page_count": 4,
   "order": 387,
   "p1": "paper 1142-Tue3BuP.7",
   "pn": "",
   "abstract": [
    "The level of quality that can be achieved in concatenative text-to-speech synthesis depends, among other things, on a judicious segmentation of all units in the underlying unit selection inventory. We have recently advocated the iterative refinement of unit boundaries based on a data-driven feature extraction framework separately optimized for each boundary region [1]. This paper presents the formal proof of convergence of the iterative algorithm, as well as a detailed analysis of its potential benefits for concatenative TTS synthesis. A formal listening test, in particular, underscores the practical viability of the approach for unit boundary optimization.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-387"
  },
  "nose06_interspeech": {
   "authors": [
    [
     "Takashi",
     "Nose"
    ],
    [
     "Junichi",
     "Yamagishi"
    ],
    [
     "Takao",
     "Kobayashi"
    ]
   ],
   "title": "A style control technique for speech synthesis using multiple regression HSMM",
   "original": "i06_1184",
   "page_count": 4,
   "order": 388,
   "p1": "paper 1184-Tue3BuP.8",
   "pn": "",
   "abstract": [
    "This paper presents a technique for controlling intuitively the degree or intensity of speaking styles and emotional expressions of synthetic speech. The conventional style control technique based on multiple regression HMM (MRHMM) has a problem that it is difficult to control phone duration of synthetic speech because HMM has no explicit parameter which models phone duration appropriately. To overcome this problem, we use multiple regression hidden semi-Markov model (MRHSMM) which has explicit state duration distributions to control phone duration. We show that the duration control is important for style control of synthetic speech from the results of subjective tests. We also compare the proposed technique with another control technique based on model interpolation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-388"
  },
  "ogata06_interspeech": {
   "authors": [
    [
     "Katsumi",
     "Ogata"
    ],
    [
     "Makoto",
     "Tachibana"
    ],
    [
     "Junichi",
     "Yamagishi"
    ],
    [
     "Takao",
     "Kobayashi"
    ]
   ],
   "title": "Acoustic model training based on linear transformation and MAP modification for HSMM-based speech synthesis",
   "original": "i06_1787",
   "page_count": 4,
   "order": 389,
   "p1": "paper 1787-Tue3BuP.9",
   "pn": "",
   "abstract": [
    "This paper describes the use of combined linear regression and expost MAP methods for average-voice-based speech synthesis system based on HMM. To generate more natural sounding speech using the average-voice-based speech synthesis system when a large amount of training data is available, we apply ex-post MAP estimation after the linear transformation based adaptation. We investigate how the amount of data used in the training of the average voice model and the tying topology affect the naturalness of synthetic speech. From the results of evaluation tests, we show that the adapted average voice model trained using a large amount of data can generate more natural sounding speech than the speaker dependent model.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-389"
  },
  "abdelhamid06_interspeech": {
   "authors": [
    [
     "Ossama",
     "Abdel-Hamid"
    ],
    [
     "Sherif Mahdy",
     "Abdou"
    ],
    [
     "Mohsen",
     "Rashwan"
    ]
   ],
   "title": "Improving Arabic HMM based speech synthesis quality",
   "original": "i06_1693",
   "page_count": 4,
   "order": 390,
   "p1": "paper 1693-Tue3BuP.10",
   "pn": "",
   "abstract": [
    "HMM based speech synthesis, where speech parameters are generated directly from HMM models, is a new technique relative to other speech synthesis techniques. In this paper, we propose some modifications to the basic system to improve its quality. We apply a multi-band excitation model. And we use samples extracted from the spectral envelop as spectral parameters. In the synthesis, the voiced and unvoiced speech parts are mixed according to bands voicing parameters. The voiced part is generated based on a harmonic sinusoidal model. Experimental tests performed on Arabic dataset show that the applied modifications improved the quality.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-390"
  },
  "homayounpour06_interspeech": {
   "authors": [
    [
     "M. Mehdi",
     "Homayounpour"
    ],
    [
     "Majid",
     "Namnabat"
    ]
   ],
   "title": "Farsbayan: a unit selection based Farsi speech synthesizer",
   "original": "i06_1997",
   "page_count": 4,
   "order": 391,
   "p1": "paper 1997-Tue3BuP.11",
   "pn": "",
   "abstract": [
    "In recent years, the unit selection-based concatenative speech synthesis method using a large corpus has attracted great attention. This method provides more natural quality speech compared to the parameter driven methods. The Formant Synthesis, HNM method and use of MLSA filter are the prevalent methods for synthesizing Farsi speech. In this paper, we present the structure of a proposed unit selection synthesizer for Farsi language. In the proposed system, the linear regression method has been used for determination of weights of discrete sub-costs in the target cost, while the weights of other sub-costs have been considered constant. We have also presented a pre-selection algorithm using adaptive threshold for pruning the units. In addition, the efficiency of TD-PSOLA algorithm in improvement of resulting speech quality has been studied. Informal tests show the degrading effect of this algorithm on the output quality. The output speech was found to be remarkably fluent and natural. The quality of the output speech has been evaluated using MOS subjective test, and we have obtained a MOS test value of 3.8 for overall quality.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-391"
  },
  "anberbir06_interspeech": {
   "authors": [
    [
     "Tadesse",
     "Anberbir"
    ],
    [
     "Tomio",
     "Takara"
    ]
   ],
   "title": "Amharic speech synthesis using cepstral method with stress generation rule",
   "original": "i06_1107",
   "page_count": 4,
   "order": 392,
   "p1": "paper 1107-Tue3BuP.12",
   "pn": "",
   "abstract": [
    "Amharic is the official language of Ethiopia. In this paper, we present our study on Amharic stress. Stress (Gemination of consonants) in Amharic language is very important for proper pronunciation of words. It is also one of the most distinctive characteristics of the rhythm of the speech. We discuss a method employed for generating stressed syllables from unstressed syllables, and its application to our speech synthesizer. First, we analyzed waveforms of minimal pair words concerned with stressed and unstressed syllables into the time patterns of pitch, power and spectrum. Then, by combining or exchanging these patterns, speech sounds were synthesized. Using the synthesized sounds, listening tests were performed to examine the acoustic correlates of stress among pitch, spectrum, power and duration. We found that consonants duration is the most important factor. A further listening test was performed to determine the threshold of duration of consonants between unstressed and stressed syllables, and we observed that 50ms is the average threshold duration for voiced consonants and 70ms is for unvoiced consonants.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-392"
  },
  "thangthai06_interspeech": {
   "authors": [
    [
     "Ausdang",
     "Thangthai"
    ],
    [
     "Chatchawarn",
     "Hansakunbuntheung"
    ],
    [
     "Rungkarn",
     "Siricharoenchai"
    ],
    [
     "Chai",
     "Wutiwiwatchai"
    ]
   ],
   "title": "Automatic syllable-pattern induction in statistical Thai text-to-phone transcription",
   "original": "i06_1964",
   "page_count": 4,
   "order": 393,
   "p1": "paper 1964-Tue3BuP.13",
   "pn": "",
   "abstract": [
    "This paper proposes a technique of automatic syllable-pattern induction in statistical Thai text-to-phone transcription. A general process of building a statistical text-to-phone transcription is to first define a set of rules describing syllable patterns, which is used for syllabification. Given an input text, the syllabification process generates all possible syllable sequences, which are then scored and selected using a statistical model. Updating the handcrafted rule set of syllable patterns is time-consuming and requires expert linguists. Instead of the manual process, automatic induction of new syllable patterns from a large raw text if proposed. The process that can deal with raw text is particularly needed for Thai as segmenting Thai text is a very tedious task. Experiments show that the proposed Thai text-to-phone transcription system after applying a large raw text for syllable-pattern induction achieves approximately 2% improvement. A comparison with other Thai text-to-phone transcription models and error analyses are also given in the paper.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-393"
  },
  "oosthuizen06_interspeech": {
   "authors": [
    [
     "H. J.",
     "Oosthuizen"
    ],
    [
     "S. T.",
     "Phihlela"
    ],
    [
     "M. J. D.",
     "Manamela"
    ]
   ],
   "title": "Development of prototype text-to-speech systems for northern sotho",
   "original": "i06_1070",
   "page_count": 4,
   "order": 394,
   "p1": "paper 1070-Tue3BuP.14",
   "pn": "",
   "abstract": [
    "Two text-to-speech synthesis systems were developed for one of the eleven official languages of South Africa, viz. Northern Sotho. A diphone synthesis system, based on extraction of diphones from nonsense words, was constructed. A cluster unit selection synthesis system, based on recordings of sentences containing a selection of most common words in Northern Sotho, was also built. The Festival speech synthesis system was used for both systems. Both of these systems performed well in a subjective evaluation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-394"
  },
  "you06_interspeech": {
   "authors": [
    [
     "Jiali",
     "You"
    ],
    [
     "Yining",
     "Chen"
    ],
    [
     "Min",
     "Chu"
    ],
    [
     "Yong",
     "Zhao"
    ],
    [
     "Jinlin",
     "Wang"
    ]
   ],
   "title": "Identify language origin of personal names with normalized appearance number of web pages",
   "original": "i06_1353",
   "page_count": 4,
   "order": 395,
   "p1": "paper 1353-Tue3BuP.15",
   "pn": "",
   "abstract": [
    "Identifying the language origin of a personal name without context is interesting and useful in many areas. Morphological structure, which has long been considered as the main source of language origin information, is modeled by N-grams of letters or letter chunks. In this paper, we introduce a new information source, the appearance number of a name in web pages of different languages, for identifying its language origin. Since the distribution of web pages in various languages is not identical, and the state-of-the-art search engines can only provide the number of pages that contain the queried words, we propose a method to normalize the appearance number obtained from a search engine and use it as a new feature. When this new feature is used independently to identify language origin of names among four closely related languages (English, German, French, and Portuguese), the error rate is 26.9%, which is comparable to that of letter 4-gram features. When it is used together with the letter N-gram models, the error rate is reduced to 14.2%, which is about 43.2% error reduction, compared with the letter 4-gram based baseline model.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-395"
  },
  "weiss06_interspeech": {
   "authors": [
    [
     "Christian",
     "Weiss"
    ],
    [
     "Wolfgang",
     "Hess"
    ]
   ],
   "title": "Conditional random fields for hierarchical segment selection in text-to-speech synthesis",
   "original": "i06_1090",
   "page_count": 4,
   "order": 396,
   "p1": "paper 1090-Wed3BuP.1",
   "pn": "",
   "abstract": [
    "In this paper we present the statistically motivated conditional random fields (CRF) approach to concatenative TTS. We use contextual CRFs for speech segment selection where we concatenate the selected segments to an acoustic speech waveform. The CRF approach is used in our corpus-based TTS system AVISS. The acoustic synthesis module consists of trained context dependent CRF models on a multi-level acoustic unit inventory where we apply a hierarchical top-down search to select appropriate segments. The acoustic synthesis is easily adaptable to other languages while there is only the need of a language specific module for text and symbolic preprocessing as well as duration and F0 prediction which can be performed by a prosodic module. The system shows good results in the generated speech waveforms. The CRF approach is usable for acoustic units as well as a parametric synthesis where the speech parameters are generated by CRFs and the speech waveform is produced by a synthesis filter.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-396"
  },
  "krul06_interspeech": {
   "authors": [
    [
     "Aleksandra",
     "Krul"
    ],
    [
     "Géraldine",
     "Damnati"
    ],
    [
     "François",
     "Yvon"
    ],
    [
     "Thierry",
     "Moudenc"
    ]
   ],
   "title": "Corpus design based on the kullback-leibler divergence for text-to-speech synthesis application",
   "original": "i06_1647",
   "page_count": 4,
   "order": 397,
   "p1": "paper 1647-Wed3BuP.2",
   "pn": "",
   "abstract": [
    "This paper presents a corpus design method for Text-To-Speech (TTS) synthesis application. The aim of this method is to build a corpus whose unit distribution approximates a given target distribution. Corpus selection can be expressed as a set covering problem, which is known to be NP-complete: we therefore resort to a heuristic approach, based on greedy algorithm. We propose the Kullback-Leibler divergence to guide the iterative selection of candidate sentences: indeed, this criterion gives the possibility to control the unit distribution at each step of the algorithm. We first show how to efficiently update, in an incremental manner, this criterion. We then present and discuss experimental results, where our selection algorithm is compared, for various unit sets, with alternative selection criteria.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-397"
  },
  "ling06_interspeech": {
   "authors": [
    [
     "Zhen-Hua",
     "Ling"
    ],
    [
     "Ren-Hua",
     "Wang"
    ]
   ],
   "title": "HMM-based unit selection using frame sized speech segments",
   "original": "i06_1104",
   "page_count": 4,
   "order": 398,
   "p1": "paper 1104-Wed3BuP.3",
   "pn": "",
   "abstract": [
    "This paper presents a hidden Markov model (HMM) based unit selection method for concatenative speech synthesis system. Frame sized waveform segments are adopted as basic synthesis units here to increase the coverage rate of candidate units and the chance of finding appropriate ones. In training stage, a set of contextual dependent HMMs are trained with static and dynamic acoustic features. When synthesizing a sentence, the optimal frame sequence is searched out from speech corpus by maximizing the output probability of a sentence HMM constructed according to the contextual information of input text. Listening test proves that proposed method can achieve better performance of synthesized speech compared with the method using state sized units and cost function criterion.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-398"
  },
  "taylor06_interspeech": {
   "authors": [
    [
     "Paul",
     "Taylor"
    ]
   ],
   "title": "The target cost formulation in unit selection speech synthesis",
   "original": "i06_1455",
   "page_count": 4,
   "order": 399,
   "p1": "paper 1455-Wed3BuP.4",
   "pn": "",
   "abstract": [
    "We review the various approaches that have been used to define the target cost in unit selection speech synthesis and show that there are a number of different and sometimes incompatible ways of defining this. We propose that this cost should be thought of as a measure of how similar two units sound to a human listener. We discuss the issue of what features should be used in unit selection and the pros and cons of using derived features such as F0. We then explore some algorithms used to calculate target costs and show that none are really ideal for the problem. Finally, we propose a new solution to this that uses a neural network to synthesise points in acoustic space around which we can build new clusters of units at run time.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-399"
  },
  "tihelka06_interspeech": {
   "authors": [
    [
     "Daniel",
     "Tihelka"
    ],
    [
     "Jindrich",
     "Matousek"
    ]
   ],
   "title": "Unit selection and its relation to symbolic prosody: a new approach",
   "original": "i06_1618",
   "page_count": 4,
   "order": 400,
   "p1": "paper 1618-Wed3BuP.5",
   "pn": "",
   "abstract": [
    "Aiming at the improvement of the quality of synthetic speech generated by our native TTS ARTIC, we adopted the unit selection method. Our unit selection module is driven by prosody described solely by high-level symbolic features which are linked to the prosody of synthesized phrases through the phenomena of prosodic synonymy and homonymy. It was confirmed that such an approach not only generates speech with high naturalness but also keeps the richness of prosody. Our first version of this approach significantly increased the quality of the output speech, which was assessed by listeners as very close to natural.\n",
    "The concept of prosodic synonymy and homonymy is, therefore, further extended and formally described in this paper, and its importance to the unit selection treatment is demonstrated. In addition, the difference of this concept from the concepts most frequently used is shown. Moreover, the first experiment following the formal definition of the problem presented in this paper has been carried out, proving that the whole concept is feasible.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-400"
  },
  "wu06c_interspeech": {
   "authors": [
    [
     "Yi-Jian",
     "Wu"
    ],
    [
     "Wu",
     "Guo"
    ],
    [
     "Ren-Hua",
     "Wang"
    ]
   ],
   "title": "Minimum generation error criterion for tree-based clustering of context dependent HMMs",
   "original": "i06_1373",
   "page_count": 4,
   "order": 401,
   "p1": "paper 1373-Wed3BuP.6",
   "pn": "",
   "abstract": [
    "Due to the inconsistency between HMM training and synthesis application in HMM-based speech synthesis, the minimum generation error (MGE) criterion had been proposed for HMM training. This paper continues to apply the MGE criterion for tree-based clustering of context dependent HMMs. As directly applying the MGE criterion results in an unacceptable computational cost, the parameter updating rules of the MGE criterion are simplified to rapidly update the parameters of clustered models, and an appropriate strategy by combining the MGE criterion with the ML criterion is designed to select the optimal question for tree node splitting. From the experiment results, the quality of synthetic speech was improved after applying the MGE criterion for HMM clustering.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-401"
  },
  "kang06_interspeech": {
   "authors": [
    [
     "Heng",
     "Kang"
    ],
    [
     "Wenju",
     "Liu"
    ]
   ],
   "title": "Selective-LPC based representation of STRAIGHT spectrum and its applications in spectral smoothing",
   "original": "i06_1109",
   "page_count": 4,
   "order": 402,
   "p1": "paper 1109-Wed3BuP.7",
   "pn": "",
   "abstract": [
    "In this paper we propose a new method to represent STRAIGHT spectrum. The new method provides STRAIGHT spectral parameters with the capability of interpolation and quantization, which is needed for most speech manipulation, especially for spectral smoothing. The proposed method estimates 2-band selective-LPC whose spectral envelope fits the given STRAIGHT spectrum. With the interpolation properties of LSP, the estimated selective-LPC could be converted to LSP and then simply interpolated. We apply this representation in our spectral smoothing experiments and the results show that this method can get smooth spectral envelope over the segment boundaries. Listening tests prove that this algorithm effectively smooth speech boundaries with little quality degradation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-402"
  },
  "jilka06_interspeech": {
   "authors": [
    [
     "Matthias",
     "Jilka"
    ],
    [
     "Bernd",
     "Möbius"
    ]
   ],
   "title": "Towards a comprehensive investigation of factors relevant to peak alignment using a unit selection corpus",
   "original": "i06_1565",
   "page_count": 4,
   "order": 403,
   "p1": "paper 1565-Wed3BuP.8",
   "pn": "",
   "abstract": [
    "This paper aims to demonstrate the use of a unit selection corpus, the IMS German Festival synthesis system [1], in carrying out a comprehensive investigation of factors influencing specific aspects of the phonetic realization of tonal categories. The study restricts itself to the alignment of peaks in H*L pitch accents in German. First results confirm not only well-known effects of syllable structure, e.g., peaks occurring relatively early when there is a sonorant onset or relatively late when there is a sonorant in the coda, but also attest to the special status of the nuclear pitch accent vs. accents occurring earlier in the intonation phrase. Furthermore, instances of H*L in syllables directly at the phrase boundaries (initial or final) are shown to behave significantly differently from those that are located farther away. A similar effect is observed when another pitch accent follows the H*L peak in the very next syllable as opposed to a distance of two or more syllables. In these cases it also matters whether a low or high target is following (the peaks occur relatively later when followed by a L target). The results should have the benefit of both describing the specific characteristics of the voice providing the corpus (allowing a more detailed phonetic realization of tonal categories during the synthesis process) and offering general insights into which factors are relevant to the alignment of H*L peaks in German.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-403"
  },
  "utama06_interspeech": {
   "authors": [
    [
     "Robert J.",
     "Utama"
    ],
    [
     "Ann K.",
     "Syrdal"
    ],
    [
     "Alistair",
     "Conkie"
    ]
   ],
   "title": "Six approaches to limited domain concatenative speech synthesis",
   "original": "i06_1047",
   "page_count": 4,
   "order": 404,
   "p1": "paper 1047-Wed3BuP.9",
   "pn": "",
   "abstract": [
    "This paper (this work constitute Robert Utamas master thesis in the Electrical and Computer Engineering program in Rutgers University) describes the creation of 6 limited-domain Text-to-Speech (TTS) systems that are constrained to digit string and natural number domains (cardinal numbers only). Unit selection-based concatenative TTS systems were implemented in MATLAB to fulfill this goal. We evaluate and discuss various factors that can influence the naturalness or overall quality of the synthesized voice. Some of the factors studied are the length and type of the synthesis unit and the extent of co-articulation represented in the recorded speech database. In the end, we show that it is possible to create a high quality limited domain TTS system either with maximal or with carefully controlled minimal effects of co-articulation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-404"
  },
  "fischer06_interspeech": {
   "authors": [
    [
     "V.",
     "Fischer"
    ],
    [
     "S.",
     "Kunzmann"
    ]
   ],
   "title": "From pre-recorded prompts to corporate voices: on the migration of interactive voice response applications",
   "original": "i06_1042",
   "page_count": 4,
   "order": 405,
   "p1": "paper 1042-Wed3BuP.10",
   "pn": "",
   "abstract": [
    "This paper describes our efforts towards the creation of corporate synthetic voices from low quality speech data, as it can typically be found on many Interactive Voice Response (IVR) units. In doing so, we first touch on several normalization techniques that aim on a better support of a highly automated voice construction process. Subsequently, we describe methods for the creation of enriched corporate voices which integrate speech recordings from different speakers in order to overcome problems arising from limited domain training data.\n",
    "Experiments are described which demonstrate the feasibility of the approach by comparing it to a less flexible solution that uses prerecorded prompts in combination with a large footprint standard concatenative synthesizer. Results show that the enriched voices clearly outperform those voices build solely from IVR data, while achieving almost the same overall rating as the pre-recorded prompts solution.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-405"
  },
  "park06_interspeech": {
   "authors": [
    [
     "Seung Seop",
     "Park"
    ],
    [
     "Jong Won",
     "Shin"
    ],
    [
     "Nam Soo",
     "Kim"
    ]
   ],
   "title": "Automatic speech segmentation with multiple statistical models",
   "original": "i06_1199",
   "page_count": 4,
   "order": 406,
   "p1": "paper 1199-Wed3BuP.11",
   "pn": "",
   "abstract": [
    "In this paper, we propose a novel approach to improve the performance of automatic speech segmentation techniques for concatenative text-to-speech synthesis. A number of automatic segmentation machines (ASMs) are simultaneously applied and the final boundary time marks are drawn from the multiple segmentation results. To identify the best time mark among those provided by the multiple ASMs, we apply a candidate selector trained over a set of manually-segmented speech database. The candidate selector defines a mapping from the phonetic boundary to the best ASM index which will output the time mark that may be closest to the manual segmentation result. The experimental results show that our approach dramatically improves the segmentation accuracy.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-406"
  },
  "parssinen06_interspeech": {
   "authors": [
    [
     "Kimmo",
     "Pärssinen"
    ],
    [
     "Marko",
     "Moberg"
    ]
   ],
   "title": "Evaluation of perceptual quality of control point reduction in rule-based synthesis",
   "original": "i06_1178",
   "page_count": 4,
   "order": 407,
   "p1": "paper 1178-Wed3BuP.12",
   "pn": "",
   "abstract": [
    "Text-to-speech implementations on embedded devices usually require low memory consumption and computational complexity. Due to its simplicity, formant synthesizer is still an attractive solution for some applications. The formant values and transitions are controlled by a set of rules, which assign control points for synthesis parameters. This paper investigates the possibility to reduce the number of control points for formant contours from four to two per phoneme. The reduced model contained only the values at the end of the onset transition and in the beginning of the offset transition. Various interpolation techniques were studied but linear interpolation was used for its simplicity. The 4- and 2-point models were compared in a listening evaluation test. The results show that the reduction of control points does not have any effect on the perceived quality. The dynamic, context dependent positioning of the two control points preserves the most essential information of formant contours.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-407"
  },
  "coorman06_interspeech": {
   "authors": [
    [
     "Geert",
     "Coorman"
    ]
   ],
   "title": "Segment connection networks for corpus-based speech synthesis",
   "original": "i06_1962",
   "page_count": 4,
   "order": 408,
   "p1": "paper 1962-Wed3BuP.13",
   "pn": "",
   "abstract": [
    "In this paper an efficient segment selection method for corpus-based speech synthesis is presented. Traditional unit-selectors use dynamic programming (DP) to find in a fully connected segment network the most appropriate segment sequence based on target- and concatenation costs. Instead of performing a full DP search, the presented unit-selector applies fast transformations on binary valued segment connection matrices. It is further also shown that this technique can be expanded to supra-segmental unit-selection without altering the segment size in the database.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-408"
  },
  "tsuji06_interspeech": {
   "authors": [
    [
     "Ryo",
     "Tsuji"
    ],
    [
     "Tomohiko",
     "Kasami"
    ],
    [
     "Shogo",
     "Ishikawa"
    ],
    [
     "Shinya",
     "Kiriyama"
    ],
    [
     "Yoichi",
     "Takebayashi"
    ],
    [
     "Shigeyoshi",
     "Kitazawa"
    ]
   ],
   "title": "Observations of the spoken language acquisition process based on a multimodal infant behavior corpus",
   "original": "i06_1953",
   "page_count": 4,
   "order": 409,
   "p1": "paper 1953-Tue3CaP.1",
   "pn": "",
   "abstract": [
    "We have developed a framework to record spontaneous speech of infants. Using the framework, we have accumulated infant speech data, and proved that the data is quite efficient for the explication of the spoken language acquisition process. We aim at constructing the \"multimodal infant behavior corpus,\" which contributes to the elucidation of human commonsense knowledge and its acquisition mechanism. We previously established the environments to record infant behaviors as multimodal data. We have newly developed a wearable speech recording system and succeeded to record infant utterances with high quality. We have held an infant school once a week for 10 months, and accumulated infant speech data more than 100 hours long. We observed infant utterances in the aspects of acoustic and prosodic features. Through an acoustic observation, we have succeeded to analyze alteration of the pronunciation skills focused on demonstratives that appear quite often in infant utterances. As the result of a prosodic observation, we have also obtained knowledge of how infants enrich and diversify the ways to explain their intentions or emotions corresponding to their growth.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-409"
  },
  "marklund06_interspeech": {
   "authors": [
    [
     "Ellen",
     "Marklund"
    ],
    [
     "Francisco",
     "Lacerda"
    ]
   ],
   "title": "Infants² ability to extract verbs from continuous speech",
   "original": "i06_1986",
   "page_count": 3,
   "order": 410,
   "p1": "paper 1986-Tue3CaP.2",
   "pn": "",
   "abstract": [
    "Early language acquisition is a result of the infants general associative and memory processes in combination with its ecological surroundings. Extracting a part of a continuous speech signal and associate it to for instance an object, is made possible by the structure provided by characteristically repetitive Infant Directed Speech. The parents adjust the way they speak to their infant based on the response they are given, which in turn is dependent on the infants age and cognitive development.\n",
    "It seems probable that the ability to extract lexical candidates referring to visually presented actions is developed at a later stage than the ability to extract lexical candidates referring to visually presented objects-actions are more abstract and there is a time aspect involved. Using the Visual Preference Paradigm, the ability to extract lexical candidates referring to actions was studied in infants at the age of 4 to 8 months. The results suggest that while the ability at this age is not greatly apparent, it seems to increase slightly with age.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-410"
  },
  "bion06_interspeech": {
   "authors": [
    [
     "Ricardo A.H.",
     "Bion"
    ],
    [
     "Paola",
     "Escudero"
    ],
    [
     "Andréia S.",
     "Rauber"
    ],
    [
     "Barbara O.",
     "Baptista"
    ]
   ],
   "title": "Category formation and the role of spectral quality in the perception and production of English front vowels",
   "original": "i06_1270",
   "page_count": 4,
   "order": 411,
   "p1": "paper 1270-Tue3CaP.3",
   "pn": "",
   "abstract": [
    "This study aimed at comparing the perception and production of English front vowels by 17 proficient Brazilian speakers of English as a second language (L2) and 6 native speakers of American English. Towards this end, three experiments were carried out: (i) a production test measuring the first two formants of the participants English front vowels, (ii) an oddity discrimination test investigating the formation of vowel categories, and (iii) a discrimination test with synthetic stimuli which assessed the participants reliance on spectral quality when perceiving English vowels. The results of these experiments suggest a strong relationship between L2 vowel perception and production, since the vowel pairs which were produced with similar formant values by the Brazilian participants were also poorly discriminated in the two perception tests. In addition, the findings suggest that vowel perception might precede vowel production, as high rates on the discrimination of vowel pairs on both perception tests were a prerequisite for differentiating the same two vowels on the production test. Lastly, some Brazilian participants obtained native-like scores on the category formation test without manifesting native-like reliance on spectral quality, indicating that other acoustic cues, such as vowel duration, might be playing a role in their perception of English vowels.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-411"
  },
  "bijeljacbabic06_interspeech": {
   "authors": [
    [
     "Ranka",
     "Bijeljac-Babic"
    ],
    [
     "Christelle",
     "Dodane"
    ],
    [
     "Sabine",
     "Metta"
    ],
    [
     "Claire",
     "Gérard"
    ]
   ],
   "title": "Productions in bilinguism, early foreign language learning and monolinguism: a prosodic comparison",
   "original": "i06_1582",
   "page_count": 4,
   "order": 412,
   "p1": "paper 1582-Tue3CaP.4",
   "pn": "",
   "abstract": [
    "The degree of L2 foreign accent is likely to vary, according to the age of the acquisition, the length of contact with L2 and the possible interaction between L1 and L2. This study examined how children who master French and English at different levels pronounce disyllabic words in both languages. Acoustic analysis (F0, duration and amplitude) of syllables in disyllabic words were compared between 8 bilingual children (French-English, aged between 3;6 and 6;1 years) and 16 monolingual children (8 French children and 8 English children of the same age) and confronted to the analysis of 20 (7 years aged) early French Learners of English (FLE) children. Results showed that the bilingual children acquired prosodic patterns in both languages. However, the accent of their disyllabic words differed from those of the monolingual children. French 7 years old-aged learners of English, after only two years of acquisition, produced the native-like accent. Our findings modulate the \"critical age\" hypothesis and bring some new elements in favour of the L1 and L2 obligatory interaction hypothesis.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-412"
  },
  "hirata06_interspeech": {
   "authors": [
    [
     "Yukari",
     "Hirata"
    ],
    [
     "Elizabeth",
     "Whitehurst"
    ],
    [
     "Emily",
     "Cullings"
    ],
    [
     "Jacob",
     "Whiton"
    ],
    [
     "Carol",
     "Glenn"
    ]
   ],
   "title": "Training native English speakers to identify Japanese vowel length with fast rate sentences",
   "original": "i06_1395",
   "page_count": 4,
   "order": 413,
   "p1": "paper 1395-Tue3CaP.5",
   "pn": "",
   "abstract": [
    "Native English speakers were trained to identify Japanese vowel length with sentences spoken at a fast rate, and their test scores before and after training were compared with those of a control group. Results indicated that the present training with little variability in speaking rate did not yield a robust perceptual improvement for the trained group. However, among the three rates tested (slow, normal, fast), the trained group showed significant improvement specifically for the slow-rate stimuli, distinct from the control groups.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-413"
  },
  "chen06c_interspeech": {
   "authors": [
    [
     "Jiang-Chun",
     "Chen"
    ],
    [
     "Wei-Tang",
     "Hsu"
    ],
    [
     "J.-S. Roger",
     "Jang"
    ],
    [
     "Ren-Yuan",
     "Lyu"
    ],
    [
     "Yuang-Chin",
     "Chiang"
    ]
   ],
   "title": "Formant-based English vowel assessment for Chinese in Taiwan",
   "original": "i06_1968",
   "page_count": 4,
   "order": 414,
   "p1": "paper 1968-Tue3CaP.6",
   "pn": "",
   "abstract": [
    "This paper proposes a formant-based approach for computer-assisted English vowel assessment. Various studies in formant-based speech synthesis have suggested the importance of formant coefficients; this motivates us to investigate pronunciation assessment using formant information instead of MFCC (Mel-frequency cepstral coefficients) alone. In particular, we explore the multi-stream HMM with the addition of formant information to improve the phoneme segmentation. We then propose the use of PCN (pronunciation confusion network) together with a formant-based confidence measure to improve error detection rates. Furthermore, the pros and cons of using cross-word phone model for both native speakers and L2 learners are discussed. Experimental results demonstrate the feasibility of the proposed approach for automatic vowel pronunciation assessment.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-414"
  },
  "metzner06_interspeech": {
   "authors": [
    [
     "Jörg",
     "Metzner"
    ],
    [
     "Marcel",
     "Schmittfull"
    ],
    [
     "Karl",
     "Schnell"
    ]
   ],
   "title": "Substitute sounds for ventriloquism and speech disorders",
   "original": "i06_1426",
   "page_count": 4,
   "order": 415,
   "p1": "paper 1426-Tue3CaP.7",
   "pn": "",
   "abstract": [
    "The restriction of articulation used by ventriloquists or caused by speech disorders can be compensated using substitute sounds. For a better understanding of these sounds, in this contribution the results of investigations of the substitute sounds by analysis and synthesis are presented. For that purpose the substitute sounds and their natural counterparts uttered by a ventriloquist are analyzed. Substitute sounds are also generated by articulatory synthesis and by a plaster model and are then compared to the original sounds. The results show that the perception of substitute sounds can reach the natural uttered sounds. The degree of the perceptive similarity depends on the substitute sound. The spectrum of the substitute sounds can replicate partially the spectrum of the natural sounds. However, for successful perception of the sound it is only necessary to reproduce the relevant spectral information. The knowledge about those sounds is interesting for speech therapy.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-415"
  },
  "wei06b_interspeech": {
   "authors": [
    [
     "Si",
     "Wei"
    ],
    [
     "Qing-Sheng",
     "Liu"
    ],
    [
     "Yu",
     "Hu"
    ],
    [
     "Ren-Hua",
     "Wang"
    ]
   ],
   "title": "Automatic Mandarin pronunciation scoring for native learners with dialect accent",
   "original": "i06_1669",
   "page_count": 4,
   "order": 416,
   "p1": "paper 1669-Tue3CaP.8",
   "pn": "",
   "abstract": [
    "This paper studies pronunciation scoring algorithm in CALL system aiming at teaching native Chinese learn standard Mandarin. Most of the pronunciation scoring algorithms focus on non-native environment, which may not be suitable for native speakers. We bring up a new algorithm based on traditional posterior log-likelihood algorithm by weighting the initial part of Mandarin syllables, where final-initials duration ratio is introduced to control the weight. Experiments show that the proposed algorithm is much more effective than traditional posterior log-likelihood algorithm in the Mandarin learning system. The correlation with human score achieves an increase of 11%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-416"
  },
  "fujita06_interspeech": {
   "authors": [
    [
     "Kengo",
     "Fujita"
    ],
    [
     "Tsuneo",
     "Kato"
    ],
    [
     "Hisashi",
     "Kawai"
    ]
   ],
   "title": "Quick individual fitting methods of simplified hearing compensation for elderly people",
   "original": "i06_1879",
   "page_count": 4,
   "order": 417,
   "p1": "paper 1879-Tue3CaP.9",
   "pn": "",
   "abstract": [
    "For simplified hearing compensation, which combines a high-frequency boost, pitch shift and speech rate conversion, two quick individual fitting procedures, based on subjective assessments, are proposed. Since conventional individual fitting procedures impose enormous hearing comparisons on a user for decision of an appropriate parameter set, the proposed procedures split the fitting process into two steps, namely a parameter fitting step for each hearing compensation method and a comprehensive combination step, to reduce the number of comparisons. Fitting experiments with twenty elderly subjects over sixty-five showed that the proposed procedures had effect on hearing with an average of only around 20 comparisons, while the conventional procedure requires hundreds of comparisons.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-417"
  },
  "li06f_interspeech": {
   "authors": [
    [
     "Xiao",
     "Li"
    ],
    [
     "Jonathan",
     "Malkin"
    ],
    [
     "Susumu",
     "Harada"
    ],
    [
     "Jeff A.",
     "Bilmes"
    ],
    [
     "Richard",
     "Wright"
    ],
    [
     "James",
     "Landay"
    ]
   ],
   "title": "An online adaptive filtering algorithm for the vocal joystick",
   "original": "i06_1872",
   "page_count": 4,
   "order": 418,
   "p1": "paper 1872-Tue3CaP.10",
   "pn": "",
   "abstract": [
    "This paper introduces a novel adaptive direction filtering algorithm in the Vocal Joystick (VJ) setting that utilizes context information and applies real-time inference in a continuous space. The VJ system using this algorithm is endowed with the ability to produce movements in arbitrary directions and the ability to draw smooth curves. This is in contrast to previous VJ settings whereby vowel quality was used to determine mouse movement in only a finite discrete set of directions [1].\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-418"
  },
  "nakamura06b_interspeech": {
   "authors": [
    [
     "Keigo",
     "Nakamura"
    ],
    [
     "Tomoki",
     "Toda"
    ],
    [
     "Hiroshi",
     "Saruwatari"
    ],
    [
     "Kiyohiro",
     "Shikano"
    ]
   ],
   "title": "Speaking aid system for total laryngectomees using voice conversion of body transmitted artificial speech",
   "original": "i06_1839",
   "page_count": 4,
   "order": 419,
   "p1": "paper 1839-Tue3CaP.11",
   "pn": "",
   "abstract": [
    "The aim of this paper is to improve the naturalness of speech using a medical device such as an electrolarynx. There are several problems associated with using existing electrolarynxes, such as the fact the loud volume of the electrolarynx itself might disturb smooth interpersonal communication, and that the generated speech is unnatural. We propose a novel speaking-aid system for total laryngectomees using a new sound source as an alternative to the existing electrolarynx and a statistical voice-conversion technique. The new sound-source unit outputs extremely small signals that cannot be heard by people around the speaker. Artificial speech is recorded with a NAM microphone through soft tissues of the head. From the result of voice conversion, the body-transmitted artificial speech is consistently converted to a more natural voice. We also demonstrate that the speech recognition performance of the proposed system substantially increases in terms of objective evaluation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-419"
  },
  "sansegundo06_interspeech": {
   "authors": [
    [
     "R.",
     "San-Segundo"
    ],
    [
     "R.",
     "Barra"
    ],
    [
     "L. F.",
     "D’Haro"
    ],
    [
     "J. M.",
     "Montero"
    ],
    [
     "R.",
     "Córdoba"
    ],
    [
     "J.",
     "Ferreiros"
    ]
   ],
   "title": "A Spanish speech to sign language translation system for assisting deaf-mute people",
   "original": "i06_1243",
   "page_count": 4,
   "order": 420,
   "p1": "paper 1243-Tue3CaP.12",
   "pn": "",
   "abstract": [
    "This paper describes the first experiments of a speech to sign language translation system in a real domain. The developed system is focused on the sentences spoken by an officer when assisting people in applying for, or renewing the National Identification Document (NID) and the Passport. This system translates officer explanations into sign language for deaf-mute people. The translation system is composed by a speech recognizer (for decoding the spoken utterance into a word sequence), a natural language translator (for converting a word sequence into a sequence of gestures belonging to the sign language), and a 3D avatar animation module (for playing the gestures). The field experiments have reported a 27.2% GER (Gesture Error Rate) and a 0.62 BLEU (BiLingual Evaluation Understudy).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-420"
  },
  "klintfors06_interspeech": {
   "authors": [
    [
     "Eeva",
     "Klintfors"
    ],
    [
     "Francisco",
     "Lacerda"
    ]
   ],
   "title": "Potential relevance of audio-visual integration in mammals for computational modeling",
   "original": "i06_1992",
   "page_count": 4,
   "order": 421,
   "p1": "paper 1992-Tue3CaP.13",
   "pn": "",
   "abstract": [
    "The purpose of this study was to examine typically developing infants integration of audio-visual sensory information as a fundamental process involved in early word learning. One hundred sixty pre-linguistic children were randomly assigned to watch one of four counterbalanced versions of audio-visual video sequences. The infants eye-movements were recorded and their looking behavior was analyzed throughout three repetitions of exposure-test-phases. The results indicate that the infants were able to learn covariance between shapes and colors of arbitrary geometrical objects and to them corresponding nonsense words. Implications of audio-visual integration in infants and in non-human animals for modeling within speech recognition systems, neural networks and robotics are discussed.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-421"
  },
  "rytting06_interspeech": {
   "authors": [
    [
     "C. Anton",
     "Rytting"
    ]
   ],
   "title": "Finding the gaps: applying a connectionist model of word segmentation to noisy phone-recognized speech data",
   "original": "i06_2062",
   "page_count": 4,
   "order": 422,
   "p1": "paper 2062-Tue3CaP.14",
   "pn": "",
   "abstract": [
    "The Christiansen model of word segmentation [1] is a connectionist framework for modeling how infants combine multiple cues in learning and processing language. Most studies applying this model assume idealized input with adult-like representations of phonemes and features, with little or no degradation of the input signal. From these studies, it is difficult to tell if the model is robust to non-idealized, noisy input, which may correspond more closely to an infant languagelearners experience.\n",
    "This study tests the robustness of the Christiansen model by providing input from a minimally-trained phone recognizer on infant-directed speech. Some degradation of performance is observed, but the model still performs above chance. This finding represents a first step in developing more realistic input representations for models of child language acquisition.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-422"
  },
  "wang06e_interspeech": {
   "authors": [
    [
     "Shizhen",
     "Wang"
    ],
    [
     "Xiaodong",
     "Cui"
    ],
    [
     "Abeer",
     "Alwan"
    ]
   ],
   "title": "Rapid speaker adaptation using regression-tree based spectral peak alignment",
   "original": "i06_1334",
   "page_count": 4,
   "order": 423,
   "p1": "paper 1334-Wed1A2O.1",
   "pn": "",
   "abstract": [
    "In this paper, regression-tree based spectral peak alignment is proposed for rapid speaker adaptation using the linearization of VTLN. Two different regression classes are investigated: phonetic classes (using combined knowledge and data-driven techniques) and mixture classes. Compared to MLLR and VTLN, improved performance can be obtained for both supervised and unsupervised adaptations on both medium vocabulary and connected digits recognition tasks. To further improve the performance, MLLR was integrated into this regression-tree based peak alignment. Experimental results show that the performance improvements can be achieved even with limited adaptation data.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-423"
  },
  "kim06c_interspeech": {
   "authors": [
    [
     "Chanwoo",
     "Kim"
    ],
    [
     "Yu-Hsiang",
     "Chiu"
    ],
    [
     "Richard M.",
     "Stern"
    ]
   ],
   "title": "Physiologically-motivated synchrony-based processing for robust automatic speech recognition",
   "original": "i06_1975",
   "page_count": 4,
   "order": 424,
   "p1": "paper 1975-Wed1A2O.2",
   "pn": "",
   "abstract": [
    "This paper describes the structure and performance of a new signal processing scheme, motivated by the physiology of the peripheral auditory system, that improves speech recognition accuracy in the presence of broadband noise. An important attribute of the peripheral processing is a novel mechanism to represent the cycle-by-cycle synchrony in the response of low-frequency auditory-nerve fibers, in addition to the more conventional processing based on mean rate of response. It is shown that the use of the physiologically-motivated peripheral processing improves recognition accuracy in the presence of both broadband and transient noise, and that the use of the synchrony mechanism provides further improvement beyond that which is provided by the mean rate mechanism.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-424"
  },
  "walliczek06_interspeech": {
   "authors": [
    [
     "Matthias",
     "Walliczek"
    ],
    [
     "Florian",
     "Kraft"
    ],
    [
     "Szu-Chen",
     "Jou"
    ],
    [
     "Tanja",
     "Schultz"
    ],
    [
     "Alex",
     "Waibel"
    ]
   ],
   "title": "Sub-word unit based non-audible speech recognition using surface electromyography",
   "original": "i06_1596",
   "page_count": 4,
   "order": 425,
   "p1": "paper 1596-Wed1A2O.3",
   "pn": "",
   "abstract": [
    "In this paper we present a novel approach for a surface electromyographic speech recognition system based on sub-word units. Rather than using full word models as integrated in our previous work we propose here smaller sub-word units as prerequisites for large vocabulary speech recognition. This allows the recognition of words not seen in the training set based on seen sub-word units. Therefore we report on experiments with syllables and phonemes as sub-word units. We also developed a new feature extraction method that gains significant improvement for words and sub-word units.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-425"
  },
  "vicentepena06_interspeech": {
   "authors": [
    [
     "Jesús",
     "Vicente-Peña"
    ],
    [
     "Fernando",
     "Díaz-de-María"
    ],
    [
     "Bastiaan",
     "Kleijn"
    ]
   ],
   "title": "Individual on-line variance adaptation of frequency filtered parameters for robust ASR",
   "original": "i06_1221",
   "page_count": 4,
   "order": 426,
   "p1": "paper 1221-Wed1A2O.4",
   "pn": "",
   "abstract": [
    "In this paper we address the problem of robust speech recognition. We propose a new method based on the individual variance adaptation of frequency filtered parameters to reduce the deleterious effects of additive narrow-band noise. The method can be interpreted as a spectral weighting that assigns increased importance to the most reliable spectral components, typically the spectral peaks. The experiments confirm that the suggested method results in significantly improved recognition rates for additive narrow-band noise.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-426"
  },
  "zhang06e_interspeech": {
   "authors": [
    [
     "Bing",
     "Zhang"
    ],
    [
     "Spyros",
     "Matsoukas"
    ],
    [
     "Richard",
     "Schwartz"
    ]
   ],
   "title": "Recent progress on the discriminative region-dependent transform for speech feature extraction",
   "original": "i06_1573",
   "page_count": 4,
   "order": 427,
   "p1": "paper 1573-Wed1A2O.5",
   "pn": "",
   "abstract": [
    "The region-dependent transform (RDT) is a feature extraction method for speech recognition that employs the Minimum Phoneme Error (MPE) criterion to optimize a set of feature transforms, each concentrating on a region of the acoustic space. Previous results have shown that RDT gives significant recognition-error reduction in a large vocabulary speaker-independent (SI) system. As a follow-up investigation, this paper presents the recent progress of applying RDT in speaker-adaptive training (SAT). Similar to previous SI results, the integration of RDT with SAT yields 7% relative improvement in word error rate (WER). Also, theoretical comparisons are made between RDT and other discriminative feature extraction methods, including the improved version of the feature-space MPE (fMPE) that uses the \"mean-offsets\" as additional input features.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-427"
  },
  "rademacher06_interspeech": {
   "authors": [
    [
     "Jan",
     "Rademacher"
    ],
    [
     "Matthias",
     "Wächter"
    ],
    [
     "Alfred",
     "Mertins"
    ]
   ],
   "title": "Improved warping-invariant features for automatic speech recognition",
   "original": "i06_1216",
   "page_count": 4,
   "order": 428,
   "p1": "paper 1216-Wed1A2O.6",
   "pn": "",
   "abstract": [
    "In this paper, we extend a previously introduced method for the generation of vocal tract length invariant (VTLI) features. The novelty is a reduction of the number of obtained invariances to the more desired ones, which results in a significant improvement of recognition rates. In experiments on the TIMIT database, the enhanced discrimination capabilities and robustness to mismatches between training and test conditions are shown.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-428"
  },
  "nenkova06_interspeech": {
   "authors": [
    [
     "Ani",
     "Nenkova"
    ]
   ],
   "title": "Summarization evaluation for text and speech: issues and approaches",
   "original": "i06_2079",
   "page_count": 4,
   "order": 429,
   "p1": "paper 2079-Wed1WeS.1",
   "pn": "",
   "abstract": [
    "This paper surveys current text and speech summarization evaluation approaches. It discusses advantages and disadvantages of these, with the goal of identifying summarization techniques most suitable to speech summarization. Precision/recall schemes, as well as summary accuracy measures which incorporate weightings based on multiple human decisions, are suggested as particularly suitable in evaluating speech summaries.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-429"
  },
  "zhu06b_interspeech": {
   "authors": [
    [
     "Xiaodan",
     "Zhu"
    ],
    [
     "Gerald",
     "Penn"
    ]
   ],
   "title": "Summarization of spontaneous conversations",
   "original": "i06_1899",
   "page_count": 4,
   "order": 430,
   "p1": "paper 1899-Wed1WeS.2",
   "pn": "",
   "abstract": [
    "Most speech summarization research is conducted on broadcast news. In our viewpoint, spontaneous conversations are a more \"typical\" speech source that distinguishes speech summarization from text summarization, and hence a more appropriate domain for studying speech summarization. For example, spontaneous conversations contain more spoken-language characteristics, e.g. disfluencies and false starts. They are also more vulnerable to ASR errors. Previous research has studied some aspects of this type of data, but this paper addresses the problem further in several important respects. First, we summarize spontaneous conversations with features of a wide variety that have not been explored before. Second, we examine the role of disfluencies in summarization, which in all previous work was either not explicitly handled or removed as noise. Third, we breakdown and analyze the impact of WER on the individual features for summarization.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-430"
  },
  "chatain06_interspeech": {
   "authors": [
    [
     "Pierre",
     "Chatain"
    ],
    [
     "Edward",
     "Whittaker"
    ],
    [
     "Joanna",
     "Mrozinski"
    ],
    [
     "Sadaoki",
     "Furui"
    ]
   ],
   "title": "Perplexity based linguistic model adaptation for speech summarisation",
   "original": "i06_1677",
   "page_count": 4,
   "order": 431,
   "p1": "paper 1677-Wed1WeS.3",
   "pn": "",
   "abstract": [
    "The performance of automatic speech summarisation has been improved in previous experiments by using linguistic model adaptation. One of the problems encountered was the high computational cost and low efficiency of the development phase. In this paper we compare our original development approach of evaluating summaries produced by an exhaustive search over all parameters with a much faster development method using an expectation maximization algorithm that minimizes perplexity in order to find the optimal combination of linguistic models for the speech summarisation task. Perplexity proves to be sufficiently correlated to the objective evaluation metrics used in the summarisation literature that it can be used in this fashion. For a much reduced computational cost (approximately 500 times faster), final relative improvements are very similar to those previously obtained, ranging from 1.5% to 21.3% on all investigated metrics for summaries made from automatic speech recogniser transcriptions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-431"
  },
  "lee06e_interspeech": {
   "authors": [
    [
     "Lin-shan",
     "Lee"
    ],
    [
     "Sheng-yi",
     "Kong"
    ],
    [
     "Yi-cheng",
     "Pan"
    ],
    [
     "Yi-sheng",
     "Fu"
    ],
    [
     "Yu-tsun",
     "Huang"
    ]
   ],
   "title": "Multi-layered summarization of spoken document archives by information extraction and semantic structuring",
   "original": "i06_1568",
   "page_count": 4,
   "order": 432,
   "p1": "paper 1568-Wed1WeS.4",
   "pn": "",
   "abstract": [
    "The spoken documents are very difficult to be shown on the screen, and very difficult to retrieve and browse. It is therefore important to develop technologies to summarize the entire archives of the huge quantities of spoken documents in the network content to help the user in browsing and retrieval. In this paper we propose a complete set of multi-layered technologies to handle at least some of the above issues: (1) Automatic Generation of Titles and Summaries for each of the spoken documents, such that the spoken documents become much more easier to browse, (2) Global Semantic Structuring of the entire spoken document archive, offering to the user a global picture of the semantic structure of the archive, and (3) Query-based Local Semantic Structuring for the subset of the spoken documents retrieved by the users query, providing the user the detailed semantic structure of the relevant spoken documents given the query he entered. The Probabilistic Latent Semantic Analysis (PLSA) is found to be helpful, and an initial prototype system for the functions mentioned above has been successfully developed, in which the broadcast news archive in Mandarin Chinese is taken as the example archive.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-432"
  },
  "maskey06b_interspeech": {
   "authors": [
    [
     "Sameer",
     "Maskey"
    ],
    [
     "Julia",
     "Hirschberg"
    ]
   ],
   "title": "Soundbite detection in broadcast news domain",
   "original": "i06_1690",
   "page_count": 4,
   "order": 433,
   "p1": "paper 1690-Wed1WeS.5",
   "pn": "",
   "abstract": [
    "In this paper, we present results of a study designed to identify soundbites in Broadcast News. We describe a Conditional Random Field-based model for the detection of these included speech segments uttered by individuals who are interviewed or who are the subject of a news story. Our goal is to identify direct quotations in spoken corpora which can be directly attributable to particular individuals, as well as to associate these soundbites with their speakers. We frame soundbite detection as a binary classification problem in which each turn is categorized either as a soundbite or not. We use lexical, acoustic/prosodic and structural features on a turn level to train a CRF. We performed a 10-fold cross validation experiment in which we obtained an accuracy of 67.4% and an F-measure of 0.566 which is 20.9% and 38.6% higher than a chance baseline.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-433"
  },
  "murray06_interspeech": {
   "authors": [
    [
     "Gabriel",
     "Murray"
    ],
    [
     "Steve",
     "Renals"
    ]
   ],
   "title": "Dialogue act compression via pitch contour preservation",
   "original": "i06_1585",
   "page_count": 4,
   "order": 434,
   "p1": "paper 1585-Wed1WeS.6",
   "pn": "",
   "abstract": [
    "This paper explores the usefulness of prosody in automatically compressing dialogue acts from meeting speech. Specifically, this work attempts to compress utterances by preserving the pitch contour of the original whole utterance. Two methods of doing this are described in detail and are evaluated subjectively using human annotators and objectively using edit distance with a human-authored gold-standard. Both metrics show that such a prosodic approach is much better than the random baseline approach and significantly better than a simple text compression method.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-434"
  },
  "kubo06_interspeech": {
   "authors": [
    [
     "Toshiaki",
     "Kubo"
    ],
    [
     "Tetsuji",
     "Ogawa"
    ],
    [
     "Tetsunori",
     "Kobayashi"
    ]
   ],
   "title": "Manifold HLDA and its application to robust speech recognition",
   "original": "i06_1949",
   "page_count": 4,
   "order": 435,
   "p1": "paper 1949-Wed1BuP.1",
   "pn": "",
   "abstract": [
    "A manifold heteroscedastic linear discriminant analysis (MHLDA) which removes environmental information explicitly from the useful information for discrimination is proposed. Usually, a feature parameter used in pattern recognition involves categorical information and also environmental information. A well-known HLDA tries to extract useful information (UI) to represent categorical information from the feature parameter. However, environmental information is still remained in the UI parameters extracted by HLDA, and it causes slight degradation in performance. This is because HLDA does not handle the environmental information explicitly. The proposed MHLDA also tries to extract UI like HLDA, but it handles environmental information explicitly. This handling makes MHLDA-based UI parameter less influenced of environment. However, as compensation, in MHLDA, the categorical information is little bit destroyed. In this paper, we try to combine HLDA-based UI and MHLDA-based UI for pattern recognition, and draw benefit of both parameters. Experimental results show the effectiveness of this combining method.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-435"
  },
  "buera06_interspeech": {
   "authors": [
    [
     "Luis",
     "Buera"
    ],
    [
     "Eduardo",
     "Lleida"
    ],
    [
     "Juan A.",
     "Nolazco-Flores"
    ],
    [
     "Antonio",
     "Miguel"
    ],
    [
     "Alfonso",
     "Ortega"
    ]
   ],
   "title": "Time-dependent cross-probability model for multi-environment model based LInear normalization",
   "original": "i06_1271",
   "page_count": 4,
   "order": 436,
   "p1": "paper 1271-Wed1BuP.2",
   "pn": "",
   "abstract": [
    "In a previous work, Multi-Environment Model based LInear Normalization, MEMLIN, was presented and it was proved to be effective to compensate environment mismatch. MEMLIN is an empirical feature vector normalization which models clean and noisy spaces by Gaussian Mixture Models (GMMs). In this algorithm, the probability of the clean model Gaussian, given the noisy model one and the noisy feature vector (cross-probability model) is a critical point. In the previous work the cross-model probability was approximated as time-independent. In this paper, a time-dependent estimation of the cross-probability model based on GMM is proposed. Some experiments with SpeechDat Car database were carried out in order to study the performance of the proposed estimation in a real acoustic environment. MEMLIN with time-independent cross-probability model reached 70.21% of mean improvement in Word Error Rate (WER), however, when timedependent cross-probability model based on GMM was applied, the mean improvement in WER went up to 78.47%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-436"
  },
  "povey06b_interspeech": {
   "authors": [
    [
     "Daniel",
     "Povey"
    ]
   ],
   "title": "SPAM and full covariance for speech recognition",
   "original": "i06_2047",
   "page_count": 4,
   "order": 437,
   "p1": "paper 2047-Wed1BuP.3",
   "pn": "",
   "abstract": [
    "The Subspace Precision and Mean model (SPAM) is a way of representing Gaussian precision and mean values in a reduced dimension. This paper presents some large vocabulary experiments with SPAM and introduces an efficient way to optimize the SPAM basis. We present experiments comparing SPAM, diagonal covariance and full covariance models on a large vocabulary task. We also give explicit formulae for an implementation of SPAM.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-437"
  },
  "sakti06_interspeech": {
   "authors": [
    [
     "Sakriani",
     "Sakti"
    ],
    [
     "Konstantin",
     "Markov"
    ],
    [
     "Satoshi",
     "Nakamura"
    ]
   ],
   "title": "The use of Bayesian network for incorporating accent, gender and wide-context dependency information",
   "original": "i06_1812",
   "page_count": 4,
   "order": 438,
   "p1": "paper 1812-Wed1BuP.4",
   "pn": "",
   "abstract": [
    "We propose a new method of incorporating the additional knowledge of accent, gender, and wide-context dependency information into ASR systems by utilizing the advantages of Bayesian networks. First, we only incorporate pentaphone-context dependency information. After that, accent and gender information are also integrated. In this method, we can easily extend conventional triphone HMMs to cover various sources of knowledge. The probabilistic dependencies between a triphone context unit and additional knowledge are learned through a BN. Another advantage is that during recognition, additional knowledge variables are assumed to be hidden, so that the existing standard triphone-based decoding system can be used without modification. The performance of the proposed model was evaluated on an LVCSR task using two different types of accented English speech data. Experimental results show that this proposed method improves word accuracy with respect to standard triphone models.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-438"
  },
  "wang06f_interspeech": {
   "authors": [
    [
     "Yu",
     "Wang"
    ],
    [
     "Eric",
     "Fosler-Lussier"
    ]
   ],
   "title": "Integrating phonetic boundary discrimination explicitly into HMM systems",
   "original": "i06_1820",
   "page_count": 4,
   "order": 439,
   "p1": "paper 1820-Wed1BuP.5",
   "pn": "",
   "abstract": [
    "In this study, we investigate methods of (a) detecting phonetic boundaries directly from acoustics, and (b) integrating these into HMM-based speech recognition. We test the hypothesis that detecting phone boundaries may be easier using phonological features rather than phonetic or direct acoustic information. We also show how HMMs can be more attuned to the transition of phone boundaries by explicitly modeling transition states. Using a 5-state HMM phone model, we improve the accuracy of phone recognition on the TIMIT task.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-439"
  },
  "xie06_interspeech": {
   "authors": [
    [
     "Zhimin",
     "Xie"
    ],
    [
     "Partha",
     "Niyogi"
    ]
   ],
   "title": "Robust acoustic-based syllable detection",
   "original": "i06_1327",
   "page_count": 4,
   "order": 440,
   "p1": "paper 1327-Wed1BuP.6",
   "pn": "",
   "abstract": [
    "In this paper, we describe a method to detect syllabic nuclei in continuous speech. It employs two basic and robust acoustic features, periodicity and energy, to detect syllable landmarks. This method is evaluated on TIMIT, noise additive TIMIT and NTIMIT datasets with typical total error rates of around 30% in all the datasets, except for extremely adverse 0dB signal-noise-ratio environments, while HMMbased systems degrade rigorously. Based on the landmarks, a vowel classifier is further constructed and achieves the same performance as HMM-based systems.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-440"
  },
  "he06_interspeech": {
   "authors": [
    [
     "Lei",
     "He"
    ],
    [
     "Jie",
     "Hao"
    ]
   ],
   "title": "A tone recognition framework for continuous Mandarin speech",
   "original": "i06_1348",
   "page_count": 4,
   "order": 441,
   "p1": "paper 1348-Wed1BuP.7",
   "pn": "",
   "abstract": [
    "In this paper, we present a tone recognition framework for continuous Mandarin speech. To model the variations of F0 pattern caused by co-articulation and phonetic effects, a set of discriminating features are extracted: 1) outlined features from the F0 contours of target syllable and neighboring syllables are combined; 2) contextual tone information is utilized within an iterative process; 3) phonetic information from target and neighboring syllables is incorporated. These features are put into a decision tree for tone classification, which follows an HMM-based toneless decoder. The results in 5-tone recognition experiments show more than 40% relative error rate reduction against the baseline local outlined features. Moreover, the proposed method obviously outperforms HMM-based tone model in speaker-independent evaluation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-441"
  },
  "hamalainen06_interspeech": {
   "authors": [
    [
     "Annika",
     "Hämäläinen"
    ],
    [
     "Louis ten",
     "Bosch"
    ],
    [
     "Lou",
     "Boves"
    ]
   ],
   "title": "Pronunciation variant-based multi-path HMMs for syllables",
   "original": "i06_1630",
   "page_count": 4,
   "order": 442,
   "p1": "paper 1630-Wed1BuP.8",
   "pn": "",
   "abstract": [
    "Recent research suggests that it is more appropriate to model pronunciation variation with syllable-length acoustic models than with context-dependent phones. Due to the large number of factors contributing to pronunciation variation at the syllable level, the creation of multi-path model topologies appears necessary. In this paper, we propose a novel approach for constructing multi-path models for frequent syllables. The suggested approach uses phonetic knowledge for the initialisation of the parallel paths, and a data-driven solution for their re-estimation. When applied to 94 frequent syllables in a 37-hour corpus of Dutch read speech, it leads to improved recognition performance when compared with a triphone recogniser of similar complexity.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-442"
  },
  "park06b_interspeech": {
   "authors": [
    [
     "Junho",
     "Park"
    ],
    [
     "Hanseok",
     "Ko"
    ]
   ],
   "title": "A new state-dependent phonetic tied-mixture model with head-body-tail structured HMM for real-time continuous phoneme recognition system",
   "original": "i06_1982",
   "page_count": 4,
   "order": 443,
   "p1": "paper 1982-Wed1BuP.9",
   "pn": "",
   "abstract": [
    "An acoustic model for a real-time continuous phoneme recognition system must exhibit the following desirable feature: an ability to minimize the recognition performance degradation while solving the model complexity problem to confine the delay to a minimum in recognition process. To cope with the challenges, we introduce the state-dependent Phonetic Tied-Mixture (PTM) model with Head-Body- Tail (HBT) structured HMM as an acoustic model optimization. The proposed acoustic modeling method shows a significant improvement in recognition performance and becomes a solution to the sparse training data problem and the model complexity problem. Moreover, defining the exceptional Gaussian mixtures in tying process achieves a drastic reduction in phoneme error rate compared to traditional state-dependent PTM method. In this paper, we describe the new acoustic model optimization procedure and show the outstanding performance evaluation results for real-time continuous phoneme recognition system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-443"
  },
  "zgank06b_interspeech": {
   "authors": [
    [
     "Andrej",
     "Zgank"
    ],
    [
     "Zdravko",
     "Kacic"
    ]
   ],
   "title": "Conversion from phoneme based to grapheme based acoustic models for speech recognition",
   "original": "i06_1500",
   "page_count": 4,
   "order": 444,
   "p1": "paper 1500-Wed1BuP.10",
   "pn": "",
   "abstract": [
    "This paper focuses on acoustic modeling in speech recognition. A novel approach how to build grapheme based acoustic models with conversion from existing phoneme based acoustic models is proposed. The grapheme based acoustic models are created as weighted sum from monophone acoustic models. The influence of particular monophone is determined with the phoneme to grapheme confusion matrix. Further, the context-dependent acoustic models are being trained within the grapheme training procedure. The decision tree based clustering approach is used to tie similar states. A modified data-driven method for generation of grapheme broad classes needed during the initialization of decision tree is being applied. The data-driven broad classes are created using the grapheme based confusion matrix. All experiments were performed with the Slovenian language (1000 FDB SpeechDat(II) database), which is a highly inflectional language with no fixed set of rules for grapheme to phoneme conversion. The achieved results showed improvements of speech recognition results with the proposed methods.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-444"
  },
  "kim06d_interspeech": {
   "authors": [
    [
     "Bong-Wan",
     "Kim"
    ],
    [
     "Dae-Lim",
     "Choi"
    ],
    [
     "Yongnam",
     "Um"
    ],
    [
     "Yong-Ju",
     "Lee"
    ]
   ],
   "title": "Phone vector DHMM to decode a phone recognizer's output",
   "original": "i06_1903",
   "page_count": 4,
   "order": 445,
   "p1": "paper 1903-Wed1BuP.11",
   "pn": "",
   "abstract": [
    "In this paper we introduce a Phone Vector Discrete HMM (PVDHMM) that decodes a phone recognizer's output. The proposed PVDHMM treats a phone recognizer as a vector quantizer whose codebook size is equal to the size of its phone set. To examine the proposed method we perform two experiments. First, the output of a phone recognizer is recognized by the PVDHMM, and its results are compared with those of a continuous speech recognizer (CSR). Second, to investigate its potential application in the field of open-vocabulary spoken document retrieval, a retrieval experiment through word spotting is carried out on the output of a phone recognizer, and its results are compared with those of retrieval through the phone-based vector space model.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-445"
  },
  "nagarajan06b_interspeech": {
   "authors": [
    [
     "T.",
     "Nagarajan"
    ],
    [
     "P.",
     "Vijayalakshmi"
    ],
    [
     "Douglas",
     "O'Shaughnessy"
    ]
   ],
   "title": "Combining multiple-sized sub-word units in a speech recognition system using baseform selection",
   "original": "i06_1280",
   "page_count": 3,
   "order": 446,
   "p1": "paper 1280-Wed1BuP.12",
   "pn": "",
   "abstract": [
    "A Longer-sized sub-word unit is known to be a better candidate in the development of a continuous speech recognition system. However, the basic problem with such units is the data sparsity. To overcome this problem, researchers have tried to combine longer-sized sub-word unit models with phoneme models. In this paper, we have considered only frequently occurring syllables and VC (Vowel + Consonant) units, and phone-sized units (monophones and triphones) for the development of a continuous speech recognition system. In such a case, even for a single pronunciation of a word, there can be multiple representational baseforms in the lexicon, each with different-sized units. We show that a considerable improvement in recognition performance can be achieved if the baseforms are selected properly. Out of all possible baseforms for a given word in the lexicon, the baseform that maximizes the acoustic likelihood, for possible sub-word unit concatenations to make a word, alone is considered. In the baseline systems' word-lexicon, like pure monophone or triphone-based systems, since only the acoustically weaker baseforms are replaced by baseforms with longer-sized units, the resultant performance is guaranteed to be better than that of baseline systems. The preliminary experiments carried out on the TIMIT speech corpus show a considerable improvement in the recognition performance over a pure monophone/triphone-based systems when the larger-sized units are combined using proper selection of baseforms.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-446"
  },
  "miguel06_interspeech": {
   "authors": [
    [
     "Antonio",
     "Miguel"
    ],
    [
     "Eduardo",
     "Lleida"
    ],
    [
     "Alfons",
     "Juan"
    ],
    [
     "Luis",
     "Buera"
    ],
    [
     "Alfonso",
     "Ortega"
    ],
    [
     "Oscar",
     "Saz"
    ]
   ],
   "title": "Local transformation models for speech recognition",
   "original": "i06_1275",
   "page_count": 4,
   "order": 447,
   "p1": "paper 1275-Wed1BuP.13",
   "pn": "",
   "abstract": [
    "This paper presents a novel acoustic modeling framework that naturally extends the Hidden Markov Model (HMM) approach. The novel models reduce the errors caused by speaker variability by means of a local spectral mismatch reduction. A more complex and flexible speech production scheme can be assumed, in which the local temporal and frequency elastic deformations of the speech are captured by the model. In the new framework the states of a standard HMM, which are usually associated with temporal transitions, are expanded so that a new degree of freedom for the model is provided and it is then possible to estimate an optimum frequency warping factor at the same time as the decoder finds the best state sequence. In the local spectral warping based models the states become time-frequency related states and the number of parameters of the model is comparable to the standard HMM since they share a certain amount of parameters as it will be shown. The novel models are evaluated in the noise-free TIDIGITS corpus, which includes connected digits uttered by male, female and children. It has been found that, under speaker group (age-gender) mismatch conditions, the local frequency warping reduced Word Error Rate (WER) in mean by a 70%, using the initial models. When matched speaker group conditions were tested the error was reduced in mean in a 9.7% after reestimating the models.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-447"
  },
  "imai06_interspeech": {
   "authors": [
    [
     "Toru",
     "Imai"
    ],
    [
     "Shoei",
     "Sato"
    ],
    [
     "Akio",
     "Kobayashi"
    ],
    [
     "Kazuo",
     "Onoe"
    ],
    [
     "Shinichi",
     "Homma"
    ]
   ],
   "title": "Online speech detection and dual-gender speech recognition for captioning broadcast news",
   "original": "i06_1103",
   "page_count": 4,
   "order": 448,
   "p1": "paper 1103-Wed1CaP.1",
   "pn": "",
   "abstract": [
    "This paper describes two new methods, online speech detection and dual-gender speech recognition, for captioning broadcast news. The proposed online speech detection performs dual-gender phoneme recognition and detects a start-point and an end-point based on the ratio between the cumulative phoneme likelihood and the cumulative non-speech likelihood with a very small delay from the audio input. As soon as the start-point is detected, the subsequent continuous speech recognizer with paralleled gender-dependent acoustic models starts a search using gender change information from the preceding phoneme recognizer to reduce computational cost. Speech recognition experiments on conversational commentaries and field reporting from Japanese broadcast news showed that the proposed speech detection method was effective in reducing false segmentations and also recognition errors in comparison with a conventional method using adaptive energy thresholds. The proposed dual-gender speech recognition with the new speech detection significantly reduced the word error rate by 11.2% relative to a conventional gender-independent system, while keeping the computational cost in real-time.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-448"
  },
  "hazen06_interspeech": {
   "authors": [
    [
     "Timothy J.",
     "Hazen"
    ]
   ],
   "title": "Automatic alignment and error correction of human generated transcripts for long speech recordings",
   "original": "i06_1258",
   "page_count": 4,
   "order": 449,
   "p1": "paper 1258-Wed1CaP.2",
   "pn": "",
   "abstract": [
    "In this paper we examine the issues of aligning and correcting approximate human generated transcripts for long audio files. Accurate time-aligned transcriptions help provide easier access to audio materials by aiding downstream applications such as the indexing, summarizing and retrieving of audio segments. Accurate time alignments are also necessary when incorporating audio data into the training data for a speech recognizers acoustic model. We provide some initial analysis of manual transcriptions which show that there can be significant differences between the \"approximate\" manual transcripts generated by typical commercial transcription services and what was actually spoken in the recording. We then present a new alignment approach for approximate transcriptions of long audio files which is designed to discover and correct errors in the manual transcription during the alignment process.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-449"
  },
  "chang06b_interspeech": {
   "authors": [
    [
     "Shuangyu",
     "Chang"
    ]
   ],
   "title": "Improving speech recognition accuracy with multi-confidence thresholding",
   "original": "i06_1346",
   "page_count": 4,
   "order": 450,
   "p1": "paper 1346-Wed1CaP.3",
   "pn": "",
   "abstract": [
    "Confidence-based thresholding plays an important role in practical speech recognition applications. Most previous works have focused on directly improving confidence estimation within the recognition engine. A complementary approach that does not require access to recognizer internal is to optimize confidence threshold settings. This paper describes a general multi-confidence thresholding algorithm that automatically learns different confidence thresholds for different utterances, based on discreet or continuous features associated with a speech utterance. The algorithm can be applied to any speech recognition engine with a confidence output. A learned multi-threshold setting is guaranteed to perform at least as well as a baseline singlethreshold system on training data. A significant improvement on overall accuracy can often be obtained on test data, as demonstrated with experiments on two real-world applications.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-450"
  },
  "servan06_interspeech": {
   "authors": [
    [
     "Christophe",
     "Servan"
    ],
    [
     "Christian",
     "Raymond"
    ],
    [
     "Frédéric",
     "Béchet"
    ],
    [
     "Pascal",
     "Nocéra"
    ]
   ],
   "title": "Conceptual decoding from word lattices: application to the spoken dialogue corpus MEDIA",
   "original": "i06_1416",
   "page_count": 4,
   "order": 451,
   "p1": "paper 1416-Wed1CaP.4",
   "pn": "",
   "abstract": [
    "Within the framework of the French evaluation program MEDIA on spoken dialogue systems, this paper presents the methods proposed at the LIA for the robust extraction of basic conceptual constituents (or concepts) from an audio message. The conceptual decoding model proposed follows a stochastic paradigm and is directly integrated into the Automatic Speech Recognition (ASR) process. This approach allows us to keep the probabilistic search space on sequences of words produced by the ASR module and to project it to a probabilistic search space of sequences of concepts. This paper presents the first ASR results on the French spoken dialogue corpus MEDIA, available through ELDA. The experiments made on this corpus show that the performance reached by our approach is better than the traditional sequential approach that looks first for the best sequence of words before looking for the best sequence of concepts.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-451"
  },
  "huang06c_interspeech": {
   "authors": [
    [
     "Shilei",
     "Huang"
    ],
    [
     "Xiang",
     "Xie"
    ],
    [
     "Jingming",
     "Kuang"
    ]
   ],
   "title": "Improving the performance of out-of-vocabulary word rejection by using support vector machines",
   "original": "i06_1535",
   "page_count": 4,
   "order": 452,
   "p1": "paper 1535-Wed1CaP.5",
   "pn": "",
   "abstract": [
    "Support Vector Machines (SVM) represents a new approach to pattern classification developed from the theory of structural risk minimization [1]. In this paper, we propose an approach to improve the performance of confidence measurements for out-of-vocabulary word rejection by using SVM. Confidence measures are computed from the information of n-best candidates and anti-word by a Hidden Markov Model (HMM) based speech recognizer. The acceptance/rejection decision for a word is based on the confidence score which is provided by SVM classifier. And the decision is performed for each word in vocabulary separately. The performance of the proposed SVM classifier is compared with method based on posterior probability and anti-word probability. Experiments of Mandarin command recognition have showed that better performance can be obtained when using the proposed method.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-452"
  },
  "demuynck06_interspeech": {
   "authors": [
    [
     "Kris",
     "Demuynck"
    ],
    [
     "Dirk Van",
     "Compernolle"
    ],
    [
     "Hugo",
     "Van hamme"
    ]
   ],
   "title": "Robust phone lattice decoding",
   "original": "i06_1631",
   "page_count": 4,
   "order": 453,
   "p1": "paper 1631-Wed1CaP.6",
   "pn": "",
   "abstract": [
    "Most ASR systems adopt an all-in-one approach: acoustic model, lexicon and language model are all applied simultaneously, thus forming a single large search space. This way, both lexicon and language model help in constraining the search at an early stage which greatly improves its efficiency. However, such close integration comes at a cost: all resources must be kept simple. Achieving higher accuracy in unconstrained LVCSR tasks will require more complex resources while at the same time the unconstrainedness of the task reduces the effectiveness of the all-in-one approach. Therefore, we propose a modular two-layered architecture. First, a pure acoustic-phonemic search generates a dense phone network. Next a robust decoder finds those words from the lexicon that match well with the phone sequences encoded in the phone network. In this paper we investigate the properties the robust word decoder must have and we propose an efficient search algorithm.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-453"
  },
  "lecouteux06_interspeech": {
   "authors": [
    [
     "Benjamin",
     "Lecouteux"
    ],
    [
     "Georges",
     "Linarès"
    ],
    [
     "Pascal",
     "Nocéra"
    ],
    [
     "Jean-François",
     "Bonastre"
    ]
   ],
   "title": "Imperfect transcript driven speech recognition",
   "original": "i06_1660",
   "page_count": 4,
   "order": 454,
   "p1": "paper 1660-Wed1CaP.7",
   "pn": "",
   "abstract": [
    "In many cases, textual information can be associated with speech signals such as movie subtitles, theater scenarios, broadcast news summaries etc. This information could be considered as approximated transcripts and corresponds rarely to the exact word utterances. The goal of this work is to use this kind of information to improve the performance of an automatic speech recognition (ASR) system. Multiple applications are possible: to follow a play with closed caption aligned to the voice signal (while respecting to performer variations) to help deaf people, to watch a movie in another language using aligned and corrected closed captions, etc. We propose in this paper a method combining a linguistic analysis of the imperfect transcripts and a dynamic synchronization of these transcripts inside the search algorithm.\n",
    "The proposed technique is based on language model adaptation and on-line synchronization of the search algorithm. Experiments are carried out on an extract of the ESTER evaluation campaign [4] database, using the LIA Broadcast News system. The results show that the transcript-driven system outperforms significantly both the original recognizer and the imperfect transcript itself.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-454"
  },
  "xue06_interspeech": {
   "authors": [
    [
     "Jian",
     "Xue"
    ],
    [
     "Rusheng",
     "Hu"
    ],
    [
     "Yunxin",
     "Zhao"
    ]
   ],
   "title": "New improvements in decoding speed and latency for automatic captioning",
   "original": "i06_1739",
   "page_count": 4,
   "order": 455,
   "p1": "paper 1739-Wed1CaP.8",
   "pn": "",
   "abstract": [
    "In this paper, we present new improvements in decoding speed and latency for automatic captioning in telehealth. Complementary local word confidence scores are used to prune uncompetitive search paths. Subspace distribution clustering hidden Markov modeling (SDCHMM) is used for fast generation of acoustic and local confidence scores, where overlap accumulative probability (OAP) is used to measure the similarity of Gaussian pdfs in SDCHMM. We propose to use pre-backtrace based on detection of prosodic boundaries defined by unfilled pauses, filled pauses, as well as pitch contour to decrease latency. Experiments were conducted on a telehealth captioning task with vocabulary sizes of 21 K and 46 K. The proposed methods led to 33% improvement in decoding speed without loss of word accuracy, and to 3 folds of decrease in maximum latency with about 1.6% loss of word accuracy.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-455"
  },
  "saleem06_interspeech": {
   "authors": [
    [
     "Shirin",
     "Saleem"
    ],
    [
     "Rohit",
     "Prasad"
    ],
    [
     "Prem",
     "Natarajan"
    ]
   ],
   "title": "Colloquial Iraqi ASR for speech translation",
   "original": "i06_1771",
   "page_count": 4,
   "order": 456,
   "p1": "paper 1771-Wed1CaP.9",
   "pn": "",
   "abstract": [
    "In this paper we describe a real-time speech recognition system developed for colloquial Iraqi Arabic. This system is currently used in our speech-to-speech translation system configured for bi-directional communication in English and Iraqi on a laptop. We present experimental results on Iraqi utterances from different speech-to-speech translation domains, and analyze the usefulness of acoustic and language modeling data from different domains. We highlight the improvements obtained by modeling techniques that are language-independent, such as lattice-based discriminative training and domain-biased language model interpolation. In addition, we report on initial experiments we have performed to address specific challenges posed by Iraqi for speech recognition such as absence of short vowels and multiple forms of glottal stop, or the hamza, in the written form.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-456"
  },
  "hakamata06_interspeech": {
   "authors": [
    [
     "Tomohiro",
     "Hakamata"
    ],
    [
     "Akinobu",
     "Lee"
    ],
    [
     "Yoshihiko",
     "Nankaku"
    ],
    [
     "Keiichi",
     "Tokuda"
    ]
   ],
   "title": "Reducing computation on parallel decoding using frame-wise confidence scores",
   "original": "i06_1878",
   "page_count": 4,
   "order": 457,
   "p1": "paper 1878-Wed1CaP.10",
   "pn": "",
   "abstract": [
    "Parallel decoding based on multiple models has been studied to cover various conditions and speakers at a time on a speech recognition system. However, running many recognizers in parallel applying all models causes the total computational cost to grow in proportion to the number of models. In this paper, an efficient way of finding and pruning unpromising decoding processes during search is proposed. By comparing temporal search statistics at each frame among all decoders, decoders with relatively unmatched model can be pruned in the middle of recognition process to save computational cost. This method allows the model structures to be mutually independent. Two frame-wise pruning measures based on maximum hypothesis likelihoods and top confidence scores respectively, and their combinations are investigated. Experimental results on parallel recognition of seven acoustic models showed that by using the both criteria, the total computational cost was reduced to 36.53% compared to full computation without degrading the recognition accuracy.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-457"
  },
  "ketabdar06_interspeech": {
   "authors": [
    [
     "Hamed",
     "Ketabdar"
    ],
    [
     "Jithendra",
     "Vepa"
    ],
    [
     "Samy",
     "Bengio"
    ],
    [
     "Hervé",
     "Bourlard"
    ]
   ],
   "title": "Posterior based keyword spotting with a priori thresholds",
   "original": "i06_1939",
   "page_count": 4,
   "order": 458,
   "p1": "paper 1939-Wed1CaP.11",
   "pn": "",
   "abstract": [
    "In this paper, we propose a new posterior based scoring approach for keyword and non keyword (garbage) elements. The estimation of these scores is based on HMM state posterior probability definition, taking into account long contextual information and the prior knowledge (e.g. keyword model topology). The state posteriors are then integrated into keyword and garbage posteriors for every frame. These posteriors are used to make a decision on detection of the keyword at each frame. The frame level decisions are then accumulated (in this case, by counting) to make a global decision on having the keyword in the utterance. In this way, the contribution of possible outliers are minimized, as opposed to the conventional Viterbi decoding approach which accumulates likelihoods. Experiments on keywords from the Conversational Telephone Speech (CTS) and Numbers95 databases are reported. Results show that the new scoring approach leads to better trade off between true and false alarms compared to the Viterbi decoding approach, while also providing the possibility to precalculate keyword specific spotting thresholds related to the length of the keywords.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-458"
  },
  "zhou06_interspeech": {
   "authors": [
    [
     "Zhengyu",
     "Zhou"
    ],
    [
     "Helen M.",
     "Meng"
    ],
    [
     "Wai Kit",
     "Lo"
    ]
   ],
   "title": "A multi-pass error detection and correction framework for Mandarin LVCSR",
   "original": "i06_1947",
   "page_count": 4,
   "order": 459,
   "p1": "paper 1947-Wed1CaP.12",
   "pn": "",
   "abstract": [
    "We previously proposed a multi-pass framework for Large Vocabulary Continuous Speech Recognition (LVCSR). The objective of this framework is to apply sophisticated linguistic models for recognition, while maintaining a balance between complexity and efficiency. The framework is composed of three passes: initial recognition, error detection and error correction. This paper presents and evaluates a prototype of the multi-pass framework based on Mandarin dictation. In this prototype, the first pass recognizes speech with a well-trained state-of-the-art recognizer incorporating an efficient language model; the second pass detects recognition errors by a new three-step error detection procedure; and the third pass corrects errors detected in those lightly erroneous utterances by a novel error correction approach. The error correction algorithm corrects recognition errors by first creating candidate lists for errors, and then re-ranking the candidates with a combined model of mutual information and trigram. Mandarin dictation experiments show a relative reduction of 4% in character error rate (CER) over the initial recognition performance based on those light erroneous utterances detected.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-459"
  },
  "nouza06_interspeech": {
   "authors": [
    [
     "Jan",
     "Nouza"
    ],
    [
     "Jindrich",
     "Zdansky"
    ],
    [
     "Petr",
     "Cerva"
    ],
    [
     "Jan",
     "Kolorenc"
    ]
   ],
   "title": "Continual on-line monitoring of Czech spoken broadcast programs",
   "original": "i06_1478",
   "page_count": 4,
   "order": 460,
   "p1": "paper 1478-Wed1CaP.13",
   "pn": "",
   "abstract": [
    "In the paper we describe the development of the first practical system that performs automatic on-line monitoring of Czech broadcast stations. It is based on our own speech recognition server that operates with 300K word lexicon and 2.3 RT factor. For true on-line service, several servers are connected to the platform that controls acoustic stream segmentation, distribution of data to the servers, collection of results and production of the final transcription. We show practical results achieved on different types of broadcast programs, such as news (21% WER), parliament debates (21% WER) and talk-shows (34%).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-460"
  },
  "zhang06f_interspeech": {
   "authors": [
    [
     "Shilei",
     "Zhang"
    ],
    [
     "Hongchen",
     "Jiang"
    ],
    [
     "Shuwu",
     "Zhang"
    ],
    [
     "Bo",
     "Xu"
    ]
   ],
   "title": "Fast SVM training based on the choice of effective samples for audio classification",
   "original": "i06_1073",
   "page_count": 4,
   "order": 461,
   "p1": "paper 1073-Wed1FoP.1",
   "pn": "",
   "abstract": [
    "In this paper, we propose a new method to choose the effective samples for support vector machines (SVM) training based on regression tree in audio classification task. The objective is to reduce the training time of SVM by choosing effective examples from the training set and to balance the number of training points of binary classes. One obvious advantage of such method is that it provides a flexible framework to implement the choice procedure based on the training data for a given classification task. We test the performances of our new method on a dataset composed of about 6-hour audio data which illustrate that the computation time can be significantly reduced without a significant decrease in the prediction accuracy.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-461"
  },
  "schmalenstroeer06_interspeech": {
   "authors": [
    [
     "Joerg",
     "Schmalenstroeer"
    ],
    [
     "Reinhold",
     "Haeb-Umbach"
    ]
   ],
   "title": "Online speaker change detection by combining BIC with microphone array beamforming",
   "original": "i06_1078",
   "page_count": 4,
   "order": 462,
   "p1": "paper 1078-Wed1FoP.2",
   "pn": "",
   "abstract": [
    "In this paper we consider the problem of detecting speaker changes in audio signals recorded by distant microphones. It is shown that the possibility to exploit the spatial separation of speakers more than makes up the degradation in detection accuracy due to the increased source-to-sensor distance compared to close-talking microphones. Speaker direction information is derived from the filter coefficients of an adaptive Filter-and-Sum Beamformer and is combined with BIC analysis. The experimental results reveal significant improvements compared to BIC-only change detection, be it with the distant or close-talking microphone.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-462"
  },
  "ramirez06_interspeech": {
   "authors": [
    [
     "Javier",
     "Ramírez"
    ],
    [
     "Pablo",
     "Yélamos"
    ],
    [
     "J. M.",
     "Górriz"
    ],
    [
     "José C.",
     "Segura"
    ],
    [
     "L.",
     "García"
    ]
   ],
   "title": "Speech/non-speech discrimination combining advanced feature extraction and SVM learning",
   "original": "i06_1134",
   "page_count": 4,
   "order": 463,
   "p1": "paper 1134-Wed1FoP.3",
   "pn": "",
   "abstract": [
    "This paper shows an effective speech/non-speech discrimination method for improving the performance of speech processing systems working in noisy environment. The proposed method uses a trained support vector machine (SVM) that defines an optimized non-linear decision rule over different sets of speech features. Two alternative feature extraction processes based on: i) subband SNR estimation after denoising, and ii) long-term SNR estimation were compared. Both methods show the ability of the SVM-based classifier to learn how the signal is masked by the acoustic noise and to define an effective non-linear decision rule. However, it is shown that a feature vector incorporating contextual information yielded better speech/non-speech discrimination even when no denoising is applied. The experimental analysis carried out on the Spanish SpeechDat-Car database shows clear improvements over standard VADs including ITU G.729, ETSI AMR and ETSI AFE for distributed speech recognition (DSR), and other recently reported VADs.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-463"
  },
  "jarifi06_interspeech": {
   "authors": [
    [
     "Safaa",
     "Jarifi"
    ],
    [
     "Dominique",
     "Pastor"
    ],
    [
     "Olivier",
     "Rosec"
    ]
   ],
   "title": "Cooperation between global and local methods for the automatic segmentation of speech synthesis corpora",
   "original": "i06_1160",
   "page_count": 4,
   "order": 464,
   "p1": "paper 1160-Wed1FoP.4",
   "pn": "",
   "abstract": [
    "This paper introduces a new approach for the automatic segmentation of corpora dedicated to speech synthesis. The main idea behind this approach is to merge the outputs of three segmentation algorithms. The first one is the standard HMM-based (Hidden Markov Model) approach. The second algorithm uses a phone boundary model, namely a GMM (Gaussian Mixture Model). The third method is based on Brandts GLR (Generalized Likelihood Ratio) and aims to detect signal discontinuities in the vicinity of the HMM boundaries. Different combination strategies are considered for each phonetic class. The experiments presented in this paper show that the proposed approach yields better accuracy than existing methods.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-464"
  },
  "heckmann06_interspeech": {
   "authors": [
    [
     "Martin",
     "Heckmann"
    ],
    [
     "Marco",
     "Moebus"
    ],
    [
     "Frank",
     "Joublin"
    ],
    [
     "Christian",
     "Goerick"
    ]
   ],
   "title": "Speaker independent voiced-unvoiced detection evaluated in different speaking styles",
   "original": "i06_1249",
   "page_count": 4,
   "order": 465,
   "p1": "paper 1249-Wed1FoP.5",
   "pn": "",
   "abstract": [
    "We propose a new algorithm for voiced/unvoiced classification of speech on a phoneme or sample level. The algorithm is inspired by auditory based approaches and combines two cues. One cue is based on the energy distribution of the signal and the other on the harmonicity. In order to extract the harmonicity of the signal we calculate a histogram of the zero crossings of the filter channels after applying a Gammatone filterbank to the signal. A measure similar to the variance of the zero crossings yields the harmonicity cue. The performance of the algorithm was measured on several minutes of read and spontaneous speech with various speakers. An algorithm proposed by Mustafa et al. [1] served as benchmark. The results show that our algorithm performs significantly better as well on read as on spontaneous speech and seems in particular be better able to cope with different speaking styles.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-465"
  },
  "anguera06b_interspeech": {
   "authors": [
    [
     "Xavier",
     "Anguera"
    ],
    [
     "Chuck",
     "Wooters"
    ],
    [
     "Jose M.",
     "Pardo"
    ]
   ],
   "title": "Robust speaker diarization for meetings: ICSI RT06s evaluation system",
   "original": "i06_1716",
   "page_count": 4,
   "order": 466,
   "p1": "paper 1716-Wed1FoP.6",
   "pn": "",
   "abstract": [
    "In this paper we present the ICSI speaker diarization system submitted for the NIST Rich Transcription evaluation (RT06s) [1] conducted on the meetings environment. This is a set of yearly evaluations which in the last two years have included speaker diarization of two kinds of distinct meetings: conference room and lecture room. The system presented focuses on being robust to changes in the meeting conditions by not using any training data. In this paper we introduce four of the main improvements to the system from last years submission: The first is a new training-free speech/non-speech detection algorithm. The second is the introduction of a new algorithm for system initialization. The third is the use of a frame purification algorithm to increase clusters differentiability. The last improvement is the use of inter-channel delays as features, greatly improving performance. We show the diarization error rate (DER) score of this system on all available meeting datasets to date for the multiple distant microphone (MDM) and single distant microphone (SDM) conditions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-466"
  },
  "coy06_interspeech": {
   "authors": [
    [
     "André",
     "Coy"
    ],
    [
     "Jon",
     "Barker"
    ]
   ],
   "title": "A multipitch tracker for monaural speech segmentation",
   "original": "i06_1000",
   "page_count": 4,
   "order": 467,
   "p1": "paper 1000-Wed1FoP.7",
   "pn": "",
   "abstract": [
    "This paper presents a novel algorithm for forming coherent harmonic fragments from a mixture of speech sources. A multiple\npitch detection algorithm is used to produce pitch candidates which are tracked using a pair of parallel HMMs. One novel\naspect of the technique is that it systematically models pitch doubling and halving errors, thereby facilitating the identification of smooth pitch segments even in the absence of the fundamental frequency. The system does not face the problem of incorrect source assignment that can occur when sources have similar fundamental frequency or are harmonically related. An evaluation of the technique shows that the algorithm’s emphasis on tracking coherent segments leads to the formation of speech fragments with high coherence, indicating a more reliable segmentation of the harmonic speech regions."
   ],
   "doi": "10.21437/Interspeech.2006-467"
  },
  "chitturi06b_interspeech": {
   "authors": [
    [
     "Rahul",
     "Chitturi"
    ],
    [
     "Mark",
     "Hasegawa-Johnson"
    ]
   ],
   "title": "Novel entropy based moving average refiners for HMM landmarks",
   "original": "i06_1911",
   "page_count": 4,
   "order": 468,
   "p1": "paper 1911-Wed1FoP.8",
   "pn": "",
   "abstract": [
    "The training of precise speech recognition models depends on accurate segmentation of the phonemes in a training corpus. Segmentation is typically performed using HMMs, but recent speech recognition work suggests that the transient acoustic features characteristic of manner-class phoneme boundaries (landmarks) may be more precisely localized using acoustic classifiers specifically designed for the task of landmark detection. This paper makes an empirical exploration of entropy based moving average techniques that are capable of improving the time alignment of phoneme boundaries proposed by an HMM-based speech recognizer. On a standard benchmark data set (A database of Hindi - National Language of India), we achieve new state-of-the-art performance, reducing RMS phone boundary alignment error from 28ms to 15ms.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-468"
  },
  "kim06e_interspeech": {
   "authors": [
    [
     "Gibak",
     "Kim"
    ],
    [
     "Nam Ik",
     "Cho"
    ]
   ],
   "title": "Two-microphone voice activity detection in the presence of coherent interference",
   "original": "i06_1917",
   "page_count": 4,
   "order": 469,
   "p1": "paper 1917-Wed1FoP.9",
   "pn": "",
   "abstract": [
    "In this paper, we propose a two-microphone Voice Activity Detection (VAD) method in the presence of coherent interference. The proposed method is based on the Cross Power Spectrum Phase (CPSP) which is an implementation of the Phase Transform (PHAT) weighted cross correlation between two microphones. The PHAT weighting whitens the spectrum of input signals and makes the cross correlation dependent entirely on the phase of the cross spectrum. If we assume that the direction of desired speech signal is known and the time delay between microphones is compensated, the Averaged CPSP (A-CPSP) can be utilized as a VAD measure. In order to enhance the VAD performance in the presence of strong coherent interference from other direction, we propose a Maximum Partially Averaged Real CPSP (MPA-RCPSP) method which detects the cophased frequency region with high Signal-to-Interference Ratio (SIR). Simulation results demonstrate that the proposed MPA-RCPSP is a more reliable measure to the conventional A-CPSP in the presence of strong coherent interference.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-469"
  },
  "myrvoll06_interspeech": {
   "authors": [
    [
     "Tor André",
     "Myrvoll"
    ],
    [
     "Tomoko",
     "Matsui"
    ]
   ],
   "title": "On a greedy learning algorithm for dPLRM with applications to phonetic feature detection",
   "original": "i06_2063",
   "page_count": 4,
   "order": 470,
   "p1": "paper 2063-Wed1FoP.10",
   "pn": "",
   "abstract": [
    "In this work we investigate the use of a greedy training algorithm for the dual Penalized Logistic Regression Machine (dPLRM), and our target application is detection of broad class phonetic features. The use of a greedy training algorithm is meant to alleviate the infeasible memory and computational demands that arises during the learning phase when the amount of training data increases. We show that using only a subset of the training data, chosen in a greedy manner, we can achieve as good as or better performance as when using the full training set. We can also train dPLRMs using data sets that are significantly larger than what our current computational resources can accommodate when using non-greedy approaches.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-470"
  },
  "mooreii06_interspeech": {
   "authors": [
    [
     "Elliot",
     "Moore II"
    ],
    [
     "Juan",
     "Torres"
    ]
   ],
   "title": "Improving glottal waveform estimation through rank-based glottal quality assessment",
   "original": "i06_1296",
   "page_count": 4,
   "order": 471,
   "p1": "paper 1296-Wed2A1O.1",
   "pn": "",
   "abstract": [
    "Information on the glottal waveform is an important part of many speech applications. However, glottal waveform estimation remains one of the more inexact sciences of speech processing. The work presented here describes an enhancement to a recently presented algorithm by a new technique involving Rank-Based Glottal Quality Assessment (RB-GQA). The basic premise is to investigate potential measures of glottal quality and use these measures to mark the general trends for determining which glottal waveform estimations are better than others. The work presented here is the beginning of a new research initiative to identify robust methods of glottal waveform estimation across genders for use in speaker analysis applications of normal voices (i.e., no voice pathology).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-471"
  },
  "alias06b_interspeech": {
   "authors": [
    [
     "Francesc",
     "Alías"
    ],
    [
     "Carlos",
     "Monzo"
    ],
    [
     "Joan Claudi",
     "Socoró"
    ]
   ],
   "title": "A pitch marks filtering algorithm based on restricted dynamic programming",
   "original": "i06_1625",
   "page_count": 4,
   "order": 472,
   "p1": "paper 1625-Wed2A1O.2",
   "pn": "",
   "abstract": [
    "In this paper, a generic pitch marks filtering algorithm (PMFA) is introduced in order to achieve reliable and smooth pitch marks from any input pitch tracking or marking algorithm. The proposed PMFA is a simple yet effective filtering process based on restricted dynamic programming, but very helpful for minimizing human intervention when creating large speech corpora. Moreover, this work introduces a novel pitch marking evaluation measure for directly comparing pitch marking algorithms with different location criteria. The experiments demonstrate that the proposed PFMA improves the results of the input state-of-the-art pitch tracking and marking algorithms dramatically.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-472"
  },
  "malyska06_interspeech": {
   "authors": [
    [
     "Nicolas",
     "Malyska"
    ],
    [
     "Thomas F.",
     "Quatieri"
    ]
   ],
   "title": "Analysis of nonmodal phonation using minimum entropy deconvolution",
   "original": "i06_1807",
   "page_count": 4,
   "order": 473,
   "p1": "paper 1807-Wed2A1O.3",
   "pn": "",
   "abstract": [
    "Nonmodal phonation occurs when glottal pulses exhibit non-uniform pulse-to-pulse characteristics such as irregular spacings, amplitudes, and/or shapes. The analysis of regions of such nonmodality has application to automatic speech, speaker, language, and dialect recognition. In this paper, we examine the usefulness of a technique called minimum-entropy deconvolution, or MED [1], for the analysis of pulse events in nonmodal speech. Our study presents evidence for both natural and synthetic speech that MED decomposes nonmodal phonation into a series of sharp pulses and a set of mixed-phase impulse responses. We show that the estimated impulse responses are quantitatively similar to those in our synthesis model. A hybrid method incorporating aspects of both MED and linear prediction is also introduced. We show preliminary evidence that the hybrid method has benefit over MED alone for composite impulse-response estimation by being more robust to short-time windowing effects as well as a speech aspiration noise component.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-473"
  },
  "nakano06_interspeech": {
   "authors": [
    [
     "Tomoyasu",
     "Nakano"
    ],
    [
     "Masataka",
     "Goto"
    ],
    [
     "Yuzuru",
     "Hiraga"
    ]
   ],
   "title": "An automatic singing skill evaluation method for unknown melodies using pitch interval accuracy and vibrato features",
   "original": "i06_1854",
   "page_count": 4,
   "order": 474,
   "p1": "paper 1854-Wed2A1O.4",
   "pn": "",
   "abstract": [
    "This paper presents a method of evaluating singing skills that does not require score information of the sung melody. This requires an approach that is different from existing systems, such as those currently used for Karaoke systems. Previous research on singing evaluation has focused on analyzing the characteristics of singing voice, but were not aimed at developing an automatic evaluation method. The approach presented in this study uses pitch interval accuracy and vibrato as acoustic features which are independent from specific characteristics of the singer or melody. The approach was tested by a 2-class (good/poor) classification test with 600 song sequences, and achieved an average classification rate of 83.5%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-474"
  },
  "zahorian06_interspeech": {
   "authors": [
    [
     "Stephen A.",
     "Zahorian"
    ],
    [
     "Princy",
     "Dikshit"
    ],
    [
     "Hongbing",
     "Hu"
    ]
   ],
   "title": "A spectral-temporal method for pitch tracking",
   "original": "i06_1910",
   "page_count": 4,
   "order": 475,
   "p1": "paper 1910-Wed2A1O.5",
   "pn": "",
   "abstract": [
    "In this paper, a new spectral/temporal method is described for robust pitch tracking for both high quality and telephone speech. A previous version of this algorithm was presented as YAAPT (Kasi and Zahorian, 2002) [10]. In the current paper, a novel method is presented for spectral pitch tracking, using nonlinear processing to partially restore the potentially missing fundamental frequency. A frequency domain modified autocorrelation is used to determine the spacing between harmonic peaks in the spectrum. The frequency domain spectral track is then used to refine time-domain pitch candidates obtained using the \"NCCF or Normalized Cross Correlation\" reported by Talkin [1]. Dynamic programming is used to find the \"best\" pitch track among all the candidates, using both local and transition costs. The algorithm was evaluated using the Keele pitch extraction reference database.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-475"
  },
  "rahman06_interspeech": {
   "authors": [
    [
     "M. Shahidur",
     "Rahman"
    ],
    [
     "Hirobumi",
     "Tanaka"
    ],
    [
     "Tetsuya",
     "Shimamura"
    ]
   ],
   "title": "Pitch determination using aligned AMDF",
   "original": "i06_1960",
   "page_count": 4,
   "order": 476,
   "p1": "paper 1960-Wed2A1O.6",
   "pn": "",
   "abstract": [
    "A pitch determination method based on AMDF (Average Magnitude Difference Function) is proposed in this paper. The AMDF is often used to determine the pitch parameter in real-time speech processing applications. Falling trend of AMDF at higher lags, however, makes the method vulnerable to octave errors (pitch doubling or halving). In this paper, we propose an alignment technique that effectively eliminates the falling trend by aligning the AMDF peaks along a straight line. Experimental results on speech signals spoken by male and female speakers show that the current method can reduce the occurrence of octave errors in greater numbers when compared with other AMDF based functions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-476"
  },
  "han06b_interspeech": {
   "authors": [
    [
     "Yan",
     "Han"
    ],
    [
     "Lou",
     "Boves"
    ]
   ],
   "title": "Syllable-length path mixture hidden Markov models with trajectory clustering for continuous speech recognition",
   "original": "i06_1460",
   "page_count": 4,
   "order": 477,
   "p1": "paper 1460-Wed2A2O.1",
   "pn": "",
   "abstract": [
    "Recent research suggests that modeling coarticulation in speech is more appropriate at the syllable level. However, due to a number of additional factors that can affect the way syllables are articulated, creating multiple acoustic models per syllable might be necessary. Our previous research on longer-length multi-path models has proved that data-driven trajectory clustering to be an attractive approach to derive multi-path models. However, the use of single distribution with unvarying covariance to model a trajectory cluster may degrade its capability of detecting pronunciation variants. In this paper, we propose a new method, namely path mixture hidden Markov model, to alleviate the adverse effects of trajectory clustering. The improvement on performance observed in continuous speech recognition experiments show path mixture model is a very effective approach.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-477"
  },
  "cincarek06_interspeech": {
   "authors": [
    [
     "Tobias",
     "Cincarek"
    ],
    [
     "Tomoki",
     "Toda"
    ],
    [
     "Hiroshi",
     "Saruwatari"
    ],
    [
     "Kiyohiro",
     "Shikano"
    ]
   ],
   "title": "Acoustic modeling for spoken dialogue systems based on unsupervised utterance-based selective training",
   "original": "i06_1481",
   "page_count": 4,
   "order": 478,
   "p1": "paper 1481-Wed2A2O.2",
   "pn": "",
   "abstract": [
    "The construction of high-performance acoustic models for certain speech recognition tasks is very costly and time-consuming, since it most often requires the collection and transcription of large amounts of task-specific speech data. In this paper acoustic modeling for spoken dialogue systems based on unsupervised selective training is examined. The main idea is to select those training utterances from an (untranscribed) speech data pool, so that the likelihood of a separate small (transcribed) development speech data set is maximized. If only the selected data are employed to retrain the initial acoustic models, a better performance is achieved than when retraining with all collected data. Using the proposed approach it is also possible to considerably reduce the costs for human-labeling of the speech data without compromising the performance. Furthermore, the method provides means for automatic task-adaptation of acoustic models, e.g. to adult or children speech. This is important, since detailed information about each automatically collected utterance is usually not available.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-478"
  },
  "levy06_interspeech": {
   "authors": [
    [
     "Christophe",
     "Lévy"
    ],
    [
     "Georges",
     "Linarès"
    ],
    [
     "Jean-François",
     "Bonastre"
    ]
   ],
   "title": "GMM-based acoustic modeling for embedded speech recognition",
   "original": "i06_1255",
   "page_count": 4,
   "order": 479,
   "p1": "paper 1255-Wed2A2O.3",
   "pn": "",
   "abstract": [
    "Speech recognition applications are known to require a significant amount of resources (training data, memory, computing power). However, the targeted context of this work - mobile phone embedded speech recognition system - only authorizes few KB of memory, few MIPS and usually small amount of training data.\n",
    "In order to fit the resource constraints, an approach based on a semi-continuous HMM system using a GMM-based state-independent acoustic modeling is proposed in this paper. A transformation is computed and applied to the global GMM in order to obtain each of the HMM state-dependent probability density functions. This strategy aims at storing only the transformation function parameters for each state and authorizes to decrease the amount of computing power needed for the likelihood computation.\n",
    "The proposed approach is evaluated on two tasks: a digit recognition task using the French corpus BDSON (which allows a Digit Error Rate of 2.5%) and a voice command task using French corpus VODIS (the Command Error Rate leads around 4.1%).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-479"
  },
  "wachter06_interspeech": {
   "authors": [
    [
     "Mathias De",
     "Wachter"
    ],
    [
     "Kris",
     "Demuynck"
    ],
    [
     "Dirk Van",
     "Compernolle"
    ]
   ],
   "title": "Boosting HMM performance with a memory upgrade",
   "original": "i06_1126",
   "page_count": 4,
   "order": 480,
   "p1": "paper 1126-Wed2A2O.4",
   "pn": "",
   "abstract": [
    "The state-of-the-art in automatic speech recognition is distinctly Markovian. The ubiquitous beads-on-a-string approach, where sentences are explained as a sequence of words, words as a sequence of phones and phones as a sequence of acoustically stable states, is bound to lose a lot of dynamic information. In this paper we show that a combination with example-based recognition can be used to recapture some of that information. A new approach to combine Hidden Markov Model (HMM) and phone-example-based continuous speech recognition is presented. Experiments show that the combination outperforms the HMM recognizer, and indicate that adding long-span information is especially beneficial.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-480"
  },
  "deng06_interspeech": {
   "authors": [
    [
     "Y.",
     "Deng"
    ],
    [
     "X.",
     "Li"
    ],
    [
     "C.",
     "Kwan"
    ],
    [
     "R.",
     "Xu"
    ],
    [
     "B.",
     "Raj"
    ],
    [
     "Richard M.",
     "Stern"
    ],
    [
     "D.",
     "Williamson"
    ]
   ],
   "title": "An integrated approach to improve speech recognition rate for non-native speakers",
   "original": "i06_1472",
   "page_count": 4,
   "order": 481,
   "p1": "paper 1472-Wed2A2O.5",
   "pn": "",
   "abstract": [
    "The current speech interfaces in many military applications may be adequate for native speakers. However, the recognition rate drops quite a lot for non-native speakers (people with foreign accents). This is mainly because the non-native speakers have large temporal and intra-phoneme variations when they pronounce the same words. This problem is also complicated by the presence of loud environmental noise such as tank noise, helicopter noise, etc. In this paper, we proposed a novel speech feature adaptation algorithm for continuous accent and environmental adaptation. This feature-based adaptation method is then integrated with conventional model-based maximum likelihood linear regression (MLLR) algorithm. Extensive experiments have been performed on the NATO non-native speech corpus with baseline acoustic model trained on native American English. The proposed feature-based adaptation algorithm improved the average recognition accuracy by 15%, while the MLLR model-based adaptation achieved 11% improvement. The combined adaptation achieved overall recognition accuracy improvement of 29.5%, and word error rate reduction of 31.8%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-481"
  },
  "hu06b_interspeech": {
   "authors": [
    [
     "Rusheng",
     "Hu"
    ],
    [
     "Yunxin",
     "Zhao"
    ]
   ],
   "title": "Bayesian decision tree state tying for conversational speech recognition",
   "original": "i06_1263",
   "page_count": 4,
   "order": 482,
   "p1": "paper 1263-Wed2A2O.6",
   "pn": "",
   "abstract": [
    "This paper presents a new method of constructing phonetic decision trees (PDTs) for acoustic model state tying based on implicitly induced prior knowledge. Our hypothesis is that knowledge on pronunciation variation in spontaneous, conversational speech contained in a relatively large corpus can be used for building domain-specific or speaker-dependent PDTs. In the view of tree structure adaptation, this method leads to transformation of tree topology in contrast to keeping fixed tree structure as in traditional methods of speaker adaptation. A Bayesian learning framework is proposed to incorporate prior knowledge on decision rules in a greedy search of new decision trees, where the prior is generated by a decision tree growing process on a large data set. Experimental results on the Telemedicine automatic captioning task demonstrate that the proposed approach results in consistent improvement in model quality and recognition accuracy.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-482"
  },
  "kirkpatrick06_interspeech": {
   "authors": [
    [
     "Barry",
     "Kirkpatrick"
    ],
    [
     "Darragh",
     "O’Brien"
    ],
    [
     "Ronán",
     "Scaife"
    ]
   ],
   "title": "Feature extraction for spectral continuity measures in concatenative speech synthesis",
   "original": "i06_1385",
   "page_count": 4,
   "order": 483,
   "p1": "paper 1385-Wed2A3O.1",
   "pn": "",
   "abstract": [
    "The quality of concatenative speech synthesis depends on the cost function employed for unit selection. Effective cost functions for spectral continuity are difficult to define and standard measures often do not accurately reflect human perception of discontinuity across a concatenated join. In this study the performance of a number of standard distance measures are compared for the task of detecting audible discontinuities in concatenated speech. Feature sets derived from the phase spectrum are also investigated. Feature extraction based on wavelet analysis is proposed to overcome some of the limitations of the standard measures tested. Receiver Operating Characteristic (ROC) curves are constructed for each measure from the results of a perceptual experiment and are used to rank the performance of each measure. Results indicate that phase spectra is comparable to magnitude spectra as a join cost for spectral continuity. Measures based on wavelet transform coefficients outperform all other measures tested.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-483"
  },
  "sakai06_interspeech": {
   "authors": [
    [
     "Shinsuke",
     "Sakai"
    ],
    [
     "Tatsuya",
     "Kawahara"
    ]
   ],
   "title": "Decision tree-based training of probabilistic concatenation models for corpus-based speech synthesis",
   "original": "i06_1564",
   "page_count": 4,
   "order": 484,
   "p1": "paper 1564-Wed2A3O.2",
   "pn": "",
   "abstract": [
    "The measure of the goodness, or cost, of concatenating synthesis units plays an important role in concatenative speech synthesis. In this paper, we present a probabilistic approach to concatenation modeling in which the goodness of concatenation is represented as the conditional probability of observing the spectral shape of a unit given the previous unit and the current phonetic context. This conditional probability is modeled by a conditional Gaussian density whose mean vector has a form of linear transform of the past spectral shape. A phonetic decision-tree based parameter tying is performed to achieve a robust training that balances between model complexity and the amount of training data available. The concatenation models are implemented in a corpus-based speech synthesizer trained with a CMU Arctic database and the effectiveness of the proposed method was confirmed by a subjective listening test.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-484"
  },
  "zhao06_interspeech": {
   "authors": [
    [
     "Yong",
     "Zhao"
    ],
    [
     "Di",
     "Peng"
    ],
    [
     "Lijuan",
     "Wang"
    ],
    [
     "Min",
     "Chu"
    ],
    [
     "Yining",
     "Chen"
    ],
    [
     "Peng",
     "Yu"
    ],
    [
     "Jun",
     "Guo"
    ]
   ],
   "title": "Constructing stylistic synthesis databases from audio books",
   "original": "i06_1559",
   "page_count": 4,
   "order": 485,
   "p1": "paper 1559-Wed2A3O.3",
   "pn": "",
   "abstract": [
    "In this paper, we explore how to construct stylistic TTS databases from audio books, in which a storyteller performs multiple roles. The goal is to identify and build a set of speech corpora, each of which not only portrays a representative voice style performed by the speaker, but also has sufficient sentences to synthesize natural speech using unit selection approach. We solve the problem in two procedures: first, by representing each role with Gaussian Mixture Models (GMM), all speech data are partitioned into a number of voice style clusters with a criterion that maximizes the likelihood of all utterances with respect to roles speaker models; then, pruning in terms of both acoustic and prosodic measures is followed to purify the clusters. The resulting 4 voice styles are subjectively interpreted as Neutral, Young, Elder and Adult, respectively. Perceptual experiments show that the proposed approach can synthesize speech with the recognizable voice styles with an average 72.5% identification rate, and the synthesized speech sounds better than those synthesized with utterances from a single role.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-485"
  },
  "conkie06_interspeech": {
   "authors": [
    [
     "Alistair",
     "Conkie"
    ],
    [
     "Ann K.",
     "Syrdal"
    ]
   ],
   "title": "Expanding phonetic coverage in unit selection synthesis through unit substitution from a donor voice",
   "original": "i06_2001",
   "page_count": 4,
   "order": 486,
   "p1": "paper 2001-Wed2A3O.4",
   "pn": "",
   "abstract": [
    "This paper describes experiments with synthetic voices using unit selection [1] concatenative synthesis where portions of the database audio recordings are modified for the purpose of producing a wider set of phonemes than is contained in the original voice recordings. Since it is known that performing global signal modification for the purposes of speech synthesis significantly reduces perceived voice quality [2] [3], the modifications that we perform are specifically confined to aperiodic portions of the signal that tend neither to cause concatenation discontinuities nor to convey much of the individual character or affect of the speaker.\n",
    "We propose three methods to extend the phonetic coverage of unit selection voices (1) by modifying parts of a voice so that extra phones extracted from a donor voice can be added off line; (2) by extending the above methodology by using a harmonic plus noise model (HNM) [4] for speech representation in order to control aspects of the modification; (3) by combining recorded inventories from two voices so that at synthesis time selections can be made from either.\n",
    "Experiments were conducted to evaluate the strengths and weaknesses of the three methods.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-486"
  },
  "taylor06b_interspeech": {
   "authors": [
    [
     "Paul",
     "Taylor"
    ]
   ],
   "title": "Unifying unit selection and hidden Markov model speech synthesis",
   "original": "i06_1456",
   "page_count": 4,
   "order": 487,
   "p1": "paper 1456-Wed2A3O.5",
   "pn": "",
   "abstract": [
    "This paper presents a framework which can accommodate the two most widely used contemporary speech synthesis techniques, namely unit selection and hidden Markov models (HMMs). This is achieved by building a very general HMM where we have a network of states, each representing a single frame for a single unit. This network exactly mimics the behaviour of a unit selection system and is effectively memorising the data as an HMM. From this, we can merge states in the network so as to produce a synthesis system of any desired size. The paper discusses this technique as well as a statistical formulation of the join cost and a number of ways to represent the acoustic observations of the states.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-487"
  },
  "black06_interspeech": {
   "authors": [
    [
     "Alan W.",
     "Black"
    ]
   ],
   "title": "CLUSTERGEN: a statistical parametric synthesizer using trajectory modeling",
   "original": "i06_1394",
   "page_count": 4,
   "order": 488,
   "p1": "paper 1394-Wed2A3O.6",
   "pn": "",
   "abstract": [
    "Unit selection synthesis has shown itself to be capable of producing high quality natural sounding synthetic speech when constructed from large databases of well-recorded, well-labeled speech. However, the cost in time and expertise of building such voices is still too expensive and specialized to be able to build individual voices for everyone. The quality in unit selection synthesis is directly related to the quality and size of the database used. As we require our speech synthesizers to have more variation, style and emotion, for unit selection synthesis, much larger databases will be required. As an alternative, more recently we have started looking for parametric models for speech synthesis, that are still trained from databases of natural speech but are more robust to errors and allow for better modeling of variation. This paper presents the CLUSTERGEN synthesizer which is implemented within the Festival/FestVox voice building environment. As well as the basic technique, three methods of modeling dynamics in the signal are presented and compared: a simple point model, a basic trajectory model and a trajectory model with overlap and add.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-488"
  },
  "rieser06_interspeech": {
   "authors": [
    [
     "Verena",
     "Rieser"
    ],
    [
     "Oliver",
     "Lemon"
    ]
   ],
   "title": "Cluster-based user simulations for learning dialogue strategies",
   "original": "i06_1127",
   "page_count": 4,
   "order": 489,
   "p1": "paper 1127-Wed2WeS.1",
   "pn": "",
   "abstract": [
    "Good dialogue strategies in spoken dialogue systems help to ensure and maintain mutual understanding and thus play a crucial role in robust conversational interaction. We focus on clarification strategies and build user simulations which are critical for reinforcement learning, which is a cheap and principled way to automatically optimise dialogue management. In this paper we present a novel cluster-based technique for building user simulations which show varying, but complete and consistent behaviour with respect to real users. We use this technique to build user simulations and we also introduce the super evaluation metric which allows us to evaluate user simulations with respect to these desiderata. We show that the cluster-based user simulation technique performs significantly better (at P < 0.01) than decisions made using either the one most likely action or a random baseline. The cluster-based user simulations reduce the average error of these other models by 53% and 34% respectively.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-489"
  },
  "lewis06_interspeech": {
   "authors": [
    [
     "Charles",
     "Lewis"
    ],
    [
     "Giuseppe Di",
     "Fabbrizio"
    ]
   ],
   "title": "Prompt selection with reinforcement learning in an AT&t call routing application",
   "original": "i06_1744",
   "page_count": 4,
   "order": 490,
   "p1": "paper 1744-Wed2WeS.2",
   "pn": "",
   "abstract": [
    "Reinforcement Learning (RL) algorithms provide a type of unsupervised learning that is especially well suited for the challenges of spoken dialogue systems (SDS) design. SDS are constantly subjected to new environments in the form of new groups of users, and RL provides an approach for automated learning that can adapt to new environments without costly supervision. In this paper, we describe some results from experiments with RL to select prompts for a call routing application. A simulation of the dialogue outcomes were used to experiment with different scenarios and demonstrate how RL can make a system more robust without supervision or developer intervention.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-490"
  },
  "goronzy06_interspeech": {
   "authors": [
    [
     "Silke",
     "Goronzy"
    ],
    [
     "Raquel",
     "Mochales"
    ],
    [
     "Nicole",
     "Beringer"
    ]
   ],
   "title": "Developing speech dialogs for multimodal HMIs using finite state machines",
   "original": "i06_1544",
   "page_count": 4,
   "order": 491,
   "p1": "paper 1544-Wed2WeS.3",
   "pn": "",
   "abstract": [
    "We present a tool for model-based development of multimodal interfaces. The HMI model captures all involved modalities, thus ensuring highly consistent interfaces. In this paper we focus on the development of speech dialogs. These are specified using state machines, which is in contrast to the traditional way of using flow-charts. The usage of state machines gives us the possibility to fully specify the HMI so that it contains enough information to be fully simulated without the need to connect any target applications as well as for automatic target code generation. Due to the extensive simulation capabilities usability evaluations can be conducted at very early design stages. We further explain how different dialog strategies for different user types can be developed with the help of the user modelling plug-in. The tool thus supports the whole development chain starting from design studies to specification, development and testing over usability studies and target implementation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-491"
  },
  "pfleger06_interspeech": {
   "authors": [
    [
     "Norbert",
     "Pfleger"
    ],
    [
     "Jan",
     "Schehl"
    ]
   ],
   "title": "Development of advanced dialog systems with PATE",
   "original": "i06_1598",
   "page_count": 4,
   "order": 492,
   "p1": "paper 1598-Wed2WeS.4",
   "pn": "",
   "abstract": [
    "Current commercial dialog systems show only limited capabilities with regard to the phenomena occurring in spontaneous, natural dialog. Many research prototypes, in contrast, are already able to deal with a great number of phenomena but lack the clarity and maintainability of commercial systems. In this paper we present a framework for developing advanced multimodal dialog systems designed to bridge this gap.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-492"
  },
  "subramanian06_interspeech": {
   "authors": [
    [
     "Rajah Annamalai",
     "Subramanian"
    ],
    [
     "Philip",
     "Cohen"
    ]
   ],
   "title": "A joint intention-based dialogue engine",
   "original": "i06_1843",
   "page_count": 4,
   "order": 493,
   "p1": "paper 1843-Wed2WeS.5",
   "pn": "",
   "abstract": [
    "In this paper we discuss some of the advantages of using a joint intention-based interpreter for building spoken dialogue systems. We describe STAPLE [1], a joint-intention interpreter that enables a system to obtain team and communicative behavior automatically without having to program this behavior explicitly. With this approach there is no necessity for a programmer to indicate when a question should be posed, when information should be shared etc. The interpreter enables the agents to exhibit team-oriented dialogue by interpreting the constructs of Joint Intention Theory (JIT) along with first principles reasoning over a formal semantics of communicative acts [2, 3]. We try to show that STAPLE can subsume and extend the finite-state and frame-based dialogue approaches available today from commercial dialogue systems. In particular, we show how STAPLE can handle over-answering, dynamic environment changes and teamwork in a general domain independent manner.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-493"
  },
  "moller06_interspeech": {
   "authors": [
    [
     "Sebastian",
     "Möller"
    ],
    [
     "Roman",
     "Englert"
    ],
    [
     "Klaus",
     "Engelbrecht"
    ],
    [
     "Verena",
     "Hafner"
    ],
    [
     "Anthony",
     "Jameson"
    ],
    [
     "Antti",
     "Oulasvirta"
    ],
    [
     "Alexander",
     "Raake"
    ],
    [
     "Norbert",
     "Reithinger"
    ]
   ],
   "title": "Memo: towards automatic usability evaluation of spoken dialogue services by user error simulations",
   "original": "i06_1131",
   "page_count": 4,
   "order": 494,
   "p1": "paper 1131-Wed2WeS.6",
   "pn": "",
   "abstract": [
    "Proper usability evaluations of spoken dialogue systems are costly and cumbersome to carry out. In this paper, we present a new approach for facilitating usability evaluations which is based on user error simulations. The idea is to replace real users with simulations derived from empirical observations of users erroneous behavior. The simulated errors must cover both system-driven errors (e.g., due to poor speech recognition) as well as conceptual errors and slips of the user, because neither alone is predictive of perceived usability. The simulation is integrated into a workbench which produces reports of typical and rare errors, and which allows usability ratings to be predicted. If successful, this workbench will help designers in making choices between system versions and lower testing costs at early phases of development. Challenges to the approach are discussed and solutions proposed.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-494"
  },
  "matthews06_interspeech": {
   "authors": [
    [
     "Brett",
     "Matthews"
    ],
    [
     "Raimo",
     "Bakis"
    ],
    [
     "Ellen",
     "Eide"
    ]
   ],
   "title": "Synthesizing breathiness in natural speech with sinusoidal modelling",
   "original": "i06_1087",
   "page_count": 4,
   "order": 495,
   "p1": "paper 1087-Wed2BuP.1",
   "pn": "",
   "abstract": [
    "This paper discusses recent work in synthesizing a breathy quality in pre-recorded speech, which has applications in voice morphing and concatenative TTS. Previous work has shown that the breathy quality in speech is characterized in part by the presence of random noise in the upper region of the spectrum [1]. The sinusoidal modelling representation of speech facilitates making high-quality modifications to speech signals as well as modifying regions of the spectrum independently. We use sinusoidal modelling, along with techniques borrowed from analog communication systems to simulate aspiration noise in wideband speech signals above some lower cutoff frequency. Specifically, we use techniques based on amplitude modulation (AM) and phase modulation (PM), with the harmonics from the sinusoidal model of speech as carriers and lowpass random noise as the message signal. Formal listening tests were conducted and listeners rated the synthesized effect as \"breathy\" more often than in natural non-breathy speech, but significantly less often than in naturally breathy speech.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-495"
  },
  "nicolao06_interspeech": {
   "authors": [
    [
     "Mauro",
     "Nicolao"
    ],
    [
     "Carlo",
     "Drioli"
    ],
    [
     "Piero",
     "Cosi"
    ]
   ],
   "title": "Voice GMM modelling for FESTIVAL/MBROLA emotive TTS synthesis",
   "original": "i06_1597",
   "page_count": 4,
   "order": 496,
   "p1": "paper 1597-Wed2BuP.2",
   "pn": "",
   "abstract": [
    "Voice quality is recognized to play an important role for the rendering of emotions in verbal communication. In this paper we explore the effectiveness of a processing framework for voice transformations finalized to the analysis and synthesis of emotive speech. We use a GMM-based model to compute the differences between an MBROLA voice and an anger voice, and we address the modification of the MBROLA voice spectra by using a set of spectral conversion functions trained on the data.\n",
    "We propose to organize the speech data for the training in such way that the target emotive speech data and the diphone database used for the text-to-speech synthesis, both come from the same speaker. A copy-synthesis procedure is used to produce synthesis speech utterances where pitch patterns, phoneme duration, and principal speaker characteristics are the same as in the target emotive utterances. This results in a better isolation of the voice quality differences due to the emotive arousal.\n",
    "Three different models to represent voice quality differences are applied and compared. The models are all based on a GMM representation of the acoustic space. The performance of these models is discussed and the experimental results and assessment are presented.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-496"
  },
  "cabral06_interspeech": {
   "authors": [
    [
     "João P.",
     "Cabral"
    ],
    [
     "Luís C.",
     "Oliveira"
    ]
   ],
   "title": "Emovoice: a system to generate emotions in speech",
   "original": "i06_1645",
   "page_count": 4,
   "order": 497,
   "p1": "paper 1645-Wed2BuP.3",
   "pn": "",
   "abstract": [
    "Generating emotions in speech is currently a hot topic of research given the requirement of modern human-machine interaction systems to produce expressive speech.\n",
    "We present the EmoVoice system, which implements acoustic rules to simulate seven basic emotions in neutral speech. It uses the pitchsynchronous time-scaling (PSTS) of the excitation signal to change the prosody and the most relevant glottal source parameters related to voice quality. The system also transforms other parameters of the vocal source signal to produce the irregular voicing quality. The correlation of the speech parameters with the basic emotions was derived from measurements of the glottal parameters and from results reported by other authors. The evaluation of the system showed that it can generate recognizable emotions but improvements are still necessary to discriminate some pairs of emotions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-497"
  },
  "wu06d_interspeech": {
   "authors": [
    [
     "Zhiyong",
     "Wu"
    ],
    [
     "Shen",
     "Zhang"
    ],
    [
     "Lianhong",
     "Cai"
    ],
    [
     "Helen M.",
     "Meng"
    ]
   ],
   "title": "Real-time synthesis of Chinese visual speech and facial expressions using MPEG-4 FAP features in a three-dimensional avatar",
   "original": "i06_1823",
   "page_count": 4,
   "order": 498,
   "p1": "paper 1823-Wed2BuP.4",
   "pn": "",
   "abstract": [
    "This paper describes our initial work in developing a real-time audio-visual Chinese speech synthesizer with a 3D expressive avatar. The avatar model is parameterized according to the MPEG-4 facial animation standard [1]. This standard offers a compact set of facial animation parameters (FAPs) and feature points (FPs) to enable realization of 20 Chinese visemes and 7 facial expressions (i.e. 27 target facial configurations). The Xface [2] open source toolkit enables us to define the influence zone for each FP and the deformation function that relates them. Hence we can easily animate a large number of coordinates in the 3D model by specifying values for a small set of FAPs and their FPs. FAP values for 27 target facial configurations were estimated from available corpora. We extended the dominance blending approach to effect animations for coarticulated visemes superposed with expression changes. We selected six sentiment-carrying text messages and synthesized expressive visual speech (for all expressions, in randomized order) with neutral audio speech. A perceptual experiment involving 11 subjects shows that they can identify the facial expression that matches the text messages sentiment 85% of the time.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-498"
  },
  "yang06b_interspeech": {
   "authors": [
    [
     "Hongwu",
     "Yang"
    ],
    [
     "Helen M.",
     "Meng"
    ],
    [
     "Lianhong",
     "Cai"
    ]
   ],
   "title": "Modeling the acoustic correlates of expressive elements in text genres for expressive text-to-speech synthesis",
   "original": "i06_1935",
   "page_count": 4,
   "order": 499,
   "p1": "paper 1935-Wed2BuP.5",
   "pn": "",
   "abstract": [
    "This paper proposes a novel approach for describing the expressive elements in text genres and modeling their acoustic correlates for expressive text-to-speech synthesis (TTS). We apply the threedimensional PAD (pleasure-displeasure, arousal-nonarousal and dominance-submissiveness) model in describing expressivity. In particular, we define a set of principles for annotating the P and A values of prosodic words found in texts from the tourist information domain. These text passages may be categorized into the descriptive genre (e.g. describing a beautiful scenic spot), the informative genre (e.g. presenting the opening hours of a museum) and the procedural genre (e.g. offering bus routes to a landmark). We choose the prosodic word as the basic unit for analysis since it bridges textual input with (synthetic) speech output. Analysis of contrastive (neutral versus expressive) recordings uncovers the acoustic correlates of annotated P and A values. This enables us to develop a non-linear model that can transform neutral speech to resemble expressive speech, according to the P and A values of the input text. Perceptual evaluation of the speech outputs shows that over 70% of the prosodic words carry appropriate expressivity.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-499"
  },
  "zhang06g_interspeech": {
   "authors": [
    [
     "Sheng",
     "Zhang"
    ],
    [
     "P. C.",
     "Ching"
    ],
    [
     "Fanrang",
     "Kong"
    ]
   ],
   "title": "Automatic emotion recognition of speech signal in Mandarin",
   "original": "i06_1128",
   "page_count": 4,
   "order": 500,
   "p1": "paper 1128-Wed2BuP.6",
   "pn": "",
   "abstract": [
    "Traditionally, a simultaneous recognition process using the same feature set of a spoken utterance is used to classify the emotional state of the speaker in addition to its content. However, an analysis on the classification performance for every pair of emotions shows that different features have distinctive classification abilities for different emotions. Therefore, we propose an efficient emotion recognition process called cascade bisection (CB-process), which carries out emotion recognition by means of several bisecting steps and applies different feature sets for every step. This process is based on the features different abilities of classifying emotions. Through this, we can fully utilize the information extracted from features and achieve a better recognition performance. Five discrete emotional states, namely, neutral, anger, fear, joy, and sadness are distinguished from the input Mandarin speech. After extracting the acoustic features that contain information on short-time energy (amplitude), signal amplitude, and pitch, we derive the representation feature set for further use in the CB-process, which achieves better emotion recognition as demonstrated seen from the experimental results.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-500"
  },
  "kao06_interspeech": {
   "authors": [
    [
     "Yi-hao",
     "Kao"
    ],
    [
     "Lin-shan",
     "Lee"
    ]
   ],
   "title": "Feature analysis for emotion recognition from Mandarin speech considering the special characteristics of Chinese language",
   "original": "i06_1504",
   "page_count": 4,
   "order": 501,
   "p1": "paper 1504-Wed2BuP.7",
   "pn": "",
   "abstract": [
    "Emotion recognition from speech signals is regarded as a critical step toward intelligent human-machine interface. However, feature parameters useful for this purpose may have to do with the special structures of the language. In this paper we present a detailed analysis of the feature parameters for emotion recognition considering the characteristics of the Chinese language, primarily the monosyllable structure and the tone behavior. The analysis is based on the feature parameters on three levels: frame-level, syllable-level, and word-level. The results show that the frame-level and syllable-level ones are good indicators, while taking the ensemble features on all three levels can yield a recognition accuracy of 90.0%. We also found that the pitch and power related features are the most important, and the fourth tone in Mandarin serves as the strongest indicator to emotions. All these findings are consistent with the characteristics of Mandarin Chinese.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-501"
  },
  "schuller06b_interspeech": {
   "authors": [
    [
     "Björn",
     "Schuller"
    ],
    [
     "Gerhard",
     "Rigoll"
    ]
   ],
   "title": "Timing levels in segment-based speech emotion recognition",
   "original": "i06_1695",
   "page_count": 4,
   "order": 502,
   "p1": "paper 1695-Wed2BuP.8",
   "pn": "",
   "abstract": [
    "Additional sub-phrase level information is believed to improve accuracy in speech emotion recognition systems. Yet, automatic segmentation is a challenge on its own considering word- or syllable boundaries. Further more clarification is needed which timing level leads to optimal results. In this paper we therefore quantitatively discuss three approaches to segment-level features based on 276 statistical hi-level prosodic, articulatory and speech quality features. Apart from the choice of the optimal segmentation scheme also fusion of segments with respect to classification and combination of diverse timing levels is analyzed. Tests are carried out on the popular Berlin Database of Emotional Speech (EMO-DB). Significant improvement over existing works can be reported for combination of phrase-level features with relative time interval features.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-502"
  },
  "nisimura06_interspeech": {
   "authors": [
    [
     "Ryuichi",
     "Nisimura"
    ],
    [
     "Souji",
     "Omae"
    ],
    [
     "Hideki",
     "Kawahara"
    ],
    [
     "Toshio",
     "Irino"
    ]
   ],
   "title": "Analyzing dialogue data for real-world emotional speech classification",
   "original": "i06_1675",
   "page_count": 4,
   "order": 503,
   "p1": "paper 1675-Wed2BuP.9",
   "pn": "",
   "abstract": [
    "In order to obtain an understanding of the users emotion in humanmachine dialogues, an analysis of dialogical utterances in the real world was performed. This work comprises three major steps. (1) The actual conditions of 16 basic emotions were evaluated using Japanese child voices, which were collected through the field test of the public spoken dialogue system. (2) Two factors were derived by a factor analysis. The factors were defined as fundamental psychological factors representing \"delightful\" and \"hateable\" emotions. (3) The relationships between the factors and the physical acoustic features were investigated to establish a capability to sense a users mental state for the dialogue system. In the experimental discriminations between the delightful and hateable emotions, a correct rate of 98.8% was achieved in classifying childs utterances by the SVM (Support Vector Machine) with 11 acoustic features.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-503"
  },
  "alm06_interspeech": {
   "authors": [
    [
     "Cecilia Ovesdotter",
     "Alm"
    ],
    [
     "Xavier",
     "Llorà"
    ]
   ],
   "title": "Evolving emotional prosody",
   "original": "i06_1741",
   "page_count": 4,
   "order": 504,
   "p1": "paper 1741-Wed2BuP.10",
   "pn": "",
   "abstract": [
    "Emotion is expressed by prosodic cues, and this study uses the active interactive Genetic Algorithm to search a wide space for sad and angry parameters of intensity, F0, and duration in perceptual resynthesis experiments with users. This method avoids large recorded databases and is flexible for exploring prosodic emotion parameters. Solutions from multiple runs are analyzed graphically and statistically. Average results indicate parameter evolution by emotion, and appear more distinct for sad. Solutions are quite successfully classified by CART, with duration as main predictor.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-504"
  },
  "luo06_interspeech": {
   "authors": [
    [
     "Xin",
     "Luo"
    ],
    [
     "Qian-Jie",
     "Fu"
    ],
    [
     "John J.",
     "Galvin III"
    ]
   ],
   "title": "Vocal emotion recognition with cochlear implants",
   "original": "i06_1315",
   "page_count": 4,
   "order": 505,
   "p1": "paper 1315-Wed2BuP.11",
   "pn": "",
   "abstract": [
    "Besides conveying linguistic information, spoken language can also transmit important cues regarding the emotion of a talker. These prosodic cues are most strongly coded by changes in amplitude, pitch, speech rate, voice quality and articulation. The present study investigated the ability of cochlear implant (CI) users to recognize vocal emotions, as well as the relative contributions of spectral and temporal cues to vocal emotion recognition. An English sentence database was recorded for the experiment; each test sentence was produced according to five target emotions. Vocal emotion recognition was tested in 6 CI and 6 normal-hearing (NH) subjects. With unprocessed speech, NH listeners mean vocal emotion recognition performance was 90% correct, while CI users mean performance was only 45% correct. Vocal emotion recognition was also measured in NH subjects while listening to acoustic, sine-wave vocoder CI simulations. To test the contribution of spectral cues to vocal emotion recognition, 1-, 2-, 4-, 8- and 16-channel CI processors were simulated; to test the contribution of temporal cues, the temporal envelope filter cutoff frequency in each channel was either 50 or 500 Hz. Results showed that both spectral and temporal cues significantly contributed to performance. With the 50-Hz envelope filter, performance generally improved as the number of spectral channels was increased. With the 500-Hz envelope filter, performance significantly improved only when the spectral resolution was increased from 1 to 2, and then from 2 to 16 channels. For all but the 16-channel simulations, increasing the envelope filter cutoff frequency from 50 Hz to 500 Hz significantly improved performance. CI users vocal emotion recognition performance was statistically similar to that of NH subjects listening to 1-8 spectral channels with the 50-Hz envelope filter, and to 1 channel with the 500-Hz envelope filter. The results suggest that, while spectral cues may contribute more strongly to recognition of linguistic information, temporal cues may contribute more strongly to recognition of emotional content coded in spoken language.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-505"
  },
  "matsunaga06_interspeech": {
   "authors": [
    [
     "S.",
     "Matsunaga"
    ],
    [
     "S.",
     "Sakaguchi"
    ],
    [
     "M.",
     "Yamashita"
    ],
    [
     "S.",
     "Miyahara"
    ],
    [
     "S.",
     "Nishitani"
    ],
    [
     "K.",
     "Shinohara"
    ]
   ],
   "title": "Emotion detection in infants² cries based on a maximum likelihood approach",
   "original": "i06_1345",
   "page_count": 4,
   "order": 506,
   "p1": "paper 1345-Wed2BuP.12",
   "pn": "",
   "abstract": [
    "This paper proposes a new procedure based on a maximum likelihood approach using hidden Markov models to detect infants emotions through their cries. Our procedure uses stochastic acoustic models for each kind of emotion. The acoustic models are generated using infants cries that are labeled segmentally according to their acoustic features. The procedure detects segment sequences with the highest likelihood among all kinds of emotions. The results of our preliminary recognition experiments on two emotions using three types of segment labeling show that the proposed procedure is applicable to emotion detection in an infants cry and that the detailed transcription of acoustic segments is useful. In this paper, using detailed transcriptions, we broaden the experiments to include five emotions. Assuming the judgment of each infants mother to be correct, we compared the result of the experiment and that of a subjective opinion test. We conducted the opinion test with the help of three child-rearing experts. Emotion recognition using the proposed procedure displays a favorable comparison with the judgment of our experts, showing the validity of the proposed procedure.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-506"
  },
  "tepperman06b_interspeech": {
   "authors": [
    [
     "Joseph",
     "Tepperman"
    ],
    [
     "David",
     "Traum"
    ],
    [
     "Shrikanth",
     "Narayanan"
    ]
   ],
   "title": "yeah right: sarcasm recognition for spoken dialogue systems",
   "original": "i06_1821",
   "page_count": 4,
   "order": 507,
   "p1": "paper 1821-Wed2BuP.13",
   "pn": "",
   "abstract": [
    "The robust understanding of sarcasm in a spoken dialogue system requires a reformulation of the dialogue managers basic assumptions behind, for example, user behavior and grounding strategies. But automatically detecting a sarcastic tone of voice is not a simple matter. This paper presents some experiments toward sarcasm recognition using prosodic, spectral, and contextual cues. Our results demonstrate that spectral and contextual features can be used to detect sarcasm as well as a human annotator would, and confirm a long-held claim in the field of psychology - that prosody alone is not sufficient to discern whether a speaker is being sarcastic.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-507"
  },
  "kumar06_interspeech": {
   "authors": [
    [
     "Rohit",
     "Kumar"
    ],
    [
     "Carolyn P.",
     "Rosé"
    ],
    [
     "Diane J.",
     "Litman"
    ]
   ],
   "title": "Identification of confusion and surprise in spoken dialog using prosodic features",
   "original": "i06_1921",
   "page_count": 4,
   "order": 508,
   "p1": "paper 1921-Wed2BuP.14",
   "pn": "",
   "abstract": [
    "Sensitivity to a users emotional state offers promise in improving the state of the art in spoken dialog systems. In this work, we attempt to detect the speakers states of confusion and surprise using prosodic features from his/her utterances. We have collected a corpus of utterances in realistic settings using an experimental methodology aimed at eliciting confusion and surprise from users. Classification experiments have yielded up to a 27.2% improvement over baseline performance using F0 and power features. We achieved the greatest success at classification of emotions that were most successfully elicited.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-508"
  },
  "nwe06_interspeech": {
   "authors": [
    [
     "Tin Lay",
     "Nwe"
    ],
    [
     "Haizhou",
     "Li"
    ],
    [
     "Minghui",
     "Dong"
    ]
   ],
   "title": "Analysis and detection of speech under sleep deprivation",
   "original": "i06_1934",
   "page_count": 4,
   "order": 509,
   "p1": "paper 1934-Wed2BuP.15",
   "pn": "",
   "abstract": [
    "Stress has effect on speech characteristics and can influence the quality of speech. In this paper, we study the effect of Sleep-Deprivation (SD) on speech characteristics and classify Normal Speech (NS) and Sleep Deprived Speech (SDS). One of the indicators of sleep deprivation is flattened voice. We examine pitch and harmonic locations to analyse flatness of voice. To investigate, we compute the spectral coefficients that can capture the variations of pitch and harmonic patterns. These are derived using Two-Layer Cascaded-Subband Filter spread according to the pitch and harmonic frequency scale. Hidden Markov Model (HMM) is employed for statistical modeling. We use DCIEM map task corpus to conduct experiments. The analysis results show that SDS has less variation of pitch and harmonic pattern than NS. In addition, we achieve the relatively high accuracy for classification of Normal Speech (NS) and Sleep Deprived Speech (SDS) using proposed spectral coefficients.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-509"
  },
  "vasilescu06_interspeech": {
   "authors": [
    [
     "Ioana",
     "Vasilescu"
    ],
    [
     "Martine",
     "Adda-Decker"
    ]
   ],
   "title": "Language, gender, speaking style and language proficiency as factors influencing the autonomous vocalic filler production in spontaneous speech",
   "original": "i06_1994",
   "page_count": 4,
   "order": 510,
   "p1": "paper 1994-Wed2BuP.16",
   "pn": "",
   "abstract": [
    "This paper deals with the factors characterizing the production of autonomous vocalic filled pauses in large spontaneous speech corpora, namely language, gender, speaking style and language proficiency. Two types of corpora are analyzed: a corpus of broadcast news in French and American English and a corpus of short talks in a conference in English spoken by native and non-native speakers. Several acoustic and prosodic parameters are evaluated and correlated with each factor, namely timbre, pitch, duration and density. Results presented here show that the timbre is correlated with language and language proficiency, whereas the duration is linked both to gender and speaking style, the latter conditioning also the hesitation density in speech.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-510"
  },
  "lavecchia06_interspeech": {
   "authors": [
    [
     "Caroline",
     "Lavecchia"
    ],
    [
     "Kamel",
     "Smaïli"
    ],
    [
     "Jean-Paul",
     "Haton"
    ]
   ],
   "title": "How to handle gender and number agreement in statistical language models?",
   "original": "i06_1362",
   "page_count": 4,
   "order": 511,
   "p1": "paper 1362-Wed2CaP.1",
   "pn": "",
   "abstract": [
    "The agreement in gender and number is a critical problem in statistical language modeling. One of the main difficulties in speech recognition of French language is the presence of misrecognized words due to the bad agreement (in gender and number) between words. Statistical language models do not treat this phenomena directly. This paper focuses on how to handle the issue of this agreement. We introduce an original model called Features-Cache (FC) to estimate the gender and the number of the word to predict. It is a dynamic variable-length Features-Cache. The size of the cache is automatically determined in accordance to syntagm delimitors. The main advantage of this model is that there is no need to any syntactic parsing : it is used as any other statistical language model. Several models have been carried out and the best one achieves an improvement of approximately 9 points in terms of perplexity. This model has been integrated in a speech recognition system based on JULIUS engine. Tests have been carried out on 280 sentences provided by AUPELF for the French automatic speech recognition evaluation campaign. This new model outperforms the baseline one, in terms of word error, by 3%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-511"
  },
  "chan06b_interspeech": {
   "authors": [
    [
     "Oscar",
     "Chan"
    ],
    [
     "Roberto",
     "Togneri"
    ]
   ],
   "title": "Prosodic features for a maximum entropy language model",
   "original": "i06_1150",
   "page_count": 4,
   "order": 512,
   "p1": "paper 1150-Wed2CaP.2",
   "pn": "",
   "abstract": [
    "This paper presents an approach for incorporating prosodic knowledge into the language modelling component of a speech recogniser. We formulate features for a maximum entropy language model which capture various aspects of the relationships between prosody, syntax and the spoken word sequence. Maximum entropy is a powerful modelling technique, and well suited to modelling prosodic information. Tests conducted on the Boston University Radio Speech Corpus using this model showed improvements in perplexity, and n-best rescoring results also demonstrated small but statistically significant gains.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-512"
  },
  "mori06_interspeech": {
   "authors": [
    [
     "Shinsuke",
     "Mori"
    ]
   ],
   "title": "Language model adaptation with a word list and a raw corpus",
   "original": "i06_1146",
   "page_count": 4,
   "order": 513,
   "p1": "paper 1146-Wed2CaP.3",
   "pn": "",
   "abstract": [
    "In this paper, we discuss language model adaptation methods given a word list and a raw corpus. In this situation, the general method is to segment the raw corpus automatically using a word list, correct the output sentences by hand, and build a model from the segmented corpus. In this sentence-by-sentence error correction method, however, the annotator encounters grammatically complicated positions and this results in a decrease of productivity. In this paper, we propose to concentrate on correcting the positions in which the words in the list appear by taking a word as a correction unit. This method allows us to avoid these problems and go directly to capturing the statistical behavior of specific words in the application. In the experiments, we used a variety of methods for preparing a segmented corpus and compared the language models by their speech recognition accuracies. The results showed the advantages of our method.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-513"
  },
  "wiggers06_interspeech": {
   "authors": [
    [
     "Pascal",
     "Wiggers"
    ],
    [
     "Léon J.M.",
     "Rothkrantz"
    ]
   ],
   "title": "Topic-based language modeling with dynamic Bayesian networks",
   "original": "i06_1882",
   "page_count": 4,
   "order": 514,
   "p1": "paper 1882-Wed2CaP.4",
   "pn": "",
   "abstract": [
    "Although n-gram models are still the de facto standard in language modeling for speech recognition, more sophisticated models achieve better accuracy by taking additional information, such as syntactic rules, semantic relations or domain knowledge into account. Unfortunately, most of the effort in developing such models goes into the implementation of handcrafted inference routines. A generic mechanism to introduce background knowledge into a language model is lacking. We propose using dynamic Bayesian networks. Dynamic Bayesian networks are a generalization of the n-gram models and HMMs traditionally used in language modeling and speech recognition. Whereas those models use a single random variable to represent state, Bayesian networks can have any number of variables. As such they are particularly well-suited for the construction of models that take additional information into account. This paper discusses language modeling with Bayesian networks. Examples of Bayesian network implementations of well-known language models are given and a novel topic-based language model is presented.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-514"
  },
  "yamamoto06_interspeech": {
   "authors": [
    [
     "Hirofumi",
     "Yamamoto"
    ],
    [
     "Genichiro",
     "Kikui"
    ],
    [
     "Satoshi",
     "Nakamura"
    ],
    [
     "Yoshinori",
     "Sagisaka"
    ]
   ],
   "title": "Speech recognition of foreign out-of-vocabulary words using a hierarchical language model",
   "original": "i06_1692",
   "page_count": 4,
   "order": 515,
   "p1": "paper 1692-Wed2CaP.5",
   "pn": "",
   "abstract": [
    "This paper proposes a new speech recognition scheme for foreign out-of-vocabulary words embedded in native-language speech. To recognize foreign names frequently observed in news speech or in translation speech, we adopted a hierarchical language model that had been successfully applied to OOV words covering native vocabularies. In this hierarchical language model, OOV vocabularies are modeled as a word-class model in the upper-layered model, and their statistical phonotactic constraints are modeled in the lower-layered model. Since extra statistics are needed to cover foreign words and their pronunciation differences, we have introduced two techniques. The first is to combine translation target language models and translation source statistics of OOVs using the hierarchical language model. The second is to automatically generate recognition target pronunciations from original pronunciations by syllable-to-syllable mapping. To confirm the validity of this recognition scheme, we have conducted speech recognition experiments using English speech including Japanese personal names as OOV words. The proposed method outperformed the existing algorithm using a lexicon consisting of all the words in the training set. Surprisingly, it achieved better OOV recognition results than the non-OOV condition where all the proper names in the test set are registered in the lexicon.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-515"
  },
  "hu06c_interspeech": {
   "authors": [
    [
     "Xinhui",
     "Hu"
    ],
    [
     "Hirofumi",
     "Yamamoto"
    ],
    [
     "Genichiro",
     "Kikui"
    ],
    [
     "Yoshinori",
     "Sagisaka"
    ]
   ],
   "title": "Language modeling of Chinese personal names based on character units for continuous Chinese speech recognition",
   "original": "i06_1979",
   "page_count": 4,
   "order": 516,
   "p1": "paper 1979-Wed2CaP.6",
   "pn": "",
   "abstract": [
    "In this paper, we analyze Chinese personal names to model their statistical phonotactic characteristics for continuous Chinese speech recognition. The analysis showed language-specific characteristics of Chinese personal names and strongly suggested the advantage of character-unit oriented modeling. A hierarchical language model was composed by reflecting statistical phonotactic characteristics of Chinese personal names as a lower intra-word model, and ordinary inter-word neighboring characteristics as an upper multi-class composite N-gram model. These two layers of models were trained independently using different language corpora. For the modeling of given names, the syllable without tone information was selected as the unit for training the bi-gram. The properties of either one or two characters of a given name were introduced to simplify the length constraint of the modeling process. For Chinese family names, we simply added them directly in the recognition lexicon, since their numbers are very restricted. The results from Chinese speech recognition experiments revealed that the proposed hierarchical language model greatly improved the identification accuracy of the Chinese given names compared with the conventional word-class N-gram model.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-516"
  },
  "lakshmi06_interspeech": {
   "authors": [
    [
     "A.",
     "Lakshmi"
    ],
    [
     "Hema A.",
     "Murthy"
    ]
   ],
   "title": "A syllable based continuous speech recognizer for Tamil",
   "original": "i06_1055",
   "page_count": 4,
   "order": 517,
   "p1": "paper 1055-Wed2CaP.7",
   "pn": "",
   "abstract": [
    "This paper presents a novel technique for building a syllable based continuous speech recognizer when unannotated transcribed train data is available. We present two different segmentation algorithms to segment the speech and the corresponding text into comparable syllable like units. A group delay based two level segmentation algorithm is proposed to extract accurate syllable units from the speech data. A rule based text segmentation algorithm is used to automatically annotate the text corresponding to the speech into syllable units. Isolated style syllable models are built using multiple frame size (MFS) and multiple frame rate (MFR) for all unique syllables by collecting examples from annotated speech. Experiments performed on Tamil language show that the recognition performance is comparable to recognizers built using manually segmented train data. These experiments suggest that system development cost can be reduced by using minimum manual effort if sentence level transcription of the speech data is available.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-517"
  },
  "woszczyna06_interspeech": {
   "authors": [
    [
     "Monika",
     "Woszczyna"
    ],
    [
     "Paisarn",
     "Charoenpornsawat"
    ],
    [
     "Tanja",
     "Schultz"
    ]
   ],
   "title": "Spontaneous Thai speech recognition",
   "original": "i06_1419",
   "page_count": 4,
   "order": 518,
   "p1": "paper 1419-Wed2CaP.8",
   "pn": "",
   "abstract": [
    "This paper expands previous work on Thai speech recognition, investigating pronunciation changes such as syllable and phoneme elisions as well as phoneme shifts in Thai spontaneous speech. We compare several approaches to model these effects in large vocabulary continuous speech recognition across multiple domains. This work includes experiments on two new speech databases that significantly alleviate the data sparseness problem of earlier publications. We found that given sufficient training data, a fully data driven approach using an allophone cluster tree yields the best results. Explicit modeling of pronunciation changes does not improve performance across domains.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-518"
  },
  "gerosa06_interspeech": {
   "authors": [
    [
     "M.",
     "Gerosa"
    ],
    [
     "D.",
     "Giuliani"
    ],
    [
     "Shrikanth",
     "Narayanan"
    ]
   ],
   "title": "Acoustic analysis and automatic recognition of spontaneous children's speech",
   "original": "i06_1082",
   "page_count": 4,
   "order": 519,
   "p1": "paper 1082-Wed2CaP.9",
   "pn": "",
   "abstract": [
    "This paper presents analyses, and recognition experiments, on spontaneous American English speech collected from children aged from 8 to 13 years. These analyses focused on variations in phone duration and on the scattering of phones in the acoustic space and were aimed at achieving a better understanding of spectral and temporal changes occurring in spontaneous speech produced by children of various ages with a view toward developing robust automatic speech recognition applications. The speech data were partitioned in two subsets depending on the annotated presence/absence of explicit occurrences of spontaneous speech phenomena such as fillers, false starts and other disfluencies. All the analyses carried out, as well as the results of recognition experiments, show a significant difference between these two partitions. In particular, recognition performance for the subset containing annotated spontaneous speech phenomena was significantly worse (by almost 15%) than the one achieved for the other subset. Relative improvements due to acoustic model adaptation and normalization on both data partitions were comparable, underscoring that significant performance degradation happens due to spontaneous speech variability beyond those reflected in segmental spectral characteristics.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-519"
  },
  "vertanen06_interspeech": {
   "authors": [
    [
     "Keith",
     "Vertanen"
    ]
   ],
   "title": "Speech and speech recognition during dictation corrections",
   "original": "i06_1094",
   "page_count": 4,
   "order": 520,
   "p1": "paper 1094-Wed2CaP.10",
   "pn": "",
   "abstract": [
    "A natural way to correct errors made while dictating to a computer is to respeak portions of the original sentence. But often spoken corrections are themselves misrecognized, costing the user time and testing their patience. To better understand how users behave while correcting, I created a simulated dictation interface and fooled users into believing they were correcting errors by respeaking. I found that users not only hyperarticulate during corrections, but they do so preemptively before any misrecognition. Depending on the recognizer, hyperarticulation was found to cause relatively minor changes in error rate. The correction of isolated words or phrases was more troublesome, causing substantial recognition problems for an HTK recognizer. Dragon Naturally Speaking, on the other hand, performed slightly better on hyperarticulated speech and only degraded slightly on isolated corrections.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-520"
  },
  "smidl06_interspeech": {
   "authors": [
    [
     "Lubos",
     "Smídl"
    ],
    [
     "Josef V.",
     "Psutka"
    ]
   ],
   "title": "Comparison of keyword spotting methods for searching in speech",
   "original": "i06_1587",
   "page_count": 4,
   "order": 521,
   "p1": "paper 1587-Wed2CaP.11",
   "pn": "",
   "abstract": [
    "This paper presents and discusses keyword spotting methods for searching in speech. In contrast with searching in text, the searching in speech or generally in multimedia data still represents a challenge. The aim of the paper is to present a keyword spotting (KWS) method based on a large vocabulary continuous speech recognition (LVCSR) system, based on phonetics decoder, and keyword spotting using a filler model. All the methods are evaluated and compared from various points of view - speed, quality, requirements on training data and so on. All experiments are done using a telephone-quality speech corpus. Furthermore, this paper presents a new block decision in filler model-based keyword spotting which brings the speedup of decision together with better detection.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-521"
  },
  "balakrishna06_interspeech": {
   "authors": [
    [
     "Mithun",
     "Balakrishna"
    ],
    [
     "Cyril",
     "Cerovic"
    ],
    [
     "Dan",
     "Moldovan"
    ],
    [
     "Ellis",
     "Cave"
    ]
   ],
   "title": "Automatic generation of statistical language models for interactive voice response applications",
   "original": "i06_1648",
   "page_count": 4,
   "order": 522,
   "p1": "paper 1648-Wed2CaP.12",
   "pn": "",
   "abstract": [
    "This paper proposes a methodology to automatically generate statistical language models (SLM)s for the recognition of utterances in Interactive Voice Response (IVR) systems. The paper aims at creating SLMs for each IVR prompt [1] with minimum amount of human intervention and prior knowledge regarding the expected user utterances at a particular prompt. A combination of prefiller patterns based on spontaneous speech utterances, WordNet [2] and Rogets thesaurus based content word extraction and, world wide web based statistical validation is used to generate SLMs automatically. The created SLM not only reduces the manual labor involved in IVR application development but also focuses on minimizing the Word Error Rate (WER) and the Semantic Error Rate (SemER) of the ASR transcriptions. We use a WordNet [2] lexical chain based semantic categorizer to classify ASR transcriptions into semantic categories representing each IVR prompt.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-522"
  },
  "ju06_interspeech": {
   "authors": [
    [
     "Yun-Cheng",
     "Ju"
    ],
    [
     "Ye-Yi",
     "Wang"
    ],
    [
     "Alex",
     "Acero"
    ]
   ],
   "title": "Call analysis with classification using speech and non-speech features",
   "original": "i06_2011",
   "page_count": 4,
   "order": 523,
   "p1": "paper 2011-Wed2CaP.13",
   "pn": "",
   "abstract": [
    "This paper reports our recent development of a highly reliable call analysis technique that makes novel use of automatic speech recognition (ASR), speech utterance classification and non-speech features. The main ideas include the use the N-Gram filler model to improve the ASR accuracy on important words in a message, and the integration of recognized utterance with non-speech features such as utterance length, and the use of utterance classification technique to interpret the message and extract additional information. Experimental evaluation shows that the use of the utterance length, recognized text, and the classifiers confidence measure reduces the classification error rate to 2.5% of the test sets.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-523"
  },
  "wu06e_interspeech": {
   "authors": [
    [
     "Wei-Lin",
     "Wu"
    ],
    [
     "Ru-Zhan",
     "Lu"
    ],
    [
     "Hui",
     "Liu"
    ],
    [
     "Feng",
     "Gao"
    ]
   ],
   "title": "A spoken language understanding approach using successive learners",
   "original": "i06_1987",
   "page_count": 4,
   "order": 524,
   "p1": "paper 1987-Wed2FoP.1",
   "pn": "",
   "abstract": [
    "In this paper, we describe a novel spoken language understanding approach using two successive learners. The first learner is used to identify the topic of an input utterance. With the restriction of the recognized target topic, the second learner is trained to extract the corresponding slot-value pairs. The advantage of the proposed approach is that it is mainly data-driven and requires only minimally annotated corpus for training whilst retaining the understanding robustness and deepness for spoken language. Experiments have been conducted in the context of Chinese public transportation information inquiry domain. The good performance demonstrates the viability of the proposed approach.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-524"
  },
  "stewart06_interspeech": {
   "authors": [
    [
     "Osamuyimen",
     "Stewart"
    ],
    [
     "Juan",
     "Huerta"
    ],
    [
     "Ea-Ee",
     "Jan"
    ],
    [
     "Cheng",
     "Wu"
    ],
    [
     "Xiang",
     "Li"
    ],
    [
     "David",
     "Lubensky"
    ]
   ],
   "title": "Conversational help desk: vague callers and context switch",
   "original": "i06_1291",
   "page_count": 4,
   "order": 525,
   "p1": "paper 1291-Wed2FoP.2",
   "pn": "",
   "abstract": [
    "Two salient properties of user behavior make Help Desk a unique speech application different from the more general transactional kind: (a) majority of users have only vague ideas about their problem, and (b) these users are likely to context-switch (change discourse topic) during the course of a dialog. We describe a conversational Help Desk natural language call routing application and show how the alignment of Voice User Interface (VUI), Grammar Development, and Application Architecture results in a conversational user interface that is able to guide the vague user in the most optimal way while being flexible to allow mid-discourse context switches. Usability evaluation confirms the peculiar user behavior and provides empirical evidence that users perception of time in a speech application can be influenced by the dialog; in this case, Help Desk users tend not to become impatient going through three-five dialog turns, as long as dialog is progressing toward problem-resolution.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-525"
  },
  "rosset06_interspeech": {
   "authors": [
    [
     "Sophie",
     "Rosset"
    ],
    [
     "Olivier",
     "Galibert"
    ],
    [
     "Gabriel",
     "Illouz"
    ],
    [
     "Aurélien",
     "Max"
    ]
   ],
   "title": "Integrating spoken dialog and question answering: the ritel project",
   "original": "i06_1529",
   "page_count": 4,
   "order": 526,
   "p1": "paper 1529-Wed2FoP.3",
   "pn": "",
   "abstract": [
    "The Ritel project aims at integrating spoken language dialog and open-domain information retrieval to allow a human to ask general questions (e.g. Who is currently presiding the French Senate?) and refine her search interactively. This project is at the junction of several distinct research communities, and has therefore several challenges to tackle: real-time streamed speech recognition with very large vocabulary, open-domain dialog management, fast information retrieval from text, query cobuilding, communication between information retrieval and dialog components, and generation of natural sounding answers. In this paper, we present our work on the different components of Ritel, and provide initial results.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-526"
  },
  "prommer06_interspeech": {
   "authors": [
    [
     "Thomas",
     "Prommer"
    ],
    [
     "Hartwig",
     "Holzapfel"
    ],
    [
     "Alex",
     "Waibel"
    ]
   ],
   "title": "Rapid simulation-driven reinforcement learning of multimodal dialog strategies in human-robot interaction",
   "original": "i06_1551",
   "page_count": 4,
   "order": 527,
   "p1": "paper 1551-Wed2FoP.4",
   "pn": "",
   "abstract": [
    "In this work we propose a procedure model for rapid automatic strategy learning in multimodal dialogs. Our approach is tailored for typical task-oriented human-robot dialog interactions, with no prior knowledge about the expected user and system dynamics being present. For such scenarios, we propose the use of stochastic dialog simulation for strategy learning, where the user and system error models are solely trained through the initial execution of an inexpensive Wizard-of-Oz experiment. We argue that for the addressed dialogs, already a small data corpus combined with a low-conditioned simulation model facilitates learning of strong and complex dialog strategies. To validate our overall approach, we empirically show the supremacy of the learned strategy over a hand-crafted strategy for a concrete human-robot dialog scenario. To the authors knowledge, this work is the first to perform strategy learning from multimodal dialog simulation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-527"
  },
  "aist06_interspeech": {
   "authors": [
    [
     "Gregory",
     "Aist"
    ],
    [
     "James",
     "Allen"
    ],
    [
     "Ellen",
     "Campana"
    ],
    [
     "Lucian",
     "Galescu"
    ],
    [
     "Carlos A. Gómez",
     "Gallo"
    ],
    [
     "Scott C.",
     "Stoness"
    ],
    [
     "Mary",
     "Swift"
    ],
    [
     "Michael",
     "Tanenhaus"
    ]
   ],
   "title": "Software architectures for incremental understanding of human speech",
   "original": "i06_1869",
   "page_count": 4,
   "order": 528,
   "p1": "paper 1869-Wed2FoP.5",
   "pn": "",
   "abstract": [
    "The prevalent state of the art in spoken language understanding by spoken dialog systems is both modular and whole-utterance. It is modular in that incoming utterances are processed by independent components that handle different aspects, such as acoustics, syntax, semantics, and intention / goal recognition. It is whole-utterance in that each component completes its work for an entire utterance prior to handing off the utterance to the next component. However, a growing body of evidence suggests that humans do not process language that way. Rather, people process speech by rapidly integrating constraints from multiple sources of knowledge and multiple linguistic levels incrementally, as the utterance unfolds. In this paper we describe ongoing work aimed at developing an architecture that will allow machines to understand spoken language in a similar way. This revolutionary approach is promising for two reasons: 1) it more accurately reflects contemporary models of human language understanding, and 2) it results in empirical improvements including increased parsing performance.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-528"
  },
  "schiel06_interspeech": {
   "authors": [
    [
     "Florian",
     "Schiel"
    ],
    [
     "Christoph",
     "Draxler"
    ],
    [
     "Marion",
     "Libossek"
    ]
   ],
   "title": "Lingua machinae - an unorthodox proposal",
   "original": "i06_2026",
   "page_count": 4,
   "order": 529,
   "p1": "paper 2026-Wed2FoP.6",
   "pn": "",
   "abstract": [
    "Voice User Interfaces (VUI) are slowly emerging into todays IT applications. A decade ago, members of the speech science community (as well as market analysts) predicted a much faster growth of commercial VUI usage and are now wondering why the acceptance was so moderate during the last five years. One reason for this is certainly the difficult economic situation since the crash of the new technology market. Another reason is the fact that people are slow, especially in the light of an uncertain economy, to leave well-trodden paths for new shores. In this paper we raise the unorthodox question whether VUIs might work more efficiently, be better accepted by users and have a greater commercial impact on future IT markets if there existed a widely agreed communication paradigm on how to use VUIs. For the moment wed like to call this paradigm Lingua Machinae.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-529"
  },
  "ponbarry06_interspeech": {
   "authors": [
    [
     "Heather",
     "Pon-Barry"
    ],
    [
     "Fuliang",
     "Weng"
    ],
    [
     "Sebastian",
     "Varges"
    ]
   ],
   "title": "Evaluation of content presentation strategies for an in-car spoken dialogue system",
   "original": "i06_2044",
   "page_count": 4,
   "order": 530,
   "p1": "paper 2044-Wed2FoP.7",
   "pn": "",
   "abstract": [
    "In this paper we present a framework for managing information presentation in spoken dialogue systems. We describe a content optimization module that makes use of ontological relationships in information-seeking dialogues in order to organize knowledge base items and perform adjustments such as relaxing or tightening user constraints. We present the results of an experimental evaluation comparing two response strategies: (a) one that uses the content optimization module to offer suggestions and (b) one that gives no suggestions. The results indicate that giving such suggestions is preferred when a user query matches either no items or many items in the knowledge base, and may also lead to more efficient dialogues.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-530"
  },
  "goel06_interspeech": {
   "authors": [
    [
     "Vaibhava",
     "Goel"
    ],
    [
     "Ramesh",
     "Gopinath"
    ]
   ],
   "title": "On designing context sensitive language models for spoken dialog systems",
   "original": "i06_2052",
   "page_count": 4,
   "order": 531,
   "p1": "paper 2052-Wed2FoP.8",
   "pn": "",
   "abstract": [
    "In this paper we describe our approach to building dialog context sensitive language models for improving the recognition performance in spoken dialog systems. These methods were developed and successfully tested in the context of a large-scale commercially deployed system that takes in over ten million calls each month. Dialog sensitive language models are typically built by clustering dialog histories into groups that elicit similar responses. A key innovation in this paper is to use an EM clustering procedure in lieu of a k-means clustering procedure that is typically used. The EM procedure results in clusters with higher log-probability which we argue leads to better recognition performance. Additionally, we empirically observe that the EM approach has much better worst case behavior than the k-means approach as it pertains to local optima.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-531"
  },
  "liu06e_interspeech": {
   "authors": [
    [
     "Yang",
     "Liu"
    ]
   ],
   "title": "Using SVM and error-correcting codes for multiclass dialog act classification in meeting corpus",
   "original": "i06_1306",
   "page_count": 4,
   "order": 532,
   "p1": "paper 1306-Wed2FoP.9",
   "pn": "",
   "abstract": [
    "Accurate classification of dialog acts (DAs) is important for many spoken language applications. Different methods have been proposed such as hidden Markov models (HMM), maximum entropy (Maxent), graphical models, and support vector machines (SVMs). In this paper, we investigate using SVMs for multiclass DA classification in the ICSI meeting corpus. We evaluate (1) representing DA tagging directly as a multiclass task, and (2) combining multiple binary classifiers via error correction output codes (ECOC). For the ECOC combination, different code matrices are utilized (e.g., the identity matrix, exhaustive code, BCH code, and random code matrix). We also compare using SVMs with our previous Maxent model. We find that for DA tagging, using multiple binary SVMs via ECOC outperforms a direct multiclass SVM, but neither achieves better performance than the Maxent model, possibly because of the small class set and the features currently used in the task.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-532"
  },
  "holzapfel06_interspeech": {
   "authors": [
    [
     "Hartwig",
     "Holzapfel"
    ],
    [
     "Alex",
     "Waibel"
    ]
   ],
   "title": "A multilingual expectations model for contextual utterances in mixed-initiative spoken dialogue",
   "original": "i06_1614",
   "page_count": 4,
   "order": 533,
   "p1": "paper 1614-Wed2FoP.10",
   "pn": "",
   "abstract": [
    "This paper describes a model of generating expectations that are used to improve speech recognition and to resolve elliptical expressions in dialogue context. The algorithm is domain and language independent and part of the dialogue manager. We use the expectation model to weight a speech recognizers grammar rules in dialogue context which improves recognition rates significantly as shown in the evaluation. We explain what types of expectations the system can generate and give a classification of system actions based on speech act theory, explain the resolution of elliptical expressions and their interpretation in context, and evaluate the presented algorithm in a multilingual system with English and German speech recognition.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-533"
  },
  "fukubayashi06_interspeech": {
   "authors": [
    [
     "Yuichiro",
     "Fukubayashi"
    ],
    [
     "Kazunori",
     "Komatani"
    ],
    [
     "Tetsuya",
     "Ogata"
    ],
    [
     "Hiroshi G.",
     "Okuno"
    ]
   ],
   "title": "Dynamic help generation by estimating user²s mental model in spoken dialogue systems",
   "original": "i06_1750",
   "page_count": 4,
   "order": 534,
   "p1": "paper 1750-Wed2FoP.11",
   "pn": "",
   "abstract": [
    "In a speech interface, a gap between a users mental model and actual structures of systems tends to be large because the amount of information conveyed by speech is limited. We address dynamic help generation adapted to users, to decrease the gap between them. We defined a domain concept tree as an expression of a systems actual structure. We estimated and maintained users knowledge about the system on the tree. Every node in the tree has values representing the degree to which a user understands the concepts corresponding to the nodes. The values are updated based on the content of users utterances and help messages the system gives. Help messages provided for users are determined by referring to the domain concept tree and identifying concepts the user does not understand. We evaluated our method by testing twelve novice subjects. Both the average time to complete tasks and the number of utterances significantly decreased because of the help messages provided by our method.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-534"
  },
  "surendran06_interspeech": {
   "authors": [
    [
     "Dinoj",
     "Surendran"
    ],
    [
     "Gina-Anne",
     "Levow"
    ]
   ],
   "title": "Dialog act tagging with support vector machines and hidden Markov models",
   "original": "i06_1831",
   "page_count": 4,
   "order": 535,
   "p1": "paper 1831-Wed2FoP.12",
   "pn": "",
   "abstract": [
    "We use a combination of linear support vector machines and hidden markov models for dialog act tagging in the HCRC MapTask corpus, and obtain better results than those previously reported. Support vector machines allow easy integration of sparse high-dimensional text features and dense low-dimensional acoustic features, and produce posterior probabilities usable by sequence labelling algorithms. The relative contribution of text and acoustic features for each class of dialog act is analyzed.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-535"
  },
  "torre06b_interspeech": {
   "authors": [
    [
     "Ángel de la",
     "Torre"
    ],
    [
     "Javier",
     "Ramírez"
    ],
    [
     "Carmen",
     "Benítez"
    ],
    [
     "José C.",
     "Segura"
    ],
    [
     "L.",
     "García"
    ],
    [
     "Antonio J.",
     "Rubio"
    ]
   ],
   "title": "Noise robust model-based voice activity detection",
   "original": "i06_1476",
   "page_count": 4,
   "order": 536,
   "p1": "paper 1476-Wed3A1O.1",
   "pn": "",
   "abstract": [
    "We propose a model-based VAD derived from the Vector Taylor Series (VTS) approach. A Gaussian mixture (trained with clean speech) is used in order to provide an appropriate decision rule for speech/nonspeech detection. Additionally, VTS approach adapts the Gaussian mixture to noise conditions, yielding a stable performance for a wide range of SNRs. We have evaluated its ability for speech/non-speech detection and also its application for robust speech recognition. When compared to other VAD methods, the proposed VAD shows the best trade-off in speech/non-speech detection. When applied for Wiener Filtering and for frame dropping, the proposed VAD also provides the best recognition results.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-536"
  },
  "shi06b_interspeech": {
   "authors": [
    [
     "Yu",
     "Shi"
    ],
    [
     "Frank K.",
     "Soong"
    ],
    [
     "Jian-Lai",
     "Zhou"
    ]
   ],
   "title": "Auto-segmentation based VAD for robust ASR",
   "original": "i06_1749",
   "page_count": 4,
   "order": 537,
   "p1": "paper 1749-Wed3A1O.2",
   "pn": "",
   "abstract": [
    "An auto-segmentation based endpointing algorithm for robust ASR is proposed. The algorithm consists of two successive steps: (1) homogeneous segment partitioning and (2) segment clustering. The first step, due to its self-segmentation nature, does not need a noise model, and is applicable to different noises at various SNRs. The dynamic programming based segment partitioning, which can generate more homogeneous segments than individual frames for clustering, yields a more robust VAD mechanism. Experiments are performed on the AURORA2 digit database by comparing the new algorithm with the ETSI standard for DSR. Quantitative assessment of the new algorithm is performed via different evaluation criteria, including: ROC curves, speech/non-speech discrimination, and speech recognition performance.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-537"
  },
  "boakye06_interspeech": {
   "authors": [
    [
     "Kofi",
     "Boakye"
    ],
    [
     "Andreas",
     "Stolcke"
    ]
   ],
   "title": "Improved speech activity detection using cross-channel features for recognition of multiparty meetings",
   "original": "i06_1824",
   "page_count": 4,
   "order": 538,
   "p1": "paper 1824-Wed3A1O.3",
   "pn": "",
   "abstract": [
    "We describe the development of a speech activity detection system using an HMM-based segmenter for automatic speech recognition on individual headset microphones in multispeaker meetings. We look at cross-channel features (energy and correlation based) to incorporate into the segmenter for the purpose of addressing errors related to cross-channel phenomena such as crosstalk. Results demonstrate that these features provide a marked improvement (18% relative) over a baseline system using single-channel features as well as an improvement (8% relative) over our previous solution of separate speech activity detection and cross-channel analysis. In addition, the simple cross-channel energy features are shown to be more robust - and consequently better performing - than the more common correlation-based features.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-538"
  },
  "kida06_interspeech": {
   "authors": [
    [
     "Yusuke",
     "Kida"
    ],
    [
     "Tatsuya",
     "Kawahara"
    ]
   ],
   "title": "Evaluation of voice activity detection by combining multiple features with weight adaptation",
   "original": "i06_1152",
   "page_count": 4,
   "order": 539,
   "p1": "paper 1152-Wed3A1O.4",
   "pn": "",
   "abstract": [
    "For noise-robust automatic speech recognition (ASR), we propose a novel voice activity detection (VAD) method based on a combination of multiple features. The scheme uses a weighted combination of four conventional VAD features: amplitude level, zero crossing rate, spectral information, and Gaussian mixture model (GMM) likelihood. The weights for combination are adaptively updated using minimum classification error (MCE) training. In this paper, we first investigate the effect of adaptation of the combination weights and GMM parameters, and demonstrate that the weights can be effectively adapted with a single utterance. Then, we present application of the method to ASR. It is confirmed that the proposed method significantly outperforms conventional methods in various noise conditions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-539"
  },
  "lee06f_interspeech": {
   "authors": [
    [
     "Keansub",
     "Lee"
    ],
    [
     "Daniel P. W.",
     "Ellis"
    ]
   ],
   "title": "Voice activity detection in personal audio recordings using autocorrelogram compensation",
   "original": "i06_1753",
   "page_count": 4,
   "order": 540,
   "p1": "paper 1753-Wed3A1O.5",
   "pn": "",
   "abstract": [
    "This paper presents a novel method for identifying regions of speech in the kinds of energetic and highly-variable noise present in personal audio collected by body-worn continuous recorders. Motivated by psychoacoustic evidence that pitch is crucial in the perception and organization of sound, we use a noise-robust pitch detection algorithm to locate speech-like regions. To avoid false alarms resulting from background noise with strong periodic components (such as air-conditioning), we add a new channel selection scheme to suppress frequency subbands where the autocorrelation is more stationary than encountered in voiced speech. Quantitative evaluation shows that these harmonic noises are effectively removed by this compensation technique in the domain of auto-correlogram, and that detection performance is significantly better than existing algorithms for detecting the presence of speech in real-world personal audio recordings.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-540"
  },
  "rifkin06_interspeech": {
   "authors": [
    [
     "Ryan",
     "Rifkin"
    ],
    [
     "Nima",
     "Mesgarani"
    ]
   ],
   "title": "Discriminating speech and non-speech with regularized least squares",
   "original": "i06_1779",
   "page_count": 4,
   "order": 541,
   "p1": "paper 1779-Wed3A1O.6",
   "pn": "",
   "abstract": [
    "We consider the task of discriminating speech and non-speech in noisy environments. Previously, Mesgarani et. al [1] achieved state-of-the-art performance using a cortical representation of sound in conjunction with a feature reduction algorithm and a nonlinear support vector machine classifier. In the present work, we show that we can achieve the same or better accuracy by using a linear regularized least squares classifier directly on the high-dimensional cortical representation; the new system is substantially simpler conceptually and computationally. We select the regularization constant automatically, yielding a parameter-free learning system. Intriguingly, we find that optimal classifiers for noisy data can be trained on clean data using heavy regularization.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-541"
  },
  "lee06g_interspeech": {
   "authors": [
    [
     "John",
     "Lee"
    ],
    [
     "Stephanie",
     "Seneff"
    ]
   ],
   "title": "Automatic grammar correction for second-language learners",
   "original": "i06_1299",
   "page_count": 4,
   "order": 542,
   "p1": "paper 1299-Wed3A3O.1",
   "pn": "",
   "abstract": [
    "A computer conversational system can potentially help a foreignlanguage student improve his/her fluency through practice dialogues. One of its potential roles could be to correct ungrammatical sentences. This paper describes our research on a sentence-level, generation-based approach to grammar correction: first, a word lattice of candidate corrections is generated from an ill-formed input. A traditional n-gram language model is used to produce a small set of N-best candidates, which are then reranked by parsing using a stochastic context-free grammar. We evaluate this approach in a flight domain with simulated ill-formed sentences. We discuss its potential applications in a few related tasks.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-542"
  },
  "neri06_interspeech": {
   "authors": [
    [
     "Ambra",
     "Neri"
    ],
    [
     "Catia",
     "Cucchiarini"
    ],
    [
     "Helmer",
     "Strik"
    ]
   ],
   "title": "ASR-based corrective feedback on pronunciation: does it really work?",
   "original": "i06_1372",
   "page_count": 4,
   "order": 543,
   "p1": "paper 1372-Wed3A3O.2",
   "pn": "",
   "abstract": [
    "We studied a group of immigrants who were following regular, teacherfronted Dutch classes, and who were assigned to three groups using either a) Dutch CAPT, an ASR-based Computer Assisted Pronunciation Training (CAPT) system that provides feedback on a number of Dutch speech sounds that are problematic for L2 learners b) a CAPT system without feedback c) no CAPT system. Participants were tested before and after the training. The results show that the ASR-based feedback was effective in correcting the errors addressed in the training.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-543"
  },
  "dong06_interspeech": {
   "authors": [
    [
     "Minghui",
     "Dong"
    ],
    [
     "Haizhou",
     "Li"
    ],
    [
     "Tin Lay",
     "Nwe"
    ]
   ],
   "title": "Evaluating prosody of Mandarin speech for language learning",
   "original": "i06_1432",
   "page_count": 4,
   "order": 544,
   "p1": "paper 1432-Wed3A3O.3",
   "pn": "",
   "abstract": [
    "This paper proposes an approach to automatically evaluate the prosody of Chinese Mandarin speech for language learning. In this approach, we grade the appropriateness of prosody of speech units according to a model speech corpus from a teachers voice. To this end, we build two models, which are the prosody model and the scoring model. The prosody model that is built from the teachers speech predicts the reference prosody for the learning text. The scoring model compares the students prosody with the reference prosody and gives a prosody rating score. Both the prosody model and the scoring model are built using regression tree. To make the two prosodies comparable, we transform the students prosody into the teachers prosody space. To build the scoring model, we derive from the corpus a reference data set, in which prosody rating is associated with prosody parameters. During speech evaluation, the students prosody is first transformed into the teachers prosody space and then evaluated by the scoring model. Experiments show that our model works well for speech of new speakers.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-544"
  },
  "trancoso06b_interspeech": {
   "authors": [
    [
     "Isabel",
     "Trancoso"
    ],
    [
     "Carlos",
     "Duarte"
    ],
    [
     "António",
     "Serralheiro"
    ],
    [
     "Diamantino",
     "Caseiro"
    ],
    [
     "Luís",
     "Carriço"
    ],
    [
     "Céu",
     "Viana"
    ]
   ],
   "title": "Spoken language technologies applied to digital talking books",
   "original": "i06_1448",
   "page_count": 4,
   "order": 545,
   "p1": "paper 1448-Wed3A3O.4",
   "pn": "",
   "abstract": [
    "Digital Talking Books (DTBs) offer to visually impaired users an evolution of analogue talking books that mimics the interaction possibilities of print books. This paper describes a new DTB player which tries to improve the usability and accessibility of current players, through the combination of the possibilities offered by multimodal interaction and interface adaptability, and the integration of several language processing components. Besides the potential for a greater enjoyment of the reader in general, these modifications also pave the way to the use of DTBs in different domains, from e-inclusion to e-learning applications.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-545"
  },
  "iida06_interspeech": {
   "authors": [
    [
     "Akemi",
     "Iida"
    ],
    [
     "Jun",
     "Ito"
    ],
    [
     "Shimpei",
     "Kajima"
    ],
    [
     "Tsutomu",
     "Sugawara"
    ]
   ],
   "title": "Building an English speech synthesis system from a Japanese ALS patient²s voice",
   "original": "i06_1948",
   "page_count": 4,
   "order": 546,
   "p1": "paper 1948-Wed3A3O.5",
   "pn": "",
   "abstract": [
    "This paper reports on the development of an English speech synthesis system for a Japanese amyotropic lateral sclerosis patient as part of the project of developing a bilingual communication aid for this patient. The patient had a tracheotomy three years ago and anticipates the possibility of losing his phonatory function. His English speech database for Festival, a free speech synthesis system, was generated from his reading of a US diphone list. There were two problems with the recording. The first was the noise that the artificial ventilator made and the second was his difficulty in pronouncing English. Although the speakers English database was successfully built by Festvox and the voice was recognized as his voice, the utterance was unintelligible. We therefore proposed reconstructing the patients database by partially combining it with an English native speakers database. Results showed that the proposed approach can be promising for those facing this problem.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-546"
  },
  "karpov06_interspeech": {
   "authors": [
    [
     "Alexey",
     "Karpov"
    ],
    [
     "Andrey",
     "Ronzhin"
    ],
    [
     "Alexandre",
     "Cadiou"
    ]
   ],
   "title": "Multi-modal system ICANDO: intellectual computer assistant for disabled operators",
   "original": "i06_1234",
   "page_count": 4,
   "order": 547,
   "p1": "paper 1234-Wed3A3O.6",
   "pn": "",
   "abstract": [
    "The paper describes a multi-modal system ICANDO (an Intellectual Computer AssistaNt for Disabled Operators) developed by Speech Informatics Group of SPIIRAS and intended for assistance to the persons without hands or with disabilities of their hands or arms in human-computer interaction. This system combines the modules for automatic speech recognition and head tracking in one multi-modal system. The developed system was applied for hands-free work with Graphical User Interface of MS Windows in such tasks as Internet communication and work with text documents. The architecture of the system, the methods for recognition and tracking, information fusion and synchronization, experimental conditions and the obtained results are described in the paper.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-547"
  },
  "skantze06_interspeech": {
   "authors": [
    [
     "Gabriel",
     "Skantze"
    ],
    [
     "David",
     "House"
    ],
    [
     "Jens",
     "Edlund"
    ]
   ],
   "title": "User responses to prosodic variation in fragmentary grounding utterances in dialog",
   "original": "i06_1229",
   "page_count": 4,
   "order": 548,
   "p1": "paper 1229-Wed3WeS.1",
   "pn": "",
   "abstract": [
    "In a previous study we demonstrated that subjects could use prosodic features (primarily peak height and alignment) to make different interpretations of synthesized fragmentary grounding utterances. In the present study we test the hypothesis that subjects also change their behavior accordingly in a human-computer dialog setting. We report on an experiment in which subjects participate in a color-naming task in a Wizard-of-Oz controlled human-computer dialog in Swedish. The results show that two annotators were able to categorize the subjects responses based on pragmatic meaning. Moreover, the subjects response times differed significantly, depending on the prosodic features of the grounding fragment spoken by the system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-548"
  },
  "ishi06_interspeech": {
   "authors": [
    [
     "Carlos Toshinori",
     "Ishi"
    ],
    [
     "Hiroshi",
     "Ishiguro"
    ],
    [
     "Norihiro",
     "Hagita"
    ]
   ],
   "title": "Analysis of prosodic and linguistic cues of phrase finals for turn-taking and dialog acts",
   "original": "i06_1961",
   "page_count": 4,
   "order": 549,
   "p1": "paper 1961-Wed3WeS.2",
   "pn": "",
   "abstract": [
    "This paper presents an analysis on the functions carried by phrase final tones in turn-taking and dialog acts, taking into account linguistic information about the part of speech (particles and auxiliary verbs) attributed to the morphemes at phrase finals. Natural conversational speech data are segmented in inter-pause units, and each utterance unit is arranged according to the phrase final morphemes. Turn-taking functions are annotated, and tones of each phrase final are described by acoustic-prosodic features. Analysis results show a relationship between tones and turn-taking functions in most of the morphemes, while no clear relationship is found in some classes of morphemes which are final particles.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-549"
  },
  "schlangen06_interspeech": {
   "authors": [
    [
     "David",
     "Schlangen"
    ]
   ],
   "title": "From reaction to prediction: experiments with computational models of turn-taking",
   "original": "i06_1200",
   "page_count": 4,
   "order": 550,
   "p1": "paper 1200-Wed3WeS.3",
   "pn": "",
   "abstract": [
    "Deciding when to take (or not to take) the turn in a conversation is an important task. It has been stressed in the descriptive literature that such decisions must involve prediction, as they often seem to be made before a transition place has been reached. In computational systems, however, turn-taking is normally a reaction to parameters like pause length. In this paper, we report on experiments that try to bridge this gap. We describe an experiment (using controlled stimuli) that shows human performance at prediction of turn-taking decisions and then show that a model automatically induced from data can reach a similar level of performance. We then describe a series of experiments on spontaneous dialogue data where we combine pause thresholds with syntactic and prosodic information to make turn-taking decisions, successively reducing the pause threshold until reaction becomes prediction. All our classifiers improve significantly over the baselines; prediction however is shown to be the hardest task, and we discuss additional information sources that could improve it.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-550"
  },
  "kolar06_interspeech": {
   "authors": [
    [
     "Jáchym",
     "Kolár"
    ],
    [
     "Elizabeth",
     "Shriberg"
    ],
    [
     "Yang",
     "Liu"
    ]
   ],
   "title": "On speaker-specific prosodic models for automatic dialog act segmentation of multi-party meetings",
   "original": "i06_1900",
   "page_count": 4,
   "order": 551,
   "p1": "paper 1900-Wed3WeS.4",
   "pn": "",
   "abstract": [
    "We explore speaker-specific prosodic modeling for dialog act segmentation of speech from the ICSI Meeting Corpus. We ask whether features beyond pauses help individual speakers, and whether some speakers benefit from prosody models trained on only their speech. We find positive results for both questions, although the second is more complex. Feature analysis reveals that duration is the most used feature type, followed by pause and pitch features. Results also suggest a difference between native and nonnative speakers in feature usage patterns. We conclude that features beyond pauses are useful for dialog act segmentation in natural conversation, and that for some speakers, speaker-specific training yields further gains.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-551"
  },
  "ward06_interspeech": {
   "authors": [
    [
     "Nigel G.",
     "Ward"
    ],
    [
     "Yaffa Al",
     "Bayyari"
    ]
   ],
   "title": "A case study in the identification of prosodic cues to turn-taking: back-channeling in Arabic",
   "original": "i06_1257",
   "page_count": 4,
   "order": 552,
   "p1": "paper 1257-Wed3WeS.5",
   "pn": "",
   "abstract": [
    "Discovering and quantifying the prosodic signals that help manage turn-taking is difficult, in part because of the limitations of commonly used methods. This paper presents an integrated method that uses both perceptually-based analysis and quantitative analysis. The eight activities involved in the method - clarification of aims, problem formulation, corpus preparation, feature discovery, feature combination, hypothesis refinement, tuning, and evaluation - are illustrated using task of finding prosodic cues for back-channel feedback in Arabic.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-552"
  },
  "edlund06_interspeech": {
   "authors": [
    [
     "Jens",
     "Edlund"
    ],
    [
     "Mattias",
     "Heldner"
    ]
   ],
   "title": "/nailon/ - software for online analysis of prosody",
   "original": "i06_1557",
   "page_count": 4,
   "order": 553,
   "p1": "paper 1557-Wed3WeS.6",
   "pn": "",
   "abstract": [
    "This paper presents /nailon/ - a software package for online realtime prosodic analysis that captures a number of prosodic features relevant for interaction control in spoken dialogue systems. The current implementation captures silence durations; voicing, intensity, and pitch; pseudo-syllable durations; and intonation patterns. The paper provides detailed information on how this is achieved. As an example application of /nailon/, we demonstrate how it is used to improve the efficiency of identifying relevant places at which a machine can legitimately begin to talk to a human interlocutor, as well as to shorten system response times.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-553"
  },
  "li06g_interspeech": {
   "authors": [
    [
     "Junfeng",
     "Li"
    ],
    [
     "Masato",
     "Akagi"
    ],
    [
     "Yôiti",
     "Suzuki"
    ]
   ],
   "title": "Improved hybrid microphone array post-filter by integrating a robust speech absence probability estimator for speech enhancement",
   "original": "i06_1035",
   "page_count": 4,
   "order": 554,
   "p1": "paper 1035-Wed3FoP.1",
   "pn": "",
   "abstract": [
    "To improve the performance of multi-channel speech enhancement algorithms, we previously proposed a hybrid Wiener post-filter for microphone arrays under the assumption of a diffuse noise field [4]. In this paper, considering the speech presence uncertainty, we further improve the hybrid post-filter presented before by integrating a novel robust estimator for the a priori speech absence probability, which makes full use of the correlation characteristics of the noises on different microphone pairs and hence offers the much more accurate speech absence probability estimates. The effectiveness of this improved hybrid post-filter was finally confirmed by the experiments using multi-channel recordings in various car environments.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-554"
  },
  "gerkmann06_interspeech": {
   "authors": [
    [
     "Timo",
     "Gerkmann"
    ],
    [
     "Rainer",
     "Martin"
    ]
   ],
   "title": "Soft decision combining for dual channel noise reduction",
   "original": "i06_1059",
   "page_count": 4,
   "order": 555,
   "p1": "paper 1059-Wed3FoP.2",
   "pn": "",
   "abstract": [
    "We present a flexible dual channel noise reduction algorithm that combines the information of two microphone channels in the discrete Fourier transform domain. This algorithm can cope with substantially different signal-to-noise ratios at different time-frequency bins in both channels. The output is obtained by combining the outputs of two single channel filters and a dual channel filter weighted by the probability of speech presence. The algorithm has a low latency and does not require any additional information such as the microphone positions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-555"
  },
  "chen06d_interspeech": {
   "authors": [
    [
     "Guo",
     "Chen"
    ],
    [
     "Vijay",
     "Parsa"
    ]
   ],
   "title": "An improved affine projection algorithm based crosstalk resistant adaptive noise canceller",
   "original": "i06_1320",
   "page_count": 4,
   "order": 556,
   "p1": "paper 1320-Wed3FoP.3",
   "pn": "",
   "abstract": [
    "This paper presents an improved affine projection algorithm based crosstalk resistant adaptive noise canceller (CRANC). The proposed CRANC consists of three adaptive filters, one of which is used for detecting speech activity and the others are employed to estimate the interference and crosstalk transfer functions, respectively. An efficient sequential speech detector is also proposed in this paper, which is completely built into the adaptive strategy and thus causes no delay in the input and output of the noise canceller. The proposed CRANC was tested and compared with the conventional CRANC in the white Gaussian noise environment. Experimental results show that the proposed CRANC achieved higher signal-to-noise ratio (SNR) improvements than the conventional CRANC.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-556"
  },
  "leukimmiatis06_interspeech": {
   "authors": [
    [
     "Stamatis",
     "Leukimmiatis"
    ],
    [
     "Dimitrios",
     "Dimitriadis"
    ],
    [
     "Petros",
     "Maragos"
    ]
   ],
   "title": "An optimum microphone array post-filter for speech applications",
   "original": "i06_1389",
   "page_count": 4,
   "order": 557,
   "p1": "paper 1389-Wed3FoP.4",
   "pn": "",
   "abstract": [
    "This paper proposes a post-filtering estimation scheme for multichannel noise reduction. The proposed method extends and improves the existing Zelinskis and, the most general and prominent, McCowans post-filtering methods that use the auto- and cross-spectral densities of the multichannel input signals to estimate the transfer function of the Wiener post-filter. A major drawback of these two speech enhancement algorithms is that the noise power spectrum at the beamformers output is over-estimated and therefore the derived filters are sub-optimal in the Wiener sense. The proposed method deals with this problem and can be considered as an optimal postfilter that is appropriate for a wide variety of different noise fields. In experiments over real-noise multichannel recordings, the proposed technique is shown to obtain a significant headstart over the other methods in terms of signal-to-noise ratio and speech degradation measures. In addition it is used for ASR experiments where promising preliminary results are presented.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-557"
  },
  "flego06_interspeech": {
   "authors": [
    [
     "Federico",
     "Flego"
    ],
    [
     "Maurizio",
     "Omologo"
    ]
   ],
   "title": "Multi-microphone periodicity function for robust F0 estimation in real noisy and reverberant environments",
   "original": "i06_1536",
   "page_count": 4,
   "order": 558,
   "p1": "paper 1536-Wed3FoP.5",
   "pn": "",
   "abstract": [
    "This paper outlines a new method to extract F0 from distant-talking speech signals acquired by a microphone network, which exploits the redundancy across the signals proceeding from each microphone, by jointly processing the different contributes. To this purpose, a multi-microphone periodicity function is derived from the magnitude spectrum computed on each microphone signal. This function allows to estimate F0 reliably, even under reverberant conditions, without the need of any post-processing or smoothing technique. Experiments, conducted on real lectures, showed that the proposed frequencydomain algorithm is more suitable than other time-domain based ones.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-558"
  },
  "abutalebi06_interspeech": {
   "authors": [
    [
     "H. R.",
     "Abutalebi"
    ],
    [
     "M.",
     "Pourahmadi"
    ],
    [
     "M.R.",
     "Aghabozorgi"
    ]
   ],
   "title": "A new dual-microphone speech enhancement method for oriented noises",
   "original": "i06_1973",
   "page_count": 4,
   "order": 559,
   "p1": "paper 1973-Wed3FoP.6",
   "pn": "",
   "abstract": [
    "In this paper, we examine a modified Cross-Talk Resistant Adaptive Noise Canceller (CTRANC) structure that contains a delay unit on the primary channel to solve the causality constraint of conventional CTRANC. This Asymmetric CTRANC (ACTRANC) structure allows flexible alignment of noise source with the sensors array when the speech source is fixed in its position. Inserting delay unit on the primary path releases the placement of the noise sources and consequently, improves the capability of the structure in noise reduction. This can also be interpreted as the releasing causality constraint on the adaptive filters. We have shown that the best delay value in the structure is proportional to the propagation delay between reference and primary microphones (in terms of sample). Objective evaluations and informal listening tests demonstrate superior performance of the proposed method rather than conventional CTRANC.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-559"
  },
  "lovitt06_interspeech": {
   "authors": [
    [
     "Andrew",
     "Lovitt"
    ],
    [
     "Jont B.",
     "Allen"
    ]
   ],
   "title": "50 years late: repeating miller-nicely 1955",
   "original": "i06_1297",
   "page_count": 4,
   "order": 560,
   "p1": "paper 1297-Wed3FoP.7",
   "pn": "",
   "abstract": [
    "Portions of the procedure and analysis of the wide-band noise masking experiment in Miller-Nicelys 1955 JASA paper (MN55) was repeated and a new set of data was collected in 2005. This classic paper is a commonly referenced work in which confusion matrices were collected for a set of consonant-vowels (CVs). From an analysis of the original results, they made conclusions about the robustness of various distinctive features when the CVs are degraded in masking noise. Our repeat experiment shows a number of similarities and differences. The two experiments show significantly different amounts of relative information transmitted for each distinctive feature. In the repeat experiment the voicing feature is less robust whereas the place feature is more robust.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-560"
  },
  "sakamoto06_interspeech": {
   "authors": [
    [
     "Shuichi",
     "Sakamoto"
    ],
    [
     "Tadahiro",
     "Yoshikawa"
    ],
    [
     "Shigeaki",
     "Amano"
    ],
    [
     "Yôiti",
     "Suzuki"
    ],
    [
     "Tadahisa",
     "Kondo"
    ]
   ],
   "title": "New 20-word lists for word intelligibility test in Japanese",
   "original": "i06_1517",
   "page_count": 4,
   "order": 561,
   "p1": "paper 1517-Wed3FoP.8",
   "pn": "",
   "abstract": [
    "\"Familiarity controlled Word lists 2003 (FW03)\" is a set of 20 lists of 50 Japanese words each in four ranks of word familiarity (for a total of 4000 words). FW03 is useful for evaluating the ability of a person to hear human speech. However, it has been pointed out that the FW03 lists have too many words to evaluate the ability of a person to hear human speech in a short time. This can cause problems, such as when evaluating the ability of an elderly person to hear human speech during a medical exam. To remedy this problem, we propose new word lists that are a subset of FW03. The new lists were created by considering both phonetic balance and word familiarity. Each of the new word lists comprises only 20 words. This reduces the duration of an examination to around two-fifths of a conventional FW03-based examination. Word intelligibility tests suggest that these new lists provide word intelligibility scores nearly equal to those resulting from FW03.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-561"
  },
  "li06h_interspeech": {
   "authors": [
    [
     "Guoping",
     "Li"
    ],
    [
     "Mark E.",
     "Lutman"
    ]
   ],
   "title": "Sparseness and speech perception in noise",
   "original": "i06_1466",
   "page_count": 4,
   "order": 562,
   "p1": "paper 1466-Wed3FoP.9",
   "pn": "",
   "abstract": [
    "Can we model speech recognition in noise by exploring higher order statistics of the combined signal? How will changes in these statistics affect speech perception in noise? This study addresses these questions in two experiments. One investigated the relationship between an established \"glimpsing\" model and the fourth order statistic, kurtosis. The glimpsing model [1] proposes that listeners can explore the local speech-to-noise ratio (SNR) in short time segments (glimpses) and focus on areas where SNR is high. Results showed that there is a very high correlation between percentages of glimpsing area and kurtosis (r = 0.99;p < 0.01), suggesting that kurtosis can serve as a simpler index for measuring glimpsing. The experiment also examined the association between kurtosis and recognition of nonsense words (vowel-consonant-vowel, VCV) in babble modulated noise, also showing very high correlation (r = 0.97;p < 0.01). Another separate study focused on the relationship of sparseness to speech recognition score for VCV words in natural babble noise made of 100 people talking simultaneously [2]. Results show that there is also high correlation between kurtosis and speech recognition score with this noise. Logistic regression analysis to obtain the kurtosis for 50% correct showed this was achieved at a kurtosis of approximately 1.0.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-562"
  },
  "liu06f_interspeech": {
   "authors": [
    [
     "Wei M.",
     "Liu"
    ],
    [
     "John S. D.",
     "Mason"
    ],
    [
     "Nicholas W. D.",
     "Evans"
    ],
    [
     "Keith A.",
     "Jellyman"
    ]
   ],
   "title": "An assessment of automatic speech recognition as speech intelligibility estimation in the context of additive noise",
   "original": "i06_1191",
   "page_count": 4,
   "order": 563,
   "p1": "paper 1191-Wed3FoP.10",
   "pn": "",
   "abstract": [
    "This paper investigates the potential applicability of automatic speech recognition (ASR) and 6 well-reported objective quality measures for the task of ranking intelligibility of speech degraded by different real life background noises. In a recent investigation ASR has been reported to give high subjective correlation with human assessment when tested with various system degradations. This paper extends this investigation in two directions. First, the usefulness of the measures in the context of different real-life noises is considered. Second, the direct correspondence between statistics computed by an ASR system and human perceived intelligibility is assessed. Subjective listening tests are carried out to provide ground truth. Results show that ASR and WSS (weighted spectral slope) are the only two measures out of the seven considered to give good correlation with human opinion. Specially noted is performance of ASR with correlations ranging from 0.77 to 0.90.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-563"
  },
  "waltermann06_interspeech": {
   "authors": [
    [
     "Marcel",
     "Wältermann"
    ],
    [
     "Kirstin",
     "Scholz"
    ],
    [
     "Alexander",
     "Raake"
    ],
    [
     "Ulrich",
     "Heute"
    ],
    [
     "Sebastian",
     "Möller"
    ]
   ],
   "title": "Underlying quality dimensions of modern telephone connections",
   "original": "i06_1089",
   "page_count": 4,
   "order": 564,
   "p1": "paper 1089-Wed3FoP.11",
   "pn": "",
   "abstract": [
    "It is the aim of the present paper to analyze the perceptual quality dimensions of modern telephone connections. Such connections differ from standard connections in their time-variant characteristics (e.g., due to Voice-over-IP transmission or due to noise reduction algorithms) and their user interfaces (e.g., hands-free terminals). With the help of two independent auditory experiments with subsequent multidimensional analyses, three perceptual dimensions were identified for a diverse set of stimuli. These dimensions were labeled \"directness/frequency content\", \"continuity\", and \"noisiness\". Overall listening quality scores were collected in a separate experiment. A mapping of the obtained dimensions onto the overall listening quality scores by means of a linear model revealed that \"continuity\" appears to be the most important dimension in terms of overall listening quality.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-564"
  },
  "chen06e_interspeech": {
   "authors": [
    [
     "Guo",
     "Chen"
    ],
    [
     "Vijay",
     "Parsa"
    ],
    [
     "Susan",
     "Scollie"
    ]
   ],
   "title": "An ERB loudness pattern based objective speech quality measure",
   "original": "i06_1318",
   "page_count": 4,
   "order": 565,
   "p1": "paper 1318-Wed3FoP.12",
   "pn": "",
   "abstract": [
    "This paper presents an objective speech quality measure which is based on loudness patterns using the equivalent rectangular bandwidth (ERB) scale. The proposed measure, called the loudness pattern distortion (LPD), is computed from the differences between the loudness patterns of the original and processed speech. The LPD measure takes into account the transmission through the outer and middle ear, the calculation of an excitation pattern from the physical spectrum, and the transformation of an excitation pattern to a loudness pattern. The effectiveness of the proposed measure was demonstrated by experimental evaluations in comparison with the standard ITU-T P.862 (PESQ) using three coded speech database of the ITU-T P-series Supplementary 23.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-565"
  },
  "ning06_interspeech": {
   "authors": [
    [
     "Huazhong",
     "Ning"
    ],
    [
     "Ming",
     "Liu"
    ],
    [
     "Hao",
     "Tang"
    ],
    [
     "Thomas S.",
     "Huang"
    ]
   ],
   "title": "A spectral clustering approach to speaker diarization",
   "original": "i06_1607",
   "page_count": 4,
   "order": 566,
   "p1": "paper 1607-Thu1A1O.1",
   "pn": "",
   "abstract": [
    "In this paper, we present a spectral clustering approach to explore the possibility of discovering structure from audio data. To apply the Ng-Jordan-Weiss (NJW) spectral clustering algorithm to speaker diarization, we propose some domain specific solutions to the open issues of this algorithm: choice of metric; selection of scaling parameter; estimation of the number of clusters. Then, a postprocessing step - \"Cross EM refinement\" - is conducted to further improve the performance of spectral learning. In experiments, this approach has performance very similar to the traditional hierarchical clustering on the audio data of Japanese Parliament Panel Discussions, but it runs much faster than the latter.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-566"
  },
  "zdansky06_interspeech": {
   "authors": [
    [
     "Jindrich",
     "Zdansky"
    ]
   ],
   "title": "BINSEG: an efficient speaker-based segmentation technique",
   "original": "i06_1459",
   "page_count": 4,
   "order": 567,
   "p1": "paper 1459-Thu1A1O.2",
   "pn": "",
   "abstract": [
    "In this paper we present a new efficient approach to speaker-based audio stream segmentation. It employs binary segmentation technique that is well-known from mathematical statistic. Because integral part of this technique is hypotheses testing, we compare two well-founded (Maximum Likelihood, Informational) and one commonly used (BIC difference) approach for deriving speaker-change test statistics. Based on results of this comparison we propose both off-line and on-line speaker change detection algorithms (including way of effective training) that have merits of high accuracy and low computational costs. In simulated tests with artificially mixed data the on-line algorithm identified 95.7% of all speaker changes with precision of 96.9%. In tests done with 30 hours of real broadcast news (in 9 languages) the average recall was 74.4% and precision 70.3%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-567"
  },
  "gallardoantolin06_interspeech": {
   "authors": [
    [
     "Ascensión",
     "Gallardo-Antolín"
    ],
    [
     "Xavier",
     "Anguera"
    ],
    [
     "Chuck",
     "Wooters"
    ]
   ],
   "title": "Multi-stream speaker diarization systems for the meetings domain",
   "original": "i06_1620",
   "page_count": 4,
   "order": 568,
   "p1": "paper 1620-Thu1A1O.3",
   "pn": "",
   "abstract": [
    "In the context of speech and speaker recognition systems, it is well known that the combination of different feature streams can improve significantly their performance. However, the application of multi-stream (MS) techniques to speaker diarization systems has not been extensively studied. In this paper, we address this issue: we formulate different MS techniques, such as feature combination, probability combination and selection, for their specific application to the segmentation and clustering modules of a speaker diarization system. We evaluate the different methods proposed for the meetings domain (RT04s database) and two different pairs of streams: first, MFCC and PLP and second, MFCC and prosodic features. For both types of multi-streams, results show that the MS probability combination approach applied to the segmentation stage clearly outperforms the single-stream, MS feature combination and MS selection systems.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-568"
  },
  "lopes06_interspeech": {
   "authors": [
    [
     "Carla",
     "Lopes"
    ],
    [
     "Fernando",
     "Perdigão"
    ]
   ],
   "title": "Improved performance evaluation of speech event detectors",
   "original": "i06_1615",
   "page_count": 4,
   "order": 569,
   "p1": "paper 1615-Thu1A1O.4",
   "pn": "",
   "abstract": [
    "The goal of event-based (EB) systems is the detection of the occurrence of important elements in the speech signal for different sound classes. In a speech recognition system, events can be combined to detect phones, words or sentences, or to identify landmarks to which a classifier or a decoder could be synchronized. The time boundaries of the events are then as important as the events themselves. Accordingly, the assessment of EB systems must take into account not only the correct identified sequence of events but also their correct time localization. Usually, only the token sequence or its boundaries are taken for evaluation. In this paper we propose an extension to standard recognition evaluation procedure, which combines recognition and segmentation performance. In our proposal, a modified Levensthein algorithm is used in the alignment between labeled and recognized events, where the degree of overlapping between them is taken in the local distance definition. We evaluate our approach on a rule based event detector, using the TIMIT corpus and compare the results of the new evaluation procedure with standard metrics. The results show that accuracy drops if alignment is made as a function of the overlapping between labels; nevertheless the agreement with the labeled boundaries is significantly improved.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-569"
  },
  "pardo06_interspeech": {
   "authors": [
    [
     "Jose M.",
     "Pardo"
    ],
    [
     "Xavier",
     "Anguera"
    ],
    [
     "Chuck",
     "Wooters"
    ]
   ],
   "title": "Speaker diarization for multiple distant microphone meetings: mixing acoustic features and inter-channel time differences",
   "original": "i06_1337",
   "page_count": 4,
   "order": 570,
   "p1": "paper 1337-Thu1A1O.5",
   "pn": "",
   "abstract": [
    "Speaker diarization for recordings made in meetings consists of identifying the number of participants in each meeting and creating a list of speech time intervals for each participant. In recently published work [7] we presented some experiments using only TDOA values (Time Delay Of Arrival for different channels) applied to this task. We demonstrated that information in those values can be used to segment the speakers. In this paper we have developed a method to mix the TDOA values with the acoustic values by calculating a combined log-likelihood between both sets of vectors. Using this method we have been able to reduce the DER by 16.34% (relative) for the NIST RT05s set (scored without overlap and manually transcribed references) the DER for our devel06s set (scored with overlap and force-aligned references) by 21% (relative) and the DER for the NIST RT06s (scored with overlap and manually transcribed references) by 15% (relative).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-570"
  },
  "pham06_interspeech": {
   "authors": [
    [
     "Tuan Van",
     "Pham"
    ],
    [
     "Gernot",
     "Kubin"
    ]
   ],
   "title": "Low-complexity and efficient classification of voiced/unvoiced/silence for noisy environments",
   "original": "i06_1400",
   "page_count": 4,
   "order": 571,
   "p1": "paper 1400-Thu1A1O.6",
   "pn": "",
   "abstract": [
    "This paper describes a low-complexity and efficient speech classifier for noisy environments. The proposed algorithm utilizes the advantage of time-scale analysis of the Wavelet decomposition to classify speech frames into voiced, unvoiced and silence classes. The classifier uses only one single multidimensional feature which is extracted from the Teager energy operator of the wavelet coefficients. The feature is enhanced and compared with quantile-based adaptive thresholds to detect phonetical classes. Furthermore, to save memory, the adaptive thresholds are replaced by a slope tracking method on the filtered feature. These algorithms are tested with the TIMIT database and additive white, car, factory noise, and compared with other methods to demonstrate their superior performance and robustness.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-571"
  },
  "suzuki06b_interspeech": {
   "authors": [
    [
     "Motoyuki",
     "Suzuki"
    ],
    [
     "Yasutomo",
     "Kajiura"
    ],
    [
     "Akinori",
     "Ito"
    ],
    [
     "Shozo",
     "Makino"
    ]
   ],
   "title": "Unsupervised language model adaptation based on automatic text collection from WWW",
   "original": "i06_1806",
   "page_count": 4,
   "order": 572,
   "p1": "paper 1806Thu1A2O.1",
   "pn": "",
   "abstract": [
    "An n-gram trained by a general corpus gives high performance. However, it is well known that a topic-specialized n-gram gives higher performance than that of the general n-gram. In order to make a topic specialized n-gram, several adaptation methods were proposed. These methods use a given corpus corresponding to the target topic, or collect documents related to the topic from a database. If there is neither the given corpus nor the topic-related documents in the database, the general n-gram cannot be adapted to the topic-specialized n-gram. In this paper, a new unsupervised adaptation method is proposed. The method collects topic-related documents from the world wide web. Several query terms are extracted from recognized text, and collected web pages given by a search engine are used for adaptation. Experimental results showed the proposed method gave 7.2 points higher word accuracy than that given by the general n-gram.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-572"
  },
  "tam06_interspeech": {
   "authors": [
    [
     "Yik-Cheung",
     "Tam"
    ],
    [
     "Tanja",
     "Schultz"
    ]
   ],
   "title": "Unsupervised language model adaptation using latent semantic marginals",
   "original": "i06_1705",
   "page_count": 4,
   "order": 573,
   "p1": "paper 1705-Thu1A2O.2",
   "pn": "",
   "abstract": [
    "We integrated the Latent Dirichlet Allocation (LDA) approach, a latent semantic analysis model, into unsupervised language model adaptation framework. We adapted a background language model by minimizing the Kullback-Leibler divergence between the adapted model and the background model subject to a constraint that the marginalized unigram probability distribution of the adapted model is equal to the corresponding distribution estimated by the LDA model - the latent semantic marginals. We evaluated our approach on the RT04 Mandarin Broadcast News test set and experimented with different LM training settings. Results showed that our approach reduces the perplexity and the character error rates using supervised and unsupervised adaptation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-573"
  },
  "mrva06_interspeech": {
   "authors": [
    [
     "David",
     "Mrva"
    ],
    [
     "Philip C.",
     "Woodland"
    ]
   ],
   "title": "Unsupervised language model adaptation for Mandarin broadcast conversation transcription",
   "original": "i06_1549",
   "page_count": 4,
   "order": 574,
   "p1": "paper 1549-Thu1A2O.3",
   "pn": "",
   "abstract": [
    "This paper investigates unsupervised language model adaptation on a new task of Mandarin broadcast conversation transcription. It was found that N-gram adaptation yields 1.1% absolute character error rate gain and continuous space language model adaptation done with PLSA and LDA brings 1.3% absolute gain. Moreover, using broadcast news language model alone trained on large data under-performs a model that includes additional small amount of broadcast conversations by 1.8% absolute character error rate. Although, broadcast news and broadcast conversation tasks are related, this result shows their large mismatch. In addition, it was found that it is possible to do a reliable detection of broadcast news and broadcast conversation data with the N-gram adaptation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-574"
  },
  "klakow06_interspeech": {
   "authors": [
    [
     "Dietrich",
     "Klakow"
    ]
   ],
   "title": "Language model adaptation for tiny adaptation corpora",
   "original": "i06_1446",
   "page_count": 4,
   "order": 575,
   "p1": "paper 1446-Thu1A2O.4",
   "pn": "",
   "abstract": [
    "In this paper we address the issue of building language models for very small training sets by adapting existing corpora. In particular we investigate methods that combine task specific unigrams with longer range models trained on a background corpus. We propose a new method to adapt class models and show how fast marginal adaptation can be improved. Instead of estimating the adaptation unigram only on the adaptation corpus, we study specific methods to adapt unigram models as well. In extensive experimental studies we show the effectiveness of the proposed methods. As compared to FMA as described in [1] we obtain an improvement of nearly 60% for ten utterances of adaptation data.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-575"
  },
  "ljolje06b_interspeech": {
   "authors": [
    [
     "Andrej",
     "Ljolje"
    ]
   ],
   "title": "Pronunciation dependent language models",
   "original": "i06_1991",
   "page_count": 4,
   "order": 576,
   "p1": "paper 1991-Thu1A2O.5",
   "pn": "",
   "abstract": [
    "Speech recognition systems are conventionally broken up into phonemic acoustic models, pronouncing dictionaries in terms of the phonemic units in the acoustic model and language models in terms of lexical units from the pronouncing dictionary. Here we explore a new method for incorporating pronunciation probabilities into recognition systems by moving them from the pronouncing lexicon into the language model. The advantages are that pronunciation dependencies across word boundaries can be modeled including contextual dependencies like geminates or consistency in pronunciation style throughout the utterance. The disadvantage is that the number of lexical items grows proportionally to the number of pronunciation alternatives per word and that language models which could be trained using text, now need phonetically transcribed speech or equivalent training data. Here this problem is avoided by only considering the most frequent words and word clusters. Those new lexical items are given entries in the dictionary and the language model dependent on the chosen pronunciation. The consequence is that pronunciation probabilities are incorporated into the language model and removed form the dictionary, resulting in an error rate reduction. Also, the introduction of pronunciation dependent word pairs as lexical items changes the behavior of the language model to approximate higher order n-gram language models, also resulting in improved recognition accuracy.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-576"
  },
  "nanavati06_interspeech": {
   "authors": [
    [
     "Amit Anil",
     "Nanavati"
    ],
    [
     "Nitendra",
     "Rajput"
    ]
   ],
   "title": "Improving perplexity measures to incorporate acoustic confusability",
   "original": "i06_1940",
   "page_count": 4,
   "order": 577,
   "p1": "paper 1940-Thu1A2O.6",
   "pn": "",
   "abstract": [
    "Traditionally, Perplexity has been used as a measure of language model performance to predict its goodness in a speech recognition system. However this measure does not take into account the acoustic confusability between words in the language model. In this paper, we introduce Equivocality - modification of the perplexity measure for it to incorporate the acoustic features of words in a language. This gives an improved measuring criterion that matches much better with the recognition results than conventional Perplexity measure. The acoustic distance is used as a feature to represent the acoustic characteristic of the language model. This distance is measurable only with the acoustic model parameters and does not require any experimentation. We derive the Equivocality measure and calculate it for a set of grammars. Speech recognition experiments further justify the appropriateness of using Equivocality over Perplexity.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-577"
  },
  "qin06_interspeech": {
   "authors": [
    [
     "Long",
     "Qin"
    ],
    [
     "Yi-Jian",
     "Wu"
    ],
    [
     "Zhen-Hua",
     "Ling"
    ],
    [
     "Ren-Hua",
     "Wang"
    ]
   ],
   "title": "Improving the performance of HMM-based voice conversion using context clustering decision tree and appropriate regression matrix format",
   "original": "i06_1105",
   "page_count": 4,
   "order": 578,
   "p1": "paper 1105-Thu1BuP.1",
   "pn": "",
   "abstract": [
    "To improve the performance of the HMM-based voice conversion system in which the LSP coefficient is introduced as the spectral representation, a model clustering technique to tie HMMs into classes for the model adaptation, considering the phonetic and linguistic contextual factors of HMMs, is adopted in this paper. Besides, due to the relationship between the LSP coefficients of adjacent orders, an appropriate format of the regression matrix is suggested according to the small amount of the adaptation training data. Subjective and objective tests prove that the source HMMs can be adapted more accurately using the proposed method, meanwhile the synthetic speech generated from the adapted model has better discrimination and speech quality.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-578"
  },
  "lee06h_interspeech": {
   "authors": [
    [
     "Chung-Han",
     "Lee"
    ],
    [
     "Chung-Hsien",
     "Wu"
    ]
   ],
   "title": "Map-based adaptation for speech conversion using adaptation data selection and non-parallel training",
   "original": "i06_1164",
   "page_count": 4,
   "order": 579,
   "p1": "paper 1164-Thu1BuP.2",
   "pn": "",
   "abstract": [
    "This study presents an approach to GMM-based speech conversion using maximum a posteriori probability (MAP) adaptation. First, a conversion function is trained using a parallel corpus containing the same utterances spoken by both the source and the reference speakers. Then a non-parallel corpus from a new target speaker is used for the adaptation of the conversion function which models the voice conversion between the source speaker and the new target speaker. The consistency among the adaptation data is estimated to select suitable data from the non-parallel corpus for MAP-based adaptation of the GMMs. In speech conversion evaluation, experimental results show that MAP adaptation using a small non-parallel corpus can reduce the conversion error and improve the speech quality for speaker identification compared to the method without adaptation. Objective and subjective tests also confirm the promising performance of the proposed approach.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-579"
  },
  "nurminen06b_interspeech": {
   "authors": [
    [
     "Jani",
     "Nurminen"
    ],
    [
     "Jilei",
     "Tian"
    ],
    [
     "Victor",
     "Popa"
    ]
   ],
   "title": "Novel method for data clustering and mode selection with application in voice conversion",
   "original": "i06_1463",
   "page_count": 4,
   "order": 580,
   "p1": "paper 1463-Thu1BuP.3",
   "pn": "",
   "abstract": [
    "Since the statistical properties of speech signals are variable and depend heavily on the content, it is hard to design speech processing techniques that would perform well on all inputs. For example, in voice conversion, where the aim is to transform the speech uttered by a source speaker to sound as if it was spoken by a target speaker, different types of inter-speaker relationships can be found from different types of speech segments. To tackle this problem in a robust manner, we have developed a novel scheme for data clustering and mode selection. When applied in the voice conversion application, the main idea of the proposed approach is to first cluster the target data to achieve a minimized intra-cluster variability. Then, a mode selector or a classifier is trained on aligned source-related data to recognize the target-based clusters. Auxiliary speech features can be used to enhance the classification accuracy, in addition to the source data. Finally, a separate conversion scheme is trained and used for each cluster. The proposed scheme is fully data-driven and it avoids the need to use heuristic solutions. The superior performance of the proposed scheme has been verified in a practical voice conversion system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-580"
  },
  "sundermann06_interspeech": {
   "authors": [
    [
     "David",
     "Sündermann"
    ],
    [
     "Harald",
     "Höge"
    ],
    [
     "Antonio",
     "Bonafonte"
    ],
    [
     "Hermann",
     "Ney"
    ],
    [
     "Julia",
     "Hirschberg"
    ]
   ],
   "title": "Text-independent cross-language voice conversion",
   "original": "i06_1665",
   "page_count": 4,
   "order": 581,
   "p1": "paper 1665-Thu1BuP.4",
   "pn": "",
   "abstract": [
    "So far, cross-language voice conversion requires at least one bilingual speaker and parallel speech data to perform the training. This paper shows how these obstacles can be overcome by means of a recently presented text-independent training method based on unit selection. The new method is evaluated in the framework of the European speech-to-speech translation project TC-Star and achieves a performance similar to that of text-dependent intra-lingual voice conversion.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-581"
  },
  "ohtani06_interspeech": {
   "authors": [
    [
     "Yamato",
     "Ohtani"
    ],
    [
     "Tomoki",
     "Toda"
    ],
    [
     "Hiroshi",
     "Saruwatari"
    ],
    [
     "Kiyohiro",
     "Shikano"
    ]
   ],
   "title": "Maximum likelihood voice conversion based on GMM with STRAIGHT mixed excitation",
   "original": "i06_1681",
   "page_count": 4,
   "order": 582,
   "p1": "paper 1681-Thu1BuP.5",
   "pn": "",
   "abstract": [
    "The performance of voice conversion has been considerably improved through statistical modeling of spectral sequences. However, the converted speech still contains traces of artificial sounds. To alleviate this, it is necessary to statistically model a source sequence as well as a spectral sequence. In this paper, we introduce STRAIGHT mixed excitation to a framework of the voice conversion based on a Gaussian Mixture Model (GMM) on joint probability density of source and target features. We convert both spectral and source feature sequences based on Maximum Likelihood Estimation (MLE). Objective and subjective evaluation results demonstrate that the proposed source conversion produces strong improvements in both the converted speech quality and the conversion accuracy for speaker individuality.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-582"
  },
  "nakagiri06_interspeech": {
   "authors": [
    [
     "Mikihiro",
     "Nakagiri"
    ],
    [
     "Tomoki",
     "Toda"
    ],
    [
     "Hideki",
     "Kashioka"
    ],
    [
     "Kiyohiro",
     "Shikano"
    ]
   ],
   "title": "Improving body transmitted unvoiced speech with statistical voice conversion",
   "original": "i06_1719",
   "page_count": 4,
   "order": 583,
   "p1": "paper 1719-Thu1BuP.6",
   "pn": "",
   "abstract": [
    "The conversion method from Non-Audible Murmur (NAM) to ordinary speech based on the statistical voice conversion (NAM-to-Speech) has been proposed towards realization of \"silent speech telephone.\" Although NAM-to-Speech converts NAM to intelligible voices with similar quality to speech, there is still a large problem, i.e., difficulties of the F0 estimation from unvoiced speech. In order to avoid this problem, we propose a conversion method from NAM to whisper that is a familiar and intelligible unvoiced speech (NAM-to-Whisper). Moreover, we enhance NAM-to-Whisper so that multiple types of body-transmitted unvoiced speech such as NAM and Body Transmitted Whisper (BTW) are accepted as input voices. We evaluate the performance of the proposed conversion method. Experimental results demonstrate that 1) intelligibility and naturalness of NAM are significantly improved by NAM-to-Whisper, 2) NAM-to-Whisper outperforms NAM-to-Speech, and 3) we can train a single conversion model successfully converting both NAM and BTW to the target voice.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-583"
  },
  "saino06_interspeech": {
   "authors": [
    [
     "Keijiro",
     "Saino"
    ],
    [
     "Heiga",
     "Zen"
    ],
    [
     "Yoshihiko",
     "Nankaku"
    ],
    [
     "Akinobu",
     "Lee"
    ],
    [
     "Keiichi",
     "Tokuda"
    ]
   ],
   "title": "An HMM-based singing voice synthesis system",
   "original": "i06_2077",
   "page_count": 4,
   "order": 584,
   "p1": "paper 2077-Thu1BuP.7",
   "pn": "",
   "abstract": [
    "The present paper describes a corpus-based singing voice synthesis system based on hidden Markov models (HMMs). This system employs the HMM-based speech synthesis to synthesize singing voice. Musical information such as lyrics, tones, durations is modeled simultaneously in a unified framework of the context-dependent HMM. It can mimic the voice quality and singing style of the original singer. Results of a singing voice synthesis experiment show that the proposed system can synthesize smooth and natural-sounding singing voice.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-584"
  },
  "uto06_interspeech": {
   "authors": [
    [
     "Yosuke",
     "Uto"
    ],
    [
     "Yoshihiko",
     "Nankaku"
    ],
    [
     "Tomoki",
     "Toda"
    ],
    [
     "Akinobu",
     "Lee"
    ],
    [
     "Keiichi",
     "Tokuda"
    ]
   ],
   "title": "Voice conversion based on mixtures of factor analyzers",
   "original": "i06_2076",
   "page_count": 4,
   "order": 585,
   "p1": "paper 2076-Thu1BuP.8",
   "pn": "",
   "abstract": [
    "This paper describes the voice conversion based on the Mixtures of Factor Analyzers (MFA) which can provide an efficient modeling with a limited amount of training data. As a typical spectral conversion method, a mapping algorithm based on the Gaussian Mixture Model (GMM) has been proposed. In this method two kinds of covariance matrix structures are often used : the diagonal and full covariance matrices. GMM with diagonal covariance matrices requires a large number of mixture components for accurately estimating spectral features. On the other hand, GMM with full covariance matrices needs sufficient training data to estimate model parameters. In order to cope with these problems, we apply MFA to voice conversion. MFA can be regarded as intermediate model between GMM with diagonal covariance and with full covariance. Experimental results show that MFA can improve the conversion accuracy compared with the conventional GMM.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-585"
  },
  "tian06_interspeech": {
   "authors": [
    [
     "Jilei",
     "Tian"
    ],
    [
     "Jani",
     "Nurminen"
    ],
    [
     "Victor",
     "Popa"
    ]
   ],
   "title": "Efficient Gaussian mixture model evaluation in voice conversion",
   "original": "i06_1533",
   "page_count": 4,
   "order": 586,
   "p1": "paper 1533-Thu1BuP.9",
   "pn": "",
   "abstract": [
    "Voice conversion refers to the adaptation of the characteristics of a source speakers voice to those of a target speaker. Gaussian mixture models (GMM) have been found to be efficient in the voice conversion task. The GMM parameters are estimated from a training set with the goal to minimize the mean squared error (MSE) between the transformed and target vectors. Obviously, the quality of the GMM model plays an important role in achieving better voice conversion quality. This paper presents a very efficient approach for the evaluation of GMM models directly from the model parameters without using any test data, facilitating the improvement of the transformation performance especially in the case of embedded implementations. Though the proposed approach can be used in any application that utilizes GMM based transformation, we take voice conversion as an example application throughout the paper. The proposed approach is experimented with in this context and evaluated against an MSE based evaluation method. The results show that the proposed method is in line with all subjective observations and MSE results.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-586"
  },
  "nakano06b_interspeech": {
   "authors": [
    [
     "Yuji",
     "Nakano"
    ],
    [
     "Makoto",
     "Tachibana"
    ],
    [
     "Junichi",
     "Yamagishi"
    ],
    [
     "Takao",
     "Kobayashi"
    ]
   ],
   "title": "Constrained structural maximum a posteriori linear regression for average-voice-based speech synthesis",
   "original": "i06_1784",
   "page_count": 4,
   "order": 587,
   "p1": "paper 1784-Thu1BuP.10",
   "pn": "",
   "abstract": [
    "This paper proposes a constrained structural maximum a posteriori linear regression (CSMAPLR) algorithm for further improvement of speaker adaptation performance in HMM-based speech synthesis. In the algorithm, the concept of structural maximum a posteriori (SMAP) adaptation is applied to estimation of transformation matrices of the constrained MLLR (CMLLR), where recursive MAP-based estimation of the transformation matrices from the root node to lower nodes of context decision tree is conducted. We incorporate the algorithm into HSMM-based speech synthesis system and show that CSMAPLR adaptation utilizes both of the advantage of CMLLR and SMAPLR adaptation from the result of objective evaluation test. We also show that CSMAPLR adaptation provides more similar synthetic speech to the target speaker than CMLLR and SMAPLR adaptation from the result of subjective evaluation test.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-587"
  },
  "shuang06_interspeech": {
   "authors": [
    [
     "Zhi-Wei",
     "Shuang"
    ],
    [
     "Raimo",
     "Bakis"
    ],
    [
     "Slava",
     "Shechtman"
    ],
    [
     "Dan",
     "Chazan"
    ],
    [
     "Yong",
     "Qin"
    ]
   ],
   "title": "Frequency warping based on mapping formant parameters",
   "original": "i06_1768",
   "page_count": 4,
   "order": 588,
   "p1": "paper 1768-Thu1BuP.11",
   "pn": "",
   "abstract": [
    "We propose a novel method of generating a frequency warping function by mapping formant parameters of the source speaker and the target speaker. Alignment and selection process are performed to ensure that the mapping formants can represent speakers difference well. This approach requires only a very small amount of training data for generating the warping function, which can greatly facilitate its application. It can also achieve high quality of the converted speech while successfully converting a speakers identity. A practical voice morphing system has been built based on this approach. And experimental results show its effectiveness.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-588"
  },
  "lin06c_interspeech": {
   "authors": [
    [
     "Cheng-Yuan",
     "Lin"
    ],
    [
     "J.-S. Roger",
     "Jang"
    ]
   ],
   "title": "Automatic phonetic segmentation by using a SPM-based approach for a Mandarin singing voice corpus",
   "original": "i06_1489",
   "page_count": 4,
   "order": 589,
   "p1": "paper 1489-Thu1BuP.12",
   "pn": "",
   "abstract": [
    "This paper proposes a score predictive model (SPM) based approach to integrate two segmentation results obtained by HMM and DTW for a Mandarin singing voice corpus. The SPM can predict the score of a boundary according to its corresponding 14 dimensional feature vector. In order to verify the performance of the proposed method, several experiments were performed. The experimental results demonstrate the feasibility of the proposed approach.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-589"
  },
  "lal06_interspeech": {
   "authors": [
    [
     "Partha",
     "Lal"
    ]
   ],
   "title": "A comparison of singing evaluation algorithms",
   "original": "i06_1119",
   "page_count": 4,
   "order": 590,
   "p1": "paper 1119-Thu1BuP.13",
   "pn": "",
   "abstract": [
    "This paper describes a system that compares user renditions of short sung clips with the original version of those clips. The F0 of both recordings was estimated and then Viterbi-aligned with each other. The total difference in pitch after alignment was used as a distance metric and transformed into a rating out of ten, to indicate to the user how close he or she was to the original singer. An existing corpus of sung speech was used for initial design and optimisation of the system. We then collected further development and evaluation corpora - these recordings were judged for closeness to an original recording by two human judges. The rankings assigned by those judges were used to design and optimise the system. The design was then implemented and deployed as part of a telephone-based entertainment application.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-590"
  },
  "ng06_interspeech": {
   "authors": [
    [
     "Raymond W. M.",
     "Ng"
    ],
    [
     "Tan",
     "Lee"
    ],
    [
     "Wentao",
     "Gu"
    ]
   ],
   "title": "Towards automatic parameter extraction of command-response model for Cantonese",
   "original": "i06_1363",
   "page_count": 4,
   "order": 591,
   "p1": "paper 1363-Thu1FoP.1",
   "pn": "",
   "abstract": [
    "Command-response model is one of the quantitative models capable of illustrating the voice fundament frequency (F0) characteristics of different languages. In recent years, the model has been adapted to tone languages including Cantonese. This paper presents an automatic optimization procedure to enhance parameter extraction of command-response model for short phrases of Cantonese. We conduct a parameter extraction test on 128 speech segments produced by three speakers, all of which are modeled by single phrase command in the command-response model. The extracted parameters attained accuracy compatible with that obtained from the conventional manual estimation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-591"
  },
  "campillo06_interspeech": {
   "authors": [
    [
     "Francisco",
     "Campillo"
    ],
    [
     "Jan P. H. van",
     "Santen"
    ],
    [
     "Eduardo R.",
     "Banga"
    ]
   ],
   "title": "A model for the f0 reset in corpus-based intonation approaches",
   "original": "i06_1404",
   "page_count": 4,
   "order": 592,
   "p1": "paper 1404-Thu1FoP.2",
   "pn": "",
   "abstract": [
    "Concatenative intonation systems model the intonation contours as the concatenation of small natural units extracted from suitable contexts. The special characteristics of this type of models make it difficult to include some factors whose effect overcomes the phonic group domain. In this paper one of those factors, the f0 reset related to the sentence internal pauses, is addressed.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-592"
  },
  "bailly06_interspeech": {
   "authors": [
    [
     "Gérard",
     "Bailly"
    ],
    [
     "Jan",
     "Gorisch"
    ]
   ],
   "title": "Generating German intonation with a trainable prosodic model",
   "original": "i06_2017",
   "page_count": 4,
   "order": 593,
   "p1": "paper 2017-Thu1FoP.3",
   "pn": "",
   "abstract": [
    "A trainable prosodic model called SFC (Superposition of Functional Contours), proposed by Holm and Bailly, is here confronted to German intonation. Training material is the publicly available Siemens Synthesis Corpus that provides spoken utterances for high-quality speech synthesis. We describe the labeling framework and first evaluation results that compares the original prosody of test sentences of this corpus with their prosodic rendering by the proposed model and state-of-the-art systems available on-line on the web.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-593"
  },
  "kim06f_interspeech": {
   "authors": [
    [
     "Seungwon",
     "Kim"
    ],
    [
     "Jinsik",
     "Lee"
    ],
    [
     "Byeongchang",
     "Kim"
    ],
    [
     "Gary Geunbae",
     "Lee"
    ]
   ],
   "title": "Incorporating second-order information into two-step major phrase break prediction for Korean",
   "original": "i06_1487",
   "page_count": 4,
   "order": 594,
   "p1": "paper 1487-Thu1FoP.4",
   "pn": "",
   "abstract": [
    "In this paper, we present a new phrase break prediction method that integrates second-order information into general maximum entropy model. The phrase break prediction problem was mapped into a classification problem in our research. The features we used for the prediction of phrase breaks are of several layers such as local features (part-of-speech (POS) tags, a lexicon, lengths of eojeols and location of juncture in the sentence), global features (chunk label derived from a eojeol parse tree) and second-order features (distance probability of previous and next phrase break). These three features were combined and used in the experiments, and we were able to generate good performance especially in the major phrase break prediction.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-594"
  },
  "yi06c_interspeech": {
   "authors": [
    [
     "Lifu",
     "Yi"
    ],
    [
     "Jian",
     "Li"
    ],
    [
     "Xiaoyan",
     "Lou"
    ],
    [
     "Jie",
     "Hao"
    ]
   ],
   "title": "Totally data-driven duration modeling based on generalized linear model for Mandarin TTS",
   "original": "i06_1837",
   "page_count": 4,
   "order": 595,
   "p1": "paper 1837-Thu1FoP.5",
   "pn": "",
   "abstract": [
    "This paper proposes a totally data-driven duration modeling method for Mandarin TTS, which uses Generalized Linear Models (GLM) to model duration and stepwise regression to automatically select the attribute set with statistical measurements. This method can get a good tradeoff between model complexity and goodness of fit. Besides, speaking rate is introduced as a new modeling attribute, which not only achieves higher performance but also provides a novel approach to adjust speaking rate when synthesizing. We also propose to use R2 to fairly evaluate the modeling performances on different databases, since R2 refers to the fraction of corresponding variance explained by a model. Experiments show the performance of GLM is significantly higher than that of CART. With our much smaller models and corpus, the proposed method also achieves comparable results reported by other excellent researches.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-595"
  },
  "ozturk06_interspeech": {
   "authors": [
    [
     "Özlem",
     "Özturk"
    ],
    [
     "Tolga",
     "Ciloglu"
    ]
   ],
   "title": "Segmental duration modeling in Turkish",
   "original": "i06_2004",
   "page_count": 4,
   "order": 596,
   "p1": "paper 2004-Thu1FoP.6",
   "pn": "",
   "abstract": [
    "Naturalness of synthetic speech highly depends on appropriate modeling of prosodic aspects. Mostly, three prosody components are modeled: segmental duration, pitch contour and intensity. In this study, we present our work on modeling segmental duration in Turkish using machine-learning algorithms, especially Classification and Regression Trees (CART). The models predict phone durations based on attributes such as phone identity, neighboring phone identities, lexical stress, position of syllable in word, part-of-speech (POS) information, word length in number of syllables and position of word in utterance extracted from a speech corpus of approximately 700 sentences. Obtained models predict segment durations better than mean duration approximations (¡«0.77 Correlation Coefficient, CC, and 20.4 ms Root-Mean Squared Error, RMSE). Attributes phone identity, neighboring phone identities, lexical stress, syllable type, POS, phrase break information, and location of word in the phrase constitute best predictor set for phoneme duration modeling.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-596"
  },
  "dalen06_interspeech": {
   "authors": [
    [
     "Rogier C. van",
     "Dalen"
    ],
    [
     "Pascal",
     "Wiggers"
    ],
    [
     "Léon J. M.",
     "Rothkrantz"
    ]
   ],
   "title": "Lexical stress in continuous speech recognition",
   "original": "i06_1578",
   "page_count": 4,
   "order": 597,
   "p1": "paper 1578-Thu1FoP.7",
   "pn": "",
   "abstract": [
    "Human listeners use lexical stress for word segmentation and disambiguation. We look into using lexical stress for large-vocabulary speech recognition for the Dutch language. It appears that beside vowels, consonants should be taken into account. By introducing stressed phonemes, and features for spectral bands and the fundamental frequency, we reduce the word error rate by 2.6%.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-597"
  },
  "wang06g_interspeech": {
   "authors": [
    [
     "Siwei",
     "Wang"
    ],
    [
     "Gina-Anne",
     "Levow"
    ]
   ],
   "title": "Improving tone recognition with combined frequency and amplitude modelling",
   "original": "i06_1651",
   "page_count": 4,
   "order": 598,
   "p1": "paper 1651-Thu1FoP.8",
   "pn": "",
   "abstract": [
    "To improve tone recognition in continuous speech, we propose a strategy focusing on separating regions influenced by tonal coarticulation from regions that more closely approximate canonical tone production. Given a syllable segmentation, this approach employs amplitude and pitch information to generate an improved sub-syllable segmentation and feature representation. This sub-syllable segmentation is derived from the convex hull of the amplitude-pitch plot. Our approach achieves a 15% improvement using our segmentation strategy over a simple time-only segmentation. Finally, a future extension with sequential labelling is discussed.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-598"
  },
  "lin06d_interspeech": {
   "authors": [
    [
     "Che-Kuang",
     "Lin"
    ],
    [
     "Lin-shan",
     "Lee"
    ]
   ],
   "title": "Latent prosodic modeling (LPM) for speech with applications in recognizing spontaneous Mandarin speech with disfluencies",
   "original": "i06_1901",
   "page_count": 4,
   "order": 599,
   "p1": "paper 1901-Thu1FoP.9",
   "pn": "",
   "abstract": [
    "In this paper, a new approach of Latent Prosodic Modeling (LPM) for analyzing the prosody of speech is presented. Based on a set of newly defined prosodic characters, prosodic terms, documents, and the Probabilistic Latent Semantic Analysis (PLSA) framework, prosody can be modeled using a set of prosodic states representing various latent factors such as speakers, speaking rate, utterance modality, intonation behavior, etc. in terms of some probabilistic relationships with the observed prosodic features. Organizing the training data based on this new model may also produce more delicate classification models for various speech processing applications considering the prosody. In the initial application example, we showed the use of this model on the task of disfluency IP detection for spontaneous Mandarin speech recognition, and improved IP detection accuracy and speech recognition performance were obtained in the experiments.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-599"
  },
  "hirose06b_interspeech": {
   "authors": [
    [
     "Keikichi",
     "Hirose"
    ],
    [
     "Hui",
     "Hu"
    ],
    [
     "Xiaodong",
     "Wang"
    ],
    [
     "Nobuaki",
     "Minematsu"
    ]
   ],
   "title": "Tone recognition of continuous speech of standard Chinese using neural network and tone nucleus model",
   "original": "i06_1929",
   "page_count": 4,
   "order": 600,
   "p1": "paper 1929-Thu1FoP.10",
   "pn": "",
   "abstract": [
    "A method is developed for recognizing lexical tone types of Standard Chinese syllables in continuous speech. Neural network (four-layered perceptron) is adopted as classifier. The method includes two steps; first recognizing tone types using prosodic features of voiced part, and then re-recognizing by viewing only on tone nucleus, which is a portion of the syllable showing rather stable fundamental frequency (F0) contour regardless of tone types of the preceding and following syllables. The voiced part (or tone nucleus) is divided into 20 segments, and F0, delta-F0, F0 slope and short-term energy of each segment are served as inputs to the neural network. In order to cope with tone coarticulation, prosodic feature parameters for the last 5 segments of the preceding syllable and the initial 5 segments of the following syllable are included in the neural network inputs. Information on syllable length is also added to the inputs. Tone recognition experiment was conducted for a female speakers utterances included in HKU96 corpus. The average recognition rate was 86.5% including neutral tone syllables, when the tone nucleus model was not used. It increased to 86.9%, when the model was used. The obtained rate is higher by more than 3 points as compared to that obtained by the hidden-Markov-model-based tone recognizer developed by the authors formerly.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-600"
  },
  "solorio06_interspeech": {
   "authors": [
    [
     "Thamar",
     "Solorio"
    ],
    [
     "Olac",
     "Fuentes"
    ],
    [
     "Nigel G.",
     "Ward"
    ],
    [
     "Yaffa Al",
     "Bayyari"
    ]
   ],
   "title": "Prosodic feature generation for back-channel prediction",
   "original": "i06_1724",
   "page_count": 4,
   "order": 601,
   "p1": "paper 1724-Thu1FoP.11",
   "pn": "",
   "abstract": [
    "Using prosodic information to predict when back-channels are appropriate in spontaneous dialogs has become somewhat of a reference problem for automatic discovery techniques. Here we present experiments with two ideas: the use of features derived from randomly generated pitch and energy filters, and the use of instance-based learning, specifically the Locally Weighted Linear Regression (LWLR) algorithm. For the task of predicting possible back-channel locations in Iraqi Arabic [6], we obtain 22% precision and 51% recall, which is as good as that obtained using a laboriously developed and hand-tuned rule.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-601"
  },
  "wesseling06_interspeech": {
   "authors": [
    [
     "Wieneke",
     "Wesseling"
    ],
    [
     "Rob J. J. H. van",
     "Son"
    ],
    [
     "Louis C. W.",
     "Pols"
    ]
   ],
   "title": "On the sufficiency and redundancy of pitch for TRP projection",
   "original": "i06_1972",
   "page_count": 4,
   "order": 602,
   "p1": "paper 1972-Thu1FoP.12",
   "pn": "",
   "abstract": [
    "In two Reaction Times (RT) experiments, subjects were asked to respond with minimal responses to prerecorded dialogs and impoverished versions of these dialogs, containing either only intonation and pause information, hummed stimuli, or no periodic component at all, whispered stimuli. For the hummed, stimuli, response delays and, especially, variances were higher than the original recordings. Responses to mid-frequency pitch utterance-ends were significantly longer than responses to low pitch utterance-ends, suggesting that our subjects fell back to reacting to pauses when presented with hummed utterances ending in a mid-frequency tone. This suggests that, in contrast to low or high end-tones, intonation contours that end in a mid-frequency tone might not contain any useful information for predicting end-of-utterance Turn Relevant Places (TRPs). We conclude that just the intonation and pauses of a conversation contain sufficient information for projection of TRPs. However this information is measurably impoverished with respect to original to an extent that increases the \"processing\" time by 10%. No difference was found between whispered and original speech. This lack of any effect of removing all periodic sound components from the speech signal indicates that in natural speech the pitch signal itself might be redundant for predicting TRPs.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-602"
  },
  "gibson06_interspeech": {
   "authors": [
    [
     "Matthew",
     "Gibson"
    ],
    [
     "Thomas",
     "Hain"
    ]
   ],
   "title": "Hypothesis spaces for minimum Bayes risk training in large vocabulary speech recognition",
   "original": "i06_1653",
   "page_count": 4,
   "order": 603,
   "p1": "paper 1653-Thu2A1O.1",
   "pn": "",
   "abstract": [
    "The Minimum Bayes Risk (MBR) framework has been a successful strategy for the training of hidden Markov models for large vocabulary speech recognition. Practical implementations of MBR must select an appropriate hypothesis space and loss function. The set of word sequences and a word-based Levenshtein distance may be assumed to be the optimal choice but use of phoneme-based criteria appears to be more successful. This paper compares the use of different hypothesis spaces and loss functions defined using the system constituents of word, phone, physical triphone, physical state and physical mixture component. For practical reasons the competing hypotheses are constrained by sampling. The impact of the sampling technique on the performance of MBR training is also examined.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-603"
  },
  "du06_interspeech": {
   "authors": [
    [
     "Jun",
     "Du"
    ],
    [
     "Peng",
     "Liu"
    ],
    [
     "Frank K.",
     "Soong"
    ],
    [
     "Jian-Lai",
     "Zhou"
    ],
    [
     "Ren-Hua",
     "Wang"
    ]
   ],
   "title": "Minimum divergence based discriminative training",
   "original": "i06_1703",
   "page_count": 4,
   "order": 604,
   "p1": "paper 1703-Thu2A1O.2",
   "pn": "",
   "abstract": [
    "We propose to use Minimum Divergence (MD) as a new measure of errors in discriminative training. To focus on improving discrimination between any two given acoustic models, we refine the error definition in terms of Kullback-Leibler Divergence (KLD) between them. The new measure can be regarded as a modified version of Minimum Phone Error (MPE) but with a higher resolution than just a symbol matching based criterion. Experimental recognition results show the new MD based training yields relative word error rate reductions of 57.8% and 6.1% on TIDigits and Switchboard databases, respectively, in comparing with the ML trained baseline systems. The recognition performance of MD is also shown to be consistently better than that of MPE.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-604"
  },
  "li06i_interspeech": {
   "authors": [
    [
     "Xinwei",
     "Li"
    ],
    [
     "Hui",
     "Jiang"
    ]
   ],
   "title": "Solving large margin estimation of HMMS via semidefinite programming",
   "original": "i06_1064",
   "page_count": 4,
   "order": 605,
   "p1": "paper 1064-Thu2A1O.3",
   "pn": "",
   "abstract": [
    "In this paper, we propose to use a new optimization method, i.e., semidefinite programming (SDP), to solve large margin estimation (LME) problem of continuous density hidden Markov models (CDHMM) for speech recognition. First of all, we introduce a new constraint into the LME to guarantee the boundedness of the margin of CDHMM. Secondly, we show that the LME problem under this new constraint can be formulated as an SDP problem under some relaxation conditions and it can be solved very efficiently by using some fast optimization algorithms specially designed for SDP. The new method is evaluated in a continuous digit string recognition task by using the TIDIGITS database. Experimental results clearly demonstrate that the new SDP-based method outperforms the previously proposed optimization methods using gradient descent search in both recognition accuracy and convergence speed. With the SDP-based optimization method, the best LME models achieves 0.53% in string error rate and 0.18% in WER on the TIDIGITS task. To our best knowledge, this is the best result ever reported in this task.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-605"
  },
  "yu06b_interspeech": {
   "authors": [
    [
     "Dong",
     "Yu"
    ],
    [
     "Li",
     "Deng"
    ],
    [
     "Xiaodong",
     "He"
    ],
    [
     "Alex",
     "Acero"
    ]
   ],
   "title": "Use of incrementally regulated discriminative margins in MCE training for speech recognition",
   "original": "i06_1410",
   "page_count": 4,
   "order": 606,
   "p1": "paper 1410-Thu2A1O.4",
   "pn": "",
   "abstract": [
    "In this paper, we report our recent development of a novel discriminative learning technique which embeds the concept of discriminative margin into the well established minimum classification error (MCE) method. The idea is to impose an incrementally adjusted \"margin\" in the loss function of MCE algorithm so that not only error rates are minimized but also discrimination \"robustness\" between training and test sets is maintained. Experimental evaluation shows that the use of the margin improves a state-of-the-art MCE method by reducing 17% digit errors and 19% string errors in the TIDigits recognition task. The string error rate of 0.55% and digit error rate of 0.19% we have obtained are the best-ever results reported on this task in the literature.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-606"
  },
  "li06j_interspeech": {
   "authors": [
    [
     "Jinyu",
     "Li"
    ],
    [
     "Ming",
     "Yuan"
    ],
    [
     "Chin-Hui",
     "Lee"
    ]
   ],
   "title": "Soft margin estimation of hidden Markov model parameters",
   "original": "i06_1316",
   "page_count": 4,
   "order": 607,
   "p1": "paper 1316-Thu2A1O.5",
   "pn": "",
   "abstract": [
    "We propose a new discriminative learning framework, called soft margin estimation (SME), for estimating parameters of continuous density hidden Markov models. The proposed method makes direct usage of the successful ideas of soft margin in support vector machines to improve generalization capability, and of decision feedback learning in minimum classification error training to enhance model separation in classifier design. We attempt to incorporate frame selection, utterance selection and discriminative separation in a single unified objective function that can be optimized with the well-known generalized probabilistic descent algorithm. We demonstrate the advantage of SME in theory and practice over other state-of-the-art techniques. Tested on a connected digit recognition task, the proposed SME approach achieves a string accuracy of 99.33%. To our knowledge, this is the best result ever reported on the TIDIGITS database.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-607"
  },
  "wang06h_interspeech": {
   "authors": [
    [
     "Ye-Yi",
     "Wang"
    ],
    [
     "Alex",
     "Acero"
    ]
   ],
   "title": "Discriminative models for spoken language understanding",
   "original": "i06_1766",
   "page_count": 4,
   "order": 608,
   "p1": "paper 1766-Thu2A1O.6",
   "pn": "",
   "abstract": [
    "This paper studies several discriminative models for spoken language understanding (SLU). While all of them fall into the conditional model framework, different optimization criteria lead to conditional random fields, perceptron, minimum classification error and large margin models. The paper discusses the relationship amongst these models and compares them in terms of accuracy, training speed and robustness.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-608"
  },
  "gibert06_interspeech": {
   "authors": [
    [
     "G.",
     "Gibert"
    ],
    [
     "Gérard",
     "Bailly"
    ],
    [
     "F.",
     "Elisei"
    ]
   ],
   "title": "Evaluating a virtual speech cuer",
   "original": "i06_1539",
   "page_count": 4,
   "order": 609,
   "p1": "paper 1539-Thu2A3O.1",
   "pn": "",
   "abstract": [
    "This paper presents the virtual speech cuer built in the context of the ARTUS project aiming at watermarking hand and face gestures of a virtual animated agent in a broadcasted audiovisual sequence. For deaf televiewers that master cued speech, the animated agent can be then superimposed - on demand and at the reception - on the original broadcast as an alternative to subtitling. The paper presents the multimodal text-to-speech synthesis system and the first evaluation performed by deaf users.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-609"
  },
  "tomokiyo06_interspeech": {
   "authors": [
    [
     "Laura Mayfield",
     "Tomokiyo"
    ],
    [
     "Kay",
     "Peterson"
    ],
    [
     "Alan W.",
     "Black"
    ],
    [
     "Kevin A.",
     "Lenzo"
    ]
   ],
   "title": "Intelligibility of machine translation output in speech synthesis",
   "original": "i06_1268",
   "page_count": 4,
   "order": 610,
   "p1": "paper 1268-Thu2A3O.2",
   "pn": "",
   "abstract": [
    "One use of text-to-speech synthesis (TTS) is as a component of speechto- speech translation systems. The output of automatic machine translation (MT) can vary widely in quality, however. A synthetic voice that is extremely intelligible on naturally-occurring text may be far less intelligible when asked to render text that is automatically generated. In this paper, we compare the quality of synthesis of naturally-occurring text and its MT counterpart. We find that intelligibility of TTS on MT output is significantly lower than on either naturally-occurring text or semantically unpredictable sentences, and explore the reasons why.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-610"
  },
  "tachibana06_interspeech": {
   "authors": [
    [
     "Makoto",
     "Tachibana"
    ],
    [
     "Takashi",
     "Nose"
    ],
    [
     "Junichi",
     "Yamagishi"
    ],
    [
     "Takao",
     "Kobayashi"
    ]
   ],
   "title": "A technique for controlling voice quality of synthetic speech using multiple regression HSMM",
   "original": "i06_1778",
   "page_count": 4,
   "order": 611,
   "p1": "paper 1778-Thu2A3O.3",
   "pn": "",
   "abstract": [
    "This paper describes a technique for controlling voice quality of synthetic speech using multiple regression hidden semi-Markov model (HSMM). In the technique, we assume that the mean vectors of output and state duration distribution of HSMM are modeled by multiple regression with a parameter vector called voice quality control vector. We first choose three features for controlling voice qualities, that is, \"smooth voice - nonsmooth voice,\" \"warm - cold,\" \"high-pitched - low-pitched,\" and then we attempt to control voice quality of synthetic speech for these features. From the results of several subjective tests, we show that the proposed technique can change these features of voice quality intuitively.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-611"
  },
  "polyakova06_interspeech": {
   "authors": [
    [
     "Tatyana",
     "Polyakova"
    ],
    [
     "Antonio",
     "Bonafonte"
    ]
   ],
   "title": "Learning from errors in grapheme-to-phoneme conversion",
   "original": "i06_1742",
   "page_count": 4,
   "order": 612,
   "p1": "paper 1742-Thu2A3O.4",
   "pn": "",
   "abstract": [
    "In speech technology it is very important to have a system capable of accurately performing grapheme-to-phoneme (G2P) conversion, which is not an easy task especially if talking about languages like English where there is no obvious letter-phone correspondence. Manual rules so widely used before are now leaving the way open for the machine learning techniques and language independent tools. In this paper we present an extension of the use of transformationbased error-driven algorithm to G2P task. A set of explicit rules was inferred to correct the pronunciation for U.S. English, Spanish and Catalan using well-known machine-learning techniques in combination with transformation based algorithm. All methods applied in combination with transformation rules significantly outperform the results obtained by these methods alone.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-612"
  },
  "toda06_interspeech": {
   "authors": [
    [
     "Tomoki",
     "Toda"
    ],
    [
     "Yamato",
     "Ohtani"
    ],
    [
     "Kiyohiro",
     "Shikano"
    ]
   ],
   "title": "Eigenvoice conversion based on Gaussian mixture model",
   "original": "i06_1717",
   "page_count": 4,
   "order": 613,
   "p1": "paper 1717-Thu2A3O.5",
   "pn": "",
   "abstract": [
    "This paper describes a novel framework of voice conversion (VC). We call it eigenvoice conversion (EVC). We apply EVC to the conversion from a source speakers voice to arbitrary target speakers voices. Using multiple parallel data sets consisting of utterance-pairs of the source and multiple pre-stored target speakers, a canonical eigenvoice GMM (EV-GMM) is trained in advance. That conversion model enables us to flexibly control the speaker individuality of the converted speech by manually setting weight parameters. In addition, the optimum weight set for a specific target speaker is estimated using only speech data of the target speaker without any linguistic restrictions. We evaluate the performance of EVC by a spectral distortion measure. Experimental results demonstrate that EVC works very well even if we use only a few utterances of the target speaker for the weight estimation.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-613"
  },
  "langner06_interspeech": {
   "authors": [
    [
     "Brian",
     "Langner"
    ],
    [
     "Rohit",
     "Kumar"
    ],
    [
     "Arthur",
     "Chan"
    ],
    [
     "Lingyun",
     "Gu"
    ],
    [
     "Alan W.",
     "Black"
    ]
   ],
   "title": "Generating time-constrained audio presentations of structured information",
   "original": "i06_2075",
   "page_count": 4,
   "order": 614,
   "p1": "paper 2075-Thu2A3O.6",
   "pn": "",
   "abstract": [
    "Presenting complex information in an understandable manner using speech is a challenging task to do well. Significant limitations, both in the generation process and from the human listeners capabilities, typically make for poorly understood speech. This work examines possible strategies for producing understandable spoken complex information working within those limitations, as well as identifying ways to improve systems to reduce the limitations impact. We discuss a simple user study that explores these strategies with complex structured information, and describe a spoken dialog system that will make use of this work to provide a speech interface to structured information in a more understandable manner.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-614"
  },
  "alsaade06_interspeech": {
   "authors": [
    [
     "F.",
     "Alsaade"
    ],
    [
     "A.",
     "Ariyaeeinia"
    ],
    [
     "L.",
     "Meng"
    ],
    [
     "A.",
     "Malegaonkar"
    ]
   ],
   "title": "Multimodal authentication using qualitative support vector machines",
   "original": "i06_1364",
   "page_count": 4,
   "order": 615,
   "p1": "paper 1364-Thu2WeO.1",
   "pn": "",
   "abstract": [
    "This paper proposes an approach to enhancing the accuracy of multimodal biometrics in uncontrolled environments. Variation in operating conditions results in mismatch between the training and test material, and thereby affects the biometric authentication performance regardless of this being unimodal or multimodal. The paper proposes a technique to reduce the effects of such variations in multimodal fusion. The proposed technique is based on estimating the quality aspect of the test scores and then passing these aspects into the Support Vector Machine either as features or weights. Since the fusion process is based on the learning classifier of Support Vector Machine, the technique is termed Support Vector Machine with Quality Measurement (SVM-QM). The experimental investigation is conducted using face and speech modalities. The results clearly show the benefits gained from learning the quality aspects of the biometric data used for authentication.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-615"
  },
  "pitsikalis06_interspeech": {
   "authors": [
    [
     "Vassilis",
     "Pitsikalis"
    ],
    [
     "Athanassios",
     "Katsamanis"
    ],
    [
     "George",
     "Papandreou"
    ],
    [
     "Petros",
     "Maragos"
    ]
   ],
   "title": "Adaptive multimodal fusion by uncertainty compensation",
   "original": "i06_1950",
   "page_count": 4,
   "order": 616,
   "p1": "paper 1950-Thu2WeO.2",
   "pn": "",
   "abstract": [
    "While the accuracy of feature measurements heavily depends on changing environmental conditions, studying the consequences of this fact in pattern recognition tasks has received relatively little attention to date. In this work we explicitly take into account feature measurement uncertainty and we show how classification rules should be adjusted to compensate for its effects. Our approach is particularly fruitful in multimodal fusion scenarios, such as audio-visual speech recognition, where multiple streams of complementary time-evolving features are integrated. For such applications, provided that the measurement noise uncertainty for each feature stream can be estimated, the proposed framework leads to highly adaptive multimodal fusion rules which are widely applicable and easy to implement. We further show that previous multimodal fusion methods relying on stream weights fall under our scheme under certain assumptions; this provides novel insights into their applicability for various tasks and suggests new practical ways for estimating the stream weights adaptively. The potential of our approach is demonstrated in audio-visual speech recognition using either synchronous or asynchronous models.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-616"
  },
  "hardison06_interspeech": {
   "authors": [
    [
     "Debra M.",
     "Hardison"
    ]
   ],
   "title": "Effects of familiarity with faces and voices on second-language speech processing: components of memory traces",
   "original": "i06_1097",
   "page_count": 4,
   "order": 617,
   "p1": "paper 1097-Thu2WeO.3",
   "pn": "",
   "abstract": [
    "Familiarity with a talkers voice and face was found to facilitate processing of second-language speech. This advantage is accentuated when visual cues are limited to either the mouth and jaw area, or eyes and upper cheek areas of a talkers face. Findings are compatible with a multiple-trace model of bimodal speech processing.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-617"
  },
  "tamura06_interspeech": {
   "authors": [
    [
     "Satoshi",
     "Tamura"
    ],
    [
     "Koji",
     "Hashimoto"
    ],
    [
     "Jiong",
     "Zhu"
    ],
    [
     "Satoru",
     "Hayamizu"
    ],
    [
     "Hirotsugu",
     "Asai"
    ],
    [
     "Hideki",
     "Tanahashi"
    ],
    [
     "Makoto",
     "Kanagawa"
    ]
   ],
   "title": "Automatic metadata generation and video editing based on speech and image recognition for medical education contents",
   "original": "i06_1132",
   "page_count": 4,
   "order": 618,
   "p1": "paper 1132-Thu2WeO.4",
   "pn": "",
   "abstract": [
    "This paper reports a metadata generation system as well as an automatic video edit system. The metadata are information described about the other data. In the audio metadata generation system, speech recognition using general language model (LM) and specialized LM is performed to input speech in order to obtain segment (event group) and audio metadata (event information) respectively. In the video edit system, visual metadata obtained by image recognition and audio metadata are combined into audio-visual metadata. Subsequently, multiple videos are edited to one video using the audio-visual metadata. Experiments were conducted to evaluate event detection of the systems using medical education contents, ACLS and BLS. The audio metadata system achieved about a 78% event detection correctness. In the edit system, an 87% event correctness was obtained by audio-visual metadata, and the survey proved that the edited video is appropriate and useful.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-618"
  },
  "almajai06_interspeech": {
   "authors": [
    [
     "Ibrahim",
     "Almajai"
    ],
    [
     "Ben",
     "Milner"
    ],
    [
     "Jonathan",
     "Darch"
    ]
   ],
   "title": "Analysis of correlation between audio and visual speech features for clean audio feature prediction in noise",
   "original": "i06_1634",
   "page_count": 4,
   "order": 619,
   "p1": "paper 1634-Thu2WeO.5",
   "pn": "",
   "abstract": [
    "The aim of this work is to examine the correlation between audio and visual speech features. The motivation is to find visual features that can provide clean audio feature estimates which can be used for speech enhancement when the original audio signal is corrupted by noise. Two audio features (MFCCs and formants) and three visual features (active appearance model, 2-D DCT and cross-DCT) are considered with correlation measured using multiple linear regression. The correlation is then exploited through the development of a maximum a posteriori (MAP) prediction of audio features solely from the visual features. Experiments reveal that features representing broad spectral information have higher correlation to visual features than those representing finer spectral detail. The accuracy of prediction follows the results found in the correlation measurements.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-619"
  },
  "govokhina06_interspeech": {
   "authors": [
    [
     "Oxana",
     "Govokhina"
    ],
    [
     "Gérard",
     "Bailly"
    ],
    [
     "Gaspard",
     "Breton"
    ],
    [
     "Paul",
     "Bagshaw"
    ]
   ],
   "title": "TDA: a new trainable trajectory formation system for facial animation",
   "original": "i06_1274",
   "page_count": 4,
   "order": 620,
   "p1": "paper 1274-Thu2WeO.6",
   "pn": "",
   "abstract": [
    "A new trainable trajectory formation system - named TDA - for facial animation is here proposed that dissociates parametric spaces and methods for movement planning and execution. Movement planning is achieved by HMM-based trajectory formation. This module essentially plans configurations of lip geometry (aperture, spreading and protrusion). Movement execution is performed by concatenation of multi-represented diphones. This module is responsible for selecting and concatenating detailed facial movements that best obey to the target kinematics of the geometry previously planned. Movement planning ensures that the essential visual characteristics of visemes are reached (lip closing for bilabials, rounding and opening for palatal fricatives, etc) and that appropriate coarticulation is planned. Movement execution grafts phonetic details and idiosyncratic articulatory strategies (dissymetries, importance of jaw movements, etc) to the planned gestural score. This planning scheme is compared to alternative planning strategies using articulatory modeling and motion capture data.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-620"
  },
  "biagetti06_interspeech": {
   "authors": [
    [
     "Giorgio",
     "Biagetti"
    ],
    [
     "Paolo",
     "Crippa"
    ],
    [
     "Claudio",
     "Turchetti"
    ]
   ],
   "title": "Modeling of speech signals based on Bessel-like orthogonal transform",
   "original": "i06_1054",
   "page_count": 4,
   "order": 621,
   "p1": "paper 1054-Thu2BuP.1",
   "pn": "",
   "abstract": [
    "In this paper a novel modeling technique for speech signals, based on the source-filter model of speech production and on orthogonal transform theory, is presented. The proposed approach models the impulse response of such filter, by projection onto a basis of damped Bessel functions, which have been chosen for their similarity to the signal to be modeled. In such a way an orthogonal transform pair is defined which provides a simple and effective methodology for the extraction of model parameters, and its effectiveness in the case of voiced speech has been demonstrated by synthesizing natural sounding speech signals with the aid of only a few extracted parameters.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-621"
  },
  "jinachitra06_interspeech": {
   "authors": [
    [
     "Pamornpol",
     "Jinachitra"
    ]
   ],
   "title": "Glottal closure and opening detection for flexible parametric voice coding",
   "original": "i06_1359",
   "page_count": 4,
   "order": 622,
   "p1": "paper 1359-Thu2BuP.2",
   "pn": "",
   "abstract": [
    "The knowledge of glottal closure and opening instants (GCI/GOI) is useful for many speech analysis applications. A Pitch-synchronous waveform encoding of voice is one such application. In this paper, a dynamic programming is employed to solve for the global close/open phase segmentation based on the polynomial parametric waveform of the derivative glottal waveform and its quasi-periodicity. Not only does the algorithm identify GCIs, but also the elusive GOIs, and as a by-product, the parameters of the glottal excitation waveform. The results show its effectiveness compared with a classical method. Its application to parametric voice encoding which allows for simple time-pitch scaling as well as voicing quality conversion is demonstrated.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-622"
  },
  "trmal06_interspeech": {
   "authors": [
    [
     "Jan",
     "Trmal"
    ],
    [
     "Jan",
     "Vanek"
    ],
    [
     "Ludek",
     "Müller"
    ],
    [
     "Jan",
     "Zelinka"
    ]
   ],
   "title": "Independent components for acoustic modeling",
   "original": "i06_1526",
   "page_count": 4,
   "order": 623,
   "p1": "paper 1526-Thu2BuP.3",
   "pn": "",
   "abstract": [
    "In the paper, we present a comparative study of several methods used nowadays in the field of feature and information extraction. We compared several Independent Component Analysis (ICA) algorithms together with the commonly used Principal Component Analysis (PCA) algorithm in two real-world tasks. The first task was a Voice Activity Detection (VAD), the second is Speaker Verification and Recognition (SVR). The VAD system as well as the SVR system benefited from the ICA decompositions. Moreover, a brief comparison of the information extraction ability is described.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-623"
  },
  "mehta06_interspeech": {
   "authors": [
    [
     "Daryush",
     "Mehta"
    ],
    [
     "Thomas F.",
     "Quatieri"
    ]
   ],
   "title": "Pitch-scale modification using the modulated aspiration noise source",
   "original": "i06_1542",
   "page_count": 4,
   "order": 624,
   "p1": "paper 1542-Thu2BuP.4",
   "pn": "",
   "abstract": [
    "Spectral harmonic/noise component analysis of spoken vowels shows evidence of noise modulations with peaks in the estimated noise source component synchronous with both the open phase of the periodic source and with time instants of glottal closure. Inspired by this observation of natural modulations and of fullband energy in the aspiration noise source, we develop an alternate approach to high-quality pitch-scale modification of continuous speech. Our strategy takes a dual processing approach, in which the harmonic and noise components of the speech signal are separately analyzed, modified, and re-synthesized. The periodic component is modified using standard modification techniques, and the noise component is handled by modifying characteristics of its source waveform. Since we have modeled an inherent coupling between the periodic and aspiration noise sources, the modification algorithm is designed to preserve the synchrony between temporal modulations of the two sources. The reconstructed modified signal is perceived in informal listening to be natural-sounding and typically reduces artifacts that occur in standard modification techniques.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-624"
  },
  "ezzat06_interspeech": {
   "authors": [
    [
     "Tony",
     "Ezzat"
    ],
    [
     "Jake",
     "Bouvrie"
    ],
    [
     "Tomaso",
     "Poggio"
    ]
   ],
   "title": "Max-Gabor analysis and synthesis of spectrograms",
   "original": "i06_1561",
   "page_count": 4,
   "order": 625,
   "p1": "paper 1561-Thu2BuP.5",
   "pn": "",
   "abstract": [
    "We present a method that analyzes a two-dimensional magnitude spectrogram S(f, t) into its local constituent spectro-temporal amplitudes A(f, t), frequencies F(f, t), orientations ¦(f , t), and phases Ó(f, t). The method operates by performing a two-dimensional local Gabor-like analysis of the spectrogram, retaining only the parameters of the 2D-Gabor filter with maximal amplitude response within the local region. We demonstrate the technique over a wide variety of speakers, and show how the spectrograms in each case may be adequately reconstructed using the parameters of the Max-Gabor analysis. Finally, we discuss the nature of the extracted Max-Gabor parameters.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-625"
  },
  "quintanamorales06_interspeech": {
   "authors": [
    [
     "Pedro J.",
     "Quintana-Morales"
    ],
    [
     "Juan L.",
     "Navarro-Mesa"
    ],
    [
     "Antonio G.",
     "Ravelo-Garcia"
    ],
    [
     "Fernando D.",
     "Lorenzo-Garcia"
    ]
   ],
   "title": "Monitoring of the natural voice variations in open and closed phases with frequency warped ARMA modeling",
   "original": "i06_1572",
   "page_count": 4,
   "order": 626,
   "p1": "paper 1572-Thu2BuP.6",
   "pn": "",
   "abstract": [
    "The objective of this paper is to propose the use of a speech model with psychoacoustical information to distinguish between the open and closed phases of the vocal folds, in order to monitor formants as phonetic speech characteristics. Taking a frequency warped ARMA model for each one of the phases, the aim is to integrate the information of various consecutive periods in which the poles and the zeros that form the vocal tract can be considered common due to its slow variation. To analyze the capacity of phonetic monitoring, different phonetic voice transitions of speech registers are used from a database that provides information on glottal closings. First, we show the dependency with the warping factor. Then, the consistency and reliability of the method is demonstrated, as well as its better performance compared to no warped ones. And finally, we will see the well behavior against moderate noise.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-626"
  },
  "kameoka06_interspeech": {
   "authors": [
    [
     "Hirokazu",
     "Kameoka"
    ],
    [
     "Jonathan Le",
     "Roux"
    ],
    [
     "Nobutaka",
     "Ono"
    ],
    [
     "Shigeki",
     "Sagayama"
    ]
   ],
   "title": "Speech analyzer using a joint estimation model of spectral envelope and fine structure",
   "original": "i06_1641",
   "page_count": 4,
   "order": 627,
   "p1": "paper 1641-Thu2BuP.7",
   "pn": "",
   "abstract": [
    "We have been working on a new speech analyzer based on a parametric representation of speech governed by the F0 parameter, towards practical human-machine interfaces. As a precise estimation of the frequency response of the vocal tract from a real speech signal requires the power of each component of the harmonic structure to be accurately estimated, one hopes to have a high-precision estimation of F0. At the same time, under the empirical constraint that speech spectral envelopes are usually smooth in the power domain, half pitch errors can be significantly avoided. Therefore, F0 and the envelope should be estimated jointly rather than separately through an optimal estimation of the spectral envelope and the spectral fine structure. In this article, we introduce a new speech analysis method using a spectral model with a composite function of envelope and fine structure models.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-627"
  },
  "errity06_interspeech": {
   "authors": [
    [
     "Andrew",
     "Errity"
    ],
    [
     "John",
     "McKenna"
    ]
   ],
   "title": "An investigation of manifold learning for speech analysis",
   "original": "i06_1667",
   "page_count": 4,
   "order": 628,
   "p1": "paper 1667-Thu2BuP.8",
   "pn": "",
   "abstract": [
    "Due to the physiological constraints of articulatory motion the speech apparatus has limited degrees of freedom. As a result, the range of speech sounds a human is capable of producing may lie on a low dimensional submanifold of the high dimensional space of all possible sounds. In this study a number of manifold learning algorithms are applied to speech data in an effort to extract useful low dimensional structure from the high dimensional speech signal. The ability of these manifold learning algorithms to separate vowels in a low dimensional space is evaluated and compared to a classical linear dimensionality reduction method. Results indicate that manifold learning algorithms outperform classical methods in low dimensions and are capable of discovering useful manifold structure in speech data.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-628"
  },
  "bouvrie06_interspeech": {
   "authors": [
    [
     "Jake",
     "Bouvrie"
    ],
    [
     "Tony",
     "Ezzat"
    ]
   ],
   "title": "An incremental algorithm for signal reconstruction from short-time fourier transform magnitude",
   "original": "i06_1691",
   "page_count": 4,
   "order": 629,
   "p1": "paper 1691-Thu2BuP.9",
   "pn": "",
   "abstract": [
    "?We present an algorithm for reconstructing a time-domain signal from the magnitude of a short-time Fourier transform (STFT). In contrast to existing algorithms based on alternating projections, we offer a novel approach involving numerical root-finding combined with explicit smoothness assumptions. Our technique produces high-quality reconstructions that have lower signal-to-noise ratios when compared to other existing algorithms. If there is little redundancy in the given STFT, in particular, the algorithm can produce signals which also sound significantly better perceptually, as compared to existing work.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-629"
  },
  "takahashi06_interspeech": {
   "authors": [
    [
     "Toru",
     "Takahashi"
    ],
    [
     "Masashi",
     "Nishi"
    ],
    [
     "Toshio",
     "Irino"
    ],
    [
     "Hideki",
     "Kawahara"
    ]
   ],
   "title": "Automatic assignment of anchoring points on vowel templates for defining correspondence between time-frequency representations of speech samples",
   "original": "i06_1737",
   "page_count": 4,
   "order": 630,
   "p1": "paper 1737-Thu2BuP.10",
   "pn": "",
   "abstract": [
    "The automatic assignment of anchoring points is proposed to define the correspondence between the time-frequency representations of speech samples for speech morphing, speech texture mapping, and so on. The correspondence is modeled as a set of segmental bilinear function. These model parameters are called anchoring points. Although, the correspondence significantly affects the quality of such manipulated speech sounds as morphed and texture mapped speech sounds, anchoring points were manually aligned on time-frequency representations.\n",
    "Anchoring points should be placed at auditorily important locations. When a spectrogram is presented as a time-frequency representation, auditorily important locations are given by formant frequencies around vowel transitions. The central idea of the proposed method is to prepare vowel template spectra with pre-assigned anchoring points in advance and to deform one of the templates to match the input speech spectrum. Finally, anchoring points on the input spectrum are copied from pre-assigned anchoring points.\n",
    "Experimental results suggest that the naturalness of morphed speech based on the proposed automatic assignment method has equivalent quality to STRAIGHT synthetic speech samples.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-630"
  },
  "prasad06_interspeech": {
   "authors": [
    [
     "S.",
     "Prasad"
    ],
    [
     "S.",
     "Srinivasan"
    ],
    [
     "M.",
     "Pannuri"
    ],
    [
     "G.",
     "Lazarou"
    ],
    [
     "Joseph",
     "Picone"
    ]
   ],
   "title": "Nonlinear dynamical invariants for speech recognition",
   "original": "i06_1799",
   "page_count": 4,
   "order": 631,
   "p1": "paper 1799-Thu2BuP.11",
   "pn": "",
   "abstract": [
    "There is growing interest in modeling nonlinear behavior in the speech signal, particularly for applications such as speech recognition. Conventional tools for analyzing speech data use information from the power spectral density of the time series, and hence are restricted to the first two moments of the data. These moments do not provide a sufficient representation of a signal with strong nonlinear properties. In this paper, we investigate the use of features, known as invariants, that measure the nonlinearity in a signal. We analyze three popular measures: Lyapunov exponents, Kolmogorov entropy and correlation dimension. These measures quantify the presence (and extent) of chaos in the underlying system that generated the observable. We show that these invariants can discriminate between broad phonetic classes on a simple database consisting of sustained vowels using the Kullback-Leibler divergence measure. These features show promise in improving the robustness of speech recognition systems in noisy environments.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-631"
  },
  "lin06e_interspeech": {
   "authors": [
    [
     "Shih-Hsiang",
     "Lin"
    ],
    [
     "Yao-Ming",
     "Yeh"
    ],
    [
     "Berlin",
     "Chen"
    ]
   ],
   "title": "Exploiting polynomial-fit histogram equalization and temporal average for robust speech recognition",
   "original": "i06_1195",
   "page_count": 4,
   "order": 632,
   "p1": "paper 1195-Thu2CaP.1",
   "pn": "",
   "abstract": [
    "The performance of current automatic speech recognition (ASR) systems radically deteriorates when the input speech is corrupted by various kinds of noise sources. Quite a few of techniques have been proposed to improve ASR robustness in the past several years. Histogram equalization (HEQ) is one of the most efficient techniques that have been used to compensate the nonlinear distortion. In this paper, we explored the use of the data fitting scheme to efficiently approximate the inverse of the cumulative density function of training speech for HEQ, in contrast to the conventional table-lookup or quantile based approaches. Moreover, the temporal average operation was also performed on the feature vector components to alleviate the influence of sharp peaks and valleys that were caused by non-stationary noises. Finally, we also investigated the possibility of combining our approaches with other feature discrimination and decorrelation methods. All experiments were carried out on the Aurora-2 database and task. Encouraging results were initially demonstrated.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-632"
  },
  "demange06_interspeech": {
   "authors": [
    [
     "Sébastien",
     "Demange"
    ],
    [
     "Christophe",
     "Cerisara"
    ],
    [
     "Jean-Paul",
     "Haton"
    ]
   ],
   "title": "Missing data mask models with global frequency and temporal constraints",
   "original": "i06_1226",
   "page_count": 4,
   "order": 633,
   "p1": "paper 1226-Thu2CaP.2",
   "pn": "",
   "abstract": [
    "Missing data recognition has been developed in order to increase noise robustness in automatic speech recognition. Many different factors, including the speech decoding process itself, shall be considered to locate the masks. In this work, we are considering Bayesian models of the masks, where every spectral feature is classified as reliable or masked, and is independent from the rest of the signal.\n",
    "This classification strategy can produce unrelated small \"spots\", while experiments suggest that oracle reliable and unreliable features tend to be clustered into time-frequency blocks. We call this undesired effect: the \"checkerboard\" effect.\n",
    "In this paper, we propose a new Bayesian missing data classifier that integrates frequency and temporal constraints in order to reduce, or avoid, this \"checkerboard\" effect. The proposed classifier is evaluated on the Aurora2 connected digit corpora. Integrating such constraints in the missing data classification leads to significant improvements in recognition accuracy.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-633"
  },
  "misra06_interspeech": {
   "authors": [
    [
     "Hemant",
     "Misra"
    ],
    [
     "Jithendra",
     "Vepa"
    ],
    [
     "Hervé",
     "Bourlard"
    ]
   ],
   "title": "Multi-stream ASR: an oracle perspective",
   "original": "i06_1663",
   "page_count": 4,
   "order": 634,
   "p1": "paper 1663-Thu2CaP.3",
   "pn": "",
   "abstract": [
    "Multi-stream based automatic speech recognition (ASR) systems are usually shown to outperform single stream systems, specially in noisy test conditions. And, indeed, there is a trend today in ASR towards using more and more acoustic features combined at the input (early integration, possibly preceded by some linear or nonlinear transformation) or later in the recognition process (e.g., at the level of likelihoods, then referred to as late integration). However, to guarantee optimal exploitation of such multi-stream systems, we need to use features that are as much complementary as possible, while also using the best combination method for those streams. In practice, it is never clear whether we fully exploit the potential of the available streams. This present paper investigates an oracle test to provide some insight in these issues. Although not providing us with an absolute performance upper bound, oracle is shown to indicate the complimentary of the feature streams used, and to provide a reasonable reference target to evaluate combination strategies. The oracle analysis is supported by results obtained on Numbers95 database using different feature streams and entropy based combination method.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-634"
  },
  "iwano06_interspeech": {
   "authors": [
    [
     "Koji",
     "Iwano"
    ],
    [
     "Kaname",
     "Kojima"
    ],
    [
     "Sadaoki",
     "Furui"
    ]
   ],
   "title": "A weight estimation method using LDA for multi-band speech recognition",
   "original": "i06_1680",
   "page_count": 4,
   "order": 635,
   "p1": "paper 1680-Thu2CaP.4",
   "pn": "",
   "abstract": [
    "This paper proposes a band-weight estimation method using Linear Discriminant Analysis (LDA) for multi-band automatic speech recognition (ASR). In our scheme, a spectral domain feature, SPEC, is modeled using a multi-stream HMM technique. This paper also proposes the use of Output Likelihood Normalization (OLN) in combination with the LDA-based weight-estimation method in order to adjust the relative weights of individual word (phoneme) models. Experiments were conducted using Japanese connected digit speech in various kinds of noise and SNR conditions. Experimental results show that the proposed LDA-based method is effective in all noise conditions. The results also confirm that the combination of OLN with the LDA-based method further increases noise robustness of the multi-band ASR. Furthermore, comparing the results of LDA applied to the SPEC and MFCC features respectively, it can be seen that greater performance gains are achieved with the former case than with the latter; this means that SPEC within a multi-band speech recognition framework can more effectively deal with the noise contamination than MFCC.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-635"
  },
  "hsu06b_interspeech": {
   "authors": [
    [
     "Chang-wen",
     "Hsu"
    ],
    [
     "Lin-shan",
     "Lee"
    ]
   ],
   "title": "Powered cepstral normalization (p-CN) for robust features in speech recognition",
   "original": "i06_1746",
   "page_count": 4,
   "order": 636,
   "p1": "paper 1746-Thu2CaP.5",
   "pn": "",
   "abstract": [
    "Cepstral normalization has been popularly used as a powerful approach to produce robust features for speech recognition. Good examples of approaches in this family include the well known Cepstral Mean Subtraction (CMS) and Cepstral Mean and Variance Normalization (CMVN), in which either the first or both the first and the second moments of the Mel-frequency Cepstral Coefficients (MFCCs) are normalized. In this paper, an improved approach of Powered Cepstral Normalization (P-CN) is proposed to normalize the MFCC parameters in the r -th powered domain, where r > 1.0. The basic idea is that when the MFCC parameters are raised to the r -th power, the harmful parts of environmental disturbances may be more emphasized than the speech features which are relatively smooth. Therefore performing the normalization in the domain of the r -th power may be more helpful. But the value of r should not be too large because in that case the environmental disturbances may be exaggerated and further corrupt the speech features. This approach is computationally simple and efficient. Initial experimental results on AURORA 2.0 testing environment showed that significant improvements in recognition rates are consistently obtainable under all different noisy conditions.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-636"
  },
  "ding06_interspeech": {
   "authors": [
    [
     "Pei",
     "Ding"
    ],
    [
     "Lei",
     "He"
    ],
    [
     "Xiang",
     "Yan"
    ],
    [
     "Jie",
     "Hao"
    ]
   ],
   "title": "Robust automatic speech recognition for accented Mandarin in car environments",
   "original": "i06_1764",
   "page_count": 4,
   "order": 637,
   "p1": "paper 1764-Thu2CaP.6",
   "pn": "",
   "abstract": [
    "This paper addresses the issues of robust automatic speech recognition (ASR) for accented Mandarin in car environments. A robust front-end is proposed, which adopts a Minimum Mean-Square Error (MMSE) estimator to suppress the background noise in frequency domain, and then implements spectrum smoothing both in time and frequency index to compensate those spectrum components distorted by the noise over-reduction. In the context of Mandarin speech recognition, a special adverse factor is the diversification of Chinese dialects, i.e. the pronunciation difference among dialects decreases the recognition performance if the acoustic models are trained with an unmatched accented database. We propose to train the models with multiple accented Mandarin databases to solve this problem. Evaluation results of isolated phrase recognition show that the proposed front-end can obtained the average error rate reduction (ERR) of 58.3% and 9.7% for artificial car noisy speech and real in-car speech respectively, when compared with the baseline in which no noise compensation technology is used. The efficiency of the proposed model training scheme is also proved in the experiments.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-637"
  },
  "lu06_interspeech": {
   "authors": [
    [
     "Xugang",
     "Lu"
    ],
    [
     "Masashi",
     "Unoki"
    ],
    [
     "Masato",
     "Akagi"
    ]
   ],
   "title": "A robust feature extraction based on the MTF concept for speech recognition in reverberant environment",
   "original": "i06_1801",
   "page_count": 4,
   "order": 638,
   "p1": "paper 1801-Thu2CaP.7",
   "pn": "",
   "abstract": [
    "This paper proposes a robust feature extraction method for automatic speech recognition (ASR) systems in reverberant environment. In this method, a sub-band power envelope inverse filtering algorithm based on the modulation transfer function (MTF), that we have previously proposed, is incorporated as a front-end processor for ASR. The impulse response of the room acoustics is assumed to be exponential decay modulated white noise, and speech is assumed to be temporal modulated white noise in each sub-band. Therefore, the impulse response of the environment does not need to be measured. Testing demonstrated that this algorithm can restore the temporal power envelope of reverberant speech in subbands and thus reduce the loss of speech intelligibility caused by reverberation. Testing of its ability to recognize digitized Japanese speech was done by using reverberant speech created by simple convolution of the room acoustics and speech. The algorithm had a 32.1% higher error reduction rate (on average, for reverberation times from 0.1 to 2.0 s) compared with the traditional cepstral mean normalization (CMN) of the auditory power spectrum based method (AFCC).\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-638"
  },
  "kim06g_interspeech": {
   "authors": [
    [
     "Young Joon",
     "Kim"
    ],
    [
     "Woohyung",
     "Lim"
    ],
    [
     "Nam Soo",
     "Kim"
    ]
   ],
   "title": "Clean speech feature estimation based on soft spectral masking",
   "original": "i06_1897",
   "page_count": 4,
   "order": 639,
   "p1": "paper 1897-Thu2CaP.8",
   "pn": "",
   "abstract": [
    "In this paper, we first analyze the problems of speech and noise contamination process in noise-masking point of view, and propose a new approach to estimate degree of noise masking effect on clean speech distribution model based on sequential noise estimation. Sequential noise estimation is performed frame-by-frame using interacting multiple model (IMM) algorithm, so that real-time implementation is possible. After applying IMM algorithm, degree of noise masking effect named as noise masking probability (NMP) is calculated. Estimation of clean speech spectrum in noisy environments is performed by controlling the advantages of log spectrum domain and those of linear spectrum domain algorithm based on NMP. We have performed recognition experiments under noise conditions using the AURORA2 database which is developed for a standard reference of speech recognition performance. Simulation results show that this approach is effective when noise masking effect is dominated at low SNR.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-639"
  },
  "vali06_interspeech": {
   "authors": [
    [
     "Mansoor",
     "Vali"
    ],
    [
     "Seyyed Ali Seyyed",
     "Salehi"
    ],
    [
     "Kazem",
     "Karimi"
    ]
   ],
   "title": "Robust speech recognition by modifying clean and telephone feature vectors using bidirectional neural network",
   "original": "i06_2072",
   "page_count": 4,
   "order": 640,
   "p1": "paper 2072-Thu2CaP.9",
   "pn": "",
   "abstract": [
    "In this paper we present a new method for nonlinear compensation of distortions, e.g. channel effects and additive noise, in clean and telephone speech recognition. A Bidirectional Neural Network (Bidi- NN) was developed and implemented in order to modify distorted input feature vectors and improve the overall recognition accuracy. Distorted components in feature vectors were estimated in accordance with the latent knowledge in the hidden layer of the neural network. This knowledge is obtained by training with clean and telephone speech, simultaneously and is mostly induced by phonemic content and less influenced by the irrelevant variations in speech signal. An MLP neural network was trained with these modified feature vectors. Comparing the achieved results with a reference model that was trained with unmodified feature vectors, demonstrate significant improvement in clean and telephone speech recognition accuracy.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-640"
  },
  "tai06_interspeech": {
   "authors": [
    [
     "Chung-fu",
     "Tai"
    ],
    [
     "Jeih-weih",
     "Hung"
    ]
   ],
   "title": "Silence energy normalization for robust speech recognition in additive noise environment",
   "original": "i06_1492",
   "page_count": 4,
   "order": 641,
   "p1": "paper 1492-Thu2CaP.10",
   "pn": "",
   "abstract": [
    "The energy parameter has been widely used as an extension to the basic features of mel-frequency cepstral coefficients (MFCCs) to improve the recognition accuracy in speech recognition. In this paper, a simple and effective approach for energy normalization for silence (non-speech) portions in an utterance is proposed. This approach, named as silence energy normalization (SEN), uses the high-pass filtered log-energy as the feature for speech/non-speech classification, and then the log-energy of non-speech frames is set to be a small constant while that of speech frames is kept unchanged. In the experiments conducted on AURORA2 database, we showed that SEN provides an averaged word error rate reduction of 34.9% and 44.6% for Test Sets A and B, respectively, when compared with the baseline processing. It was also shown that SEN outperforms similar approaches like energy subtraction (ES) and feature vector selection (FVS). Finally, we showed that SEN can be integrated with cepstral mean and variance normalization (CMVN), to achieve further improved recognition performance.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-641"
  },
  "segbroeck06_interspeech": {
   "authors": [
    [
     "Maarten Van",
     "Segbroeck"
    ],
    [
     "Hugo",
     "Van hamme"
    ]
   ],
   "title": "Handling convolutional noise in missing data automatic speech recognition",
   "original": "i06_1248",
   "page_count": 4,
   "order": 642,
   "p1": "paper 1248-Thu2CaP.11",
   "pn": "",
   "abstract": [
    "Missing Data Techniques have already shown their effectiveness in dealing with additive noise in automatic speech recognition systems. For real-life deployments, a compensation for linear filtering distortions is also required. Channel compensation in speech recognition typically involves estimating an additive shift in the log-spectral or cepstral domain. This paper explores a maximum likelihood technique to estimate this model offset while some data are missing. Recognition experiments on the Aurora2 recognition task demonstrate the effectiveness of this technique. In particular, we show that our method is more accurate than previously published methods and can handle narrow-band data.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-642"
  },
  "kitaoka06_interspeech": {
   "authors": [
    [
     "Norihide",
     "Kitaoka"
    ],
    [
     "Souta",
     "Hamaguchi"
    ],
    [
     "Seiichi",
     "Nakagawa"
    ]
   ],
   "title": "Noisy speech recognition based on selection of multiple noise suppression methods using noise GMMs",
   "original": "i06_1207",
   "page_count": 4,
   "order": 643,
   "p1": "paper 1207-Thu2CaP.12",
   "pn": "",
   "abstract": [
    "To achieve high recognition performance for a wide variety of noise and for a wide range of signal-to-noise ratio, this paper presents integration methods of four noise reduction algorithms: spectral subtraction with smoothing of time direction, temporal domain SVD-based speech enhancement, GMM-based speech estimation and KLT-based comb-filtering. In this paper, we proposed two types of ?combination methods of noise suppression algorithms: selection of front-end processor and combination of results from multiple recognition processes. Recognition results on the AURORA-2J task showed the effectiveness of our proposed methods.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-643"
  },
  "aradilla06_interspeech": {
   "authors": [
    [
     "Guillermo",
     "Aradilla"
    ],
    [
     "Jithendra",
     "Vepa"
    ],
    [
     "Hervé",
     "Bourlard"
    ]
   ],
   "title": "Using posterior-based features in template matching for speech recognition",
   "original": "i06_1186",
   "page_count": 4,
   "order": 644,
   "p1": "paper 1186-Thu2CaP.13",
   "pn": "",
   "abstract": [
    "Given the availability of large speech corpora, as well as the increasing of memory and computational resources, the use of template matching approaches for automatic speech recognition (ASR) have recently attracted new attention. In such template-based approaches, speech is typically represented in terms of acoustic vector sequences, using spectral-based features such as MFCC of PLP, and local distances are usually based on Euclidean or Mahalanobis distances. In the present paper, we further investigate template-based ASR and show (on a continuous digit recognition task) that the use of posterior-based features significantly improves the standard template-based approaches, yielding to systems that are very competitive to state-of-the-art HMMs, even when using a very limited number (e.g., 10) of reference templates. Since those posteriors-based features can also be interpreted as a probability distribution, we also show that using Kullback-Leibler (KL) divergence as a local distance further improves the performance of the template-based approach, now beating state-of-the-art of more complex posterior-based HMMs systems (usually referred to as \"Tandem\").\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-644"
  },
  "obuchi06b_interspeech": {
   "authors": [
    [
     "Yasunari",
     "Obuchi"
    ],
    [
     "Nobuo",
     "Hataoka"
    ]
   ],
   "title": "Hypothesis-based feature combination of multiple speech inputs for robust speech recognition in automotive environments",
   "original": "i06_1165",
   "page_count": 4,
   "order": 645,
   "p1": "paper 1165-Thu2CaP.14",
   "pn": "",
   "abstract": [
    "In a microphone array system, feature combination in the MFCC domain can improve speech recognition accuracy. Multiple microphones provide different feature parameters such as MFCCs even if they have similar speech and noise signals, because of the phase difference and transmission characteristics. In this paper, we investigate how the recognition performance changes when we average multiple MFCC feature vectors. In addition, we extend Hypothesis-Based Feature Combination, which we formerly proposed for dual-microphone systems, to multi-input systems. Experimental results show that variance re-scaling is necessary when we combine multiple inputs with Cepstral Mean Normalization (CMN), in both MFCC average and HBFC. However, we can obtain better results without variance re-scaling if we use Mean and Variance Normalization (MVN) with MFCC average or HBFC. In the experiments using the database collected in a real automotive environment, HBFC-MVN reduced 22% of the recognition errors from the baseline single-microphone system.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-645"
  },
  "koldovsky06_interspeech": {
   "authors": [
    [
     "Zbynek",
     "Koldovsky"
    ],
    [
     "Jan",
     "Nouza"
    ],
    [
     "Jan",
     "Kolorenc"
    ]
   ],
   "title": "Continuous time-frequency masking method for blind speech separation with adaptive choice of threshold parameter using ICA",
   "original": "i06_1224",
   "page_count": 4,
   "order": 646,
   "p1": "paper 1224-Thu2FoP.1",
   "pn": "",
   "abstract": [
    "We propose a novel method for blind speech separation using continuous time-frequency masking. The method is equipped with an adaptive choice of a threshold parameter that is based on utilization of ICA methods. We present a direct application that consists in the speech segregation for automatic transcription of spoken broadcasts disturbed by background music. Experimental results show improved performance in comparison with traditionally used binary masking methods.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-646"
  },
  "liang06b_interspeech": {
   "authors": [
    [
     "Yanxue",
     "Liang"
    ],
    [
     "Ichiro",
     "Hagiwara"
    ]
   ],
   "title": "Multistage convolutive blind source separation for speech mixture",
   "original": "i06_1369",
   "page_count": 4,
   "order": 647,
   "p1": "paper 1369-Thu2FoP.2",
   "pn": "",
   "abstract": [
    "Blind source separation for convolutive mixture of speech signals has been addressed in many literatures. However, widely applied Multichannel Blind Deconvolution (MBD) method suffers whitening effect or arbitrary filtering problem which results in dramatic decrease of Automatic Speech Recognition system's performance. In present paper, a new MBD based multistage method is proposed, in which contributions of each source to every microphone are final goal rather than original signals. In detail, MBD is first implemented using entropy maximization criterion combined with Natural Gradient (NG) algorithm, then compensation matrix is constructed, based on which sources are recovered to its contribution to every microphone, i.e., whitening effect or arbitrary filtering problem has been transformed to fixed filtering problem. After compensation processing, for a certain source, it becomes Single Input and Multi-Output (SIMO) problem. 1098-Thus, not only spatial quality of source can be preserved, but also SIMO blind deconvolution can be further applied to fully recover temporal structure of speech signal. Finally, experiment shows validity and superiority over other methods in both spectra preservation efficiency and speed.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-647"
  },
  "asano06_interspeech": {
   "authors": [
    [
     "Futoshi",
     "Asano"
    ],
    [
     "Jun",
     "Ogata"
    ]
   ],
   "title": "Detection and separation of speech events in meeting recordings",
   "original": "i06_1098",
   "page_count": 4,
   "order": 648,
   "p1": "paper 1098-Thu2FoP.3",
   "pn": "",
   "abstract": [
    "When applying automatic speech recognition (ASR) to meeting recordings including spontaneous speech, the performance of ASR is greatly reduced by the overlap of speech events. In this paper, a method of separating the overlapping speech events using an adaptive beamforming (ABF) framework is proposed. The main feature of this method is that all the necessary information for the adaptation of ABF, including microphone calibration, is obtained from meeting recordings based on the results of speech event detection. The performance of the separation is evaluated via ASR using real meeting recordings.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-648"
  },
  "abad06_interspeech": {
   "authors": [
    [
     "Alberto",
     "Abad"
    ],
    [
     "Carlos",
     "Segura"
    ],
    [
     "Duàn",
     "Macho"
    ],
    [
     "Javier",
     "Hernando"
    ],
    [
     "Climent",
     "Nadeu"
    ]
   ],
   "title": "Audio person tracking in a smart-room environment",
   "original": "i06_1649",
   "page_count": 4,
   "order": 649,
   "p1": "paper 1649-Thu2FoP.4",
   "pn": "",
   "abstract": [
    "Reliable measures of speaker positions are needed for computational perception of human activities taking place in a smart-room environment. In this work, it is described the development process and the experiments conducted in the design and implementation of an Audio Person Tracking system for smart-room environments. The proposed system is based on the SRP-PHAT algorithm, as it is known to perform robustly in most environmental conditions. Novelties proposed are aimed to enhance the accuracy of the system independently on the application scenario and to reduce the computational complexity.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-649"
  },
  "gehrig06b_interspeech": {
   "authors": [
    [
     "Tobias",
     "Gehrig"
    ],
    [
     "Ulrich",
     "Klee"
    ],
    [
     "John W.",
     "McDonough"
    ],
    [
     "Shajith",
     "Ikbal"
    ],
    [
     "Matthias",
     "Wölfel"
    ],
    [
     "Christian",
     "Fügen"
    ]
   ],
   "title": "Tracking and beamforming for multiple simultaneous speakers with probabilistic data association filters",
   "original": "i06_2038",
   "page_count": 4,
   "order": 650,
   "p1": "paper 2038-Thu2FoP.5",
   "pn": "",
   "abstract": [
    "In prior work, we developed a speaker tracking system based on an extended Kalman filter using time delays of arrival (TDOAs) as acoustic features. While this system functioned well, its utility was limited to scenarios in which a single speaker was to be tracked. In this work, we remove this restriction by generalizing the IEKF, first to a probabilistic data association filter, which incorporates a clutter model for rejection of spurious acoustic events, and then to a joint probabilistic data association filter (JPDAF), which maintains a separate state vector for each active speaker. In a set of experiments conducted on seminar and meeting data, the JPDAF speaker tracking system reduced the multiple object tracking error from 20.7% to 14.3% with respect to the IEKF system. In a set of automatic speech recognition experiments conducted on the output of a 64 channel microphone array which was beamformed using automatic speaker position estimates, applying the JPDAF tracking system reduced word error rate from 67.3% to 66.0%. Moreover, the word error rate on the beamformed output was 13.0% absolute lower than on a single channel of the array.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-650"
  },
  "heckmann06b_interspeech": {
   "authors": [
    [
     "Martin",
     "Heckmann"
    ],
    [
     "Tobias",
     "Rodemann"
    ],
    [
     "Bjorn",
     "Scholling"
    ],
    [
     "Frank",
     "Joublin"
    ],
    [
     "Christian",
     "Goerick"
    ]
   ],
   "title": "Modeling the precedence effect for binaural sound source localization in noisy and echoic environments",
   "original": "i06_1196",
   "page_count": 4,
   "order": 651,
   "p1": "paper 1196-Thu2FoP.6",
   "pn": "",
   "abstract": [
    "We present a new way of modelling the Precedence Effect to enable the robust measurement of localization cues (ITD and IID) in echoic environments. Based on this we developed a localization system which is inspired by the auditory system of mammals. It uses a Gammatone filter bank for preprocessing and extracts the ITD cue via zero crossings (IID calculation is straight forward). The mapping between the cue values and the different angles is learned offline which facilitates the adaptation to different head geometries. The performance of the system is demonstrated by localization results for two simultaneous speakers and the mixture of a speaker, music, and fan noise in a normal meeting room. A real-time demonstrator of the system is presented in [1].\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-651"
  },
  "talantzis06_interspeech": {
   "authors": [
    [
     "Fotios",
     "Talantzis"
    ],
    [
     "Anthony G.",
     "Constantinides"
    ],
    [
     "Lazaros C.",
     "Polymenakos"
    ]
   ],
   "title": "Using a differential microphone array to estimate the direction of arrival of two acoustic sources",
   "original": "i06_1190",
   "page_count": 4,
   "order": 652,
   "p1": "paper 1190-Thu2FoP.7",
   "pn": "",
   "abstract": [
    "We present a system that estimates the direction of arrival of two competing acoustic sources using two closely spaced receivers that form a differential microphone array. The main advantage of the proposed array topology is that null steering can be essentially performed by adapting a set of two scalars. The direction of arrival estimation relies on the successful estimation of the relative delays between the microphone signals using the decorrelation constraint. Processing is performed in real-time by operating on blocks of recorded data. We examine the performance of the system for different block sizes and investigate its robustness in environments of strong multipath reflections where algorithms often fail to distinguish between the true direction of arrival and that of a dominant reflection. The overall performance of the system is compared to the simple omni-directional array topology. The results indicate that the examined framework can track the two directions of arrival adequately.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-652"
  },
  "brutti06_interspeech": {
   "authors": [
    [
     "Alessio",
     "Brutti"
    ],
    [
     "Maurizio",
     "Omologo"
    ],
    [
     "Piergiorgio",
     "Svaizer"
    ]
   ],
   "title": "Speaker localization based on oriented global coherence field",
   "original": "i06_1467",
   "page_count": 4,
   "order": 653,
   "p1": "paper 1467-Thu2FoP.8",
   "pn": "",
   "abstract": [
    "This paper proposes a new speaker localization method that is based on a preliminary estimation of the head orientation. The basic information on which the estimation is accomplished is called Oriented Global Coherence Field (OGCF).\n",
    "The new algorithm is shown to be significantly more robust than the traditional ones so far explored. Its robustness is also due to an effective speech activity detection, implicitly performed by a thresholding technique applied to OGCF information. To show the performance of the proposed system, experiments were conducted on the NIST RT-05 Spring Evaluation source localization task, which is based on real recordings of lectures in noisy and reverberant environments.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-653"
  },
  "radfar06_interspeech": {
   "authors": [
    [
     "M. H.",
     "Radfar"
    ],
    [
     "R. M.",
     "Dansereau"
    ],
    [
     "A.",
     "Sayadiyan"
    ]
   ],
   "title": "Performance evaluation of three features for model-based single channel speech separation problem",
   "original": "i06_2005",
   "page_count": 4,
   "order": 654,
   "p1": "paper 2005-Thu2FoP.9",
   "pn": "",
   "abstract": [
    "This paper addresses the efficiency of three features for the modelbased single channel speech separation problem. The separability of three features: log spectrum, modulated lapped transform (MLT) coefficients, and a fusion of pitch and envelop information are evaluated using a VQ-based speech separation technique. At the core of this approach are two trained codebooks of the quantized feature vectors of speakers, whereby the main evaluation for separation is performed. The experiments are conducted in two different scenarios: speakerdependent and speaker independent. The results show that the log spectrum outperforms the other features for speaker-dependent scenario. However, for the speaker-independent scenario, the best results are obtained from applying the pitch-envelop feature.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-654"
  },
  "schmidt06_interspeech": {
   "authors": [
    [
     "Mikkel N.",
     "Schmidt"
    ],
    [
     "Rasmus K.",
     "Olsson"
    ]
   ],
   "title": "Single-channel speech separation using sparse non-negative matrix factorization",
   "original": "i06_1652",
   "page_count": 4,
   "order": 655,
   "p1": "paper 1652-Thu2FoP.10",
   "pn": "",
   "abstract": [
    "We apply machine learning techniques to the problem of separating multiple speech sources from a single microphone recording. The method of choice is a sparse non-negative matrix factorization algorithm, which in an unsupervised manner can learn sparse representations of the data. This is applied to the learning of personalized dictionaries from a speech corpus, which in turn are used to separate the audio stream into its components. We show that computational savings can be achieved by segmenting the training data on a phoneme level. To split the data, a conventional speech recognizer is used. The performance of the unsupervised and supervised adaptation schemes result in significant improvements in terms of the target-to-masker ratio.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-655"
  },
  "hu06d_interspeech": {
   "authors": [
    [
     "Rong",
     "Hu"
    ],
    [
     "Yunxin",
     "Zhao"
    ]
   ],
   "title": "Adaptive speech enhancement for speech separation in diffuse noise",
   "original": "i06_1751",
   "page_count": 4,
   "order": 656,
   "p1": "paper 1751-Thu2FoP.11",
   "pn": "",
   "abstract": [
    "An adaptive enhancement method is proposed to improve recognition accuracy on the outputs of blind speech separation (BSS) system based on adaptive decorrelation filtering (ADF) in diffuse noise. A divide and conquer strategy is taken to deal with the noise effects on both system adaptation and ADF outputs. First, fast noise compensation (NC) is performed for filter adaptation, forcing ADF to focus on the task of separation; then, output noises are reduced by conventional speech enhancement, such as spectral subtraction or subspace methods. To make stationary-noise reduction techniques fit for output noises with time-varying properties caused by ADF adaptations, a fast adaptive procedure is developed to map known stationary input noise statistics to output. Separation and recognition experiments were conducted for both real and simulated diffuse noises, based on TIMIT speech data and impulse response data from a room with reverberation time T60 = 0.3sec. The proposed techniques significantly improved phone recognition accuracy of ADF results.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-656"
  },
  "attias06_interspeech": {
   "authors": [
    [
     "H. T.",
     "Attias"
    ]
   ],
   "title": "A probabilistic graphical model for microphone array source separation using rich pre-trained source models",
   "original": "i06_1946",
   "page_count": 4,
   "order": 657,
   "p1": "paper 1946-Thu2FoP.12",
   "pn": "",
   "abstract": [
    "Voice based computing applications, such as phone communication and speech recognition, use microphone arrays to capture voice from a human speaker. In many environments of interest, however, sounds from other sources interfere with the speakers voice, posing severe problems for subsequent processing. This paper describes a new framework for treating this problem, and presents and demonstrates a new algorithm for the cancellation of interfering sounds. Our framework combines techniques from statistical machine learning with ideas from speech and audio processing. An important feature involves training rich probabilistic models on data from different types of relevant sound sources. Those source models are then incorporated into a larger probabilistic model of the observed microphone data. Using that model we derive our algorithm, which is of the expectationmaximization type and infers from data the clean sound of separate individual sources. We report very good results on data recorded in different environments.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-657"
  },
  "visser06_interspeech": {
   "authors": [
    [
     "Erik",
     "Visser"
    ]
   ],
   "title": "Geometrically constrained permutation-free source separation in an undercomplete speech unmixing scenario",
   "original": "i06_1086",
   "page_count": 4,
   "order": 658,
   "p1": "paper 1086-Thu2FoP.13",
   "pn": "",
   "abstract": [
    "Frequency domain blind source separation (BSS) problems are typically solved in each frequency bin independently and therefore require additional measures to resolve the resulting permutation problem. In this paper, a frequency domain methodology is presented based on a recently introduced extension of Independent Component Analysis (ICA) to multi-variate components which uses a multi-variate activation function to model dependencies between frequency bins and therefore inherently manages to align most of the permutations. Since the latter approach shows slow convergence behaviour and is prone to converging to local optima, additional geometric constraints are used here to force the BSS algorithm to separate sources with a consistent direction of arrival (DOA) over all frequencies into a minimum number of output channels. DOA information is obtained from a priori knowledge or from subband analysis of partially separated source signals. The methodology is illustrated in an undercomplete acoustic source separation scenario with 3 speakers and 4 microphones.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-658"
  },
  "olszewski06_interspeech": {
   "authors": [
    [
     "Dirk",
     "Olszewski"
    ],
    [
     "Klaus",
     "Linhard"
    ]
   ],
   "title": "Highly directional multi-beam audio loudspeaker",
   "original": "i06_1239",
   "page_count": 4,
   "order": 659,
   "p1": "paper 1239-Thu2FoP.14",
   "pn": "",
   "abstract": [
    "This work presents an audio beam loudspeaker that possesses the capability of emitting multiple audio beams into different directions at the same time. Ultrasonic loudspeakers applying parametric array technology are used to generate highly directional audible sound beams. The loudspeaker itself appears as a single device and can be mounted under the ceiling of a room in order to supply selected room segments with highly directional audio information such as music or speech.\n",
    "Due to the fact that parametric array technology requires very high ultrasound levels, high-power transducers have to be used. Because phased array technology applied on those transducers for beam steering does not work successfully, an alternative approach based on pre-steered segments is presented in this work.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2006-659"
  }
 },
 "sessions": [
  {
   "title": "Language Modeling for Spoken Dialog Systems",
   "papers": [
    "purver06_interspeech",
    "ye06_interspeech",
    "misu06_interspeech",
    "horndasch06_interspeech",
    "weilhammer06_interspeech",
    "feng06_interspeech"
   ]
  },
  {
   "title": "Feature Enhancement for Robust ASR",
   "papers": [
    "kenny06_interspeech",
    "hsieh06_interspeech",
    "nasersharif06_interspeech",
    "faubel06_interspeech",
    "hsu06_interspeech",
    "islam06_interspeech"
   ]
  },
  {
   "title": "Dialog and Discourse",
   "papers": [
    "hurtado06_interspeech",
    "rotaru06_interspeech",
    "banerjee06_interspeech",
    "schulz06_interspeech",
    "raux06_interspeech",
    "liscombe06_interspeech"
   ]
  },
  {
   "title": "The Speech Separation Challenge",
   "papers": [
    "srinivasan06_interspeech",
    "han06_interspeech",
    "every06_interspeech",
    "barker06_interspeech",
    "virtanen06_interspeech",
    "ming06_interspeech",
    "kristjansson06_interspeech",
    "deshmukh06_interspeech"
   ]
  },
  {
   "title": "Multilingual and Multi-Accent Processing",
   "papers": [
    "loof06_interspeech",
    "bouselmi06_interspeech",
    "chan06_interspeech",
    "zimmerman06_interspeech",
    "cheng06_interspeech",
    "wang06_interspeech",
    "le06_interspeech",
    "liu06_interspeech",
    "ghorshi06_interspeech",
    "liu06b_interspeech",
    "sarikaya06_interspeech",
    "lihan06_interspeech"
   ]
  },
  {
   "title": "Corpora, Annotation, and Assessment Metrics I, II",
   "papers": [
    "jones06_interspeech",
    "munteanu06_interspeech",
    "nagino06_interspeech",
    "fitt06_interspeech",
    "yamada06_interspeech",
    "draxler06_interspeech",
    "murphy06_interspeech",
    "nichasaide06_interspeech",
    "fraga06_interspeech",
    "grancharov06_interspeech",
    "liang06_interspeech",
    "zgank06_interspeech",
    "toh06_interspeech",
    "aoki06_interspeech",
    "kilanski06_interspeech",
    "sityaev06_interspeech",
    "heeman06_interspeech",
    "bael06_interspeech",
    "shi06_interspeech"
   ]
  },
  {
   "title": "Speech Coding",
   "papers": [
    "chang06_interspeech",
    "nurminen06_interspeech",
    "lee06_interspeech",
    "ramasubramanian06_interspeech",
    "thyssen06_interspeech",
    "qian06_interspeech",
    "kovesi06_interspeech",
    "chatterjee06_interspeech",
    "chatterjee06b_interspeech",
    "hofbauer06_interspeech",
    "anderson06_interspeech"
   ]
  },
  {
   "title": "Speech Enhancement I, II",
   "papers": [
    "suhadi06_interspeech",
    "zavarehei06_interspeech",
    "jensen06_interspeech",
    "subramanya06_interspeech",
    "rank06_interspeech",
    "deshmukh06b_interspeech",
    "jin06_interspeech",
    "shin06_interspeech",
    "pulakka06_interspeech",
    "shannon06_interspeech",
    "shannon06b_interspeech",
    "krishnamurthy06_interspeech",
    "shahina06_interspeech",
    "das06_interspeech",
    "murakami06_interspeech",
    "hu06_interspeech",
    "song06_interspeech"
   ]
  },
  {
   "title": "ASR Other I, II",
   "papers": [
    "boulianne06_interspeech",
    "afify06_interspeech",
    "trancoso06_interspeech",
    "pellegrini06_interspeech",
    "nimaan06_interspeech",
    "cetin06_interspeech",
    "takeda06_interspeech",
    "kim06_interspeech",
    "koo06_interspeech",
    "obuchi06_interspeech",
    "yu06_interspeech",
    "gorriz06_interspeech",
    "cerva06_interspeech",
    "nakamura06_interspeech",
    "chu06_interspeech",
    "huo06_interspeech",
    "gomez06_interspeech",
    "lin06_interspeech",
    "ma06_interspeech",
    "chitturi06_interspeech"
   ]
  },
  {
   "title": "Modeling Prosodic Features",
   "papers": [
    "ananthakrishnan06_interspeech",
    "rosenberg06_interspeech",
    "hirose06_interspeech",
    "dubeda06_interspeech",
    "yi06_interspeech",
    "dilley06_interspeech"
   ]
  },
  {
   "title": "Spoken Information Retrieval",
   "papers": [
    "alphonso06_interspeech",
    "iwata06_interspeech",
    "li06_interspeech",
    "pan06_interspeech",
    "sudoh06_interspeech",
    "turunen06_interspeech"
   ]
  },
  {
   "title": "Front-End Methods for ASR",
   "papers": [
    "schluter06_interspeech",
    "valente06_interspeech",
    "uraga06_interspeech",
    "stouten06_interspeech",
    "wolfel06_interspeech",
    "breithaupt06_interspeech",
    "garcia06_interspeech",
    "ghulam06_interspeech",
    "ariki06_interspeech",
    "boril06_interspeech",
    "ljolje06_interspeech",
    "pylkkonen06_interspeech",
    "farahani06_interspeech",
    "panchapagesan06_interspeech"
   ]
  },
  {
   "title": "Language and Dialect Recognition",
   "papers": [
    "reyesherrera06_interspeech",
    "bauer06_interspeech",
    "timoshenko06_interspeech",
    "basavaraja06_interspeech",
    "yang06_interspeech",
    "navratil06_interspeech",
    "lin06b_interspeech",
    "li06b_interspeech",
    "guijarrubia06_interspeech",
    "ikeno06_interspeech",
    "vierudimulescu06_interspeech",
    "huang06_interspeech"
   ]
  },
  {
   "title": "Spoken Dialog Systems I, II",
   "papers": [
    "gieselmann06_interspeech",
    "gruenstein06_interspeech",
    "ackermann06_interspeech",
    "krsmanovic06_interspeech",
    "son06_interspeech",
    "cuayahuitl06_interspeech",
    "mayer06_interspeech",
    "roque06_interspeech",
    "yamada06b_interspeech",
    "juhar06_interspeech",
    "degerstedt06_interspeech",
    "ito06_interspeech",
    "jokinen06_interspeech",
    "burkhardt06_interspeech",
    "turunen06b_interspeech",
    "weng06_interspeech",
    "georgila06_interspeech"
   ]
  },
  {
   "title": "Speaker Characterization and Recognition I-IV",
   "papers": [
    "chao06_interspeech",
    "lei06_interspeech",
    "scheffer06_interspeech",
    "ma06b_interspeech",
    "garreton06_interspeech",
    "chetty06_interspeech",
    "prahallad06_interspeech",
    "zamalloa06_interspeech",
    "padilla06_interspeech",
    "mary06_interspeech",
    "liu06c_interspeech",
    "anguita06_interspeech",
    "liu06d_interspeech",
    "prakash06_interspeech",
    "varadarajan06_interspeech",
    "mccree06_interspeech",
    "kato06_interspeech",
    "vishnubhotla06_interspeech",
    "ramasubramanian06b_interspeech",
    "fujihara06_interspeech",
    "stergiou06_interspeech",
    "longworth06_interspeech",
    "hatch06_interspeech",
    "espywilson06_interspeech",
    "ofoegbu06_interspeech",
    "biatov06_interspeech",
    "narayanaswamy06_interspeech",
    "preti06_interspeech",
    "zheng06_interspeech",
    "zhang06_interspeech",
    "wu06_interspeech",
    "farrus06_interspeech",
    "pruthi06_interspeech",
    "kojima06_interspeech",
    "muller06_interspeech",
    "fox06_interspeech",
    "potamitis06_interspeech"
   ]
  },
  {
   "title": "System Combination",
   "papers": [
    "siniscalchi06_interspeech",
    "stuker06_interspeech",
    "breslin06_interspeech",
    "zhang06b_interspeech",
    "chen06_interspeech",
    "hoffmeister06_interspeech"
   ]
  },
  {
   "title": "Interpreting Prosodic Variation",
   "papers": [
    "yuan06_interspeech",
    "vu06_interspeech",
    "venditti06_interspeech",
    "krahmer06_interspeech",
    "gravano06_interspeech",
    "vainio06_interspeech"
   ]
  },
  {
   "title": "Articulatory Modeling",
   "papers": [
    "kroger06_interspeech",
    "fontecave06_interspeech",
    "jou06_interspeech",
    "richmond06_interspeech",
    "metze06_interspeech",
    "krnoul06_interspeech"
   ]
  },
  {
   "title": "Acoustic Modeling I - Training and Topologies",
   "papers": [
    "zhang06c_interspeech",
    "keshet06_interspeech",
    "morris06_interspeech",
    "nagarajan06_interspeech",
    "li06c_interspeech",
    "li06d_interspeech",
    "casar06_interspeech",
    "tang06_interspeech",
    "markov06_interspeech",
    "gehrig06_interspeech",
    "suk06_interspeech",
    "ion06_interspeech"
   ]
  },
  {
   "title": "Acoustic Signal Segmentation and Classification",
   "papers": [
    "kuhne06_interspeech",
    "wu06b_interspeech",
    "dusan06_interspeech",
    "yingthawornsuk06_interspeech",
    "didiot06_interspeech",
    "nagino06b_interspeech",
    "pernkopf06_interspeech",
    "beringer06_interspeech",
    "ma06c_interspeech",
    "leelaphattarakij06_interspeech",
    "fu06_interspeech",
    "fu06b_interspeech",
    "carlin06_interspeech",
    "anguera06_interspeech"
   ]
  },
  {
   "title": "Linguistics, Phonology, and Phonetics I, II",
   "papers": [
    "surana06_interspeech",
    "nitisaroj06_interspeech",
    "jacobi06_interspeech",
    "li06e_interspeech",
    "zhang06d_interspeech",
    "nielsen06_interspeech",
    "bael06b_interspeech",
    "kazemzadeh06_interspeech",
    "hirano06_interspeech",
    "hamabe06_interspeech",
    "tseng06_interspeech",
    "suzuki06_interspeech",
    "schone06_interspeech",
    "hertz06_interspeech",
    "speyer06_interspeech",
    "watanabe06_interspeech",
    "davel06_interspeech",
    "lee06b_interspeech",
    "charoenpornsawat06_interspeech"
   ]
  },
  {
   "title": "Speech Translation",
   "papers": [
    "riesa06_interspeech",
    "maskey06_interspeech",
    "lee06c_interspeech",
    "stallard06_interspeech",
    "wang06b_interspeech",
    "hsiao06_interspeech"
   ]
  },
  {
   "title": "Acoustic Modeling II - Adaptation",
   "papers": [
    "sehr06_interspeech",
    "lei06b_interspeech",
    "thatphithakkul06_interspeech",
    "hirsch06_interspeech",
    "tsao06_interspeech",
    "chien06_interspeech"
   ]
  },
  {
   "title": "Emotional Speech and Speaker State",
   "papers": [
    "schuller06_interspeech",
    "ai06_interspeech",
    "devillers06_interspeech",
    "wilting06_interspeech",
    "neiberg06_interspeech",
    "enos06_interspeech"
   ]
  },
  {
   "title": "Speech and Language in Education",
   "papers": [
    "cleuren06_interspeech",
    "waple06_interspeech",
    "massaro06_interspeech",
    "heilman06_interspeech",
    "petersen06_interspeech",
    "mostow06_interspeech",
    "tsurutani06_interspeech",
    "tepperman06_interspeech",
    "abdou06_interspeech"
   ]
  },
  {
   "title": "Speech Perception I, II",
   "papers": [
    "lobdell06_interspeech",
    "morrison06_interspeech",
    "viswanathan06_interspeech",
    "yip06_interspeech",
    "haque06_interspeech",
    "hodoshima06_interspeech",
    "joto06_interspeech",
    "wang06c_interspeech",
    "grawunder06_interspeech",
    "assmann06_interspeech",
    "tohyama06_interspeech",
    "schwarz06_interspeech",
    "torre06_interspeech",
    "creel06_interspeech",
    "mixdorff06_interspeech",
    "woehrling06_interspeech",
    "phatak06_interspeech",
    "broersma06_interspeech",
    "scholz06_interspeech"
   ]
  },
  {
   "title": "Speech Production, Physiology, and Pathology I, II",
   "papers": [
    "pluymaekers06_interspeech",
    "niu06_interspeech",
    "aboutabit06_interspeech",
    "gobl06_interspeech",
    "wei06_interspeech",
    "kacha06_interspeech",
    "kovacs06_interspeech",
    "vijayalakshmi06_interspeech",
    "finan06_interspeech",
    "bunnell06_interspeech",
    "saz06_interspeech",
    "iseli06_interspeech",
    "bosch06_interspeech",
    "lee06d_interspeech",
    "kjellstrom06_interspeech",
    "feng06b_interspeech",
    "garnier06_interspeech"
   ]
  },
  {
   "title": "Formant Estimation",
   "papers": [
    "cnockaert06_interspeech",
    "weruaga06_interspeech",
    "darch06_interspeech",
    "anand06_interspeech",
    "ozbek06_interspeech",
    "chaari06_interspeech"
   ]
  },
  {
   "title": "Language Processing Beyond and Below the Word-Level",
   "papers": [
    "kurimo06_interspeech",
    "arsoy06_interspeech",
    "kobus06_interspeech",
    "akita06_interspeech",
    "virpioja06_interspeech",
    "camelin06_interspeech"
   ]
  },
  {
   "title": "Robustness and Adaptation for ASR",
   "papers": [
    "peinado06_interspeech",
    "gomez06b_interspeech",
    "chen06b_interspeech",
    "kuroiwa06_interspeech",
    "selouani06_interspeech",
    "wang06d_interspeech",
    "clarke06_interspeech",
    "liao06_interspeech",
    "xu06_interspeech",
    "huo06b_interspeech",
    "mandal06_interspeech",
    "tan06_interspeech",
    "zen06_interspeech",
    "povey06_interspeech"
   ]
  },
  {
   "title": "Multimodal, Translation and Information Retrieval",
   "papers": [
    "gispert06_interspeech",
    "oba06_interspeech",
    "bangalore06_interspeech",
    "itoh06_interspeech",
    "surdeanu06_interspeech",
    "takezawa06_interspeech",
    "ettelaie06_interspeech",
    "akbacak06_interspeech",
    "zhu06_interspeech",
    "stenchikova06_interspeech",
    "maas06_interspeech",
    "kaiser06_interspeech",
    "hui06_interspeech"
   ]
  },
  {
   "title": "Advances in Acoustic Segmentation",
   "papers": [
    "cournapeau06_interspeech",
    "hugginsdaines06_interspeech",
    "valente06b_interspeech",
    "dines06_interspeech",
    "kuo06_interspeech",
    "schuler06_interspeech"
   ]
  },
  {
   "title": "Acoustic Modeling III - LVCSR",
   "papers": [
    "ramabhadran06_interspeech",
    "fugen06_interspeech",
    "hwang06_interspeech",
    "lei06c_interspeech",
    "huang06b_interspeech",
    "sun06_interspeech"
   ]
  },
  {
   "title": "Speech and Visual Processing",
   "papers": [
    "beskow06_interspeech",
    "barkhuysen06_interspeech",
    "swerts06_interspeech",
    "chaloupka06_interspeech",
    "cortes06_interspeech",
    "shao06_interspeech"
   ]
  },
  {
   "title": "Text-to-Speech I, II",
   "papers": [
    "strom06_interspeech",
    "carlson06_interspeech",
    "alias06_interspeech",
    "yi06b_interspeech",
    "clark06_interspeech",
    "kim06b_interspeech",
    "bellegarda06_interspeech",
    "nose06_interspeech",
    "ogata06_interspeech",
    "abdelhamid06_interspeech",
    "homayounpour06_interspeech",
    "anberbir06_interspeech",
    "thangthai06_interspeech",
    "oosthuizen06_interspeech",
    "you06_interspeech",
    "weiss06_interspeech",
    "krul06_interspeech",
    "ling06_interspeech",
    "taylor06_interspeech",
    "tihelka06_interspeech",
    "wu06c_interspeech",
    "kang06_interspeech",
    "jilka06_interspeech",
    "utama06_interspeech",
    "fischer06_interspeech",
    "park06_interspeech",
    "parssinen06_interspeech",
    "coorman06_interspeech"
   ]
  },
  {
   "title": "Special Populations - Learners, Aged, Challenged",
   "papers": [
    "tsuji06_interspeech",
    "marklund06_interspeech",
    "bion06_interspeech",
    "bijeljacbabic06_interspeech",
    "hirata06_interspeech",
    "chen06c_interspeech",
    "metzner06_interspeech",
    "wei06b_interspeech",
    "fujita06_interspeech",
    "li06f_interspeech",
    "nakamura06b_interspeech",
    "sansegundo06_interspeech",
    "klintfors06_interspeech",
    "rytting06_interspeech"
   ]
  },
  {
   "title": "Robust ASR",
   "papers": [
    "wang06e_interspeech",
    "kim06c_interspeech",
    "walliczek06_interspeech",
    "vicentepena06_interspeech",
    "zhang06e_interspeech",
    "rademacher06_interspeech"
   ]
  },
  {
   "title": "Speech Summarization",
   "papers": [
    "nenkova06_interspeech",
    "zhu06b_interspeech",
    "chatain06_interspeech",
    "lee06e_interspeech",
    "maskey06b_interspeech",
    "murray06_interspeech"
   ]
  },
  {
   "title": "Acoustic Modeling IV",
   "papers": [
    "kubo06_interspeech",
    "buera06_interspeech",
    "povey06b_interspeech",
    "sakti06_interspeech",
    "wang06f_interspeech",
    "xie06_interspeech",
    "he06_interspeech",
    "hamalainen06_interspeech",
    "park06b_interspeech",
    "zgank06b_interspeech",
    "kim06d_interspeech",
    "nagarajan06b_interspeech",
    "miguel06_interspeech"
   ]
  },
  {
   "title": "Large Vocabulary Speech Recognition",
   "papers": [
    "imai06_interspeech",
    "hazen06_interspeech",
    "chang06b_interspeech",
    "servan06_interspeech",
    "huang06c_interspeech",
    "demuynck06_interspeech",
    "lecouteux06_interspeech",
    "xue06_interspeech",
    "saleem06_interspeech",
    "hakamata06_interspeech",
    "ketabdar06_interspeech",
    "zhou06_interspeech",
    "nouza06_interspeech"
   ]
  },
  {
   "title": "Speech/Noise/Music Segmentation",
   "papers": [
    "zhang06f_interspeech",
    "schmalenstroeer06_interspeech",
    "ramirez06_interspeech",
    "jarifi06_interspeech",
    "heckmann06_interspeech",
    "anguera06b_interspeech",
    "coy06_interspeech",
    "chitturi06b_interspeech",
    "kim06e_interspeech",
    "myrvoll06_interspeech"
   ]
  },
  {
   "title": "Pitch Estimation",
   "papers": [
    "mooreii06_interspeech",
    "alias06b_interspeech",
    "malyska06_interspeech",
    "nakano06_interspeech",
    "zahorian06_interspeech",
    "rahman06_interspeech"
   ]
  },
  {
   "title": "Acoustic Modeling V - Novel Approaches",
   "papers": [
    "han06b_interspeech",
    "cincarek06_interspeech",
    "levy06_interspeech",
    "wachter06_interspeech",
    "deng06_interspeech",
    "hu06b_interspeech"
   ]
  },
  {
   "title": "Corpus-Based Synthesis",
   "papers": [
    "kirkpatrick06_interspeech",
    "sakai06_interspeech",
    "zhao06_interspeech",
    "conkie06_interspeech",
    "taylor06b_interspeech",
    "black06_interspeech"
   ]
  },
  {
   "title": "Spoken Dialog Technology R&D",
   "papers": [
    "rieser06_interspeech",
    "lewis06_interspeech",
    "goronzy06_interspeech",
    "pfleger06_interspeech",
    "subramanian06_interspeech",
    "moller06_interspeech"
   ]
  },
  {
   "title": "Modeling Speaker Emotional State",
   "papers": [
    "matthews06_interspeech",
    "nicolao06_interspeech",
    "cabral06_interspeech",
    "wu06d_interspeech",
    "yang06b_interspeech",
    "zhang06g_interspeech",
    "kao06_interspeech",
    "schuller06b_interspeech",
    "nisimura06_interspeech",
    "alm06_interspeech",
    "luo06_interspeech",
    "matsunaga06_interspeech",
    "tepperman06b_interspeech",
    "kumar06_interspeech",
    "nwe06_interspeech",
    "vasilescu06_interspeech"
   ]
  },
  {
   "title": "Language Modeling and ASR Applications",
   "papers": [
    "lavecchia06_interspeech",
    "chan06b_interspeech",
    "mori06_interspeech",
    "wiggers06_interspeech",
    "yamamoto06_interspeech",
    "hu06c_interspeech",
    "lakshmi06_interspeech",
    "woszczyna06_interspeech",
    "gerosa06_interspeech",
    "vertanen06_interspeech",
    "smidl06_interspeech",
    "balakrishna06_interspeech",
    "ju06_interspeech"
   ]
  },
  {
   "title": "Spoken Language Understanding",
   "papers": [
    "wu06e_interspeech",
    "stewart06_interspeech",
    "rosset06_interspeech",
    "prommer06_interspeech",
    "aist06_interspeech",
    "schiel06_interspeech",
    "ponbarry06_interspeech",
    "goel06_interspeech",
    "liu06e_interspeech",
    "holzapfel06_interspeech",
    "fukubayashi06_interspeech",
    "surendran06_interspeech"
   ]
  },
  {
   "title": "Segmentation and VAD",
   "papers": [
    "torre06b_interspeech",
    "shi06b_interspeech",
    "boakye06_interspeech",
    "kida06_interspeech",
    "lee06f_interspeech",
    "rifkin06_interspeech"
   ]
  },
  {
   "title": "Technologies for Specific Populations: Learners and Challenged",
   "papers": [
    "lee06g_interspeech",
    "neri06_interspeech",
    "dong06_interspeech",
    "trancoso06b_interspeech",
    "iida06_interspeech",
    "karpov06_interspeech"
   ]
  },
  {
   "title": "The Prosody of Turn-Taking and Dialog Acts",
   "papers": [
    "skantze06_interspeech",
    "ishi06_interspeech",
    "schlangen06_interspeech",
    "kolar06_interspeech",
    "ward06_interspeech",
    "edlund06_interspeech"
   ]
  },
  {
   "title": "Multichannel Speech Enhancement/Speech Perception",
   "papers": [
    "li06g_interspeech",
    "gerkmann06_interspeech",
    "chen06d_interspeech",
    "leukimmiatis06_interspeech",
    "flego06_interspeech",
    "abutalebi06_interspeech",
    "lovitt06_interspeech",
    "sakamoto06_interspeech",
    "li06h_interspeech",
    "liu06f_interspeech",
    "waltermann06_interspeech",
    "chen06e_interspeech"
   ]
  },
  {
   "title": "Diarization in ASR",
   "papers": [
    "ning06_interspeech",
    "zdansky06_interspeech",
    "gallardoantolin06_interspeech",
    "lopes06_interspeech",
    "pardo06_interspeech",
    "pham06_interspeech"
   ]
  },
  {
   "title": "Language Model Adaptation, Refinement, and Evaluation",
   "papers": [
    "suzuki06b_interspeech",
    "tam06_interspeech",
    "mrva06_interspeech",
    "klakow06_interspeech",
    "ljolje06b_interspeech",
    "nanavati06_interspeech"
   ]
  },
  {
   "title": "Voice Morphing",
   "papers": [
    "qin06_interspeech",
    "lee06h_interspeech",
    "nurminen06b_interspeech",
    "sundermann06_interspeech",
    "ohtani06_interspeech",
    "nakagiri06_interspeech",
    "saino06_interspeech",
    "uto06_interspeech",
    "tian06_interspeech",
    "nakano06b_interspeech",
    "shuang06_interspeech",
    "lin06c_interspeech",
    "lal06_interspeech"
   ]
  },
  {
   "title": "Prosody",
   "papers": [
    "ng06_interspeech",
    "campillo06_interspeech",
    "bailly06_interspeech",
    "kim06f_interspeech",
    "yi06c_interspeech",
    "ozturk06_interspeech",
    "dalen06_interspeech",
    "wang06g_interspeech",
    "lin06d_interspeech",
    "hirose06b_interspeech",
    "solorio06_interspeech",
    "wesseling06_interspeech"
   ]
  },
  {
   "title": "Discriminative Training",
   "papers": [
    "gibson06_interspeech",
    "du06_interspeech",
    "li06i_interspeech",
    "yu06b_interspeech",
    "li06j_interspeech",
    "wang06h_interspeech"
   ]
  },
  {
   "title": "Speech Synthesis",
   "papers": [
    "gibert06_interspeech",
    "tomokiyo06_interspeech",
    "tachibana06_interspeech",
    "polyakova06_interspeech",
    "toda06_interspeech",
    "langner06_interspeech"
   ]
  },
  {
   "title": "Multimodal Processing",
   "papers": [
    "alsaade06_interspeech",
    "pitsikalis06_interspeech",
    "hardison06_interspeech",
    "tamura06_interspeech",
    "almajai06_interspeech",
    "govokhina06_interspeech"
   ]
  },
  {
   "title": "Speech Analysis",
   "papers": [
    "biagetti06_interspeech",
    "jinachitra06_interspeech",
    "trmal06_interspeech",
    "mehta06_interspeech",
    "ezzat06_interspeech",
    "quintanamorales06_interspeech",
    "kameoka06_interspeech",
    "errity06_interspeech",
    "bouvrie06_interspeech",
    "takahashi06_interspeech",
    "prasad06_interspeech"
   ]
  },
  {
   "title": "Advances in Noisy ASR",
   "papers": [
    "lin06e_interspeech",
    "demange06_interspeech",
    "misra06_interspeech",
    "iwano06_interspeech",
    "hsu06b_interspeech",
    "ding06_interspeech",
    "lu06_interspeech",
    "kim06g_interspeech",
    "vali06_interspeech",
    "tai06_interspeech",
    "segbroeck06_interspeech",
    "kitaoka06_interspeech",
    "aradilla06_interspeech",
    "obuchi06b_interspeech"
   ]
  },
  {
   "title": "Source Separation and Localization",
   "papers": [
    "koldovsky06_interspeech",
    "liang06b_interspeech",
    "asano06_interspeech",
    "abad06_interspeech",
    "gehrig06b_interspeech",
    "heckmann06b_interspeech",
    "talantzis06_interspeech",
    "brutti06_interspeech",
    "radfar06_interspeech",
    "schmidt06_interspeech",
    "hu06d_interspeech",
    "attias06_interspeech",
    "visser06_interspeech",
    "olszewski06_interspeech"
   ]
  }
 ],
 "doi": "10.21437/Interspeech.2006"
}