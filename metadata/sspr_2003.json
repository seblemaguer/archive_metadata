{
 "title": "ISCA/IEEE Workshop on Spontaneous Speech Processing and Recognition",
 "location": "Tokyo Institute of Technology, Tokyo, Japan",
 "startDate": "13/4/2003",
 "endDate": "16/4/2003",
 "conf": "SSPR",
 "year": "2003",
 "name": "sspr_2003",
 "series": "",
 "SIG": "",
 "title1": "ISCA/IEEE Workshop on Spontaneous Speech Processing and Recognition",
 "date": "13-16 April 2003",
 "papers": {
  "furui03_sspr": {
   "authors": [
    [
     "Sadaoki",
     "Furui"
    ]
   ],
   "title": "Recent advances in spontaneous speech recognition and understanding",
   "original": "sspr_mmo1",
   "page_count": 6,
   "order": 1,
   "p1": "paper MMO1",
   "pn": "",
   "abstract": [
    "How to recognize and understand spontaneous speech is one of the most important issues in state-of-the-art speech recognition technology. In this context, a five-year large-scale national project entitled \"Spontaneous Speech: Corpus and Processing Technology\" started in Japan in 1999. This paper gives an overview of the project and reports on the major results of experiments that have been conducted so far at Tokyo Institute of Technology, including spontaneous presentation speech recognition, automatic speech summarization, and message-driven speech recognition. The paper also discusses the most important research problems to be solved in order to achieve ultimate spontaneous speech recognition systems.\n",
    ""
   ]
  },
  "maekawa03_sspr": {
   "authors": [
    [
     "Kikuo",
     "Maekawa"
    ]
   ],
   "title": "Corpus of spontaneous Japanese: its design and evaluation",
   "original": "sspr_mmo2",
   "page_count": 6,
   "order": 2,
   "p1": "paper MMO2",
   "pn": "",
   "abstract": [
    "Corpus of Spontaneous Japanese, or CSJ, is a large-scale database of spontaneous Japanese. It contains speech signal and transcription of about 7 million words along with various annotations like POS and phonetic labels. After describing its design issues, preliminary evaluation of the CSJ was presented. The results suggest strongly the usefulness of the CSJ as the resource for the study of spontaneous speech.\n",
    ""
   ]
  },
  "isahara03_sspr": {
   "authors": [
    [
     "Hitoshi",
     "Isahara"
    ]
   ],
   "title": "Corpus and text analysis of spontaneous Japanese",
   "original": "sspr_mmo3",
   "page_count": 4,
   "order": 3,
   "p1": "paper MMO3",
   "pn": "",
   "abstract": [
    "There are three major parts of the \"Spontaneous Speech: Corpus and Processing Technology\" project; (1) compilation of large spontaneous speech corpus, (2) establishment of spoken language engineering based on the corpus, and (3) developing a prototype of a spoken language summarization system. This paper describes how we help to develop this large corpus, i.e., (1), using technology developed as a part of (2). Firstly, we discuss how to annotate whole corpus morphologically. Secondly, we explain how we annotate sentence boundaries. And thirdly we discuss discourse annotation for CSJ. This paper describes overviews of these works and details of the works described in this paper are explained in the other papers in this volume.\n",
    ""
   ]
  },
  "peng03_sspr": {
   "authors": [
    [
     "Shu-Hui",
     "Peng"
    ],
    [
     "Mary E.",
     "Beckman"
    ]
   ],
   "title": "Annotation conventions and corpus design in the investigation of spontaneous speech prosody in Taiwanese",
   "original": "sspr_mao1",
   "page_count": 6,
   "order": 4,
   "p1": "paper MAO1 (invited paper)",
   "pn": "",
   "abstract": [
    "Understanding how intonational phrasing and focal prominence interact with  lexically specified tone patterns is one of several problems in the investigation  of speech processing in Chinese languages that cannot be addressed fully with  read speech alone. This paper explores such problems for Taiwanese, one of  the major languages in the southern Min dialect group. It outlines what is  known about Taiwanese prosody and describes prosodic annotation conventions  currently under development. It illustrates the problems with examples from  a corpus of spontaneous speech dialogues and read versions of sentences extracted  from the dialogue transcripts.\n",
    ""
   ]
  },
  "tseng03_sspr": {
   "authors": [
    [
     "Shu-Chuan",
     "Tseng"
    ]
   ],
   "title": "Taxonomy of spontaneous speech phenomena in Mandarin conversation",
   "original": "sspr_mao2",
   "page_count": 4,
   "order": 5,
   "p1": "paper MAO2",
   "pn": "",
   "abstract": [
    "Spontaneous speech raises a number of research issues which cannot be observed in other types of speech data. Disfluent speech, ill-formed sequences and particular pronunciation variations mark the most important facet of spontaneous speech. The goal of this paper is to provide a taxonomy scheme of spontaneous speech phenomena, which offers the necessary basis for research works and applications dealing with spontaneous speech. The proposed taxonomy focuses on the phonetic and structural peculiarity and subsequently, the adequacy of this taxonomy is evaluated by empirically examining a corpus of annotated Mandarin data.\n",
    ""
   ]
  },
  "yoneyama03_sspr": {
   "authors": [
    [
     "Kiyoko",
     "Yoneyama"
    ],
    [
     "Hanae",
     "Koiso"
    ],
    [
     "Janice",
     "Fon"
    ]
   ],
   "title": "A corpus-based analysis on prosody and discourse structure in Japanese spontaneous monologues",
   "original": "sspr_mao3",
   "page_count": 4,
   "order": 6,
   "p1": "paper MAO3",
   "pn": "",
   "abstract": [
    "The aim of this paper is two folds. First, the paper attempts to investigate prosody and discourse structure in Japanese spontaneous monologues by using the prosodic labels of the Corpus of Spontaneous Japanese (CSJ). The analyses of F0 peak trends and prosodic breaks confirmed previous findings in [1]. Secondly, the paper attempts to evaluate the validity of prosodic labels of the X-JToBI system that was adopted in CSJ (pitch-accents, boundary tones and break indices). The results of the analyses using three types of prosodic labels in the X-JToBI system provide evidence that the prosodic labels in CSJ can be useful for studies on discourse structures of Japanese spontaneous speech. Reference\n",
    "Y-J.J. Fon, A Cross-linguistic study on syntactic and discourse boundary cues in spontaneous speech, Unpublished doctoral dissertation, The Ohio State University, Columbus, OH, U.S.A., 2002.\n",
    ""
   ]
  },
  "swerts03_sspr": {
   "authors": [
    [
     "Marc",
     "Swerts"
    ],
    [
     "Hanne",
     "Kloots"
    ],
    [
     "Steven",
     "Gillis"
    ],
    [
     "Georges de",
     "Schutter"
    ]
   ],
   "title": "Vowel Reduction in spontaneous spoken Dutch",
   "original": "sspr_mao4",
   "page_count": 4,
   "order": 7,
   "p1": "paper MAO4",
   "pn": "",
   "abstract": [
    "This paper reports on a study of vowel reduction in contemporary Standard Dutch. The focus is on the first, unstressed syllable of four bisyllabic Dutch words: moment, manier, probeer(t) and docent. Vowel reduction is studied in a corpus of spontaneously spoken Standard Dutch, produced by 80 Flemish and 80 Dutch teachers of Dutch. Three labelers independently evaluated and scored the stimuli on a seven point scale with long vowel and complete deletion as its extreme values. Three main types of vowel reduction were distinguished: reduction to schwa (e.g. moment > m[@]ment), vowel shortening (e.g. m[o:]ment > m[o]ment) and complete reduction (e.g. moment > ment). Short vowels appeared to be most frequent, especially in Flanders. Reduction to schwa and complete reduction only occurred in the Netherlands. The Dutch material also supports the assumption that in highfrequency words vowels are more easily reduced than in words with a lower frequency.\n",
    ""
   ]
  },
  "greenberg03_sspr": {
   "authors": [
    [
     "Steven",
     "Greenberg"
    ],
    [
     "Hannah",
     "Carvey"
    ],
    [
     "Leah",
     "Hitchcock"
    ],
    [
     "Shuangyu",
     "Chang"
    ]
   ],
   "title": "The phonetic patterning of spontaneous American English discourse",
   "original": "sspr_mao5",
   "page_count": 4,
   "order": 8,
   "p1": "paper MAO5",
   "pn": "",
   "abstract": [
    "Statistical analysis of a manually annotated, 45-minute subset of the SWITCHBOARD corpus indicates that pronunciation variation observed in spontaneous American English discourse is highly structured at the level of the syllable, particularly when prosodic stress accent (i.e., syllable prominence) is taken into account. The pattern of segmental substitutions and deletions observed are largely associated with different constituents within the syllable (nuclei and codas, respectively); their frequency of occurrence is inversely proportional to stress-accent magnitude. The phonetic identity of vocalic nuclei is also related to stress accent, as is the probability of segmental deletion in the coda. Such data imply that \"information\" governs much of the phonetic patterning of spoken language characteristic of the real world.\n",
    ""
   ]
  },
  "lima03_sspr": {
   "authors": [
    [
     "Carlos",
     "Lima"
    ],
    [
     "Luís B.",
     "Almeida"
    ],
    [
     "Adriano",
     "Tavares"
    ],
    [
     "Carlos",
     "Silva"
    ]
   ],
   "title": "Spectral multi-normalisation for robust speech recognition",
   "original": "sspr_map1",
   "page_count": 4,
   "order": 9,
   "p1": "paper MAP1",
   "pn": "",
   "abstract": [
    "This paper presents an improved version of a spectral normalisation based method for extraction of speech robust features in additive noise. The baseline normalisation method was developed by taking into consideration that, while the speech regions with less energy need more robustness, since in these regions the noise is more dominant, the \"peaked\" spectral regions which are the most reliable due to the higher speech energy must also be preserved as much as possible by the feature extraction process.\n",
    "The additive noise effect tends to flatten the \"peaked\" spectral zones while the spectral zones of less energy are usually raised.\n",
    "The algorithm proposed in this paper showed to alleviate the noise effect by emphasising the voiced nature of the speech signal by raising the spectral \"peaks\", which are \"flatten\" by the noise effect. The clean speech database is assumed as lightly contaminated, the additive noise is estimated in a frame by frame basis and then used to restore both the \"peaked\" and the flat spectral zones of the speech spectrum.\n",
    ""
   ]
  },
  "jitsuhiro03_sspr": {
   "authors": [
    [
     "Takatoshi",
     "Jitsuhiro"
    ],
    [
     "Tomoko",
     "Matsui"
    ],
    [
     "Satoshi",
     "Nakamura"
    ]
   ],
   "title": "A Successive state splitting algorithm based on the MDL Criterion by data-driven and decision tree clustering",
   "original": "sspr_map2",
   "page_count": 4,
   "order": 10,
   "p1": "paper MAP2",
   "pn": "",
   "abstract": [
    "We propose a new Successive State Splitting (SSS) algorithm based on the Minimum Description Length (MDL) criterion to design tied-state HMM topologies automatically. The SSS algorithm is a mechanism for creating both temporal and contextual variations based on the Maximum Likelihood (ML) criterion. However, it also needs to empirically predetermine control parameters for use as stop criteria, for example, the total number of states. We introduce the MDL criterion to the ML-SSS algorithm so that it can automatically create proper topologies without such parameters. Experimental results show that our extended algorithm can automatically stop splitting and obtain more appropriate HMM topologies than the original one. We also extend the MDL-SSS algorithm by using phonetic decision tree clustering for contextual splitting. A method using a combination of phonetic decision tree clustering and data-driven clustering can automatically obtain almost the same performance as the original method.\n",
    ""
   ]
  },
  "watanabe03_sspr": {
   "authors": [
    [
     "Shinji",
     "Watanabe"
    ],
    [
     "Yasuhiro",
     "Minami"
    ],
    [
     "Atsushi",
     "Nakamura"
    ],
    [
     "Naonori",
     "Ueda"
    ]
   ],
   "title": "Bayesian acoustic modeling for spontaneous speech recognition",
   "original": "sspr_map3",
   "page_count": 4,
   "order": 11,
   "p1": "paper MAP3",
   "pn": "",
   "abstract": [
    "Accurate acoustic model construction for spontaneous speech recognition requires that various speech fluctuation factors such as speaking variations and speaker variances are dealt with. The Bayesian approach has advantages for the speech fluctuation modeling because it enables an appropriate model selection for given speech data, unlike the maximum likelihood approach. However, the Bayesian approach includes complicated integrals that have prevented it from being realized in a large-scale task such as spontaneous speech recognition. In this paper, we apply a practical Bayesian framework: Variational Bayesian Estimation and Clustering for speech recognition (VBEC), to spontaneous speech recognition. In particular, we focus on the selection of an appropriate acoustic model structure. The effectiveness of the VBEC is shown through recognition experiments using real spontaneous speech data.\n",
    ""
   ]
  },
  "yu03_sspr": {
   "authors": [
    [
     "Hua",
     "Yu"
    ],
    [
     "Alex",
     "Waibel"
    ]
   ],
   "title": "Flexible parameter tying for conversational speech recognition",
   "original": "sspr_map4",
   "page_count": 4,
   "order": 12,
   "p1": "paper MAP4",
   "pn": "",
   "abstract": [
    "Modeling pronunciation variation is key for recognizing conversational speech. Previous efforts on pronunciation modeling by modifying dictionaries only yielded marginal improvement. Due to complex interaction between dictionaries and acoustic models, we believe a pronunciation modeling scheme is plausible only when closely coupled with the underlying acoustic model. This paper explores the use of flexible parameter tying for pronunciation modeling. In particular, two new techniques are investigated: Gaussian tying and flexible tree clustering. We report a 1.3% absolute WER improvement over the traditional modeling framework on the Switchboard task.\n",
    ""
   ]
  },
  "lee03_sspr": {
   "authors": [
    [
     "Kyong-Nim",
     "Lee"
    ],
    [
     "Minhwa",
     "Chung"
    ]
   ],
   "title": "Variants modeling in Korean spontaneous speech recognition",
   "original": "sspr_map5",
   "page_count": 4,
   "order": 13,
   "p1": "paper MAP5",
   "pn": "",
   "abstract": [
    "Pronunciation variants in spontaneous speech tend to be more variable  in planned speech. Spontaneous speech has significant sources of variations  as well as serious phonological variations, which make recognition  extremely difficult. In this paper, we analyzed the auditory  transcriptions of the dialogue for spontaneous speech recognition,  and then classified the characteristics of conversational speech.  To deal with these characteristics, we first used the special  garbage model, the silence model and the filled pause model for  the improvement the acoustic model; second, we optimized the  multiple alternative pronunciations using the pruning method.  Finally, for reflecting on freely the phonological variation,  we enhanced the pronunciation lexicon by adding alternative  pronunciation based on the frequently used phonological variants.  Experimental results showed that modeling of garbage, silence,  and filled pause reduce word error rate by a relatively 4.9%,  while pruning the lexicon and adding the alternative pronunciation  reduced word error rate by relatively 0.8%.\n",
    ""
   ]
  },
  "tsai03_sspr": {
   "authors": [
    [
     "Ming-yi",
     "Tsai"
    ],
    [
     "Lin-shan",
     "Lee"
    ]
   ],
   "title": "Pronunciation modeling for spontaneous speech by maximizing word correct rate in a production-recognition model",
   "original": "sspr_map6",
   "page_count": 4,
   "order": 14,
   "p1": "paper MAP6",
   "pn": "",
   "abstract": [
    "In this paper, we develop a new method for compiling a pronunciation dictionary to model pronunciation variation in spontaneous speech recognition. The pronunciation dictionary is assembled by iteratively selecting pronunciations from a datadriven word confusion table, based on directly maximizing the word correct rate simulated by a production-recognition model such that the optimal performance of recognition can be achieved. In other words, the compiled pronunciation dictionary can not only accommodate as many as necessary pronunciations but also avoid possible introduced confusion during recognition. The simulation of word correct rate is performed with a novel human-machine communication model, consisting of a human speech production module and a machine speech recognition module. Our experimental results on LDC Mandarin Call Home and Call Friend corpora showed that significant improvement is achieved with this new approach. Furthermore, the framework and theory presented here are applicable to other languages.\n",
    ""
   ]
  },
  "ikeno03_sspr": {
   "authors": [
    [
     "Ayako",
     "Ikeno"
    ],
    [
     "Bryan",
     "Pellom"
    ],
    [
     "Dan",
     "Cer"
    ],
    [
     "Ashley",
     "Thornton"
    ],
    [
     "Jason M.",
     "Brenier"
    ],
    [
     "Dan",
     "Jurafsky"
    ],
    [
     "Wayne",
     "Ward"
    ],
    [
     "William",
     "Byrne"
    ]
   ],
   "title": "Issues in recognition of Spanish-accented spontaneous English",
   "original": "sspr_map7",
   "page_count": 4,
   "order": 15,
   "p1": "paper MAP7",
   "pn": "",
   "abstract": [
    "We describe a recognition experiment and two analytic experiments on a database of strongly Hispanic-accented English. We show the crucial importance of training on the Hispanicaccented data for acoustic model performance, and describe the tendency of Spanish-accented speakers to use longer, and presumably less-reduced, schwa vowels than native-English speakers.\n",
    ""
   ]
  },
  "qu03_sspr": {
   "authors": [
    [
     "Dan",
     "Qu"
    ],
    [
     "Bingxi",
     "Wang"
    ]
   ],
   "title": "Discriminative training of GMM for language identification",
   "original": "sspr_map8",
   "page_count": 4,
   "order": 16,
   "p1": "paper MAP8",
   "pn": "",
   "abstract": [
    "In this paper, a discriminative training procedure for a Gaussian Mixture Model (GMM) language identification system is described. The proposal is based on the Generalized Probabilistic Descent (GPD) algorithm and Minimum Classification Error Rates formulated to estimate the GMM parameters. The evaluation is conducted using the OGI multi-language telephone speech corpus. The experimental results show such system is very effective in language identification tasks.\n",
    ""
   ]
  },
  "yokoyama03_sspr": {
   "authors": [
    [
     "T.",
     "Yokoyama"
    ],
    [
     "Takahiro",
     "Shinozaki"
    ],
    [
     "K.",
     "Iwano"
    ],
    [
     "Sadaoki",
     "Furui"
    ]
   ],
   "title": "Unsupervised language model adaptation using word classes for spontaneous speech recognition",
   "original": "sspr_map9",
   "page_count": 4,
   "order": 17,
   "p1": "paper MAP9",
   "pn": "",
   "abstract": [
    "This paper proposes an unsupervised, batch-type, class-based language model adaptation method for spontaneous speech recognition. The word classes are automatically determined by maximizing the average mutual information between the classes using a training set. A class-based language model is built based on recognition hypotheses obtained using a general word-based language model, and linearly interpolated with that general language model. All the input utterances are re-recognized using the adapted language model. It was confirmed that the proposed method is effective in improving the recognition accuracy in spontaneous presentation recognition. The proposed method was combined with acoustic model adaptation, and it was found that the effects of language model adaptation and acoustic model adaptation are additive. The optimum number of classes is 100 irrespective of whether the acoustic model adaptation is combined or not, and in this condition the language model adaptation yields approximately 2% absolute value improvement in the word accuracy.\n",
    ""
   ]
  },
  "nanjo03_sspr": {
   "authors": [
    [
     "Hiroaki",
     "Nanjo"
    ],
    [
     "Tatsuya",
     "Kawahara"
    ]
   ],
   "title": "Unsupervised language model adaptation for lecture speech recognition",
   "original": "sspr_map10",
   "page_count": 4,
   "order": 18,
   "p1": "paper MAP10",
   "pn": "",
   "abstract": [
    "This paper addresses speaker adaptation of language model in large vocabulary spontaneous speech recognition. In spontaneous speech, the expression and pronunciation of words vary a lot depending on the speaker and topic. Therefore, we present unsupervised methods of language model adaptation to a specific speaker by (1) making direct use of the initial recognition result for generating an enhanced model, and (2) selecting similar texts, utterance by utterance, based on the model. We also investigate the pronunciation variation modeling and its adaptation in the same framework. It is confirmed that all proposed adaptation methods and their combinations reduced the perplexity and word error rate in transcription of real lectures.\n",
    ""
   ]
  },
  "akita03_sspr": {
   "authors": [
    [
     "Yuya",
     "Akita"
    ],
    [
     "Masafumi",
     "Nishida"
    ],
    [
     "Tatsuya",
     "Kawahara"
    ]
   ],
   "title": "Automatic transcription of discussions using unsupervised speaker indexing",
   "original": "sspr_map11",
   "page_count": 4,
   "order": 19,
   "p1": "paper MAP11",
   "pn": "",
   "abstract": [
    "We present unsupervised speaker indexing combined with automatic speech recognition (ASR) for speech archives such as discussions. Our proposed indexing method is based on anchor models, by which we define a feature vector based on the similarity with speakers of a large scale speech database, and we incorporate several techniques to improve discriminant ability. ASR is performed using the results of this indexing. No discussion corpus is available to train acoustic and language models. So we applied the speaker adaptation technique to the baseline acoustic model based on the indexing. We also constructed a language model by merging two models that cover different linguistic features. We achieved the speaker indexing accuracy of 93% and the word recognition accuracy of 57% for real discussion data.\n",
    ""
   ]
  },
  "psutka03_sspr": {
   "authors": [
    [
     "Josef",
     "Psutka"
    ],
    [
     "J. V.",
     "Psutka"
    ],
    [
     "Pavel",
     "Ircing"
    ],
    [
     "Jan",
     "Hoidekr"
    ]
   ],
   "title": "Recognition of spontaneously pronounced TV ice-hockey commentary",
   "original": "sspr_map12",
   "page_count": 4,
   "order": 20,
   "p1": "paper MAP12",
   "pn": "",
   "abstract": [
    "This paper describes our effort with an automatic transcription of TV ice-hockey commentaries1. The ice-hockey matches were played during the World Championships 2000 and 2001 in St. Petersburg (Russia) and Hannover (Germany), respectively and were transmitted by the Czech TV channels NOVA and CTV1 with an accompanying commentary of Robert Záruba. Annotation  rules designed for the processing of a commentary comprising specific background noise are formed and a list of non-speech events is proposed. A baseline ASR system was built using commentaries of 15 training matches and tested using utterances randomly selected from 4 tested matches. Several types of adaptive lexicons were designed to decrease a number of OOV words and improve recognition accuracy.\n",
    ""
   ]
  },
  "lucke03_sspr": {
   "authors": [
    [
     "H.",
     "Lucke"
    ],
    [
     "H.",
     "Honda"
    ],
    [
     "K.",
     "Minamino"
    ],
    [
     "A.",
     "Hiroe"
    ],
    [
     "H.",
     "Mori"
    ],
    [
     "H.",
     "Ogawa"
    ],
    [
     "Y.",
     "Asano"
    ],
    [
     "H.",
     "Kishi"
    ]
   ],
   "title": "Development of a spontaneous speech recognition engine for an entertainment robot",
   "original": "sspr_map13",
   "page_count": 4,
   "order": 21,
   "p1": "paper MAP13",
   "pn": "",
   "abstract": [
    "Natural speech interaction is a difficult, yet important, capability for a social humanoidal robot. We address the problems of spontaneous speaking style in a real environment and report on our progress of developing a robust large vocabulary speech recognition engine for an anthropomorphic entertainment robot SDR-4X.\n",
    ""
   ]
  },
  "kitaoka03_sspr": {
   "authors": [
    [
     "Norihide",
     "Kitaoka"
    ],
    [
     "Masahisa",
     "Shingu"
    ],
    [
     "Seiichi",
     "Nakagawa"
    ]
   ],
   "title": "Effects of acoustic and language knowledge of human and automatic speech recognizer on spontaneous speech perception/recognition",
   "original": "sspr_map14",
   "page_count": 4,
   "order": 22,
   "p1": "paper MAP14",
   "pn": "",
   "abstract": [
    "An automatic speech recognizer uses acoustic knowledge and linguistic knowledge in large vocabulary speech recognition,  acoustic knowledge is modeled by hidden Markov models (HMM), linguistic knowledge is modeled by N-gram (typically bi-gram or trigram), and these models are stochastically  integrated. lt is thought that humans also integrate acoustic and linguistic knowledge of speech when perceiving  continuous speech. Automatic speech recognition with HMM and N-gram is thought to roughly model the process of human pereeption.\n",
    "Although these models have drastically improved the performanee of automatic speech recognition of well-formed read speech so far, they cannot deliver sufficient performance  on spontaneous speech recognition tasks because of various particular phenomena of spontanous speech.\n",
    "In this paper, we conducted simulation experiments of N-gram language models by combining human acoustic knowledge  and instruction of local context and assured that using two words neighboring the target word was enough to  improve the performance of recognition when we could use only local information as linguistic knowledge. We also  assured that coarticulation affected the perception of syllahles.\n",
    "We then compared some language models on speech recognizer. We calculated acoustic scores with HMM and then linguistic scores calculated from a language model were added. We obtained 37.5% recognition rate only with acoustic model, whereas we obtained 51.0% with both acoustic  and language models, thus the relative performance improvement was 36%. We obtained a l6.5% recognition rate only with the language model, so the acoustic model  improved the performance relatively 209%. Thus, the  improvement of the acoustic models is more effective than that of the language model.\n",
    ""
   ]
  },
  "shinozaki03_sspr": {
   "authors": [
    [
     "Takahiro",
     "Shinozaki"
    ],
    [
     "Sadaoki",
     "Furui"
    ]
   ],
   "title": "An assessment of automatic recognition techniques for spontaneous speech in comparison with human performance",
   "original": "sspr_map15",
   "page_count": 4,
   "order": 23,
   "p1": "paper MAP15",
   "pn": "",
   "abstract": [
    "To investigate problems of spontaneous speech recognition using N-grams and HMMs and estimate the room for improvement in the recognition rate, an automatic speech recognizer is evaluated in comparison with performances by human listeners. The evaluation task is to recognize spontaneous speech presentations from the Corpus of Spontaneous Japanese. Both the automatic recognizer and human listeners are requested to choose the most likely word from a dictionary, given a speech signal with a three word length including ± one word context extracted from a presentation. Recognition performances are compared using the same criteria for both experiments. The results show that recognition error rate by human listeners is roughly half of that by the recognizer. By examining words that are easy for humans but difficult for the recognizer, it is found that causes of the recognition errors by the decoder include insufficiency of model accuracy and lack of robustness against vague and variable pronunciations.\n",
    ""
   ]
  },
  "nitta03_sspr": {
   "authors": [
    [
     "Tsuneo",
     "Nitta"
    ],
    [
     "Shingo",
     "Iseji"
    ],
    [
     "Takashi",
     "Fukuda"
    ],
    [
     "Hirobumi",
     "Yamada"
    ],
    [
     "Kouichi",
     "Katsurada"
    ]
   ],
   "title": "Key-word spotting using phonetic distinctive features extracted from output of an LVCSR engine",
   "original": "sspr_map16",
   "page_count": 4,
   "order": 24,
   "p1": "paper MAP16",
   "pn": "",
   "abstract": [
    "In this paper, we attempt to adopt a general-purpose LVCSR engine designed for dictation as a spoken dialogue recognition system. In the proposed system, a phoneme string output from the LVCSR engine is converted into a sequence of vectors represented with distinctive features (DF), then keywords assigned by a dialogue manager are detected from the input vector sequence using dynamic time warping (DTW). The proposed system takes advantage of the potential abilities of: (1) precise phoneme discrimination achieved by relaxing the linguistic constraint in the LVCSR engine, and (2) coping with the issues of substitution, deletion and insertion errors by combining a process of conversion from a phoneme into a distinctive feature vector and a key-word spotting process. The proposed system is compared with the general-purpose LVCSR engine in an experiment with a spoken dialogue corpus of a map guidance task and shows significant improvements. Comparative studies on language models and acoustic scoring procedure in key-word detection are also discussed with sub-word model and with confusion matrix, respectively.\n",
    ""
   ]
  },
  "zhang03_sspr": {
   "authors": [
    [
     "Tong",
     "Zhang"
    ],
    [
     "Mark",
     "Hasegawa-Johnson"
    ],
    [
     "Stephen E.",
     "Levinson"
    ]
   ],
   "title": "Mental state detection of dialogue system users via spoken language",
   "original": "sspr_map17",
   "page_count": 4,
   "order": 25,
   "p1": "paper MAP17",
   "pn": "",
   "abstract": [
    "This paper presents an approach to simulate the mental activities of children during their interaction with computers through their spoken language. The mental activities are categorized into three states: confidence, confusion and frustration. Two knowledge sources are used in the detection. One is prosody, which indicates utterance type and userís attitude. The other is embedded key words/phrases which help interpret the utterances. Moreover, it is found that childrenís speech exhibits very different acoustic characteristics from adults. Given the uniqueness of childrenís speech, this paper applies a vocal-tract-length-normalization (VTLN)-based technique to compensate for both inter-speaker variability and intraspeaker variability in childrenís speech. The detected key words/phrases are then integrated with prosodic information as the cues for the MAP decision of mental states. Tests on a set of 50 utterances collected from the project experiment showed the classification accuracy was 74%.\n",
    ""
   ]
  },
  "araki03_sspr": {
   "authors": [
    [
     "Masahiro",
     "Araki"
    ],
    [
     "Johji",
     "Kurahashi"
    ],
    [
     "Takeo",
     "Matsumoto"
    ]
   ],
   "title": "Extreme experiment (XE) method for developing mixed initiative dialogue systems",
   "original": "sspr_map18",
   "page_count": 4,
   "order": 26,
   "p1": "paper MAP18",
   "pn": "",
   "abstract": [
    "In this paper, we propose a new spontaneous speech collection method and a methodology for rapid test-andmodify development of mixed initiative spoken dialogue systems. Our method uses VoiceXML environment for helping WOZ experiments and uses tool suite for analyzing the users utterance and for giving feedback to the systems. The core techniques of our method are (1) bootstrapping by Web application framework, (2) WOZ data collection using VoiceXML platform and (3) log analyzing tools for feedback.\n",
    ""
   ]
  },
  "hori03_sspr": {
   "authors": [
    [
     "Chiori",
     "Hori"
    ],
    [
     "Takaaki",
     "Hori"
    ],
    [
     "Hideki",
     "Isozaki"
    ],
    [
     "Eisaku",
     "Maeda"
    ],
    [
     "Shigeru",
     "Katagiri"
    ],
    [
     "Sadaoki",
     "Furui"
    ]
   ],
   "title": "Study on spoken interactive open domain question answering",
   "original": "sspr_map19",
   "page_count": 4,
   "order": 27,
   "p1": "paper MAP19",
   "pn": "",
   "abstract": [
    "This paper proposes an interactive approach to spoken interactive open-domain question answering (ODQA) systems. The goal of ODQA systems is to extract an exact answer to users question from unstructured information sources such as large text corpora. When the reliabilities for answer hypotheses obtained by an ODQA system are low, systems need more information to effectively distinguish the exact answer required by users. In our spoken interactive ODQA system, SPIQA, spoken questions are recognized by an automatic speech recognition (ASR) system and disambiguous queries (DQs) are automatically generated to disambiguate transcribed questions. To derive appropriate DQs, ambiguous information is detected based on recognition reliability, dependency structures between phrases in the users questions, and features of word occurrence in the retrieved corpus. We confirmed the appropriateness of the derived DQs by comparing them with manually prepared ones. We also reconstructed the questions manually using additional information that was required by the DQs. We then tested the effect of the additional information on the performance of our ODQA system.\n",
    ""
   ]
  },
  "nagarajan03_sspr": {
   "authors": [
    [
     "T.",
     "Nagarajan"
    ],
    [
     "Hema A.",
     "Murthy"
    ],
    [
     "Rajesh M.",
     "Hegde"
    ]
   ],
   "title": "Group delay based segmentation of spontaneous speech into syllable-like units",
   "original": "sspr_map20",
   "page_count": 4,
   "order": 28,
   "p1": "paper MAP20",
   "pn": "",
   "abstract": [
    "In the development of a syllable-centric ASR system, segmentation of the acoustic signal into syllabic units is an important stage. This paper presents a minimum phase group delay based approach to segment spontaneous speech into syllable-like units. Here, three different minimum phase signals are derived from the short term energy functions of three sub-bands of speech signals, as if it were a magnitude spectrum. The experiments are carried out on Switchboard corpus and the error in segmentation is found to be utmost 40msec for 85% of the syllable segments, in addition to 5.25% insertions and 7.10% deletions.\n",
    ""
   ]
  },
  "hirschberg03_sspr": {
   "authors": [
    [
     "Julia",
     "Hirschberg"
    ],
    [
     "Jackson",
     "Liscombe"
    ],
    [
     "Jennifer",
     "Venditti"
    ]
   ],
   "title": "Experiments in emotional speech",
   "original": "sspr_tmo1",
   "page_count": 7,
   "order": 29,
   "p1": "paper TMO1 (keynote paper)",
   "pn": "",
   "abstract": [
    "Speech is a rich source of information, not only about what a speaker says, but also about what the speakers attitude is toward the listener and toward the topic under discussion - as well as the speakers own current state of mind. Until recently, most research on spoken language systems has focused on propositional content: what words is the speaker producing? Currently there is considerable interest in going beyond mere words to discover the semantic content of utterances. However, we believe it is important to go beyond mere semantic content, in order to fully interpret what human listeners infer from listening to other humans.\n",
    "In this paper we present results from some recent and ongoing experiments in the study of emotional speech. In Section 1 we discuss previous research in this area, and in Section 2 we describe a recent and several planned experiments addressing important methodological issues in the study of emotional speech. We conclude in Section 4 with remarks on the ultimate application of results from these experiments to the automatic identification of emotion in speech.\n",
    ""
   ]
  },
  "waibel03_sspr": {
   "authors": [
    [
     "Alex",
     "Waibel"
    ],
    [
     "Ivica",
     "Rogina"
    ]
   ],
   "title": "Advances in ISL's lecture and meeting trackers",
   "original": "sspr_tmo2",
   "page_count": 4,
   "order": 30,
   "p1": "paper TMO2 (keynote paper)",
   "pn": "",
   "abstract": [
    "Most speech applications to-date have attempted to provide a more natural interface for human-computer interaction or human-computer data-input. Only recently, a whole new class of application is coming to the fore: computer enhanced human-human interaction. In these applications the computer is no longer addressed directly, but must observe, process and understand the interactions between humans in a room. In this paper we discuss two such applications: a meeting browser that observes and tracks meetings for later review and summarization, and a lecture tracker that provides not only summarization, but also implicit services during a presentation, such as control of AV equipmend and selection of the most suitable slides. Processing human-human conversational speech under unpredictable recording conditions and vocabularies presents new challenges for speech and language processing. We describe techniques designed to overcome these difficulties and report speech recognition as well as overall system performance results.\n",
    ""
   ]
  },
  "rigoll03_sspr": {
   "authors": [
    [
     "Gerhard",
     "Rigoll"
    ]
   ],
   "title": "An overview on European projects related to spontaneous speech recognition",
   "original": "sspr_tmo3",
   "page_count": 4,
   "order": 31,
   "p1": "paper TMO3 (invited paper)",
   "pn": "",
   "abstract": [
    "The purpose of this paper is the presentation of a survey on activities in spontaneous speech recognition in Europe over the past 10 years. A brief historical review on the development of this topic in Europe is presented first, and then a few technical issues are addressed, that distinguish research projects on spontaneous speech recognition from other research activities in speech. Some of the major projects that have been carried out in the last years are then briefly presented and finally, the paper concludes with an outlook on the future of research in spontaneous speech recognition in Europe.\n",
    ""
   ]
  },
  "kawahara03_sspr": {
   "authors": [
    [
     "Tatsuya",
     "Kawahara"
    ],
    [
     "Hiroaki",
     "Nanjo"
    ],
    [
     "Takahiro",
     "Shinozaki"
    ],
    [
     "Sadaoki",
     "Furui"
    ]
   ],
   "title": "Benchmark test for speech recognition using the corpus of spontaneous Japanese",
   "original": "sspr_tmo4",
   "page_count": 4,
   "order": 32,
   "p1": "paper TMO4",
   "pn": "",
   "abstract": [
    "We present benchmark results of automatic speech recognition using the Corpus of Spontaneous Japanese (CSJ), which has been developed in the five-year national project and will be the largest spontaneous speech databases. New test-sets are designed for both academic presentation speech and extemporaneous public speech, which are the two major categories in the corpus. The testsets are selected to cover the variation of acoustic and linguistic factors in spontaneous speech: word perplexity, degree of disfluency, and the speaking rate. Baseline acoustic and language models are set up using an almost complete set (500 hours and 6.67M words) of the CSJ. Statistical modeling of pronunciation variation is also incorporated into the language model based on the alignment of large-scale transcriptions. The benchmark results verified the effects of the factors considered in the test-set design.\n",
    ""
   ]
  },
  "schwenk03_sspr": {
   "authors": [
    [
     "Holger",
     "Schwenk"
    ],
    [
     "Jean-Luc",
     "Gauvain"
    ]
   ],
   "title": "Using continuous space language models for conversational speech recognition",
   "original": "sspr_tmo5",
   "page_count": 4,
   "order": 33,
   "p1": "paper TMO5",
   "pn": "",
   "abstract": [
    "Language modeling for conversational speech suffers from the limited amount of available adequate training data. This paper describes a new approach that performs the estimation of the language model probabilities in a continuous space, allowing by these means smooth interpolation of unobserved n-grams. This continuous space language model is used during the last decoding pass of a state-of-the-art conversational telephone speech recognizer to rescore word lattices. For this type of speech data, it achieves consistent word error reductions of more than 0.4% compared to a carefully tuned backoff n-gram language model.\n",
    ""
   ]
  },
  "schramm03_sspr": {
   "authors": [
    [
     "Hauke",
     "Schramm"
    ],
    [
     "Xavier L.",
     "Aubert"
    ],
    [
     "Carsten",
     "Meyer"
    ],
    [
     "Jochen",
     "Peters"
    ]
   ],
   "title": "Filled-pause modeling for medical transcriptions",
   "original": "sspr_tmo6",
   "page_count": 4,
   "order": 34,
   "p1": "paper TMO6",
   "pn": "",
   "abstract": [
    "We present our recent progress in filled pause (FP) modeling for a highly spontaneous medical transcription task. Our studies con- firm that FP modeling is an important topic for spontaneous speech applications, which must be explicitly addressed in acoustic, lexical, and language modeling. We provide a framework for datadriven lexical modeling of FP acoustic variability with respect to phonemic realization and duration. By using a number of properly weighted FP pronunciation variants of variable lengths and applying specific acoustic models for FP, we achieved an 8% relative reduction of the word error rate. We also tested different approaches for handling FP in the language model and integrating FP into the decoder. Best results with respect to both perplexity and word error rate have been achieved by predicting FP probabilistically and removing it from the language model history. This approach reduces the perplexity by 4% and provides a further gain in word accuracy.\n",
    ""
   ]
  },
  "binnenpoorte03_sspr": {
   "authors": [
    [
     "Diana",
     "Binnenpoorte"
    ],
    [
     "Simo",
     "Goddijn"
    ],
    [
     "Catia",
     "Cucchiarini"
    ]
   ],
   "title": "How to improve human and machine transcriptions of spontaneous speech",
   "original": "sspr_tmo7",
   "page_count": 4,
   "order": 35,
   "p1": "paper TMO7",
   "pn": "",
   "abstract": [
    "This paper reports on an experiment aimed at measuring the quality of automatic and human phonetic transcriptions of different speech styles that were produced within the framework of a large speech corpus project for Dutch, the Spoken Dutch Corpus (Corpus Gesproken Nederlands, CGN). The results indicate that the procedure adopted in the CGN to improve the quality of phonetic transcriptions does indeed contribute to achieving this aim. However, better transcriptions of spontaneous speech could probably be obtained by resorting to ASR techniques for pronunciation variation modeling. Our research indicates how this could be achieved.\n",
    ""
   ]
  },
  "tsujii03_sspr": {
   "authors": [
    [
     "Jun-ichi",
     "Tsujii"
    ]
   ],
   "title": "New perspectives of linguistic study - how can linguistic theories help corpus-based techniques in NLP or vice versa?",
   "original": "sspr_tao1",
   "page_count": 7,
   "order": 36,
   "p1": "paper TAO1 (keynote paper)",
   "pn": "",
   "abstract": [
    "Corpus-based techniques and symbolic, linguistics-based NLP techniques  jave to be unified for further development of the field. The integration  of these two streams of research will open up new perspectives of  linguistic study that will unify rational-ists theories with  empirical study of lan-guages. Several research directions  are discussed.\n",
    ""
   ]
  },
  "uchimoto03_sspr": {
   "authors": [
    [
     "Kiyotaka",
     "Uchimoto"
    ],
    [
     "Chikashi",
     "Nobata"
    ],
    [
     "Atsushi",
     "Yamada"
    ],
    [
     "Satoshi",
     "Sekine"
    ],
    [
     "Hitoshi",
     "Isahara"
    ]
   ],
   "title": "Morphological analysis of the corpus of spontaneous Japanese",
   "original": "sspr_tao2",
   "page_count": 4,
   "order": 37,
   "p1": "paper TAO2",
   "pn": "",
   "abstract": [
    "This paper describes two methods for detecting word  segments and their morphological information in a Japanese spontaneous speech corpus, and a method for accurately  tagging a large spontaneous speech corpus. In this paper, we show that by using semi-automatic analysis we can expect a precision of over 99% for detecting and tagging short words and 97% for long words; the two types of words comprising the corpus.\n",
    ""
   ]
  },
  "asahara03_sspr": {
   "authors": [
    [
     "Masayuki",
     "Asahara"
    ],
    [
     "Yuji",
     "Matsumoto"
    ]
   ],
   "title": "Filler and disfluency identification based on morphological analysis and chunking",
   "original": "sspr_tao3",
   "page_count": 4,
   "order": 38,
   "p1": "paper TAO3",
   "pn": "",
   "abstract": [
    "We propose a novel filler/disfluency identification method for transcription of spontaneous speech in Japanese. Our method is hased on Japanese morphological analysis and chunking. Firstly, input sentences are analyzed with redundant  outputs by a statistical morphological analyzer. Since fillers and disfluencies produce ambiguity in morphological analysis, we do this so as to take into account several  possible roles for each character in the input. Secondly, a  support vector machine-based chunker detects some ambiguous points as fillers or disfluencies. Although it cannot detect disfluency of function words satisfactorily, it achieves high performance for fillers and disfluencies of content words.\n",
    ""
   ]
  },
  "ohtake03_sspr": {
   "authors": [
    [
     "Kiyonori",
     "Ohtake"
    ],
    [
     "Kazuhide",
     "Yamamoto"
    ],
    [
     "Yuji",
     "Toma"
    ],
    [
     "Shiro",
     "Sado"
    ],
    [
     "Shigeru",
     "Masuyama"
    ],
    [
     "Seiichi",
     "Nakagawa"
    ]
   ],
   "title": "Newscast speech summarization via sentence shortening based on prosodic features",
   "original": "sspr_tao4",
   "page_count": 4,
   "order": 39,
   "p1": "paper TAO4",
   "pn": "",
   "abstract": [
    "This paper presents a speech summarizer that summarizes input speech via several prosodic features, unlike models that use a speech recognizer and conventional summarizing techniques proposed in natural language processing. Our approach analyzes the borders of summary units by employing prosodic features of pitch, power, and pause to summarize the input speech. Our summary generation trial implies robustness against noisy input compared with both a sequential connection model of a speech recognizer and a text summarizer.\n",
    ""
   ]
  },
  "boves03_sspr": {
   "authors": [
    [
     "Lou",
     "Boves"
    ],
    [
     "Nelleke",
     "Oostdijk"
    ]
   ],
   "title": "Spontaneous speech in the spoken Dutch corpus",
   "original": "sspr_tap1",
   "page_count": 4,
   "order": 40,
   "p1": "paper TAP1",
   "pn": "",
   "abstract": [
    "In this paper the Spoken Dutch Corpus project is presented, a joint Flemish-Dutch undertaking aimed at the compilation and annotation of a corpus of 1,000 hours of spoken Dutch. Upon completion, the corpus will constitute a valuable resource for research in the fields of (computational) linguistics and language and speech technology. Although the corpus will contain a fair amount of read speech (mainly to train initial acoustic models for speech recognizers), the lions share of the data will consist of spontaneous speech, ranging from lectures to unobtrusively recorded conversations. The corpus is unique in that all speech recordings will be made available together with several levels of high quality annotations, from verbatim orthographic transcriptions to syntactic analyses and prosodic labeling.\n",
    ""
   ]
  },
  "makarova03_sspr": {
   "authors": [
    [
     "Veronika",
     "Makarova"
    ],
    [
     "Valery A.",
     "Petrushin"
    ]
   ],
   "title": "The map task corpus of spoken Russian",
   "original": "sspr_tap2",
   "page_count": 4,
   "order": 41,
   "p1": "paper TAP2",
   "pn": "",
   "abstract": [
    "This paper describes the purposes, structure and applications as well as the speakers, material, recording, digitizing, labeling and storage procedures of Map Task Corpus of spoken Russian. The database is comprised of recordings of 116 spontaneous unscripted taskoriented dialogues produced by 64 native speakers of Russian while performing the task of marking a route on a printed map. The task was performed only via verbal communication, other forms (such as eye contact or gestures) being excluded in experimental settings. The total duration of the recorded dialogues is 18 hours. The database is constructed as a material source for theoretical and applied linguistics, language teaching, psycholinguistics, communication, speech processing, recognition as well as for interdisciplinary applications.\n",
    ""
   ]
  },
  "wang03_sspr": {
   "authors": [
    [
     "Hsin-min",
     "Wang"
    ]
   ],
   "title": "MATBN 2002: A Mandarin Chinese broadcast news corpus",
   "original": "sspr_tap3",
   "page_count": 4,
   "order": 42,
   "p1": "paper TAP3",
   "pn": "",
   "abstract": [
    "The MATBN 2002 Mandarin Chinese broadcast news corpus contains  a total of 40 hours of broadcast news from Public Television  Service Foundation (Taiwan) with corresponding transcripts.  The primary motivation for this collection is to provide  training and testing data for continuous speech recognition  evaluation in the broadcast domain. We expect to collect and  process 220 hours of Mandarin Chinese broadcast news speech  over 3 years. At the end of the first year, the 40 hour  broadcast news corpus has been completed on schedule and is  scheduled to be releasable in early 2003. According to our  plan, we expect to release the interim 120 hour broadcast  news corpus in late 2003 and the final 220 hour broadcast  news corpus in late 2004.\n",
    ""
   ]
  },
  "takanashi03_sspr": {
   "authors": [
    [
     "Katsuya",
     "Takanashi"
    ],
    [
     "Takehiko",
     "Maruyama"
    ],
    [
     "Kiyotaka",
     "Uchimoto"
    ],
    [
     "Hitoshi",
     "Isahara"
    ]
   ],
   "title": "Identification of \"sentences\" in spontaneous Japanese - detection and modification of clause boundaries",
   "original": "sspr_tap4",
   "page_count": 4,
   "order": 43,
   "p1": "paper TAP4",
   "pn": "",
   "abstract": [
    "The identificaion of basic units in spontaneous Japanese is an indispensable issue for spokan language processing. This paper describes a method for the semi-automatic detection of \"sentences\" from the Corpus of Spontaneous Japanese (CSJ).\n",
    ""
   ]
  },
  "rodriguez03_sspr": {
   "authors": [
    [
     "L. J.",
     "Rodríguez"
    ],
    [
     "I.",
     "Torres"
    ]
   ],
   "title": "Annotation and analysis of acoustic and lexical events in a generic corpus of spontaneous speech in Spanish",
   "original": "sspr_tap5",
   "page_count": 4,
   "order": 44,
   "p1": "paper TAP5",
   "pn": "",
   "abstract": [
    "This paper presents the annotation and statistical analysis of spontaneous speech events in a series of broadcast news interviews drawn from the so called Corpus Oral de Referencia de la Lengua Espanola Contemporánea. The annotated corpus consists of 42 interviews taken from radio and television broadcasts, fully transcribed and lasting 6.41 hours. The corpus is intended primarily to compare frequencies and typologies of spontaneous speech events between task-specific and generic speech, but also to train acoustic and language models and carry out recognition experiments. The annotation process involved two steps: (1) filtering the initial transcriptions, and (2) augmenting the filtered transcriptions with acoustic and lexical events. Filtering was applied not only to adapt the orthographic conventions and the mark-up format but also to discard some of the marks, which were irrelevant from the point of view of speech recognition. Besides human and non-human noises, annotation included acoustic events: lengthenings, silent pauses and filled pauses; lexical events: cut-off words, mispronunciations and guttural affirmations; and speech overlaps, which rarely appear in human-computer dialogues. Statistics show that the probability of finding one of such events at each word is 0.19.\n",
    ""
   ]
  },
  "kikuchi03_sspr": {
   "authors": [
    [
     "Hideaki",
     "Kikuchi"
    ],
    [
     "Kikuo",
     "Maekawa"
    ]
   ],
   "title": "Performance of segmental and prosodic labelling of spontaneous speech",
   "original": "sspr_tap6",
   "page_count": 4,
   "order": 45,
   "p1": "paper TAP6",
   "pn": "",
   "abstract": [
    "In an attempt to construct a large-scale database of spontaneous speech, the authors planned to give segmental and prosodic labels to spontaneous Japanese speech. In this paper, the performance of those lebeling will be reported. First, the performance of automatic segmental labaling by Hidden Markov Model was verified. Sample speech of about four hours long was automatically phoneme labeled and compared to the results of hand-labeling. It turned out that average of label boundary difference with hand labeled data was 14.3 ms. Second, the performance of prosodic labeling, by newly proposed labeling scheme named X-JToBI (extended J-ToBI) was verified. The analysis of labeled data showed that newly added inventories appeared in the data of spontaneous speech and rate of inter-labeler agreement increased in nearly all types of labels.\n",
    ""
   ]
  },
  "maruyama03_sspr": {
   "authors": [
    [
     "Takehiko",
     "Maruyama"
    ],
    [
     "Hideki",
     "Tanaka"
    ],
    [
     "Hideki",
     "Kashioka"
    ]
   ],
   "title": "Japanese copula marker works as a filler in spontaneous speech",
   "original": "sspr_tap7",
   "page_count": 4,
   "order": 46,
   "p1": "paper TAP7",
   "pn": "",
   "abstract": [
    "The Japanese syntactic form desune generally works as a copula marker, but sometimes it also works as a filler in spoken language. We examine the distribution and characteristics of two aspects of desune through spontaneous speech corpora, called ASU and CSJ. We also compare desune with other fillers and indicate the similarities between them.\n",
    ""
   ]
  },
  "takeuchi03_sspr": {
   "authors": [
    [
     "Kazuhiro",
     "Takeuchi"
    ],
    [
     "Katsuya",
     "Takanashi"
    ],
    [
     "Ikuyo",
     "Morimoto"
    ],
    [
     "Hanae",
     "Koiso"
    ],
    [
     "Hitoshi",
     "Isahara"
    ]
   ],
   "title": "Committee-based discourse purpose assignment: Discourse structure annotations of spontaneous Japanese monologue",
   "original": "sspr_tap8",
   "page_count": 4,
   "order": 47,
   "p1": "paper TAP8",
   "pn": "",
   "abstract": [
    "Identifying the regularity of middle level discourse is useful for annotating discourse structure in long monologues. A convincing way to find segments is described on the middle level of discourse structure (sub-story), based on a committee-based decision of discourse purposes and a detection of transitive expressions.\n",
    ""
   ]
  },
  "yang03_sspr": {
   "authors": [
    [
     "Li-chiung",
     "Yang"
    ]
   ],
   "title": "Intonational structures and topic discourse: Patterns in discourse",
   "original": "sspr_tap9",
   "page_count": 4,
   "order": 48,
   "p1": "paper TAP9",
   "pn": "",
   "abstract": [
    "Prosody is critical in conveying topic coherence and the salience of information in speech. In this study we propose that the overall coherence is brought about through pitch level structuring of phrases at both the local level of hierarchical phrase unit positioning and the global level of pitch baseline rise and fall as climax and resolution. Our results show that prosody has critical importance in the understanding and generation of natural conversation, and is crucial to enhance performance in topic segmentation, topic tracking, and advanced conversational synthesis and recognition.\n",
    ""
   ]
  },
  "kikuchi03b_sspr": {
   "authors": [
    [
     "Tomonori",
     "Kikuchi"
    ],
    [
     "Sadaoki",
     "Furui"
    ],
    [
     "Chiori",
     "Hori"
    ]
   ],
   "title": "Two-stage automatic speech summarization by sentence extraction and compaction",
   "original": "sspr_tap10",
   "page_count": 4,
   "order": 49,
   "p1": "paper TAP10",
   "pn": "",
   "abstract": [
    "This paper proposes a new automatic speech summarization method having two stages: important sentence extraction and sentence compaction. Relatively important sentences are extracted from the results of large-vocabulary continuous speech recognition (LVCSR) based on the amount of information and the confidence measures of constituent words. The set of extracted sentences is compressed by our sentence compaction method. Sentence compaction is performed by selecting a word set that maximizes a summarization score which comprises the amount of information and the con- fidence measure of each word, the linguistic likelihood of word strings, and the word concatenation probability. The selected words are concatenated to create a summary. Effectiveness of the proposed method was confirmed by testing summarization of spontaneous presentations. Optimal ratio of sentence extraction to sentence compaction changes according to the target summarization ratio and features of presentations.\n",
    ""
   ]
  },
  "kobayashi03_sspr": {
   "authors": [
    [
     "Satoshi",
     "Kobayashi"
    ],
    [
     "Noriki",
     "Yoshikawa"
    ],
    [
     "Seiichi",
     "Nakagawa"
    ]
   ],
   "title": "Extracting summarization of lectures based on linguistic surface and prosodic information",
   "original": "sspr_tap11",
   "page_count": 4,
   "order": 50,
   "p1": "paper TAP11",
   "pn": "",
   "abstract": [
    "It is easy to record speech, but it is not easy to refer to audio recordings. If it is enable to index or summarize audio recordings, referring audio recordings will become easier. In this paper, we aim automatic extracting summarization of spoken lectures. For this purpose, at first we compared  results of extracting summarization by human subjects. Then we investigate relations between linguistic surface information  and human results and we got the useful surface  information. Next, we made summarized audios based on this  information, and we compared them with human results.  Additionally, we focused on prosodic features: F0 and power. We did same experiments on them. Lastly, we combine  linguistic surface information and prosodic information. As the result, we got better K-value. And those were compara- ble with human results.\n",
    ""
   ]
  },
  "nanjo03b_sspr": {
   "authors": [
    [
     "Hiroaki",
     "Nanjo"
    ],
    [
     "Kazuya",
     "Shitaoka"
    ],
    [
     "Tatsuya",
     "Kawahara"
    ]
   ],
   "title": "Automatic transformation of lecture transcription into document style using statistical framework",
   "original": "sspr_tap12",
   "page_count": 4,
   "order": 51,
   "p1": "paper TAP12",
   "pn": "",
   "abstract": [
    "This paper addresses automatic transformation from spoken style texts to written style texts. Exact transcriptions and speech recognition results of live lectures include many spoken language expressions, and thus, are not suitable for documents and need to be edited. In this paper, we present a method of applying of the statistical approach used in machine translation to this post-processing task. Specifically, we implement the correction of colloquial expressions, the deletion of fillers, the insertion of periods, and the insertion of particles in an integrated manner. A preliminary evaluation confirms that the statistical transformation framework works well and we achieved high recall and precision rate of period and particle insertion.\n",
    ""
   ]
  },
  "hori03b_sspr": {
   "authors": [
    [
     "Takaaki",
     "Hori"
    ],
    [
     "Daniel",
     "Willett"
    ],
    [
     "Yasuhiro",
     "Minami"
    ]
   ],
   "title": "Paraphrasing spontaneous speech using weighted finite-state transducers",
   "original": "sspr_tap13",
   "page_count": 4,
   "order": 52,
   "p1": "paper TAP13",
   "pn": "",
   "abstract": [
    "This paper describes an integrated framework to paraphrase spontaneous speech into written-style sentences. Most current speech recognition systems try to transcribe whole spoken words correctly. However, recognition results of spontaneous speech are usually difficult to understand, even if the recognition is perfect, because spontaneous speech includes redundant information, and it has a different style from that of written sentences. Especially, the style of spoken Japanese is much different from that of written one. Therefore, techniques to paraphrase recognition results are indispensable for generating captions or minutes from speech. To realize efficient speech paraphrasing, we attempt to translate spontaneous speech directly into writtenstyle sentences using a Weighted Finite-State Transducer (WFST). This approach enables to use all the knowledge sources in a one-pass search strategy and reduces the search error, since the constraint of the paraphrasing model is used from the beginning of the search. We conducted experiments on a 20k-word Japanese lecture speech recognition and paraphrasing task. Our approach yielded improvements on both recognition accuracy and paraphrasing accuracy compared with other approaches that deal with speech recognition and paraphrasing performed separately.\n",
    ""
   ]
  },
  "duez03_sspr": {
   "authors": [
    [
     "Danielle",
     "Duez"
    ]
   ],
   "title": "Modelling aspects of reduction and assimilation of consonant sequences in spontaneous French speech",
   "original": "sspr_tap14",
   "page_count": 4,
   "order": 53,
   "p1": "paper TAP14",
   "pn": "",
   "abstract": [
    "The following paper presents spectrographic data of consonant sequences containing one or two consonants omitted and/or changed into another consonant when compared to an existing perception analysis. In most cases, perceptual and acoustic data are shown to strongly correspond, proving that consonants had indeed been changed, significantly reduced or deleted, mainly in a weak position, thereby preserving acoustic information crucial for lexical access, integration of prosodic structure and successful communication. Tentative rules summarise the tendencies observed in reduction and assimilation patterns.\n",
    ""
   ]
  },
  "bu03_sspr": {
   "authors": [
    [
     "Shehui",
     "Bu"
    ],
    [
     "Mikio",
     "Yamamoto"
    ],
    [
     "Shuichi",
     "Itahashi"
    ]
   ],
   "title": "A Method of automatic extraction of F<sub>0</sub> model parameters",
   "original": "sspr_tap16",
   "page_count": 4,
   "order": 54,
   "p1": "paper TAP16",
   "pn": "",
   "abstract": [
    "This paper presents an automatic algorithm to extract the discrete phrase and accent commands with their timing parameters from the speech wave. This algorithm is based on dynamic programming (also called DP algorithm) and least square error methods. The DP algorithm is composed of two steps. First, it decides the timing of the phrase commands; then, it determines the suitable parameters of phrase and accent commands. Finally, the experiment results of one example will be shown.\n",
    ""
   ]
  },
  "fujisawa03_sspr": {
   "authors": [
    [
     "Takashi",
     "Fujisawa"
    ],
    [
     "Kazuaki",
     "Takami"
    ],
    [
     "Norman D.",
     "Cook"
    ]
   ],
   "title": "On the role of pitch intervals in the perception of emotional speech",
   "original": "sspr_tap17",
   "page_count": 4,
   "order": 55,
   "p1": "paper TAP17",
   "pn": "",
   "abstract": [
    "Evaluation of the emotional valence of speech cannot be done on the basis of first-order statistics concerning the fundamental frequency (F0), and the quantification of \"pitch contours\" has remained an intractable problem. We have addressed the issue of the relationship between emotion and F0 using a new technique for the extraction of the dominant tones within speech utterances and then the analysis of the interval structure. Our approach entails the summation of F0 over the entire utterance and calculation of the underlying pitch structure using an unsupervised \"cluster\" (radial basis function) algorithm. The technique normally results in 2-5 Gaussian \"pitch clusters\" per utterance that can then be evaluated in terms of their inherent dissonance and harmonic tension. We have found greater dissonance and greater harmonic tension in utterances with negative affect, relative to utterances with positive affect.\n",
    ""
   ]
  },
  "tao03_sspr": {
   "authors": [
    [
     "Jianhua",
     "Tao"
    ]
   ],
   "title": "Modeling of accent perception in Chinese spontaneous speech",
   "original": "sspr_tap18",
   "page_count": 4,
   "order": 56,
   "p1": "paper TAP18",
   "pn": "",
   "abstract": [
    "The accent was proved to be the essential links between linguistics and acoustics, and behaves as an important parameter for prosody processing and unit selection in speech synthesis system. In the paper, some acoustical measurements are carried out on F0, duration, silence in order to disclose the relationship between accent and corresponding acoustical parameters. The normalized acoustic parameters are induced to facilitate the accent detecting. Based on this, a model is proposed to predict accent in spontaneous speech. The method is proved to be very successful and has been used to label accent automatically for our large corpus. Further listening tests also show that the labelling results reaches 86% accurate rate, which is nearly the same as the agreement of hand labelling results. Furthermore, different natives evaluations are also compared in the paper.\n",
    ""
   ]
  },
  "campbell03_sspr": {
   "authors": [
    [
     "Nick",
     "Campbell"
    ],
    [
     "Parham",
     "Mokhiari"
    ]
   ],
   "title": "Using a non-spontaneous speech synthesiser as a driver for a spontaneous speech synthesiser",
   "original": "sspr_tap19",
   "page_count": 4,
   "order": 57,
   "p1": "paper TAP19",
   "pn": "",
   "abstract": [
    "This paper describes a method by which the synthesis of spontaneous-sounding speech can be generated using a current speech synthesizer as a driver for unit selection from a spontaneous speech database. The driver is used to generate acoustic targets, which are then used for pre-selection of synthesis units, with the final candidates selected according to further acoustic constraints to ensure the selection of units having appropriate voice and speaking style characteristics.\n",
    ""
   ]
  },
  "grishman03_sspr": {
   "authors": [
    [
     "Ralph",
     "Grishman"
    ]
   ],
   "title": "Discovery methods for information extraction",
   "original": "sspr_wmo1",
   "page_count": 5,
   "order": 58,
   "p1": "paper WMO1 (keynote paper)",
   "pn": "",
   "abstract": [
    "Information extraction (IE) involves automatically identifying instances of a specified type of relation or event in text, and collecting the arguments and modifiers of the relation/event. High quality, easily adaptable IE systems would have a major effect on the ways in which we can make use of information in text (and ultimately, in speech as well).\n",
    "At the present state of the art, however, performance varies widely depending on the nature of the language being processed and the complexity of the relation being extracted. For restricted sublanguages and simple relations, levels of accuracy comparable to human coders are possible. This has been achieved, for example, for some types of medical records, where both physicians and an extraction system identified diseases with 70-80% accuracy (Friedman et al. 1995). High performance has also been achieved for semi-structured Web documents - documents with some explicit mark-up (Cohen and Jensen 2001). In contrast, for more complex relations and more general texts, accuracies of 50-60% are more typical. Even at these levels IE can be of significant value in situations where the text is too voluminous to be reviewed manually; for example, to provide a document search tool much richer than current keyword systems (Grishman et al 2002). IE is also being used in other applications where perfect recall is not required, such as data mining from text collections and the generation of time lines for texts. To make IE a more widely-useable technology, we face a two-fold challenge: improving its performance and improving its portability to new domains. Our group, and other research groups, are exploring how corpusbased training methods can address these challenges.\n",
    "The difficulty of IE lies in part in the wide variety of ways in which a given relation may be expressed. Automated tools for corpus analysis can help in analyzing large corpora to find these varied expressions, and hopefully can find a wider range of expressions with less human effort than current methods. References\n",
    "William Cohen and Lee Jensen. A structured wrapper induction system for extracting information from semi-structured documents. Workshop on Adaptive Text Extraction and Mining, 17th Intl Joint Conf. on Artificial Intelligence, Seattle, Wash., August, 2001.\n",
    "C. Friedman, G. Hripcsak, W. DuMouchel, S. B. Johnson , and P. D. Clayton. Natural language processing in an operational clinical information system. Natural Language Engineering 1995; 1:1-28.\n",
    "Ralph Grishman, Silja Huttunen, and Roman Yangarber. Real-time event extraction for infectious disease outbreaks. Proc. HLT 2002 (Human Language Technology Conference), San Diego, California, March 2002.\n",
    ""
   ]
  },
  "lustgarten03_sspr": {
   "authors": [
    [
     "Paul C.",
     "Lustgarten"
    ],
    [
     "B. H.",
     "Juang"
    ]
   ],
   "title": "Naturalness in speech communications",
   "original": "sspr_wmo2",
   "page_count": 6,
   "order": 59,
   "p1": "paper WMO2 (keynote paper)",
   "pn": "",
   "abstract": [
    "Speech has long been considered the most natural form of human communications. People with normal speaking and hearing abilities use speech to exchange information every day, often effortlessly. An interesting question, however, remains rarely addressed: What constitutes naturalness in speech communications and how is naturalness achieved? In this paper, we attempt to analyze the \"human behavioral components\" that contribute to the naturalness or perceived naturalness in human speech communications. We further attempt to mechanize one of these components, namely use of \"reference\", in an intelligent service scenario involving spoken language, to demonstrate that indeed such a component greatly adds to the natural experience in human-machine dialog. We hope to inspire further research into other naturalness dimensions that may enrich human machine interaction.\n",
    ""
   ]
  }
 },
 "sessions": [
  {
   "title": "Table of Contents and Access to Abstracts",
   "papers": [
    "furui03_sspr",
    "maekawa03_sspr",
    "isahara03_sspr",
    "peng03_sspr",
    "tseng03_sspr",
    "yoneyama03_sspr",
    "swerts03_sspr",
    "greenberg03_sspr",
    "lima03_sspr",
    "jitsuhiro03_sspr",
    "watanabe03_sspr",
    "yu03_sspr",
    "lee03_sspr",
    "tsai03_sspr",
    "ikeno03_sspr",
    "qu03_sspr",
    "yokoyama03_sspr",
    "nanjo03_sspr",
    "akita03_sspr",
    "psutka03_sspr",
    "lucke03_sspr",
    "kitaoka03_sspr",
    "shinozaki03_sspr",
    "nitta03_sspr",
    "zhang03_sspr",
    "araki03_sspr",
    "hori03_sspr",
    "nagarajan03_sspr",
    "hirschberg03_sspr",
    "waibel03_sspr",
    "rigoll03_sspr",
    "kawahara03_sspr",
    "schwenk03_sspr",
    "schramm03_sspr",
    "binnenpoorte03_sspr",
    "tsujii03_sspr",
    "uchimoto03_sspr",
    "asahara03_sspr",
    "ohtake03_sspr",
    "boves03_sspr",
    "makarova03_sspr",
    "wang03_sspr",
    "takanashi03_sspr",
    "rodriguez03_sspr",
    "kikuchi03_sspr",
    "maruyama03_sspr",
    "takeuchi03_sspr",
    "yang03_sspr",
    "kikuchi03b_sspr",
    "kobayashi03_sspr",
    "nanjo03b_sspr",
    "hori03b_sspr",
    "duez03_sspr",
    "bu03_sspr",
    "fujisawa03_sspr",
    "tao03_sspr",
    "campbell03_sspr",
    "grishman03_sspr",
    "lustgarten03_sspr"
   ]
  }
 ]
}