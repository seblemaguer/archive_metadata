{
 "title": "Interspeech 2016",
 "location": "San Francisco, USA",
 "startDate": "8/9/2016",
 "endDate": "12/9/2016",
 "URL": "http://www.interspeech2016.org/",
 "chair": "Chair: Nelson Morgan",
 "conf": "Interspeech",
 "year": "2016",
 "name": "interspeech_2016",
 "series": "Interspeech",
 "SIG": "",
 "title1": "Interspeech 2016",
 "date": "8-12 September 2016",
 "booklet": "interspeech_2016.pdf",
 "papers": {
  "makhoul16_interspeech": {
   "authors": [
    [
     "John",
     "Makhoul"
    ]
   ],
   "title": "A 50-Year Retrospective on Speech and Language Processing",
   "original": "3001",
   "page_count": 1,
   "order": 1,
   "p1": "1",
   "pn": "1",
   "abstract": [
    "This talk is a retrospective of speech and language processing as witnessed\nby the speaker during the last 50 years. From exploratory scientific\nbeginnings that emphasized the discovery of how speech is produced\nand perceived by humans to today&#8217;s plethora of applications using\nour technology, our field has witnessed explosive growth. The talk\nwill review the historical development of our community and some of\nthe key technical ideas that have shaped our field. Some of the ideas\nwere influenced by developments in other fields, while some of the\ndevelopments in our field have been instrumental in key advances in\nother fields, such as optical character recognition and machine translation.\nImportant developments include the source-filter model, digital signal\nprocessing, linear prediction, vector quantization, deep neural networks,\nand statistical modeling methods, especially hidden Markov models (HMMs),\nwith primary applications to speech analysis, synthesis, coding, and\nrecognition. The talk will be sprinkled with lessons learned in the\nimportance of various factors in performing our research, and will\nbe peppered with interesting tidbits about key moments in the development\nof our technology. The talk will end with a brief prospective peek\nat the next 50 years.\n"
   ]
  },
  "medennikov16_interspeech": {
   "authors": [
    [
     "Ivan",
     "Medennikov"
    ],
    [
     "Alexey",
     "Prudnikov"
    ],
    [
     "Alexander",
     "Zatvornitskiy"
    ]
   ],
   "title": "Improving English Conversational Telephone Speech Recognition",
   "original": "0473",
   "page_count": 5,
   "order": 2,
   "p1": "2",
   "pn": "6",
   "abstract": [
    "The goal of this work is to build a state-of-the-art English conversational\ntelephone speech recognition system. We investigated several techniques\nto improve acoustic modeling, namely speaker-dependent bottleneck features,\ndeep Bidirectional Long Short-Term Memory (BLSTM) recurrent neural\nnetworks, data augmentation and score fusion of DNN and BLSTM models.\nTraining set consisted of the 300 hour Switchboard English speech corpus.\nWe also examined the hypothesis rescoring using language models based\non recurrent neural networks. The resulting system achieves a word\nerror rate of 7.8% on the Switchboard part of the HUB5 2000 evaluation\nset which is the competitive result.\n"
   ],
   "doi": "10.21437/Interspeech.2016-473"
  },
  "saon16_interspeech": {
   "authors": [
    [
     "George",
     "Saon"
    ],
    [
     "Tom",
     "Sercu"
    ],
    [
     "Steven",
     "Rennie"
    ],
    [
     "Hong-Kwang J.",
     "Kuo"
    ]
   ],
   "title": "The IBM 2016 English Conversational Telephone Speech Recognition System",
   "original": "1460",
   "page_count": 5,
   "order": 3,
   "p1": "7",
   "pn": "11",
   "abstract": [
    "We describe a collection of acoustic and language modeling techniques\nthat lowered the word error rate of our English conversational telephone\nLVCSR system to a record 6.6% on the Switchboard subset of the Hub5\n2000 evaluation testset. On the acoustic side, we use a score fusion\nof three strong models: recurrent nets with maxout activations, very\ndeep convolutional nets with 3&#215;3 kernels, and bidirectional long\nshort-term memory nets which operate on FMLLR and i-vector features.\nOn the language modeling side, we use an updated model &#8220;M&#8221;\nand hierarchical neural network LMs.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1460"
  },
  "lu16_interspeech": {
   "authors": [
    [
     "Liang",
     "Lu"
    ],
    [
     "Steve",
     "Renals"
    ]
   ],
   "title": "Small-Footprint Deep Neural Networks with Highway Connections for Speech Recognition",
   "original": "0039",
   "page_count": 5,
   "order": 4,
   "p1": "12",
   "pn": "16",
   "abstract": [
    "For speech recognition, deep neural networks (DNNs) have significantly\nimproved the recognition accuracy in most of benchmark datasets and\napplication domains. However, compared to the conventional Gaussian\nmixture models, DNN-based acoustic models usually have much larger\nnumber of model parameters, making it challenging for their applications\nin resource constrained platforms, e.g., mobile devices. In this paper,\nwe study the application of the recently proposed highway network to\ntrain small-footprint DNNs, which are  thinner and  deeper, and have\nsignificantly smaller number of model parameters compared to conventional\nDNNs. We investigated this approach on the AMI meeting speech transcription\ncorpus which has around 80 hours of audio data. The highway neural\nnetworks constantly outperformed their plain DNN counterparts, and\nthe number of model parameters can be reduced significantly without\nsacrificing the recognition accuracy.\n"
   ],
   "doi": "10.21437/Interspeech.2016-39"
  },
  "yu16_interspeech": {
   "authors": [
    [
     "Dong",
     "Yu"
    ],
    [
     "Wayne",
     "Xiong"
    ],
    [
     "Jasha",
     "Droppo"
    ],
    [
     "Andreas",
     "Stolcke"
    ],
    [
     "Guoli",
     "Ye"
    ],
    [
     "Jinyu",
     "Li"
    ],
    [
     "Geoffrey",
     "Zweig"
    ]
   ],
   "title": "Deep Convolutional Neural Networks with Layer-Wise Context Expansion and Attention",
   "original": "0251",
   "page_count": 5,
   "order": 5,
   "p1": "17",
   "pn": "21",
   "abstract": [
    "In this paper, we propose a deep convolutional neural network (CNN)\nwith layer-wise context expansion and location-based attention, for\nlarge vocabulary speech recognition. In our model each higher layer\nuses information from broader contexts, along both the time and frequency\ndimensions, than its immediate lower layer. We show that both the layer-wise\ncontext expansion and the location-based attention can be implemented\nusing the element-wise matrix product and the convolution operation.\nFor this reason, contrary to other CNNs, no pooling operation is used\nin our model. Experiments on the 309hr Switchboard task and the 375hr\nshort message dictation task indicates that our model outperforms both\nthe DNN and LSTM significantly.\n"
   ],
   "doi": "10.21437/Interspeech.2016-251"
  },
  "pundak16_interspeech": {
   "authors": [
    [
     "Golan",
     "Pundak"
    ],
    [
     "Tara N.",
     "Sainath"
    ]
   ],
   "title": "Lower Frame Rate Neural Network Acoustic Models",
   "original": "0275",
   "page_count": 5,
   "order": 6,
   "p1": "22",
   "pn": "26",
   "abstract": [
    "Recently neural network acoustic models trained with Connectionist\nTemporal Classification (CTC) were proposed as an alternative approach\nto conventional cross-entropy trained neural network acoustic models\nwhich output frame-level decisions every 10ms [1]. As opposed to conventional\nmodels, CTC learns an alignment jointly with the acoustic model, and\noutputs a  blank symbol in addition to the regular acoustic state units.\nThis allows the CTC model to run with a lower frame rate, outputting\ndecisions every 30ms rather than 10ms as in conventional models, thus\nimproving overall system speed. In this work, we explore how conventional\nmodels behave with lower frame rates. On a large vocabulary Voice Search\ntask, we will show that with conventional models, we can slow the frame\nrate to 40ms while improving WER by 3% relative over a CTC-based model.\n"
   ],
   "doi": "10.21437/Interspeech.2016-275"
  },
  "kurata16_interspeech": {
   "authors": [
    [
     "Gakuto",
     "Kurata"
    ],
    [
     "Brian",
     "Kingsbury"
    ]
   ],
   "title": "Improved Neural Network Initialization by Grouping Context-Dependent Targets for Acoustic Modeling",
   "original": "0725",
   "page_count": 5,
   "order": 7,
   "p1": "27",
   "pn": "31",
   "abstract": [
    "Neural Network (NN) Acoustic Models (AMs) are usually trained using\ncontext-dependent Hidden Markov Model (CD-HMM) states as independent\ntargets. For example, the CD-HMM states of  A-b-2 (second variant of\nbeginning state of  A) and  A-m-1 (first variant of middle state of\n A) both correspond to the phone  A, and  A-b-1 and  A-b-2 both correspond\nto the Context-independent HMM (CI-HMM) state  A-b, but this relationship\nis not explicitly modeled. We propose a method that treats some neurons\nin the final hidden layer just below the output layer as dedicated\nneurons for phones or CI-HMM states by initializing connections between\nthe dedicated neurons and the corresponding CD-HMM outputs with stronger\nweights than to other outputs. We obtained 6.5% and 3.6% relative error\nreductions with a DNN AM and a CNN AM, respectively, on a 50-hour English\nbroadcast news task and 4.6% reduction with a CNN AM on a 500-hour\nJapanese task, in all cases after Hessian-free sequence training. Our\nproposed method only changes the NN parameter initialization and requires\nno additional computation in NN training or speech recognition run-time.\n"
   ],
   "doi": "10.21437/Interspeech.2016-725"
  },
  "chen16_interspeech": {
   "authors": [
    [
     "Lei",
     "Chen"
    ],
    [
     "Gary",
     "Feng"
    ],
    [
     "Michelle",
     "Martin-Raugh"
    ],
    [
     "Chee Wee",
     "Leong"
    ],
    [
     "Christopher",
     "Kitchen"
    ],
    [
     "Su-Youn",
     "Yoon"
    ],
    [
     "Blair",
     "Lehman"
    ],
    [
     "Harrison",
     "Kell"
    ],
    [
     "Chong Min",
     "Lee"
    ]
   ],
   "title": "Automatic Scoring of Monologue Video Interviews Using Multimodal Cues",
   "original": "1453",
   "page_count": 5,
   "order": 8,
   "p1": "32",
   "pn": "36",
   "abstract": [
    "Job interviews are an important tool for employee selection. When making\nhiring decisions, a variety of information from interviewees, such\nas previous work experience, skills, and their verbal and nonverbal\ncommunication, are jointly considered. In recent years, Social Signal\nProcessing (SSP), an emerging research area on enabling computers to\nsense and understand human social signals, is being used develop systems\nfor the coaching and evaluation of job interview performance. However\nthis research area is still in its infancy and lacks essential resources\n(e.g., adequate corpora). In this paper, we report on our efforts to\ncreate an automatic interview rating system for monologue-style video\ninterviews, which have been widely used in today&#8217;s job hiring\nmarket. We created the first multimodal corpus for such video interviews.\nAdditionally, we conducted manual rating on the interviewee&#8217;s\npersonality and performance during 12 structured interview questions\nmeasuring different types of job-related skills. Finally, focusing\non predicting overall interview performance, we explored a set of verbal\nand nonverbal features and several machine learning models. We found\nthat using both verbal and nonverbal features provides more accurate\npredictions. Our initial results suggest that it is feasible to continue\nworking in this newly formed area.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1453"
  },
  "chong16_interspeech": {
   "authors": [
    [
     "Chee Seng",
     "Chong"
    ],
    [
     "Jeesun",
     "Kim"
    ],
    [
     "Chris",
     "Davis"
    ]
   ],
   "title": "The Sound of Disgust: How Facial Expression May Influence Speech Production",
   "original": "1463",
   "page_count": 5,
   "order": 9,
   "p1": "37",
   "pn": "41",
   "abstract": [
    "In speech articulation, mouth/lip shapes determine properties of the\nfront part of the vocal tract, and so alter vowel formant frequencies.\nMouth and lip shapes also determine facial emotional expressions, e.g.,\ndisgust is typically expressed with a distinctive lip and mouth configuration\n(i.e., closed mouth, pulled back lip corners). This overlap of speech\nand emotion gestures suggests that expressive speech will have different\nvowel formant frequencies from neutral speech. This study tested this\nhypothesis by comparing vowels produced in neutral versus disgust expressions.\nWe used our database of five female native Cantonese talkers each uttering\n50 CHINT sentences in both a neutral tone of voice and in disgust to\nexamine five vowels ([&#592;], [&#949;&#720;], [i&#720;], [&#596;&#720;],\n[&#7452;&#720;]). Mean fundamental frequency (F0) and the first two\nformants (F1 and F2) were calculated and analysed using mixed effects\nlogistic regression. The results showed that the disgust vowels showed\na significant reduction in either or both formant values (depending\non vowel type) compared to neutral. We discuss the results in terms\nof how vowel synthesis could be used to alter the recognition of the\nsound of disgust.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1463"
  },
  "yang16_interspeech": {
   "authors": [
    [
     "Zhaojun",
     "Yang"
    ],
    [
     "Shrikanth S.",
     "Narayanan"
    ]
   ],
   "title": "Analyzing Temporal Dynamics of Dyadic Synchrony in Affective Interactions",
   "original": "0158",
   "page_count": 5,
   "order": 10,
   "p1": "42",
   "pn": "46",
   "abstract": [
    "Human communication is a dynamical and interactive process that naturally\ninduces an active flow of interpersonal coordination, and synchrony,\nalong various behavioral dimensions. Assessing and characterizing the\ntemporal dynamics of synchrony during an interaction is essential for\nfully understanding the human communication mechanisms. In this work,\nwe focus on uncovering the temporal variability patterns of synchrony\nin visual gesture and vocal behavior in affectively rich interactions.\nWe propose a statistical scheme to robustly quantify the turn-wise\ninterpersonal synchrony. The analysis of the synchrony dynamics measure\nrelies heavily on functional data analysis techniques. Our analysis\nresults reveal that: 1) the dynamical patterns of interpersonal synchrony\ndiffer depending on the global emotions of an interaction dyad; 2)\nthere generally exists a tight dynamical emotion-synchrony coupling\nover the interaction. These observations corroborate that interpersonal\nbehavioral synchrony is a critical manifestation of the underlying\naffective processes, shedding light toward improved affective interaction\nmodeling and automatic emotion recognition.\n"
   ],
   "doi": "10.21437/Interspeech.2016-158"
  },
  "ganesh16_interspeech": {
   "authors": [
    [
     "Attigodu C.",
     "Ganesh"
    ],
    [
     "Frédéric",
     "Berthommier"
    ],
    [
     "Jean-Luc",
     "Schwartz"
    ]
   ],
   "title": "Audiovisual Speech Scene Analysis in the Context of Competing Sources",
   "original": "0062",
   "page_count": 5,
   "order": 11,
   "p1": "47",
   "pn": "51",
   "abstract": [
    "Audiovisual fusion in speech perception is generally conceived as a\nprocess independent from scene analysis, which is supposed to occur\nseparately in the auditory and visual domain. On the contrary, we have\nbeen proposing in the last years that scene analysis such as what takes\nplace in the cocktail party effect was an audiovisual process. We review\nhere a series of experiments illustrating how audiovisual speech scene\nanalysis occurs in the context of competing sources. Indeed, we show\nthat a short contextual audiovisual stimulus made of competing auditory\nand visual sources modifies the perception of a following McGurk target.\nWe interpret this in terms of binding, unbinding and rebinding processes,\nand we show how these processes depend on audiovisual correlations\nin time, attentional processes and differences between junior and senior\nparticipants.\n"
   ],
   "doi": "10.21437/Interspeech.2016-62"
  },
  "sadoughi16_interspeech": {
   "authors": [
    [
     "Najmeh",
     "Sadoughi"
    ],
    [
     "Carlos",
     "Busso"
    ]
   ],
   "title": "Head Motion Generation with Synthetic Speech: A Data Driven Approach",
   "original": "0419",
   "page_count": 5,
   "order": 12,
   "p1": "52",
   "pn": "56",
   "abstract": [
    "To have believable head movements for  conversational agents (CAs),\nthe natural coupling between speech and head movements needs to be\npreserved, even when the CA uses synthetic speech. To incorporate the\nrelation between speech head movements, studies have learned these\ncouplings from real recordings, where speech is used to derive head\nmovements. However, relying on recorded speech for every sentence that\na virtual agent utters constrains the versatility and scalability of\nthe interface, so most practical solutions for CAs use text to speech.\nWhile we can generate head motion using rule-based models, the head\nmovements may become repetitive, spanning only a limited range of behaviors.\nThis paper proposes strategies to leverage speech-driven models for\nhead motion generation for cases relying on synthetic speech. The straightforward\napproach is to drive the speech-based models using synthetic speech,\nwhich creates mismatch between the test and train conditions. Instead,\nwe propose to create a parallel corpus of synthetic speech aligned\nwith natural recordings for which we have motion capture recordings.\nWe use this parallel corpus to either retrain or adapt the speech-based\nmodels with synthetic speech. Objective and subjective metrics show\nsignificant improvements of the proposed approaches over the case with\nmismatched condition.\n"
   ],
   "doi": "10.21437/Interspeech.2016-419"
  },
  "kim16_interspeech": {
   "authors": [
    [
     "Jeesun",
     "Kim"
    ],
    [
     "Chris",
     "Davis"
    ]
   ],
   "title": "The Consistency and Stability of Acoustic and Visual Cues for Different Prosodic Attitudes",
   "original": "1505",
   "page_count": 5,
   "order": 13,
   "p1": "57",
   "pn": "61",
   "abstract": [
    "Recently it has been argued that speakers use conventionalized forms\nto express different prosodic attitudes [1]. We examined this by looking\nat across speaker consistency in the expression of auditory and visual\n(head and face motion) prosodic attitudes produced on multiple different\noccasions. Specifically, we examined acoustic and motion profiles of\na female and a male speaker expressing six different prosodic attitudes\nfor four within-session repetitions across four different sessions.\nWe used the same acoustic features as [1] and visual prosody was assessed\nby examining patterns of speaker&#8217;s mouth, eyebrow and head movements.\nThere was considerable variation in how prosody was realized across\nspeakers, with the productions of one speaker more discriminable than\nthe other. Within-session variation for both the acoustic and movement\ndata was smaller than across-session variation, suggesting that short-term\nmemory plays a role in consistency. The expression of some attitudes\nwas less variable than others and better discrimination was found with\nthe acoustic compared to the visual data, although certain visual features\n(e.g., eyebrow brow motion) provided better discrimination than others.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1505"
  },
  "kim16b_interspeech": {
   "authors": [
    [
     "Jeesun",
     "Kim"
    ],
    [
     "Gérard",
     "Bailly"
    ]
   ],
   "title": "Introduction to Poster Presentation of Part II",
   "original": "abs1",
   "page_count": 0,
   "order": 14,
   "p1": "0",
   "pn": "",
   "abstract": [
    "(No abstract available at the time of publication)\n"
   ]
  },
  "vogel16_interspeech": {
   "authors": [
    [
     "Irene",
     "Vogel"
    ],
    [
     "Laura",
     "Spinu"
    ]
   ],
   "title": "The Unit of Speech Encoding: The Case of Romanian",
   "original": "1601",
   "page_count": 5,
   "order": 15,
   "p1": "62",
   "pn": "66",
   "abstract": [
    "The number of units in an utterance determines how much time speakers\nrequire to physically plan and begin their production [1]&#8211;[2].\nPrevious research proposed that the crucial units are prosodic i.e.,\nPhonological Words (PWs), not syntactic or morphological [3]. Experiments\non Dutch using a prepared speech paradigm claimed to support this view\n[4]&#8211;[5]; however, compounds did not conform to predictions and\nrequired the introduction of a different way of counting units. Since\ntwo PWs in compounds patterned with one PW, with or without clitics,\nrather than a phrase containing two PWs, a recursive PW&#8217; was\ninvoked. Similar results emerged using the same methodology with compounds\nin Italian [6], and it was thus proposed that the relevant unit for\nspeech encoding is not the PW, but rather the Composite Group (CompG),\na constituent of the Prosodic Hierarchy between the PW and Phonological\nPhrase that comprises both compounds and clitic constructions [7].\nWe further investigate the relevant unit for speech encoding using\nthe same methodology in Romanian. Similar findings support the CompG\nas the speech planning unit since, again, compounds with two PWs pattern\nwith single words and clitic constructions, not Phonological Phrases\nwhich also contain two PWs.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1601"
  },
  "jugler16_interspeech": {
   "authors": [
    [
     "Jeanin",
     "Jügler"
    ],
    [
     "Frank",
     "Zimmerer"
    ],
    [
     "Jürgen",
     "Trouvain"
    ],
    [
     "Bernd",
     "Möbius"
    ]
   ],
   "title": "The Perceptual Effect of L1 Prosody Transplantation on L2 Speech: The Case of French Accented German",
   "original": "1268",
   "page_count": 5,
   "order": 16,
   "p1": "67",
   "pn": "71",
   "abstract": [
    "Research has shown that language learners are not only challenged by\nsegmental differences between their native language (L1) and the second\nlanguage (L2). They also have problems with the correct production\nof suprasegmental structures, like phone/syllable duration and the\nrealization of pitch. These difficulties often lead to a perceptible\nforeign accent. This study investigates the influence of prosody transplantation\non foreign accent ratings. Syllable duration and pitch contour were\ntransferred from utterances of a male and female German native speaker\nto utterances of ten French native speakers speaking German. Acoustic\nmeasurements show that French learners spoke with a significantly lower\nspeaking rate. As expected, results of a perception experiment judging\nthe accentedness of 1) German native utterances, 2) unmanipulated and\n3) manipulated utterances of French learners of German suggest that\nthe transplantation of the prosodic features syllable duration and\npitch leads to a decrease in accentedness rating. These findings confirm\nresults found in similar studies investigating prosody transplantation\nwith different L1 and L2 and provide a beneficial technique for (computer-assisted)\npronunciation training.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1268"
  },
  "ling16_interspeech": {
   "authors": [
    [
     "Bijun",
     "Ling"
    ],
    [
     "Jie",
     "Liang"
    ]
   ],
   "title": "Organizing Syllables into Sandhi Domains &#8212; Evidence from F0 and Duration Patterns in Shanghai Chinese",
   "original": "0631",
   "page_count": 5,
   "order": 17,
   "p1": "72",
   "pn": "76",
   "abstract": [
    "In this study we investigated grouping-related F0 patterns in Shanghai\nChinese by examining the effect of syllable position in a sandhi domain\nwhile controlling for tone, number of syllables in a domain, and focus\ncondition. Results showed that F0 alignment had the most consistent\ngrouping-related patterns, and syllable duration was positively related\nto F0 movement. Focus and word length both increased F0 peak and F0\nexcursion, but they had opposite influence on F0 slope, which indicated\nthat focus and word length had different mechanisms in affecting F0\nimplementation, as focus increased articulation strength while word\nlength influenced speaker&#8217;s pre-planning.\n"
   ],
   "doi": "10.21437/Interspeech.2016-631"
  },
  "ryant16_interspeech": {
   "authors": [
    [
     "Neville",
     "Ryant"
    ],
    [
     "Mark",
     "Liberman"
    ]
   ],
   "title": "Automatic Analysis of Phonetic Speech Style Dimensions",
   "original": "1355",
   "page_count": 5,
   "order": 18,
   "p1": "77",
   "pn": "81",
   "abstract": [
    "We apply automated analysis methods to create a multidimensional characterization\nof the prosodic characteristics of a large variety of speech datasets,\nwith the goal of developing a general framework for comparing prosodic\nstyles. Our datasets span styles including conversation, fluent reading,\nextemporized narratives, political speech, and advertisements; we compare\nseveral different languages including English, Spanish, and Chinese;\nand the features we extract are based on the joint distributions of\nF0 and amplitude values and sequences, speech and silence segment durations,\nsyllable durations, and modulation spectra. Rather than focus on the\nacoustic correlates of a small number of discrete and mutually exclusive\ncategories, we aim to characterize the space in which diverse speech\nstyles live.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1355"
  },
  "athanasopoulou16_interspeech": {
   "authors": [
    [
     "Angeliki",
     "Athanasopoulou"
    ],
    [
     "Irene",
     "Vogel"
    ]
   ],
   "title": "The Acoustic Manifestation of Prominence in Stressless Languages",
   "original": "1424",
   "page_count": 5,
   "order": 19,
   "p1": "82",
   "pn": "86",
   "abstract": [
    "Languages frequently express focus by enhancing various acoustic attributes\nof an utterance, but it is widely accepted that the main enhancement\nappears on stressed syllables. In languages without lexical stress,\nthe question arises as to how focus is acoustically manifested. We\nthus examine the acoustic properties associated with prominence in\nthree stressless languages, Indonesian, Korean and Vietnamese, comparing\nreal three-syllable words in non-focused and focused contexts. Despite\nother prosodic differences, our findings confirm that none of the languages\nexhibits stress in the absence of focus, and under focus, no syllable\nshows consistent enhancement that could be indirectly interpreted as\na manifestation of focus. Instead, a combination of boundary phenomena\nconsistent with the right edge of a major prosodic constituent (Intonational\nPhrase) appears in each language: increased duration on the final syllable\nand in Indonesian and Korean, a decrease in F0. Since these properties\nare also found in languages with stress, we suggest that boundary phenomena\nsignaling a major prosodic constituent break are used universally to\nindicate focus, regardless of a language&#8217;s word-prosody; stress\nlanguages may use the same boundary properties, but these are most\nlikely to be combined with enhancement of the stressed syllable of\na word.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1424"
  },
  "lai16_interspeech": {
   "authors": [
    [
     "Wei",
     "Lai"
    ],
    [
     "Jiahong",
     "Yuan"
    ],
    [
     "Ya",
     "Li"
    ],
    [
     "Xiaoying",
     "Xu"
    ],
    [
     "Mark",
     "Liberman"
    ]
   ],
   "title": "The Rhythmic Constraint on Prosodic Boundaries in Mandarin Chinese Based on Corpora of Silent Reading and Speech Perception",
   "original": "0607",
   "page_count": 5,
   "order": 20,
   "p1": "87",
   "pn": "91",
   "abstract": [
    "This study investigated the interaction between rhythmic and syntactic\nconstraints on prosodic phrases in Mandarin Chinese. A set of 4000\nsentences was annotated twice, once based on silent reading by 130\nstudents assigned 500 sentences each, and a second time by speech perception\nbased on a recording by one professional speaker. In both types of\nannotation, the general pattern of phrasing was consistent, with short\n&#8220;rhythmic phrases&#8221; behaving differently from longer &#8220;intonational\nphrases&#8221;. The probability of a rhythmic-phrase boundary between\ntwo words increased with the total length of those two words, and was\nalso influenced by the nature of the syntactic boundary between them.\nThe resulting rhythmic phrases were mainly 2&#8211;5 syllables long,\nindependent of the length of the sentence. In contrast, the length\nof intonational phrases was not stable, and was heavily affected by\nsentence length. Intonational-phrase boundaries were also found to\nbe affected by higher-level syntactic features, such as the depth of\nsyntactic tree and the number of IP nodes. However, these syntactic\ninfluences on intonational phrases were weakened in long sentences\n(&#62;20 syllable) and also in short sentences (&#60;10 syllable),\nwhere the length effect played the main role.\n"
   ],
   "doi": "10.21437/Interspeech.2016-607"
  },
  "tsai16_interspeech": {
   "authors": [
    [
     "Fu-Sheng",
     "Tsai"
    ],
    [
     "Ya-Ling",
     "Hsu"
    ],
    [
     "Wei-Chen",
     "Chen"
    ],
    [
     "Yi-Ming",
     "Weng"
    ],
    [
     "Chip-Jin",
     "Ng"
    ],
    [
     "Chi-Chun",
     "Lee"
    ]
   ],
   "title": "Toward Development and Evaluation of Pain Level-Rating Scale for Emergency Triage based on Vocal Characteristics and Facial Expressions",
   "original": "0408",
   "page_count": 5,
   "order": 21,
   "p1": "92",
   "pn": "96",
   "abstract": [
    "In order to allocate the healthcare resource, triage classification\nsystem plays an important role in assessing the severity of illness\nof the boarding patient at emergency department. The self-report pain\nintensity numerical-rating scale (NRS) is one of the major modifiers\nof the current triage system based on the Taiwan Triage and Acuity\nScale (TTAS). The validity and reliability of  self-report scheme for\npain level assessment is a major concern. In this study, we model the\nobserved expressive behaviors, i.e., facial expressions and vocal characteristics,\ndirectly from audio-video recordings in order to measure pain level\nfor patients during triage. This work demonstrates a feasible model,\nwhich achieves an accuracy of 72.3% and 51.6% in a binary and ternary\npain intensity classification. Moreover, the study result reveals a\nsignificant association of current model and analgesic prescription/patient\ndisposition after adjusted for patient-report NRS and triage vital\nsigns.\n"
   ],
   "doi": "10.21437/Interspeech.2016-408"
  },
  "lee16_interspeech": {
   "authors": [
    [
     "Tan",
     "Lee"
    ],
    [
     "Yuanyuan",
     "Liu"
    ],
    [
     "Yu Ting",
     "Yeung"
    ],
    [
     "Thomas K.T.",
     "Law"
    ],
    [
     "Kathy Y.S.",
     "Lee"
    ]
   ],
   "title": "Predicting Severity of Voice Disorder from DNN-HMM Acoustic Posteriors",
   "original": "1098",
   "page_count": 5,
   "order": 22,
   "p1": "97",
   "pn": "101",
   "abstract": [
    "Acoustical analysis of speech is considered a favorable and promising\napproach to objective assessment of voice disorders. Previous research\nemphasized on the extraction and classification of voice quality features\nfrom sustained vowel sounds. In this paper, an investigation on voice\nassessment using continuous speech utterances of Cantonese is presented.\nA DNN-HMM based speech recognition system is trained with speech data\nof unimpaired voice. The recognition accuracy for pathological utterances\nis found to decrease significantly with the disorder severity increasing.\nAverage acoustic posterior probabilities are computed for individual\nphones from the speech recognition output lattices and the DNN soft-max\nlayer. The phone posteriors obtained for continuous speech from the\nmild, moderate and severe categories are highly distinctive and thus\nuseful to the determination of voice disorder severity. A subset of\nCantonese phonemes are identified to be suitable and reliable for voice\nassessment with continuous speech.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1098"
  },
  "sluis16_interspeech": {
   "authors": [
    [
     "Klaske E. van",
     "Sluis"
    ],
    [
     "Michiel W.M. van den",
     "Brekel"
    ],
    [
     "Frans J.M.",
     "Hilgers"
    ],
    [
     "Rob J.J.H. van",
     "Son"
    ]
   ],
   "title": "Long-Term Stability of Tracheoesophageal Voices",
   "original": "0114",
   "page_count": 5,
   "order": 23,
   "p1": "102",
   "pn": "106",
   "abstract": [
    "Long-term voice outcomes of 13 tracheoesophageal speakers are assessed\nusing speech samples that were recorded with at least 7 years in between.\nIntelligibility and voice quality are perceptually evaluated by 10\nexperienced speech and language pathologists. In addition, automatic\nspeech evaluations are performed with tools from Ghent University.\nNo significant group effect was found for changes in voice quality\nand intelligibility. The recordings showed a wide interspeaker variability.\nIt is concluded that intelligibility and voice quality of tracheoesophageal\nvoice is mostly stable over a period of 7 to 18 years.\n"
   ],
   "doi": "10.21437/Interspeech.2016-114"
  },
  "gosztolya16_interspeech": {
   "authors": [
    [
     "Gábor",
     "Gosztolya"
    ],
    [
     "László",
     "Tóth"
    ],
    [
     "Tamás",
     "Grósz"
    ],
    [
     "Veronika",
     "Vincze"
    ],
    [
     "Ildikó",
     "Hoffmann"
    ],
    [
     "Gréta",
     "Szatlóczki"
    ],
    [
     "Magdolna",
     "Pákáski"
    ],
    [
     "János",
     "Kálmán"
    ]
   ],
   "title": "Detecting Mild Cognitive Impairment from Spontaneous Speech by Correlation-Based Phonetic Feature Selection",
   "original": "0384",
   "page_count": 5,
   "order": 24,
   "p1": "107",
   "pn": "111",
   "abstract": [
    "Mild Cognitive Impairment (MCI), sometimes regarded as a prodromal\nstage of Alzheimer&#8217;s disease, is a mental disorder that is difficult\nto diagnose. Recent studies reported that MCI causes slight changes\nin the speech of the patient. Our previous studies showed that MCI\ncan be efficiently classified by machine learning methods such as Support-Vector\nMachines and Random Forest, using features describing the amount of\npause in the spontaneous speech of the subject. Furthermore, as hesitation\nis the most important indicator of MCI, we took special care when handling\nfilled pauses, which usually correspond to hesitation. In contrast\nto our previous studies which employed manually constructed feature\nsets, we now employ (automatic) correlation-based feature selection\nmethods to find the relevant feature subset for MCI classification.\nBy analyzing the selected feature subsets we also show that features\nrelated to filled pauses are useful for MCI detection from speech samples.\n"
   ],
   "doi": "10.21437/Interspeech.2016-384"
  },
  "gong16_interspeech": {
   "authors": [
    [
     "Jen J.",
     "Gong"
    ],
    [
     "Maryann",
     "Gong"
    ],
    [
     "Dina",
     "Levy-Lambert"
    ],
    [
     "Jordan R.",
     "Green"
    ],
    [
     "Tiffany P.",
     "Hogan"
    ],
    [
     "John V.",
     "Guttag"
    ]
   ],
   "title": "Towards an Automated Screening Tool for Developmental Speech and Language Impairments",
   "original": "0549",
   "page_count": 5,
   "order": 25,
   "p1": "112",
   "pn": "116",
   "abstract": [
    "Approximately 60% of children with speech and language impairments\ndo not receive the intervention they need because their impairment\nwas missed by parents and professionals who lack specialized training.\nDiagnoses of these disorders require a time-intensive battery of assessments,\nand these are often only administered after parents, doctors, or teachers\nshow concern.<br style=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  An automated test could enable more widespread screening for speech\nand language impairments. To build classification models to distinguish\nchildren with speech or language impairments from typically developing\nchildren, we use acoustic features describing speech and pause events\nin story retell tasks. We developed and evaluated our method using\ntwo datasets. The smaller dataset contains many children with severe\nspeech or language impairments and few typically developing children.\nThe larger dataset contains primarily typically developing children.\nIn three out of five classification tasks, even after accounting for\nage, gender, and dataset differences, our models achieve good discrimination\nperformance (AUC &#62; 0.70).\n"
   ],
   "doi": "10.21437/Interspeech.2016-549"
  },
  "cm16_interspeech": {
   "authors": [
    [
     "Vikram",
     "C.M."
    ],
    [
     "Nagaraj",
     "Adiga"
    ],
    [
     "S.R. Mahadeva",
     "Prasanna"
    ]
   ],
   "title": "Spectral Enhancement of Cleft Lip and Palate Speech",
   "original": "0842",
   "page_count": 5,
   "order": 26,
   "p1": "117",
   "pn": "121",
   "abstract": [
    "The quality of cleft lip and palate (CLP) speech is affected due to\nhyper-nasality and mis-articulation. Surgery and speech therapy are\nrequired to correct the structural and functional defects of CLP, which\nwill result in an enhanced speech signal. The quality of the enhanced\nspeech is perceptually evaluated by speech-language pathologists and\nresults are highly biased. In this work, a signal processing based\ntwo stage speech enhancement method is proposed to get the perceptual\nbenchmark to compare the signal after the surgery / therapy. In the\nfirst stage, CLP speech is enhanced by suppressing the nasal formant\nand in the second stage, spectral peak-valley enhancement is carried\nout to reduce the hyper-nasality associated with the CLP speech. The\nevaluation results show that the perceptual quality of CLP speech signal\nis improved after enhancement in both stages. Further, the improvement\nin the quality of the enhanced signal is compared with the speech signal\nafter palatal prosthesis / surgery. The perceptual evaluation results\nshow that the enhanced speech signals are better than the speech after\nprosthesis / surgery\n"
   ],
   "doi": "10.21437/Interspeech.2016-842"
  },
  "guan16_interspeech": {
   "authors": [
    [
     "Tian",
     "Guan"
    ],
    [
     "Guangxing",
     "Chu"
    ],
    [
     "Fei",
     "Chen"
    ],
    [
     "Feng",
     "Yang"
    ]
   ],
   "title": "Assessing Level-Dependent Segmental Contribution to the Intelligibility of Speech Processed by Single-Channel Noise-Suppression Algorithms",
   "original": "0043",
   "page_count": 4,
   "order": 27,
   "p1": "122",
   "pn": "125",
   "abstract": [
    "Most existing single-channel noise-suppression algorithms cannot improve\nspeech intelligibility for normal-hearing listeners; however, the underlying\nreason for this performance deficit is still unclear. Given that various\nspeech segments contain different perceptual contributions, the present\nwork assesses whether the intelligibility of noisy speech can be improved\nwhen selectively suppressing its noise at high-level (vowel-dominated)\nor middle-level (containing vowel-consonant transitions) segments by\nexisting single-channel noise-suppression algorithms. The speech signal\nwas corrupted by speech-spectrum shaped noise and two-talker babble\nmasker, and its noisy high- or middle-level segments were replaced\nby their noise-suppressed versions processed by four types of existing\nsingle-channel noise-suppression algorithms. Experimental results showed\nthat performing segmental noise-suppression at high- or middle-level\nled to decreased intelligibility relative to noisy speech. This suggests\nthat the lack of intelligibility improvement by existing noise-suppression\nalgorithms is also present at segmental level, which may account for\nthe deficit traditionally observed at full-sentence level.\n"
   ],
   "doi": "10.21437/Interspeech.2016-43"
  },
  "zorila16_interspeech": {
   "authors": [
    [
     "Tudor-Cătălin",
     "Zorilă"
    ],
    [
     "Sheila",
     "Flanagan"
    ],
    [
     "Brian C.J.",
     "Moore"
    ],
    [
     "Yannis",
     "Stylianou"
    ]
   ],
   "title": "Effectiveness of Near-End Speech Enhancement Under Equal-Loudness and Equal-Level Constraints",
   "original": "0594",
   "page_count": 5,
   "order": 28,
   "p1": "126",
   "pn": "130",
   "abstract": [
    "Most recently proposed near-end speech enhancement methods have been\nevaluated with the overall power (RMS) of the speech held constant.\nWhile significant intelligibility gains have been reported in various\nnoisy conditions, an equal-RMS constraint may lead to enhancement solutions\nthat increase the loudness of the original speech. Comparable effects\nmight be produced simply by increasing the power of the original speech,\nwhich also leads to an increase in loudness. Here we suggest modifying\nthe equal-RMS constraint to one of equal loudness between the original\nand the modified signals, based on a loudness model for time-varying\nsounds. Four state-of-the-art speech-in-noise intelligibility enhancement\nsystems were evaluated under the equal-loudness constraint, using intelligibility\ntests with normal-hearing listeners. Results were compared with those\nobtained under the equal-RMS constraint. The methods based on spectral\nshaping and dynamic range compression yielded significant intelligibility\ngains regardless of the constraint, while for the method without dynamic\nrange compression the intelligibility gain was lower under the equal-loudness\nthan under the equal-RMS constraint.\n"
   ],
   "doi": "10.21437/Interspeech.2016-594"
  },
  "sharma16_interspeech": {
   "authors": [
    [
     "Bidisha",
     "Sharma"
    ],
    [
     "S.R. Mahadeva",
     "Prasanna"
    ]
   ],
   "title": "Speech Synthesis in Noisy Environment by Enhancing Strength of Excitation and Formant Prominence",
   "original": "1005",
   "page_count": 5,
   "order": 29,
   "p1": "131",
   "pn": "135",
   "abstract": [
    "Text-to-speech (TTS) synthesis systems have grown popularity due to\ntheir diverse practical usability. While most of the technologies developed\naims to meet requirements in laboratory environment, the practical\nappliance is not limited to a specific environment. This work aims\ntowards improving intelligibility of synthesized speech to make it\ndeployable in realism. Based on the comparison of Lombard speech and\nspeech produced in quiet, strength of excitation is found to play a\ncrucial role in making speech intelligible in noisy situation. A novel\nmethod for enhancement of strength of excitation is proposed which\nmakes the synthesized speech more intelligible in practical scenario.\nLinear-prediction analysis based formant enhancement method is also\nemployed to further improve the intelligibility. The proposed enhancement\nframework is applied in synthesized speech and evaluated in presence\nof different types and levels of noise. Subjective evaluation results\nshow that, the proposed method makes the synthesized speech applicable\nin practical noisy environment.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1005"
  },
  "wang16_interspeech": {
   "authors": [
    [
     "Lei",
     "Wang"
    ],
    [
     "Shufeng",
     "Zhu"
    ],
    [
     "Diliang",
     "Chen"
    ],
    [
     "Yong",
     "Feng"
    ],
    [
     "Fei",
     "Chen"
    ]
   ],
   "title": "Relative Contributions of Amplitude and Phase to the Intelligibility Advantage of Ideal Binary Masked Sentences",
   "original": "0018",
   "page_count": 4,
   "order": 30,
   "p1": "136",
   "pn": "139",
   "abstract": [
    "Many studies have shown the advantage of using ideal binary masking\n(IdBM) to improve the intelligibility of speech corrupted by interfering\nmaskers. Given the fact that amplitude and phase are two important\nacoustic cues for speech perception, the present work further investigated\nthe relative contributions of these two cues to the intelligibility\nadvantage of IdBM-processed sentences. Three types of Mandarin IdBM-processed\nstimuli (i.e., amplitude-only, phase-only, and amplitude-and-phase)\nwere generated, and played to normal-hearing listeners to recognize.\nExperiment results showed that amplitude- or phase-only cue could lead\nto significantly improved intelligibility of IdBM-processed sentences\nin relative to noise-masked sentences. A masker-dependent amplitude\nover phase advantage was observed when accounting for their relative\ncontributions to the intelligibility advantage of IdBM-processed sentences.\nUnder steady-state speech-spectrum shaped noise, both amplitude- and\nphase-only IdBM-processed sentences contained intelligibility information\nclose to that contained in amplitude-and-phase IdBM-processed sentences.\nIn contrast, under competing babble masker, amplitude-only IdBM-processed\nsentences were more intelligible than phase-only IdBM-processed sentences,\nand neither could account for the intelligibility advantage of amplitude-and-phase\nIdBM-processed sentences.\n"
   ],
   "doi": "10.21437/Interspeech.2016-18"
  },
  "liu16_interspeech": {
   "authors": [
    [
     "Qingju",
     "Liu"
    ],
    [
     "Yan",
     "Tang"
    ],
    [
     "Philip J.B.",
     "Jackson"
    ],
    [
     "Wenwu",
     "Wang"
    ]
   ],
   "title": "Predicting Binaural Speech Intelligibility from Signals Estimated by a Blind Source Separation Algorithm",
   "original": "0410",
   "page_count": 5,
   "order": 31,
   "p1": "140",
   "pn": "144",
   "abstract": [
    "State-of-the-art binaural objective intelligibility measures (OIMs)\nrequire individual source signals for making intelligibility predictions,\nlimiting their usability in real-time online operations. This limitation\nmay be addressed by a blind source separation (BSS) process, which\nis able to extract the underlying sources from a mixture. In this study,\na speech source is presented with either a stationary noise masker\nor a fluctuating noise masker whose azimuth varies in a horizontal\nplane, at two speech-to-noise ratios (SNRs). Three binaural OIMs are\nused to predict speech intelligibility from the signals separated by\na BSS algorithm. The model predictions are compared with listeners&#8217;\nword identification rate in a perceptual listening experiment. The\nresults suggest that with SNR compensation to the BSS-separated speech\nsignal, the OIMs can maintain their predictive power for individual\nmaskers compared to their performance measured from the direct signals.\nIt also reveals that the errors in SNR between the estimated signals\nare not the only factors that decrease the predictive accuracy of the\nOIMs with the separated signals. Artefacts or distortions on the estimated\nsignals caused by the BSS algorithm may also be concerns.\n"
   ],
   "doi": "10.21437/Interspeech.2016-410"
  },
  "petkov16_interspeech": {
   "authors": [
    [
     "Petko N.",
     "Petkov"
    ],
    [
     "Norbert",
     "Braunschweiler"
    ],
    [
     "Yannis",
     "Stylianou"
    ]
   ],
   "title": "Automated Pause Insertion for Improved Intelligibility Under Reverberation",
   "original": "0960",
   "page_count": 5,
   "order": 32,
   "p1": "145",
   "pn": "149",
   "abstract": [
    "Speech intelligibility in reverberant environments is reduced because\nof overlap-masking. Signal modification prior to presentation in such\nlistening environments, e.g., with a public announcement system, can\nbe employed to alleviate this problem. Time-scale modifications are\nparticularly effective in reducing the effect of overlap-masking. A\nmethod for introducing linguistically-motivated pauses is proposed\nin this paper. Given the transcription of a sentence, pause strengths\nare predicted at word boundaries. Pause duration is obtained by combining\nthe pause strength and the time it takes late reverberation to decay\nto a level where a target signal-to-late-reverberation ratio criterion\nis satisfied. Considering a moderate reverberation condition and both\nbinary and continuous pause strengths, a formal listening test was\nperformed. The results show that the proposed methodology offers a\nsignificant intelligibility improvement over unmodified speech while\ncontinuous pause strengths offer an advantage over binary pause strengths.\n"
   ],
   "doi": "10.21437/Interspeech.2016-960"
  },
  "rouas16_interspeech": {
   "authors": [
    [
     "Jean-Luc",
     "Rouas"
    ],
    [
     "Leonidas",
     "Ioannidis"
    ]
   ],
   "title": "Automatic Classification of Phonation Modes in Singing Voice: Towards Singing Style Characterisation and Application to Ethnomusicological Recordings",
   "original": "1135",
   "page_count": 5,
   "order": 33,
   "p1": "150",
   "pn": "154",
   "abstract": [
    "This paper describes our work on automatic classification of phonation\nmodes on singing voice. In the first part of the paper, we will briefly\nreview the main characteristics of the different phonation modes. Then,\nwe will describe the isolated vowels databases we used, with emphasis\non a new database we recorded specifically for the purpose of this\nwork. The next section will be dedicated to the description of the\nproposed set of parameters (acoustic and glottal) and the classification\nframework. The results obtained with only acoustic parameters are close\nto 80% of correct recognition, which seems sufficient for experimenting\nwith continuous singing. Therefore, we set up two other experiments\nin order to see if the system may be of any practical use for singing\nvoice characterisation. The first experiment aims at assessing if automatic\ndetection of phonation modes may help classify singing into different\nstyles. This experiment is carried out using a database of one singer\nsinging the same song in 8 styles. The second experiment is carried\nout on field recordings from ethnomusicologists and concerns the distinction\nbetween &#8220;normal&#8221; singing and &#8220;laments&#8221; from\na variety of countries.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1135"
  },
  "bhavsar16_interspeech": {
   "authors": [
    [
     "Himanshu N.",
     "Bhavsar"
    ],
    [
     "Tanvina B.",
     "Patel"
    ],
    [
     "Hemant A.",
     "Patil"
    ]
   ],
   "title": "Novel Nonlinear Prediction Based Features for Spoofed Speech Detection",
   "original": "1002",
   "page_count": 5,
   "order": 34,
   "p1": "155",
   "pn": "159",
   "abstract": [
    "Several speech synthesis and voice conversion techniques can easily\ngenerate or manipulate speech to deceive the speaker verification (SV)\nsystems. Hence, there is a need to develop spoofing countermeasures\nto detect the human speech from spoofed speech. System-based features\nhave been known to contribute significantly to this task. In this paper,\nwe extend a recent study of Linear Prediction (LP) and Long-Term Prediction\n(LTP)-based features to LP and Nonlinear Prediction (NLP)-based features.\nTo evaluate the effectiveness of the proposed countermeasure, we use\nthe corpora provided at the ASVspoof 2015 challenge. A Gaussian Mixture\nModel (GMM)-based classifier is used and the % Equal Error Rate (EER)\nis used as a performance measure. On the development set, it is found\nthat LP-LTP and LP-NLP features gave an average EER of 4.78% and 9.18%,\nrespectively. Score-level fusion of LP-LTP (and LP-NLP) with Mel Frequency\nCepstral Coefficients (MFCC) gave an EER of 0.8% (and 1.37%), respectively.\nAfter score-level fusion of LP-LTP, LP-NLP and MFCC features, the EER\nis significantly reduced to 0.57%. The LP-LTP and LP-NLP features have\nfound to work well even for Blizzard Challenge 2012 speech database.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1002"
  },
  "dumpala16_interspeech": {
   "authors": [
    [
     "Sri Harsha",
     "Dumpala"
    ],
    [
     "Bhanu Teja",
     "Nellore"
    ],
    [
     "Raghu Ram",
     "Nevali"
    ],
    [
     "Suryakanth V.",
     "Gangashetty"
    ],
    [
     "B.",
     "Yegnanarayana"
    ]
   ],
   "title": "Robust Vowel Landmark Detection Using Epoch-Based Features",
   "original": "1074",
   "page_count": 5,
   "order": 35,
   "p1": "160",
   "pn": "164",
   "abstract": [
    "Automatic detection of vowel landmarks is useful in many applications\nsuch as automatic speech recognition (ASR), audio search, syllabification\nof speech and expressive speech processing. In this paper, acoustic\nfeatures extracted around epochs are proposed for detection of vowel\nlandmarks in continuous speech. These features are based on zero frequency\nfiltering (ZFF) and single frequency filtering (SFF) analyses of speech.\nExcitation source based features are extracted using ZFF method and\nvocal tract system based features are extracted using SFF method. Based\non these features, a rule-based algorithm is developed for vowel landmark\ndetection (VLD). Performance of the proposed VLD algorithm is studied\non three different databases namely, TIMIT (read), NTIMIT (channel\ndegraded) and Switchboard corpus (conversational speech). Results show\nthat the proposed algorithm performs equally well compared to state-of-the-art\ntechniques on TIMIT and better on NTIMIT and Switchboard corpora. Proposed\nalgorithm also displays consistent performance on TIMIT and NTIMIT\ndatasets for different levels of noise degradations.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1074"
  },
  "toger16_interspeech": {
   "authors": [
    [
     "Johannes",
     "Töger"
    ],
    [
     "Yongwan",
     "Lim"
    ],
    [
     "Sajan Goud",
     "Lingala"
    ],
    [
     "Shrikanth S.",
     "Narayanan"
    ],
    [
     "Krishna S.",
     "Nayak"
    ]
   ],
   "title": "Sensitivity of Quantitative RT-MRI Metrics of Vocal Tract Dynamics to Image Reconstruction Settings",
   "original": "0168",
   "page_count": 5,
   "order": 36,
   "p1": "165",
   "pn": "169",
   "abstract": [
    "Real-time Magnetic Resonance Imaging (RT-MRI) is a powerful method\nfor quantitative analysis of speech. Current state-of-the-art methods\nuse constrained reconstruction to achieve high frame rates and spatial\nresolution. The reconstruction involves two free parameters that can\nbe retrospectively selected: 1) the temporal resolution and 2) the\nregularization parameter &#955;, which balances temporal regularization\nand fidelity to the collected MRI data. In this work, we study the\nsensitivity of derived quantitative measures of vocal tract function\nto these two parameters. Specifically, the cross-distance between the\ntongue tip and the alveolar ridge was investigated for different temporal\nresolutions (21, 42, 56 and 83 frames per second) and values of the\nregularization parameter. Data from one subject is included. The phrase\n&#8216;one two three four five&#8217; was repeated 8 times at a normal\npace. The results show that 1) a high regularization factor leads to\nlower cross-distance values 2) using a low value for the regularization\nparameter gives poor reproducibility and 3) a temporal resolution of\nat least 42 frames per second is desirable to achieve good reproducibility\nfor all utterances in this speech task. The process employed here can\nbe generalized to quantitative imaging of the vocal tract and other\nbody parts.\n"
   ],
   "doi": "10.21437/Interspeech.2016-168"
  },
  "cernak16_interspeech": {
   "authors": [
    [
     "Milos",
     "Cernak"
    ],
    [
     "Afsaneh",
     "Asaei"
    ],
    [
     "Pierre-Edouard",
     "Honnet"
    ],
    [
     "Philip N.",
     "Garner"
    ],
    [
     "Hervé",
     "Bourlard"
    ]
   ],
   "title": "Sound Pattern Matching for Automatic Prosodic Event Detection",
   "original": "0875",
   "page_count": 5,
   "order": 37,
   "p1": "170",
   "pn": "174",
   "abstract": [
    "Prosody in speech is manifested by variations of loudness, exaggeration\nof pitch, and specific phonetic variations of prosodic segments. For\nexample, in the stressed and unstressed syllables, there are differences\nin place or manner of articulation, vowels in unstressed syllables\nmay have a more central articulation, and vowel reduction may occur\nwhen a vowel changes from a stressed to an unstressed position.<br\nstyle=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  In this paper, we characterize the sound patterns using phonological\nposteriors to capture the phonetic variations in a concise manner.\nThe phonological posteriors quantify the posterior probabilities of\nthe phonological classes given the input speech acoustics, and they\nare obtained using the deep neural network (DNN) computational method.\nBuilt on the assumption that there are unique sound patterns in different\nprosodic segments, we devise a sound pattern matching (SPM) method\nbased on 1-nearest neighbour classifier. In this work, we focus on\nautomatic detection of prosodic stress placed on words, called also\nemphasized words. We evaluate the SPM method on English and French\ndata with emphasized words. The word emphasis detection works very\nwell also on cross-lingual tests, that is using a French classifier\non English data, and vice versa.\n"
   ],
   "doi": "10.21437/Interspeech.2016-875"
  },
  "shahin16_interspeech": {
   "authors": [
    [
     "Mostafa",
     "Shahin"
    ],
    [
     "Julien",
     "Epps"
    ],
    [
     "Beena",
     "Ahmed"
    ]
   ],
   "title": "Automatic Classification of Lexical Stress in English and Arabic Languages Using Deep Learning",
   "original": "0644",
   "page_count": 5,
   "order": 38,
   "p1": "175",
   "pn": "179",
   "abstract": [
    "Prosodic features are important for the intelligibility and proficiency\nof stress-timed languages such as English and Arabic. Producing the\nappropriate lexical stress is challenging for second language (L2)\nlearners, in particular, those whose first language (L1) is a syllable-timed\nlanguage such as Spanish, French, etc. In this paper we introduce a\nmethod for automatic classification of lexical stress to be integrated\ninto computer-aided pronunciation learning (CAPL) tools for L2 learning.\nWe trained two different deep learning architectures, the deep feedforward\nneural network (DNN) and the deep convolutional neural network (CNN)\nusing a set of temporal and spectral features related to the intensity,\nduration, pitch and energies in different frequency bands. The system\nwas applied on both English (kids and adult) and Arabic (adult) speech\ncorpora collected from native speakers. Our method results in error\nrates of 9%, 7% and 18% when tested on the English children corpus,\nEnglish adult corpus and Arabic adult corpus respectively.\n"
   ],
   "doi": "10.21437/Interspeech.2016-644"
  },
  "chen16b_interspeech": {
   "authors": [
    [
     "Fei",
     "Chen"
    ],
    [
     "Nan",
     "Yan"
    ],
    [
     "Xunan",
     "Huang"
    ],
    [
     "Hao",
     "Zhang"
    ],
    [
     "Lan",
     "Wang"
    ],
    [
     "Gang",
     "Peng"
    ]
   ],
   "title": "Development of Mandarin Onset-Rime Detection in Relation to Age and Pinyin Instruction",
   "original": "0895",
   "page_count": 5,
   "order": 39,
   "p1": "180",
   "pn": "184",
   "abstract": [
    "Development of explicit phonological awareness (PA) is thought to be\ndependent on formal instruction in reading or spelling. However, the\ndevelopment of implicit PA emerges before literacy instruction and\ninteracts with how the phonological representations are constructed\nwithin a certain language. The present study systematically investigated\nthe development of implicit PA of Mandarin onset-rime detection in\nrelation to age and Pinyin instruction, involving 70 four- to seven-year-old\nkindergarten and first-grade children. Results indicated that the overall\nrate of correct responses in the rime detection task was much higher\nthan that in the onset detection one, with better discrimination ability\nof larger units. Moreover, the underlying factors facilitating the\ndevelopment of Mandarin onset and rime detection were different, although\nboth correlated positively with Pinyin instruction. On one hand, with\nage, development of rime detection appeared to develop naturally through\nspoken language experience before schooling, and was further optimized\nto the best after Pinyin instruction. On the other hand, the accuracy\nof onset detection exhibited a drastic improvement, boosting from 66%\namong preschoolers to 93% among first graders, establishing the primacy\nof Pinyin instruction responsible for the development of implicit onset\nawareness in Mandarin.\n"
   ],
   "doi": "10.21437/Interspeech.2016-895"
  },
  "wen16_interspeech": {
   "authors": [
    [
     "Xinyi",
     "Wen"
    ],
    [
     "Yuan",
     "Jia"
    ]
   ],
   "title": "Joint Effect of Dialect and Mandarin on English Vowel Production: A Case Study in Changsha EFL Learners",
   "original": "1022",
   "page_count": 5,
   "order": 40,
   "p1": "185",
   "pn": "189",
   "abstract": [
    "Phonetic acquisition of English as a Foreign Language (EFL) for learners\nin dialectal areas has been increasingly regarded as an important research\narea in second language acquisition. However, most existing research\nhas been focused on finding out the transfer effect of dialect on English\nproduction from a second language acquisition point of view, but ignores\nthe impact of Mandarin. The present research aims to investigate the\njoint effect of dialect and Mandarin on Changsha EFL learners&#8217;\nvowel production through acoustic analysis, from both spectral and\ntemporal perspectives. We will further explain the results with the\nSpeech Learning Model (SLM). Three corner vowels, i.e., /a/ /i/ /u/,\nare studied, and the results show that: English vowels /i/ and /a/\nproduced by Changsha learners are significantly different from those\nof American speakers; specifically, /i/ is more affected by Mandarin,\nand /a/ is more affected by Changsha dialect, which can be explained\nby SLM. While /u/ produced by Changsha learners is similar to that\nof American speakers. Besides, Changsha learners produce shorter vowels\nin duration, due to dialect and Mandarin&#8217;s transfer effect, but\ncan still make tense-lax contrasts in /i-&#618;/ and /u-&#650;/ pairs.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1022"
  },
  "katayama16_interspeech": {
   "authors": [
    [
     "Tamami",
     "Katayama"
    ]
   ],
   "title": "Effects of L1 Phonotactic Constraints on L2 Word Segmentation Strategies",
   "original": "0182",
   "page_count": 5,
   "order": 41,
   "p1": "190",
   "pn": "194",
   "abstract": [
    "In the present study, it was examined whether phonotactic constraints\nof the first language affect speech processing by Japanese learners\nof English and whether L2 proficiency influences it. Seventeen native\nEnglish speakers (ES), 18 Japanese speakers with high proficiency of\nEnglish (JH), and 20 Japanese speakers with relatively low English\nproficiency (JL) took part in a monitoring task. Two types of target\nwords (CVC/CV, e.g.,  team/tea) were embedded in bisyllabic non-words\n(e.g.,  teamfesh) and given to the participants with other non-words\nin the lists. The three groups were instructed to respond as soon as\nthey spot targets, and response times and error rates were analyzed.\nThe results showed that all of the groups segmented the CVC target\nwords significantly faster and more accurately than the CV targets.\nL1 phonotactic constraints did not hinder L2 speech processing, and\na word segmentation strategy was not language-specific in the case\nof Japanese English learners.\n"
   ],
   "doi": "10.21437/Interspeech.2016-182"
  },
  "wottawa16_interspeech": {
   "authors": [
    [
     "Jane",
     "Wottawa"
    ],
    [
     "Martine",
     "Adda-Decker"
    ],
    [
     "Frédéric",
     "Isel"
    ]
   ],
   "title": "Putting German [&#643;] and [&#231;] in Two Different Boxes: Native German vs L2 German of French Learners",
   "original": "0457",
   "page_count": 5,
   "order": 42,
   "p1": "195",
   "pn": "199",
   "abstract": [
    "French L2 Learners of German (FG) often replace the palatal fricative\n/&#231;/ absent in French with the post alveolar fricative /&#643;/.\nIn our study we investigate which cues can be used to distinguish whether\nFG speakers produce [&#643;] or [&#231;] in words with the final syllables\n/&#618;&#643;/ or /&#618;&#231;/. In literature of German as an L2,\nto our knowledge, this contrast has not yet been studied. In this perspective,\nwe first compared native German (GG) productions of [/&#643;/] and\n[&#231;] to the FG speaker productions. Comparisons concerned the F2\nof the preceding vowel, the F2 transition between the preceding vowel\nand the fricative, the center of gravity and intensity of the fricatives\nin high and low frequencies. To decide which cues are effectively choices\nto separate [&#643;] and [&#231;], the Weka interface in R (RWeka)\nwas used. Results show that for German native speech, the F2 of the\npreceding vowel and the F2 transition are valid cues to distinguish\nbetween [&#643;] and [&#231;]. For FG speakers these cues are not valid.\nTo distinguish between [&#643;] and [&#231;] in FG speakers, the intensity\nof high and low frequencies as well as the center of gravity of the\nfricatives help to decide whether [&#643;] and [&#231;] was produced.\nIn German native speech, cues furnished only by the fricative itself\ncan as well be used to distinguish between [&#643;] and [&#231;]. \n"
   ],
   "doi": "10.21437/Interspeech.2016-457"
  },
  "luo16_interspeech": {
   "authors": [
    [
     "Dean",
     "Luo"
    ],
    [
     "Ruxin",
     "Luo"
    ],
    [
     "Lixin",
     "Wang"
    ]
   ],
   "title": "Naturalness Judgement of L2 English Through Dubbing Practice",
   "original": "0623",
   "page_count": 4,
   "order": 43,
   "p1": "200",
   "pn": "203",
   "abstract": [
    "This Study investigates how different prosodic features affect native\nspeakers&#8217; perception of L2 English spoken by Chinese students\nthrough dubbing, or re-voicing practice on video clips. Learning oral\nforeign language through dubbing on movie or animation clips has become\nvery popular in China. In this practice, learners try to reproduce\nutterances as closely as possible to the original speech by closely\nmatching lip movements on the clips. The L2 utterances before and after\nsubstantial dubbing practices were recorded and categorized according\nto different prosodic error patterns. Objective acoustic features were\nextracted and analyzed with naturalness scores based on perceptual\nexperiment. Experimental results show that stress and timing play key\nroles in native speakers&#8217; perception of naturalness. With the\npractice of dubbing, prosodic features, especially timing, can be considerably\nimproved and thus the naturalness of the reproduced utterances increases.\n"
   ],
   "doi": "10.21437/Interspeech.2016-623"
  },
  "shinohara16_interspeech": {
   "authors": [
    [
     "Yasuaki",
     "Shinohara"
    ]
   ],
   "title": "Audiovisual Training Effects for Japanese Children Learning English /r/-/l/",
   "original": "0641",
   "page_count": 4,
   "order": 44,
   "p1": "204",
   "pn": "207",
   "abstract": [
    "In this study, the effects of audiovisual training were examined for\nJapanese children learning the English /r/-/l/ contrast. After 10 audiovisual\ntraining sessions, participants&#8217; improvement in English /r/-/l/\nidentification in audiovisual, visual-only and audio-only conditions\nwas assessed. The results demonstrated that Japanese children significantly\nimproved in their English /r/-/l/ identification accuracy in all three\nconditions. Although there was no significant modality effect on identification\naccuracy at pre test, the participants improved their identification\naccuracy in the audiovisual condition significantly more than in the\naudio-only condition. The improvement in the audiovisual condition\nwas not significantly different from that in the visual-only condition.\nThese results suggest that Japanese children can improve their identification\naccuracy of the English /r/-/l/ contrasts using each of visual and\nauditory modalities, and they appear to improve their lip-reading skills\nas much as audiovisual identification. Nonetheless, due to the ceiling\neffect in their improvement, it is unclear whether Japanese children\nimproved their integrated processing of visual and auditory information.\n"
   ],
   "doi": "10.21437/Interspeech.2016-641"
  },
  "harper16_interspeech": {
   "authors": [
    [
     "Sarah",
     "Harper"
    ],
    [
     "Louis",
     "Goldstein"
    ],
    [
     "Shrikanth S.",
     "Narayanan"
    ]
   ],
   "title": "L2 Acquisition and Production of the English Rhotic Pharyngeal Gesture",
   "original": "0658",
   "page_count": 5,
   "order": 45,
   "p1": "208",
   "pn": "212",
   "abstract": [
    "This study is an investigation of L2 speakers&#8217; production of\nthe pharyngeal gesture in the English /&#633;/. Real-time MRI recordings\nfrom one L1 French/L2 English and one L1 Greek/L2 English speaker were\nanalyzed and compared with recordings from a native English speaker\nto examine whether the gestural composition of the rhotic consonant(s)\nin a speaker&#8217;s L1, particularly the presence and location of\na pharyngeal gesture, influences their production of English /&#633;/.\nWhile the L1 French speaker produced the expected high pharyngeal constriction\nin their production of the French rhotic, he did not appear to consistently\nproduce an English-like low pharyngeal constriction in his production\nof English /&#633;/. Similarly, the native Greek speaker did not consistently\nproduce a pharyngeal constriction of any kind in either his L1 rhotic\n(as expected) or in English /&#633;/. These results suggest that the\nacquisition and production of the pharyngeal gesture in the English\nrhotic approximant is particularly difficult for learners whose L1\nrhotics lack an identical constriction, potentially due to a general\ndifficulty of acquiring pharyngeal gestures that are not in the L1,\nthe similarity of the acoustic consequences of the different components\nof a rhotic, or L1 transfer into the L2.\n"
   ],
   "doi": "10.21437/Interspeech.2016-658"
  },
  "hennequin16_interspeech": {
   "authors": [
    [
     "Alexandre",
     "Hennequin"
    ],
    [
     "Amélie",
     "Rochet-Capellan"
    ],
    [
     "Marion",
     "Dohen"
    ]
   ],
   "title": "Auditory-Visual Perception of VCVs Produced by People with Down Syndrome: Preliminary Results",
   "original": "1198",
   "page_count": 5,
   "order": 46,
   "p1": "213",
   "pn": "217",
   "abstract": [
    "Down Syndrome (DS) is a genetic disease involving a number of anatomical,\nphysiological and cognitive impairments. More particularly it affects\nspeech production abilities. This results in reduced intelligibility\nwhich has however only been evaluated auditorily. Yet, many studies\nhave demonstrated that adding vision to audition helps perception of\nspeech produced by people without impairments especially when it is\ndegraded as is the case in noise. The present study aims at examining\nwhether the visual information improves intelligibility of people with\nDS. 24 participants without DS were presented with VCV sequences (vowel-consonant-vowel)\nproduced by four adults (2 with DS and 2 without DS). These stimuli\nwere presented in noise in three modalities: auditory, auditory-visual\nand visual. The results confirm a reduced auditory intelligibility\nof speakers with DS. They also show that, for the speakers involved\nin this study, visual intelligibility is equivalent to that of speakers\nwithout DS and compensates for the auditory intelligibility loss. An\nanalysis of the perceptual errors shows that most of them involve confusions\nbetween consonants. These results put forward the crucial role of multimodality\nin the improvement of the intelligibility of people with DS.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1198"
  },
  "ylmaz16_interspeech": {
   "authors": [
    [
     "Emre",
     "Yılmaz"
    ],
    [
     "Mario",
     "Ganzeboom"
    ],
    [
     "Catia",
     "Cucchiarini"
    ],
    [
     "Helmer",
     "Strik"
    ]
   ],
   "title": "Combining Non-Pathological Data of Different Language Varieties to Improve DNN-HMM Performance on Pathological Speech",
   "original": "0109",
   "page_count": 5,
   "order": 47,
   "p1": "218",
   "pn": "222",
   "abstract": [
    "Research on automatic speech recognition (ASR) of pathological speech\nis particularly hindered by scarce in-domain data resources. Collecting\nrepresentative pathological speech data is difficult due to the large\nvariability caused by the nature and severity of the disorders, and\nthe rigorous ethical and medical permission requirements. This task\nbecomes even more challenging for languages which have fewer resources,\nfewer speakers and fewer patients than English, such as the mid-sized\nlanguage Dutch. In this paper, we investigate the impact of combining\nspeech data from different varieties of the Dutch language for training\ndeep neural network (DNN)-based acoustic models. Flemish is chosen\nas the target variety for testing the acoustic models, since a Flemish\ndatabase of pathological speech, the COPAS database, is available.\nWe use non-pathological speech data from the northern Dutch and Flemish\nvarieties and perform speaker-independent recognition using the DNN-HMM\nsystem trained on the combined data. The results show that this system\nprovides improved recognition of pathological Flemish speech compared\nto a baseline system trained only on Flemish data. These findings open\nup new opportunities for developing useful ASR-based pathological speech\napplications for languages that are smaller in size and less resourced\nthan English.\n"
   ],
   "doi": "10.21437/Interspeech.2016-109"
  },
  "laaridh16_interspeech": {
   "authors": [
    [
     "Imed",
     "Laaridh"
    ],
    [
     "Corinne",
     "Fredouille"
    ],
    [
     "Christine",
     "Meunier"
    ]
   ],
   "title": "Evaluation of a Phone-Based Anomaly Detection Approach for Dysarthric Speech",
   "original": "1077",
   "page_count": 5,
   "order": 48,
   "p1": "223",
   "pn": "227",
   "abstract": [
    "Perceptual evaluation is still the most common method in clinical practice\nfor the diagnosing and the following of the condition progression of\npeople with speech disorders. Many automatic approaches were proposed\nto provide objective tools to deal with speech disorders and help professionals\nin the severity evaluation of speech impairments. This paper investigates\nan automatic phone-based anomaly detection approach implying an automatic\ntext-constrained phone alignment. Here, anomalies are related to speech\nsegments, for which an unexpected acoustic pattern is observed, compared\nwith a normal speech production. This objective tool is applied to\nFrench dysarthric speech recordings produced by patients suffering\nfrom four different pathologies. The behavior of the anomaly detection\napproach is studied according to the precision of the automatic phone\nalignment. Faced with the difficulties of having a gold standard reference,\nespecially for the phone-based anomaly annotation, this behavior is\nobserved on both annotated and non-annotated corpora. As expected,\nalignment errors (large shifts compared with a manual segmentation)\nlead to a large amount of anomalies automatically detected. However,\nabout 50% of correctly detected anomalies are not related to alignment\nerrors. This behavior shows that the automatic approach is able to\ncatch irregular acoustic patterns of phones.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1077"
  },
  "bhat16_interspeech": {
   "authors": [
    [
     "Chitralekha",
     "Bhat"
    ],
    [
     "Bhavik",
     "Vachhani"
    ],
    [
     "Sunil",
     "Kopparapu"
    ]
   ],
   "title": "Recognition of Dysarthric Speech Using Voice Parameters for Speaker Adaptation and Multi-Taper Spectral Estimation",
   "original": "1085",
   "page_count": 5,
   "order": 49,
   "p1": "228",
   "pn": "232",
   "abstract": [
    "Dysarthria is a motor speech disorder resulting from impairment in\nmuscles responsible for speech production, often characterized by slurred\nor slow speech resulting in low intelligibility. With speech based\napplications such as voice biometrics and personal assistants gaining\npopularity, automatic recognition of dysarthric speech becomes imperative\nas a step towards including people with dysarthria into mainstream.\nIn this paper we examine the applicability of voice parameters that\nare traditionally used for pathological voice classification such as\njitter, shimmer, F0 and Noise Harmonic Ratio (NHR) contour in addition\nto Mel Frequency Cepstral Coefficients (MFCC) for dysarthric speech\nrecognition. Additionally, we show that multi-taper spectral estimation\nfor computing MFCC improves the unseen dysarthric speech recognition.\nA Deep neural network (DNN) - hidden Markov model (HMM) recognition\nsystem fared better than a Gaussian Mixture Model (GMM) - HMM based\nsystem for dysarthric speech recognition. We propose a method to optimally\nuse incremental dysarthric data to improve dysarthric speech recognition\nfor an ASR with DNN-HMM. All evaluations were done on Universal Access\nSpeech Corpus.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1085"
  },
  "chen16c_interspeech": {
   "authors": [
    [
     "Fei",
     "Chen"
    ],
    [
     "Nan",
     "Yan"
    ],
    [
     "Xiaojie",
     "Pan"
    ],
    [
     "Feng",
     "Yang"
    ],
    [
     "Zhuanzhuan",
     "Ji"
    ],
    [
     "Lan",
     "Wang"
    ],
    [
     "Gang",
     "Peng"
    ]
   ],
   "title": "Impaired Categorical Perception of Mandarin Tones and its Relationship to Language Ability in Autism Spectrum Disorders",
   "original": "1133",
   "page_count": 5,
   "order": 50,
   "p1": "233",
   "pn": "237",
   "abstract": [
    "While enhanced pitch processing appears to be characteristic of many\nindividuals with autism spectrum disorders (ASD), it remains unclear\nwhether enhancement in pitch perception applies to those who speak\na tone language. Using a classic paradigm of categorical perception\n(CP), the present study investigated the perception of Mandarin tones\nin six- to eight-year-old children with ASD, and compared it with age-matched\ntypically developing children. In stark contrast to controls, the child\nparticipants with ASD exhibited a much wider boundary width (i.e.,\nmore gentle slope), and showed no improved discrimination for pairs\nstraddling the boundary, indicating impaired CP of Mandarin tones.\nMoreover, identification skills of different tone categories were positively\ncorrelated with language ability among children with ASD. These findings\nrevealed aberrant tone processing in Mandarin-speaking individuals\nwith ASD, especially in those with significant language impairment.\nOur results are in support of the notion of impaired change detection\nfor the linguistic elements of speech in children with ASD.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1133"
  },
  "nagle16_interspeech": {
   "authors": [
    [
     "K.F.",
     "Nagle"
    ],
    [
     "J.T.",
     "Heaton"
    ]
   ],
   "title": "Perceived Naturalness of Electrolaryngeal Speech Produced Using sEMG-Controlled vs. Manual Pitch Modulation",
   "original": "1476",
   "page_count": 5,
   "order": 51,
   "p1": "238",
   "pn": "242",
   "abstract": [
    "Producing speech with natural prosodic patterns is an ongoing challenge\nfor users of electrolaryngeal (EL) speech. This study describes speech\nproduced using a method currently in development, wherein a prosodic\npattern is derived from skin surface electromyographical (sEMG) signals\nrecorded from under the chin (submental surface).<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  Eight laryngectomees\nwho currently use a TruTone EL as their primary or backup mode of speech\nprovided samples of EL speech in two modes: conventional thumb-pressure\npitch-modulated control (represented by the TruTone EL; Griffin Laboratories,\nCA, U.S.A.) and sEMG-based pitch-modulated control (EMG-EL). Ratings\nof perceived naturalness were obtained from ten listeners unfamiliar\nwith EL speech.<br style=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  Listener ratings indicated that five speakers produced equally\nnatural speech using both devices, and three produced significantly\nmore natural speech using the EMG-EL than the TruTone EL. Mean fundamental\nfrequency (f0) was similar within speakers for both modes; however,\nmean f0 range and standard deviation were significantly larger for\nthe EMG-EL than for the TruTone EL, despite both devices having similar\npotential f0 range. This study showed that the EMG-EL provides an intuitive\nmeans of controlling f0-based prosodic patterns that are more natural-sounding\nthan push-button control for some EL users.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1476"
  },
  "najnin16_interspeech": {
   "authors": [
    [
     "Shamima",
     "Najnin"
    ],
    [
     "Bonny",
     "Banerjee"
    ],
    [
     "Lisa Lucks",
     "Mendel"
    ],
    [
     "Masoumeh Heidari",
     "Kapourchali"
    ],
    [
     "Jayanta Kumar",
     "Dutta"
    ],
    [
     "Sungmin",
     "Lee"
    ],
    [
     "Chhayakanta",
     "Patro"
    ],
    [
     "Monique",
     "Pousson"
    ]
   ],
   "title": "Identifying Hearing Loss from Learned Speech Kernels",
   "original": "1488",
   "page_count": 5,
   "order": 52,
   "p1": "243",
   "pn": "247",
   "abstract": [
    "Does a hearing-impaired individual&#8217;s speech reflect his hearing\nloss? To investigate this question, we recorded at least four hours\nof speech data from each of 29 adult individuals, both male and female,\nbelonging to four classes: 3 normal, and 26 severely-to-profoundly\nhearing impaired with high, medium or low speech intelligibility. Acoustic\nkernels were learned for each individual by capturing the distribution\nof his speech data points represented as 20 ms duration windows. These\nkernels were evaluated using a set of neurophysiological metrics, namely,\ndistribution of characteristic frequencies, equal loudness contour,\nbandwidth and Q<SUB>10</SUB> value of tuning curve. It turns out that,\nfor our cohort, a feature vector can be constructed out of four properties\nof these metrics that would accurately classify hearing-impaired individuals\nwith low intelligible speech from normal ones using a linear classifier.\nHowever, the overlap in the feature space between normal and hearing-impaired\nindividuals increases as the speech becomes more intelligible. We conclude\nthat a hearing-impaired individual&#8217;s speech does reflect his\nhearing loss provided his loss of hearing has considerably affected\nthe intelligibility of his speech.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1488"
  },
  "rong16_interspeech": {
   "authors": [
    [
     "Panying",
     "Rong"
    ],
    [
     "Yana",
     "Yunusova"
    ],
    [
     "Jordan R.",
     "Green"
    ]
   ],
   "title": "Differential Effects of Velopharyngeal Dysfunction on Speech Intelligibility During Early and Late Stages of Amyotrophic Lateral Sclerosis",
   "original": "1524",
   "page_count": 5,
   "order": 53,
   "p1": "248",
   "pn": "252",
   "abstract": [
    "The detrimental effects of velopharyngeal dysfunction (VPD) on speech\nintelligibility in persons with progressive motor speech disorders\nare poorly understood. In this study, we longitudinally investigated\nthe velopharyngeal and articulatory performance of 142 individuals\nwith varying severities of amyotrophic lateral sclerosis (ALS). Our\ngoal was to determine the mechanisms that underlie the effects of VPD\non speech intelligibility during early and late stages of ALS progression.\nWe found that during the early stages of the disease, the effect of\nVPD on intelligibility was partially mitigated by an increase in articulatory\n(e.g., lower lip and jaw) movement speed. This apparent articulatory\ncompensation eventually became unavailable during the late stages of\ndisease progression, which led to rapid declines of speech intelligibility.\nThe transition across the early and late stages was characterized by\nthe slowing of the composite movement of lower lip and jaw below 138\nmm/s, which indicated the onset of precipitous speech decline and thus,\nmay provide important timing information for helping clinicians to\nplan interventions.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1524"
  },
  "delvaux16_interspeech": {
   "authors": [
    [
     "V.",
     "Delvaux"
    ],
    [
     "V.",
     "Roland"
    ],
    [
     "K.",
     "Huet"
    ],
    [
     "M.",
     "Piccaluga"
    ],
    [
     "M.C.",
     "Haelewyck"
    ],
    [
     "B.",
     "Harmegnies"
    ]
   ],
   "title": "The Production of Intervocalic Glides in Non Dysarthric Parkinsonian Speech",
   "original": "0349",
   "page_count": 4,
   "order": 54,
   "p1": "253",
   "pn": "256",
   "abstract": [
    "In the context of a research project aiming at investigating the relationships\nbetween speech disorders, quality of life and social participation\nin Parkinson&#8217;s Disease (PD), we report here on an acoustic study\nof glides and steady vowels by non dysarthric parkinsonian and control\nspeakers. Our specific aim is to explore the dynamics of supra-laryngeal\narticulators in PD. Results suggest that non dysarthric Parkinsonian\nspeakers maintain an accurate production of glides in VC[glide]V pseudo-words\nat the expense of articulatory undershoot in the surrounding vowels,\nand some asymmetry between the V1-to-glide and glide-to-V2 articulatory\nmovements. We discuss how these results both support and challenge\nthe accuracy-tempo trade-off hypothesis (Ackermann and Ziegler, 1991).\n"
   ],
   "doi": "10.21437/Interspeech.2016-349"
  },
  "feng16_interspeech": {
   "authors": [
    [
     "Yang",
     "Feng"
    ],
    [
     "Zhang",
     "Lu"
    ]
   ],
   "title": "Auditory Processing Impairments Under Background Noise in Children with Non-Syndromic Cleft Lip and/or Palate",
   "original": "0038",
   "page_count": 5,
   "order": 55,
   "p1": "257",
   "pn": "261",
   "abstract": [
    "Cleft lip and/or palate (CL/P) disorders are commonly occurring congenital\nmalformations and hearing impairment is a very common co-morbidity.\nMost previous research has only focused on middle ear disorders and\nrelated auditory consequences in this group. Studies of higher level\nauditory status and central auditory processing abilities of this group\nhave been unsystematic. The present study was conducted in order to\nobjectively investigate the central auditory abilities in children\nwith non-syndromic cleft lip and/or palate (NSCLP). A structured behavioral\ncentral auditory test battery was conducted in a group of children\nwith NSCLP and their age/sex matched normal peers. The following behavioral\ncentral auditory tasks were undertaken, including hearing in noise\ntest (HINT), dichotic digits test (DDT), and gaps in noise test (GIN).\nResults showed that there were no significant group differences in\nDDT test, indicating that the binaural separation and integration abilities\ncould be normal in children with NSCLP. However, the cleft group performed\nsignificantly poorer than their normal peers for each ear in HINT test\nunder noise condition and GIN test, suggesting that the children with\nNSCLP could have impaired monaural low redundancy auditory processing\nability, and at risk of temporal resolution disability.\n"
   ],
   "doi": "10.21437/Interspeech.2016-38"
  },
  "zhu16_interspeech": {
   "authors": [
    [
     "Zhi",
     "Zhu"
    ],
    [
     "Ryota",
     "Miyauchi"
    ],
    [
     "Yukiko",
     "Araki"
    ],
    [
     "Masashi",
     "Unoki"
    ]
   ],
   "title": "Modulation Spectral Features for Predicting Vocal Emotion Recognition by Simulated Cochlear Implants",
   "original": "0737",
   "page_count": 5,
   "order": 56,
   "p1": "262",
   "pn": "266",
   "abstract": [
    "It has been reported that vocal emotion recognition is challenging\nfor cochlear implant (CI) listeners due to the limited spectral cues\nwith CI devices. As the mechanism of CI, modulation information is\nprovided as a primarily cue. Previous studies have revealed that the\nmodulation components of speech are important for speech intelligibility.\nHowever, it is unclear whether modulation information can contribute\nto vocal emotion recognition. We investigated the relationship between\nhuman perception of vocal emotion and the modulation spectral features\nof emotional speech. For human perception, we carried out a vocal-emotion\nrecognition experiment using noise-vocoder simulations with normal-hearing\nlisteners to predict the response from CI listeners. For modulation\nspectral features, we used auditory-inspired processing (auditory filterbank,\ntemporal envelope extraction, modulation filterbank) to obtain the\nmodulation spectrogram of emotional speech signals. Ten types of modulation\nspectral feature were then extracted from the modulation spectrogram.\nAs a result, modulation spectral centroid, modulation spectral kurtosis,\nand modulation spectral tilt exhibited similar trends with the results\nof human perception. This suggests that these modulation spectral features\nmay be important cues for voice emotion recognition with noise-vocoded\nspeech.\n"
   ],
   "doi": "10.21437/Interspeech.2016-737"
  },
  "ochi16_interspeech": {
   "authors": [
    [
     "Keiko",
     "Ochi"
    ],
    [
     "Koichi",
     "Mori"
    ],
    [
     "Naomi",
     "Sakai"
    ],
    [
     "Nobutaka",
     "Ono"
    ]
   ],
   "title": "Automatic Discrimination of Soft Voice Onset Using Acoustic Features of Breathy Voicing",
   "original": "0765",
   "page_count": 5,
   "order": 57,
   "p1": "267",
   "pn": "271",
   "abstract": [
    "Soft onset vocalization is used in certain speech therapies. However,\nit is not easy to practice it at home because the acoustical evaluation\nitself needs training. It would be helpful for speech patients to get\nobjective feedback during training. In this paper, new parameters for\nidentifying soft onset with high accuracy are described. One of the\nparameters measures an aspect of the soft voice onset, in which the\nvocal folds start to oscillate periodically before coming in contact\nwith each other at the beginning of vocalization. Combined with an\nonset time exceeding a threshold, the proposed parameters gave about\n99% accuracy in identifying soft onset vocalization.\n"
   ],
   "doi": "10.21437/Interspeech.2016-765"
  },
  "shao16_interspeech": {
   "authors": [
    [
     "Jing",
     "Shao"
    ],
    [
     "Caicai",
     "Zhang"
    ],
    [
     "Gang",
     "Peng"
    ],
    [
     "Yike",
     "Yang"
    ],
    [
     "William S.-Y.",
     "Wang"
    ]
   ],
   "title": "Effect of Noise on Lexical Tone Perception in Cantonese-Speaking Amusics",
   "original": "0891",
   "page_count": 5,
   "order": 58,
   "p1": "272",
   "pn": "276",
   "abstract": [
    "Congenital amusia is a neurogenetic disorder affecting musical pitch\nprocessing. It also affects lexical tone perception. It is well documented\nthat noisy conditions impact speech perception in second language learners\nand cochlear implant users. However, it is yet unclear whether and\nhow noise affects lexical tone perception in the amusics. This paper\nexamined the effect of multi-talker babble noise [1] on lexical tone\nidentification and discrimination in 14 Cantonese-speaking amusics\nand 14 controls at three levels of signal-to-noise ratio (SNR). Results\nreveal that the amusics were less accurate in the  identification of\ntones compared to controls in all SNR conditions. They also showed\ndegraded performance in the  discrimination, but less severe than in\nthe identification. These results confirmed that amusia influences\nlexical tone processing. But the amusics were not influenced more by\nnoise than the controls in either identification or discrimination.\nThis indicates that the deficits of amusia may not be due to the lack\nof native-like language processing mechanisms or are mechanical in\nnature, as in the case of second language learners and cochlear implant\nusers. Instead, the amusics may be impaired in the linguistic processing\nof native tones, showing impaired tone perception already under the\nclear condition.\n"
   ],
   "doi": "10.21437/Interspeech.2016-891"
  },
  "takashima16_interspeech": {
   "authors": [
    [
     "Yuki",
     "Takashima"
    ],
    [
     "Ryo",
     "Aihara"
    ],
    [
     "Tetsuya",
     "Takiguchi"
    ],
    [
     "Yasuo",
     "Ariki"
    ],
    [
     "Nobuyuki",
     "Mitani"
    ],
    [
     "Kiyohiro",
     "Omori"
    ],
    [
     "Kaoru",
     "Nakazono"
    ]
   ],
   "title": "Audio-Visual Speech Recognition Using Bimodal-Trained Bottleneck Features for a Person with Severe Hearing Loss",
   "original": "0721",
   "page_count": 5,
   "order": 59,
   "p1": "277",
   "pn": "281",
   "abstract": [
    "In this paper, we propose an audio-visual speech recognition system\nfor a person with an articulation disorder resulting from severe hearing\nloss. In the case of a person with this type of articulation disorder,\nthe speech style is quite different from those of people without hearing\nloss that a speaker-independent acoustic model for unimpaired persons\nis hardly useful for recognizing it. The audio-visual speech recognition\nsystem we present in this paper is for a person with severe hearing\nloss in noisy environments. Although feature integration is an important\nfactor in multimodal speech recognition, it is difficult to integrate\nefficiently because those features are different intrinsically. We\npropose a novel visual feature extraction approach that connects the\nlip image to audio features efficiently, and the use of convolutive\nbottleneck networks (CBNs) increases robustness with respect to speech\nfluctuations caused by hearing loss. The effectiveness of this approach\nwas confirmed through word-recognition experiments in noisy environments,\nwhere the CBN-based feature extraction method outperformed the conventional\nmethods.\n"
   ],
   "doi": "10.21437/Interspeech.2016-721"
  },
  "gu16_interspeech": {
   "authors": [
    [
     "Yuling",
     "Gu"
    ],
    [
     "Boon Pang",
     "Lim"
    ],
    [
     "Nancy F.",
     "Chen"
    ]
   ],
   "title": "Perception of Tone in Whispered Mandarin Sentences: The Case for Singapore Mandarin",
   "original": "0297",
   "page_count": 5,
   "order": 60,
   "p1": "282",
   "pn": "286",
   "abstract": [
    "Whispering is commonly used when one needs to speak softly (for instance,\nin a library). Whispered speech mainly differs from neutral speech\nin that voicing, and thus its acoustic correlate F0, is absent. It\nis well known that in tonal languages such as Mandarin, tone identity\nis primarily conveyed by the F0 contour. Previous works also suggest\nthat secondary correlates are both consistent and sufficient to convey\nMandarin tone in whisper. However, these results are focused on Standard\nMandarin spoken in Mainland China and have only been obtained via small-scale\nexperiments using citation-form speech. To investigate whether these\nresults will carry over to continuous sentences in other variations\nof Mandarin, we present a study that is the first of its nature to\nexplore native Singapore Mandarin. Unlike related works, our large-scale\nperceptual experiment thoroughly investigates lexical tones in whispered\nand neutral Mandarin by involving more diverse speech data, greater\nnumber of listeners and use syllables excised from continuous speech\nto better simulate natural speech conditions. Our findings differ significantly\nfrom earlier works in terms of the recognition patterns observed. We\npresent further in-depth analysis on how various phonetic characteristics\n(vowel contexts, place and manner of articulation) affect whispered\ntone perception.\n"
   ],
   "doi": "10.21437/Interspeech.2016-297"
  },
  "xie16_interspeech": {
   "authors": [
    [
     "Feng-Long",
     "Xie"
    ],
    [
     "Frank K.",
     "Soong"
    ],
    [
     "Haifeng",
     "Li"
    ]
   ],
   "title": "A KL Divergence and DNN-Based Approach to Voice Conversion without Parallel Training Sentences",
   "original": "0116",
   "page_count": 5,
   "order": 61,
   "p1": "287",
   "pn": "291",
   "abstract": [
    "We extend our recently proposed approach to cross-lingual TTS training\nto voice conversion, without using parallel training sentences. It\nemploys Speaker Independent, Deep Neural Net (SI-DNN) ASR to equalize\nthe difference between source and target speakers and Kullback-Leibler\nDivergence (KLD) to convert spectral parameters probabilistically in\nthe phonetic space via ASR senone posterior probabilities of the two\nspeakers. With or without knowing the transcriptions of the target\nspeaker&#8217;s training speech, the approach can be either supervised\nor unsupervised. In a supervised mode, where adequate training data\nof the target speaker with transcriptions is used to train a GMM-HMM\nTTS of the target speaker, each frame of the source speakers input\ndata is mapped to the closest senone in thus trained TTS. The mapping\nis done via the posterior probabilities computed by SI-DNN ASR and\nthe minimum KLD matching. In a unsupervised mode, all training data\nof the target speaker is first grouped into phonetic clusters where\nKLD is used as the sole distortion measure. Once the phonetic clusters\nare trained, each frame of the source speakers input is then mapped\nto the mean of the closest phonetic cluster. The final converted speech\nis generated with the max probability trajectory generation algorithm.\nBoth objective and subjective evaluations show the proposed approach\ncan achieve higher speaker similarity and better spectral distortions,\nwhen comparing with the baseline system based upon our sequential error\nminimization trained DNN algorithm.\n"
   ],
   "doi": "10.21437/Interspeech.2016-116"
  },
  "aihara16_interspeech": {
   "authors": [
    [
     "Ryo",
     "Aihara"
    ],
    [
     "Tetsuya",
     "Takiguchi"
    ],
    [
     "Yasuo",
     "Ariki"
    ]
   ],
   "title": "Parallel Dictionary Learning for Voice Conversion Using Discriminative Graph-Embedded Non-Negative Matrix Factorization",
   "original": "0227",
   "page_count": 5,
   "order": 62,
   "p1": "292",
   "pn": "296",
   "abstract": [
    "This paper proposes a discriminative learning method for Non-negative\nMatrix Factorization (NMF)-based Voice Conversion (VC). NMF-based VC\nhas been researched because of the natural-sounding voice it produces\ncompared with conventional Gaussian Mixture Model (GMM)-based VC. In\nconventional NMF-based VC, parallel exemplars are used as the dictionary;\ntherefore, dictionary learning is not adopted. In order to enhance\nthe conversion quality of NMF-based VC, we propose Discriminative Graph-embedded\nNon-negative Matrix Factorization (DGNMF). Parallel dictionaries of\nthe source and target speakers are discriminatively estimated by using\nDGNMF based on the phoneme labels of the training data. Experimental\nresults show that our proposed method can not only improve the conversion\nquality but also reduce the computational times.\n"
   ],
   "doi": "10.21437/Interspeech.2016-227"
  },
  "gu16b_interspeech": {
   "authors": [
    [
     "Yu",
     "Gu"
    ],
    [
     "Zhen-Hua",
     "Ling"
    ],
    [
     "Li-Rong",
     "Dai"
    ]
   ],
   "title": "Speech Bandwidth Extension Using Bottleneck Features and Deep Recurrent Neural Networks",
   "original": "0678",
   "page_count": 5,
   "order": 63,
   "p1": "297",
   "pn": "301",
   "abstract": [
    "This paper presents a novel method for speech bandwidth extension (BWE)\nusing deep structured neural networks. In order to utilize linguistic\ninformation during the prediction of high-frequency spectral components,\nthe bottleneck (BN) features derived from a deep neural network (DNN)-based\nstate classifier for narrowband speech are employed as auxiliary input.\nFurthermore, recurrent neural networks (RNNs) incorporating long short-term\nmemory (LSTM) cells are adopted to model the complex mapping relationship\nbetween the feature sequences describing low-frequency and high-frequency\nspectra. Experimental results show that the BWE method proposed in\nthis paper can achieve better performance than the conventional method\nbased on Gaussian mixture models (GMMs) and the state-of-the-art approach\nbased on DNNs in both objective and subjective tests.\n"
   ],
   "doi": "10.21437/Interspeech.2016-678"
  },
  "yang16b_interspeech": {
   "authors": [
    [
     "Yi",
     "Yang"
    ],
    [
     "Hidetsugu",
     "Uchida"
    ],
    [
     "Daisuke",
     "Saito"
    ],
    [
     "Nobuaki",
     "Minematsu"
    ]
   ],
   "title": "Voice Conversion Based on Matrix Variate Gaussian Mixture Model Using Multiple Frame Features",
   "original": "0705",
   "page_count": 5,
   "order": 64,
   "p1": "302",
   "pn": "306",
   "abstract": [
    "This paper presents a novel voice conversion method based on matrix\nvariate Gaussian mixture model (MV-GMM) using features of multiple\nframes. In voice conversion studies, approaches based on Gaussian mixture\nmodels (GMM) are still widely utilized because of their flexibility\nand easiness in handling. They treat the joint probability density\nfunction (PDF) of feature vectors from source and target speakers as\nthat of joint vectors of the two vectors. Addition of dynamic features\nto the feature vectors in GMM-based approaches achieves certain performance\nimprovements because the correlation between multiple frames is taken\ninto account. Recently, a voice conversion framework based on MV-GMM,\nin which the joint PDF is modeled in a matrix variate space, has been\nproposed and it is able to precisely model both the characteristics\nof the feature spaces and the relation between the source and target\nspeakers. In this paper, in order to additionally model the correlation\nbetween multiple frames in the framework more consistently, MV-GMM\nis constructed in a matrix variate space containing the features of\nneighboring frames. Experimental results show that an certain performance\nimprovement in both objective and subjective evaluations is observed.\n"
   ],
   "doi": "10.21437/Interspeech.2016-705"
  },
  "hosaka16_interspeech": {
   "authors": [
    [
     "Naoki",
     "Hosaka"
    ],
    [
     "Kei",
     "Hashimoto"
    ],
    [
     "Keiichiro",
     "Oura"
    ],
    [
     "Yoshihiko",
     "Nankaku"
    ],
    [
     "Keiichi",
     "Tokuda"
    ]
   ],
   "title": "Voice Conversion Based on Trajectory Model Training of Neural Networks Considering Global Variance",
   "original": "1035",
   "page_count": 5,
   "order": 65,
   "p1": "307",
   "pn": "311",
   "abstract": [
    "This paper proposes a new training method of deep neural networks (DNNs)\nfor statistical voice conversion. DNNs are now being used as conversion\nmodels that represent mapping from source features to target features\nin statistical voice conversion. However, there are two major problems\nto be solved in conventional DNN-based voice conversion: 1) the inconsistency\nbetween the training and synthesis criteria, and 2) the over-smoothing\nof the generated parameter trajectories. In this paper, we introduce\na parameter trajectory generation process considering the global variance\n(GV) into the training of DNNs for voice conversion. A consistent framework\nusing the same criterion for both training and synthesis provides better\nconversion accuracy in the original static feature domain, and the\nover-smoothing can be avoided by optimizing the DNN parameters on the\nbasis of the trajectory likelihood considering the GV. Experimental\nresults show that the proposed method outperforms the DNN-based method\nin term of both speech quality and speaker similarity.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1035"
  },
  "aryal16_interspeech": {
   "authors": [
    [
     "Sandesh",
     "Aryal"
    ],
    [
     "Ricardo",
     "Gutierrez-Osuna"
    ]
   ],
   "title": "Comparing Articulatory and Acoustic Strategies for Reducing Non-Native Accents",
   "original": "1131",
   "page_count": 5,
   "order": 66,
   "p1": "312",
   "pn": "316",
   "abstract": [
    "This article presents an experimental comparison of two types of techniques,\narticulatory and acoustic, for transforming non-native speech to sound\nmore native-like. Articulatory techniques use articulators from a native\nspeaker to drive an articulatory synthesizer of the non-native speaker.\nThese methods have a good theoretical justification, but articulatory\nmeasurements (e.g., via electromagnetic articulography) are difficult\nto obtain. In contrast, acoustic methods use techniques from the voice\nconversion literature to build a mapping between the two acoustic spaces,\nmaking them more attractive for practical applications (e.g., language\nlearning). We compare two representative implementations of these approaches,\nboth based on statistical parametric speech synthesis. Through a series\nof perceptual listening tests, we evaluate the two approaches in terms\nof accent reduction, speech intelligibility and speaker quality. Our\nresults show that the acoustic method is more effective than the articulatory\nmethod in reducing perceptual ratings of non-native accents, and also\nproduces synthesis of higher intelligibility while preserving voice\nquality.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1131"
  },
  "sarfjoo16_interspeech": {
   "authors": [
    [
     "Seyyed Saeed",
     "Sarfjoo"
    ],
    [
     "Cenk",
     "Demiroglu"
    ]
   ],
   "title": "Cross-Lingual Speaker Adaptation for Statistical Speech Synthesis Using Limited Data",
   "original": "0345",
   "page_count": 5,
   "order": 67,
   "p1": "317",
   "pn": "321",
   "abstract": [
    "Cross-lingual speaker adaptation with limited adaptation data has many\napplications such as use in speech-to-speech translation systems. Here,\nwe focus on cross-lingual adaptation for statistical speech synthesis\n(SSS) systems using limited adaptation data. To that end, we propose\ntwo techniques exploiting a bilingual Turkish-English speech database\nthat we collected. In one approach, speaker-specific state-mapping\nis proposed for cross-lingual adaptation which performed significantly\nbetter than the baseline state-mapping algorithm in adapting the excitation\nparameter both in objective and subjective tests. In the second approach,\neigenvoice adaptation is done in the input language which is then used\nto estimate the eigenvoice weights in the output language using weighted\nlinear regression. The second approach performed significantly better\nthan the baseline system in adapting the spectral envelope parameters\nboth in objective and subjective tests.\n"
   ],
   "doi": "10.21437/Interspeech.2016-345"
  },
  "sun16_interspeech": {
   "authors": [
    [
     "Lifa",
     "Sun"
    ],
    [
     "Hao",
     "Wang"
    ],
    [
     "Shiyin",
     "Kang"
    ],
    [
     "Kun",
     "Li"
    ],
    [
     "Helen",
     "Meng"
    ]
   ],
   "title": "Personalized, Cross-Lingual TTS Using Phonetic Posteriorgrams",
   "original": "1043",
   "page_count": 5,
   "order": 68,
   "p1": "322",
   "pn": "326",
   "abstract": [
    "We present a novel approach that enables a target speaker (e.g. monolingual\nChinese speaker) to speak a new language (e.g. English) based on arbitrary\ntextual input. Our system includes a trained English speaker-independent\nautomatic speech recognition (SI-ASR) engine using TIMIT. Given the\ntarget speaker&#8217;s speech in a non-target language, we generate\nPhonetic PosteriorGrams (PPGs) with the SI-ASR and then train a Deep\nBidirectional Long Short-Term Memory based Recurrent Neural Networks\n(DBLSTM) to model the relationships between the PPGs and the acoustic\nsignal. Synthesis involves input of arbitrary text to a general TTS\nengine (trained on any non-target speaker), the output of which is\nindexed by SI-ASR as PPGs. These are used by the DBLSTM to synthesize\nthe target language in the target speaker&#8217;s voice. A main advantage\nof this approach has very low training data requirement of the target\nspeaker which can be in any language, as compared with a reference\napproach of training a special TTS engine using many recordings from\nthe target speaker only in the target language. For a given target\nspeaker, our proposed approach trained on 100 Mandarin (i.e. non-target\nlanguage) utterances achieves comparable performance (in MOS and ABX\ntest) of English synthetic speech as an HTS system trained on 1,000\nEnglish utterances.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1043"
  },
  "prakash16_interspeech": {
   "authors": [
    [
     "Anusha",
     "Prakash"
    ],
    [
     "Jeena J.",
     "Prakash"
    ],
    [
     "Hema A.",
     "Murthy"
    ]
   ],
   "title": "Acoustic Analysis of Syllables Across Indian Languages",
   "original": "1127",
   "page_count": 5,
   "order": 69,
   "p1": "327",
   "pn": "331",
   "abstract": [
    "Indian languages are broadly classified as Indo-Aryan or Dravidian.\nThe basic set of phones is more or less the same, varying mostly in\nthe phonotactics across languages. There has also been borrowing of\nsounds and words across languages over time due to intermixing of cultures.\nSince syllables are fundamental units of speech production and Indian\nlanguages are characterised by syllable-timed rhythm, acoustic analysis\nof syllables has been carried out.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  In this paper, instances\nof common and most frequent syllables in continuous speech have been\nstudied across six Indian languages, from both Indo-Aryan and Dravidian\nlanguage groups. The distributions of acoustic features have been compared\nacross these languages. This kind of analysis is useful for developing\nspeech technologies in a multilingual scenario. Owing to similarities\nin the languages, text-to-speech (TTS) synthesisers have been developed\nby segmenting speech data at the phone level using hidden Markov models\n(HMM) from other languages as initial models. Degradation mean opinion\nscores and word error rates indicate that the quality of synthesised\nspeech is comparable to that of TTSes developed by segmenting the data\nusing language-specific HMMs.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1127"
  },
  "zhang16_interspeech": {
   "authors": [
    [
     "Teng",
     "Zhang"
    ],
    [
     "Zhipeng",
     "Chen"
    ],
    [
     "Ji",
     "Wu"
    ],
    [
     "Sam",
     "Lai"
    ],
    [
     "Wenhui",
     "Lei"
    ],
    [
     "Carsten",
     "Isert"
    ]
   ],
   "title": "Objective Evaluation Methods for Chinese Text-To-Speech Systems",
   "original": "0421",
   "page_count": 5,
   "order": 70,
   "p1": "332",
   "pn": "336",
   "abstract": [
    "To objectively evaluate the performance of text-to-speech (TTS) systems,\nmany studies have been conducted in the straightforward way to compare\nsynthesized speech and natural speech with the alignment. However,\nin most situations, there is no natural speech can be used. In this\npaper, we focus on machine learning approaches for the TTS evaluation.\nWe exploit a subspace decomposition method to separate different components\nin speech, which generates distinctive acoustic features automatically.\nFurthermore, a pairwise based Support Vector Machine (SVM) model is\nused to evaluate TTS systems. With the original prosodic acoustic features\nand Support Vector Regression model, we obtain a ranking relevance\nof 0.7709. Meanwhile, with the proposed oblique matrix projection method\nand pairwise SVM model, we achieve a much better result of 0.9115.\n"
   ],
   "doi": "10.21437/Interspeech.2016-421"
  },
  "ijima16_interspeech": {
   "authors": [
    [
     "Yusuke",
     "Ijima"
    ],
    [
     "Taichi",
     "Asami"
    ],
    [
     "Hideyuki",
     "Mizuno"
    ]
   ],
   "title": "Objective Evaluation Using Association Between Dimensions Within Spectral Features for Statistical Parametric Speech Synthesis",
   "original": "0584",
   "page_count": 5,
   "order": 71,
   "p1": "337",
   "pn": "341",
   "abstract": [
    "This paper presents a novel objective evaluation technique for statistical\nparametric speech synthesis. One of its novel features is that it focuses\non the association between dimensions within the spectral features.\nWe first use a maximal information coefficient to analyze the relationship\nbetween subjective scores and associations of spectral features obtained\nfrom natural and various types of synthesized speech. The analysis\nresults indicate that the scores improve as the association becomes\nweaker. We then describe the proposed objective evaluation technique,\nwhich uses a voice conversion method to detect the associations within\nspectral features. We perform subjective and objective experiments\nto investigate the relationship between subjective scores and objective\nscores. The proposed objective scores are compared to the mel-cepstral\ndistortion. The results indicate that our objective scores achieve\ndramatically higher correlation to subjective scores than the mel-cepstral\ndistortion.\n"
   ],
   "doi": "10.21437/Interspeech.2016-584"
  },
  "yoshimura16_interspeech": {
   "authors": [
    [
     "Takenori",
     "Yoshimura"
    ],
    [
     "Gustav Eje",
     "Henter"
    ],
    [
     "Oliver",
     "Watts"
    ],
    [
     "Mirjam",
     "Wester"
    ],
    [
     "Junichi",
     "Yamagishi"
    ],
    [
     "Keiichi",
     "Tokuda"
    ]
   ],
   "title": "A Hierarchical Predictor of Synthetic Speech Naturalness Using Neural Networks",
   "original": "0847",
   "page_count": 5,
   "order": 72,
   "p1": "342",
   "pn": "346",
   "abstract": [
    "A problem when developing and tuning speech synthesis systems is that\nthere is no well-established method of automatically rating the quality\nof the synthetic speech. This research attempts to obtain a new automated\nmeasure which is trained on the result of large-scale subjective evaluations\nemploying many human listeners,  i.e., the Blizzard Challenge. To exploit\nthe data, we experiment with linear regression, feed-forward and convolutional\nneural network models, and combinations of them to regress from synthetic\nspeech to the perceptual scores obtained from listeners. The biggest\nimprovements were seen when combining stimulus- and system-level predictions.\n"
   ],
   "doi": "10.21437/Interspeech.2016-847"
  },
  "podsiado16_interspeech": {
   "authors": [
    [
     "Monika",
     "Podsiadło"
    ],
    [
     "Shweta",
     "Chahar"
    ]
   ],
   "title": "Text-to-Speech for Individuals with Vision Loss: A User Study",
   "original": "1376",
   "page_count": 5,
   "order": 73,
   "p1": "347",
   "pn": "351",
   "abstract": [
    "Individuals with vision loss use text-to-speech (TTS) for most of their\ninteraction with devices, and rely on the quality of synthetic voices\nto a much larger extent than any other user group. A significant amount\nof local synthesis requests for Google TTS comes from TalkBack, the\nAndroid screenreader, making it our top client and making the visually-impaired\nusers the heaviest consumers of the technology. Despite this, very\nlittle attention has been devoted to optimizing TTS voices for this\nuser group and the feedback on TTS voices from the blind has been traditionally\nless-favourable. We present the findings from a TTS user experience\nstudy conducted by Google with visually-impaired screen reader users.\nThe study comprised 14 focus groups and evaluated a total of 95 candidate\nvoices with 90 participants across 3 countries. The study uncovered\nthe distinctive usage patterns of this user group, which point to different\nTTS requirements and voice preferences from those of sighted users.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1376"
  },
  "valentinibotinhao16_interspeech": {
   "authors": [
    [
     "Cassia",
     "Valentini-Botinhao"
    ],
    [
     "Xin",
     "Wang"
    ],
    [
     "Shinji",
     "Takaki"
    ],
    [
     "Junichi",
     "Yamagishi"
    ]
   ],
   "title": "Speech Enhancement for a Noise-Robust Text-to-Speech Synthesis System Using Deep Recurrent Neural Networks",
   "original": "0159",
   "page_count": 5,
   "order": 74,
   "p1": "352",
   "pn": "356",
   "abstract": [
    "Quality of text-to-speech voices built from noisy recordings is diminished.\nIn order to improve it we propose the use of a recurrent neural network\nto enhance acoustic parameters prior to training. We trained a deep\nrecurrent neural network using a parallel database of noisy and clean\nacoustics parameters as input and output of the network. The database\nconsisted of multiple speakers and diverse noise conditions. We investigated\nusing text-derived features as an additional input of the network.\nWe processed a noisy database of two other speakers using this network\nand used its output to train an HMM acoustic text-to-synthesis model\nfor each voice. Listening experiment results showed that the voice\nbuilt with enhanced parameters was ranked significantly higher than\nthe ones trained with noisy speech and speech that has been enhanced\nusing a conventional enhancement system. The text-derived features\nimproved results only for the female voice, where it was ranked as\nhighly as a voice trained with clean speech.\n"
   ],
   "doi": "10.21437/Interspeech.2016-159"
  },
  "cooper16_interspeech": {
   "authors": [
    [
     "Erica",
     "Cooper"
    ],
    [
     "Alison",
     "Chang"
    ],
    [
     "Yocheved",
     "Levitan"
    ],
    [
     "Julia",
     "Hirschberg"
    ]
   ],
   "title": "Data Selection and Adaptation for Naturalness in HMM-Based Speech Synthesis",
   "original": "0502",
   "page_count": 5,
   "order": 75,
   "p1": "357",
   "pn": "361",
   "abstract": [
    "We describe experiments in building HMM text-to-speech voices on professional\nbroadcast news data from multiple speakers. We build on earlier work\ncomparing techniques for selecting utterances from the corpus and voice\nadaptation to produce the most natural-sounding voices. While our ultimate\ngoal is to develop intelligible and natural-sounding synthetic voices\nin low-resource languages rapidly and without the expense of collecting\nand annotating data specifically for text-to-speech, we focus on English\ninitially, in order to develop and evaluate our methods. We evaluate\nour approaches using crowdsourced listening tests for naturalness.\nWe have found that removing utterances that are outliers with respect\nto hyper-articulation, as well as combining the selection of hypo-articulated\nutterances and low mean f0 utterances, produce the most natural-sounding\nvoices.\n"
   ],
   "doi": "10.21437/Interspeech.2016-502"
  },
  "tao16_interspeech": {
   "authors": [
    [
     "Fei",
     "Tao"
    ],
    [
     "Louis",
     "Daudet"
    ],
    [
     "Christian",
     "Poellabauer"
    ],
    [
     "Sandra L.",
     "Schneider"
    ],
    [
     "Carlos",
     "Busso"
    ]
   ],
   "title": "A Portable Automatic  PA-TA-KA Syllable Detection System to Derive Biomarkers for Neurological Disorders",
   "original": "0789",
   "page_count": 5,
   "order": 76,
   "p1": "362",
   "pn": "366",
   "abstract": [
    "Neurological disorders disrupt brain functions, affecting the life\nof many individuals. Conventional neurological disorder diagnosis methods\nrequire inconvenient and expensive devices. Several studies have identified\nspeech biomarkers that are informative of neurological disorders, so\nspeech-based interfaces can provide effective, convenient and affordable\nprescreening tools for diagnosis. We have investigated stand-alone\nautomatic speech-based assessment tools for portable devices. Our current\ndata collection protocol includes seven brief tests for which we have\ndeveloped specialized  automatic speech recognition (ASR) systems.\nThe most challenging task from an ASR perspective is a popular diadochokinetic\ntest consisting of fast repetitions of &#8220;PA-TA-KA&#8221;, where\nsubjects tend to alter, replace, insert or skip syllables. This paper\npresents our efforts to build a speech-based application specific for\nthis task, where the computation is fast, efficient, and accurate on\na portable device, not in the cloud. The tool recognizes the target\nsyllables, providing phonetic alignment. This information is crucial\nto reliably estimate biomarkers such as the number of repetitions,\ninsertions, mispronunciations, and temporal prosodic structure of the\nrepetitions. We train and evaluate the application for two neurological\ndisorders:  traumatic brain injuries (TBIs) and Parkinson&#8217;s disease.\nThe results show low syllable error rates and high boundary detection,\nacross populations.\n"
   ],
   "doi": "10.21437/Interspeech.2016-789"
  },
  "ghahabi16_interspeech": {
   "authors": [
    [
     "Omid",
     "Ghahabi"
    ],
    [
     "Antonio",
     "Bonafonte"
    ],
    [
     "Javier",
     "Hernando"
    ],
    [
     "Asunción",
     "Moreno"
    ]
   ],
   "title": "Deep Neural Networks for i-Vector Language Identification of Short Utterances in Cars",
   "original": "1045",
   "page_count": 5,
   "order": 77,
   "p1": "367",
   "pn": "371",
   "abstract": [
    "This paper is focused on the application of the Language Identification\n(LID) technology for intelligent vehicles. We cope with short sentences\nor words spoken in moving cars in four languages: English, Spanish,\nGerman, and Finnish. As the response time of the LID system is crucial\nfor user acceptance in this particular task, speech signals of different\ndurations with total average of 3.8s are analyzed. In this paper, the\nauthors propose the use of Deep Neural Networks (DNN) to model effectively\nthe i-vector space of languages. Both raw i-vectors and session variability\ncompensated i-vectors are evaluated as input vectors to DNNs. The performance\nof the proposed DNN architecture is compared with both conventional\nGMM-UBM and i-vector/LDA systems considering the effect of durations\nof signals. It is shown that the signals with durations between 2 and\n3s meet the requirements of this application, i.e., high accuracy and\nfast decision, in which the proposed DNN architecture outperforms GMM-UBM\nand i-vector/LDA systems by 37% and 28%, respectively.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1045"
  },
  "woubie16_interspeech": {
   "authors": [
    [
     "Abraham",
     "Woubie"
    ],
    [
     "Jordi",
     "Luque"
    ],
    [
     "Javier",
     "Hernando"
    ]
   ],
   "title": "Improving i-Vector and PLDA Based Speaker Clustering with Long-Term Features",
   "original": "0339",
   "page_count": 5,
   "order": 78,
   "p1": "372",
   "pn": "376",
   "abstract": [
    "i-vector modeling techniques have been successfully used for speaker\nclustering task recently. In this work, we propose the extraction of\ni-vectors from short- and long-term speech features, and the fusion\nof their PLDA scores within the frame of speaker diarization. Two sets\nof i-vectors are first extracted from short-term spectral and long-term\nvoice-quality, prosodic and glottal to noise excitation ratio (GNE)\nfeatures. Then, the PLDA scores of these two i-vectors are fused for\nspeaker clustering task. Experiments have been carried out on single\nand multiple site scenario test sets of Augmented Multi-party Interaction\n(AMI) corpus. Experimental results show that i-vector based PLDA speaker\nclustering technique provides a significant diarization error rate\n(DER) improvement than GMM based BIC clustering technique.\n"
   ],
   "doi": "10.21437/Interspeech.2016-339"
  },
  "lawson16_interspeech": {
   "authors": [
    [
     "Aaron",
     "Lawson"
    ],
    [
     "Mitchell",
     "McLaren"
    ],
    [
     "Harry",
     "Bratt"
    ],
    [
     "Martin",
     "Graciarena"
    ],
    [
     "Horacio",
     "Franco"
    ],
    [
     "Christopher",
     "George"
    ],
    [
     "Allen",
     "Stauffer"
    ],
    [
     "Chris",
     "Bartels"
    ],
    [
     "Julien",
     "VanHout"
    ]
   ],
   "title": "Open Language Interface for Voice Exploitation (OLIVE)",
   "original": "2000",
   "page_count": 2,
   "order": 79,
   "p1": "377",
   "pn": "378",
   "abstract": [
    "We propose to demonstrate the Open Language Interface for Voice Exploitation\n(OLIVE) speech-processing system, which SRI International developed\nunder the DARPA Robust Automatic Transcription of Speech (RATS) program.\nThe technology underlying OLIVE was designed to achieve robustness\nto high levels of noise and distortion for speech activity detection\n(SAD), speaker identification (SID), language and dialect identification\n(LID), and keyword spotting (KWS). Our demonstration will show OLIVE\nperforming those four tasks. We will also demonstrate SRI&#8217;s speaker\nrecognition capability live on a mobile phone for visitors to interact\nwith.\n"
   ]
  },
  "smidl16_interspeech": {
   "authors": [
    [
     "Luboš",
     "Šmídl"
    ],
    [
     "Adam",
     "Chýlek"
    ],
    [
     "Jan",
     "Švec"
    ]
   ],
   "title": "A Multimodal Dialogue System for Air Traffic Control Trainees Based on Discrete-Event Simulation",
   "original": "2002",
   "page_count": 2,
   "order": 80,
   "p1": "379",
   "pn": "380",
   "abstract": [
    "In this paper we present a multimodal dialogue system designed as a\nlearning tool for air traffic control officer trainees (ATCO). It was\ndeveloped using our discrete-event simulation dialogue management framework\nwith cloud-based speech recognition and text-to-speech systems. Our\nsystem mimics pilots in an air traffic communication, allowing the\nATCOs to practice a control of a virtual airspace using spoken commands\nfrom air traffic control English phraseology.\n"
   ]
  },
  "gauthier16_interspeech": {
   "authors": [
    [
     "Elodie",
     "Gauthier"
    ],
    [
     "David",
     "Blachon"
    ],
    [
     "Laurent",
     "Besacier"
    ],
    [
     "Guy-Noël",
     "Kouarata"
    ],
    [
     "Martine",
     "Adda-Decker"
    ],
    [
     "Annie",
     "Rialland"
    ],
    [
     "Gilles",
     "Adda"
    ],
    [
     "Grégoire",
     "Bachman"
    ]
   ],
   "title": " Lig-Aikuma: A Mobile App to Collect Parallel Speech for Under-Resourced Language Studies",
   "original": "2003",
   "page_count": 2,
   "order": 81,
   "p1": "381",
   "pn": "382",
   "abstract": [
    "This paper reports on our ongoing efforts to collect speech data in\nunder-resourced or endangered languages of Africa. Data collection\nis carried out using an improved version of the Android application\n( Aikuma) developed by Steven Bird and colleagues [1]. Features were\nadded to the app in order to facilitate the collection of parallel\nspeech data in line with the requirements of the French-German ANR/DFG\nBULB (Breaking the Unwritten Language Barrier) project. The resulting\napp, called  Lig-Aikuma, runs on various mobile phones and tablets\nand proposes a range of different speech collection modes (recording,\nrespeaking, translation and elicitation). It was used for field data\ncollections in Congo-Brazzaville resulting in a total of over 80 hours\nof speech.\n"
   ]
  },
  "gruber16_interspeech": {
   "authors": [
    [
     "Martin",
     "Grůber"
    ],
    [
     "Jindřich",
     "Matoušek"
    ],
    [
     "Zdeněk",
     "Hanzlíček"
    ],
    [
     "Zdeněk",
     "Krňoul"
    ],
    [
     "Zbyněk",
     "Zajíc"
    ]
   ],
   "title": "ARET &#8212; Automatic Reading of Educational Texts for Visually Impaired Students",
   "original": "2004",
   "page_count": 2,
   "order": 82,
   "p1": "383",
   "pn": "384",
   "abstract": [
    "This paper deals with a presentation of an application which was developed\nto help in education of visually impaired pupils at a secondary school,\ni.e. at the pupils&#8217; age of 12 to 14 years. The web-based application\nintegrates speech and language technologies to make the education easier\nin several areas, e.g. in mathematics, physics, chemistry or languages\n(Czech, English, German). TTS system is used for automatic reading\nof educational texts and it makes use of a special preprocessing of\nthe texts, namely any formulas which may occur therein. The application\nis used by both teachers to create and manage the teaching material\nand pupils to view and listen to the prepared material. The application\nis currently being used by one special school for visually impaired\npupils in daily lessons.\n"
   ]
  },
  "lu16b_interspeech": {
   "authors": [
    [
     "Liang",
     "Lu"
    ],
    [
     "Lingpeng",
     "Kong"
    ],
    [
     "Chris",
     "Dyer"
    ],
    [
     "Noah A.",
     "Smith"
    ],
    [
     "Steve",
     "Renals"
    ]
   ],
   "title": "Segmental Recurrent Neural Networks for End-to-End Speech Recognition",
   "original": "0040",
   "page_count": 5,
   "order": 83,
   "p1": "385",
   "pn": "389",
   "abstract": [
    "We study the segmental recurrent neural network for end-to-end acoustic\nmodelling. This model connects the segmental conditional random field\n(CRF) with a recurrent neural network (RNN) used for feature extraction.\nCompared to most previous CRF-based acoustic models, it does not rely\non an external system to provide features or segmentation boundaries.\nInstead, this model marginalises out all the possible segmentations,\nand features are extracted from the RNN trained together with the segmental\nCRF. Essentially, this model is self-contained and can be trained end-to-end.\nIn this paper, we discuss practical training and decoding issues as\nwell as the method to speed up the training in the context of speech\nrecognition. We performed experiments on the TIMIT dataset. We achieved\n17.3% phone error rate (PER) from the first-pass decoding &#8212; the\nbest reported result using CRFs, despite the fact that we only used\na zeroth-order CRF and without using any language model.\n"
   ],
   "doi": "10.21437/Interspeech.2016-40"
  },
  "nussbaumthom16_interspeech": {
   "authors": [
    [
     "Markus",
     "Nussbaum-Thom"
    ],
    [
     "Jia",
     "Cui"
    ],
    [
     "Bhuvana",
     "Ramabhadran"
    ],
    [
     "Vaibhava",
     "Goel"
    ]
   ],
   "title": "Acoustic Modeling Using Bidirectional Gated Recurrent Convolutional Units",
   "original": "0212",
   "page_count": 5,
   "order": 84,
   "p1": "390",
   "pn": "394",
   "abstract": [
    "Convolutional and bidirectional recurrent neural networks have achieved\nconsiderable performance gains as acoustic models in automatic speech\nrecognition in recent years. Latest architectures unify long short-term\nmemory, gated recurrent unit and convolutional neural networks by stacking\nthese different neural network types on each other, and providing short\nand long-term features to different depth levels of the network.<br\nstyle=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  For the first time, we propose a unified layer for acoustic modeling\nwhich is simultaneously recurrent and convolutional, and which operates\nonly on short-term features. Our unified model introduces a bidirectional\ngated recurrent unit that uses convolutional operations for the gating\nunits. We analyze the performance behavior of the proposed layer, compare\nand combine it with bidirectional gated recurrent units, deep neural\nnetworks and frequency-domain convolutional neural networks on a 50\nhour English broadcast news task. The analysis indicates that the proposed\nlayer in combination with stacked bidirectional gated recurrent units\noutperforms other architectures.\n"
   ],
   "doi": "10.21437/Interspeech.2016-212"
  },
  "hsu16_interspeech": {
   "authors": [
    [
     "Wei-Ning",
     "Hsu"
    ],
    [
     "Yu",
     "Zhang"
    ],
    [
     "Ann",
     "Lee"
    ],
    [
     "James",
     "Glass"
    ]
   ],
   "title": "Exploiting Depth and Highway Connections in Convolutional Recurrent Deep Neural Networks for Speech Recognition",
   "original": "0515",
   "page_count": 5,
   "order": 85,
   "p1": "395",
   "pn": "399",
   "abstract": [
    "Deep neural network models have achieved considerable success in a\nwide range of fields. Several architectures have been proposed to alleviate\nthe vanishing gradient problem, and hence enable training of very deep\nnetworks. In the speech recognition area, convolutional neural networks,\nrecurrent neural networks, and fully connected deep neural networks\nhave been shown to be complimentary in their modeling capabilities.\nCombining all three components, called CLDNN, yields the best performance\nto date. In this paper, we extend the CLDNN model by introducing a\nhighway connection between LSTM layers, which enables direct information\nflow from cells of lower layers to cells of upper layers. With this\ndesign, we are able to better exploit the advantages of a deeper structure.\nExperiments on the GALE Chinese Broadcast Conversation/News Speech\ndataset indicate that our model outperforms all previous models and\nachieves a new benchmark, which is 22.41% character error rate on the\ndataset.\n"
   ],
   "doi": "10.21437/Interspeech.2016-515"
  },
  "wu16_interspeech": {
   "authors": [
    [
     "Chunyang",
     "Wu"
    ],
    [
     "Penny",
     "Karanasou"
    ],
    [
     "Mark J.F.",
     "Gales"
    ],
    [
     "Khe Chai",
     "Sim"
    ]
   ],
   "title": "Stimulated Deep Neural Network for Speech Recognition",
   "original": "0580",
   "page_count": 5,
   "order": 86,
   "p1": "400",
   "pn": "404",
   "abstract": [
    "Deep neural networks (DNNs) and deep learning approaches yield state-of-the-art\nperformance in a range of tasks, including speech recognition. However,\nthe parameters of the network are hard to analyze, making network regularization\nand robust adaptation challenging. Stimulated training has recently\nbeen proposed to address this problem by encouraging the node activation\noutputs in regions of the network to be related. This kind of information\naids visualization of the network, but also has the potential to improve\nregularization and adaptation. This paper investigates stimulated training\nof DNNs for both of these options. These schemes take advantage of\nthe smoothness constraints that stimulated training offers. The approaches\nare evaluated on two large vocabulary speech recognition tasks: a U.S.\nEnglish broadcast news (BN) task and a Javanese conversational telephone\nspeech task from the IARPA Babel program. Stimulated DNN training acquires\nconsistent performance gains on both tasks over unstimulated baselines.\nOn the BN task, the proposed smoothing approach is also applied to\nrapid adaptation, again outperforming the standard adaptation scheme.\n"
   ],
   "doi": "10.21437/Interspeech.2016-580"
  },
  "badino16_interspeech": {
   "authors": [
    [
     "Leonardo",
     "Badino"
    ]
   ],
   "title": "Phonetic Context Embeddings for DNN-HMM Phone Recognition",
   "original": "1036",
   "page_count": 5,
   "order": 87,
   "p1": "405",
   "pn": "409",
   "abstract": [
    "This paper proposes an approach, named phonetic context embedding,\nto model phonetic context effects for deep neural network - hidden\nMarkov model (DNN-HMM) phone recognition. Phonetic context embeddings\ncan be regarded as continuous and distributed vector representations\nof context-dependent phonetic units (e.g., triphones). In this work\nthey are computed using neural networks. First, all phone labels are\nmapped into vectors of binary distinctive features (DFs, e.g., nasal/not-nasal).\nThen for each speech frame the corresponding DF vector is concatenated\nwith DF vectors of previous and next frames and fed into a neural network\nthat is trained to estimate the acoustic coefficients (e.g., MFCCs)\nof that frame. The values of the first hidden layer represent the embedding\nof the input DF vectors. Finally, the resulting embeddings are used\nas secondary task targets in a multi-task learning (MTL) setting when\ntraining the DNN that computes phone state posteriors. The approach\nallows to easily encode a much larger context than alternative MTL-based\napproaches. Results on TIMIT with a fully connected DNN shows phone\nerror rate (PER) reductions from 22.4% to 21.0% and from 21.3% to 19.8%\non the test core and the validation set respectively and lower PER\nthan an alternative strong MTL approach.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1036"
  },
  "zhang16b_interspeech": {
   "authors": [
    [
     "Ying",
     "Zhang"
    ],
    [
     "Mohammad",
     "Pezeshki"
    ],
    [
     "Philémon",
     "Brakel"
    ],
    [
     "Saizheng",
     "Zhang"
    ],
    [
     "César",
     "Laurent"
    ],
    [
     "Yoshua",
     "Bengio"
    ],
    [
     "Aaron",
     "Courville"
    ]
   ],
   "title": "Towards End-to-End Speech Recognition with Deep Convolutional Neural Networks",
   "original": "1446",
   "page_count": 5,
   "order": 88,
   "p1": "410",
   "pn": "414",
   "abstract": [
    "Convolutional Neural Networks (CNNs) are effective models for reducing\nspectral variations and modeling spectral correlations in acoustic\nfeatures for automatic speech recognition (ASR). Hybrid speech recognition\nsystems incorporating CNNs with Hidden Markov Models/Gaussian Mixture\nModels (HMMs/GMMs) have achieved the state-of-the-art in various benchmarks.\nMeanwhile, Connectionist Temporal Classification (CTC) with Recurrent\nNeural Networks (RNNs), which is proposed for labeling unsegmented\nsequences, makes it feasible to train an &#8216;end-to-end&#8217; speech\nrecognition system instead of hybrid settings. However, RNNs are computationally\nexpensive and sometimes difficult to train. In this paper, inspired\nby the advantages of both CNNs and the CTC approach, we propose an\nend-to-end speech framework for sequence labeling, by combining hierarchical\nCNNs with CTC directly without recurrent connections. By evaluating\nthe approach on the TIMIT phoneme recognition task, we show that the\nproposed model is not only computationally efficient, but also competitive\nwith the existing baseline systems. Moreover, we argue that CNNs have\nthe capability to model temporal correlations with appropriate context\ninformation.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1446"
  },
  "wang16b_interspeech": {
   "authors": [
    [
     "Guangsen",
     "Wang"
    ],
    [
     "Kong Aik",
     "Lee"
    ],
    [
     "Trung Hieu",
     "Nguyen"
    ],
    [
     "Hanwu",
     "Sun"
    ],
    [
     "Bin",
     "Ma"
    ]
   ],
   "title": "Joint Speaker and Lexical Modeling for Short-Term Characterization of Speaker",
   "original": "0929",
   "page_count": 5,
   "order": 89,
   "p1": "415",
   "pn": "419",
   "abstract": [
    "For speech utterances of very short duration, speaker characterization\nhas shown strong dependency on the lexical content. In this context,\nspeaker verification is always performed by analyzing and matching\nspeaker pronunciation of individual words, syllables, or phones. In\nthis paper, we advocate the use of hidden Markov model (HMM) for joint\nmodeling of speaker characteristic and lexical content. We then develop\na scoring model that scores only the speaker part rather than the joint\nspeaker-lexical component leading to a better speaker verification\nperformance. Experiments were conducted on the text-prompted task of\nRSR2015 and the RedDots datasets. In the RSR2015, the prompted texts\nare limited to random sequences of digits. The RedDots dataset dictates\nan unconstrained scenario where the prompted texts are free-text sentences.\nBoth RSR2015 and RedDots datasets are publicly available.\n"
   ],
   "doi": "10.21437/Interspeech.2016-929"
  },
  "alam16_interspeech": {
   "authors": [
    [
     "Md Jahangir",
     "Alam"
    ],
    [
     "Patrick",
     "Kenny"
    ],
    [
     "Vishwa",
     "Gupta"
    ]
   ],
   "title": "Tandem Features for Text-Dependent Speaker Verification on the RedDots Corpus",
   "original": "1465",
   "page_count": 5,
   "order": 90,
   "p1": "420",
   "pn": "424",
   "abstract": [
    "We use tandem features and a fusion of four systems for text-dependent\nspeaker verification on the RedDots corpus. In the tandem system, a\nsenone-discriminant neural network provides a low-dimensional bottleneck\nfeature at each frame which are concatenated with a standard Mel-frequency\ncepstral coefficients (MFCC) feature representation. The concatenated\nfeatures are propagated to a conventional GMM/UBM speaker recognition\nframework. In order to capture complementary information to the MFCC,\nwe also use linear frequency cepstral coefficients and wavelet-based\ncepstral coefficients features for score level fusion. We report results\non the part 1 and part 4 (text-dependent) tasks of RedDots corpus.\nBoth the tandem feature-based system and fused system provided significant\nimprovements over the baseline GMM/UBM system in terms of equal error\nrates (EER) and detection cost functions (DCFs) as defined in the 2008\nand 2010 NIST speaker recognition evaluations. On the part 1 task (impostor\ncorrect condition) the fused system reduced the EER from 2.63% to 2.28%\nfor male trials and from 7.01% to 3.48% for female trials. On the part4\ntask (impostor correct condition) the fused system helped to reduce\nthe EER from 2.49% to 1.96% and from 5.9% to 3.22% for male and female\ntrials respectively.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1465"
  },
  "sarkar16_interspeech": {
   "authors": [
    [
     "Achintya Kr.",
     "Sarkar"
    ],
    [
     "Zheng-Hua",
     "Tan"
    ]
   ],
   "title": "Text Dependent Speaker Verification Using Un-Supervised HMM-UBM and Temporal GMM-UBM",
   "original": "0362",
   "page_count": 5,
   "order": 91,
   "p1": "425",
   "pn": "429",
   "abstract": [
    "In this paper, we investigate the Hidden Markov Model (HMM) and the\ntemporal Gaussian Mixture Model (GMM) systems based on the Universal\nBackground Model (UBM) concept to capture temporal information of speech\nfor Text Dependent (TD) Speaker Verification (SV). In TD-SV, target\nspeakers are constrained to use only predefined fixed sentence/s during\nboth the enrollment and the test process. The temporal information\nis therefore important in the sense of utterance verification, i.e.\nwhether the test utterance has the same sequence of textual content\nas the utterance used during the target enrollment. However, the temporal\ninformation is not considered in the classical GMM-UBM based TD-SV\nsystem. Moreover, no transcription knowledge of the speech is required\nin the HMM-UBM and temporal GMM-UBM based systems. We also study the\nfusion of the HMM-UBM, the temporal GMM-UBM and the classical GMM-UBM\nsystems in SV. We show that the HMM-UBM system yields better performance\nthan the other systems in most cases. Further, fusion of the systems\nimprove the overall speaker verification performance. The results are\nshown in the different tasks of the RedDots challenge 2016 database.\n"
   ],
   "doi": "10.21437/Interspeech.2016-362"
  },
  "kinnunen16_interspeech": {
   "authors": [
    [
     "Tomi",
     "Kinnunen"
    ],
    [
     "Md.",
     "Sahidullah"
    ],
    [
     "Ivan",
     "Kukanov"
    ],
    [
     "Héctor",
     "Delgado"
    ],
    [
     "Massimiliano",
     "Todisco"
    ],
    [
     "Achintya Kr.",
     "Sarkar"
    ],
    [
     "Nicolai Bæk",
     "Thomsen"
    ],
    [
     "Ville",
     "Hautamäki"
    ],
    [
     "Nicholas",
     "Evans"
    ],
    [
     "Zheng-Hua",
     "Tan"
    ]
   ],
   "title": "Utterance Verification for Text-Dependent Speaker Recognition: A Comparative Assessment Using the RedDots Corpus",
   "original": "1125",
   "page_count": 5,
   "order": 92,
   "p1": "430",
   "pn": "434",
   "abstract": [
    "Text-dependent automatic speaker verification naturally calls for the\nsimultaneous verification of speaker identity and spoken content. These\ntwo tasks can be achieved with automatic speaker verification (ASV)\nand utterance verification (UV) technologies. While both have been\naddressed previously in the literature, a treatment of simultaneous\nspeaker and utterance verification with a modern, standard database\nis so far lacking. This is despite the burgeoning demand for voice\nbiometrics in a plethora of practical security applications. With the\ngoal of improving overall verification performance, this paper reports\ndifferent strategies for simultaneous ASV and UV in the context of\nshort-duration, text-dependent speaker verification. Experiments performed\non the recently released RedDots corpus are reported for three different\nASV systems and four different UV systems. Results show that the combination\nof utterance verification with automatic speaker verification is (almost)\nuniversally beneficial with significant performance improvements being\nobserved.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1125"
  },
  "ma16_interspeech": {
   "authors": [
    [
     "Jianbo",
     "Ma"
    ],
    [
     "Saad",
     "Irtza"
    ],
    [
     "Kaavya",
     "Sriskandaraja"
    ],
    [
     "Vidhyasaharan",
     "Sethu"
    ],
    [
     "Eliathamby",
     "Ambikairajah"
    ]
   ],
   "title": "Parallel Speaker and Content Modelling for Text-Dependent Speaker Verification",
   "original": "0825",
   "page_count": 5,
   "order": 93,
   "p1": "435",
   "pn": "439",
   "abstract": [
    "Text-dependent short duration speaker verification involves two challenges.\nThe primary challenge of interest is the verification of the speaker&#8217;s\nidentity, and often a secondary challenge of interest is the verification\nof the lexical content of the pass-phrase. In this paper, we propose\nthe use of two systems to handle these two tasks in parallel with one\nsub-system modelling speaker identity based on the assumption that\nlexical content is known and the other sub-system modelling lexical\ncontent in a speaker dependent manner. The text-dependent speaker verification\nsub-system is based on hidden Markov models and the lexical content\nverification system is based on models of speech segments that use\na distinct Gaussian mixture model for each segment. Furthermore, a\nmixture selection method based on KL divergence was applied to refine\nthe lexical content sub-system by making the models more discriminative.\nExperiments on part 1 of the RedDots database showed that the proposed\ncombination of two sub-systems outperformed the baseline system by\n39.8%, 51.1% and 37.3% in terms of the &#8216;imposter_correct&#8217;,\n&#8216;target_wrong&#8217; and &#8216;imposter_wrong&#8217; metrics\nrespectively.\n"
   ],
   "doi": "10.21437/Interspeech.2016-825"
  },
  "zeinali16_interspeech": {
   "authors": [
    [
     "Hossein",
     "Zeinali"
    ],
    [
     "Hossein",
     "Sameti"
    ],
    [
     "Lukáš",
     "Burget"
    ],
    [
     "Jan",
     "Černocký"
    ],
    [
     "Nooshin",
     "Maghsoodi"
    ],
    [
     "Pavel",
     "Matějka"
    ]
   ],
   "title": "i-Vector/HMM Based Text-Dependent Speaker Verification System for RedDots Challenge",
   "original": "1174",
   "page_count": 5,
   "order": 94,
   "p1": "440",
   "pn": "444",
   "abstract": [
    "Recently, a new data collection was initiated within the RedDots project\nin order to evaluate text-dependent and text-prompted speaker recognition\ntechnology on data from a wider speaker population and with more realistic\nnoise, channel and phonetic variability. This paper analyses our systems\nbuilt for RedDots challenge &#8212; the effort to collect and compare\nthe initial results on this new evaluation data set obtained at different\nsites. We use our recently introduced HMM based i-vector approach,\nwhere, instead of the traditional GMM, a set of phone specific HMMs\nis used to collect the sufficient statistics for i-vector extraction.\nOur systems are trained in a completely phrase-independent way on the\ndata from RSR2015 and Libri speech databases. We compare systems making\nuse of standard cepstral features and their combination with neural\nnetwork based bottle-neck features. The best results are obtained with\na score-level fusion of such systems.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1174"
  },
  "das16_interspeech": {
   "authors": [
    [
     "Rohan Kumar",
     "Das"
    ],
    [
     "Sarfaraz",
     "Jelil"
    ],
    [
     "S.R. Mahadeva",
     "Prasanna"
    ]
   ],
   "title": "Exploring Session Variability and Template Aging in Speaker Verification for Fixed Phrase Short Utterances",
   "original": "1001",
   "page_count": 5,
   "order": 95,
   "p1": "445",
   "pn": "449",
   "abstract": [
    "This work highlights the impact of session variability and template\naging on speaker verification (SV) using fixed phrase short utterances\nfrom the RedDots database. These have been collected over a period\nof one year and contain a large number of sessions per speaker. Session\nvariation has been found to have a direct influence on SV performance\nand its significance is even greater for the case of fixed phrase short\nutterances as a very small amount of speech data is involved for speaker\nmodeling as well as testing. Similarly for a practical deployable SV\nsystem when there is large session variation involved over a period\nof time, the template aging of the speakers may effect the SV performance.\nThis work attempts to address some issues related to session variability\nand template aging of speakers which are found for data having large\nsession variability, that if considered can be utilized for improving\nthe performance of an SV system.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1001"
  },
  "uchida16_interspeech": {
   "authors": [
    [
     "Hidetsugu",
     "Uchida"
    ],
    [
     "Daisuke",
     "Saito"
    ],
    [
     "Nobuaki",
     "Minematsu"
    ]
   ],
   "title": "Prediction of the Articulatory Movements of Unseen Phonemes of a Speaker Using the Speech Structure of Another Speaker",
   "original": "1138",
   "page_count": 5,
   "order": 96,
   "p1": "450",
   "pn": "454",
   "abstract": [
    "In this paper, we propose a method to predict the articulatory movements\nof phonemes that are difficult for a speaker to pronounce correctly\nbecause those phonemes are not seen in the native language of that\nspeaker. When one wants to predict the articulatory movements of those\nunseen phonemes, since he/she has difficulty to generate those sounds,\nthe conventional acoustic-to-articulatory mapping cannot be applied\nas it is. Here, we propose a solution by using the speech structure\nof another reference speaker who can pronounce the unseen phonemes.\nSpeech structure is a kind of speech feature that represents only the\nlinguistic information by suppressing the non-linguistic information,\ne.g. speaker identity, of an input utterance. In the proposed method,\nby using the speech structure of those unseen phonemes and other phonemes\nas constraint, the articulatory movements of the unseen phonemes are\nsearched for in the articulatory space of the original speaker. Experiments\nusing English short vowels show that the averaged prediction error\nwas 1.02 mm.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1138"
  },
  "sivaraman16_interspeech": {
   "authors": [
    [
     "Ganesh",
     "Sivaraman"
    ],
    [
     "Vikramjit",
     "Mitra"
    ],
    [
     "Hosung",
     "Nam"
    ],
    [
     "Mark",
     "Tiede"
    ],
    [
     "Carol",
     "Espy-Wilson"
    ]
   ],
   "title": "Vocal Tract Length Normalization for Speaker Independent Acoustic-to-Articulatory Speech Inversion",
   "original": "1399",
   "page_count": 5,
   "order": 97,
   "p1": "455",
   "pn": "459",
   "abstract": [
    "Speech inversion is a well-known ill-posed problem and addition of\nspeaker differences typically makes it even harder. This paper investigates\na vocal tract length normalization (VTLN) technique to transform the\nacoustic space of different speakers to a target speaker space such\nthat speaker specific details are minimized. The speaker normalized\nfeatures are then used to train a feed-forward neural network based\nacoustic-to-articulatory speech inversion system. The acoustic features\nare parameterized as time-contextualized mel-frequency cepstral coefficients\nand the articulatory features are represented by six tract-variable\n(TV) trajectories. Experiments are performed with ten speakers from\nthe U. Wisc. X-ray microbeam database. Speaker dependent speech inversion\nsystems are trained for each speaker as baselines to compare the performance\nof the speaker independent approach. For each target speaker, data\nfrom the remaining nine speakers are transformed using the proposed\napproach and the transformed features are used to train a speech inversion\nsystem. The performances of the individual systems are compared using\nthe correlation between the estimated and the actual TVs on the target\nspeaker&#8217;s test set. Results show that the proposed speaker normalization\napproach provides a 7% absolute improvement in correlation as compared\nto the system where speaker normalization was not performed.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1399"
  },
  "lammert16_interspeech": {
   "authors": [
    [
     "Adam C.",
     "Lammert"
    ],
    [
     "Christine H.",
     "Shadle"
    ],
    [
     "Shrikanth S.",
     "Narayanan"
    ],
    [
     "Thomas F.",
     "Quatieri"
    ]
   ],
   "title": "Investigation of Speed-Accuracy Tradeoffs in Speech Production Using Real-Time Magnetic Resonance Imaging",
   "original": "0157",
   "page_count": 5,
   "order": 98,
   "p1": "460",
   "pn": "464",
   "abstract": [
    "Motor actions in speech production are both rapid and highly dexterous,\neven though speed and accuracy are often thought to conflict. Fitts&#8217;\nlaw has served as a rigorous formulation of the fundamental speed-accuracy\ntradeoff in other domains of human motor action, but has not been directly\nexamined with respect to speech production. This paper examines Fitts&#8217;\nlaw in speech articulation kinematics by analyzing USC-TIMIT, a large\ndatabase of real-time magnetic resonance imaging data of speech production.\nThis paper also addresses methodological challenges in applying Fitts-style\nanalysis, including the definition and operational measurement of key\nvariables in real-time MRI data. Results suggest high variability in\nthe task demands associated with targeted articulatory kinematics,\nas well as a clear tradeoff between speed and accuracy for certain\ntypes of speech production actions. Consonant targets, and particularly\nthose following vowels, show the strongest evidence of this tradeoff,\nwith correlations as high as 0.71 between movement time and difficulty.\nOther speech actions seem to challenge Fitts&#8217; law. Results are\ndiscussed with respect to limitations of Fitts&#8217; law in the context\nof speech production, as well as future improvements and applications.\n"
   ],
   "doi": "10.21437/Interspeech.2016-157"
  },
  "sorensen16_interspeech": {
   "authors": [
    [
     "Tanner",
     "Sorensen"
    ],
    [
     "Asterios",
     "Toutios"
    ],
    [
     "Louis",
     "Goldstein"
    ],
    [
     "Shrikanth S.",
     "Narayanan"
    ]
   ],
   "title": "Characterizing Vocal Tract Dynamics Across Speakers Using Real-Time MRI",
   "original": "0583",
   "page_count": 5,
   "order": 99,
   "p1": "465",
   "pn": "469",
   "abstract": [
    "Real-time magnetic resonance imaging (rtMRI) provides information about\nthe dynamic shaping of the vocal tract during speech production and\nvaluable data for creating and testing models of speech production.\nIn this paper, we use rtMRI videos to develop a dynamical system in\nthe framework of Task Dynamics which controls vocal tract constrictions\nand induces deformation of the air-tissue boundary. This is the first\ntask dynamical system explicitly derived from speech kinematic data.\nSimulation identifies differences in articulatory strategy across speakers\n(n = 18), specifically in the relative contribution of articulators\nto vocal tract constrictions.\n"
   ],
   "doi": "10.21437/Interspeech.2016-583"
  },
  "labrunie16_interspeech": {
   "authors": [
    [
     "Mathieu",
     "Labrunie"
    ],
    [
     "Pierre",
     "Badin"
    ],
    [
     "Dirk",
     "Voit"
    ],
    [
     "Arun A.",
     "Joseph"
    ],
    [
     "Laurent",
     "Lamalle"
    ],
    [
     "Coriandre",
     "Vilain"
    ],
    [
     "Louis-Jean",
     "Boë"
    ],
    [
     "Jens",
     "Frahm"
    ]
   ],
   "title": "Tracking Contours of Orofacial Articulators from Real-Time MRI of Speech",
   "original": "0078",
   "page_count": 5,
   "order": 100,
   "p1": "470",
   "pn": "474",
   "abstract": [
    "We introduce a method for predicting midsagittal contours of orofacial\narticulators from real-time MRI data. A corpus of about 26 minutes\nof speech has been recorded of a French speaker at a rate of 55 images\n/ s using highly undersampled radial gradient-echo MRI with image reconstruction\nby nonlinear inversion. The contours of each articulator have been\nmanually traced for a set of about 60 images selected &#8212; by hierarchical\nclustering &#8212; to optimally represent the diversity of the speaker\narticulations. The data serve to build articulator-specific Principal\nComponent Analysis (PCA) models of contours and associated image intensities,\nas well as multilinear regression (MLR) models that predict contour\nparameters from image parameters. The contours obtained by MLR are\nthen refined, using the local information about pixel intensity profiles\nalong the contours&#8217; normals, by means of modified Active Shape\nModels (ASM) trained on the same data. The method reaches RMS of predicted\npoints to reference contour distances between 0.54 and 0.93 mm, depending\non articulators. The processing of the corpus demonstrated the efficiency\nof the procedure, despite the possibility of further improvements.\nThis work opens new perspectives for studying articulatory motion in\nspeech.\n"
   ],
   "doi": "10.21437/Interspeech.2016-78"
  },
  "lingala16_interspeech": {
   "authors": [
    [
     "Sajan Goud",
     "Lingala"
    ],
    [
     "Asterios",
     "Toutios"
    ],
    [
     "Johannes",
     "Töger"
    ],
    [
     "Yongwan",
     "Lim"
    ],
    [
     "Yinghua",
     "Zhu"
    ],
    [
     "Yoon-Chul",
     "Kim"
    ],
    [
     "Colin",
     "Vaz"
    ],
    [
     "Shrikanth S.",
     "Narayanan"
    ],
    [
     "Krishna S.",
     "Nayak"
    ]
   ],
   "title": "State-of-the-Art MRI Protocol for Comprehensive Assessment of Vocal Tract Structure and Function",
   "original": "0559",
   "page_count": 5,
   "order": 101,
   "p1": "475",
   "pn": "479",
   "abstract": [
    "Magnetic Resonance Imaging (MRI) provides a safe and flexible means\nto study the vocal tract, and is increasingly used in speech production\nresearch. This work details a state-of-the-art MRI protocol for comprehensive\nassessment of vocal tract structure and function, and presents results\nfrom representative speakers. The system incorporates (a) custom upper\nairway coils that are maximally sensitive to vocal tract tissues, (b)\ngraphical user interface for 2D real-time MRI that provides on-the-fly\nreconstruction for interactive localization, and correction of imaging\nartifacts, (c) off-line constrained reconstruction for generating high\nspatio-temporal resolution dynamic images at (83 frames per sec, 2.4\nmm<SUP>2</SUP>), (d) 3D static imaging of sounds sustained for 7 sec\nwith full vocal tract coverage and isotropic resolution (resolution:\n1.25 mm<SUP>3</SUP>), (e) T2-weighted high-resolution, high-contrast\ndepiction of soft-tissue boundaries of the full vocal tract (axial,\ncoronal, sagittal sweeps with resolution: 0.58 &#215; 0.58 &#215; 3\nmm<SUP>3</SUP>), and (f) simultaneous audio recording with off-line\nnoise cancellation and temporal alignment of audio with 2D real-time\nMRI. A stimuli set was designed to capture efficiently salient, static\nand dynamic, articulatory and morphological aspects of speech production\nin 90-minute data acquisition sessions.\n"
   ],
   "doi": "10.21437/Interspeech.2016-559"
  },
  "xia16_interspeech": {
   "authors": [
    [
     "Rui",
     "Xia"
    ],
    [
     "Yang",
     "Liu"
    ]
   ],
   "title": "DBN-ivector Framework for Acoustic Emotion Recognition",
   "original": "0488",
   "page_count": 5,
   "order": 102,
   "p1": "480",
   "pn": "484",
   "abstract": [
    "Deep learning and i-vectors have been successfully used in speech and\nspeaker recognition recently. In this work we propose a framework based\non deep belief network (DBN) and i-vector space modeling for acoustic\nemotion recognition. We use two types of labels for frame level DBN\ntraining. The first one is the vector of posterior probabilities calculated\nfrom the GMM universal background model (UBM). The second one is the\npredicted label based on the GMMs. The DBN is trained to minimize errors\nfor both types. After DBN training, we use the vector of posterior\nprobabilities estimated by DBN to replace the UBM for i-vector extraction.\nFinally the extracted i-vectors are used in backend classifiers for\nemotion recognition. Our experiments on the USC IEMOCAP data show the\neffectiveness of our proposed DBN-ivector framework. In particular,\nwith decision level combination, our proposed system yields significant\nimprovement on both unweighted and weighted accuracy.\n"
   ],
   "doi": "10.21437/Interspeech.2016-488"
  },
  "stasak16_interspeech": {
   "authors": [
    [
     "Brian",
     "Stasak"
    ],
    [
     "Julien",
     "Epps"
    ],
    [
     "Nicholas",
     "Cummins"
    ],
    [
     "Roland",
     "Goecke"
    ]
   ],
   "title": "An Investigation of Emotional Speech in Depression Classification",
   "original": "0867",
   "page_count": 5,
   "order": 103,
   "p1": "485",
   "pn": "489",
   "abstract": [
    "Assessing depression via speech characteristics is a growing area of\ninterest in quantitative mental health research with a view to a clinical\nmental health assessment tool. As a mood disorder, depression induces\nchanges in response to emotional stimuli, which motivates this investigation\ninto the relationship between emotion and depression affected speech.\nThis paper investigates how emotional information expressed in speech\n(i.e. arousal, valence, dominance) contributes to the classification\nof minimally depressed and moderately-severely depressed individuals.\nExperiments based on a subset of the AVEC 2014 database show that manual\nemotion ratings alone are discriminative of depression and combining\nrating-based emotion features with acoustic features improves classification\nbetween mild and severe depression. Emotion-based data selection is\nalso shown to provide improvements in depression classification and\na range of threshold methods are explored. Finally, the experiments\npresented demonstrate that automatically predicted emotion ratings\ncan be incorporated into a fully automatic depression classification\nto produce a 5% accuracy improvement over an acoustic-only baseline\nsystem.\n"
   ],
   "doi": "10.21437/Interspeech.2016-867"
  },
  "lotfian16_interspeech": {
   "authors": [
    [
     "Reza",
     "Lotfian"
    ],
    [
     "Carlos",
     "Busso"
    ]
   ],
   "title": "Retrieving Categorical Emotions Using a Probabilistic Framework to Define Preference Learning Samples",
   "original": "1052",
   "page_count": 5,
   "order": 104,
   "p1": "490",
   "pn": "494",
   "abstract": [
    "Preference learning is an appealing approach for affective recognition.\nInstead of predicting the underlying emotional class of a sample, this\nframework relies on pairwise comparisons to rank-order the testing\ndata according to an emotional dimension. This framework is relevant\nnot only for continuous attributes such as arousal or valence, but\nalso for categorical classes (e.g., is this sample happier than the\nother?). A preference learning system for categorical classes can have\napplications in several domains including retrieving emotional behaviors\nconveying a target emotion, and defining the emotional intensity associated\nwith a given class. One important challenge to build such a system\nis to define relative labels defining the preference between training\nsamples. Instead of building these labels from scratch, we propose\na probabilistic framework that creates relative labels from existing\ncategorical annotations. The approach considers individual assessments\ninstead of consensus labels, creating a metrics that is sensitive to\nthe underlying ambiguity of emotional classes. The proposed metric\nquantifies the likelihood that a sample belong to a target emotion.\nWe build  happy, angry and  sad rank-classifiers using this metric.\nWe evaluate the approach over cross-corpus experiments, showing improved\nperformance over binary classifiers and rank-based classifiers trained\nwith consensus labels.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1052"
  },
  "schmitt16_interspeech": {
   "authors": [
    [
     "Maximilian",
     "Schmitt"
    ],
    [
     "Fabien",
     "Ringeval"
    ],
    [
     "Björn",
     "Schuller"
    ]
   ],
   "title": "At the Border of Acoustics and Linguistics: Bag-of-Audio-Words for the Recognition of Emotions in Speech",
   "original": "1124",
   "page_count": 5,
   "order": 105,
   "p1": "495",
   "pn": "499",
   "abstract": [
    "Recognition of natural emotion in speech is a challenging task. Different\nmethods have been proposed to tackle this complex task, such as acoustic\nfeature brute-forcing or even end-to-end learning. Recently, bag-of-audio-words\n(BoAW) representations of acoustic low-level descriptors (LLDs) have\nbeen employed successfully in the domain of acoustic event classification\nand other audio recognition tasks. In this approach, feature vectors\nof acoustic LLDs are quantised according to a learnt codebook of audio\nwords. Then, a histogram of the occurring &#8216;words&#8217; is built.\nDespite their massive potential, BoAW have not been thoroughly studied\nin emotion recognition. Here, we propose a method using BoAW created\nonly of mel-frequency cepstral coefficients (MFCCs). Support vector\nregression is then used to predict emotion continuously in time and\nvalue, such as in the dimensions arousal and valence. We compare this\napproach with the computation of functionals based on the MFCCs and\nperform extensive evaluations on the RECOLA database, which features\nspontaneous and natural emotions. Results show that, BoAW representation\nof MFCCs does not only perform significantly better than functionals,\nbut also outperforms by far most of recently published deep learning\napproaches, including convolutional and recurrent networks.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1124"
  },
  "chorianopoulou16_interspeech": {
   "authors": [
    [
     "Arodami",
     "Chorianopoulou"
    ],
    [
     "Polychronis",
     "Koutsakis"
    ],
    [
     "Alexandros",
     "Potamianos"
    ]
   ],
   "title": "Speech Emotion Recognition Using Affective Saliency",
   "original": "1311",
   "page_count": 5,
   "order": 106,
   "p1": "500",
   "pn": "504",
   "abstract": [
    "We investigate an affective saliency approach for speech emotion recognition\nof spoken dialogue utterances that estimates the amount of emotional\ninformation over time. The proposed saliency approach uses a regression\nmodel that combines features extracted from the acoustic signal and\nthe posteriors of a segment-level classifier to obtain frame or segment-level\nratings. The affective saliency model is trained using a minimum classification\nerror (MCE) criterion that learns the weights by optimizing an objective\nloss function related to the classification error rate of the emotion\nrecognition system. Affective saliency scores are then used to weight\nthe contribution of frame-level posteriors and/or features to the speech\nemotion classification decision. The algorithm is evaluated for the\ntask of anger detection on four call-center datasets for two languages,\nGreek and English, with good results.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1311"
  },
  "gupta16_interspeech": {
   "authors": [
    [
     "Rahul",
     "Gupta"
    ],
    [
     "Nishant",
     "Nath"
    ],
    [
     "Taruna",
     "Agrawal"
    ],
    [
     "Panayiotis",
     "Georgiou"
    ],
    [
     "David C.",
     "Atkins"
    ],
    [
     "Shrikanth S.",
     "Narayanan"
    ]
   ],
   "title": "Laughter Valence Prediction in Motivational Interviewing Based on Lexical and Acoustic Cues",
   "original": "0184",
   "page_count": 5,
   "order": 107,
   "p1": "505",
   "pn": "509",
   "abstract": [
    "Motivational Interviewing (MI) is a goal oriented psychotherapy counseling\nthat aims to instill positive change in a client through discussion.\nSince the discourse is in the form of semi-structured natural conversation,\nit often involves a variety of non-verbal social and affective behaviors\nsuch as laughter. Laughter carries information related to affect, mood\nand personality and can offer a window into the mental state of a person.\nIn this work, we conduct an analytical study on predicting the valence\nof laughters (positive, neutral or negative) based on lexical and acoustic\ncues, within the context of MI. We hypothesize that the valence of\nlaughter can be predicted using a window of past and future context\naround the laughter and, design models to incorporate context, from\nboth text and audio. Through these experiments we validate the relation\nof the two modalities to perceived laughter valence. Based on the outputs\nof the prediction experiment, we perform a follow up analysis of the\nresults including: (i) identification of the optimal past and future\ncontext in the audio and lexical channels, (ii) investigation of the\ndifferences in the prediction patterns for the counselor and the client\nand, (iii) analysis of feature patterns across the two modalities.\n"
   ],
   "doi": "10.21437/Interspeech.2016-184"
  },
  "wodarczak16_interspeech": {
   "authors": [
    [
     "Marcin",
     "Włodarczak"
    ],
    [
     "Mattias",
     "Heldner"
    ]
   ],
   "title": "Respiratory Belts and Whistles: A Preliminary Study of Breathing Acoustics for Turn-Taking",
   "original": "0344",
   "page_count": 5,
   "order": 108,
   "p1": "510",
   "pn": "514",
   "abstract": [
    "This paper presents first results on using acoustic intensity of inhalations\nas a cue to speech initiation in spontaneous multiparty conversations.\nWe demonstrate that inhalation intensity significantly differentiates\nbetween cycles coinciding with no speech activity, shorter (&#60; 1\ns) and longer stretches of speech. While the model fit is relatively\nweak, it is comparable to the fit of a model using kinematic features\ncollected with Respiratory Inductance Plethysmography. We also show\nthat incorporating both kinematic and acoustic features further improves\nthe model. Given the ease of capturing breath acoustics, we consider\nthe results to be a promising first step towards studying communicative\nfunctions of respiratory sounds. We discuss possible extensions to\nthe data collection procedure with a view to improving predictive power\nof the model.\n"
   ],
   "doi": "10.21437/Interspeech.2016-344"
  },
  "kaland16_interspeech": {
   "authors": [
    [
     "Constantijn",
     "Kaland"
    ],
    [
     "Vincenzo",
     "Galatà"
    ],
    [
     "Lorenzo",
     "Spreafico"
    ],
    [
     "Alessandro",
     "Vietti"
    ]
   ],
   "title": "/r/ as Language Marker in Bilingual Speech Production and Perception",
   "original": "0418",
   "page_count": 5,
   "order": 109,
   "p1": "515",
   "pn": "519",
   "abstract": [
    "Across languages of the world /r/ is known for its variability. Recent\nliterature incorporates sociolinguistic factors, such as bilingualism,\nin order to explain /r/ variation. The current study investigates to\nwhat extent /r/ is a marker of a bilingual&#8217;s dominant language.\nSpecifically, the effects of several sociolinguistic and phonotactic\nfactors on the production and perception of /r/ are investigated, such\nas the bilingual speaker&#8217;s linguistic background, the language\nspoken as well as syllable position and place of articulation. To this\nend a reading task is carried out with bilingual speakers from South\nTyrol (Italy). The major languages spoken in this region are Tyrolean\n(German dialect) and Italian. The recorded reading data is subsequently\nused in a perception experiment to investigate whether South Tyrolean\nlisteners can identify the dominant language of the speaker on the\nbasis of the presence of /r/ and the /r/ variant. Results show that\nlisteners can identify the dominant language of the bilingual speakers\non the basis of /r/. Specifically, the more Italian dominant the sociolinguistic\nbackground of the speaker, the more /r/ is produced frontally and the\nmore that speaker is perceived as Italian dominant.\n"
   ],
   "doi": "10.21437/Interspeech.2016-418"
  },
  "putzer16_interspeech": {
   "authors": [
    [
     "Manfred",
     "Pützer"
    ],
    [
     "Frank",
     "Zimmerer"
    ],
    [
     "Wolfgang",
     "Wokurek"
    ],
    [
     "Jeanin",
     "Jügler"
    ]
   ],
   "title": "Evaluation of Phonatory Behavior of German and French Speakers in Native and Non-Native Speech",
   "original": "0049",
   "page_count": 5,
   "order": 110,
   "p1": "520",
   "pn": "524",
   "abstract": [
    "Phonatory behavior of German speakers (GS) and French speakers (FS)\nin native (L1) and non-native (L2) speech was instrumentally examined.\nVowel productions of the two groups were analyzed using a parametrization\nof phonatory behavior and phonatory quality properties in the acoustic\nsignal. The behavior of GS is characterized by more strained adduction\nof the vocal folds whereas FS show more incomplete glottal closure.\nFurthermore, GS change their phonatory behavior in the foreign language\n(=French) by adapting phonatory strategies of FS, whereas FS do not\nshow this tendency. In addition, German beginners (BEG) and partly\nGerman advanced learners (ADV) are already orientated on production\ncharacteristics of the L2. French BEG however retain their phonatory\nbehavior in L2 (=German) by showing less vocal fold adduction in comparison\nto their L1. French ADV show the opposite behavior. Finally, ADV of\nthe two speaker groups generally show more strained behavior in L2\nproductions than BEG. The results provide evidence that GS and FS apply\ndifferent laryngeal phonatory settings and that they altered their\nsettings in L2 differently. Perceptual evaluation of voice quality\nof the speech material and a correlation analysis between acoustic\nand perceptual results are suggested for future research.\n"
   ],
   "doi": "10.21437/Interspeech.2016-49"
  },
  "strombergsson16_interspeech": {
   "authors": [
    [
     "Sofia",
     "Strömbergsson"
    ]
   ],
   "title": "Today&#8217;s Most Frequently Used F<SUB>0</SUB> Estimation Methods, and Their Accuracy in Estimating Male and Female Pitch in Clean Speech",
   "original": "0240",
   "page_count": 5,
   "order": 111,
   "p1": "525",
   "pn": "529",
   "abstract": [
    "Variation in fundamental frequency (F<SUB>0</SUB>) constitutes a valuable\nsource of information for researches across many disciplines, with\na shared interest in speech. Different methods for estimating F<SUB>0</SUB>\nvary in estimation accuracy and accessibility, and there is yet no\ngold standard. Through a bibliometric survey, this study examines what\nmethods were the most frequently used in the speech scientific community\nduring the years 2010&#8211;2016. Secondly, the most used methods are\nevaluated against a ground truth reference, with a specific focus on\ntheir accuracy in estimating F<SUB>0</SUB> in male and female speakers,\nrespectively.<br style=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  The results show that Praat is the dominant method by far, followed\nby STRAIGHT, RAPT and YIN. This pattern holds across a range of different\nresearch areas, although within Acoustics and Engineering, Praat&#8217;s\ndominance is less pronounced. In the evaluation including Praat, RAPT\nand YIN &#8212; with their default and gender-adapted settings &#8212;\nPraat also proved to be the most accurate. The finding that adapting\nPraat&#8217;s pitch range settings by gender leads to further improvements\nshould encourage researchers to do this routinely.\n"
   ],
   "doi": "10.21437/Interspeech.2016-240"
  },
  "he16_interspeech": {
   "authors": [
    [
     "Lei",
     "He"
    ],
    [
     "Volker",
     "Dellwo"
    ]
   ],
   "title": "A Praat-Based Algorithm to Extract the Amplitude Envelope and Temporal Fine Structure Using the Hilbert Transform",
   "original": "1447",
   "page_count": 5,
   "order": 112,
   "p1": "530",
   "pn": "534",
   "abstract": [
    "A speech signal can be viewed as a high frequency carrier signal containing\nthe temporal fine structure (TFS) that is modulated by a low frequency\nenvelope (ENV). A widely used method to decompose a speech signal into\nthe TFS and ENV is the Hilbert transform. Although this method has\nbeen available for about one century and is widely applied in various\nkinds of speech processing tasks (e.g. speech chimeras), there are\nonly very few speech processing packages that contain readily available\nfunctions for the Hilbert transform, and there is very little textbook\ntype literature tailored for speech scientists to explain the processes\nbehind the transform. With this paper we provide the code for carrying\nout the Hilbert operation to obtain the TFS and ENV in the widely used\nspeech processing software Praat, and explain the basics of the procedure.\nTo verify our code, we compare the Hilbert transform in Praat with\na widely applied function for the same purpose in  Matlab (&#8220;hilbert(&#8230;)&#8221;).\nWe can confirm that both methods arrive at identical outputs.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1447"
  },
  "enzinger16_interspeech": {
   "authors": [
    [
     "Ewald",
     "Enzinger"
    ]
   ],
   "title": "Likelihood Ratio Calculation in Acoustic-Phonetic Forensic Voice Comparison: Comparison of Three Statistical Modelling Approaches",
   "original": "1611",
   "page_count": 5,
   "order": 113,
   "p1": "535",
   "pn": "539",
   "abstract": [
    "This study compares three statistical models used to calculate likelihood\nratios in acoustic-phonetic forensic-voice-comparison systems: Multivariate\nkernel density, principal component analysis kernel density, and a\nmultivariate normal model. The data were coefficient values obtained\nfrom discrete cosine transforms fitted to human-supervised formant-trajectory\nmeasurements of tokens of /iau/ from a database of recordings of 60\nfemale speakers of Chinese. Tests were conducted using high-quality\nrecordings as nominal suspect samples and mobile-to-landline transmitted\nrecordings as nominal offender samples. Performance was assessed before\nand after fusion with a baseline automatic mel frequency cepstral coefficient\nGaussian mixture model universal background model system. In addition,\nMonte Carlo simulations were used to compare the output of the statistical\nmodels to true likelihood-ratio values calculated on the basis of the\ndistribution specified for a simulated population.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1611"
  },
  "qi16_interspeech": {
   "authors": [
    [
     "Xiaoke",
     "Qi"
    ],
    [
     "Jianhua",
     "Tao"
    ]
   ],
   "title": "A Sparse Spherical Harmonic-Based Model in Subbands for Head-Related Transfer Functions",
   "original": "0987",
   "page_count": 5,
   "order": 114,
   "p1": "540",
   "pn": "544",
   "abstract": [
    "Several functional models for head-related transfer function (HRTF)\nhave been proposed based on spherical harmonic (SH) orthogonal functions,\nwhich yield an encouraging performance level in terms of log-spectral\ndistortion (LSD). However, since the properties of subbands are quite\ndifferent and highly subject-dependent, the degree of SH expansion\nshould be adapted to the subband and the subject, which is quite challenging.\nIn this paper, a sparse spherical harmonic-based model termed SSHM\nis proposed in order to achieve an intelligent frequency truncation.\nDifferent from SH-based model (SHM) which assigns the degree for each\nsubband, SSHM constrains the number of SH coefficients by using an\nl<SUB>1</SUB> penalty, and automatically preserves the significant\ncoefficients in each subband. As a result, SSHM requires less coefficients\nat the same SD level than other truncation methods to reconstruct HRTFs\n. Furthermore, when used for interpolation, SSHM gives a better fitting\nprecision since it naturally reduces the influence of the fluctuation\ncaused by the movement of the subject and the processing error. The\nexperiments show that even using about 40% less coefficients, SSHM\nhas a slightly lower LSD than SHM. Therefore, SSHM can achieve a better\ntradeoff between efficiency and accuracy.\n"
   ],
   "doi": "10.21437/Interspeech.2016-987"
  },
  "isik16_interspeech": {
   "authors": [
    [
     "Yusuf",
     "Isik"
    ],
    [
     "Jonathan Le",
     "Roux"
    ],
    [
     "Zhuo",
     "Chen"
    ],
    [
     "Shinji",
     "Watanabe"
    ],
    [
     "John R.",
     "Hershey"
    ]
   ],
   "title": "Single-Channel Multi-Speaker Separation Using Deep Clustering",
   "original": "1176",
   "page_count": 5,
   "order": 115,
   "p1": "545",
   "pn": "549",
   "abstract": [
    "Deep clustering is a recently introduced deep learning architecture\nthat uses discriminatively trained embeddings as the basis for clustering.\nIt was recently applied to spectrogram segmentation, resulting in impressive\nresults on speaker-independent multi-speaker separation. In this paper\nwe extend the baseline system with an end-to-end signal approximation\nobjective that greatly improves performance on a challenging speech\nseparation. We first significantly improve upon the baseline system\nperformance by incorporating better regularization, larger temporal\ncontext, and a deeper architecture, culminating in an overall improvement\nin signal to distortion ratio (SDR) of 10.3 dB compared to the baseline\nof 6.0 dB for two-speaker separation, as well as a 7.1 dB SDR improvement\nfor three-speaker separation. We then extend the model to incorporate\nan enhancement layer to refine the signal estimates, and perform end-to-end\ntraining through both the clustering and enhancement stages to maximize\nsignal fidelity. We evaluate the results using automatic speech recognition.\nThe new signal approximation objective, combined with end-to-end training,\nproduces unprecedented performance, reducing the word error rate (WER)\nfrom 89.1% down to 30.8%. This represents a major advancement towards\nsolving the cocktail party problem.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1176"
  },
  "li16_interspeech": {
   "authors": [
    [
     "Hao",
     "Li"
    ],
    [
     "Shuai",
     "Nie"
    ],
    [
     "Xueliang",
     "Zhang"
    ],
    [
     "Hui",
     "Zhang"
    ]
   ],
   "title": "Jointly Optimizing Activation Coefficients of Convolutive NMF Using DNN for Speech Separation",
   "original": "0120",
   "page_count": 5,
   "order": 116,
   "p1": "550",
   "pn": "554",
   "abstract": [
    "Convolutive non-negative matrix factorization (CNMF) and deep neural\nnetworks (DNN) are two efficient methods for monaural speech separation.\nConventional DNN focuses on building the non-linear relationship between\nmixture and target speech. However, it ignores the prominent structure\nof the target speech. Conventional CNMF model concentrates on capturing\nprominent harmonic structures and temporal continuities of speech but\nit ignores the non-linear relationship between the mixture and target.\nTaking these two aspects into consideration at the same time may result\nin better performance. In this paper, we propose a joint optimization\nof DNN models with an extra CNMF layer for speech separation task.\nWe also utilize an extra masking layer on the proposed model to constrain\nthe speech reconstruction. Moreover, a discriminative training criterion\nis proposed to further enhance the performance of the separation. Experimental\nresults show that the proposed model has significant improvement in\nPESQ, SAR, SIR and SDR compared with conventional methods.\n"
   ],
   "doi": "10.21437/Interspeech.2016-120"
  },
  "delfarah16_interspeech": {
   "authors": [
    [
     "Masood",
     "Delfarah"
    ],
    [
     "DeLiang",
     "Wang"
    ]
   ],
   "title": "A Feature Study for Masking-Based Reverberant Speech Separation",
   "original": "0382",
   "page_count": 5,
   "order": 117,
   "p1": "555",
   "pn": "559",
   "abstract": [
    "Monaural speech separation in reverberant conditions is very challenging.\nIn masking-based separation, features extracted from speech mixtures\nare employed to predict a time-frequency mask. Robust feature extraction\nis crucial for the performance of supervised speech separation in adverse\nacoustic environments. Using objective speech intelligibility as the\nmetric, we investigate a wide variety of monaural features in low signal-to-noise\nratios and moderate to high reverberation. Deep neural networks are\nemployed as the learning machine in our feature investigation. We find\nconsiderable performance gain using a contextual window in reverberant\nspeech processing, likely due to temporal structure of reverberation.\nIn addition, we systematically evaluate feature combinations. In unmatched\nnoise and reverberation conditions, the resulting feature set from\nthis study substantially outperforms previously employed sets for speech\nseparation in anechoic conditions.\n"
   ],
   "doi": "10.21437/Interspeech.2016-382"
  },
  "hsu16b_interspeech": {
   "authors": [
    [
     "Chung-Chien",
     "Hsu"
    ],
    [
     "Tai-Shih",
     "Chi"
    ],
    [
     "Jen-Tzung",
     "Chien"
    ]
   ],
   "title": "Discriminative Layered Nonnegative Matrix Factorization for Speech Separation",
   "original": "0415",
   "page_count": 5,
   "order": 118,
   "p1": "560",
   "pn": "564",
   "abstract": [
    "This paper proposes a discriminative layered nonnegative matrix factorization\n(DL-NMF) for monaural speech separation. The standard NMF conducts\nthe parts-based representation using a single-layer of bases which\nwas recently upgraded to the layered NMF (L-NMF) where a tree of bases\nwas estimated for multi-level or multi-aspect decomposition of a complex\nmixed signal. In this study, we develop the DL-NMF by extending the\ngenerative bases in L-NMF to the discriminative bases which are estimated\naccording to a discriminative criterion. The discriminative criterion\nis conducted by optimizing the recovery of the mixed spectra from the\nseparated spectra and minimizing the reconstruction errors between\nseparated spectra and original source spectra. The experiments on single-channel\nspeech separation show the superiority of DL-NMF to NMF and L-NMF in\nterms of the SDR, SIR and SAR measures.\n"
   ],
   "doi": "10.21437/Interspeech.2016-415"
  },
  "gang16_interspeech": {
   "authors": [
    [
     "Arpita",
     "Gang"
    ],
    [
     "Pravesh",
     "Biyani"
    ]
   ],
   "title": "On Discriminative Framework for Single Channel Audio Source Separation",
   "original": "0701",
   "page_count": 5,
   "order": 119,
   "p1": "565",
   "pn": "569",
   "abstract": [
    "Single channel source separation (SCSS) algorithms that utilise discriminative\nsource models perform better in comparison to those that are trained\nindependently. However, all the aspects of training discriminative\nmodels have not been addressed in the literature. For instance, the\nchoice of dimensions of source models (number of columns of NMF, Dictionary\netc) not only influences the fidelity of a given source but also impacts\nthe interference introduced in it. Therefore choosing a right dimension\nparameter for every source model is crucial for an effective separation.\nIn fact, the similarity between the constituent sources can be different\nfor different mixtures and thus, dimensions should also be chosen specific\nto the sources in the concerned mixture. Further, separation of a given\nconstituent from a mixture, assuming remaining to be interferers, offers\nmore freedom for the particular constituent and hence provide better\nseparation. In this paper, we propose a generic discriminative learning\nframework where we separate one source at a time and embed our dimension\nsearch algorithm in the training of discriminative source models. We\napply our framework on the NMF based SCSS algorithms and demonstrate\na performance improvement in separation for both speech-speech and\nspeech-music mixture.\n"
   ],
   "doi": "10.21437/Interspeech.2016-701"
  },
  "jin16_interspeech": {
   "authors": [
    [
     "Qin",
     "Jin"
    ],
    [
     "Junwei",
     "Liang"
    ],
    [
     "Xiaozhu",
     "Lin"
    ]
   ],
   "title": "Generating Natural Video Descriptions via Multimodal Processing",
   "original": "0380",
   "page_count": 5,
   "order": 120,
   "p1": "570",
   "pn": "574",
   "abstract": [
    "Generating natural language descriptions of visual content is an intriguing\ntask which has wide applications such as assisting blind people. The\nrecent advances in image captioning stimulate further study of this\ntask in more depth including generating natural descriptions for videos.\nMost works of video description generation focus on visual information\nin the video. However, audio provides rich information for describing\nvideo contents as well. In this paper, we propose to generate video\ndescriptions in natural sentences via multimodal processing, which\nrefers to using both audio and visual cues via unified deep neural\nnetworks with both convolutional and recurrent structure. Experimental\nresults on the Microsoft Research Video Description (MSVD) corpus prove\nthat fusing audio information greatly improves the video description\nperformance. We also investigate the impact of image amount vs caption\namount on the image caption performance and see the trend that when\nlimited amount of training is available, number of various captions\nis more important than number of various images. This will guide us\nto investigate in the future how to improve the video description system\nvia increasing amount of training data.\n"
   ],
   "doi": "10.21437/Interspeech.2016-380"
  },
  "heckmann16_interspeech": {
   "authors": [
    [
     "Martin",
     "Heckmann"
    ]
   ],
   "title": "Feature-Level Decision Fusion for Audio-Visual Word Prominence Detection",
   "original": "0163",
   "page_count": 5,
   "order": 121,
   "p1": "575",
   "pn": "579",
   "abstract": [
    "Common fusion techniques in audio-visual speech processing operate\non the modality level. I.e. they either combine the features extracted\nfrom the two modalities directly or derive a decision for each modality\nseparately and then combine the modalities on the decision level. We\ninvestigate the audio-visual processing of linguistic prosody, more\nprecisely the extraction of word prominence. In this context the different\nfeatures for each modality can be assumed to be only partially dependent.\nHence we propose to train a classifier for each of these features,\nacoustic and visual modality, and then combine them on a decision level.\nWe compare this approach with conventional fusion methods, i.e. feature\nfusion and decision fusion on the modality level. Our results show\nthat the feature-level decision fusion clearly outperforms the other\napproaches, in particular when we also additionally integrate the features\nresulting from the feature fusion. Compared to a detection based only\non the full audio stream we obtain relative improvements from the audio-visual\ndetection of 19% for clean audio and up to 50% for noisy audio.\n"
   ],
   "doi": "10.21437/Interspeech.2016-163"
  },
  "ouni16_interspeech": {
   "authors": [
    [
     "Slim",
     "Ouni"
    ],
    [
     "Vincent",
     "Colotte"
    ],
    [
     "Sara",
     "Dahmani"
    ],
    [
     "Soumaya",
     "Azzi"
    ]
   ],
   "title": "Acoustic and Visual Analysis of Expressive Speech: A Case Study of French Acted Speech",
   "original": "0730",
   "page_count": 5,
   "order": 122,
   "p1": "580",
   "pn": "584",
   "abstract": [
    "Within the framework of developing an expressive audiovisual speech\nsynthesis, an acoustic and visual analysis of expressive acted speech\nis proposed in this paper. Our purpose is to identify the main characteristics\nof audiovisual expressions that need to be integrated during synthesis\nto provide believable emotions to the virtual 3D talking head. We conducted\na case study of a semi-professional actor who uttered a set of sentences\nfor 6 different emotions in addition to neutral speech. We have recorded\nconcurrently audio and motion capture data. The acoustic and the visual\ndata have been analyzed. The main finding is that although some expressions\nare not well identified, some expressions were well characterized and\ntied in both acoustic and visual space.\n"
   ],
   "doi": "10.21437/Interspeech.2016-730"
  },
  "barbulescu16_interspeech": {
   "authors": [
    [
     "Adela",
     "Barbulescu"
    ],
    [
     "Rémi",
     "Ronfard"
    ],
    [
     "Gérard",
     "Bailly"
    ]
   ],
   "title": "Characterization of Audiovisual Dramatic Attitudes",
   "original": "0075",
   "page_count": 5,
   "order": 123,
   "p1": "585",
   "pn": "589",
   "abstract": [
    "In this work we explore the capability of audiovisual parameters (such\nas fundamental frequency, rhythm, head motion or facial expressions)\nto discriminate among different dramatic attitudes. We extract the\naudiovisual parameters from an acted corpus of attitudes and structure\nthem as frame, syllable, and sentence-level features. Using Linear\nDiscriminant Analysis classifiers, we show that sentence-level features\npresent a higher discriminating rate among the attitudes. We also compare\nthe classification results with the perceptual evaluation tests, showing\nthat F0 is correlated to the perceptual results for all attitudes,\nwhile other features, such as head motion, contribute differently,\ndepending both on the attitude and the speaker.\n"
   ],
   "doi": "10.21437/Interspeech.2016-75"
  },
  "huang16_interspeech": {
   "authors": [
    [
     "Yuyun",
     "Huang"
    ],
    [
     "Emer",
     "Gilmartin"
    ],
    [
     "Nick",
     "Campbell"
    ]
   ],
   "title": "Conversational Engagement Recognition Using Auditory and Visual Cues",
   "original": "0846",
   "page_count": 5,
   "order": 124,
   "p1": "590",
   "pn": "594",
   "abstract": [
    "Automatic prediction of engagement in human-human and human-machine\ndyadic and multiparty interaction scenarios could greatly aid in evaluation\nof the success of communication. A corpus of eight face-to-face dyadic\ncasual conversations was recorded and used as the basis for an engagement\nstudy, which examined the effectiveness of several methods of engagement\nlevel recognition. A convolutional neural network based analysis was\nseen to be the most effective.\n"
   ],
   "doi": "10.21437/Interspeech.2016-846"
  },
  "chaspari16_interspeech": {
   "authors": [
    [
     "Theodora",
     "Chaspari"
    ],
    [
     "Jill Fain",
     "Lehman"
    ]
   ],
   "title": "An Acoustic Analysis of Child-Child and Child-Robot Interactions for Understanding Engagement during Speech-Controlled Computer Games",
   "original": "0085",
   "page_count": 5,
   "order": 125,
   "p1": "595",
   "pn": "599",
   "abstract": [
    "Engagement is an essential factor towards successful game design and\neffective human-computer interaction. We analyze the prosodic patterns\nof child-child and child-robot pairs playing a language-based computer\ngame. Acoustic features include speech loudness and fundamental frequency.\nWe use a linear mixed-effects model to capture the coordination of\nacoustic patterns between interactors as well as its relation to annotated\nengagement levels. Our results indicate that the considered acoustic\nfeatures are related to engagement levels for both the child-child\nand child-robot interaction. They further suggest significant association\nof the prosodic patterns during the child-child scenario, which is\nmoderated by the co-occurring engagement. This acoustic coordination\nis not present in the child-robot interaction, since the robot&#8217;s\nbehavior was not automatically adjusted to the child. These findings\nare discussed in relation to automatic robot adaptation and provide\na foundation for promoting engagement and enhancing rapport during\nthe considered game-based interactions.\n"
   ],
   "doi": "10.21437/Interspeech.2016-85"
  },
  "kasisopa16_interspeech": {
   "authors": [
    [
     "Benjawan",
     "Kasisopa"
    ],
    [
     "Chutamanee",
     "Onsuwan"
    ],
    [
     "Charturong",
     "Tantibundhit"
    ],
    [
     "Nittayapa",
     "Klangpornkun"
    ],
    [
     "Suparak",
     "Techacharoenrungrueang"
    ],
    [
     "Sudaporn",
     "Luksaneeyanawin"
    ],
    [
     "Denis",
     "Burnham"
    ]
   ],
   "title": "Auditory-Visual Lexical Tone Perception in Thai Elderly Listeners with and without Hearing Impairment",
   "original": "0908",
   "page_count": 5,
   "order": 126,
   "p1": "600",
   "pn": "604",
   "abstract": [
    "Lexical tone perception was investigated in elderly Thais with Normal\nHearing (NH), or Hearing Impairment (HI), the latter with and without\nHearing Aids. Auditory-visual (AV), auditory-only (AO), and visual-only\n(VO) discrimination of Thai tones was investigated. Both groups performed\npoorly in VO. In AV and AO, the NH performed better than the HI group,\nand Hearing Aids facilitated tone discrimination. There was slightly\nmore visual augmentation (AV&#62;AO) for the HI group, but not the\nNH group. The Falling-Rising (FR) pair of tones was easiest to discriminate\nfor both groups and there was a similar ranking of relative discriminability\nof all 10 tone contrasts for the HI group with and without hearing\naids, but this differed from the ranking in the NH group. These results\nshow that the Hearing Impaired elderly with and without hearing aids\ncan, and do, use visual speech information to augment tone perception,\nbut do so in a similar,  not a significantly more enhanced manner than\nthe Normal Hearing elderly. Thus hearing loss in the Thai elderly does\nnot result in greater use of visual information for discrimination\nof lexical tone; rather,  all Thai elderly use visual information to\naugment their auditory perception of tone.\n"
   ],
   "doi": "10.21437/Interspeech.2016-908"
  },
  "khaki16_interspeech": {
   "authors": [
    [
     "Hossein",
     "Khaki"
    ],
    [
     "Engin",
     "Erzin"
    ]
   ],
   "title": "Use of Agreement/Disagreement Classification in Dyadic Interactions for Continuous Emotion Recognition",
   "original": "0407",
   "page_count": 5,
   "order": 127,
   "p1": "605",
   "pn": "609",
   "abstract": [
    "Natural and affective handshakes of two participants define the course\nof dyadic interaction. Affective states of the participants are expected\nto be correlated with the nature or type of the dyadic interaction.\nIn this study, we investigate relationship between affective attributes\nand nature of dyadic interaction. In this investigation we use the\nJESTKOD database, which consists of speech and full-body motion capture\ndata recordings for dyadic interactions under agreement and disagreement\nscenarios. The dataset also has affective annotations in activation,\nvalence and dominance (AVD) attributes. We pose the continuous affect\nrecognition problem under agreement and disagreement scenarios of dyadic\ninteractions. We define a statistical mapping using the support vector\nregression (SVR) from speech and motion modalities to affective attributes\nwith and without the dyadic interaction type (DIT) information. We\nobserve an improvement in estimation of the valence attribute when\nthe DIT is available. Furthermore this improvement sustains even we\nestimate the DIT from the speech and motion modalities of the dyadic\ninteraction.\n"
   ],
   "doi": "10.21437/Interspeech.2016-407"
  },
  "schadler16_interspeech": {
   "authors": [
    [
     "Marc René",
     "Schädler"
    ],
    [
     "David",
     "Hülsmeier"
    ],
    [
     "Anna",
     "Warzybok"
    ],
    [
     "Sabine",
     "Hochmuth"
    ],
    [
     "Birger",
     "Kollmeier"
    ]
   ],
   "title": "Microscopic Multilingual Matrix Test Predictions Using an ASR-Based Speech Recognition Model",
   "original": "1119",
   "page_count": 5,
   "order": 128,
   "p1": "610",
   "pn": "614",
   "abstract": [
    "In an attempt to predict the outcomes of matrix sentence tests in different\nlanguages and various noise conditions for native listeners, the simulation\nframework for auditory discrimination experiments (FADE) and the extended\nSpeech Intelligibility Index (eSII) is employed. FADE uses an automatic\nspeech recognition system to simulate recognition experiments and reports\nthe highest achievable performance as the outcome, which showed good\npredictions for the German matrix test in noise. The eSII is based\non the short-time analysis of weighted signal-to-noise ratios in different\nfrequency bands. In contrast to many other approaches, including the\neSII, FADE uses no empirical reference. In this work, the FADE approach\nis evaluated for predictions of the German, Polish, Russian, and Spanish\nmatrix test in stationary and fluctuating noise conditions. The FADE-based\npredictions yield a high correlation (Pearsons R<SUP>2</SUP> = 0.94)\nwith the empirical data and a root-mean-square (RMS) prediction error\nof 1.9 dB outperforming the eSII-based predictions (R<SUP>2</SUP> =\n0.78, RMS = 4.2 dB). FADE can also predict the data of subgroups with\nonly stationary or only fluctuating noises, while the eSII cannot.\nThe FADE-based predictions seem to generalize over different languages\nand noise conditions.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1119"
  },
  "exter16_interspeech": {
   "authors": [
    [
     "Mats",
     "Exter"
    ],
    [
     "Bernd T.",
     "Meyer"
    ]
   ],
   "title": "DNN-Based Automatic Speech Recognition as a Model for Human Phoneme Perception",
   "original": "1285",
   "page_count": 5,
   "order": 129,
   "p1": "615",
   "pn": "619",
   "abstract": [
    "In this paper, we test the applicability of state-of-the-art automatic\nspeech recognition (ASR) to predict phoneme confusions in human listeners.\nPhoneme-specific response rates are obtained from ASR based on deep\nneural networks (DNNs) and from listening tests with six normal-hearing\nsubjects. The measure for model quality is the correlation of phoneme\nrecognition accuracies obtained in ASR and in human speech recognition\n(HSR). Various feature representations are used as input to the DNNs\nto explore their relation to overall ASR performance and model prediction\npower. Standard filterbank output and perceptual linear prediction\n(PLP) features result in best predictions, with correlation coefficients\nreaching r = 0.9. \n"
   ],
   "doi": "10.21437/Interspeech.2016-1285"
  },
  "toth16_interspeech": {
   "authors": [
    [
     "Attila Máté",
     "Tóth"
    ],
    [
     "Martin",
     "Cooke"
    ]
   ],
   "title": "Undoing Misperceptions: A Microscopic Analysis of Consistent Confusions Through Signal Modifications",
   "original": "1030",
   "page_count": 5,
   "order": 130,
   "p1": "620",
   "pn": "624",
   "abstract": [
    "Consistent confusions &#8212; word misperceptions reported in an open\nset task with a high agreement across listeners &#8212; can be especially\nvaluable in understanding the detailed processes underlying speech\nperception. The current study investigates the origin of a set of consistent\nconfusions collected in a variety of masking conditions, by applying\nsignal-level modifications to the stimuli eliciting the confusion,\nand subsequently reevaluating listeners&#8217; percepts. Modifications\nwere selected to provide release from either the energetic or the informational\ncomponent of the maskers and involved manipulations of signal-to-noise\nratio, fundamental frequency, and resynthesis of the noise-mixture\nin glimpsed regions of the target speech. Increasing signal-to-noise\nratio and glimpse resynthesis showed the expected release from energetic\nand informational masking respectively. However, manipulations targeting\ninformational masking release, including fundamental frequency modification,\naffected a surprisingly high number of confusions stemming from energetic\nmaskers. The degree of fundamental frequency shift did not have a significant\neffect on the response patterns observed. Around 30% of confusions\ncan be explained solely based on the information contained within the\ntarget glimpses surviving energetic masking, while for the rest of\nthe cases additional factors, such as recruitment of information from\nthe masker, appear to be involved.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1030"
  },
  "karbasi16_interspeech": {
   "authors": [
    [
     "Mahdie",
     "Karbasi"
    ],
    [
     "Ahmed Hussen",
     "Abdelaziz"
    ],
    [
     "Hendrik",
     "Meutzner"
    ],
    [
     "Dorothea",
     "Kolossa"
    ]
   ],
   "title": "Blind Non-Intrusive Speech Intelligibility Prediction Using Twin-HMMs",
   "original": "0155",
   "page_count": 5,
   "order": 131,
   "p1": "625",
   "pn": "629",
   "abstract": [
    "Automatic prediction of speech intelligibility is highly desirable\nin the speech research community, since listening tests are time-consuming\nand can not be used online. Most of the available objective speech\nintelligibility measures are intrusive methods, as they require a clean\nreference signal in addition to the corresponding noisy/processed signal\nat hand. In order to overcome the problem of predicting the speech\nintelligibility in the absence of the clean reference signal, we have\nproposed in [1] to employ a recognition/synthesis framework called\ntwin hidden Markov model (THMM) for synthesizing the clean features,\nrequired inside an intrusive intelligibility prediction method. The\nnew framework can predict the speech intelligibility equally well as\nwell-known intrusive methods like the short-time objective intelligibility\n(STOI). The original THMM, however, requires the correct transcription\nfor synthesizing the clean reference features, which is not always\navailable. In this paper, we go one step further and investigate the\nuse of the recognized transcription instead of the oracle transcription\nfor obtaining a more widely applicable speech intelligibility prediction.\nWe show that the output of the newly-proposed blind approach is highly\ncorrelated with the human speech recognition results, collected via\ncrowdsourcing in different noise conditions.\n"
   ],
   "doi": "10.21437/Interspeech.2016-155"
  },
  "toth16b_interspeech": {
   "authors": [
    [
     "Attila Máté",
     "Tóth"
    ],
    [
     "Martin",
     "Cooke"
    ],
    [
     "Jon",
     "Barker"
    ]
   ],
   "title": "Misperceptions Arising from Speech-in-Babble Interactions",
   "original": "0024",
   "page_count": 5,
   "order": 132,
   "p1": "630",
   "pn": "634",
   "abstract": [
    "The deterioration of speech intelligibility in the presence of other\nsound sources has been explained in terms of both energetic masking,\nwhich renders parts of the speech signal inaudible, and informational\nmasking, in which audible components of the masker interfere with speech\nidentification. The current study focuses on the role of a specific\nform of informational masking in which audible glimpses of both target\nand masker combine to produce an incorrect listener percept. We examine\na corpus of word misperceptions in Spanish which occur when target\nwords are combined with a babble masker. Glimpses originating in both\nthe target and the masker are force-aligned to the reported misperceived\nword in order to identify the most likely acoustic evidential basis\nfor the confusion. In this way, the degree of involvement of both target\nand masker can be quantified. In nearly all cases, the best explanation\nfor the misperception involves recruiting evidence from the babble\nmasker (type I error), and in more than 80% of the tokens some of the\naudible target evidence is ignored (type II error). These findings\nsuggest misallocation of acoustic-phonetic material plays a significant\nrole in the generation of speech-in-babble confusions.\n"
   ],
   "doi": "10.21437/Interspeech.2016-24"
  },
  "eichenauer16_interspeech": {
   "authors": [
    [
     "Anja",
     "Eichenauer"
    ],
    [
     "Mathias",
     "Dietz"
    ],
    [
     "Bernd T.",
     "Meyer"
    ],
    [
     "Tim",
     "Jürgens"
    ]
   ],
   "title": "Introducing Temporal Rate Coding for Speech in Cochlear Implants: A Microscopic Evaluation in Humans and Models",
   "original": "0267",
   "page_count": 5,
   "order": 133,
   "p1": "635",
   "pn": "639",
   "abstract": [
    "Standard cochlea implant (CI) speech coding strategies transmit formant\ninformation only via the place of the stimulated electrode. In acoustic\nhearing, however, formant frequencies are additionally coded via the\ntemporal rate of auditory nerve firing. This study presents a novel\nCI coding strategy (&#8220;Formant Locking (FL)-strategy&#8221;) that\nvaries stimulation rates in relation to extracted fundamental and formant\nfrequencies. Simulated auditory nerve activity resulting from stimulation\nwith the FL-strategy shows that the FL-strategy triggers spike rates\nthat are related to the formant frequencies similar as in normal hearing,\nand greatly different than in a standard CI strategy. Vowel recognition\nin seven CI users via direct stimulation of their electrode array shows\nthat the FL-strategy results in significantly increased scores of the\nvowels /u/ and /i/ compared to a standard CI strategy. However, at\nthe same time, a decrease in scores for /o/ and /e/ occurred. A microscopic\nspeech intelligibility model involving an automatic speech recognizer\nreveals good agreement between modeled and predicted confusion matrices\nfor the FL-strategy. This suggests that microscopic models can be used\nto test CI strategies in the development phase, and gives indications\nwhich cues might be used by the listeners for speech recognition.\n"
   ],
   "doi": "10.21437/Interspeech.2016-267"
  },
  "lecumberri16_interspeech": {
   "authors": [
    [
     "Maria Luisa Garcia",
     "Lecumberri"
    ],
    [
     "Jon",
     "Barker"
    ],
    [
     "Ricard",
     "Marxer"
    ],
    [
     "Martin",
     "Cooke"
    ]
   ],
   "title": "Language Effects in Noise-Induced Word Misperceptions",
   "original": "0330",
   "page_count": 5,
   "order": 134,
   "p1": "640",
   "pn": "644",
   "abstract": [
    "Speech misperceptions provide a window into the processes underlying\nspoken language comprehension. One approach shown to catalyse robust\nmisperceptions is to embed words in noise. However, the use of masking\nnoise makes it difficult to measure the relative contributions of low-level\nauditory processing and higher-level factors which involve the deployment\nof linguistic experience. The current study addresses this confound\nby comparing noise-induced misperceptions in two languages, Spanish\nand English, which display marked phonological differences in properties\nsuch as consonant-vowel ratio, rhythm and syllable structure. An analysis\nof over 5000 word-level misperceptions generated using a common experimental\nframework in the two languages reveals some striking similarities:\nthe proportion of confusions generated by three distinct types of masker\nare almost identical for the two languages, as are the proportions\nof phonemic and syllabic insertions, deletions and substitutions. The\nbiggest difference is seen for babble noise, which tends to induce\nrelatively complex confusions in English and simpler confusions in\nSpanish. We speculate that the inflectional morphology of Spanish lends\nitself to more easily recruit single elements from a babble masker\ninto valid word hypotheses.\n"
   ],
   "doi": "10.21437/Interspeech.2016-330"
  },
  "varnet16_interspeech": {
   "authors": [
    [
     "Léo",
     "Varnet"
    ],
    [
     "Fanny",
     "Meunier"
    ],
    [
     "Michel",
     "Hoen"
    ]
   ],
   "title": "Speech Reductions Cause a De-Weighting of Secondary Acoustic Cues",
   "original": "0343",
   "page_count": 5,
   "order": 135,
   "p1": "645",
   "pn": "649",
   "abstract": [
    "The ability of the auditory system to change the perceptual weighting\nof acoustic cues when faced with degraded speech has long been evidenced.\nHowever, the exact changes that occur remain mostly unknown. Here,\nwe proposed to use the Auditory Classification Image (ACI) methodology\nto reveal the acoustic cues used in natural speech comprehension and\nin reduced (i.e. noise-vocoded or re-synthesized) speech comprehension.\nThe results show that in the latter case the auditory system updates\nits listening strategy by de-weighting secondary acoustic cues. Indeed,\nthese are often weaker and thus more easily erased in adverse listening\nconditions. Furthermore our data suggests that this de-weighting does\nnot directly depend on the actual reliability of the cues, but rather\non the expected change in informativeness.\n"
   ],
   "doi": "10.21437/Interspeech.2016-343"
  },
  "fontan16_interspeech": {
   "authors": [
    [
     "Lionel",
     "Fontan"
    ],
    [
     "Isabelle",
     "Ferrané"
    ],
    [
     "Jérôme",
     "Farinas"
    ],
    [
     "Julien",
     "Pinquier"
    ],
    [
     "Xavier",
     "Aumont"
    ]
   ],
   "title": "Using Phonologically Weighted Levenshtein Distances for the Prediction of Microscopic Intelligibility",
   "original": "0431",
   "page_count": 5,
   "order": 136,
   "p1": "650",
   "pn": "654",
   "abstract": [
    "This article presents a new method for analyzing Automatic Speech Recognition\n(ASR) results at the phonological feature level. To this end the Levenshtein\ndistance algorithm is refined in order to take into account the distinctive\nfeatures opposing substituted phonemes. This method allows to survey\nfeatures additions or deletions, providing microscopic qualitative\ninformation as a complement to word recognition scores. To explore\nthe relevance of the qualitative data gathered by this method, a study\nis conducted on a speech corpus simulating presbycusis effects on speech\nperception at eight severity stages. Consonantic features additions\nand deletions in ASR outputs are analyzed and put in relation with\nintelligibility data collected in 30 human subjects. ASR results show\nmonotonic trends in most consonantic features along the degradation\nconditions, which appear to be consistent with the misperceptions that\ncould be observed in human subjects.\n"
   ],
   "doi": "10.21437/Interspeech.2016-431"
  },
  "matsui16_interspeech": {
   "authors": [
    [
     "Mayuki",
     "Matsui"
    ]
   ],
   "title": "The Impact of Manner of Articulation on the Intelligibility of Voicing Contrast in Noise: Cross-Linguistic Implications",
   "original": "0697",
   "page_count": 5,
   "order": 137,
   "p1": "655",
   "pn": "659",
   "abstract": [
    "The current study addresses the impact of manner of articulation on\nthe intelligibility of voicing contrast in noise from a cross-linguistic\nperspective. Previous noise-masking studies have suggested that the\nimpact of manner of articulation on the intelligibility of voicing\ncontrast in noise is apparently different in Russian and English. In\norder to further assess the source of such a cross-linguistic inconsistency,\nthe current study examines how Russian voicing contrast is perceived\nby English listeners. Native listeners of English performed a forced-choice\nidentification task with Russian voiced and voiceless stimuli in quiet\nand noisy conditions. The results showed that the voicing contrast\nin stops were more confused than that in fricatives for English listeners,\nshowing a pattern similar to Russian listeners. The results suggest\nthat the source of the cross-linguistic difference identified in previous\nstudies comes from the difference in the acoustic properties of the\nstimuli, reflecting the difference in phonetic implementation of voicing\ncontrasts in each language. The results in turn suggest that perceptual\ncue weighting strategies for perceiving voicing contrast in different\nmanners of articulation is similar among Russian and English listeners.\n"
   ],
   "doi": "10.21437/Interspeech.2016-697"
  },
  "mandel16_interspeech": {
   "authors": [
    [
     "Michael I.",
     "Mandel"
    ]
   ],
   "title": "Directly Comparing the Listening Strategies of Humans and Machines",
   "original": "0932",
   "page_count": 5,
   "order": 138,
   "p1": "660",
   "pn": "664",
   "abstract": [
    "In a given noisy environment, human listeners can more accurately identify\nspoken words than automatic speech recognizers. It is not clear, however,\nwhat information the humans are able to utilize in doing so that the\nmachines are not. This paper uses a recently introduced technique to\ndirectly characterize the information used by humans and machines on\nthe same task. The task was a forced choice between eight sentences\nspoken by a single talker from the small-vocabulary GRID corpus that\nwere selected to be maximally confusable with one another. These sentences\nwere mixed with &#8220;bubble&#8221; noise, which is designed to reveal\nrandomly selected time-frequency glimpses of the sentence. Responses\nto these noisy mixtures allowed the identification of time-frequency\nregions that were important for each listener to recognize each sentence,\ni.e., regions that were frequently audible when a sentence was correctly\nidentified and inaudible when it was not. In comparing these regions\nacross human and machine listeners, we found that dips in noise allowed\nthe humans to recognize words based on informative speech cues. In\ncontrast, the baseline CHiME-2-GRID recognizer correctly identified\nsentences only when the time-frequency profile of the noisy mixture\nmatched that of the underlying speech.\n"
   ],
   "doi": "10.21437/Interspeech.2016-932"
  },
  "rondeau16_interspeech": {
   "authors": [
    [
     "Marc-Antoine",
     "Rondeau"
    ],
    [
     "Yi",
     "Su"
    ]
   ],
   "title": "LSTM-Based NeuroCRFs for Named Entity Recognition",
   "original": "0288",
   "page_count": 5,
   "order": 139,
   "p1": "665",
   "pn": "669",
   "abstract": [
    "Although NeuroCRF, an augmented Conditional Random Fields (CRF) model\nwhose feature function is parameterized as a Feed-Forward Neural Network\n(FF NN) on word embeddings, has soundly outperformed traditional linear-chain\nCRF on many sequence labeling tasks, it is held back by the fact that\nFF NNs have a fixed input length and therefore cannot take advantage\nof the full input sentence. We propose to address this issue by replacing\nthe FF NN with a Long Short-Term Memory (LSTM) NN, which can summarize\nan input of arbitrary length into a fixed dimension representation.\nThe resulting model obtains F<SUB>1</SUB>=89.28 on WikiNER dataset,\na significant improvement over the NeuroCRF baseline&#8217;s F<SUB>1</SUB>=87.58,\nwhich is already a highly competitive result.\n"
   ],
   "doi": "10.21437/Interspeech.2016-288"
  },
  "liu16b_interspeech": {
   "authors": [
    [
     "Shih-Hung",
     "Liu"
    ],
    [
     "Kuan-Yu",
     "Chen"
    ],
    [
     "Yu-Lun",
     "Hsieh"
    ],
    [
     "Berlin",
     "Chen"
    ],
    [
     "Hsin-Min",
     "Wang"
    ],
    [
     "Hsu-Chun",
     "Yen"
    ],
    [
     "Wen-Lian",
     "Hsu"
    ]
   ],
   "title": "Exploring Word Mover&#8217;s Distance and Semantic-Aware Embedding Techniques for Extractive Broadcast News Summarization",
   "original": "0710",
   "page_count": 5,
   "order": 140,
   "p1": "670",
   "pn": "674",
   "abstract": [
    "Extractive summarization is a process that manages to select the most\nsalient sentences from a document (or a set of documents) and subsequently\nassemble them to form an informative summary, facilitating users to\nbrowse and assimilate the main theme of the document efficiently. Our\nwork in this paper continues this general line of research and its\nmain contributions are two-fold. First, we explore to leverage the\nrecently proposed word mover&#8217;s distance (WMD) metric, in conjunction\nwith semantic-aware continuous space representations of words, to authentically\ncapture finer-grained sentence-to-document and/or sentence-to-sentence\nsemantic relatedness for effective use in the summarization process.\nSecond, we investigate to combine our proposed approach with several\nstate-of-the-art summarization methods, which originally adopted the\nconventional term-overlap or bag-of-words (BOW) approaches for similarity\ncalculation. A series of experiments conducted on a typical broadcast\nnews summarization task seem to suggest the performance merits of our\nproposed approach, in comparison to the mainstream methods.\n"
   ],
   "doi": "10.21437/Interspeech.2016-710"
  },
  "sheikh16_interspeech": {
   "authors": [
    [
     "Imran",
     "Sheikh"
    ],
    [
     "Irina",
     "Illina"
    ],
    [
     "Dominique",
     "Fohr"
    ],
    [
     "Georges",
     "Linarès"
    ]
   ],
   "title": "Improved Neural Bag-of-Words Model to Retrieve Out-of-Vocabulary Words in Speech Recognition",
   "original": "1219",
   "page_count": 5,
   "order": 141,
   "p1": "675",
   "pn": "679",
   "abstract": [
    "Many Proper Names (PNs) are Out-Of-Vocabulary (OOV) words for speech\nrecognition systems used to process diachronic audio data. To enable\nrecovery of the PNs missed by the system, relevant OOV PNs can be retrieved\nby exploiting the semantic context of the spoken content. In this paper,\nwe explore the Neural Bag-of-Words (NBOW) model, proposed previously\nfor text classification, to retrieve relevant OOV PNs. We propose a\nNeural Bag-of-Weighted-Words (NBOW2) model in which the input embedding\nlayer is augmented with a context anchor layer. This layer learns to\nassign importance to input words and has the ability to capture (task\nspecific) key-words in a NBOW model. With experiments on French broadcast\nnews videos we show that the NBOW and NBOW2 models outperform earlier\nmethods based on raw embeddings from LDA and Skip-gram. Combining NBOW\nwith NBOW2 gives faster convergence during training.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1219"
  },
  "trione16_interspeech": {
   "authors": [
    [
     "Jérémy",
     "Trione"
    ],
    [
     "Benoit",
     "Favre"
    ],
    [
     "Frederic",
     "Bechet"
    ]
   ],
   "title": "Beyond Utterance Extraction: Summary Recombination for Speech Summarization",
   "original": "0855",
   "page_count": 5,
   "order": 142,
   "p1": "680",
   "pn": "684",
   "abstract": [
    "This paper describes a template filling approach for creating conversation\nsummaries. The templates are generated from generalized summary fragments\nfrom a training corpus. Necessary pieces of information for filling\nthem are extracted automatically from the conversation transcripts\ngiven linguistic features, and drive the fragment selection process.\nThe approach obtains ROUGE-2 scores of 0.08471 on the RATP-DECODA corpus,\nwhich represents a significant improvement over extractive baselines\nand hand-written templates.\n"
   ],
   "doi": "10.21437/Interspeech.2016-855"
  },
  "liu16c_interspeech": {
   "authors": [
    [
     "Bing",
     "Liu"
    ],
    [
     "Ian",
     "Lane"
    ]
   ],
   "title": "Attention-Based Recurrent Neural Network Models for Joint Intent Detection and Slot Filling",
   "original": "1352",
   "page_count": 5,
   "order": 143,
   "p1": "685",
   "pn": "689",
   "abstract": [
    "Attention-based encoder-decoder neural network models have recently\nshown promising results in machine translation and speech recognition.\nIn this work, we propose an attention-based neural network model for\njoint intent detection and slot filling, both of which are critical\nsteps for many speech understanding and dialog systems. Unlike in machine\ntranslation and speech recognition, alignment is explicit in slot filling.\nWe explore different strategies in incorporating this alignment information\nto the encoder-decoder framework. Learning from the attention mechanism\nin encoder-decoder model, we further propose introducing attention\nto the alignment-based RNN models. Such attentions provide additional\ninformation to the intent classification and slot label prediction.\nOur independent task models achieve state-of-the-art intent detection\nerror rate and slot filling F1 score on the benchmark ATIS task. Our\njoint training model further obtains 0.56% absolute (23.8% relative)\nerror reduction on intent detection and 0.23% absolute gain on slot\nfilling over the independent task models.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1352"
  },
  "jaech16_interspeech": {
   "authors": [
    [
     "Aaron",
     "Jaech"
    ],
    [
     "Larry",
     "Heck"
    ],
    [
     "Mari",
     "Ostendorf"
    ]
   ],
   "title": "Domain Adaptation of Recurrent Neural Networks for Natural Language Understanding",
   "original": "1598",
   "page_count": 5,
   "order": 144,
   "p1": "690",
   "pn": "694",
   "abstract": [
    "The goal of this paper is to use multi-task learning to efficiently\nscale slot filling models for natural language understanding to handle\nmultiple target tasks or domains. The key to scalability is reducing\nthe amount of training data needed to learn a model for a new task.\nThe proposed multi-task model delivers better performance with less\ndata by leveraging patterns that it learns from the other tasks. The\napproach supports an open vocabulary, which allows the models to generalize\nto unseen words, which is particularly important when very little training\ndata is used. A newly collected crowd-sourced data set, covering four\ndifferent domains, is used to demonstrate the effectiveness of the\ndomain adaptation and open vocabulary techniques.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1598"
  },
  "ladhak16_interspeech": {
   "authors": [
    [
     "Faisal",
     "Ladhak"
    ],
    [
     "Ankur",
     "Gandhe"
    ],
    [
     "Markus",
     "Dreyer"
    ],
    [
     "Lambert",
     "Mathias"
    ],
    [
     "Ariya",
     "Rastrow"
    ],
    [
     "Björn",
     "Hoffmeister"
    ]
   ],
   "title": " LatticeRnn: Recurrent Neural Networks Over Lattices",
   "original": "1583",
   "page_count": 5,
   "order": 145,
   "p1": "695",
   "pn": "699",
   "abstract": [
    "We present a new model called  LatticeRnn, which generalizes recurrent\nneural networks (RNNs) to process weighted lattices as input, instead\nof sequences. A  LatticeRnn can encode the complete structure of a\nlattice into a dense representation, which makes it suitable to a variety\nof problems, including rescoring, classifying, parsing, or translating\nlattices using deep neural networks (DNNs). In this paper, we use \nLatticeRnns for a classification task: each lattice represents the\noutput from an automatic speech recognition (ASR) component of a spoken\nlanguage understanding (SLU) system, and we classify the intent of\nthe spoken utterance based on the lattice embedding computed by a \nLatticeRnn. We show that making decisions based on the full ASR output\nlattice, as opposed to 1-best or n-best hypotheses, makes SLU systems\nmore robust to ASR errors. Our experiments yield improvements of 13%\nover a baseline RNN system trained on transcriptions and 10% over an\nn-best list rescoring system for intent classification.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1583"
  },
  "kesiraju16_interspeech": {
   "authors": [
    [
     "Santosh",
     "Kesiraju"
    ],
    [
     "Lukáš",
     "Burget"
    ],
    [
     "Igor",
     "Szőke"
    ],
    [
     "Jan",
     "Černocký"
    ]
   ],
   "title": "Learning Document Representations Using Subspace Multinomial Model",
   "original": "1634",
   "page_count": 5,
   "order": 146,
   "p1": "700",
   "pn": "704",
   "abstract": [
    "Subspace multinomial model (SMM) is a log-linear model and can be used\nfor learning low dimensional continuous representation for discrete\ndata. SMM and its variants have been used for speaker verification\nbased on prosodic features and phonotactic language recognition. In\nthis paper, we propose a new variant of SMM that introduces sparsity\nand call the resulting model as &#8467;<SUB>1</SUB> SMM. We show that\n&#8467;<SUB>1</SUB> SMM can be used for learning document representations\nthat are helpful in topic identification or classification and clustering\ntasks. Our experiments in document classification show that SMM achieves\ncomparable results to models such as latent Dirichlet allocation and\nsparse topical coding, while having a useful property that the resulting\ndocument vectors are Gaussian distributed.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1634"
  },
  "zhao16_interspeech": {
   "authors": [
    [
     "Zhiwei",
     "Zhao"
    ],
    [
     "Youzheng",
     "Wu"
    ]
   ],
   "title": "Attention-Based Convolutional Neural Networks for Sentence Classification",
   "original": "0354",
   "page_count": 5,
   "order": 147,
   "p1": "705",
   "pn": "709",
   "abstract": [
    "Sentence classification is one of the foundational tasks in spoken\nlanguage understanding (SLU) and natural language processing (NLP).\nIn this paper we propose a novel convolutional neural network (CNN)\nwith attention mechanism to improve the performance of sentence classification.\nIn traditional CNN, it is not easy to encode long term contextual information\nand correlation between non-consecutive words effectively. In contrast,\nour attention-based CNN is able to capture these kinds of information\nfor each word without any external features. We conducted experiments\non various public and in-house datasets. The experimental results demonstrate\nthat our proposed model significantly outperforms the traditional CNN\nmodel and achieves competitive performance with the ones that exploit\nrich syntactic features.\n"
   ],
   "doi": "10.21437/Interspeech.2016-354"
  },
  "morchid16_interspeech": {
   "authors": [
    [
     "Mohamed",
     "Morchid"
    ],
    [
     "Mohamed",
     "Bouaziz"
    ],
    [
     "Waad Ben",
     "Kheder"
    ],
    [
     "Killian",
     "Janod"
    ],
    [
     "Pierre-Michel",
     "Bousquet"
    ],
    [
     "Richard",
     "Dufour"
    ],
    [
     "Georges",
     "Linarès"
    ]
   ],
   "title": "Spoken Language Understanding in a Latent Topic-Based Subspace",
   "original": "0050",
   "page_count": 5,
   "order": 148,
   "p1": "710",
   "pn": "714",
   "abstract": [
    "Performance of spoken language understanding applications declines\nwhen spoken documents are automatically transcribed in noisy conditions\ndue to high Word Error Rates (WER). To improve the robustness to transcription\nerrors, recent solutions propose to map these automatic transcriptions\nin a latent space. These studies have proposed to compare classical\ntopic-based representations such as Latent Dirichlet Allocation (LDA),\nsupervised LDA and author-topic (AT) models. An original compact representation,\ncalled c-vector, has recently been introduced to walk around the tricky\nchoice of the number of latent topics in these topic-based representations.\nMoreover, c-vectors allow to increase the robustness of document classification\nwith respect to transcription errors by compacting different LDA representations\nof a same speech document in a reduced space and then compensate most\nof the noise of the document representation. The main drawback of this\nmethod is the number of sub-tasks needed to build the c-vector space.\nThis paper proposes to both improve this compact representation (c-vector)\nof spoken documents and to reduce the number of needed sub-tasks, using\nan original framework in a robust low dimensional space of features\nfrom a set of AT models called &#8220;Latent Topic-based Subspace&#8221;\n(LTS). In comparison to LDA, the AT model considers not only the dialogue\ncontent (words), but also the class related to the document. Experiments\nare conducted on the DECODA corpus containing speech conversations\nfrom the call-center of the RATP Paris transportation company. Results\nshow that the original LTS representation outperforms the best previous\ncompact representation (c-vector), with a substantial gain of more\nthan 2.5% in terms of correctly labeled conversations.\n"
   ],
   "doi": "10.21437/Interspeech.2016-50"
  },
  "hakkanitur16_interspeech": {
   "authors": [
    [
     "Dilek",
     "Hakkani-Tür"
    ],
    [
     "Gokhan",
     "Tur"
    ],
    [
     "Asli",
     "Celikyilmaz"
    ],
    [
     "Yun-Nung",
     "Chen"
    ],
    [
     "Jianfeng",
     "Gao"
    ],
    [
     "Li",
     "Deng"
    ],
    [
     "Ye-Yi",
     "Wang"
    ]
   ],
   "title": "Multi-Domain Joint Semantic Frame Parsing Using Bi-Directional RNN-LSTM",
   "original": "0402",
   "page_count": 5,
   "order": 149,
   "p1": "715",
   "pn": "719",
   "abstract": [
    "Sequence-to-sequence deep learning has recently emerged as a new paradigm\nin supervised learning for spoken language understanding. However,\nmost of the previous studies explored this framework for building single\ndomain models for each task, such as slot filling or domain classification,\ncomparing deep learning based approaches with conventional ones like\nconditional random fields. This paper proposes a holistic multi-domain,\nmulti-task (i.e. slot filling, domain and intent detection) modeling\napproach to estimate complete semantic frames for all user utterances\naddressed to a conversational system, demonstrating the distinctive\npower of deep learning methods, namely bi-directional recurrent neural\nnetwork (RNN) with long-short term memory (LSTM) cells (RNN-LSTM) to\nhandle such complexity. The contributions of the presented work are\nthree-fold: (i) we propose an RNN-LSTM architecture for joint modeling\nof slot filling, intent determination, and domain classification; (ii)\nwe build a joint multi-domain model enabling multi-task deep learning\nwhere the data from each domain reinforces each other; (iii) we investigate\nalternative architectures for modeling lexical context in spoken language\nunderstanding. In addition to the simplicity of the single model framework,\nexperimental results show the power of such an approach on Microsoft\nCortana real user data over alternative methods based on single domain/task\ndeep learning.\n"
   ],
   "doi": "10.21437/Interspeech.2016-402"
  },
  "janod16_interspeech": {
   "authors": [
    [
     "Killian",
     "Janod"
    ],
    [
     "Mohamed",
     "Morchid"
    ],
    [
     "Richard",
     "Dufour"
    ],
    [
     "Georges",
     "Linarès"
    ],
    [
     "Renato De",
     "Mori"
    ]
   ],
   "title": "Deep Stacked Autoencoders for Spoken Language Understanding",
   "original": "0063",
   "page_count": 5,
   "order": 150,
   "p1": "720",
   "pn": "724",
   "abstract": [
    "The automatic transcription process of spoken document results in several\nword errors, especially when very noisy conditions are encountered.\nDocument representations based on neural embedding frameworks have\nrecently shown significant improvements in different Spoken and Natural\nLanguage Understanding tasks such as denoising and filtering. Nonetheless,\nthese methods mainly need clean representations, failing to properly\nremove noise contained in noisy representations. This paper proposes\nto study the impact of residual noise contained into automatic transcripts\nof spoken dialogues in highly abstract spaces from deep neural networks.\nThe paper makes the assumption that the noise learned from &#8220;clean&#8221;\nmanual transcripts of spoken documents moves down dramatically the\nperformance of theme identification systems in noisy conditions. The\nproposed deep neural network takes, as input and output, highly imperfect\ntranscripts from spoken dialogues to improve the robustness of the\ndocument representation in a noisy environment. Results obtained on\nthe DECODA theme classification task of dialogues reach an accuracy\nof 82% with a significant gain of about 5%.\n"
   ],
   "doi": "10.21437/Interspeech.2016-63"
  },
  "kurata16b_interspeech": {
   "authors": [
    [
     "Gakuto",
     "Kurata"
    ],
    [
     "Bing",
     "Xiang"
    ],
    [
     "Bowen",
     "Zhou"
    ]
   ],
   "title": "Labeled Data Generation with Encoder-Decoder LSTM for Semantic Slot Filling",
   "original": "0727",
   "page_count": 5,
   "order": 151,
   "p1": "725",
   "pn": "729",
   "abstract": [
    "To train a model for semantic slot filling, manually labeled data in\nwhich each word is annotated with a semantic slot label is necessary\nwhile manually preparing such data is costly. Starting from a small\namount of manually labeled data, we propose a method to generate the\nlabeled data with using the encoder-decoder LSTM. We first train the\nencoder-decoder LSTM that accepts and generates the same manually labeled\ndata. Then, to generate a wide variety of labeled data, we add perturbations\nto the vector that encodes the manually labeled data and generate labeled\ndata with the decoder LSTM based on the perturbated encoded vector.\nWe also try to enhance the encoder-decoder LSTM to generate the word\nsequences and their label sequences separately to obtain new pairs\nof words and their labels. Through the experiments with the standard\nATIS slot filling task, by using the generated data, we obtained improvement\nin slot filling accuracy over the strong baseline with the NN-based\nslot filling model.\n"
   ],
   "doi": "10.21437/Interspeech.2016-727"
  },
  "stehwien16_interspeech": {
   "authors": [
    [
     "Sabrina",
     "Stehwien"
    ],
    [
     "Ngoc Thang",
     "Vu"
    ]
   ],
   "title": "Exploring the Correlation of Pitch Accents and Semantic Slots for Spoken Language Understanding",
   "original": "0511",
   "page_count": 5,
   "order": 152,
   "p1": "730",
   "pn": "734",
   "abstract": [
    "We investigate the correlation between pitch accents and semantic slots\nin human-machine speech. Using an automatic pitch accent detector on\nthe ATIS corpus, we find that most words labelled with semantic slots\nalso carry a pitch accent. Most of the pitch accented words that are\nnot associated with a semantic label are still meaningful, pointing\ntowards the speaker&#8217;s intention. Our findings show that prosody\nconstitutes a relevant and useful resource for spoken language understanding,\nespecially considering the fact that our pitch accent detector does\nnot require any kind of manual transcriptions during testing time.\n"
   ],
   "doi": "10.21437/Interspeech.2016-511"
  },
  "tang16_interspeech": {
   "authors": [
    [
     "Yaodong",
     "Tang"
    ],
    [
     "Zhiyong",
     "Wu"
    ],
    [
     "Helen",
     "Meng"
    ],
    [
     "Mingxing",
     "Xu"
    ],
    [
     "Lianhong",
     "Cai"
    ]
   ],
   "title": "Analysis on Gated Recurrent Unit Based Question Detection Approach",
   "original": "0964",
   "page_count": 5,
   "order": 153,
   "p1": "735",
   "pn": "739",
   "abstract": [
    "Recent studies have shown various kinds of recurrent neural networks\n(RNNs) are becoming powerful sequence models in speech related applications.\nOur previous work in detecting questions of Mandarin speech presents\nthat gated recurrent unit (GRU) based RNN can achieve significantly\nbetter results. In this paper, we try to open the black box to find\nthe correlations between inner architecture of GRU and phonetic features\nof question sentences. We find that both update gate and reset gate\nin GRU blocks react when people begin to pronounce a word. According\nto the reactions, experiments are conducted to show the behavior of\nGRU based question detection approach on three important factors, including\nkeywords or special structure of questions, final particles and interrogative\nintonation. We also observe that update gate and reset gate don&#8217;t\ncollaborate well on our dataset. Based on the asynchronous acts of\nupdate gate and reset gate in GRU, we adapt the structure of GRU block\nto our dataset and get further performance improvement in question\ndetection task.\n"
   ],
   "doi": "10.21437/Interspeech.2016-964"
  },
  "oishi16_interspeech": {
   "authors": [
    [
     "Shuji",
     "Oishi"
    ],
    [
     "Tatsuya",
     "Matsuba"
    ],
    [
     "Mitsuaki",
     "Makino"
    ],
    [
     "Atsuhiko",
     "Kai"
    ]
   ],
   "title": "Combining State-Level Spotting and Posterior-Based Acoustic Match for Improved Query-by-Example Spoken Term Detection",
   "original": "1259",
   "page_count": 5,
   "order": 154,
   "p1": "740",
   "pn": "744",
   "abstract": [
    "In spoken term detection (STD) systems, automatic speech recognition\n(ASR) frontend is often employed for its reasonable accuracy and efficiency.\nHowever, out-of-vocabulary (OOV) problem at ASR stage has a great impact\non the STD performance for spoken query. In this paper, we propose\ncombining feature-based acoustic match which is often employed in the\nSTD systems for low resource languages, along with the other ASR-derived\nfeatures. First, automatic transcripts for spoken document and spoken\nquery are decomposed into corresponding acoustic model state sequences\nand used for spotting plausible speech segments. Second, DTW-based\nacoustic match between the query and candidate segment is performed\nusing the posterior features derived from a monophone-state DNN. Finally,\nan integrated score is obtained by a logistic regression model, which\nis trained with a large spoken document and automatically generated\nspoken queries as development data. The experimental results on NTCIR-12\nSpokenQuery&amp;Doc-2 task showed that the proposed method significantly\noutperforms the baseline systems which use the subword-level or state-level\nspotting alone. Also, our universal scoring model trained with a separate\nset of development data could achieve the best STD performance, and\nshowed the effectiveness of additional ASR-derived features regarding\nthe confidence measure and query length.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1259"
  },
  "lv16_interspeech": {
   "authors": [
    [
     "Zhiqiang",
     "Lv"
    ],
    [
     "Meng",
     "Cai"
    ],
    [
     "Wei-Qiang",
     "Zhang"
    ],
    [
     "Jia",
     "Liu"
    ]
   ],
   "title": "A Novel Discriminative Score Calibration Method for Keyword Search",
   "original": "0606",
   "page_count": 5,
   "order": 155,
   "p1": "745",
   "pn": "749",
   "abstract": [
    "The performance of keyword search systems depends heavily on the quality\nof confidence scores. In this work, a novel discriminative score calibration\nmethod has been proposed. By training an MLP classifier employing the\nword posterior probability and several novel normalized scores, we\ncan obtain a relative improvement of 4.67% for the actual term-weighted\nvalue (ATWV) metric on the OpenKWS15 development test dataset. In addition,\na LSTM-CTC based keyword verification method has been proposed to supply\nextra acoustic information. After the information is added, a further\nimprovement of 7.05% over the baseline can be observed.\n"
   ],
   "doi": "10.21437/Interspeech.2016-606"
  },
  "proenca16_interspeech": {
   "authors": [
    [
     "Jorge",
     "Proença"
    ],
    [
     "Fernando",
     "Perdigão"
    ]
   ],
   "title": "Segmented Dynamic Time Warping for Spoken Query-by-Example Search",
   "original": "1276",
   "page_count": 5,
   "order": 156,
   "p1": "750",
   "pn": "754",
   "abstract": [
    "This paper describes a low-resource approach to a Query-by-Example\ntask, where spoken queries must be matched in a large dataset of spoken\ndocuments sometimes in complex or non-exact ways. Our approach tackles\nthese complex match cases by using Dynamic Time Warping to obtain alternative\npaths that account for reordering of words, small extra content and\nsmall lexical variations. We also report certain advances on calibration\nand fusion of sub-systems that improve overall results, such as manipulating\nthe score distribution per query and using an average posteriorgram\ndistance matrix as an extra sub-system. Results are evaluated on the\nMediaEval task of Query-by-Example Search on Speech (QUESST). For this\ntask, the language of the audio being searched is almost irrelevant,\napproaching the use case scenario to a language of very low resources.\nFor that, we use as features the posterior probabilities obtained from\nfive phonetic recognizers trained with five different languages.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1276"
  },
  "lee16b_interspeech": {
   "authors": [
    [
     "Shi-wook",
     "Lee"
    ],
    [
     "Kazuyo",
     "Tanaka"
    ],
    [
     "Yoshiaki",
     "Itoh"
    ]
   ],
   "title": "Generating Complementary Acoustic Model Spaces in DNN-Based Sequence-to-Frame DTW Scheme for Out-of-Vocabulary Spoken Term Detection",
   "original": "0838",
   "page_count": 5,
   "order": 157,
   "p1": "755",
   "pn": "759",
   "abstract": [
    "This paper proposes a sequence-to-frame dynamic time warping (DTW)\ncombination approach to improve out-of-vocabulary (OOV) spoken term\ndetection (STD) performance gain. The goal of this paper is twofold:\nfirst, we propose a method that directly adopts the posterior probability\nof deep neural network (DNN) and Gaussian mixture model (GMM) as the\nsimilarity distance for sequence-to-frame DTW. Second, we investigate\ncombinations of diverse schemes in GMM and DNN, with different subword\nunits and acoustic models, estimate the complementarity in terms of\nperformance gap and correlation of the combined systems, and discuss\nthe performance gain of the combined systems. The results of evaluations\nconducted of the combined systems on an out-of-vocabulary spoken term\ndetection task show that the performance gain of DNN-based systems\nis better than that of GMM-based systems. However, the performance\ngain obtained by combining DNN- and GMM-based systems is insignificant,\neven though DNN and GMM are highly heterogeneous. This is because the\nperformance gap between DNN-based systems and GMM-based systems is\nquite large. On the other hand, score fusion of two heterogeneous subword\nunits, triphone and sub-phonetic segments, in DNN-based systems provides\nsignificantly improved performance.\n"
   ],
   "doi": "10.21437/Interspeech.2016-838"
  },
  "panchapagesan16_interspeech": {
   "authors": [
    [
     "Sankaran",
     "Panchapagesan"
    ],
    [
     "Ming",
     "Sun"
    ],
    [
     "Aparna",
     "Khare"
    ],
    [
     "Spyros",
     "Matsoukas"
    ],
    [
     "Arindam",
     "Mandal"
    ],
    [
     "Björn",
     "Hoffmeister"
    ],
    [
     "Shiv",
     "Vitaladevuni"
    ]
   ],
   "title": "Multi-Task Learning and Weighted Cross-Entropy for DNN-Based Keyword Spotting",
   "original": "1485",
   "page_count": 5,
   "order": 158,
   "p1": "760",
   "pn": "764",
   "abstract": [
    "We propose improved Deep Neural Network (DNN) training loss functions\nfor more accurate single keyword spotting on resource-constrained embedded\ndevices. The loss function modifications consist of a combination of\nmulti-task training and weighted cross entropy. In the multi-task architecture,\nthe keyword DNN acoustic model is trained with two tasks in parallel\n&#8212; the main task of predicting the keyword-specific phone states,\nand an auxiliary task of predicting LVCSR senones. We show that multi-task\nlearning leads to comparable accuracy over a previously proposed transfer\nlearning approach where the keyword DNN training is initialized by\nan LVCSR DNN of the same input and hidden layer sizes. The combination\nof LVCSR-initialization and Multi-task training gives improved keyword\ndetection accuracy compared to either technique alone. We also propose\nmodifying the loss function to give a higher weight on input frames\ncorresponding to keyword phone targets, with a motivation to balance\nthe keyword and background training data. We show that weighted cross-entropy\nresults in additional accuracy improvements. Finally, we show that\nthe combination of 3 techniques &#8212; LVCSR-initialization, multi-task\ntraining and weighted cross-entropy gives the best results, with significantly\nlower False Alarm Rate than the LVCSR-initialization technique alone,\nacross a wide range of Miss Rates.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1485"
  },
  "chung16_interspeech": {
   "authors": [
    [
     "Yu-An",
     "Chung"
    ],
    [
     "Chao-Chung",
     "Wu"
    ],
    [
     "Chia-Hao",
     "Shen"
    ],
    [
     "Hung-Yi",
     "Lee"
    ],
    [
     "Lin-Shan",
     "Lee"
    ]
   ],
   "title": "Audio Word2Vec: Unsupervised Learning of Audio Segment Representations Using Sequence-to-Sequence Autoencoder",
   "original": "0082",
   "page_count": 5,
   "order": 159,
   "p1": "765",
   "pn": "769",
   "abstract": [
    "The vector representations of fixed dimensionality for words (in text)\noffered by Word2Vec have been shown to be very useful in many application\nscenarios, in particular due to the semantic information they carry.\nThis paper proposes a parallel version, the Audio Word2Vec. It offers\nthe vector representations of fixed dimensionality for variable-length\naudio segments. These vector representations are shown to describe\nthe sequential phonetic structures of the audio segments to a good\ndegree, with very attractive real world applications such as query-by-example\nSpoken Term Detection (STD). In this STD application, the proposed\napproach significantly outperformed the conventional Dynamic Time Warping\n(DTW) based approaches at significantly lower computation requirements.\nWe propose unsupervised learning of Audio Word2Vec from audio data\nwithout human annotation using Sequence-to-sequence Autoencoder (SA).\nSA consists of two RNNs equipped with Long Short-Term Memory (LSTM)\nunits: the first RNN (encoder) maps the input audio sequence into a\nvector representation of fixed dimensionality, and the second RNN (decoder)\nmaps the representation back to the input audio sequence. The two RNNs\nare jointly trained by minimizing the reconstruction error. Denoising\nSequence-to-sequence Autoencoder (DSA) is further proposed offering\nmore robust learning.\n"
   ],
   "doi": "10.21437/Interspeech.2016-82"
  },
  "meng16_interspeech": {
   "authors": [
    [
     "Zhong",
     "Meng"
    ],
    [
     "Biing-Hwang",
     "Juang"
    ]
   ],
   "title": "Non-Uniform Boosted MCE Training of Deep Neural Networks for Keyword Spotting",
   "original": "0642",
   "page_count": 5,
   "order": 160,
   "p1": "770",
   "pn": "774",
   "abstract": [
    "Keyword spotting can be formulated as a non-uniform error automatic\nspeech recognition (ASR) problem. It has been demonstrated [1] that\nthis new formulation with the non-uniform MCE training technique can\nlead to improved system performance in keyword spotting applications.\nIn this paper, we demonstrate that deep neural networks (DNNs) can\nbe successfully trained on the non-uniform minimum classification error\n(MCE) criterion which weighs the errors on keywords much more significantly\nthan those on non-keywords in an ASR task. The integration with a DNN-HMM\nsystem enables modeling of multi-frame distributions, which conventional\nsystems find difficult to accomplish. To further improve the performance,\nmore confusable data is generated by boosting the likelihood of the\nsentences that have more errors. The keyword spotting system is implemented\nwithin a weighted finite state transducer (WFST) framework and the\nDNN is optimized using standard backpropagation and stochastic gradient\ndecent. We evaluate the performance of the proposed framework on a\nlarge vocabulary spontaneous conversational telephone speech dataset\n(Switchboard-1 Release 2). The proposed approach achieves an absolute\nfigure of merit improvement of 3.65% over the baseline system.\n"
   ],
   "doi": "10.21437/Interspeech.2016-642"
  },
  "gorin16_interspeech": {
   "authors": [
    [
     "Arseniy",
     "Gorin"
    ],
    [
     "Rasa",
     "Lileikytė"
    ],
    [
     "Guangpu",
     "Huang"
    ],
    [
     "Lori",
     "Lamel"
    ],
    [
     "Jean-Luc",
     "Gauvain"
    ],
    [
     "Antoine",
     "Laurent"
    ]
   ],
   "title": "Language Model Data Augmentation for Keyword Spotting in Low-Resourced Training Conditions",
   "original": "1200",
   "page_count": 5,
   "order": 161,
   "p1": "775",
   "pn": "779",
   "abstract": [
    "This research extends our earlier work on using machine translation\n(MT) and word-based recurrent neural networks to augment language model\ntraining data for keyword search in conversational Cantonese speech.\nMT-based data augmentation is applied to two language pairs: English-Lithuanian\nand English-Amharic. Using filtered N-best MT hypotheses for language\nmodeling is found to perform better than just using the 1-best translation.\nTarget language texts collected from the Web and filtered to select\nconversational-like data are used in several manners. In addition to\nusing Web data for training the language model of the speech recognizer,\nwe further investigate using this data to improve the language model\nand phrase table of the MT system to get better translations of the\nEnglish data. Finally, generating text data with a character-based\nrecurrent neural network is investigated. This approach allows new\nword forms to be produced, providing a way to reduce the out-of-vocabulary\nrate and thereby improve keyword spotting performance. We study how\nthese different methods of language model data augmentation impact\nspeech-to-text and keyword spotting performance for the Lithuanian\nand Amharic languages. The best results are obtained by combining all\nof the explored methods.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1200"
  },
  "verwimp16_interspeech": {
   "authors": [
    [
     "Lyan",
     "Verwimp"
    ],
    [
     "Brecht",
     "Desplanques"
    ],
    [
     "Kris",
     "Demuynck"
    ],
    [
     "Joris",
     "Pelemans"
    ],
    [
     "Marieke",
     "Lycke"
    ],
    [
     "Patrick",
     "Wambacq"
    ]
   ],
   "title": "STON: Efficient Subtitling in Dutch Using State-of-the-Art Tools",
   "original": "2006",
   "page_count": 2,
   "order": 162,
   "p1": "780",
   "pn": "781",
   "abstract": [
    "We present a modular video subtitling platform that integrates speech/non-speech\nsegmentation, speaker diarisation, language identification, Dutch speech\nrecognition with state-of-the-art acoustic models and language models\noptimised for efficient subtitling, appropriate pre- and postprocessing\nof the data and alignment of the final result with the video fragment.\nMoreover, the system is able to learn from subtitles that are newly\ncreated. The platform is developed for the Flemish national broadcaster\nVRT in the context of the project STON, and enables the easy upload\nof a new fragment and inspection of both the timings and results of\neach step in the subtitling process.\n"
   ]
  },
  "stanislav16_interspeech": {
   "authors": [
    [
     "Petr",
     "Stanislav"
    ],
    [
     "Luboš",
     "Šmídl"
    ],
    [
     "Jan",
     "Švec"
    ]
   ],
   "title": "An Automatic Training Tool for Air Traffic Control Training",
   "original": "2007",
   "page_count": 2,
   "order": 163,
   "p1": "782",
   "pn": "783",
   "abstract": [
    "In this paper we presents an automatic training tool (ATT) for air\ntraffic control officer (ATCO) trainees. It was developed using our\ncloud-based speech recognition and text-to-speech systems and allows\ndynamically generate the content. Our system significantly expands\nthe available training materials, allowing ATCOs practice the basics\nof communication and phraseology. Furthermore, the automatic training\ntool is designed generally to be used for teaching in various areas,\nfrom specialized skills to a simple general knowledge.\n"
   ]
  },
  "karhila16_interspeech": {
   "authors": [
    [
     "Reima",
     "Karhila"
    ],
    [
     "Aku",
     "Rouhe"
    ],
    [
     "Peter",
     "Smit"
    ],
    [
     "André",
     "Mansikkaniemi"
    ],
    [
     "Heini",
     "Kallio"
    ],
    [
     "Erik",
     "Lindroos"
    ],
    [
     "Raili",
     "Hildén"
    ],
    [
     "Martti",
     "Vainio"
    ],
    [
     "Mikko",
     "Kurimo"
    ]
   ],
   "title": "Digitala: An Augmented Test and Review Process Prototype for High-Stakes Spoken Foreign Language Examination",
   "original": "2008",
   "page_count": 2,
   "order": 164,
   "p1": "784",
   "pn": "785",
   "abstract": [
    "This paper introduces the first prototype for a computerised examination\nprocedure of spoken foreign languages in Finland, intended for national\nscale upper secondary school final examinations. Speech technology\nand profiling of reviewers are used to minimise the otherwise massive\nreviewing effort.\n"
   ]
  },
  "damnati16_interspeech": {
   "authors": [
    [
     "Géraldine",
     "Damnati"
    ],
    [
     "Delphine",
     "Charlet"
    ],
    [
     "Marc",
     "Denjean"
    ]
   ],
   "title": "Exploring Collections of Multimedia Archives Through Innovative Interfaces in the Context of Digital Humanities",
   "original": "2009",
   "page_count": 2,
   "order": 165,
   "p1": "786",
   "pn": "787",
   "abstract": [
    "STIK is a platform that gathers Speech, Texts and Images of Knowledge.\nIt allows browsing and navigating through collections of multimedia,\nfacilitating access to archives in the domain of Knowledge resources.\nSTIK includes a back-end with a specific automatic metadata extraction\npipeline, a front-end with innovative interfaces for navigating within\na document and a specific implementation of a search engine with dedicated\nkey-word search functionality. It gathers multimedia contents from\nCanal-U, a French institution that exploits audiovisual archives produced\nby Higher Education and Research, with various formats and various\nacademic disciplines. STIK is a contribution to the emerging domain\nof Digital Humanities.\n"
   ]
  },
  "yuan16_interspeech": {
   "authors": [
    [
     "Yougen",
     "Yuan"
    ],
    [
     "Cheung-Chi",
     "Leung"
    ],
    [
     "Lei",
     "Xie"
    ],
    [
     "Bin",
     "Ma"
    ],
    [
     "Haizhou",
     "Li"
    ]
   ],
   "title": "Learning Neural Network Representations Using Cross-Lingual Bottleneck Features with Word-Pair Information",
   "original": "0317",
   "page_count": 5,
   "order": 166,
   "p1": "788",
   "pn": "792",
   "abstract": [
    "We assume that only word pairs identified by human are available in\na low-resource target language. The word pairs are parameterized by\na bottleneck feature (BNF) extractor that is trained using transcribed\ndata in a high-resource language. The cross-lingual BNFs of the word\npairs are used for training another neural network to generate a new\nfeature representation in the target language. Pairwise learning of\nframe-level and word-level feature representations are investigated.\nOur proposed feature representations were evaluated in a word discrimination\ntask on the Switchboard telephone speech corpus. Our learned features\ncould bring 27.5% relative improvement over the previously best reported\nresult on the task.\n"
   ],
   "doi": "10.21437/Interspeech.2016-317"
  },
  "liu16d_interspeech": {
   "authors": [
    [
     "Yuzong",
     "Liu"
    ],
    [
     "Katrin",
     "Kirchhoff"
    ]
   ],
   "title": "Novel Front-End Features Based on Neural Graph Embeddings for DNN-HMM and LSTM-CTC Acoustic Modeling",
   "original": "0542",
   "page_count": 5,
   "order": 167,
   "p1": "793",
   "pn": "797",
   "abstract": [
    "In this paper we investigate neural graph embeddings as front-end features\nfor various deep neural network (DNN) architectures for speech recognition.\nNeural graph embedding features are produced by an autoencoder that\nmaps graph structures defined over speech samples to a continuous vector\nspace. The resulting feature representation is then used to augment\nthe standard acoustic features at the input level of a DNN classifier.\nWe compare two different neural graph embedding methods, one based\non a local neighborhood graph encoding, and another based on a global\nsimilarity graph encoding. They are evaluated in DNN-HMM-based and\nLSTM-CTC-based ASR systems on a 110-hour Switchboard conversational\nspeech recognition task. Significant improvements in word error rates\nare achieved by both methods in the DNN-HMM system, and by global graph\nembeddings in the LSTM-CTC system.\n"
   ],
   "doi": "10.21437/Interspeech.2016-542"
  },
  "abraham16_interspeech": {
   "authors": [
    [
     "Basil",
     "Abraham"
    ],
    [
     "S.",
     "Umesh"
    ],
    [
     "Neethu Mariam",
     "Joy"
    ]
   ],
   "title": "Articulatory Feature Extraction Using CTC to Build Articulatory Classifiers Without Forced Frame Alignments for Speech Recognition",
   "original": "0925",
   "page_count": 5,
   "order": 168,
   "p1": "798",
   "pn": "802",
   "abstract": [
    "Articulatory features provide robustness to speaker and environment\nvariability by incorporating speech production knowledge. Pseudo articulatory\nfeatures are a way of extracting articulatory features using articulatory\nclassifiers trained from speech data. One of the major problems faced\nin building articulatory classifiers is the requirement of speech data\naligned in terms of articulatory feature values at frame level. Manually\naligning data at frame level is a tedious task and alignments obtained\nfrom the phone alignments using phone-to-articulatory feature mapping\nare prone to errors. In this paper, a technique using connectionist\ntemporal classification (CTC) criterion to train an articulatory classifier\nusing bidirectional long short-term memory (BLSTM) recurrent neural\nnetwork (RNN) is proposed. The CTC criterion eliminates the need for\nforced frame level alignments. Articulatory classifiers were also built\nusing different neural network architectures like deep neural networks\n(DNN), convolutional neural network (CNN) and BLSTM with frame level\nalignments and were compared to the proposed approach of using CTC.\nAmong the different architectures, articulatory features extracted\nusing articulatory classifiers built with BLSTM gave better recognition\nperformance. Further, the proposed approach of BLSTM with CTC gave\nthe best overall performance on both SVitchboard (6 hours) and Switchboard\n33 hours data set.\n"
   ],
   "doi": "10.21437/Interspeech.2016-925"
  },
  "nagamine16_interspeech": {
   "authors": [
    [
     "Tasha",
     "Nagamine"
    ],
    [
     "Michael L.",
     "Seltzer"
    ],
    [
     "Nima",
     "Mesgarani"
    ]
   ],
   "title": "On the Role of Nonlinear Transformations in Deep Neural Network Acoustic Models",
   "original": "1406",
   "page_count": 5,
   "order": 169,
   "p1": "803",
   "pn": "807",
   "abstract": [
    "Deep neural networks (DNNs) are widely utilized for acoustic modeling\nin speech recognition systems. Through training, DNNs used for phoneme\nrecognition nonlinearly transform the time-frequency representation\nof a speech signal into a sequence of invariant phonemic categories.\nHowever, little is known about how this nonlinear mapping is performed\nand what its implications are for the classification of individual\nphones and phonemic categories. In this paper, we analyze a sigmoid\nDNN trained for a phoneme recognition task and characterized several\naspects of the nonlinear transformations that occur in hidden layers.\nWe show that the function learned by deeper hidden layers becomes increasingly\nnonlinear, and that network selectively warps the feature space so\nas to increase the discriminability of acoustically similar phones,\naiding in their classification. We also demonstrate that the nonlinear\ntransformation of the feature space in deeper layers is more dedicated\nto the phone instances that are more difficult to discriminate, while\nthe more separable phones are dealt with in the superficial layers\nof the network. This study describes how successive nonlinear transformations\nare applied to the feature space non-uniformly when a deep neural network\nmodel learns categorical boundaries, which may partly explain their\nsuperior performance in pattern classification applications.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1406"
  },
  "variani16_interspeech": {
   "authors": [
    [
     "Ehsan",
     "Variani"
    ],
    [
     "Tara N.",
     "Sainath"
    ],
    [
     "Izhak",
     "Shafran"
    ],
    [
     "Michiel",
     "Bacchiani"
    ]
   ],
   "title": "Complex Linear Projection (CLP): A Discriminative Approach to Joint Feature Extraction and Acoustic Modeling",
   "original": "1459",
   "page_count": 5,
   "order": 170,
   "p1": "808",
   "pn": "812",
   "abstract": [
    "State-of-the-art automatic speech recognition (ASR) systems typically\nrely on pre-processed features. This paper studies the time-frequency\nduality in ASR feature extraction methods and proposes extending the\nstandard acoustic model with a complex-valued linear projection layer\nto learn and optimize features that minimize standard cost functions\nsuch as cross-entropy. The proposed Complex Linear Projection (CLP)\nfeatures achieve superior performance compared to pre-processed Log\nMel features.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1459"
  },
  "sainath16_interspeech": {
   "authors": [
    [
     "Tara N.",
     "Sainath"
    ],
    [
     "Bo",
     "Li"
    ]
   ],
   "title": "Modeling Time-Frequency Patterns with LSTM vs. Convolutional Architectures for LVCSR Tasks",
   "original": "0084",
   "page_count": 5,
   "order": 171,
   "p1": "813",
   "pn": "817",
   "abstract": [
    "Various neural network architectures have been proposed in the literature\nto model 2D correlations in the input signal, including convolutional\nlayers, frequency LSTMs and 2D LSTMs such as time-frequency LSTMs,\ngrid LSTMs and ReNet LSTMs. It has been argued that frequency LSTMs\ncan model translational variations similar to CNNs, and 2D LSTMs can\nmodel even more variations [1], but no proper comparison has been done\nfor speech tasks. While convolutional layers have been a popular technique\nin speech tasks, this paper compares convolutional and LSTM architectures\nto model time-frequency patterns as the first layer in an LDNN [2]\narchitecture. This comparison is particularly interesting when the\nconvolutional layer degrades performance, such as in noisy conditions\nor when the learned filterbank is not constant-Q [3]. We find that\ngrid-LDNNs offer the best performance of all techniques, and provide\nbetween a 1&#8211;4% relative improvement over an LDNN and CLDNN on\n3 different large vocabulary Voice Search tasks.\n"
   ],
   "doi": "10.21437/Interspeech.2016-84"
  },
  "mclaren16_interspeech": {
   "authors": [
    [
     "Mitchell",
     "McLaren"
    ],
    [
     "Luciana",
     "Ferrer"
    ],
    [
     "Diego",
     "Castan"
    ],
    [
     "Aaron",
     "Lawson"
    ]
   ],
   "title": "The Speakers in the Wild (SITW) Speaker Recognition Database",
   "original": "1129",
   "page_count": 5,
   "order": 172,
   "p1": "818",
   "pn": "822",
   "abstract": [
    "The Speakers in the Wild (SITW) speaker recognition database contains\nhand-annotated speech samples from open-source media for the purpose\nof benchmarking text-independent speaker recognition technology on\nsingle and multi-speaker audio acquired across unconstrained or &#8220;wild&#8221;\nconditions. The database consists of recordings of 299 speakers, with\nan average of eight different sessions per person. Unlike existing\ndatabases for speaker recognition, this data was not collected under\ncontrolled conditions and thus contains real noise, reverberation,\nintra-speaker variability and compression artifacts. These factors\nare often convolved in the real world, as the SITW data shows, and\nthey make SITW a challenging database for single- and multi-speaker\nrecognition.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1129"
  },
  "mclaren16b_interspeech": {
   "authors": [
    [
     "Mitchell",
     "McLaren"
    ],
    [
     "Luciana",
     "Ferrer"
    ],
    [
     "Diego",
     "Castan"
    ],
    [
     "Aaron",
     "Lawson"
    ]
   ],
   "title": "The 2016 Speakers in the Wild Speaker Recognition Evaluation",
   "original": "1137",
   "page_count": 5,
   "order": 173,
   "p1": "823",
   "pn": "827",
   "abstract": [
    "The newly collected Speakers in the Wild (SITW) database was central\nto a text-independent speaker recognition challenge held as part of\na special session at Interspeech 2016. The SITW database is composed\nof audio recordings from 299 speakers collected from open source media,\nwith an average of 8 sessions per speaker. The recordings contain unconstrained\nor &#8220;wild&#8221; acoustic conditions, rarely found in large speaker\nrecognition datasets, and multi-speaker recordings for both speaker\nenrollment and verification. This article provides details of the SITW\nspeaker recognition challenge and analysis of evaluation results. There\nwere 25 international teams involved in the challenge of which 11 teams\nparticipated in an evaluation track. Teams were tasked with applying\nexisting and novel speaker recognition algorithms to the challenges\nassociated with the real world conditions of SITW. We provide an analysis\nof some of the top performing systems submitted during the evaluation\nand provide future research directions.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1137"
  },
  "novotny16_interspeech": {
   "authors": [
    [
     "Ondřej",
     "Novotný"
    ],
    [
     "Pavel",
     "Matějka"
    ],
    [
     "Oldřich",
     "Plchot"
    ],
    [
     "Ondřej",
     "Glembek"
    ],
    [
     "Lukáš",
     "Burget"
    ],
    [
     "Jan",
     "Černocký"
    ]
   ],
   "title": "Analysis of Speaker Recognition Systems in Realistic Scenarios of the SITW 2016 Challenge",
   "original": "0981",
   "page_count": 5,
   "order": 174,
   "p1": "828",
   "pn": "832",
   "abstract": [
    "In this paper, we summarize our efforts for the Speakers In The Wild\n(SITW) challenge, and we present our findings with this new dataset\nfor speaker recognition. Apart from the standard comparison of different\nSRE systems, we analyze the use of diarization for dealing with audio\nsegments containing multiple speakers, as in part of the newly introduced\nenrollment and test protocols, diarization is a necessary system component.\nOur state-of-the-art systems used in this work utilize both cepstral\nand DNN-based bottleneck features and are based on i-vectors followed\nby Probabilistic Linear Discriminant Analysis (PLDA) classifier and\nlogistic regression calibration/fusion. We present both narrow-band\n(8 kHz) and wide-band (16 kHz) systems together with their fusions.\n"
   ],
   "doi": "10.21437/Interspeech.2016-981"
  },
  "kudashev16_interspeech": {
   "authors": [
    [
     "Oleg",
     "Kudashev"
    ],
    [
     "Sergey",
     "Novoselov"
    ],
    [
     "Konstantin",
     "Simonchik"
    ],
    [
     "Alexandr",
     "Kozlov"
    ]
   ],
   "title": "A Speaker Recognition System for the SITW Challenge",
   "original": "1197",
   "page_count": 5,
   "order": 175,
   "p1": "833",
   "pn": "837",
   "abstract": [
    "This paper presents an ITMO university system submitted to the Speakers\nin the Wild (SITW) Speaker Recognition Challenge. During evaluation\ntrack of the SITW challenge we explored conventional universal background\nmodel (UBM) Gaussian mixture model (GMM) i-vector systems and recently\ndeveloped DNN-posteriors based i-vector systems. The systems were investigated\nunder the real-world media channel conditions represented in the challenge.\nThis paper discusses practical issues of the robust i-vector systems\ntraining and performs investigation of denoising autoencoder (DAE)\nbased back-end when applied to &#8220;in the wild&#8221; conditions.\nOur speak-er diarization approach for &#8220;multi-speaker in the file&#8221;\nconditions is also briefly presented in the paper. Experiments per-formed\non the evaluation dataset demonstrate that DNN- based i-vector systems\nare superior to the UBM-GMM based sys-tems and applying DAE-based back-end\nhelps to improve system performance.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1197"
  },
  "ghaemmaghami16_interspeech": {
   "authors": [
    [
     "H.",
     "Ghaemmaghami"
    ],
    [
     "M.H.",
     "Rahman"
    ],
    [
     "Ivan",
     "Himawan"
    ],
    [
     "David",
     "Dean"
    ],
    [
     "Ahilan",
     "Kanagasundaram"
    ],
    [
     "Sridha",
     "Sridharan"
    ],
    [
     "Clinton",
     "Fookes"
    ]
   ],
   "title": "Speakers In The Wild (SITW): The QUT Speaker Recognition System",
   "original": "0945",
   "page_count": 5,
   "order": 176,
   "p1": "838",
   "pn": "842",
   "abstract": [
    "This paper presents the QUT speaker recognition system, as a competing\nsystem in the  Speakers In The Wild (SITW) speaker recognition challenge.\nOur proposed system achieved an overall ranking of second place, in\nthe main  core-core condition evaluations of the SITW challenge. This\nsystem uses an i-vector/PLDA approach, with domain adaptation and a\ndeep neural network (DNN) trained to provide feature statistics. The\nstatistics are accumulated by using class posteriors from the DNN,\nin place of GMM component posteriors in a typical GMM-UBM i-vector/PLDA\nsystem. Once the statistics have been collected, the i-vector computation\nis carried out as in a GMM-UBM based system. We apply domain adaptation\nto the extracted i-vectors to ensure robustness against dataset variability,\nPLDA modelling is used to capture speaker and session variability in\nthe i-vector space, and the processed i-vectors are compared using\nthe batch likelihood ratio. The final scores are calibrated to obtain\nthe calibrated likelihood scores, which are then used to carry out\nspeaker recognition and evaluate the performance of the system. Finally,\nwe explore the practical application of our system to the  core-multi\ncondition recordings of the SITW data and propose a technique for speaker\nrecognition in recordings with multiple speakers. \n"
   ],
   "doi": "10.21437/Interspeech.2016-945"
  },
  "khosravani16_interspeech": {
   "authors": [
    [
     "Abbas",
     "Khosravani"
    ],
    [
     "Mohammad Mehdi",
     "Homayounpour"
    ]
   ],
   "title": "AUT System for SITW Speaker Recognition Challenge",
   "original": "1378",
   "page_count": 5,
   "order": 177,
   "p1": "843",
   "pn": "847",
   "abstract": [
    "This document intends to present AUT speaker recognition system submitted\nto SITW (Speakers in the Wild) speaker recognition challenge. This\nchallenge aims to provide real world data across a wide range of acoustic\nand environmental conditions in the context of automatic speaker recognition\nso as to facilitate the development of new algorithms. The presented\nsystem is based on the state-of-the-art i-vector/PLDA and source normalization\ntechniques. The system has been developed on publically available databases\nand evaluated on the data provided by SITW challenge. Taking advantage\nof the challenge development data, our experiments indicate that source\nnormalization can help speaker recognition system to better adapt to\nthe evaluation condition. Post evaluation analysis is conducted on\nthe conditions of SITW database.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1378"
  },
  "kheder16_interspeech": {
   "authors": [
    [
     "Waad Ben",
     "Kheder"
    ],
    [
     "Moez",
     "Ajili"
    ],
    [
     "Pierre-Michel",
     "Bousquet"
    ],
    [
     "Driss",
     "Matrouf"
    ],
    [
     "Jean-François",
     "Bonastre"
    ]
   ],
   "title": "LIA System for the SITW Speaker Recognition Challenge",
   "original": "1310",
   "page_count": 5,
   "order": 178,
   "p1": "848",
   "pn": "852",
   "abstract": [
    "This paper presents the speaker verification systems developed in the\nLIA lab at the University of Avignon for the SITW (Speakers In The\nWild) challenge. We present the algorithms used to deal with additive\nnoise, short utterances and propose an improved scoring scheme using\na discriminative classifier and integrating the homogeneity of the\ntwo compared recordings. Due to the heterogeneity of this database\n(presence of background noise, reverberation, Lombard effect, etc.),\nit is hard to analyze the contribution of individual techniques used\nto deal with each problem. For this reason, a subset of the trials\nwill be studied for each algorithm in order to emphasize its contribution.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1310"
  },
  "liu16e_interspeech": {
   "authors": [
    [
     "Yi",
     "Liu"
    ],
    [
     "Yao",
     "Tian"
    ],
    [
     "Liang",
     "He"
    ],
    [
     "Jia",
     "Liu"
    ]
   ],
   "title": "Investigating Various Diarization Algorithms for Speaker in the Wild (SITW) Speaker Recognition Challenge",
   "original": "1144",
   "page_count": 5,
   "order": 179,
   "p1": "853",
   "pn": "857",
   "abstract": [
    "Collecting training data for real-world text-independent speaker recognition\nis challenging. In practice, utterances for a specific speaker are\noften mixed with many other acoustic signals. To guarantee the recognition\nperformance, the segments spoken by target speakers should be precisely\npicked out. An automatic detection could be developed to reduce the\ncost of expensive human hand-made annotations. One way to achieve this\ngoal is by using speaker diarization as a pre-processing step in the\nspeaker enrollment phase. To this end, three speaker diarization algorithms\nbased on Bayesian information criterion (BIC), agglomerative information\nbottleneck (aIB) and i-vector are investigated in this paper. The corresponding\nimpacts on the results of speaker recognition system are also studied.\nExperiments conducted on Speaker in the Wild (SITW) Speaker Recognition\nChallenge (SRC) 2016 showed that the utilization of a proper speaker\ndiarization improves the overall performance. Some more efforts are\nmade to combine these methods together as well.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1144"
  },
  "scharenborg16_interspeech": {
   "authors": [
    [
     "Odette",
     "Scharenborg"
    ],
    [
     "Juul",
     "Coumans"
    ],
    [
     "Sofoklis",
     "Kakouros"
    ],
    [
     "Roeland van",
     "Hout"
    ]
   ],
   "title": "Does the Importance of Word-Initial and Word-Final Information Differ in Native versus Non-Native Spoken-Word Recognition?",
   "original": "1095",
   "page_count": 5,
   "order": 180,
   "p1": "858",
   "pn": "862",
   "abstract": [
    "This paper investigates whether the importance and use of word-initial\nand word-final information in spoken-word recognition is dependent\non whether one is listening in a native or a non-native language and\non the presence of background noise. Native English and non-native\nDutch and Finnish listeners participated in an English word recognition\nexperiment, where either a word&#8217;s onset or offset was masked\nby speech-shaped noise with different signal-to-noise ratios. The results\nshowed that for all listener groups the masking of word onset information\nwas more detrimental to spoken-word recognition than the masking of\nword offset information. The reliance on word-initial information was\nlarger in harder listening conditions for the English but not so for\nthe Dutch and Finnish listeners. Moreover, no significant differences\nin the use of word-initial and word-final information were found between\nthe two non-native listener groups. Taken together, these results show\nthat the reliance on word-initial information in deteriorating listening\nconditions seems to be dependent on whether one is listening in one&#8217;s\nnative or a non-native language rather than on the listener&#8217;s\nnative language.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1095"
  },
  "scharenborg16b_interspeech": {
   "authors": [
    [
     "Odette",
     "Scharenborg"
    ],
    [
     "Elea",
     "Kolkman"
    ],
    [
     "Sofoklis",
     "Kakouros"
    ],
    [
     "Brechtje",
     "Post"
    ]
   ],
   "title": "The Effect of Sentence Accent on Non-Native Speech Perception in Noise",
   "original": "0019",
   "page_count": 5,
   "order": 181,
   "p1": "863",
   "pn": "867",
   "abstract": [
    "This paper investigates the uptake and use of prosodic information\nsignalling sentence accent during native and non-native speech perception\nin the presence of background noise. A phoneme monitoring experiment\nwas carried out in which English, Dutch, and Finnish listeners were\npresented with target phonemes in semantically unpredictable yet meaningful\nEnglish sentences. Sentences were presented in different levels of\nspeech-shaped noise and, crucially, in two prosodic contexts in which\nthe target-bearing word was either deaccented or accented. Results\nshowed that overall performance was high for both the native and the\nnon-native listeners; however, where native listeners seemed able to\npartially overcome the problems at the acoustic level in degraded listening\nconditions by using prosodic information signalling upcoming sentence\naccent, non-native listeners could not do so to the same extent. These\nresults support the hypothesis that the performance difference between\nnative and non-native listeners in the presence of background noise\nis, at least partially, caused by a reduced exploitation of contextual\ninformation during speech processing by non-native listeners.\n"
   ],
   "doi": "10.21437/Interspeech.2016-19"
  },
  "cooke16_interspeech": {
   "authors": [
    [
     "Martin",
     "Cooke"
    ],
    [
     "Maria Luisa Garcia",
     "Lecumberri"
    ]
   ],
   "title": "The Effects of Modified Speech Styles on Intelligibility for Non-Native Listeners",
   "original": "0041",
   "page_count": 5,
   "order": 182,
   "p1": "868",
   "pn": "872",
   "abstract": [
    "Speech output, including modified and synthetic speech, is used increasingly\nin natural settings where message reception might be affected by noise.\nRecent evaluations have demonstrated the effect of different speech\nstyles on intelligibility for native listeners, but their impact on\nlistening in a second language is less well-understood. The current\nstudy measured the intelligibility of four speech styles in the presence\nof stationary and fluctuating maskers for a non-native listener cohort,\nand compared the results with those of native listeners on the same\ntask. Both groups showed a similar pattern of effects, but the scale\nof intelligibility gains and losses with respect to plain speech was\nsignificantly compressed for the non-native group relative to native\nlisteners. In addition, non-native listeners identified speech from\nthe four styles in the absence of noise, revealing that styles shown\nto be beneficial in noise lost their benefits or were harmful in quiet\nconditions. This result suggests that while enhanced styles lead to\ngains by reducing the effect of masking noise, the same styles distort\nthe acoustic-phonetic integrity of the speech signal. More work is\nneeded to develop speech modification approaches that simultaneously\npreserve speech information and promote unmasking.\n"
   ],
   "doi": "10.21437/Interspeech.2016-41"
  },
  "zhang16c_interspeech": {
   "authors": [
    [
     "Hao",
     "Zhang"
    ],
    [
     "Fei",
     "Chen"
    ],
    [
     "Nan",
     "Yan"
    ],
    [
     "Lan",
     "Wang"
    ],
    [
     "Feng",
     "Shi"
    ],
    [
     "Manwa L.",
     "Ng"
    ]
   ],
   "title": "The Influence of Language Experience on the Categorical Perception of Vowels: Evidence from Mandarin and Korean",
   "original": "0887",
   "page_count": 5,
   "order": 183,
   "p1": "873",
   "pn": "877",
   "abstract": [
    "Previous research on categorical perception of speech sounds has demonstrated\na strong influence of language experience on the categorical perception\nof consonants and lexical tones. In order to explore the influence\nof language experience on vowel perception, the present study investigated\nthe perceptual performance for Mandarin and Korean listeners along\na vowel continuum, which spanned three vowel categories /a/, /&#604;/,\nand /u/. The results showed that both language groups exhibited categorical\nfeatures in vowel perception, with a sharper categorical boundary of\n/&#604;/-/u/ than that of /a/-/&#604;/. Moreover, the differences found\nbetween the two groups revealed that the Korean listeners&#8217; perception\ntended to be more categorical along the /a/-/&#604;/-/u/ vowel continuum\nthan that of the Mandarin listeners. Furthermore, the Mandarin listeners\ntended to label stimuli more often as /a/ and less often as /u/ than\nthe Korean counterparts. These perceptual differences between the Mandarin\nand Korean groups might be attributed to the different acoustic distribution\nin the F1&#215;F2 vowel space of the two different native languages.\n"
   ],
   "doi": "10.21437/Interspeech.2016-887"
  },
  "massaro16_interspeech": {
   "authors": [
    [
     "Dominic W.",
     "Massaro"
    ]
   ],
   "title": "Multiple Influences on Vocabulary Acquisition: Parental Input Dominates",
   "original": "0037",
   "page_count": 5,
   "order": 184,
   "p1": "878",
   "pn": "882",
   "abstract": [
    "How spoken language is acquired has been an active area of inquiry\nin linguistic, psychological, and speech science. New advances in this\ncontroversial field are promising given the recent accumulation of\nlarge databases of children&#8217;s speech understanding and production,\nas well as various properties of words. This paper explores the contribution\nof a variety of potential influences on vocabulary acquisition including\ndifficulty of articulation, iconicity, log parental input frequency,\nlexical category, and imageability. The influence of difficulty of\narticulation, iconicity ratings, and imagery ratings decreased more\nor less linearly with increasing age. Lexical category effects were\nfairly small. Parental input in terms of child directed speech has\nby far the largest influence. Multiple regressions with these variables\ngive a fairly complete account of spoken vocabulary acquisition. The\nincreasing availability of large databases promises progress in this\narea of inquiry.\n"
   ],
   "doi": "10.21437/Interspeech.2016-37"
  },
  "gong16b_interspeech": {
   "authors": [
    [
     "Jian",
     "Gong"
    ],
    [
     "Maria Luisa Garcia",
     "Lecumberri"
    ],
    [
     "Martin",
     "Cooke"
    ]
   ],
   "title": "Can Intensive Exposure to Foreign Language Sounds Affect the Perception of Native Sounds?",
   "original": "0076",
   "page_count": 5,
   "order": 185,
   "p1": "883",
   "pn": "887",
   "abstract": [
    "A possible side-effect of exposure to non-native sounds is a change\nin the way we perceive native sounds. Previous studies have demonstrated\nthat native speakers&#8217; speech production can change as a result\nof learning a new language, but little work has been carried out to\nmeasure the perceptual consequences of exposure. The current study\nexamined how intensive exposure to Spanish intervocalic consonants\naffected Chinese learners with no prior experience of Spanish. Before,\nduring and after a training period, listeners undertook both an adaptive\nnoise task, which measured the noise level at which listeners could\nidentify native language consonants, and an assimilation task, in which\nlisteners assigned Spanish consonants to Chinese consonant categories.\nListeners exhibited a significantly reduced noise tolerance for the\nChinese consonants /l/ and /w/ following exposure to Spanish. These\ntwo consonants also showed the largest reductions in Spanish to Chinese\ncategory assimilations. Taken together, these findings suggest that\nChinese listeners modified their native language categories boundaries\nas a result of exposure to Spanish sounds in order to accommodate them,\nand that as a consequence their identification performance in noise\nreduced. Some differences between the two sounds in the time-course\nof recovery from perceptual adaptation were observed.\n"
   ],
   "doi": "10.21437/Interspeech.2016-76"
  },
  "bassiou16_interspeech": {
   "authors": [
    [
     "Nikoletta",
     "Bassiou"
    ],
    [
     "Andreas",
     "Tsiartas"
    ],
    [
     "Jennifer",
     "Smith"
    ],
    [
     "Harry",
     "Bratt"
    ],
    [
     "Colleen",
     "Richey"
    ],
    [
     "Elizabeth",
     "Shriberg"
    ],
    [
     "Cynthia",
     "D’Angelo"
    ],
    [
     "Nonye",
     "Alozie"
    ]
   ],
   "title": "Privacy-Preserving Speech Analytics for Automatic Assessment of Student Collaboration",
   "original": "1569",
   "page_count": 5,
   "order": 186,
   "p1": "888",
   "pn": "892",
   "abstract": [
    "This work investigates whether nonlexical information from speech can\nautomatically predict the quality of small-group collaborations. Audio\nwas collected from students as they collaborated in groups of three\nto solve math problems. Experts in education annotated 30-second time\nwindows by hand for collaboration quality. Speech activity features\n(computed at the group level) and spectral, temporal and prosodic features\n(extracted at the speaker level) were explored. After the latter were\ntransformed from the speaker level to the group level, features were\nfused. Results using support vector machines and random forests show\nthat feature fusion yields best classification performance. The corresponding\nunweighted average F<SUB>1</SUB> measure on a 4-class prediction task\nranges between 40% and 50%, significantly higher than chance (12%).\nSpeech activity features alone are strong predictors of collaboration\nquality, achieving an F<SUB>1</SUB> measure between 35% and 43%. Speaker-based\nacoustic features alone achieve lower classification performance, but\noffer value in fusion. These findings illustrate that the approach\nunder study offers promise for future monitoring of group dynamics,\nand should be attractive for many collaboration activity settings in\nwhich privacy is desired.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1569"
  },
  "nasir16_interspeech": {
   "authors": [
    [
     "Md.",
     "Nasir"
    ],
    [
     "Brian",
     "Baucom"
    ],
    [
     "Shrikanth S.",
     "Narayanan"
    ],
    [
     "Panayiotis",
     "Georgiou"
    ]
   ],
   "title": "Complexity in Prosody: A Nonlinear Dynamical Systems Approach for Dyadic Conversations; Behavior and Outcomes in Couples Therapy",
   "original": "1367",
   "page_count": 5,
   "order": 187,
   "p1": "893",
   "pn": "897",
   "abstract": [
    "In this paper, we model dyadic human conversational interactions from\na nonlinear dynamical systems perspective. We focus on deriving measures\nof the underlying system complexity using the observed dyadic behavioral\nsignals. Specifically, we analyze different measures of complexity\nin prosody of speech (pitch and energy) during dyadic conversations\nof couples with marital conflict. We evaluate the importance of these\nmeasures as features by correlating them with different behavioral\nattributes of the couple codified in terms of behavioral codes. Furthermore,\nwe investigate the relation between the computed complexity and outcomes\nof couples therapy. The results show that the derived complexity measures\nare more correlated to session level behavioral codes, and to the marital\ntherapy outcomes, compared to traditional speech prosody features.\nIt shows that nonlinear dynamical analysis of speech acoustic features\ncan be a useful tool for behavioral analysis.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1367"
  },
  "tseng16_interspeech": {
   "authors": [
    [
     "Shao-Yen",
     "Tseng"
    ],
    [
     "Sandeep Nallan",
     "Chakravarthula"
    ],
    [
     "Brian",
     "Baucom"
    ],
    [
     "Panayiotis",
     "Georgiou"
    ]
   ],
   "title": "Couples Behavior Modeling and Annotation Using Low-Resource LSTM Language Models",
   "original": "1186",
   "page_count": 5,
   "order": 188,
   "p1": "898",
   "pn": "902",
   "abstract": [
    "Observational studies on couple interactions are often based on manual\nannotations of a set of behavior codes. Such annotations are expensive,\ntime-consuming, and often suffer from low inter-annotator agreement.\nIn previous studies it has been shown that the lexical channels contain\nsufficient information for capturing behavior and predicting the interaction\nlabels, and various automated processes using language models have\nbeen proposed. However, current methods are restricted to a small context\nwindow due to the difficulty of training language models with limited\ndata as well as the lack of frame-level labels. In this paper we investigate\nthe application of recurrent neural networks for capturing behavior\ntrajectories through larger context windows. We solve the issue of\ndata sparsity and improve robustness by introducing out-of-domain knowledge\nthrough pretrained word representations. Finally, we show that our\nsystem can accurately estimate true rating values of couples interactions\nusing a fusion of the frame-level behavior trajectories. The ratings\npredicted by our proposed system achieve inter-annotator agreements\ncomparable to those of trained human annotators.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  Importantly, our system\npromises robust handling of out of domain data, exploitation of longer\ncontext, on-line feedback with continuous labels and easy fusion with\nother modalities.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1186"
  },
  "gallardo16_interspeech": {
   "authors": [
    [
     "Laura Fernández",
     "Gallardo"
    ],
    [
     "Benjamin",
     "Weiss"
    ]
   ],
   "title": "Speech Likability and Personality-Based Social Relations: A Round-Robin Analysis over Communication Channels",
   "original": "0459",
   "page_count": 5,
   "order": 189,
   "p1": "903",
   "pn": "907",
   "abstract": [
    "The Social Relations Model is well-known for analyses of interpersonal\nattraction. As a novelty in this paper, the model is applied to assess\ndifferent effects on likability ratings from speech only. A group of\n30 unacquainted participants is considered in our experiment. Their\nvoices were recorded and transmitted through communication channels,\nand ratings of speech likability and speaker personality were then\ncollected from the same individuals following a round-robin approach.\nThis setup enabled us to detect the influence of participants&#8217;\npersonality and of narrowband and wideband speech on the sources of\nvariance according to the Social Relations Model. An analysis of acoustic\ncorrelates of speech likability has also been conducted, which shows\ndifferences in the relevance of speech features and in the description\nof likability ratings depending on the speech bandwidth.\n"
   ],
   "doi": "10.21437/Interspeech.2016-459"
  },
  "xiao16_interspeech": {
   "authors": [
    [
     "Bo",
     "Xiao"
    ],
    [
     "Doğan",
     "Can"
    ],
    [
     "James",
     "Gibson"
    ],
    [
     "Zac E.",
     "Imel"
    ],
    [
     "David C.",
     "Atkins"
    ],
    [
     "Panayiotis",
     "Georgiou"
    ],
    [
     "Shrikanth S.",
     "Narayanan"
    ]
   ],
   "title": "Behavioral Coding of Therapist Language in Addiction Counseling Using Recurrent Neural Networks",
   "original": "1560",
   "page_count": 5,
   "order": 190,
   "p1": "908",
   "pn": "912",
   "abstract": [
    "Manual annotation of human behaviors with domain specific codes is\na primary method of research and treatment fidelity evaluation in psychotherapy.\nHowever, manual annotation has a prohibitively high cost and does not\nscale to coding large amounts of psychotherapy session data. In this\npaper, we present a case study of modeling therapist language in addiction\ncounseling, and propose an automatic coding approach. The task objective\nis to code therapist utterances with domain specific codes. We employ\nRecurrent Neural Networks (RNNs) to predict these behavioral codes\nbased on session transcripts. Experiments show that RNNs outperform\nthe baseline method using Maximum Entropy models. The model with bi-directional\nGated Recurrent Units and domain specific word embeddings achieved\nthe highest overall accuracy. We also briefly discuss about client\ncode prediction and comparison to previous work.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1560"
  },
  "dang16_interspeech": {
   "authors": [
    [
     "Ting",
     "Dang"
    ],
    [
     "Vidhyasaharan",
     "Sethu"
    ],
    [
     "Eliathamby",
     "Ambikairajah"
    ]
   ],
   "title": "Factor Analysis Based Speaker Normalisation for Continuous Emotion Prediction",
   "original": "0880",
   "page_count": 5,
   "order": 191,
   "p1": "913",
   "pn": "917",
   "abstract": [
    "Speaker variability has been shown to be a significant confounding\nfactor in speech based emotion classification systems and a number\nof speaker normalisation techniques have been proposed. However, speaker\nnormalisation in systems that predict continuous multidimensional descriptions\nof emotion such as arousal and valence has not been explored. This\npaper investigates the effect of speaker variability in such speech\nbased continuous emotion prediction systems and proposes a factor analysis\nbased speaker normalisation technique. The proposed technique operates\ndirectly on the feature space and decomposes it into speaker and emotion\nspecific sub-spaces. The proposed technique is validated on both the\nUSC CreativeIT database and the SEMAINE database and leads to improvements\nof 8.2% and 11.0% (in terms of correlation coefficient) on the two\ndatabases respectively when predicting arousal.\n"
   ],
   "doi": "10.21437/Interspeech.2016-880"
  },
  "ram16_interspeech": {
   "authors": [
    [
     "Dhananjay",
     "Ram"
    ],
    [
     "Afsaneh",
     "Asaei"
    ],
    [
     "Hervé",
     "Bourlard"
    ]
   ],
   "title": "Subspace Detection of DNN Posterior Probabilities via Sparse Representation for Query by Example Spoken Term Detection",
   "original": "1278",
   "page_count": 5,
   "order": 192,
   "p1": "918",
   "pn": "922",
   "abstract": [
    "We cast the query by example spoken term detection (QbE-STD) problem\nas subspace detection where query and background subspaces are modeled\nas union of low-dimensional subspaces. The speech exemplars used for\nsubspace modeling are class-conditional posterior probabilities estimated\nusing deep neural network (DNN). The query and background training\nexemplars are exploited to model the underlying low-dimensional subspaces\nthrough dictionary learning for sparse representation. Given the dictionaries\ncharacterizing the query and background subspaces, QbE-STD is performed\nbased on the ratio of the two corresponding sparse representation reconstruction\nerrors. The proposed subspace detection method can be formulated as\nthe generalized likelihood ratio test for composite hypothesis testing.\nThe experimental evaluation demonstrate that the proposed method is\nable to detect the query given a single example and performs significantly\nbetter than a highly competitive QbE-STD baseline system based on dynamic\ntime warping (DTW) for exemplar matching.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1278"
  },
  "chen16d_interspeech": {
   "authors": [
    [
     "Hongjie",
     "Chen"
    ],
    [
     "Cheung-Chi",
     "Leung"
    ],
    [
     "Lei",
     "Xie"
    ],
    [
     "Bin",
     "Ma"
    ],
    [
     "Haizhou",
     "Li"
    ]
   ],
   "title": "Unsupervised Bottleneck Features for Low-Resource Query-by-Example Spoken Term Detection",
   "original": "0313",
   "page_count": 5,
   "order": 193,
   "p1": "923",
   "pn": "927",
   "abstract": [
    "We propose a framework which ports Dirichlet Gaussian mixture model\n(DPGMM) based labels to deep neural network (DNN). The DNN trained\nusing the unsupervised labels is used to extract a low-dimensional\nunsupervised speech representation, named as unsupervised bottleneck\nfeatures (uBNFs), which capture considerable information for sound\ncluster discrimination. We investigate the performance of uBNF in query-by-example\nspoken term detection (QbE-STD) on the TIMIT English speech corpus.\nOur uBNF performs comparably with the cross-lingual bottleneck features\n(BNFs) extracted from a DNN trained using 171 hours of transcribed\ntelephone speech in another language (Mandarin Chinese). With the score\nfusion of uBNFs and cross-lingual BNFs, we gain about 10% relative\nimprovement in terms of mean average precision (MAP) comparing with\nthe cross-lingual BNFs. We also study the performance of the framework\nwith different input features and different lengths of temporal context.\n"
   ],
   "doi": "10.21437/Interspeech.2016-313"
  },
  "torbati16_interspeech": {
   "authors": [
    [
     "Amir Hossein Harati Nejad",
     "Torbati"
    ],
    [
     "Joseph",
     "Picone"
    ]
   ],
   "title": "A Nonparametric Bayesian Approach for Spoken Term Detection by Example Query",
   "original": "0315",
   "page_count": 5,
   "order": 194,
   "p1": "928",
   "pn": "932",
   "abstract": [
    "State of the art speech recognition systems use data-intensive context-dependent\nphonemes as acoustic units. However, these approaches do not translate\nwell to low resourced languages where large amounts of training data\nis not available. For such languages, automatic discovery of acoustic\nunits is critical. In this paper, we demonstrate the application of\nnonparametric Bayesian models to acoustic unit discovery. We show that\nthe discovered units are correlated with phonemes and therefore are\nlinguistically meaningful.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  We also present a\nspoken term detection (STD) by example query algorithm based on these\nautomatically learned units. We show that our proposed system produces\na P@N of 61.2% and an EER of 13.95% on the TIMIT dataset. The improvement\nin the EER is 5% while P@N is only slightly lower than the best reported\nsystem in the literature.\n"
   ],
   "doi": "10.21437/Interspeech.2016-315"
  },
  "pham16_interspeech": {
   "authors": [
    [
     "Van Tung",
     "Pham"
    ],
    [
     "Haihua",
     "Xu"
    ],
    [
     "Xiong",
     "Xiao"
    ],
    [
     "Nancy F.",
     "Chen"
    ],
    [
     "Eng Siong",
     "Chng"
    ],
    [
     "Haizhou",
     "Li"
    ]
   ],
   "title": "Rescoring Hypothesized Detections of Out-of-Vocabulary Keywords Using Subword Samples",
   "original": "0646",
   "page_count": 5,
   "order": 195,
   "p1": "933",
   "pn": "937",
   "abstract": [
    "Rescoring hypothesized detections, using keyword&#8217;s audio samples\nextracted from training data, is an effective way to improve the performance\nof a Keyword Search (KWS) system. Unfortunately such rescoring framework\ncannot be applied directly to Out-of-Vocabulary (OOV) keywords since\nthere is no sample in the training data. To address this limitation,\nwe propose two techniques for OOV keywords in this work. The first\ntechnique generates samples for an OOV keyword by concatenating samples\nof its constituent subwords. The second technique splits hypothesized\ndetections into segments, then estimates the acoustic similarities\nbetween detections and subword&#8217;s samples according to the similarities\nbetween segments and these samples. The similarity scores from these\ntwo techniques are used to rescore and re-rank the list of detections\nreturned by the automatic speech recognition (ASR) systems. The experiments\nshow that incorporating the proposed similarity scores results in a\nbetter separation between the correct and false alarm detections than\nusing the ASR scores alone. Furthermore, experimental results on the\nNIST OpenKWS15 Evaluation show that rescoring with the proposed similarity\nscores significantly outperforms the raw ASR scores, and other methods\nthat do not use the similarity scores, in both Maximum Term Weighted\nValue (MTWV) and Mean Average Precision (MAP) metrics.\n"
   ],
   "doi": "10.21437/Interspeech.2016-646"
  },
  "zhuang16_interspeech": {
   "authors": [
    [
     "Yimeng",
     "Zhuang"
    ],
    [
     "Xuankai",
     "Chang"
    ],
    [
     "Yanmin",
     "Qian"
    ],
    [
     "Kai",
     "Yu"
    ]
   ],
   "title": "Unrestricted Vocabulary Keyword Spotting Using LSTM-CTC",
   "original": "0753",
   "page_count": 5,
   "order": 196,
   "p1": "938",
   "pn": "942",
   "abstract": [
    "Keyword spotting (KWS) aims to detect predefined keywords in continuous\nspeech. Recently, direct deep learning approaches have been used for\nKWS and achieved great success. However, these approaches mostly assume\nfixed keyword vocabulary and require significant retraining efforts\nif new keywords are to be detected. For unrestricted vocabulary, HMM\nbased keyword-filler framework is still the mainstream technique. In\nthis paper, a novel deep learning approach is proposed for unrestricted\nvocabulary KWS based on Connectionist Temporal Classification (CTC)\nwith Long Short-Term Memory (LSTM). Here, an LSTM is trained to discriminant\nphones with the CTC criterion. During KWS, an arbitrary keyword can\nbe specified and it is represented by one or more phone sequences.\nDue to the property of peaky phone posteriors of CTC, the LSTM can\nproduce a phone lattice. Then, a fast substring matching algorithm\nbased on minimum edit distance is used to search the keyword phone\nsequence on the phone lattice. The approach is highly efficient and\nvocabulary independent. Experiments showed that the proposed approach\ncan achieve significantly better results compared to a DNN-HMM based\nkeyword-filler decoding system. In addition, the proposed approach\nis also more efficient than the DNN-HMM KWS baseline.\n"
   ],
   "doi": "10.21437/Interspeech.2016-753"
  },
  "wu16b_interspeech": {
   "authors": [
    [
     "Yen-Chen",
     "Wu"
    ],
    [
     "Tzu-Hsiang",
     "Lin"
    ],
    [
     "Yang-De",
     "Chen"
    ],
    [
     "Hung-Yi",
     "Lee"
    ],
    [
     "Lin-Shan",
     "Lee"
    ]
   ],
   "title": "Interactive Spoken Content Retrieval by Deep Reinforcement Learning",
   "original": "1237",
   "page_count": 5,
   "order": 197,
   "p1": "943",
   "pn": "947",
   "abstract": [
    "User-machine interaction is important for spoken content retrieval.\nFor text content retrieval, the user can easily scan through and select\non a list of retrieved item. This is impossible for spoken content\nretrieval, because the retrieved items are difficult to show on screen.\nBesides, due to the high degree of uncertainty for speech recognition,\nthe retrieval results can be very noisy. One way to counter such difficulties\nis through user-machine interaction. The machine can take different\nactions to interact with the user to obtain better retrieval results\nbefore showing to the user. The suitable actions depend on the retrieval\nstatus, for example requesting for extra information from the user,\nreturning a list of topics for user to select, etc. In our previous\nwork, some hand-crafted states estimated from the present retrieval\nresults are used to determine the proper actions. In this paper, we\npropose to use Deep-Q-Learning techniques instead to determine the\nmachine actions for interactive spoken content retrieval. Deep-Q-Learning\nbypasses the need for estimation of the hand-crafted states, and directly\ndetermine the best action base on the present retrieval status even\nwithout any human knowledge. It is shown to achieve significantly better\nperformance compared with the previous hand-crafted states.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1237"
  },
  "godoy16_interspeech": {
   "authors": [
    [
     "Elizabeth",
     "Godoy"
    ],
    [
     "Andrew",
     "Dumas"
    ],
    [
     "Jennifer",
     "Melot"
    ],
    [
     "Nicolas",
     "Malyska"
    ],
    [
     "Thomas F.",
     "Quatieri"
    ]
   ],
   "title": "Relating Estimated Cyclic Spectral Peak Frequency to Measured Epilarynx Length Using Magnetic Resonance Imaging",
   "original": "1362",
   "page_count": 5,
   "order": 198,
   "p1": "948",
   "pn": "952",
   "abstract": [
    "The epilarynx plays an important role in speech production, carrying\ninformation about the individual speaker and manner of articulation.\nHowever, precise acoustic behavior of this lower vocal tract structure\nis difficult to establish. Focusing on acoustics observable in natural\nspeech, recent spectral processing techniques isolate a unique resonance\nwith characteristics of the epilarynx previously shown via simulation,\nspecifically cyclicity (i.e. energy differences between the closed\nand open phases of the glottal cycle) in a 3&#8211;5kHz region observed\nacross vowels. Using Magnetic Resonance Imaging (MRI), the present\nwork relates this estimated cyclic peak frequency to measured epilarynx\nlength. Assuming a simple quarter wavelength relationship, the cavity\nlength estimated from the cyclic peak frequency is shown to be directly\nproportional (linear fit slope =1.1) and highly correlated (&#961;\n= 0.85, pval&#60;10<SUP>-4</SUP>) to the measured epilarynx length\nacross speakers. Results are discussed, as are implications in speech\nscience and application domains.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1362"
  },
  "tobing16_interspeech": {
   "authors": [
    [
     "Patrick Lumban",
     "Tobing"
    ],
    [
     "Tomoki",
     "Toda"
    ],
    [
     "Hirokazu",
     "Kameoka"
    ],
    [
     "Satoshi",
     "Nakamura"
    ]
   ],
   "title": "Acoustic-to-Articulatory Inversion Mapping Based on Latent Trajectory Gaussian Mixture Model",
   "original": "1196",
   "page_count": 5,
   "order": 199,
   "p1": "953",
   "pn": "957",
   "abstract": [
    "A maximum likelihood parameter trajectory estimation based on a Gaussian\nmixture model (GMM) has been successfully implemented for acoustic-to-articulatory\ninversion mapping. In the conventional method, GMM parameters are optimized\nby maximizing a likelihood function for joint static and dynamic features\nof acoustic-articulatory data, and then, the articulatory parameter\ntrajectories are estimated for given the acoustic data by maximizing\na likelihood function for only the static features, imposing a constraint\nbetween static and dynamic features to consider the inter-frame correlation.\nDue to the inconsistency of the training and mapping criterion, the\ntrained GMM is not optimum for the mapping process. This inconsistency\nproblem is addressed within a trajectory training framework, but it\nbecomes more difficult to optimize some parameters, e.g., covariance\nmatrices and mixture component sequences. In this paper, we propose\nan inversion mapping method based on a latent trajectory GMM (LT-GMM)\nas yet another way to overcome the inconsistency issue. The proposed\nmethod makes it possible to use a well-formulated algorithm, such as\nEM algorithm, to optimize the LT-GMM parameters, which is not feasible\nin the traditional trajectory training. Experimental results demonstrate\nthat the proposed method yields higher accuracy in the inversion mapping\ncompared to the conventional GMM-based method.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1196"
  },
  "dissen16_interspeech": {
   "authors": [
    [
     "Yehoshua",
     "Dissen"
    ],
    [
     "Joseph",
     "Keshet"
    ]
   ],
   "title": "Formant Estimation and Tracking Using Deep Learning",
   "original": "0490",
   "page_count": 5,
   "order": 200,
   "p1": "958",
   "pn": "962",
   "abstract": [
    "Formant frequency estimation and tracking are among the most fundamental\nproblems in speech processing. In the former task the input is a stationary\nspeech segment such as the middle part of a vowel and the goal is to\nestimate the formant frequencies, whereas in the latter task the input\nis a series of speech frames and the goal is to track the trajectory\nof the formant frequencies throughout the signal. Traditionally, formant\nestimation and tracking is done using ad-hoc signal processing methods.\nIn this paper we propose using machine learning techniques trained\non an annotated corpus of read speech for these tasks. Our feature\nset is composed of LPC-based cepstral coefficients with a range of\nmodel orders and pitch-synchronous cepstral coefficients. Two deep\nnetwork architectures are used as learning algorithms: a deep feed-forward\nnetwork for the estimation task and a recurrent neural network for\nthe tracking task. The performance of our methods compares favorably\nwith mainstream LPC-based implementations and state-of-the-art tracking\nalgorithms.\n"
   ],
   "doi": "10.21437/Interspeech.2016-490"
  },
  "vaz16_interspeech": {
   "authors": [
    [
     "Colin",
     "Vaz"
    ],
    [
     "Asterios",
     "Toutios"
    ],
    [
     "Shrikanth S.",
     "Narayanan"
    ]
   ],
   "title": "Convex Hull Convolutive Non-Negative Matrix Factorization for Uncovering Temporal Patterns in Multivariate Time-Series Data",
   "original": "0571",
   "page_count": 5,
   "order": 201,
   "p1": "963",
   "pn": "967",
   "abstract": [
    "We propose the Convex Hull Convolutive Non-negative Matrix Factorization\n(CH-CNMF) algorithm to learn temporal patterns in multivariate time-series\ndata. The algorithm factors a data matrix into a basis tensor that\ncontains temporal patterns and an activation matrix that indicates\nthe time instants when the temporal patterns occurred in the data.\nImportantly, the temporal patterns correspond closely to the observed\ndata and represent a wide range of dynamics. Experiments with synthetic\ndata show that the temporal patterns found by CH-CNMF match the data\nbetter and provide more meaningful information than the temporal patterns\nfound by Convolutive Non-negative Matrix Factorization with sparsity\nconstraints (CNMF-SC). Additionally, CH-CNMF applied on vocal tract\nconstriction data yields a wider range of articulatory gestures compared\nto CNMF-SC. Moreover, we find that the gestures comprising the CH-CNMF\nbasis generalize better to unseen data and capture vocal tract structure\nand dynamics significantly better than those comprising the CNMF-SC\nbasis.\n"
   ],
   "doi": "10.21437/Interspeech.2016-571"
  },
  "juvela16_interspeech": {
   "authors": [
    [
     "Lauri",
     "Juvela"
    ],
    [
     "Hirokazu",
     "Kameoka"
    ],
    [
     "Manu",
     "Airaksinen"
    ],
    [
     "Junichi",
     "Yamagishi"
    ],
    [
     "Paavo",
     "Alku"
    ]
   ],
   "title": "Majorisation-Minimisation Based Optimisation of the Composite Autoregressive System with Application to Glottal Inverse Filtering",
   "original": "0735",
   "page_count": 5,
   "order": 202,
   "p1": "968",
   "pn": "972",
   "abstract": [
    "The composite autoregressive system can be used to estimate a speech\nsource-filter decomposition in a rigorous manner, thus having potential\nuse in glottal inverse filtering. By introducing a suitable prior,\nspectral tilt can be introduced into the source component estimation\nto better correspond to human voice production. However, the current\nexpectation-maximisation based composite autoregressive model optimisation\nleaves room for improvement in terms of speed. Inspired by majorisation-minimisation\ntechniques used for nonnegative matrix factorisation, this work derives\nnew update rules for the model, resulting in faster convergence compared\nto the original approach. Additionally, we present a new glottal inverse\nfiltering method based on the composite autoregressive system and compare\nit with inverse filtering methods currently used in glottal excitation\nmodelling for parametric speech synthesis. These initial results show\nthat the proposed method performs comparatively well, sometimes outperforming\nthe reference methods.\n"
   ],
   "doi": "10.21437/Interspeech.2016-735"
  },
  "wang16c_interspeech": {
   "authors": [
    [
     "Xiaoyun",
     "Wang"
    ],
    [
     "Xugang",
     "Lu"
    ],
    [
     "Hisashi",
     "Kawai"
    ],
    [
     "Seiichi",
     "Yamamoto"
    ]
   ],
   "title": "F<SUB>0</SUB> Contour Analysis Based on Empirical Mode Decomposition for DNN Acoustic Modeling in Mandarin Speech Recognition",
   "original": "0653",
   "page_count": 5,
   "order": 203,
   "p1": "973",
   "pn": "977",
   "abstract": [
    "Tone information provides a strong distinction for many ambiguous characters\nin Mandarin Chinese. The use of tonal acoustic units and F<SUB>0</SUB>\nrelated tonal features have been shown to be effective at improving\nthe accuracy of Mandarin automatic speech recognition (ASR) systems,\nas F<SUB>0</SUB> contains the most prominent tonal information for\ndistinguishing words that are phonemically identical. Both long-term\ntemporal intonations and short-term quick variations coexist in F<SUB>0</SUB>.\nUsing untreated F<SUB>0</SUB> as an acoustic feature renders the F<SUB>0</SUB>\ncontour patterns differently from their citation form and downplays\nthe significance of tonal information in ASR. In this paper, we explore\nthe empirical mode decomposition (EMD) on F<SUB>0</SUB> contours to\nreconstruct F<SUB>0</SUB> related tonal features with a view to removing\nthe components that are irrelevant for Mandarin ASR.We investigate\nboth GMM-HMM and DNN-HMM based acoustic modeling with the reconstructed\ntonal features. In comparison with the baseline systems using typical\ntonal features, our best system using reconstructed tonal features\nleads to a 4.5% relative word error rate reduction for the GMM-HMM\nsystem and a 3.5% relative word error rate reduction for the DNN-HMM\nsystem.\n"
   ],
   "doi": "10.21437/Interspeech.2016-653"
  },
  "hu16_interspeech": {
   "authors": [
    [
     "Fang",
     "Hu"
    ],
    [
     "Chunyu",
     "Ge"
    ]
   ],
   "title": "Vowels and Diphthongs in Cangnan Southern Min Chinese Dialect",
   "original": "0029",
   "page_count": 5,
   "order": 204,
   "p1": "978",
   "pn": "982",
   "abstract": [
    "This paper gives an acoustic phonetic description of the vowels and\ndiphthongs in Cangnan Southern Min Chinese dialect. Vowel formant data\nfrom 10 speakers (5 male and 5 female) show that the distribution of\nCangnan monophthongs in the acoustic vowel space is of particular typological\ninterest. Diphthong production is examined in terms of temporal organization,\nspectral property, and dynamic aspects. Results suggest that the production\nof falling diphthongs tends to be a single articulatory event with\na dynamic spectral target, while the production of rising diphthongs\nand level diphthongs is a sequence of two spectral targets.\n"
   ],
   "doi": "10.21437/Interspeech.2016-29"
  },
  "hu16b_interspeech": {
   "authors": [
    [
     "Wenqi",
     "Hu"
    ],
    [
     "Fang",
     "Hu"
    ],
    [
     "Jian",
     "Jin"
    ]
   ],
   "title": "Diphthongization of Nuclear Vowels and the Emergence of a Tetraphthong in Hetang Cantonese",
   "original": "0061",
   "page_count": 5,
   "order": 205,
   "p1": "983",
   "pn": "987",
   "abstract": [
    "This paper is an acoustic phonetic description of vowels in Hetang\nCantonese, and focuses on the diphthongization of nuclear vowels. Different\nto the representative dialect such as Guangzhou or Hong Kong Cantonese,\nthe Hetang dialect exhibits its unique characteristics regarding the\nphonetics and phonology of vowels. A noticeable phenomenon is the diphthongization\nof nuclear vowels. And, a tetraphthong [u&#596;&#7492;i] emerges when\nthe nuclear vowel is diphthongized in a triphthong.\n"
   ],
   "doi": "10.21437/Interspeech.2016-61"
  },
  "cernak16b_interspeech": {
   "authors": [
    [
     "Milos",
     "Cernak"
    ],
    [
     "Philip N.",
     "Garner"
    ]
   ],
   "title": "PhonVoc: A Phonetic and Phonological Vocoding Toolkit",
   "original": "0235",
   "page_count": 5,
   "order": 206,
   "p1": "988",
   "pn": "992",
   "abstract": [
    "We present the PhonVoc toolkit, a cascaded deep neural network (DNN)\ncomposed of speech analyser and synthesizer that use a shared phonetic\nand/or phonological speech representation. The free toolkit is distributed\nas open-source software under a BSD 3-Clause License, available at\n https://github.com/idiap/phonvoc with the pre-trained US English analysis\nand synthesis DNNs, and thus it is ready for immediate use.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  In a broader context,\nthe toolkit implements training and testing of the analysis by synthesis\nheuristic model. It is thus designed for the wider speech community\nworking in acoustic phonetics, laboratory phonology, and parametric\nspeech coding. The toolkit interprets the phonetic posterior probabilities\nas a sequential scheme, whereas the phonological posterior-class probabilities\nare considered as a parallel via K different phonological classes.\nA case study is presented on a LibriSpeech database and a LibriVox\nUS English native female speaker. The phonetic and phonological vocoding\nyield comparable performance, improving speech quality by merging the\nphonetic and phonological speech representation.\n"
   ],
   "doi": "10.21437/Interspeech.2016-235"
  },
  "xia16b_interspeech": {
   "authors": [
    [
     "Liping",
     "Xia"
    ],
    [
     "Fang",
     "Hu"
    ]
   ],
   "title": "Vowels and Diphthongs in the Taiyuan Jin Chinese Dialect",
   "original": "0249",
   "page_count": 5,
   "order": 207,
   "p1": "993",
   "pn": "997",
   "abstract": [
    "On the basis of an acoustic phonetic analysis of monophthongs and diphthongs,\nthis paper describes vowel phonology in the Taiyuan Jin dialect. The\nresults show that Taiyuan has a comparable but different vowel inventory\nfor C(G)V versus C(G)VN syllables. And the vowel contrast is dramatically\nreduced in checked syllables. The asymmetry between falling and rising\ndiphthongs suggests a dynamic account of vowels, rather than a sequential\ntaxonomy of vowels into monophthongs and diphthongs. Phonetically,\nmonophthongs are composed of a static spectral target, falling diphthongs\nare composed of a dynamic spectral target, and rising diphthongs are\nsequences of two spectral targets. Phonologically, falling diphthongs\nare grouped with monophthongs, rather than rising diphthongs.\n"
   ],
   "doi": "10.21437/Interspeech.2016-249"
  },
  "turco16_interspeech": {
   "authors": [
    [
     "Giuseppina",
     "Turco"
    ],
    [
     "Cécile",
     "Fougeron"
    ],
    [
     "Nicolas",
     "Audibert"
    ]
   ],
   "title": "The Effects of Prosody on French V-to-V Coarticulation: A Corpus-Based Study",
   "original": "1323",
   "page_count": 4,
   "order": 208,
   "p1": "998",
   "pn": "1001",
   "abstract": [
    "This study examines whether the degree of vowel-to-vowel coarticulation\nin French (better known as &#8220;vowel height harmony&#8221;, V2-to-V1\nhenceforth) varies as a function of position in prosodic domain (i.e.\nIP initial vs. word-medial) and duration of V1 (i.e. short vs. long).\nFollowing the literature on the phonetics-prosody interface, segments\nat stronger edges are more resistant to coarticulatory effects induced\nby their neighboring vowel. While previous studies have mainly looked\nat non-local V-to-V coarticulation across prosodic boundaries/domains\n(e.g.,V#(C)V), here we look at V2-to-V1 coarticulation within an Intonational\nPhrase according to whether target V1 is in absolute initial position\n(#V1C(C)V2, e.g., #essaie [es&#949;]/[&#949;s&#949;] &#8211; &#8216;try&#8217;)\nor not (word-medial, e.g., &#233;paissit [ep&#949;si]/[epesi] &#8211;\n&#8216;thikened&#8217;). The analyses are based on 33k words presenting\npossible V1C(C)V2 harmonic contexts, which were extracted from a corpus\nof French running speech. V2-to-V1 coarticulation is measured as the\nlowering of the first formant of the target V1 (/e, &#949;, o, &#596;/)\nin relation to the height of the V2 trigger /+high/ (i.e. mid-high\nand high) vs. /-high/ (i.e. mid-low and low). Results show an effect\nof prosodic position (but no effect of V1 duration) on V2-to-V1 coarticulation:\nV1 is more resistant to coarticulation when initial in an IP.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1323"
  },
  "galata16_interspeech": {
   "authors": [
    [
     "Vincenzo",
     "Galatà"
    ],
    [
     "Lorenzo",
     "Spreafico"
    ],
    [
     "Alessandro",
     "Vietti"
    ],
    [
     "Constantijn",
     "Kaland"
    ]
   ],
   "title": "An Acoustic Analysis of /r/ in Tyrolean",
   "original": "0434",
   "page_count": 5,
   "order": 209,
   "p1": "1002",
   "pn": "1006",
   "abstract": [
    "This paper offers a preliminary contribution to the phonetic description\nand acoustic characterization of /r/ allophony in Tyrolean dialect,\nan under-researched South Bavarian Dialect spoken in the North of Italy.\nThe analysis of target words containing /r/ in different phonotactic\ncontexts, produced by six Tyrolean female speakers, confirms the high\ndegree of intra-speaker variation in the production of /r/ with a uvular\nplace of articulation. The distributional analysis of the allophones\nin our sample shows a preference among all the speakers for a fricative\nmanner of articulation followed by approximants and taps and, to a\nlesser extent, by trills (with a very small amount of vocalized variants).\nThese results are in line with previous research in the South-Tyrolean\ncommunity. Due to the high variability of rhotic sounds, we further\ninvestigate and report on some of their shared acoustic features such\nas duration across the different phonotactic contexts and Harmonics-to-Noise\nRatio for the different allophones attested.\n"
   ],
   "doi": "10.21437/Interspeech.2016-434"
  },
  "chang16_interspeech": {
   "authors": [
    [
     "Seung-Eun",
     "Chang"
    ],
    [
     "Minsook",
     "Kim"
    ]
   ],
   "title": "Hyperarticulated Production of Korean Glides by Age Group",
   "original": "0023",
   "page_count": 4,
   "order": 210,
   "p1": "1007",
   "pn": "1010",
   "abstract": [
    "This research uses the hyperspace effect (Johnson, Flemming, &amp;\nWright, 1993; Lindblom, 1990) of Korean glides to address the issues\ntriggered by the diachronic sound change of some Korean vowels. Specifically,\nwe examine whether there is any difference between Korean &#8216;wae\n[w&#949;]&#8217; versus &#8216;oe [we]&#8217; by speech style (casual\nand clear speech) and speakers&#8217; age. Twenty adults from Seoul\nand the Kyunggi area participated: (i) a younger group (21&#8211;34\nyears old) and (ii) an older group (44&#8211;71 years old). The first\nand second formant frequencies (Hz) were measured at two time points:\n(i) onset of test syllable and (ii) vowel midpoint. The results showed\nthat the transitional trait of glides &#8220;wae [w&#949;]&#8221; and\n&#8220;oe [we]&#8221; at initial timing of syllable was more enhanced\nin clear speech than in casual speech, as predicted. However, no phonetic\nevidence was found for the difference between &#8220;wae [w&#949;]&#8221;\nand &#8220;oe [we]&#8221; in terms of F1 and F2, even in clear speech.\nAlso, no systematic difference of age group depending on vowel type\nwas found. Therefore, we argue that the diachronic sound merge between\n&#8220;wae [w&#949;]&#8221; and &#8220;oe [we]&#8221; is now completed\neven in the Seoul area and for older groups.\n"
   ],
   "doi": "10.21437/Interspeech.2016-23"
  },
  "pan16_interspeech": {
   "authors": [
    [
     "Ho-hsien",
     "Pan"
    ],
    [
     "Hsiao-tung",
     "Huang"
    ],
    [
     "Shao-ren",
     "Lyu"
    ]
   ],
   "title": "Coda Stop and Taiwan Min Checked Tone Sound Changes",
   "original": "0597",
   "page_count": 5,
   "order": 211,
   "p1": "1011",
   "pn": "1015",
   "abstract": [
    "This acoustical and Electroglottography (EGG) study investigates the\neffect of coda deletion and co-articulatory phasing on vowels and final\ncoda stops, [p t k &#660;], in Taiwan Min checked tones 3 and 5 syllables.\nVowel duration, f0, spectral tilt (H1*-A3*), cepstral peak prominence\n(CPP) and glottal contact quotient (CQ_H) were analyzed. Compensatory\nlengthening, f0 lowering and increasing periodic phonation during the\nproduction of vowels after coda deletion were observed. During gradual\nphasing when codas were produced as energy damping, the vowels were\nfound to be shorter in duration and less periodic in voicing than vowels\nabruptly phased with coda that were produced as full stop closure.\nHowever, spectral tilt H1*-A3* was not affected by either coda deletion\nor co-articulatory phasing. Therefore, these findings suggest that\nH1*-A3* may play a salient role in checked tone identification, and,\nas a result, is unaffected by sound change.\n"
   ],
   "doi": "10.21437/Interspeech.2016-597"
  },
  "fenwick16_interspeech": {
   "authors": [
    [
     "Sarah E.",
     "Fenwick"
    ],
    [
     "Catherine T.",
     "Best"
    ],
    [
     "Chris",
     "Davis"
    ],
    [
     "Michael D.",
     "Tyler"
    ]
   ],
   "title": "The Influence of Modality and Speaking Style on the Assimilation Type and Categorization Consistency of Non-Native Speech",
   "original": "0611",
   "page_count": 5,
   "order": 212,
   "p1": "1016",
   "pn": "1020",
   "abstract": [
    "The Perceptual Assimilation Model [1] proposes that non-native contrast\ndiscrimination accuracy can be predicted by perceptual assimilation\ntype. However, assimilation types have been based just on auditory-only\n(AO) citation speech. Since auditory-visual (AV) and clear speech can\nbenefit non-native speech perception [2, 3], we reasoned that modality\nand speaking style could influence assimilation. This was tested by\npresenting English monolinguals Sindhi consonants in a categorization\ntask. Results showed that, across speaking styles, consonants were\nassimilated the same way in AV and AO. For consonants that were uncategorized\nin visual-only (VO) conditions: 1) their AO counterpart was more consistently\ncategorized than AV; and 2) citation speech was also more consistently\ncategorized than clear. Interestingly, this set of results was reversed\nfor consonants that were assimilated to the same native category across\nmodalities; participants were able to use the visual articulatory information\nto make more consistent categorization judgments for AV than AO. This\nwas also the case for speaking style: clear speech was more consistently\ncategorized than citation. Together these results demonstrate that\nthe extent to which AV and clear speech is beneficial for cross-language\nperception may depend on the similarities between the articulatory\ncharacteristics of native and non-native consonants.\n"
   ],
   "doi": "10.21437/Interspeech.2016-611"
  },
  "zellers16_interspeech": {
   "authors": [
    [
     "Margaret",
     "Zellers"
    ]
   ],
   "title": "Prosodic Convergence with Spoken Stimuli in Laboratory Data",
   "original": "0238",
   "page_count": 5,
   "order": 213,
   "p1": "1021",
   "pn": "1025",
   "abstract": [
    "Accommodation or convergence between speakers has been shown to occur\non a variety of levels of linguistic structure. Phonetic convergence\nappears to be a very variable phenomenon in conversation, with social\nroles strongly influencing who accommodates to whom. Since phonetic\nconvergence appears to be strongly under speaker control, it is unclear\nwhether speakers might converge phonetically in a laboratory setting.\nThe current study investigates accommodation of pitch and duration\nfeatures in data collected in a laboratory setting. While speakers\nin the study did not converge to spoken stimuli in terms of duration\nfeatures, they did converge to an extent on pitch features. However,\nonly some information-structure contexts led to convergence, suggesting\nthat even in a laboratory setting, speakers are aware of the discourse\nimplications of their production.\n"
   ],
   "doi": "10.21437/Interspeech.2016-238"
  },
  "themistocleous16_interspeech": {
   "authors": [
    [
     "Charalambos",
     "Themistocleous"
    ],
    [
     "Angelandria",
     "Savva"
    ],
    [
     "Andrie",
     "Aristodemou"
    ]
   ],
   "title": "Effects of Stress on Fricatives: Evidence from Standard Modern Greek",
   "original": "1057",
   "page_count": 4,
   "order": 214,
   "p1": "1026",
   "pn": "1029",
   "abstract": [
    "This study investigates the effects of stress on the spectral properties\nof fricative noise in Standard Modern Greek (SMG). Twenty female speakers\nof SMG participated in the study. Fricatives were produced in stressed\nand unstressed positions in two vowel place positions: back and front\nvowels. Acoustic measurements were taken and the temporal and spectral\nproperties of fricatives &#8212; using spectral moments &#8212; were\ncalculated. Stressed fricatives are produced with increased duration,\ncenter of gravity, standard deviation, and normalized intensity. The\nmachine learning and classification algorithm C5.0 has been employed\nto estimate the contribution of the temporal and spectral parameters\nfor the classification of fricatives. Overall, duration and center\nof gravity contribute the most to the classification of stressed vs.\nunstressed fricatives.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1057"
  },
  "sun16b_interspeech": {
   "authors": [
    [
     "Yue",
     "Sun"
    ],
    [
     "Shudon",
     "Hsiao"
    ],
    [
     "Yoshinori",
     "Sagisaka"
    ],
    [
     "Jinsong",
     "Zhang"
    ]
   ],
   "title": "Analysis of Chinese Syllable Durations in Running Speech of Japanese L2 Learners",
   "original": "0824",
   "page_count": 4,
   "order": 215,
   "p1": "1030",
   "pn": "1033",
   "abstract": [
    "Aiming at better understanding of prosody generation by native Japanese\nlearners of Mandarin as a second language (L2), we analyzed the syllable\nduration differences between tone types. By comparing the mean syllable\ndurations and the variation of normalized syllable durations across\ntone types and speakers, significant differences were found between\ntone types as well as between speakers. Native Chinese speakers generate\ntone 1 and tone 2 with relatively long durations but smaller variations,\ncontrary to tone 3 and tone 4. Japanese L2 learners generate tone 3\nwith relatively high variations compared to the other tones, while\nthe mean duration of tone 4 was remarkably different from natives.\nCompared with native speakers, the variations of both tone 3 and tone\n4 are significantly smaller. Furthermore, the neutral tone caused a\nsignificant increase of the mean variation across tones for the Japanese\nL2 learners. The results suggest that native Chinese speakers control\nsyllable durations adaptively with tones, especially for tone 3 and\ntone 4, in running speech while Japanese L2 learners tend to pronounce\nthem in isolated syllable fashion.\n"
   ],
   "doi": "10.21437/Interspeech.2016-824"
  },
  "lai16b_interspeech": {
   "authors": [
    [
     "Catherine",
     "Lai"
    ],
    [
     "Mireia",
     "Farrús"
    ],
    [
     "Johanna D.",
     "Moore"
    ]
   ],
   "title": "Automatic Paragraph Segmentation with Lexical and Prosodic Features",
   "original": "0992",
   "page_count": 5,
   "order": 216,
   "p1": "1034",
   "pn": "1038",
   "abstract": [
    "As long-form spoken documents become more ubiquitous in everyday life,\nso does the need for automatic discourse segmentation in spoken language\nprocessing tasks. Although previous work has focused on broad topic\nsegmentation, detection of finer-grained discourse units, such as paragraphs,\nis highly desirable for presenting and analyzing spoken content. To\nbetter understand how different aspects of speech cue these subtle\ndiscourse transitions, we investigate automatic paragraph segmentation\nof TED talks. We build lexical and prosodic paragraph segmenters using\nSupport Vector Machines, AdaBoost, and Long Short Term Memory (LSTM)\nrecurrent neural networks. In general, we find that induced cue words\nand supra-sentential prosodic features outperform features based on\ntopical coherence, syntactic form and complexity. However, our best\nperformance is achieved by combining a wide range of individually weak\nlexical and prosodic features, with the sequence modelling LSTM generally\noutperforming the other classifiers by a large margin. Moreover, we\nfind that models that allow lower level interactions between different\nfeature types produce better results than treating lexical and prosodic\ncontributions as separate, independent information sources.\n"
   ],
   "doi": "10.21437/Interspeech.2016-992"
  },
  "airaksinen16_interspeech": {
   "authors": [
    [
     "Manu",
     "Airaksinen"
    ],
    [
     "Lauri",
     "Juvela"
    ],
    [
     "Tom",
     "Bäckström"
    ],
    [
     "Paavo",
     "Alku"
    ]
   ],
   "title": "Automatic Glottal Inverse Filtering with Non-Negative Matrix Factorization",
   "original": "0338",
   "page_count": 5,
   "order": 217,
   "p1": "1039",
   "pn": "1043",
   "abstract": [
    "This study presents an automatic glottal inverse filtering (GIF) technique\nbased on separating the effect of the glottal main excitation from\nthe impulse response of the vocal tract. The proposed method is based\non a non-negative matrix factorization (NMF) based decomposition of\nan ultra short-term spectrogram of the analyzed signal. Unlike other\nstate-of-the-art GIF techniques, the proposed method does not require\nestimation of glottal closure instants.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  The proposed method\nwas objectively evaluated with two test sets of continuous synthetic\nspeech created with a glottal vocoding analysis/synthesis procedure.\nWhen compared to a set of reference GIF methods, the proposed NMF technique\nshows improved estimation accuracy especially for male voices.\n"
   ],
   "doi": "10.21437/Interspeech.2016-338"
  },
  "park16_interspeech": {
   "authors": [
    [
     "Soo Jin",
     "Park"
    ],
    [
     "Caroline",
     "Sigouin"
    ],
    [
     "Jody",
     "Kreiman"
    ],
    [
     "Patricia",
     "Keating"
    ],
    [
     "Jinxi",
     "Guo"
    ],
    [
     "Gary",
     "Yeung"
    ],
    [
     "Fang-Yu",
     "Kuo"
    ],
    [
     "Abeer",
     "Alwan"
    ]
   ],
   "title": "Speaker Identity and Voice Quality: Modeling Human Responses and Automatic Speaker Recognition",
   "original": "0523",
   "page_count": 5,
   "order": 218,
   "p1": "1044",
   "pn": "1048",
   "abstract": [
    "Despite recent breakthroughs in automatic speaker recognition (ASpR),\nsystem performance still degrades when utterances are short and/or\nwhen within-speaker variability is large. This study used short test\nutterances (2&#8211;3sec) to investigate the effect of within-speaker\nvariability on state-of-the-art ASpR system performance. A subset of\na newly-developed UCLA database is used, which contains multiple speech\ntasks per speaker. The short utterances combined with a speaking-style\nmismatch between read sentences and spontaneous affective speech degraded\nsystem performance, for 25 female speakers, by 36%. Because humans\nare more robust to utterance length or within-speaker variability,\nunderstanding human perception might benefit ASpR systems. Perception\nexperiments were conducted with recorded read sentences from 3 female\nspeakers, and a model is proposed to predict the perceptual dissimilarity\nbetween tokens. Results showed that a set of voice quality features\nincluding F0, F1, F2, F3, H1*-H2*, H2*-H4*, H4*-H2k*, H2k*-H5k, and\nCPP provides information that complements MFCCs. By fusing the feature\nset with MFCCs, human response prediction RMS error was .12, which\nrepresents a 12% relative error reduction compared to using MFCCs alone.\nIn ASpR experiments with short utterances from 50 speakers, the voice\nquality feature set decreased the error rate by 11% when fused with\nMFCCs.\n"
   ],
   "doi": "10.21437/Interspeech.2016-523"
  },
  "kalita16_interspeech": {
   "authors": [
    [
     "Sishir",
     "Kalita"
    ],
    [
     "Luke",
     "Horo"
    ],
    [
     "Priyankoo",
     "Sarmah"
    ],
    [
     "S.R. Mahadeva",
     "Prasanna"
    ],
    [
     "S.",
     "Dandapat"
    ]
   ],
   "title": "Analysis of Glottal Stop in Assam Sora Language",
   "original": "0877",
   "page_count": 5,
   "order": 219,
   "p1": "1049",
   "pn": "1053",
   "abstract": [
    "The objective of this work is to characterize the intervocalic glottal\nstops in Assam Sora. Assam Sora is a low resource language of the South\nMunda language family. Glottal stops are produced with gestures in\nthe deep laryngeal level; hence, the estimated excitation source signal\nis used in this study to characterize the source dynamics during the\nproduction of Assam Sora glottal stops. From that, temporal domain\nvoice source features, Quasi-Open Quotient (QOQ) and Normalized Amplitude\nQuotient (NAQ) are extracted along with spectral features such as H1-H2\nratio and Harmonic Richness Factor (HRF). One excitation source feature\nis extracted from the zero frequency filtered version of the speech\nsignal to characterize the variations within the glottal cycles in\nglottal stop region. A recently proposed wavelet based voice source\nfeature, Maxima Dispersion Quotient (MDQ) is also used to characterize\nthe abrupt glottal closure during glottal stop production. From the\nanalysis, it is observed that the features are salient enough to uniquely\ncharacterize glottal stops from the adjacent vowel sounds and may also\nbe used in continuous speech. A Mann-Whitney U test confirmed the statistical\nsignificance of the differences between glottal stops and their adjacent\nvowels.\n"
   ],
   "doi": "10.21437/Interspeech.2016-877"
  },
  "garellek16_interspeech": {
   "authors": [
    [
     "Marc",
     "Garellek"
    ],
    [
     "Scott",
     "Seyfarth"
    ]
   ],
   "title": "Acoustic Differences Between English /t/ Glottalization and Phrasal Creak",
   "original": "1472",
   "page_count": 5,
   "order": 220,
   "p1": "1054",
   "pn": "1058",
   "abstract": [
    "In American English, the presence of creaky voice can derive from distinct\nlinguistic processes, including phrasal creak (prolonged irregular\nvoicing, often at edges of prosodic phrases) and coda /t/ glottalization\n(when the alveolar closure for syllable-final /t/ is replaced by or\nproduced simultaneously with glottal constriction). Previous work has\nshown that listeners can differentiate words in phrasal creak from\nthose with /t/ glottalization, which suggests that there are acoustic\ndifferences between the creaky voice derived from phrasal creak and\n/t/ glottalization. In this study, we analyzed vowels preceding syllable-final\n/t/ in the Buckeye Corpus, which includes audio recordings of spontaneous\nspeech from 40 speakers of American English. Tokens were coded for\npresence of phrasal creak (prolonged irregular voicing extending beyond\nthe target syllable) and /t/ glottalization (whether the /t/ was produced\nonly with glottal constriction). Eleven spectral measures of voice\nquality, including both harmonic and noise measures, were extracted\nautomatically and discriminant analyses were performed. The results\nindicate that the discriminant functions can classify these sources\nof creaky voice above chance, and that Cepstral Peak Prominence, a\nmeasure of harmonics-to-noise ratio, is important for distinguishing\nphrasal creak from glottalization.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1472"
  },
  "eriksson16_interspeech": {
   "authors": [
    [
     "Anders",
     "Eriksson"
    ],
    [
     "Pier Marco",
     "Bertinetto"
    ],
    [
     "Mattias",
     "Heldner"
    ],
    [
     "Rosalba",
     "Nodari"
    ],
    [
     "Giovanna",
     "Lenoci"
    ]
   ],
   "title": "The Acoustics of Lexical Stress in Italian as a Function of Stress Level and Speaking Style",
   "original": "0348",
   "page_count": 5,
   "order": 221,
   "p1": "1059",
   "pn": "1063",
   "abstract": [
    "The study is part of a series of studies, describing the acoustics\nof lexical stress in a way that should be applicable to any language.\nThe present database of recordings includes Brazilian Portuguese, English,\nEstonian, German, French, Italian and Swedish. The acoustic parameters\nexamined are  F<SUB>0</SUB>-level, F<SUB>0</SUB>-variation, Duration,\nand  Spectral Emphasis. Values for these parameters, computed for all\nvowels (a little over 24000 vowels for Italian), are the data upon\nwhich the analyses are based. All parameters are examined with respect\nto their correlation with  Stress (primary, secondary, unstressed)\nand speaking  Style (wordlist reading, phrase reading, spontaneous\nspeech) and  Sex of the speaker (female, male). For Italian  Duration\nwas found to be the dominant factor by a wide margin, in agreement\nwith previous studies.  Spectral Emphasis was the second most important\nfactor.  Spectral Emphasis has not been studied previously for Italian\nbut intensity, a related parameter, has been shown to correlate with\nstress.  F<SUB>0</SUB>-level was also significantly correlated but\nnot to the same degree. Speaker  Sex turned out as significant in many\ncomparisons. The differences were, however, mainly a function of the\ndegree to which a given parameter was used, not how it was used to\nsignal lexical stress contrasts.\n"
   ],
   "doi": "10.21437/Interspeech.2016-348"
  },
  "schweitzer16_interspeech": {
   "authors": [
    [
     "Antje",
     "Schweitzer"
    ],
    [
     "Ngoc Thang",
     "Vu"
    ]
   ],
   "title": "Cross-Gender and Cross-Dialect Tone Recognition for Vietnamese",
   "original": "0405",
   "page_count": 5,
   "order": 222,
   "p1": "1064",
   "pn": "1068",
   "abstract": [
    "We investigate tone recognition in Vietnamese across gender and dialects.\nIn addition to well-known parameters such as single fundamental frequency\n(F0) values and energy features, we explore the impact of harmonicity\non recognition accuracy, as well as that of the PaIntE parameters,\nwhich quantify the shape of the F0 contour over complete syllables\ninstead of providing more local single values. Using these new features\nfor tone recognition in the GlobalPhone database, we observe significant\nimprovements of approx. 1% in recognition accuracy when adding harmonicity,\nand of another approx. 4% when adding the PaIntE parameters. Furthermore,\nwe analyze the influence of gender and dialect on recognition accuracy.\nThe results show that it is easier to recognize tones for female than\nfor male speakers, and easier for the Northern dialect than for the\nSouthern dialect. Moreover, we achieve reasonable results testing models\nacross gender, while the performance drops strongly when testing across\ndialects.\n"
   ],
   "doi": "10.21437/Interspeech.2016-405"
  },
  "vijayan16_interspeech": {
   "authors": [
    [
     "Karthika",
     "Vijayan"
    ],
    [
     "K. Sri Rama",
     "Murty"
    ]
   ],
   "title": "Prosody Modification Using Allpass Residual of Speech Signals",
   "original": "0914",
   "page_count": 5,
   "order": 223,
   "p1": "1069",
   "pn": "1073",
   "abstract": [
    "In this paper, we attempt to signify the role of phase spectrum of\nspeech signals in acquiring an accurate estimate of excitation source\nfor prosody modification. The phase spectrum is parametrically modeled\nas the response of an allpass (AP) filter, and the filter coefficients\nare estimated by considering the linear prediction (LP) residual as\nthe output of the AP filter. The resultant residual signal, namely\nAP residual, exhibits unambiguous peaks corresponding to epochs, which\nare chosen as pitch markers for prosody modification. This strategy\nefficiently removes ambiguities associated with pitch marking, required\nfor pitch synchronous overlap-add (PSOLA) method. The prosody modification\nusing AP residual is advantageous than time domain PSOLA (TD-PSOLA)\nusing speech signals, as it offers fewer distortions due to its flat\nmagnitude spectrum. Windowing centered around unambiguous peaks in\nAP residual is used for segmentation, followed by pitch/duration modification\nof AP residual by mapping of pitch markers. The modified speech signal\nis obtained from modified AP residual using synthesis filters. The\nmean opinion scores are used for performance evaluation of the proposed\nmethod, and it is observed that the AP residual-based method delivers\nequivalent performance as that of LP residual-based method using epochs,\nand better performance than the linear prediction PSOLA (LP-PSOLA).\n"
   ],
   "doi": "10.21437/Interspeech.2016-914"
  },
  "kakouros16_interspeech": {
   "authors": [
    [
     "Sofoklis",
     "Kakouros"
    ],
    [
     "Joris",
     "Pelemans"
    ],
    [
     "Lyan",
     "Verwimp"
    ],
    [
     "Patrick",
     "Wambacq"
    ],
    [
     "Okko",
     "Räsänen"
    ]
   ],
   "title": "Analyzing the Contribution of Top-Down Lexical and Bottom-Up Acoustic Cues in the Detection of Sentence Prominence",
   "original": "0926",
   "page_count": 5,
   "order": 224,
   "p1": "1074",
   "pn": "1078",
   "abstract": [
    "Recent work has suggested that prominence perception could be driven\nby the predictability of the acoustic prosodic features of speech.\nOn the other hand, lexical predictability and part of speech information\nare also known to correlate with prominence. In this paper, we investigate\nhow the bottom-up acoustic and top-down lexical cues contribute to\nsentence prominence by using both types of features in unsupervised\nand supervised systems for automatic prominence detection. The study\nis conducted using a corpus of Dutch continuous speech with manually\nannotated prominence labels. Our results show that unpredictability\nof speech patterns is a consistent and important cue for prominence\nat both the lexical and acoustic levels, and also that lexical predictability\nand part-of-speech information can be used as efficient features in\nsupervised prominence classifiers.\n"
   ],
   "doi": "10.21437/Interspeech.2016-926"
  },
  "kallay16_interspeech": {
   "authors": [
    [
     "Jeffrey",
     "Kallay"
    ],
    [
     "Melissa A.",
     "Redford"
    ]
   ],
   "title": "A Longitudinal Study of Children&#8217;s Intonation in Narrative Speech",
   "original": "1396",
   "page_count": 5,
   "order": 225,
   "p1": "1079",
   "pn": "1083",
   "abstract": [
    "Adults&#8217; narratives are hierarchically structured. This structure\nis evident in the linguistic and prosodic domains. Children&#8217;s\nnarratives have a flatter structure. This structure is evident in the\nlinguistic domain, but less is known about the prosodic domain. Here,\nwe report results from a longitudinal study of children&#8217;s narratives\nthat enhance our understanding of the development of discourse prosody.\nSpontaneous narratives were obtained from 60 children (aged 5 to 7)\nover a 3-year period. F0 was tracked to obtain absolute measures of\nslope steepness and linearity for every utterance of each narrative.\nThese measures are known correlates of syntactic and semantic complexity.\nSlope direction and inter-utterance continuity in F0 were also calculated.\nThese measures are known correlates of event boundaries in adult discourse.\nThe results indicated systematic developmental changes related to age\nand year for all measures except slope steepness, consistent with developmental\nincreases in linguistic complexity and the production of more adult-like\nnarratives. The evidence also indicates that developmental change is\nmost pronounced between the ages of 5 and 7 years, and levels out afterwards.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1396"
  },
  "blaylock16_interspeech": {
   "authors": [
    [
     "Reed",
     "Blaylock"
    ],
    [
     "Louis",
     "Goldstein"
    ],
    [
     "Shrikanth S.",
     "Narayanan"
    ]
   ],
   "title": "Velum Control for Oral Sounds",
   "original": "1408",
   "page_count": 5,
   "order": 226,
   "p1": "1084",
   "pn": "1088",
   "abstract": [
    "Velum position during speech shows systematic variability within and\nacross speakers, but has a binary phonological contrast (nasal and\noral). Velum lowering is often thought to constitute an independent\nphonological unit, partly because of its robust prosodically-conditioned\ntiming during nasal stops. Velum raising, on the other hand, is usually\nconsidered to be a non-phonological consequence of other vocal tract\nmovements. Moreover, velum raising has almost always been observed\nin the context of nasals, and has rarely been studied in purely oral\ncontexts. This experiment directly contrasts velum movement in oral\nand nasal contexts. The results show that temporal coordination of\nvelum raising during oral stops resembles the temporal coordination\nof velum lowering during nasals, suggesting that velum position and\nmovement are controlled for both raising and lowering. The results\nimply that some revisions to the Articulatory Phonology model may be\nappropriate, specifically with regards to the treatment of velum raising\nas an independent phonological unit.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1408"
  },
  "son16_interspeech": {
   "authors": [
    [
     "Gayeon",
     "Son"
    ]
   ],
   "title": "F0 Development in Acquiring Korean Stop Distinction",
   "original": "0651",
   "page_count": 5,
   "order": 227,
   "p1": "1089",
   "pn": "1093",
   "abstract": [
    "A number of studies have investigated the role of Voice Onset Time\n(VOT) on acquisition of stop voicing contrast. Korean stop contrasts\n(lenis, fortis, and aspirated), however, cannot be differentiated only\nby VOT since they are all pulmonic egressive voiceless stops. For this\nthree-way distinction, another acoustic parameter, fundamental frequency\n(F0), critically operates. The present study explores how F0 is perceptually\nacquired and phonetically operates for Korean stop contrast over age.\nIn order to reveal the relationship between F0 developmental patterns\nand age, a quantitative acoustic model dealt with word-initial stop\nproductions by 58 Korean young children aged 20 months to 47 months.\nThe results showed that phonetic accuracy depends on the perceptual\nthresholds in F0, and the significant phonetic differentiation with\nF0 between lenis and aspirated stops was significantly related to age.\nThese findings suggest that acquisition of F0 plays a crucial role\nin the formation of phonemic categories for lenis and aspirated stops\nand this process significantly affects articulatory distinction.\n"
   ],
   "doi": "10.21437/Interspeech.2016-651"
  },
  "cohen16_interspeech": {
   "authors": [
    [
     "Clara",
     "Cohen"
    ],
    [
     "Matt",
     "Carlson"
    ]
   ],
   "title": "Phonetic Reduction Can Lead to Lengthening, and Enhancement Can Lead to Shortening",
   "original": "1146",
   "page_count": 5,
   "order": 228,
   "p1": "1094",
   "pn": "1098",
   "abstract": [
    "Contextually probable, high-frequency, or easily accessible words tend\nto be phonetically reduced, a pattern usually attributed to faster\nlexical access. In principle, word forms that are frequent in their\ninflectional paradigms should also enjoy faster lexical access, leading\nagain to phonetic reduction. Yet research has found evidence of both\nreduction and enhancement on paradigmatically probable inflectional\naffixes. The current corpus study uses pronunciation data from conversationally\nproduced English verbs and nouns to test the predictions of two accounts.\nIn an exemplar account, paradigmatically probable forms seem enhanced\nbecause their denser exemplar clouds resist influence from related\nword forms on the average production target. A second pressure reduces\nsuch forms because they are, after all, more easily accessed. Under\nthis account, paradigmatically probable forms should have longer affixes\nbut shorter stems. An alternative account proposes that paradigmatically\nprobable forms are produced in such a way as to enhance not articulation,\nbut  contrasts between related word forms. This account predicts lengthening\nof suffixed forms, and shortening of unsuffixed forms.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  The results of the\ncorpus study support the second account, suggesting that characterizing\npronunciation variation in terms of phonetic reduction and enhancement\noversimplifies the relationship between lexical storage, retrieval,\nand articulation.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1146"
  },
  "arai16_interspeech": {
   "authors": [
    [
     "Takayuki",
     "Arai"
    ]
   ],
   "title": "Mechanical Production of [b], [m] and [w] Using Controlled Labial and Velopharyngeal Gestures",
   "original": "0189",
   "page_count": 5,
   "order": 229,
   "p1": "1099",
   "pn": "1103",
   "abstract": [
    "As an extension of a series of models we have developed, a mechanical\nbent vocal-tract model with nasal cavity was proposed for educational\nand clinical applications, as well as for understanding human speech\nproduction. Although our recent studies have focused on flap and approximant\nsounds, this paper introduced a new model for the consonants [b], [m]\nand [w]. Because the articulatory gesture of approximants is slow compared\nto the more rapid movement of plosives, in our [b] and [m] model, the\nelastic force of a spring is applied to affect the movement of the\nlower lip block, as was done for flap sounds in our previous studies.\nThe main difference between [b] and [m] is in the velopharyngeal port,\nwhich is closed for [b] and open for [m]. In this study, we concluded\nthat 1) a slower manipulation of the lip block is needed for [w], while\n2) [b] and [m] require a faster movement, and finally, 3) close-open\ncoordination of the lip and velopharyngeal gestures is important for\n[m].\n"
   ],
   "doi": "10.21437/Interspeech.2016-189"
  },
  "fang16_interspeech": {
   "authors": [
    [
     "Qiang",
     "Fang"
    ],
    [
     "Yun",
     "Chen"
    ],
    [
     "Haibo",
     "Wang"
    ],
    [
     "Jianguo",
     "Wei"
    ],
    [
     "Jianrong",
     "Wang"
    ],
    [
     "Xiyu",
     "Wu"
    ],
    [
     "Aijun",
     "Li"
    ]
   ],
   "title": "An Improved 3D Geometric Tongue Model",
   "original": "0901",
   "page_count": 4,
   "order": 230,
   "p1": "1104",
   "pn": "1107",
   "abstract": [
    "This study describes an improved geometric articulatory model based\non MRI and CBCT (Cone Beam Computer Tomography) data. The basic idea\nis to improve the coherence of the vertices of tongue meshes so as\nto obtain more accurate tongue model. This is conducted in two aspects:\ni) The representative vertices of tongue surface are depicted in Cartesian\ncoordinate system rather than in a semi-polar gridline coordinate system.\nii) tongue surface meshes are modeled with reference to anatomical\nlandmarks. Then, guided PCA is used to extract the control components\nbased on MRI data. The average reconstruction error is less than 1.0\nmm. Both qualitative and quantitative evaluation indicates that the\nproposed method surpasses the conventional semi-polar gridline system\nbased method.\n"
   ],
   "doi": "10.21437/Interspeech.2016-901"
  },
  "tiainen16_interspeech": {
   "authors": [
    [
     "Mikko",
     "Tiainen"
    ],
    [
     "Fatima M.",
     "Felisberti"
    ],
    [
     "Kaisa",
     "Tiippana"
    ],
    [
     "Martti",
     "Vainio"
    ],
    [
     "Juraj",
     "Simko"
    ],
    [
     "Jiri",
     "Lukavsky"
    ],
    [
     "Lari",
     "Vainio"
    ]
   ],
   "title": "Congruency Effect Between Articulation and Grasping in Native English Speakers",
   "original": "1199",
   "page_count": 5,
   "order": 231,
   "p1": "1108",
   "pn": "1112",
   "abstract": [
    "Previous studies have shown congruency effects between specific speech\narticulations and manual grasping actions. For example, uttering the\nsyllable [k&#593;] facilitates power grip responses in terms of reaction\ntime and response accuracy. A similar association of the syllable [ti]\nwith precision grip has also been observed. As these congruency effects\nhave been to date shown only for Finnish native speakers, this study\nexplored whether the congruency effects generalize to native speakers\nof another language. The original experiments were therefore replicated\nwith English participants (N=16). Several previous findings were reproduced,\nnamely the association of syllables [k&#593;] and [ke] with power grip\nand of [ti] and [te] with precision grip. However, the association\nof vowels [&#593;] and [i] with power and precision grip, respectively,\npreviously found for Finnish participants, was not significant for\nEnglish speakers. This difference could be related to ambiguities of\nEnglish orthography and pronunciation variations. It is possible that\nfor English speakers seeing a certain written vowel activates several\ndifferent phonological representations associated with that letter.\nIf the congruency effects are based on interactions between specific\nphonological representations and grasp actions, this ambiguity might\nlead to weakening of the effects in the manner demonstrated here.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1199"
  },
  "najnin16b_interspeech": {
   "authors": [
    [
     "Shamima",
     "Najnin"
    ],
    [
     "Bonny",
     "Banerjee"
    ]
   ],
   "title": "Emergence of Vocal Developmental Sequences in a Predictive Coding Model of Speech Acquisition",
   "original": "1126",
   "page_count": 5,
   "order": 232,
   "p1": "1113",
   "pn": "1117",
   "abstract": [
    "Learning temporal patterns among primitive speech sequences and being\nable to control the motor apparatus for effective production of the\nlearned patterns are imperative for speech acquisition in infants.\nIn this paper, we develop a predictive coding model whose objective\nis to minimize the sensory (auditory) and proprioceptive prediction\nerrors. Temporal patterns are learned by minimizing the former while\ncontrol is learned by minimizing the latter. The model is learned using\na set of synthetically generated syllables, as in other contemporary\nmodels. We show that the proposed model outperforms existing ones in\nlearning vocalization classes. It also computes the control/muscle\nactivation which is useful for determining the degree of easiness of\nvocalization.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1126"
  },
  "meyer16_interspeech": {
   "authors": [
    [
     "Julien",
     "Meyer"
    ],
    [
     "Laure",
     "Dentel"
    ],
    [
     "Fanny",
     "Meunier"
    ]
   ],
   "title": "Categorization of Natural Spanish Whistled Vowels by Na&#239;ve Spanish Listeners",
   "original": "0379",
   "page_count": 4,
   "order": 233,
   "p1": "1118",
   "pn": "1121",
   "abstract": [
    "Whistled speech in a non tonal language consists of the natural emulation\nof vocalic and consonantal qualities in a simple modulated whistled\nsignal. This special speech register represents a natural telecommunication\nsystem that enables high levels of sentence intelligibility by trained\nspeakers. It is not directly intelligible to na&#239;ve listeners.\nYet, it is easily learned by speakers of the language that is being\nwhistled, as attested by current efforts of revitalization of whistled\nSpanish in the Canary Islands. To understand better the relation between\nwhistled and spoken speech perception, we looked here at how Spanish\nnative speakers knowing nothing about whistled speech categorized four\nSpanish whistled vowels. The results show that na&#239;ve participants\nwere able to categorize these vowels, although not as accurately as\na native whistler.\n"
   ],
   "doi": "10.21437/Interspeech.2016-379"
  },
  "voigt16_interspeech": {
   "authors": [
    [
     "Rob",
     "Voigt"
    ],
    [
     "Dan",
     "Jurafsky"
    ],
    [
     "Meghan",
     "Sumner"
    ]
   ],
   "title": "Between- and Within-Speaker Effects of Bilingualism on F0 Variation",
   "original": "1506",
   "page_count": 5,
   "order": 234,
   "p1": "1122",
   "pn": "1126",
   "abstract": [
    "To what extent is prosody shaped by cultural and social factors? Existing\nresearch has shown that an individual bilingual speaker exhibits differences\nin framing, ideology, and personality when speaking their two languages.\nTo understand whether these differences extend to prosody we study\nF0 variation in a corpus of interviews with German-Italian and German-French\nbilingual speakers. We find two primary effects. First, a  between-speaker\neffect: these two groups of bilinguals make different use of F0 even\nwhen they are all speaking German. Second, a  within-speaker effect:\nbilinguals use F0 differently depending on which language they are\nspeaking, differences that are consistent across speakers. These effects\nare modulated strongly by gender, suggesting that language-specific\nsocial positioning may play a central role. These results have important\nimplications for our understanding of bilingualism and cross-cultural\nlinguistic difference in general. Prosody appears to be a moving target\nrather than a stable feature, as speakers use prosodic variation to\nposition themselves on cultural and social axes like linguistic context\nand gender.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1506"
  },
  "graham16_interspeech": {
   "authors": [
    [
     "Calbert",
     "Graham"
    ],
    [
     "Paula",
     "Buttery"
    ],
    [
     "Francis",
     "Nolan"
    ]
   ],
   "title": "Vowel Characteristics in the Assessment of L2 English Pronunciation",
   "original": "1630",
   "page_count": 5,
   "order": 235,
   "p1": "1127",
   "pn": "1131",
   "abstract": [
    "There is considerable need to utilise linguistically meaningful measures\nof second language (L2) proficiency that are based on perceptual cues\nused by humans to assess pronunciation. Previous research on non-native\nacquisition of vowel systems suggests a strong link between vowel production\naccuracy and speech intelligibility. It is well known that the acoustic\nand perceptual identification of vowels rely on formant frequencies.\nHowever, formant analysis may not be viable in large-scale corpus research,\ngiven the need for manual correction of tracking errors. Spectral analysis\ntechniques have been shown to be a robust alternative to formant tracking.\nThis paper explores the use of one such technique &#8212; the discrete\ncosine transform (DCT) &#8212; for modelling English vowel spectra\nin the productions of non-native English speakers. Mel-scaled DCT coefficients\nwere calculated over a frequency band of 200&#8211;4000 Hz. Results\nshow a statistically significant correlation between coefficients and\nthe proficiency level of speakers, and suggest that this technique\nholds some promise in automated L2 pronunciation teaching and assessment.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1630"
  },
  "geneid16_interspeech": {
   "authors": [
    [
     "Ahmed",
     "Geneid"
    ],
    [
     "Anne-Maria",
     "Laukkanen"
    ],
    [
     "Anita",
     "McAllister"
    ],
    [
     "Robert",
     "Eklund"
    ]
   ],
   "title": "Kulning (Swedish Cattle Calls): Acoustic, EGG, Stroboscopic and High-Speed Video Analyses of an Unusual Singing Style",
   "original": "1082",
   "page_count": 4,
   "order": 236,
   "p1": "1132",
   "pn": "1135",
   "abstract": [
    "The Swedish cattle call singing style &#8216;kulning&#8217; is surprisingly\nunderstudied, despite its mythical status in folklore. While some acoustic\nand physiological aspects have been addressed previously [1,2], a more\ndetailed analysis is still lacking. Previous work [2] showed that sound\npressure level (SPL) in kulning tapered off less than in head register\nas a function of distance, which warrants a study of underlying physiological\nmechanisms responsible for this. In the present paper, the same singer,\nsinging the same song &#8212; in kulning and in head register (&#8220;falsetto&#8221;)\nmode &#8212; was recorded indoors. Electroglottographic (EGG), stroboscopic,\nhigh-speed endoscopic and audio registrations were made. Analyses examined\ndifferences between kulning and head register. Results show somewhat\nhigher SPL in kulning than in head register confirming the previous\nfindings. EGG showed longer relative glottal closed time and higher\namplitude of the signal in kulning. This suggests better vocal fold\ncontact in kulning. Flexible nasofiberoscopy and high-speed recordings\nduring kulning showed medial and antero-posterior narrowing of the\nlaryngeal inlet, a clear approximation of the false vocal folds and\nmarked adduction of the vocal folds.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1082"
  },
  "hejna16_interspeech": {
   "authors": [
    [
     "Míša",
     "Hejná"
    ],
    [
     "Pertti",
     "Palo"
    ],
    [
     "Scott",
     "Moisik"
    ]
   ],
   "title": "Glottal Squeaks in VC Sequences",
   "original": "1496",
   "page_count": 5,
   "order": 237,
   "p1": "1136",
   "pn": "1140",
   "abstract": [
    "This paper reports results related to the phenomenon referred to as\na &#8220;glottal squeak&#8221; (coined by [1]). At present, nothing\nis known about the conditioning and the articulation of this feature\nof speech. Our qualitative acoustic analyses of the conditioning of\nsqueaks (their frequency of occurrence, duration, and f<SUB>0</SUB>)\nfound in Aberystwyth English and Manchester English suggest that squeaking\nmay be a result of intrinsically tense vocal fold state associated\nwith thyroarytenoid (TA) muscle recruitment [2] required for epilaryngeal\nconstriction and vocal-ventricular fold contact (VVFC) needed to produce\nglottalisation [3]. In this interpretation, we hypothesise that squeaks\noccasionally occur during constriction disengagement: at the point\nwhen VVFC suddenly releases but the TAs have not yet fully relaxed.\nExtralinguistic conditioning identified in this study corroborates\nfindings reported by [1]: females are more prone to squeaking and the\nphenomenon is individual-dependent.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1496"
  },
  "takahashi16_interspeech": {
   "authors": [
    [
     "Naoya",
     "Takahashi"
    ],
    [
     "Tofigh",
     "Naghibi"
    ],
    [
     "Beat",
     "Pfister"
    ]
   ],
   "title": "Automatic Pronunciation Generation by Utilizing a Semi-Supervised Deep Neural Networks",
   "original": "0761",
   "page_count": 5,
   "order": 238,
   "p1": "1141",
   "pn": "1145",
   "abstract": [
    "Phonemic or phonetic sub-word units are the most commonly used atomic\nelements to represent speech signals in modern ASRs. However they are\nnot the optimal choice due to several reasons such as: large amount\nof effort required to handcraft a pronunciation dictionary, pronunciation\nvariations, human mistakes and under-resourced dialects and languages.\nHere, we propose a data-driven pronunciation estimation and acoustic\nmodeling method which only takes the orthographic transcription to\njointly estimate a set of sub-word units and a reliable dictionary.\nExperimental results show that the proposed method which is based on\nsemi-supervised training of a deep neural network largely outperforms\nphoneme based continuous speech recognition on the TIMIT dataset.\n"
   ],
   "doi": "10.21437/Interspeech.2016-761"
  },
  "liu16f_interspeech": {
   "authors": [
    [
     "Xiaohu",
     "Liu"
    ],
    [
     "Ruhi",
     "Sarikaya"
    ],
    [
     "Liang",
     "Zhao"
    ],
    [
     "Yong",
     "Ni"
    ],
    [
     "Yi-Cheng",
     "Pan"
    ]
   ],
   "title": "Personalized Natural Language Understanding",
   "original": "1172",
   "page_count": 5,
   "order": 239,
   "p1": "1146",
   "pn": "1150",
   "abstract": [
    "Natural language understanding (NLU) is one of the critical components\nof dialog systems. Its aim is to extract semantic meaning from typed\ntext input or the spoken text coming out of the speech recognizer.\nTraditionally, NLU systems are built in a user-independent fashion,\nwhere the system behavior does not adapt to the user. However, personal\ninformation can be very useful for language understanding tasks, if\nit is made available to the system. With personal digital assistant\n(PDA) systems, many forms of personal data are readily available for\nthe NLU systems to make the models and the system more personal. In\nthis paper, we propose a method to personalize language understanding\nmodels by making use of the personal data with privacy respected and\nprotected. We report experiments on two domains for intent classification\nand slot tagging, where we achieve significant accuracy improvements\ncompared to the baseline models that are trained in a user independent\nmanner.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1172"
  },
  "asri16_interspeech": {
   "authors": [
    [
     "Layla El",
     "Asri"
    ],
    [
     "Jing",
     "He"
    ],
    [
     "Kaheer",
     "Suleman"
    ]
   ],
   "title": "A Sequence-to-Sequence Model for User Simulation in Spoken Dialogue Systems",
   "original": "1175",
   "page_count": 5,
   "order": 240,
   "p1": "1151",
   "pn": "1155",
   "abstract": [
    "User simulation is essential for generating enough data to train a\nstatistical spoken dialogue system. Previous models for user simulation\nsuffer from several drawbacks, such as the inability to take dialogue\nhistory into account, the need of rigid structure to ensure coherent\nuser behaviour, heavy dependence on a specific domain, the inability\nto output several user intentions during one dialogue turn, or the\nrequirement of a summarized action space for tractability. This paper\nintroduces a data-driven user simulator based on an encoder-decoder\nrecurrent neural network. The model takes as input a sequence of dialogue\ncontexts and outputs a sequence of dialogue acts corresponding to user\nintentions. The dialogue contexts include information about the machine\nacts and the status of the user goal. We show on the Dialogue State\nTracking Challenge 2 (DSTC2) dataset that the sequence-to-sequence\nmodel outperforms an agenda-based simulator and an n-gram simulator,\naccording to F-score. Furthermore, we show how this model can be used\non the original action space and thereby models user behaviour with\nfiner granularity.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1175"
  },
  "georgiladakis16_interspeech": {
   "authors": [
    [
     "Spiros",
     "Georgiladakis"
    ],
    [
     "Georgia",
     "Athanasopoulou"
    ],
    [
     "Raveesh",
     "Meena"
    ],
    [
     "José",
     "Lopes"
    ],
    [
     "Arodami",
     "Chorianopoulou"
    ],
    [
     "Elisavet",
     "Palogiannidi"
    ],
    [
     "Elias",
     "Iosif"
    ],
    [
     "Gabriel",
     "Skantze"
    ],
    [
     "Alexandros",
     "Potamianos"
    ]
   ],
   "title": "Root Cause Analysis of Miscommunication Hotspots in Spoken Dialogue Systems",
   "original": "1273",
   "page_count": 5,
   "order": 241,
   "p1": "1156",
   "pn": "1160",
   "abstract": [
    "A major challenge in Spoken Dialogue Systems (SDS) is the detection\nof problematic communication (hotspots), as well as the classification\nof these hotspots into different types (root cause analysis). In this\nwork, we focus on two classes of root cause, namely, erroneous speech\nrecognition vs. other (e.g., dialogue strategy). Specifically, we propose\nan automatic algorithm for detecting hotspots and classifying root\ncauses in two subsequent steps. Regarding hotspot detection, various\nlexico-semantic features are used for capturing repetition patterns\nalong with affective features. Lexico-semantic and repetition features\nare also employed for root cause analysis. Both algorithms are evaluated\nwith respect to the Let&#8217;s Go dataset (bus information system).\nIn terms of classification unweighted average recall, performance of\n80% and 70% is achieved for hotspot detection and root cause analysis,\nrespectively.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1273"
  },
  "khan16_interspeech": {
   "authors": [
    [
     "Omar Zia",
     "Khan"
    ],
    [
     "Ruhi",
     "Sarikaya"
    ]
   ],
   "title": "Making Personal Digital Assistants Aware of What They Do Not Know",
   "original": "1534",
   "page_count": 5,
   "order": 242,
   "p1": "1161",
   "pn": "1165",
   "abstract": [
    "Personal digital assistants (PDAs) are spoken (and typed) dialog systems\nthat are expected to assist users without being constrained to a particular\ndomain. Typically, it is possible to construct deep multi-domain dialog\nsystems focused on a narrow set of head domains. For the long tail\n(or when the speech recognition is not correct) the PDA does not know\nwhat to do. Two common fallback approaches are to either acknowledge\nits limitation or show web search results. Either approach can severely\nundermine the user&#8217;s trust in the PDA&#8217;s intelligence if\ninvoked at the wrong time. In this paper, we propose features that\nare helpful in predicting the right fallback response. We then use\nthese features to construct dialog policies such that the PDA is able\nto correctly decide between invoking web search or acknowledging its\nlimitation. We evaluate these dialog policies on real user logs gathered\nfrom a PDA, deployed to millions of users, using both offline (judged)\nand online (user-click) metrics. We demonstrate that our hybrid dialog\npolicy significantly increases the accuracy of choosing the correct\nresponse, measured by analyzing click-rate in logs, and also enhances\nuser satisfaction, measured by human evaluations of the replayed experience.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1534"
  },
  "levitan16_interspeech": {
   "authors": [
    [
     "Rivka",
     "Levitan"
    ],
    [
     "Štefan",
     "Beňuš"
    ],
    [
     "Ramiro H.",
     "Gálvez"
    ],
    [
     "Agustín",
     "Gravano"
    ],
    [
     "Florencia",
     "Savoretti"
    ],
    [
     "Marian",
     "Trnka"
    ],
    [
     "Andreas",
     "Weise"
    ],
    [
     "Julia",
     "Hirschberg"
    ]
   ],
   "title": "Implementing Acoustic-Prosodic Entrainment in a Conversational Avatar",
   "original": "0985",
   "page_count": 5,
   "order": 243,
   "p1": "1166",
   "pn": "1170",
   "abstract": [
    "Entrainment, aka accommodation or alignment, is the phenomenon by which\nconversational partners become more similar to each other in behavior.\nWhile there has been much work on some behaviors there has been little\non entrainment in speech and even less on how Spoken Dialogue Systems\n(SDS) which entrain to their users&#8217; speech can be created. We\npresent an architecture and algorithm for implementing acoustic-prosodic\nentrainment in SDS and show that speech produced under this algorithm\nconforms to the feature targets, satisfying the properties of entrainment\nbehavior observed in human-human conversations. We present results\nof an extrinsic evaluation of this method, comparing whether subjects\nare more likely to ask advice from a conversational avatar that entrains\nvs. one that does not, in English, Spanish and Slovak SDS.\n"
   ],
   "doi": "10.21437/Interspeech.2016-985"
  },
  "silvervarg16_interspeech": {
   "authors": [
    [
     "Annika",
     "Silvervarg"
    ],
    [
     "Sofia",
     "Lindvall"
    ],
    [
     "Jonatan",
     "Andersson"
    ],
    [
     "Ida",
     "Esberg"
    ],
    [
     "Christian",
     "Jernberg"
    ],
    [
     "Filip",
     "Frumerie"
    ],
    [
     "Arne",
     "Jönsson"
    ]
   ],
   "title": "Perceived Usability and Cognitive Demand of Secondary Tasks in Spoken Versus Visual-Manual Automotive Interaction",
   "original": "0099",
   "page_count": 5,
   "order": 244,
   "p1": "1171",
   "pn": "1175",
   "abstract": [
    "We present results from a study of truck drivers&#8217; experience\nof using two different interfaces; spoken interaction and visual-manual\ninteraction, to perform secondary tasks while driving. The instruments\nused to measure their experience are based on three popular questionnaires,\nmeasuring different aspects of usability and cognitive load: SASSI,\nSUS and DALI. Our results show that the speech interface is preferred\nboth regarding usability and cognitive demand.\n"
   ],
   "doi": "10.21437/Interspeech.2016-99"
  },
  "fung16_interspeech": {
   "authors": [
    [
     "Pascale",
     "Fung"
    ],
    [
     "Anik",
     "Dey"
    ],
    [
     "Farhad Bin",
     "Siddique"
    ],
    [
     "Ruixi",
     "Lin"
    ],
    [
     "Yang",
     "Yang"
    ],
    [
     "Wan",
     "Yan"
    ],
    [
     "Ricky Ho Yin",
     "Chan"
    ]
   ],
   "title": "Zara: An Empathetic Interactive Virtual Agent",
   "original": "2012",
   "page_count": 2,
   "order": 245,
   "p1": "1176",
   "pn": "1177",
   "abstract": [
    "Zara, or &#8216;Zara the Supergirl&#8217;, is a virtual robot that\ncan show empathy while interacting with an user, and at the end of\na 5&#8211;10 minute conversation, it can give a personality analysis\nbased on the user responses. It can display and share emotions with\nthe aid of its built in sentiment analysis, facial and emotion recognition,\nand speech module. Being the first of its kind, it has successfully\nintegrated an empathetic system along with the human emotion recognition\nand sharing, into an augmented human-robot interaction system. Zara\nwas also displayed at the World Economic Forum held at Dalian in September\n2015. \n"
   ]
  },
  "tejedorgarcia16_interspeech": {
   "authors": [
    [
     "Cristian",
     "Tejedor-García"
    ],
    [
     "David",
     "Escudero-Mancebo"
    ],
    [
     "Enrique",
     "Cámara-Arenas"
    ],
    [
     "César",
     "González-Ferreras"
    ],
    [
     "Valentín",
     "Cardeñoso-Payo"
    ]
   ],
   "title": "Measuring Pronunciation Improvement in Users of CAPT Tool TipTopTalk!",
   "original": "2013",
   "page_count": 2,
   "order": 246,
   "p1": "1178",
   "pn": "1179",
   "abstract": [
    "We present a L2 pronunciation training serious game based on the minimal-pairs\ntechnique, incorporating sequences of exposure, discrimination and\nproduction, and using text-to-speech and speech recognition systems.\nWe have measured the quality of users&#8217; production during a period\nof time in order to assess improvement after using the application.\nSubstantial improvement is found among users with poorer initial performance\nlevels. The program&#8217;s gamification resources manage to engage\na high percentage of users. A need is felt to include feedback for\nusers in future versions with the purpose of increasing their performance\nand avoiding the performance drop detected after protracted use of\nthe tool.\n"
   ]
  },
  "kawahara16_interspeech": {
   "authors": [
    [
     "Hideki",
     "Kawahara"
    ]
   ],
   "title": "SparkNG: Interactive MATLAB Tools for Introduction to Speech Production, Perception and Processing Fundamentals and Application of the Aliasing-Free L-F Model Component",
   "original": "2014",
   "page_count": 2,
   "order": 247,
   "p1": "1180",
   "pn": "1181",
   "abstract": [
    "This article introduces a set of interactive tools for studying fundamentals\nof speech production, perception and processing. In addition to this\nvoice production simulator, it consists of interactive time-frequency\nrepresentation, auditory representation visualizer and a vocal tract\nshape visualizer for introductory materials. It consists of compiled\nexecutables for Windows and Mac environment, which do not require MATLAB\nlicense. The MATLAB sources of the tools and their constituent functions\nare publicly accessible under open source license.\n"
   ]
  },
  "marchi16_interspeech": {
   "authors": [
    [
     "Erik",
     "Marchi"
    ],
    [
     "Florian",
     "Eyben"
    ],
    [
     "Gerhard",
     "Hagerer"
    ],
    [
     "Björn",
     "Schuller"
    ]
   ],
   "title": "Real-Time Tracking of Speakers&#8217; Emotions, States, and Traits on Mobile Platforms",
   "original": "2015",
   "page_count": 2,
   "order": 248,
   "p1": "1182",
   "pn": "1183",
   "abstract": [
    "We demonstrate audEERING&#8217;s sensAI technology running natively\non low-resource mobile devices applied to emotion analytics and speaker\ncharacterisation tasks. A show-case application for the Android platform\nis provided, where audEERING&#8217;s highly noise robust voice activity\ndetection based on LSTM-RNN is combined with our core emotion recognition\nand speaker characterisation engine natively on the mobile device.\nThis eliminates the need for network connectivity and allows to perform\nrobust speaker state and trait recognition efficiently in real-time\nwithout network transmission lags. Real-time factors are benchmarked\nfor a popular mobile device to demonstrate the efficiency, and average\nresponse times are compared to a server based approach. The output\nof the emotion analysis is visualized graphically in the arousal and\nvalence space alongside the emotion category and further speaker characteristics.\n"
   ]
  },
  "mirghafori16_interspeech": {
   "authors": [
    [
     "Nikki",
     "Mirghafori"
    ]
   ],
   "title": "Mindfulness Special Event",
   "original": "abs2",
   "page_count": 0,
   "order": 249,
   "p1": "0",
   "pn": "",
   "abstract": [
    "Mindfulness has entered the cultural mainstream in recent years, with\nclasses and workshops offered on the topic at many universities and\ncompanies (including Google, Facebook, etc.). Mindfulness can be thought\nof as a way to train our mind to be fully present with this moment&#8217;s\nexperience with curiosity, kindness, and equanimity. The training can\nserve as a refuge in our busy professional lives and help build resilience.\nThis special event will be in the form of a guided meditation and serve\nas an introduction for those who are new to this practice, and a chance\nto practice in community for those who have previous experience. Everyone\nis welcome.\n"
   ]
  },
  "chang16b_interspeech": {
   "authors": [
    [
     "Edward",
     "Chang"
    ]
   ],
   "title": "The Human Speech Cortex",
   "original": "3002",
   "page_count": 1,
   "order": 250,
   "p1": "1184",
   "pn": "1184",
   "abstract": [
    "A unique and defining trait of human behavior is our ability to communicate\nthrough speech. The fundamental organizational principles of the neural\ncircuits within speech brain areas are largely unknown. In this talk,\nI will present new results from our research on the functional organization\nof the human higher-order auditory cortex, known as Wernicke&#8217;s\narea. I will focus on how neural populations in the superior temporal\nlobe encode acoustic-phonetic representations of speech, and also how\nthey integrate influences of linguistic context to achieve perceptual\nrobustness.\n"
   ]
  },
  "bonastre16_interspeech": {
   "authors": [
    [
     "Jean-François",
     "Bonastre"
    ],
    [
     "Joseph P.",
     "Campbell"
    ],
    [
     "Anders",
     "Eriksson"
    ],
    [
     "Hiro",
     "Nakasone"
    ],
    [
     "Reva",
     "Schwartz"
    ]
   ],
   "title": "Speaker Comparison for Forensic and Investigative Applications II",
   "original": "abs3",
   "page_count": 0,
   "order": 251,
   "p1": "0",
   "pn": "",
   "abstract": [
    "The aim of this special event is to have several structured discussions\non speaker comparison for forensic and investigative applications,\nwhere many international experts will present their views and participate\nin the free exchange of ideas. In speaker comparison, speech samples\nare compared by humans and/or machines for use in investigations or\nin court to address questions that are of interest to the legal system.\nSpeaker comparison is a high-stakes application that can change people&#8217;s\nlives and it demands the best that science has to offer; however, methods,\nprocesses, and practices vary widely. These variations are not necessarily\nfor the better and, although recognized, are not generally appreciated\nand acted upon. Methods, processes, and practices grounded in science\nare critical for the proper application (and nonapplication) of speaker\ncomparison to a variety of international investigative and forensic\napplications. This event follows the successful Interspeech 2015 special\nevent of the same name.\n"
   ]
  },
  "bone16_interspeech": {
   "authors": [
    [
     "Daniel",
     "Bone"
    ],
    [
     "Somer",
     "Bishop"
    ],
    [
     "Rahul",
     "Gupta"
    ],
    [
     "Sungbok",
     "Lee"
    ],
    [
     "Shrikanth S.",
     "Narayanan"
    ]
   ],
   "title": "Acoustic-Prosodic and Turn-Taking Features in Interactions with Children with Neurodevelopmental Disorders",
   "original": "1073",
   "page_count": 5,
   "order": 252,
   "p1": "1185",
   "pn": "1189",
   "abstract": [
    "Atypical speech prosody is a hallmark feature of autism spectrum disorder\n(ASD) that presents across the lifespan, but is difficult to reliably\ncharacterize qualitatively. Given the great heterogeneity of symptoms\nin ASD, an acoustic-based objective measure would be vital for clinical\nassessment and interventions. In this study, we investigate speech\nfeatures in child-psychologist conversational samples, including: segmental\nand suprasegmental pitch dynamics, speech rate, coordination of prosodic\nattributes, and turn-taking. Data consist of 95 children with ASD as\nwell as 81 controls with non-ASD developmental disorders. We demonstrate\nsignificant predictive performance using these features as well as\ninterpret feature correlations of both interlocutors. The most robust\nfinding is that segmental and suprasegmental prosodic variability increases\nfor both participants in interactions with children having higher ASD\nseverity. Recommendations for future research towards a fully-automatic\nquantitative measure of speech prosody in neurodevelopmental disorders\nare discussed.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1073"
  },
  "hemmerling16_interspeech": {
   "authors": [
    [
     "Daria",
     "Hemmerling"
    ],
    [
     "Juan Rafael",
     "Orozco-Arroyave"
    ],
    [
     "Andrzej",
     "Skalski"
    ],
    [
     "Janusz",
     "Gajda"
    ],
    [
     "Elmar",
     "Nöth"
    ]
   ],
   "title": "Automatic Detection of Parkinson&#8217;s Disease Based on Modulated Vowels",
   "original": "1062",
   "page_count": 5,
   "order": 253,
   "p1": "1190",
   "pn": "1194",
   "abstract": [
    "In this paper we present a novel approach of automatic detection of\nphonatory and articulatory impairments caused by Parkinson&#8217;s\ndisease (PD). Modulated (varying between low and high pitch) and sustained\nvowels are considered and analysed. The fundamental frequency of the\nphonations and its range are computed using the Hilbert-Huang transformation.\nAdditionally, a set with &#8220;standard&#8221; measures are calculated\nto model phonatory and articulatory deficits exhibited by Parkinson&#8217;s\npatients. Kernel Principal Component Analysis was also applied in order\nto reduce the dimensionality of the representation space. The automatic\ndiscrimination between speakers with PD and healthy controls (HC) is\nperformed using decision trees. According to the results, modulated\nvowels are suitable to evaluate phonatory and articulatory deficits\nobserved in PD speech.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1062"
  },
  "wang16d_interspeech": {
   "authors": [
    [
     "Jun",
     "Wang"
    ],
    [
     "Prasanna V.",
     "Kothalkar"
    ],
    [
     "Beiming",
     "Cao"
    ],
    [
     "Daragh",
     "Heitzman"
    ]
   ],
   "title": "Towards Automatic Detection of Amyotrophic Lateral Sclerosis from Speech Acoustic and Articulatory Samples",
   "original": "1542",
   "page_count": 5,
   "order": 254,
   "p1": "1195",
   "pn": "1199",
   "abstract": [
    "Amyotrophic lateral sclerosis (ALS) is a rapid neurodegenerative disease\nthat affects the speech motor functions of patients, thus causes dysarthria.\nThere is no definite marker for the diagnosis of ALS. Currently, the\ndiagnosis of ALS is primarily based on clinical observations of upper\nand lower motor neuron damage in the absence of other causes, which\nis time-consuming, of high cost, and often delayed. Timely diagnosis\nand assessment for ALS are crucial. Automatic detection of ALS from\nspeech samples would advance the diagnosis of ALS. In this paper, we\ninvestigated the automatic detection of ALS from short, pre-symptom\nspeech acoustic and articulatory samples using machine learning approaches\n(support vector machine and deep neural network). A data set of more\nthan 2,500 speech samples collected from eleven patients with ALS and\neleven healthy speakers was used. Leave-subjects-out cross validation\nexperimental results indicate the feasibility of the automatic detection\nof ALS from speech samples. Adding articulatory motion information\n(from tongue and lips) further improved the detection performance.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1542"
  },
  "ciccarelli16_interspeech": {
   "authors": [
    [
     "Gregory",
     "Ciccarelli"
    ],
    [
     "Thomas F.",
     "Quatieri"
    ],
    [
     "Satrajit S.",
     "Ghosh"
    ]
   ],
   "title": "Neurophysiological Vocal Source Modeling for Biomarkers of Disease",
   "original": "0292",
   "page_count": 5,
   "order": 255,
   "p1": "1200",
   "pn": "1204",
   "abstract": [
    "Speech is potentially a rich source of biomarkers for detecting and\nmonitoring neuropsychological disorders. Current biomarkers typically\ncomprise acoustic descriptors extracted from behavioral measures of\nsource, filter, prosodic and linguistic cues. In contrast, in this\npaper, we extract vocal features based on a neurocomputational model\nof speech production, reflecting latent or internal motor control parameters\nthat may be more sensitive to individual variation under neuropsychological\ndisease. These features, which are constrained by neurophysiology,\nmay be resilient to artifacts and provide an articulatory complement\nto acoustic features. Our features represent a mapping from a low-dimensional\nacoustics-based feature space to a high-dimensional space that captures\nthe underlying neural process including articulatory commands and auditory\nand somatosensory feedback errors. In particular, we demonstrate a\nneurophysiological vocal source model that generates biomarkers of\ndisease by modeling vocal source control. By using the fundamental\nfrequency contour and a biophysical representation of the vocal source,\nwe infer two neuromuscular time series whose coordination provides\nvocal features that are applied to depression and Parkinson&#8217;s\ndisease as examples. These vocal source coordination features alone,\non a single held vowel, outperform or are comparable to other features\nsets and reflect a significant compression of the feature space.\n"
   ],
   "doi": "10.21437/Interspeech.2016-292"
  },
  "horwitzmartin16_interspeech": {
   "authors": [
    [
     "Rachelle L.",
     "Horwitz-Martin"
    ],
    [
     "Thomas F.",
     "Quatieri"
    ],
    [
     "Adam C.",
     "Lammert"
    ],
    [
     "James R.",
     "Williamson"
    ],
    [
     "Yana",
     "Yunusova"
    ],
    [
     "Elizabeth",
     "Godoy"
    ],
    [
     "Daryush D.",
     "Mehta"
    ],
    [
     "Jordan R.",
     "Green"
    ]
   ],
   "title": "Relation of Automatically Extracted Formant Trajectories with Intelligibility Loss and Speaking Rate Decline in Amyotrophic Lateral Sclerosis",
   "original": "0403",
   "page_count": 5,
   "order": 256,
   "p1": "1205",
   "pn": "1209",
   "abstract": [
    "Effective monitoring of bulbar disease progression in persons with\namyotrophic lateral sclerosis (ALS) requires rapid, objective, automatic\nassessment of speech loss. The purpose of this work was to identify\nacoustic features that aid in predicting intelligibility loss and speaking\nrate decline in individuals with ALS. Features were derived from statistics\nof the first (F<SUB>1</SUB>) and second (F<SUB>2</SUB>) formant frequency\ntrajectories and their first and second derivatives. Motivated by a\npossible link between components of formant dynamics and specific articulator\nmovements, these features were also computed for low-pass and high-pass\nfiltered formant trajectories. When compared to clinician-rated intelligibility\nand speaking rate assessments, F<SUB>2</SUB> features, particularly\nmean F<SUB>2</SUB> speed and a novel feature, mean F<SUB>2</SUB> acceleration,\nwere most strongly correlated with intelligibility and speaking rate,\nrespectively (Spearman correlations &#62; 0.70, p &#60; 0.0001). These\nfeatures also yielded the best predictions in regression experiments\n(r &#62; 0.60, p &#60; 0.0001). Comparable results were achieved using\nlow-pass filtered F<SUB>2</SUB> trajectory features, with higher correlations\nand lower prediction errors achieved for speaking rate over intelligibility.\nThese findings suggest information can be exploited in specific frequency\ncomponents of formant trajectories, with implications for automatic\nmonitoring of ALS.\n"
   ],
   "doi": "10.21437/Interspeech.2016-403"
  },
  "ringeval16_interspeech": {
   "authors": [
    [
     "Fabien",
     "Ringeval"
    ],
    [
     "Erik",
     "Marchi"
    ],
    [
     "Charline",
     "Grossard"
    ],
    [
     "Jean",
     "Xavier"
    ],
    [
     "Mohamed",
     "Chetouani"
    ],
    [
     "David",
     "Cohen"
    ],
    [
     "Björn",
     "Schuller"
    ]
   ],
   "title": "Automatic Analysis of Typical and Atypical Encoding of Spontaneous Emotion in the Voice of Children",
   "original": "0766",
   "page_count": 5,
   "order": 257,
   "p1": "1210",
   "pn": "1214",
   "abstract": [
    "Children with Autism Spectrum Disorders (ASD) present significant difficulties\nto understand and express emotions. Systems have thus been proposed\nto provide objective measurements of acoustic features used by children\nsuffering from ASD to encode emotion in speech. However, only a few\nstudies have exploited such systems to compare different groups of\nchildren in their ability to express emotions, and even less have focused\non the analysis of spontaneous emotion. In this contribution, we provide\ninsights by extensive evaluations carried out on a new database of\nspontaneous speech inducing three emotion categories of valence (positive,\nneutral, and negative). We evaluate the potential of using an automatic\nrecognition system to differentiate groups of children, i.e., pervasive\ndevelopmental disorders, pervasive developmental disorders not-otherwise\nspecified, specific language impairments, and typically developing,\nin their abilities to express spontaneous emotion in a common unconstrained\ntask. Results show that all groups of children can be differentiated\ndirectly (diagnosis recognition) and indirectly (emotion recognition)\nby the proposed system.\n"
   ],
   "doi": "10.21437/Interspeech.2016-766"
  },
  "khorram16_interspeech": {
   "authors": [
    [
     "Soheil",
     "Khorram"
    ],
    [
     "John",
     "Gideon"
    ],
    [
     "Melvin",
     "McInnis"
    ],
    [
     "Emily Mower",
     "Provost"
    ]
   ],
   "title": "Recognition of Depression in Bipolar Disorder: Leveraging Cohort and Person-Specific Knowledge",
   "original": "0837",
   "page_count": 5,
   "order": 258,
   "p1": "1215",
   "pn": "1219",
   "abstract": [
    "Individuals with bipolar disorder typically exhibit changes in the\nacoustics of their speech. Mobile health systems seek to model these\nchanges to automatically detect and correctly identify current states\nin an individual and to ultimately predict impending mood episodes.\nWe have developed a program, PRIORI (Predicting Individual Outcomes\nfor Rapid Intervention), that analyzes acoustics of speech as predictors\nof mood states from mobile smartphone data. Mood prediction systems\ngenerally assume that the symptomatology of an individual can be modeled\nusing patterns common in a cohort population due to limitations in\nthe size of available datasets. However, individuals are unique. This\npaper explores person-level systems that can be developed from the\ncurrent PRIORI database of an extensive and longitudinal collection\ncomposed of two subsets: a smaller labeled portion and a larger unlabeled\nportion. The person-level system employs the unlabeled portion to extract\ni-vectors, which characterize single individuals. The labeled portion\nis then used to train person-level and population-level supervised\nclassifiers, operating on the i-vectors and on speech rhythm statistics,\nrespectively. The unification of these two approaches results in a\nsignificant improvement over the baseline system, demonstrating the\nimportance of a multi-level approach to capturing depression symptomatology.\n"
   ],
   "doi": "10.21437/Interspeech.2016-837"
  },
  "mirheidari16_interspeech": {
   "authors": [
    [
     "Bahman",
     "Mirheidari"
    ],
    [
     "Daniel",
     "Blackburn"
    ],
    [
     "Markus",
     "Reuber"
    ],
    [
     "Traci",
     "Walker"
    ],
    [
     "Heidi",
     "Christensen"
    ]
   ],
   "title": "Diagnosing People with Dementia Using Automatic Conversation Analysis",
   "original": "0857",
   "page_count": 5,
   "order": 259,
   "p1": "1220",
   "pn": "1224",
   "abstract": [
    "A recent study using Conversation Analysis (CA) has demonstrated that\ncommunication problems may be picked up during conversations between\npatients and neurologists, and that this can be used to differentiate\nbetween patients with (progressive neurodegenerative dementia) ND and\nthose with (nonprogressive) functional memory disorders (FMD). This\npaper presents a novel automatic method for transcribing such conversations\nand extracting CA-style features. A range of acoustic, syntactic, semantic\nand visual features were automatically extracted and used to train\na set of classifiers. In a proof-of-principle style study, using data\nrecording during real neurologist-patient consultations, we demonstrate\nthat automatically extracting CA-style features gives a classification\naccuracy of 95%when using verbatim transcripts. Replacing those transcripts\nwith automatic speech recognition transcripts, we obtain a classification\naccuracy of 79% which improves to 90% when feature selection is applied.\nThis is a first and encouraging step towards replacing inaccurate,\npotentially stressful cognitive tests with a test based on monitoring\nconversation capabilities that could be conducted in e.g. the privacy\nof the patient&#8217;s own home.\n"
   ],
   "doi": "10.21437/Interspeech.2016-857"
  },
  "chan16_interspeech": {
   "authors": [
    [
     "Paul Yaozhu",
     "Chan"
    ],
    [
     "Minghui",
     "Dong"
    ],
    [
     "Grace Xue Hui",
     "Ho"
    ],
    [
     "Haizhou",
     "Li"
    ]
   ],
   "title": "SERAPHIM: A Wavetable Synthesis System with 3D Lip Animation for Real-Time Speech and Singing Applications on Mobile Platforms",
   "original": "0484",
   "page_count": 5,
   "order": 260,
   "p1": "1225",
   "pn": "1229",
   "abstract": [
    "Singing synthesis is a rising musical art form gaining popularity amongst\ncomposers and end-listeners alike. To date, this art form is largely\nconfined to offline boundaries of the music studio, whereas a large\npart music is about live performances. This calls for a real-time synthesis\nsystem readily deployable for onstage applications.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  SERAPHIM is a wavetable\nsynthesis system that is lightweight and deployable on mobile platforms.\nApart from conventional offline studio applications, SERAPHIM also\nsupports real-time synthesis applications, enabling live control inputs\nfor on-stage performances. It also provides for easy lip animation\ncontrol. SERAPHIM will be made available as a toolbox on Unity 3D for\neasy adoption into game development across multiple platforms. A readily\ncompiled version will also be deployed as a VST studio plugin, directly\naddressing end users. It currently supports Japanese (singing only)\nand Mandarin (speech and singing) languages. This paper describes our\nwork on SERAPHIM and discusses its capabilities and applications.\n"
   ],
   "doi": "10.21437/Interspeech.2016-484"
  },
  "bonada16_interspeech": {
   "authors": [
    [
     "Jordi",
     "Bonada"
    ],
    [
     "Martí",
     "Umbert"
    ],
    [
     "Merlijn",
     "Blaauw"
    ]
   ],
   "title": "Expressive Singing Synthesis Based on Unit Selection for the Singing Synthesis Challenge 2016",
   "original": "0872",
   "page_count": 5,
   "order": 261,
   "p1": "1230",
   "pn": "1234",
   "abstract": [
    "Sample and statistically based singing synthesizers typically require\na large amount of data for automatically generating expressive synthetic\nperformances. In this paper we present a singing synthesizer that using\ntwo rather small databases is able to generate expressive synthesis\nfrom an input consisting of notes and lyrics. The system is based on\nunit selection and uses the Wide-Band Harmonic Sinusoidal Model for\ntransforming samples. The first database focuses on expression and\nconsists of less than 2 minutes of free expressive singing using solely\nvowels. The second one is the timbre database which for the English\ncase consists of roughly 35 minutes of monotonic singing of a set of\nsentences, one syllable per beat. The synthesis is divided in two steps.\nFirst, an expressive vowel singing performance of the target song is\ngenerated using the expression database. Next, this performance is\nused as input control of the synthesis using the timbre database and\nthe target lyrics. A selection of synthetic performances have been\nsubmitted to the Interspeech Singing Synthesis Challenge 2016, in which\nthey are compared to other competing systems.\n"
   ],
   "doi": "10.21437/Interspeech.2016-872"
  },
  "perrotin16_interspeech": {
   "authors": [
    [
     "Olivier",
     "Perrotin"
    ],
    [
     "Christophe",
     "d’Alessandro"
    ]
   ],
   "title": "Vocal Effort Modification for Singing Synthesis",
   "original": "1096",
   "page_count": 5,
   "order": 262,
   "p1": "1235",
   "pn": "1239",
   "abstract": [
    "Vocal effort modification of natural speech is an asset to various\napplications, in particular, for adding flexibility to concatenative\nvoice synthesis systems. Although decreasing vocal effort is not particularly\ndifficult, increasing vocal effort is a challenging issue. It requires\nthe generation of artificial harmonics in the voice spectrum, along\nwith transformation of the spectral envelope. After a raw source-filter\ndecomposition, harmonic enrichment is achieved by 1/ increasing the\nsource signal impulsiveness using time distortion, 2/ mixing the distorted\nand natural signals&#8217; spectra. Two types of spectral envelope\ntransformations are used: spectral morphing and spectral modeling.\nSpectral morphing is the transplantation of natural spectral envelopes.\nSpectral modeling focuses on spectral tilt, formant amplitudes and\nfirst formant position modifications. The effectiveness of source enrichment,\nspectrum morphing, and spectrum modeling for vocal effort modification\nof sung vowels was evaluated with the help of a perceptive experiment.\nResults showed a significant positive influence of harmonic enrichment\non vocal effort perception with both spectral envelope transformations.\nSpectral envelope morphing and harmonic enrichment applied on soft\nvoices were perceptively close to natural loud voices. Automatic spectral\nenvelope modeling did not match the results of spectral envelope morphing,\nbut it significantly increased the perception of vocal effort.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1096"
  },
  "blanco16_interspeech": {
   "authors": [
    [
     "Eder del",
     "Blanco"
    ],
    [
     "Inma",
     "Hernaez"
    ],
    [
     "Eva",
     "Navas"
    ],
    [
     "Xabier",
     "Sarasola"
    ],
    [
     "D.",
     "Erro"
    ]
   ],
   "title": "Bertsokantari: a TTS Based Singing Synthesis System",
   "original": "1123",
   "page_count": 5,
   "order": 263,
   "p1": "1240",
   "pn": "1244",
   "abstract": [
    "This paper describes the implementation of the Aholab entry for the\nSinging Synthesis Challenge: Fill-in the Gap. Our approach in this\nwork makes use of an HTS based Text-to-Speech (TTS) synthesizer for\nBasque to generate the singing voice. The prosody related parameters\nprovided by the TTS system for a spoken version of the score are modified\nto adapt them to the requirements of the music score concerning syllables\nduration and tone, while the spectral parameters are basically maintained.\nThe paper describes the processing details developed to improve the\nquality of the output signal: the syllable timing, the generation of\nthe intonation with vibrato and the manipulation of the model states.\nIn this entry, the lyrics have been freely translated into Basque and\nthe rhythm has been adapted to a Basque traditional rhythm.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1123"
  },
  "feugere16_interspeech": {
   "authors": [
    [
     "Lionel",
     "Feugère"
    ],
    [
     "Christophe",
     "d’Alessandro"
    ],
    [
     "Samuel",
     "Delalez"
    ],
    [
     "Luc",
     "Ardaillon"
    ],
    [
     "Axel",
     "Roebel"
    ]
   ],
   "title": "Evaluation of Singing Synthesis: Methodology and Case Study with Concatenative and Performative Systems",
   "original": "1248",
   "page_count": 5,
   "order": 264,
   "p1": "1245",
   "pn": "1249",
   "abstract": [
    "The special session Singing Synthesis Challenge: Fill-In the Gap aims\nat comparative evaluation of singing synthesis systems. The task is\nto synthesize a new couplet for two popular songs. This paper address\nthe methodology needed for quality assessment of singing synthesis\nsystems and reports on a case study using 2 systems with a total of\n6 different configurations. The two synthesis systems are: a concatenative\nText-to-Chant (TTC) system, including a parametric representation of\nthe melodic curve; a Singing Instrument (SI), allowing for real-time\ninterpretation of utterances made of flat-pitch natural voice or diphone\nconcatenated voice. Absolute Category Rating (ACR) and Paired Comparison\n(PC) tests are used. Natural and natural-degraded reference conditions\nare used for calibration of the ACR test. The MOS obtained using ACR\nshows that the TTC (resp. the SI) ranks below natural voice but above\n(resp. in between) degraded conditions. Then singing synthesis quality\nis judged better than auto-tuned or distorted natural voice in some\ncases. PC results show that: 1/ signal processing is an important quality\nissue, making the difference between systems; 2/ diphone concatenation\ndegrades the quality compared to flat-pitch natural voice; 3/ Automatic\nmelodic modelling is preferred to gestural control for off-line synthesis.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1248"
  },
  "ardaillon16_interspeech": {
   "authors": [
    [
     "Luc",
     "Ardaillon"
    ],
    [
     "Celine",
     "Chabot-Canet"
    ],
    [
     "Axel",
     "Roebel"
    ]
   ],
   "title": "Expressive Control of Singing Voice Synthesis Using Musical Contexts and a Parametric  F0 Model",
   "original": "1317",
   "page_count": 5,
   "order": 265,
   "p1": "1250",
   "pn": "1254",
   "abstract": [
    "Expressive singing voice synthesis requires an appropriate control\nof both prosodic and timbral aspects. While it is desirable to have\nan intuitive control over the expressive parameters, synthesis systems\nshould be able to produce convincing results directly from a score.\nAs countless interpretations of a same score are possible, the system\nshould also target a particular singing style, which implies to mimic\nthe various strategies used by different singers. Among the control\nparameters involved, the pitch ( F0) should be modeled in priority.\nIn previous work, a parametric  F0 model with intuitive controls has\nbeen proposed, but no automatic way to choose the model parameters\nwas given. In the present work, we propose a new approach for modeling\nsinging style, based on parametric templates selection. In this approach,\nthe  F0 parameters and phonemes durations are extracted from annotated\nrecordings, along with a rich description of contextual informations,\nand stored to form a database of parametric templates. This database\nis then used to build a model of the singing style using decision-trees.\nAt the synthesis stage, appropriate parameters are then selected according\nto the target contexts. The results produced by this approach have\nbeen evaluated by means of a listening test.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1317"
  },
  "cotescu16_interspeech": {
   "authors": [
    [
     "Marius",
     "Cotescu"
    ]
   ],
   "title": "Optimal Unit Stitching in a Unit Selection Singing Synthesis System",
   "original": "1390",
   "page_count": 5,
   "order": 266,
   "p1": "1255",
   "pn": "1259",
   "abstract": [
    "Unit Selection based speech synthesis systems are currently the best\nperforming, producing natural sounding speech with minimal CPU load.\nOne of the important reasons behind their success is the amount of\nrecordings that are now commonly used in synthesis applications. However,\nin the case of singing applications, it is quite hard for a database\nto cover a large phonetic space due to the relative inefficiency of\nthe recording process. Thus, due to the reduced catalogue of units,\nsinging unit selection systems are more likely to produce spectral\ndiscontinuity artefacts. Taking advantage of the quasi stable nature\nof articulation during singing, we propose a novel unit stitching method.\nThe method was implemented into the system that was used for the &#8220;Fill-In\nthe Gap&#8221; Singing Synthesis Challenge.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1390"
  },
  "hilton16_interspeech": {
   "authors": [
    [
     "Katherine",
     "Hilton"
    ]
   ],
   "title": "The Perception of Overlapping Speech: Effects of Speaker Prosody and Listener Attitudes",
   "original": "1456",
   "page_count": 5,
   "order": 267,
   "p1": "1260",
   "pn": "1264",
   "abstract": [
    "Speakers use overlapping speech to achieve a range of interactional\nmoves. Competitive overlaps, or interruptions, challenge an interlocutor&#8217;s\ncontrol of the conversational floor, while non-competitive overlaps,\nlike back-channeling and co-constructed discourse, communicate engagement\nwith the conversation and ratify the interlocutor&#8217;s right to\nbe speaking. Being able to evaluate the intentions behind moments of\noverlap is critical for interlocutors, as well as researchers seeking\nto model human-human interaction. Researchers have analyzed the acoustics\nof overlapping speech in order to understand what determines whether\nan overlap is heard as competitive or non-competitive. They have overwhelmingly\nfound that prosodic prominence plays an important role; incoming overlaps\nwith higher pitch and intensity are more competitive or interruptive.\nHowever, no research has directly tested whether and how listeners\nuse prosodic cues to evaluate moments of overlap. Furthermore, much\nof the current research on classifying overlapping speech ignores listener\nvariability. The present study uses a perception experiment with 500\nparticipants to test the effects of speaker prosody and listener attitudes\non the evaluation of overlapping speech. The results demonstrate that\nprosodic prominence does significantly affect evaluations of overlapping\nspeech, but it is mediated by the listener&#8217;s own interactional\nstyle and attitudes toward overlapping speech.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1456"
  },
  "gravano16_interspeech": {
   "authors": [
    [
     "Agustín",
     "Gravano"
    ],
    [
     "Pablo",
     "Brusco"
    ],
    [
     "Štefan",
     "Beňuš"
    ]
   ],
   "title": "Who Do You Think Will Speak Next? Perception of Turn-Taking Cues in Slovak and Argentine Spanish",
   "original": "0585",
   "page_count": 5,
   "order": 268,
   "p1": "1265",
   "pn": "1269",
   "abstract": [
    "We investigate perceptual cues in human-human dialogue management related\nto signalling the change of speaker and the interlocutor&#8217;s wish\nto backchannel or contribute with propositional content. We are interested\nprimarily in the relevance of prosodic cues in relation to textual\nones, and their cross-linguistic validity by comparing unrelated languages\nSlovak and Argentine Spanish. Results of a perception study indicate\nthat 1) in addition to textual cues, prosodic cues also play a clear\nrole in perceiving how the dialogue will unfold; and 2) there exists\na non-empty intersection of temporal and intonational prosodic turn-taking\ncues in the two languages, despite their belonging to separate families.\n"
   ],
   "doi": "10.21437/Interspeech.2016-585"
  },
  "perez16_interspeech": {
   "authors": [
    [
     "Juan M.",
     "Pérez"
    ],
    [
     "Ramiro H.",
     "Gálvez"
    ],
    [
     "Agustín",
     "Gravano"
    ]
   ],
   "title": "Disentrainment may be a Positive Thing: A Novel Measure of Unsigned Acoustic-Prosodic Synchrony, and its Relation to Speaker Engagement",
   "original": "0587",
   "page_count": 5,
   "order": 269,
   "p1": "1270",
   "pn": "1274",
   "abstract": [
    "Synchrony is a form of entrainment which consists in a relative coordination\nbetween two speakers, who throughout conversation simultaneously vary\nsome properties of their speech. We describe two novel measures of\nacoustic-prosodic synchrony that are derived from a time-series analysis\nof the speech signal. Both of these measures reward positive synchrony\n(entrainment) and, while one penalizes negative synchrony (disentrainment),\nthe other one rewards it. We describe significant correlations between\nthe second measure and a number of positive social characteristics\nof the conversations, such as degree of speaker engagement, in a corpus\nof task-oriented dialogues in Standard American English. Since these\ncorrelations are not found to be significant for the first measure,\nour results suggest that disentrainment may sometimes have a positive\neffect on the development of conversation.\n"
   ],
   "doi": "10.21437/Interspeech.2016-587"
  },
  "wodarczak16b_interspeech": {
   "authors": [
    [
     "Marcin",
     "Włodarczak"
    ],
    [
     "Mattias",
     "Heldner"
    ]
   ],
   "title": "Respiratory Turn-Taking Cues",
   "original": "0346",
   "page_count": 5,
   "order": 270,
   "p1": "1275",
   "pn": "1279",
   "abstract": [
    "This paper investigates to what extent breathing can be used as a cue\nto turn-taking behaviour. The paper improves on existing accounts by\nconsidering all possible transitions between speaker states (silent,\nspeaking, backchanneling) and by not relying on global speaker models.\nInstead, all features (including breathing range and resting expiratory\nlevel) are estimated in an incremental fashion using the left-hand\ncontext. We identify several inhalatory features relevant to turn-management,\nand assess the fit of models with these features as predictors of turn-taking\nbehaviour.\n"
   ],
   "doi": "10.21437/Interspeech.2016-346"
  },
  "rennie16_interspeech": {
   "authors": [
    [
     "Emma",
     "Rennie"
    ],
    [
     "Rebecca",
     "Lunsford"
    ],
    [
     "Peter A.",
     "Heeman"
    ]
   ],
   "title": "The Discourse Marker &#8220;so&#8221; in Turn-Taking and Turn-Releasing Behavior",
   "original": "0547",
   "page_count": 5,
   "order": 271,
   "p1": "1280",
   "pn": "1284",
   "abstract": [
    "Although  so is a recognized discourse marker, little work has explored\nits uses in turn-taking, especially when it is not followed by additional\nspeech. In this paper we explore the use of the discourse marker  so\nas it pertains to turn-taking and turn-releasing. Specifically, we\ncompare the duration and intensity of  so when used to take a turn,\nmid-utterance, and when releasing a turn. We found that durations of\nturn-retaining tokens are generally shorter than turn-releases; we\nalso found that turn-retaining tokens tend to be lower in intensity\nthan the following speech. These trends of turn-taking behavior alongside\ncertain lexical and prosodic features may prove useful for the development\nof speech-recognition software.\n"
   ],
   "doi": "10.21437/Interspeech.2016-547"
  },
  "sherrziarko16_interspeech": {
   "authors": [
    [
     "Ethan",
     "Sherr-Ziarko"
    ]
   ],
   "title": "Acoustic Properties of Formality in Conversational Japanese",
   "original": "0132",
   "page_count": 5,
   "order": 272,
   "p1": "1285",
   "pn": "1289",
   "abstract": [
    "This paper examines potential acoustic cues for level of formality\nin Japanese conversational speech using speech data gathered outside\nthe laboratory, with the objective of using any significant cues to\ndevelop a model to predict level of formality in spoken Japanese. Based\non previous work on the phonetic properties of formality in Japanese\n[1],[2] and other languages [3], and on a pilot study of informal geminate\ncontractions in Japanese (section 2), the study examined the mean f<SUB>0</SUB>,\narticulation rate, and f<SUB>0</SUB> range (the difference between\nthe minimum and maximum f<SUB>0</SUB> in an utterance) via direct examination\nof the data and a functional data analysis [4],[5]. Analysis of the\nspeech data shows significant relationships between all three variables\nand level of formality, and a binary logistic regression indicates\nthat the variables have some potential as predictors of formality independent\nof lexical cues, although further refinement of any model will be necessary.\n"
   ],
   "doi": "10.21437/Interspeech.2016-132"
  },
  "pellegrini16_interspeech": {
   "authors": [
    [
     "Thomas",
     "Pellegrini"
    ],
    [
     "Sandrine",
     "Mouysset"
    ]
   ],
   "title": "Inferring Phonemic Classes from CNN Activation Maps Using Clustering Techniques",
   "original": "1299",
   "page_count": 5,
   "order": 273,
   "p1": "1290",
   "pn": "1294",
   "abstract": [
    "Today&#8217;s state-of-art in speech recognition involves deep neural\nnetworks (DNN). These last years, a certain research effort has been\ninvested in characterizing the feature representations learned by DNNs.\nIn this paper, we focus on convolutional neural networks (CNN) trained\nfor phoneme recognition in French. We report clustering experiments\nperformed on activation maps extracted from the different layers of\na CNN comprised of two convolution and sub-sampling layers followed\nby three dense layers. Our goal was to get insights into phone separability\nand phonemic categories inferred by the network, and how they vary\naccording to the successive layers. Two directions were explored with\nboth linear and non-linear clustering techniques. First, we imposed\na number of 33 classes equal to the number of context-independent phone\nmodels for French, in order to assess the phoneme separability power\nof the different layers. As expected, we observed that this power increases\nwith the layer depth in the network: from 34% to 74% in F-measure from\nthe first convolution to the last dense layers, when using spectral\nclustering. Second, optimal numbers of classes were automatically inferred\nthrough inter- and intra-cluster measure criteria. We analyze these\nclasses in terms of standard French phonological features.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1299"
  },
  "zeghidour16_interspeech": {
   "authors": [
    [
     "Neil",
     "Zeghidour"
    ],
    [
     "Gabriel",
     "Synnaeve"
    ],
    [
     "Nicolas",
     "Usunier"
    ],
    [
     "Emmanuel",
     "Dupoux"
    ]
   ],
   "title": "Joint Learning of Speaker and Phonetic Similarities with Siamese Networks",
   "original": "0811",
   "page_count": 5,
   "order": 274,
   "p1": "1295",
   "pn": "1299",
   "abstract": [
    "Recent work has demonstrated, on small datasets, the feasibility of\njointly learning specialized speaker and phone embeddings, in a weakly\nsupervised siamese DNN architecture using word and speaker identity\nas side information. Here, we scale up these architectures to the 360\nhours of the Librispeech corpus by implementing a sampling method to\nefficiently select pairs of words from the dataset and improving the\nloss function. We also compare the standard siamese networks fed with\nsame (AA) or different (AB) pairs, to a &#8216;triamese&#8217; network\nfed with AAB triplets. We use ABX discrimination tasks to evaluate\nthe discriminability and invariance properties of the obtained joined\nembeddings, and compare these results with mono-embeddings architectures.\nWe find that the joined embeddings architectures succeed in effectively\ndisentangling speaker from phoneme information, with around 10% errors\nfor the matching tasks and embeddings (speaker task on speaker embeddings,\nand phone task on phone embedding) and near chance for the mismatched\ntask. Furthermore, the results carry over in out-of-domain datasets,\neven beating the best results obtained with similar weakly supervised\ntechniques.\n"
   ],
   "doi": "10.21437/Interspeech.2016-811"
  },
  "mitra16_interspeech": {
   "authors": [
    [
     "Vikramjit",
     "Mitra"
    ],
    [
     "Dimitra",
     "Vergyri"
    ],
    [
     "Horacio",
     "Franco"
    ]
   ],
   "title": "Unsupervised Learning of Acoustic Units Using Autoencoders and Kohonen Nets",
   "original": "1374",
   "page_count": 5,
   "order": 275,
   "p1": "1300",
   "pn": "1304",
   "abstract": [
    "Often, prior knowledge of subword units is unavailable for low-resource\nlanguages. Instead, a global subword unit description, such as a universal\nphone set, is typically used in such scenarios. One major bottleneck\nfor existing speech-processing systems is their reliance on transcriptions.\nUnfortunately, the preponderance of data becoming available everyday\nis only worsening the problem, as properly transcribing, and hence\nmaking this data useful for training speech-processing models, is impossible.\nThis work investigates learning acoustic units in an unsupervised manner\nfrom real-world speech data by using a cascade of an autoencoder and\na Kohonen net. For this purpose, a deep autoencoder with a bottleneck\nlayer at the center was trained with multiple languages. Once trained,\nthe bottleneck-layer output was used to train a Kohonen net, such that\nstate-level ids can be assigned to the bottleneck outputs. To ascertain\nhow consistent such state-level ids are with respect to the acoustic\nunits, phone-alignment information was used for a part of the data\nto qualify if indeed a functional relationship existed between the\nphone ids and the Kohonen state ids and, if yes, whether such relationship\ncan be generalized to data that are not transcribed.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1374"
  },
  "zhu16b_interspeech": {
   "authors": [
    [
     "Zhenyao",
     "Zhu"
    ],
    [
     "Jesse H.",
     "Engel"
    ],
    [
     "Awni",
     "Hannun"
    ]
   ],
   "title": "Learning Multiscale Features Directly from Waveforms",
   "original": "0256",
   "page_count": 5,
   "order": 276,
   "p1": "1305",
   "pn": "1309",
   "abstract": [
    "Deep learning has dramatically improved the performance of speech recognition\nsystems through learning hierarchies of features optimized for the\ntask at hand. However, true end-to-end learning, where features are\nlearned directly from waveforms, has only recently reached the performance\nof hand-tailored representations based on the Fourier transform. In\nthis paper, we detail an approach to use convolutional filters to push\npast the inherent tradeoff of temporal and frequency resolution that\nexists for spectral representations. At increased computational cost,\nwe show that increasing temporal resolution via reduced stride and\nincreasing frequency resolution via additional filters delivers significant\nperformance improvements. Further, we find more efficient representations\nby simultaneously learning at multiple scales, leading to an overall\ndecrease in word error rate on a difficult internal speech test set\nby 20.7% relative to networks with the same number of parameters trained\non spectrograms.\n"
   ],
   "doi": "10.21437/Interspeech.2016-256"
  },
  "heck16_interspeech": {
   "authors": [
    [
     "Michael",
     "Heck"
    ],
    [
     "Sakriani",
     "Sakti"
    ],
    [
     "Satoshi",
     "Nakamura"
    ]
   ],
   "title": "Supervised Learning of Acoustic Models in a Zero Resource Setting to Improve DPGMM Clustering",
   "original": "0988",
   "page_count": 5,
   "order": 277,
   "p1": "1310",
   "pn": "1314",
   "abstract": [
    "In this work we utilize a supervised acoustic model training pipeline\nwithout supervision to improve Dirichlet process Gaussian mixture model\n(DPGMM) based feature vector clustering. We exploit methods common\nin supervised acoustic modeling to unsupervisedly learn feature transformations\nfor application to the input data prior to clustering. The idea is\nto automatically find mappings of feature vectors into sub-spaces that\nare more robust to channel, context and speaker variability. The need\nof labels for these techniques makes it difficult to use them in a\nzero resource setting. To overcome this issue we utilize a first iteration\nof DPGMM clustering to generate frame based class labels for the target\ndata. The labels serve as basis for learning an acoustic model in the\nform of hidden Markov models (HMMs) using linear discriminant analysis\n(LDA), maximum likelihood linear transform (MLLT) and speaker adaptive\ntraining (SAT). We show that the learned transformations lead to features\nthat consistently outperform untransformed features on the ABX sound\nclass discriminability task. We also demonstrate that the combination\nof multiple clustering runs is a suitable method to further enhance\nsound class discriminability.\n"
   ],
   "doi": "10.21437/Interspeech.2016-988"
  },
  "xu16_interspeech": {
   "authors": [
    [
     "Haihua",
     "Xu"
    ],
    [
     "Hang",
     "Su"
    ],
    [
     "Chongjia",
     "Ni"
    ],
    [
     "Xiong",
     "Xiao"
    ],
    [
     "Hao",
     "Huang"
    ],
    [
     "Eng Siong",
     "Chng"
    ],
    [
     "Haizhou",
     "Li"
    ]
   ],
   "title": "Semi-Supervised and Cross-Lingual Knowledge Transfer Learnings for DNN Hybrid Acoustic Models Under Low-Resource Conditions",
   "original": "1099",
   "page_count": 5,
   "order": 278,
   "p1": "1315",
   "pn": "1319",
   "abstract": [
    "Semi-supervised and cross-lingual knowledge transfer learnings are\ntwo strategies for boosting performance of low-resource speech recognition\nsystems. In this paper, we propose a unified knowledge transfer learning\nmethod to deal with these two learning tasks. Such a knowledge transfer\nlearning is realized by fine-tuning of Deep Neural Network (DNN). We\ndemonstrate its effectiveness in both monolingual based semi-supervised\nlearning task and cross-lingual knowledge transfer learning task. We\nthen combine these two learning strategies to obtain further performance\nimprovement.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1099"
  },
  "asami16_interspeech": {
   "authors": [
    [
     "Taichi",
     "Asami"
    ],
    [
     "Ryo",
     "Masumura"
    ],
    [
     "Yushi",
     "Aono"
    ],
    [
     "Koichi",
     "Shinoda"
    ]
   ],
   "title": "Recurrent Out-of-Vocabulary Word Detection Using Distribution of Features",
   "original": "0562",
   "page_count": 5,
   "order": 279,
   "p1": "1320",
   "pn": "1324",
   "abstract": [
    "The repeated use of out-of-vocabulary (OOV) words in a spoken document\nseriously degrades a speech recognizer&#8217;s performance. This paper\nprovides a novel method for accurately detecting such recurrent OOV\nwords. Standard OOV word detection methods classify each word segment\ninto in-vocabulary (IV) or OOV. This word-by-word classification tends\nto be affected by sudden vocal irregularities in spontaneous speech,\ntriggering false alarms. To avoid this sensitivity to the irregularities,\nour proposal focuses on consistency of the repeated occurrence of OOV\nwords. The proposed method preliminarily detects recurrent segments,\nsegments that contain the same word, in a spoken document by open vocabulary\nspoken term discovery using a phoneme recognizer. If the recurrent\nsegments are OOV words, features for OOV detection in those segments\nshould exhibit consistency. We capture this consistency by using the\nmean and variance (distribution) of features (DOF) derived from the\nrecurrent segments, and use the DOF for IV/OOV classification. Experiments\nillustrate that the proposed method&#8217;s use of the DOF significantly\nimproves its performance in recurrent OOV word detection.\n"
   ],
   "doi": "10.21437/Interspeech.2016-562"
  },
  "kanda16_interspeech": {
   "authors": [
    [
     "Naoyuki",
     "Kanda"
    ],
    [
     "Shoji",
     "Harada"
    ],
    [
     "Xugang",
     "Lu"
    ],
    [
     "Hisashi",
     "Kawai"
    ]
   ],
   "title": "Investigation of Semi-Supervised Acoustic Model Training Based on the Committee of Heterogeneous Neural Networks",
   "original": "0072",
   "page_count": 5,
   "order": 280,
   "p1": "1325",
   "pn": "1329",
   "abstract": [
    "This paper investigates the semi-supervised training for deep neural\nnetwork-based acoustic models (AM). In the conventional self-learning\napproach, a &#8220;seed-AM&#8221; is first trained by using a small\ntranscribed data set. Then, a large untranscribed data set is decoded\nby using the seed-AM to create a transcription, which is finally used\nto train a new AM on the entire data. Our investigation in this paper\nfocuses on the different approach that uses additional complementary\nAMs to form a committee of label creation for untranscribed data. Especially,\nwe investigate the case of using heterogeneous neural networks as complementary\nAMs, and the case of intentional exclusion of the primary seed-AM from\nthe committee, both of which could enhance the chance to find more\ninformative training samples for the seed-AM. We investigated those\napproaches based on Japanese lecture recognition experiments with 50-hours\nof transcribed data and 190-hours of untranscribed data. In our experiment,\nthe committee-based approach showed significant improvements in the\nword error rate, and the best method finally recovered 75.2% of the\noracle improvement with full manual transcription, while the conventional\nself-learning approach recovered only 32.7% of the oracle gain.\n"
   ],
   "doi": "10.21437/Interspeech.2016-72"
  },
  "ghannay16_interspeech": {
   "authors": [
    [
     "Sahar",
     "Ghannay"
    ],
    [
     "Yannick",
     "Estève"
    ],
    [
     "Nathalie",
     "Camelin"
    ],
    [
     "Paul",
     "Deléglise"
    ]
   ],
   "title": "Acoustic Word Embeddings for ASR Error Detection",
   "original": "0784",
   "page_count": 5,
   "order": 281,
   "p1": "1330",
   "pn": "1334",
   "abstract": [
    "This paper focuses on error detection in Automatic Speech Recognition\n(ASR) outputs. A neural network architecture is proposed, which is\nwell suited to handle continuous word representations, like word embeddings.\nIn a previous study, the authors explored the use of linguistic word\nembeddings, and more particularly their combination. In this new study,\nthe use of acoustic word embeddings is explored. Acoustic word embeddings\noffer the opportunity of an  a priori acoustic representation of words\nthat can be compared, in terms of similarity, to an embedded representation\nof the audio signal.<br style=\"mso-data-placement:same-cell;\" /><br\nstyle=\"mso-data-placement:same-cell;\" />\n  First, we propose an approach\nto evaluate the intrinsic performances of acoustic word embeddings\nin comparison to orthographic representations in order to capture discriminative\nphonetic information. Since French language is targeted in experiments,\na particular focus is made on homophone words. Then, the use of acoustic\nword embeddings is evaluated for ASR error detection. The proposed\napproach gets a classification error rate of 7.94% while the previous\nstate-of-the-art CRF-based approach gets a CER of 8.56% on the outputs\nof the ASR system which won the ETAPE evaluation campaign on speech\nrecognition of French broadcast news.\n"
   ],
   "doi": "10.21437/Interspeech.2016-784"
  },
  "horndasch16_interspeech": {
   "authors": [
    [
     "Axel",
     "Horndasch"
    ],
    [
     "Anton",
     "Batliner"
    ],
    [
     "Caroline",
     "Kaufhold"
    ],
    [
     "Elmar",
     "Nöth"
    ]
   ],
   "title": "Combining Semantic Word Classes and Sub-Word Unit Speech Recognition for Robust OOV Detection",
   "original": "1250",
   "page_count": 5,
   "order": 282,
   "p1": "1335",
   "pn": "1339",
   "abstract": [
    "Out-of-vocabulary words (OOVs) are often the main reason for the failure\nof tasks like automated voice searches or human-machine dialogs. This\nis especially true if rare but task-relevant content words, e.g. person\nor location names, are not in the recognizer&#8217;s vocabulary. Since\napplications like spoken dialog systems use the result of the speech\nrecognizer to extract a semantic representation of a user utterance,\nthe detection of OOVs as well as their (semantic) word class can support\nto manage a dialog successfully. In this paper we suggest to combine\ntwo well-known approaches in the context of OOV detection: semantic\nword classes and OOV models based on sub-word units. With our system,\nwhich builds upon the widely used Kaldi speech recognition toolkit,\nwe show on two different data sets that &#8212; compared to other methods\n&#8212; such a combination improves OOV detection performance for open\nword classes at a given false alarm rate. Another result of our approach\nis a reduction of the word error rate (WER).\n"
   ],
   "doi": "10.21437/Interspeech.2016-1250"
  },
  "xie16b_interspeech": {
   "authors": [
    [
     "Chuandong",
     "Xie"
    ],
    [
     "Wu",
     "Guo"
    ],
    [
     "Guoping",
     "Hu"
    ],
    [
     "Junhua",
     "Liu"
    ]
   ],
   "title": "Web Data Selection Based on Word Embedding for Low-Resource Speech Recognition",
   "original": "0045",
   "page_count": 5,
   "order": 283,
   "p1": "1340",
   "pn": "1344",
   "abstract": [
    "The lack of transcription files will lead to a high out-of-vocabulary\n(OOV) rate and a weak language model in low-resource speech recognition\nsystems. This paper presents a web data selection method to augment\nthese systems. After mapping all the vocabularies or short sentences\nto vectors in a low-dimensional space through a word embedding technique,\nthe similarities between the web data and the small pool of training\ntranscriptions are calculated. Then, the web data with high similarity\nare selected to expand the pronunciation lexicon or language model.\nExperiments are conducted on the NIST Open KWS15 Swahili VLLP recognition\ntask. Compared with the baseline system, our methods can achieve a\n5.23% absolute reduction in word error rate (WER) using the expanded\npronunciation lexicon and a 9.54% absolute WER reduction using both\nthe expanded lexicon and language model.\n"
   ],
   "doi": "10.21437/Interspeech.2016-45"
  },
  "alshareef16_interspeech": {
   "authors": [
    [
     "Sarah",
     "Al-Shareef"
    ],
    [
     "Thomas",
     "Hain"
    ]
   ],
   "title": "Colloquialising Modern Standard Arabic Text for Improved Speech Recognition",
   "original": "0788",
   "page_count": 5,
   "order": 284,
   "p1": "1345",
   "pn": "1349",
   "abstract": [
    "Modern standard Arabic (MSA) is the official language of spoken and\nwritten Arabic media. Colloquial Arabic (CA) is the set of spoken variants\nof modern Arabic that exist in the form of regional dialects. CA is\nused in informal and everyday conversations while MSA is formal communication.\nAn Arabic speaker switches between the two variants according to the\nsituation. Developing an automatic speech recognition system always\nrequires a large collection of transcribed speech or text, and for\nCA dialects this is an issue. CA has limited textual resources because\nit exists only as a spoken language, without a standardised written\nform unlike MSA. This paper focuses on the data sparsity issue in CA\ntextual resources and proposes a strategy to emulate a native speaker\nin colloquialising MSA to be used in CA language models (LMs) by use\nof a machine translation (MT) framework. The empirical results in Levantine\nCA show that using LMs estimated from colloquialised MSA data outperformed\nMSA LMs with a perplexity reduction up to 68% relative. In addition,\ninterpolating colloquialised MSA LMs with a CA LMs improved speech\nrecognition performance by 4% relative.\n"
   ],
   "doi": "10.21437/Interspeech.2016-788"
  },
  "kuang16_interspeech": {
   "authors": [
    [
     "Jianjing",
     "Kuang"
    ],
    [
     "Mark",
     "Liberman"
    ]
   ],
   "title": "Pitch-Range Perception: The Dynamic Interaction Between Voice Quality and Fundamental Frequency",
   "original": "1483",
   "page_count": 5,
   "order": 285,
   "p1": "1350",
   "pn": "1354",
   "abstract": [
    "Effective pitch-range normalization is important to uncover intended\nlinguistic pitch targets in continuous speech. Our previous study demonstrated\nthat voice quality plays a role in pitch-range perception: &#8220;tense\nvoice&#8221;, implemented as stimuli with spectral balance tilted towards\nhigher frequency, was perceived as higher in pitch. This psychoacoustic\neffect is consistent with the co-variation between pitch and tense\nvoice in production. However, a spectral balance tilted towards higher\nfrequency is also one of the properties of creaky voice, which is often\nassociated with low pitch in production. Therefore, this raises the\npossibility that manipulating the f0 range of the stimuli or changing\nthe sex of the speaker of the stimuli can reverse the direction of\nthe shift. This current study replicates the previous experiment with\nthe same forced-choice pitch classification experiment with four spectral\nconditions, but uses a female voice to create the stimuli. In addition,\ntwo f0 ranges are used in the current experiments, which resemble the\nlower range and the higher range of a female voice. Overall, the results\nshow that spectral balance interacts with f0 range: the presence of\nvoice quality cues affect the perception of pitch range; but, the spectrum\nwith greater energy in the high-frequency range can be interpreted\nas either creaky or tense depending on the f0 range. This current study\nenriches our understanding of the interaction between voice quality\nand pitch.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1483"
  },
  "chen16e_interspeech": {
   "authors": [
    [
     "Fei",
     "Chen"
    ],
    [
     "Benson C.L.",
     "Chiao"
    ]
   ],
   "title": "Comparing the Contributions of Amplitude and Phase to Speech Intelligibility in a Vocoder-Based Speech Synthesis Model",
   "original": "0066",
   "page_count": 4,
   "order": 286,
   "p1": "1355",
   "pn": "1358",
   "abstract": [
    "Vocoder-based speech synthesis model has been long used to assess the\ncontribution of acoustic cue for speech recognition. This study compared\nthe perceptual contributions of amplitude and phase by using two types\nof stimuli, i.e., amplitude- and phase-based vocoded stimuli. The amplitude-based\nvocoded stimuli were synthesized by preserving amplitude fluctuation\ncue but discarding phase cue (i.e., setting phase to zero), while the\nphase-based vocoded stimuli were synthesized by preserving phase cue\nand discarding amplitude cue (i.e., setting amplitude to unit). Listening\nexperiments with normal-hearing participants showed consistent findings\nwith earlier studies that the intelligibility scores of both amplitude-\nand phase-based vocoded stimuli increased when using a large number\nof channels in vocoder-based speech synthesis. In addition, at all\ntested conditions, the intelligibility scores of amplitude-based vocoded\nstimuli were significantly larger than those of phase-based vocoded\nstimuli, suggesting that amplitude might carry more perceptual contribution\nthan phase. This intelligibility advantage of amplitude over phase\nmay be attributed to the difference in the amount of envelope information\ncontained in the two types of vocoded stimuli.\n"
   ],
   "doi": "10.21437/Interspeech.2016-66"
  },
  "chen16f_interspeech": {
   "authors": [
    [
     "Fei",
     "Chen"
    ]
   ],
   "title": "Modeling Noise Influence to Speech Intelligibility Non-Intrusively by Reduced Speech Dynamic Range",
   "original": "0009",
   "page_count": 4,
   "order": 287,
   "p1": "1359",
   "pn": "1362",
   "abstract": [
    "The noise influence to speech signal waveform can be characterized\nby reduced speech dynamic range (rDR). This motivated the present work\nto propose an rDR-based intelligibility measure (denoted as rDRm) that\ncould be used to non-intrusively (i.e., do not require clean reference\nspeech signal) predict speech intelligibility in noise and is computed\nonly using the dynamic range extracted from the noise-corrupted speech.\nThe rDRm indices were evaluated with intelligibility scores obtained\nfrom normal-hearing listeners presented with sentences corrupted by\nfour types of maskers in a total of 22 conditions. High correlation\n(r=0.93) was obtained between rDRm values and listeners&#8217; sentence\nrecognition scores, and this correlation was comparable to those computed\nwith existing intrusive and non-intrusive intelligibility measures.\nThis suggests that the dynamic range of speech signal may work as a\nsimple but efficient predictor of speech intelligibility in noise,\nwhose computation does not need access to the clean reference speech\nsignal.\n"
   ],
   "doi": "10.21437/Interspeech.2016-9"
  },
  "pinter16_interspeech": {
   "authors": [
    [
     "Gábor",
     "Pintér"
    ],
    [
     "Hiroki",
     "Watanabe"
    ]
   ],
   "title": "Do GMM Phoneme Classifiers Perceive Synthetic Sibilants as Humans Do?",
   "original": "0325",
   "page_count": 5,
   "order": 288,
   "p1": "1363",
   "pn": "1367",
   "abstract": [
    "This study presents a psycholinguistically motivated evaluation method\nfor phoneme classifiers by using non-categorical perceptual data elicited\nin a Japanese sibilant matching 2AFC task. Probability values of a\nperceptual [s]-[&#643;] boundary, obtained from 42 speakers over a\n7-step synthetic [s]-[&#643;] continuum, were compared to probability\nestimates of Gaussian mixture models (GMMs) of Japanese [s] and [&#643;].\nThe GMMs, trained on the Corpus of Spontaneous Japanese, differed in\nfeature vectors (MFCC, PLP, acoustic features), covariance matrix types\n(full, tied, diagonal, spherical), and numbers of mixtures (1&#8211;20).\nUsing ten-fold cross validation, it was found that GMMs trained on\nMFCC features had the best sibilant classification accuracies (87.4&#8211;90.4%),\nbut their correlations with human perceptual data were non-conclusive\n(0.35&#8211;0.98). Acoustic feature-based GMMs with tied covariance\nmatrices had near human-like synthetic stimuli perception (0.957&#8211;0.996),\nbut their classification performance was poor (71.3&#8211;80.4%). Models\ntrained on perceptual linear prediction (PLP) features were on par\nwith the acoustic feature-based models in terms correlation to the\nperceptual experiment (0.884&#8211;0.995), while losing slightly on\nclassification performance (86.1&#8211;88.9%) compared to MFCC models.\nAcross the board correlation tests and mixture-effect models confirmed\nthat GMMs with better sibilant classifying performance produced more\nhuman-like probability estimations on the synthetic sibilant continuum.\n"
   ],
   "doi": "10.21437/Interspeech.2016-325"
  },
  "frye16_interspeech": {
   "authors": [
    [
     "Marina",
     "Frye"
    ],
    [
     "Cristiano",
     "Micheli"
    ],
    [
     "Inga M.",
     "Schepers"
    ],
    [
     "Gerwin",
     "Schalk"
    ],
    [
     "Jochem W.",
     "Rieger"
    ],
    [
     "Bernd T.",
     "Meyer"
    ]
   ],
   "title": "Neural Responses to Speech-Specific Modulations Derived from a Spectro-Temporal Filter Bank",
   "original": "1327",
   "page_count": 5,
   "order": 289,
   "p1": "1368",
   "pn": "1372",
   "abstract": [
    "This paper analyzes the application of methods developed in automatic\nspeech recognition (ASR) to better understand neural activity measured\nwith electrocorticography (ECoG) during the presentation of speech.\nECoG data is collected from temporal cortex in two subjects listening\nto a matrix sentence test. We investigate the relation of ECoG signals\nand acoustic speech that has been processed with spectro-temporal filters,\nwhich have been shown to produce robust and reliable representations\nfor speech applications. The organization of spectro-temporal filters\ninto a filter bank allows for a straight-forward separation into spectral\nor temporal only, as well as true spectro-temporal components. We find\nelectrodes positioned over the superior temporal gyrus that is associated\nwith the auditory cortex to show significant specific high gamma activity\nto fine temporal and spectro-temporal patterns present in speech. This\nindicates that representations developed in machine listening are a\nsuitable tool for the analysis of biosignals.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1327"
  },
  "mulder16_interspeech": {
   "authors": [
    [
     "Kimberley",
     "Mulder"
    ],
    [
     "Louis ten",
     "Bosch"
    ],
    [
     "Lou",
     "Boves"
    ]
   ],
   "title": "Comparing Different Methods for Analyzing ERP Signals",
   "original": "0967",
   "page_count": 5,
   "order": 290,
   "p1": "1373",
   "pn": "1377",
   "abstract": [
    "Event-Related Potential (ERP) signals obtained from EEG recordings\nare widely used for studying cognitive processes in spoken language\nprocessing. The computation of ERPs involves averaging over multiple\nparticipants and multiple stimuli. Especially with speech stimuli,\nwhich also evoke substantial exogenous excitation, even averaging within\nconditions results in pooling many sources of variance. This raises\nquestions about the statistical processing needed to uncover reliable\ndifferences between conditions. In this study we investigate differences\nbetween ERPs when participants listened to full and reduced pronunciations\nof verb forms in Dutch, in isolation and in mid-sentence position.\nConventional statistical analysis uncovers some (but not all) differences\nbetween full and reduced forms in isolation, but not in mid-sentence\nposition. In this paper, we show that linear mixed models (lmer) and\ngeneralized additive models (gam), which are able to account for participant-\nand stimulus-related variance may uncover more effects than conventional\nstatistical models. However, depending on the complexity of the data,\nlmer and gam models may not be able to fit the data closely enough\nto warrant blind interpretation of the summary output. We discuss opportunities\nand threats of these approaches to analyzing ERP signals.\n"
   ],
   "doi": "10.21437/Interspeech.2016-967"
  },
  "eklund16_interspeech": {
   "authors": [
    [
     "Robert",
     "Eklund"
    ],
    [
     "Martin",
     "Ingvar"
    ]
   ],
   "title": "Supplementary Motor Area Activation in Disfluency Perception: An fMRI Study of Listener Neural Responses to Spontaneously Produced Unfilled and Filled Pauses",
   "original": "0973",
   "page_count": 4,
   "order": 291,
   "p1": "1378",
   "pn": "1381",
   "abstract": [
    "Spontaneously produced Unfilled Pauses (UPs) and Filled Pauses (FPs)\nwere played to subjects in an fMRI experiment. For both stimuli increased\nactivity was observed in the Primary Auditory Cortex (PAC). However,\nFPs, but not UPs, elicited modulation in the Supplementary Motor Area\n(SMA), Brodmann Area 6. Our results provide neurocognitive confirmation\nof the alleged difference between FPs and other kinds of speech disfluency\nand could also provide a partial explanation for the previously reported\nbeneficial effect of FPs on reaction times in speech perception. Our\nresults also have potential implications for two of the suggested functions\nof FPs: the &#8220;floor-holding&#8221; and the &#8220;help-me-out&#8221;\nhypotheses.\n"
   ],
   "doi": "10.21437/Interspeech.2016-973"
  },
  "fogerty16_interspeech": {
   "authors": [
    [
     "Daniel",
     "Fogerty"
    ],
    [
     "Fei",
     "Chen"
    ]
   ],
   "title": "Vowel Fundamental and Formant Frequency Contributions to English and Mandarin Sentence Intelligibility",
   "original": "0028",
   "page_count": 5,
   "order": 292,
   "p1": "1382",
   "pn": "1386",
   "abstract": [
    "The current study investigated spectral components of vowels that contribute\nto Mandarin and English sentence intelligibility. Sentences were processed\nto preserve various amounts of vowel information. Processing parameters\nensured similar proportions of speech preserved between the two languages.\nIn the first experiment, speech segments, primarily containing vocalic\ncues, were processed to flatten fundamental frequency (F0) cues. In\nthe second experiment, sine-wave speech synthesis was used to coarsely\ncode speech to retain only amplitude and frequency variation associated\nwith the first three formants. Results demonstrated remarkable similarity\nbetween Mandarin and English sentence intelligibility with flattened\nF0 sentences. In contrast, the intelligibility of English sentences\nsurpassed that of Mandarin sentences for sine-wave speech. Combined\nwith earlier reports of superior intelligibility of Mandarin sentences\nwith full spectrum vowels, these results highlight significant contributions\nof Mandarin F0 information, likely related to lexical tone. In contrast,\nEnglish listeners may rely more on frequency and/or amplitude variation\nof the formants.\n"
   ],
   "doi": "10.21437/Interspeech.2016-28"
  },
  "huang16b_interspeech": {
   "authors": [
    [
     "Che-Wei",
     "Huang"
    ],
    [
     "Shrikanth S.",
     "Narayanan"
    ]
   ],
   "title": "Attention Assisted Discovery of Sub-Utterance Structure in Speech Emotion Recognition",
   "original": "0448",
   "page_count": 5,
   "order": 293,
   "p1": "1387",
   "pn": "1391",
   "abstract": [
    "Recently, attention mechanism based deep learning has gained much popularity\nin speech recognition and natural language processing due to its flexibility\nat the decoding phase. Through the attention mechanism, the relevant\nencoding context vectors contribute a majority portion to the construction\nof the decoding context, while the effect of the irrelevant ones is\nminimized. Inspired by this idea, a speech emotion recognition system\nis proposed in this work for an active selection of sub-utterance representations\nto better compose a discriminative utterance representation. Compared\nto the baseline of a model based on the uniform attention, i.e. no\nattention at all, an attention based model improves the weighted accuracy\nby an absolute of 1.46% (and relative 57.87% to 59.33%) on the emotion\nclassification task. Moreover, the selection distribution leads to\na better understanding of the sub-utterance structure in an emotional\nutterance.\n"
   ],
   "doi": "10.21437/Interspeech.2016-448"
  },
  "li16b_interspeech": {
   "authors": [
    [
     "Linchuan",
     "Li"
    ],
    [
     "Zhiyong",
     "Wu"
    ],
    [
     "Mingxing",
     "Xu"
    ],
    [
     "Helen",
     "Meng"
    ],
    [
     "Lianhong",
     "Cai"
    ]
   ],
   "title": "Combining CNN and BLSTM to Extract Textual and Acoustic Features for Recognizing Stances in Mandarin Ideological Debate Competition",
   "original": "0324",
   "page_count": 5,
   "order": 294,
   "p1": "1392",
   "pn": "1396",
   "abstract": [
    "Recognizing stances in ideological debates is a relatively new and\nchallenging problem in opinion mining. While previous work mainly focused\non text modality, in this paper, we try to recognize stances from both\ntext and acoustic modalities, where how to derive more representative\ntextual and acoustic features still remains the research problem. Inspired\nby the promising performances of neural network models in natural language\nunderstanding and speech processing, we propose a unified framework\nnamed C-BLSTM by combining convolutional neural network (CNN) and bidirectional\nlong short-term memory (BLSTM) recurrent neural network (RNN) for feature\nextraction. In C-BLSTM, CNN is utilized to extract higher-level local\nfeatures of text (n-grams) and speech (emphasis, intonation), while\nBLSTM is used to extract bottleneck features for context-sensitive\nfeature compression and target-related feature representation. Maximum\nentropy model is then used to recognize stances from the bimodal textual\nacoustic bottleneck features. Experiments on four debate datasets show\nC-BLSTM outperforms all challenging baseline methods, and specifically,\nacoustic intonation and emphasis features further improve F1-measure\nby 6% as compared to textual features only.\n"
   ],
   "doi": "10.21437/Interspeech.2016-324"
  },
  "trouvain16_interspeech": {
   "authors": [
    [
     "Jürgen",
     "Trouvain"
    ],
    [
     "Zofia",
     "Malisz"
    ]
   ],
   "title": "Inter-Speech Clicks in an Interspeech Keynote",
   "original": "1064",
   "page_count": 5,
   "order": 295,
   "p1": "1397",
   "pn": "1401",
   "abstract": [
    "Clicks are usually described as phoneme realisations in some African\nlanguages or as paralinguistic vocalisations, e.g. to signal disapproval\nor as sound imitation. A more recent discovery is that clicks are,\npresumably unintentionally, used as discourse markers indexing a new\nsequence in a conversation or before a word search. In this single-case\nstudy, we investigated more than 300 apical clicks of an experienced\nspeaker during a keynote address at an Interspeech conference. The\nproduced clicks occurred only in inter-speech intervals and were often\ncombined with either hesitation particles like &#8220;uhm&#8221; or\naudible inhalation. Our observations suggest a link between click production\nand ingressive airflow as well as indicate that clicks are used as\nhesitation markers. The rather high frequency of clicks in the analysed\nsections from the 1-hour-talk shows that in larger discourse, the time\nbetween articulatory phases consists of more than silence, audible\ninhalation and typical hesitation particles. The rather large variation\nin the intensity and duration and particularly the number of bursts\nof the observed clicks indicates that this prosodic discourse marker\nseems to be a rather acoustically inconsistent phonetic category.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1064"
  },
  "grzybowska16_interspeech": {
   "authors": [
    [
     "Joanna",
     "Grzybowska"
    ],
    [
     "Stanisław",
     "Kacprzak"
    ]
   ],
   "title": "Speaker Age Classification and Regression Using i-Vectors",
   "original": "1118",
   "page_count": 5,
   "order": 296,
   "p1": "1402",
   "pn": "1406",
   "abstract": [
    "In this paper, we examine the use of i-vectors both for age regression\nas well as for age classification. Although i-vectors have been previously\nused for age regression task, we extend this approach by applying fusion\nof i-vectors and acoustic features regression to estimate the speaker\nage. By our fusion we obtain a relative improvement of 12.6% comparing\nto solely i-vector system.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  We also use i-vectors\nfor age classification, which to our knowledge is the first attempt\nto do so. Our best results reach unweighted accuracy 62.9%, which is\na relative improvement of 16.7% comparing to the best results obtained\nin age classification task at  Age Sub-Challenge at Interspeech 2010.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1118"
  },
  "li16c_interspeech": {
   "authors": [
    [
     "Haoqi",
     "Li"
    ],
    [
     "Brian",
     "Baucom"
    ],
    [
     "Panayiotis",
     "Georgiou"
    ]
   ],
   "title": "Sparsely Connected and Disjointly Trained Deep Neural Networks for Low Resource Behavioral Annotation: Acoustic Classification in Couples&#8217; Therapy",
   "original": "1217",
   "page_count": 5,
   "order": 297,
   "p1": "1407",
   "pn": "1411",
   "abstract": [
    "Observational studies are based on accurate assessment of human state.\nA behavior recognition system that models interlocutors&#8217; state\nin real-time can significantly aid the mental health domain. However,\nbehavior recognition from speech remains a challenging task since it\nis difficult to find generalizable and representative features because\nof noisy and high-dimensional data, especially when data is limited\nand annotated coarsely and subjectively. Deep Neural Networks (DNN)\nhave shown promise in a wide range of machine learning tasks, but for\nBehavioral Signal Processing (BSP) tasks their application has been\nconstrained due to limited quantity of data.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  We propose a Sparsely-Connected\nand Disjointly-Trained DNN (SD-DNN) framework to deal with limited\ndata. First, we break the acoustic feature set into subsets and train\nmultiple distinct classifiers. Then, the hidden layers of these classifiers\nbecome parts of a deeper network that integrates all feature streams.\nThe overall system allows for full connectivity while limiting the\nnumber of parameters trained at any time and allows convergence possible\nwith even limited data. We present results on multiple behavior codes\nin the couples&#8217; therapy domain and demonstrate the benefits in\nbehavior classification accuracy. We also show the viability of this\nsystem towards live behavior annotations.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1217"
  },
  "an16_interspeech": {
   "authors": [
    [
     "Guozhen",
     "An"
    ],
    [
     "Sarah Ita",
     "Levitan"
    ],
    [
     "Rivka",
     "Levitan"
    ],
    [
     "Andrew",
     "Rosenberg"
    ],
    [
     "Michelle",
     "Levine"
    ],
    [
     "Julia",
     "Hirschberg"
    ]
   ],
   "title": "Automatically Classifying Self-Rated Personality Scores from Speech",
   "original": "1328",
   "page_count": 5,
   "order": 298,
   "p1": "1412",
   "pn": "1416",
   "abstract": [
    "Automatic personality recognition is useful for many computational\napplications, including recommendation systems, dating websites, and\nadaptive dialogue systems. There have been numerous successful approaches\nto classify the &#8220;Big Five&#8221; personality traits from a speaker&#8217;s\nutterance, but these have largely relied on judgments of personality\nobtained from external raters listening to the utterances in isolation.\nThis work instead classifies personality traits based on self-reported\npersonality tests, which are more valid and more difficult to identify.\nOur approach, which uses lexical and acoustic-prosodic features, yields\npredictions that are between 6.4% and 19.2% more accurate than chance.\nThis approach predicts Openness-to-Experience and Neuroticism most\nsuccessfully, with less accurate recognition of Extroversion. We compare\nthe performance of classification and regression techniques, and also\nexplore predicting personality clusters.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1328"
  },
  "lehman16_interspeech": {
   "authors": [
    [
     "Jill Fain",
     "Lehman"
    ],
    [
     "Rita",
     "Singh"
    ]
   ],
   "title": "Estimation of Children&#8217;s Physical Characteristics from Their Voices",
   "original": "0146",
   "page_count": 5,
   "order": 299,
   "p1": "1417",
   "pn": "1421",
   "abstract": [
    "To date, multiple strategies have been proposed for the estimation\nof speakers&#8217; physical parameters such as height, weight, age,\ngender etc. from their voices. These employ various types of feature\nmeasurements in conjunction with different regression and classification\nmechanisms. While some are quite effective for adults, they are not\nso for children&#8217;s voices. This is presumably because in children,\nthe relationship between voice and physical parameters is relatively\nmore complex. The vocal tracts of adults, and the processes that accompany\nspeech production, are fully mature and do not undergo changes within\nsmall age differentials. In children, however, these factors change\ncontinuously with age, causing variations in style, content, enunciation,\nrate and quality of their speech. Strategies for the estimation of\nchildren&#8217;s physical parameters from their voice must take this\nvariability into account. In this paper, using different formant-related\nmeasurements as exemplary analysis features generated within articulatory-phonetic\nguidelines, we demonstrate the nonlinear relationships of children&#8217;s\nphysical parameters to their voice. We also show how such analysis\ncan help us focus on the specific sounds that relate well to each parameter,\nwhich can be useful in obtaining more accurate estimates of the physical\nparameters.\n"
   ],
   "doi": "10.21437/Interspeech.2016-146"
  },
  "akira16_interspeech": {
   "authors": [
    [
     "Hayakawa",
     "Akira"
    ],
    [
     "Saturnino",
     "Luz"
    ],
    [
     "Nick",
     "Campbell"
    ]
   ],
   "title": "Talking to a System and Talking to a Human: A Study from a Speech-to-Speech, Machine Translation Mediated Map Task",
   "original": "1623",
   "page_count": 5,
   "order": 300,
   "p1": "1422",
   "pn": "1426",
   "abstract": [
    "This study focuses on the properties of Human-to-Human (H2H) communication\nin spontaneous dialogues in two different settings. Direct H2H dialogues\nare compared to the ones that are mediated by a Speech-to-Speech machine\ntranslation system. For the analysis, dialogues from the HCRC Map Task\nCorpus, for direct H2H conversations, and dialogues from the ILMT-s2s\nCorpus, for computer mediated conversations, were used. In the conversations\nspeakers take the roles of information giver and follower and all the\nutterances are labelled as instructions, questions or statement, etc.\nWhile direct H2H communication enables speakers also to benefit from\nnon-verbal acts, gestures and facial expressions, machine mediated\nconversation is more complex for the interlocutors. Due to errors made\nby speech recognition system, speakers adapt their speaking style and\nalso apply repair strategies in order to accomplish the tasks successfully.\nComparing the two corpora showed that in the case of computer mediated\ncommunication the utterances of the speakers contained less words than\nin the case of direct H2H interaction where utterances were longer.\nAlso, different word count was found depending on the role of the speaker\nas well as on the type of the utterance.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1623"
  },
  "gupta16b_interspeech": {
   "authors": [
    [
     "Rahul",
     "Gupta"
    ],
    [
     "Shrikanth S.",
     "Narayanan"
    ]
   ],
   "title": "Predicting Affective Dimensions Based on Self Assessed Depression Severity",
   "original": "0187",
   "page_count": 5,
   "order": 301,
   "p1": "1427",
   "pn": "1431",
   "abstract": [
    "Depression is a state of severe despondency and affects a person&#8217;s\nthoughts and behavior. Depression leads to several psychiatric symptoms\nsuch as fatigue, restlessness, insomnia as well as other mood disorders\n(e.g. anxiety and irritation). These symptoms have a resultant impact\non the subject&#8217;s emotional expression. In this work, we address\nthe problem of predicting the emotional dimensions of valence, arousal\nand dominance in subjects suffering from variable levels of depression,\nas quantified by the Beck Depression Inventory-II (BDI-II) index. We\ninvestigate the relationship between depression severity and affect,\nand propose a novel method for incorporating the BDI-II index in affect\nprediction. We validate our models on two datasets recorded as a part\nof the AViD (Audio-Visual Depressive language) corpus: Freeform and\nNorthwind. Using the depression severity and a set of audio-visual\ncues, we obtain an average correlation coefficient of .33/.52 for affective\ndimension prediction in the Freeform/Northwind datasets, against baseline\nperformances of .24/.48 based on using the audio-visual cues only.\nOur experiments suggest that the knowledge of depression severity significantly\nimproves the emotion dimension prediction, however the BDI-II score\nincorporation scheme varies between the two datasets of interest.\n"
   ],
   "doi": "10.21437/Interspeech.2016-187"
  },
  "huang16c_interspeech": {
   "authors": [
    [
     "Wen-Yu",
     "Huang"
    ],
    [
     "Shan-Wen",
     "Hsiao"
    ],
    [
     "Hung-Ching",
     "Sun"
    ],
    [
     "Ming-Chuan",
     "Hsieh"
    ],
    [
     "Ming-Hsueh",
     "Tsai"
    ],
    [
     "Chi-Chun",
     "Lee"
    ]
   ],
   "title": "Enhancement of Automatic Oral Presentation Assessment System Using Latent N-Grams Word Representation and Part-of-Speech Information",
   "original": "0400",
   "page_count": 5,
   "order": 302,
   "p1": "1432",
   "pn": "1436",
   "abstract": [
    "The development of an automatic oral presentation assessment system\nis important for the educational researchers to assess and train the\ncommunication ability of school leaders. In this work, we aim at enhancing\nthe performance of the existing pre-service school principals&#8217;\npresentation scoring system by including lexical information as an\nadditional modality. We propose to use latent n-grams distributed word\nrepresentations and weighted counts of part-of-speech tag to derive\nfeatures from the speech transcripts in the National Academy for Educational\nResearch (NAER) oral presentation database. We carry out two different\nexperiments: Exp I is a binary classification task between high versus\nlow performing speech, and Exp II is a continuous scoring on the entire\ndataset. In Exp I, the proposed framework achieves a competitive accuracy\nof 0.79, and in Exp II, by fusing this text-based system to the existing\naudio-video based system, we obtain a spearman correlation of 0.641\n(18.05% relative improvement). The two experiments demonstrate the\nmodeling power of our proposed framework and signify the substantial\ncomplementary information in the lexical modality while assessing the\nquality of an oral presentation.\n"
   ],
   "doi": "10.21437/Interspeech.2016-400"
  },
  "dumpala16b_interspeech": {
   "authors": [
    [
     "Sri Harsha",
     "Dumpala"
    ],
    [
     "P.",
     "Gangamohan"
    ],
    [
     "Suryakanth V.",
     "Gangashetty"
    ],
    [
     "B.",
     "Yegnanarayana"
    ]
   ],
   "title": "Use of Vowels in Discriminating Speech-Laugh from Laughter and Neutral Speech",
   "original": "1114",
   "page_count": 5,
   "order": 303,
   "p1": "1437",
   "pn": "1441",
   "abstract": [
    "In natural conversations, significant part of laughter co-occurs with\nspeech which is referred to as speech-laugh. Hence, speech-laugh will\nhave characteristics of both laughter and neutral speech. But it is\nnot clearly evident how acoustic properties of neutral speech are influenced\nby its co-occurring laughter. The objective of this study is to analyze\nthe acoustic variations between vowel regions of laughter, speech-laugh\nand neutral speech. The features based on excitation source characteristics\nextracted at epochs are considered in this study. Features extracted\nin the vowel regions of speech-laugh exhibit deviations from that of\nlaughter and neutral speech. These deviations in feature values are\nexploited to discriminate speech-laugh from laughter and neutral speech.\nTwo different datasets consisting of conversational speech and meeting\nrecordings are used in this analysis. Experimental results show that\nthe discrimination between the three classes obtained by considering\nvowel regions is better than that of considering the complete utterance.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1114"
  },
  "kawabata16_interspeech": {
   "authors": [
    [
     "Kan",
     "Kawabata"
    ],
    [
     "Visar",
     "Berisha"
    ],
    [
     "Anna",
     "Scaglione"
    ],
    [
     "Amy",
     "LaCross"
    ]
   ],
   "title": "A Convex Model for Linguistic Influence in Group Conversations",
   "original": "0498",
   "page_count": 5,
   "order": 304,
   "p1": "1442",
   "pn": "1446",
   "abstract": [
    "Conversational partners can influence each other&#8217;s speaking patterns.\nIn this paper, we aim to develop a computational model that infers\ninfluence levels directly from language samples. We propose a new approach\nto modeling linguistic influence in conversations based on a well-accepted\nmodel of social influence. Very generally, this approach assumes that\nan individual&#8217;s language model can be expressed as a convex combination\nof language models from individuals with whom that person interacts.\nWe propose an optimization criterion to estimate the pairwise influence\nbetween conversational partners directly from speech and language data.\nWe evaluate the model on three different corpora: (1) a synthetic corpus\nwhere the language influence is experimentally set; (2) a corpus that\ntracks a child&#8217;s interaction with her family during the early\nstages of language development; (3) a corpus of Supreme Court cases\nanalyzing interactions between judges and attorneys.\n"
   ],
   "doi": "10.21437/Interspeech.2016-498"
  },
  "gibson16_interspeech": {
   "authors": [
    [
     "James",
     "Gibson"
    ],
    [
     "Doğan",
     "Can"
    ],
    [
     "Bo",
     "Xiao"
    ],
    [
     "Zac E.",
     "Imel"
    ],
    [
     "David C.",
     "Atkins"
    ],
    [
     "Panayiotis",
     "Georgiou"
    ],
    [
     "Shrikanth S.",
     "Narayanan"
    ]
   ],
   "title": "A Deep Learning Approach to Modeling Empathy in Addiction Counseling",
   "original": "0554",
   "page_count": 5,
   "order": 305,
   "p1": "1447",
   "pn": "1451",
   "abstract": [
    "Motivational interviewing is a goal-oriented psychotherapy, employed\nin cases such as addiction, that aims to help clients explore and resolve\ntheir ambivalence about their problem. In motivational interviewing,\nit is desirable for the counselor to communicate empathy towards the\nclient to promote better therapy outcomes. In this paper, we propose\na deep neural network (DNN) system for predicting counselors&#8217;\nsession level empathy ratings from transcripts of the interactions.\nFirst, we train a recurrent neural network mapping the text of each\nspeaker turn to a set of task-specific behavioral acts that represent\nlocal dynamics of the client-counselor interaction. Subsequently, this\nnetwork is used to initialize lower layers of a deep network predicting\nsession level counselor empathy rating. We show that this method outperforms\ntraining the DNN end-to-end in a single stage and also outperforms\na baseline neural network model that attempts to predict empathy ratings\ndirectly from text without modeling turn level behavioral dynamics.\n"
   ],
   "doi": "10.21437/Interspeech.2016-554"
  },
  "huang16d_interspeech": {
   "authors": [
    [
     "Kun-Yi",
     "Huang"
    ],
    [
     "Chung-Hsien",
     "Wu"
    ],
    [
     "Yu-Ting",
     "Kuo"
    ],
    [
     "Fong-Lin",
     "Jang"
    ]
   ],
   "title": "Unipolar Depression vs. Bipolar Disorder: An Elicitation-Based Approach to Short-Term Detection of Mood Disorder",
   "original": "0620",
   "page_count": 5,
   "order": 306,
   "p1": "1452",
   "pn": "1456",
   "abstract": [
    "Mood disorders include unipolar depression (UD) and bipolar disorder\n(BD). In this work, an elicitation-based approach to short-term detection\nof mood disorder based on the elicited speech responses is proposed.\nFirst, a long-short term memory (LSTM)-based classifier was constructed\nto generate the emotion likelihood for each segment in the elicited\nspeech responses. The emotion likelihoods were then clustered into\nemotion codewords using the K-means algorithm. Latent semantic analysis\n(LSA) was then adopted to model the latent relationship between the\nemotion codewords and the elicited responses. The structural relationships\namong the emotion codewords in the LSA-based matrix were employed to\nconstruct a latent affective structure model (LASM) for characterizing\neach mood. For mood disorder detection, the similarity between the\ninput speech LASM and each of the mood-specific LASMs was estimated.\nFinally, the mood with its LASM most similar to the input speech LASM\nis regarded as the detected mood. Experimental results show that the\nproposed LASM-based method achieved 73.3%, improving the detection\naccuracy by 13.3% compared to the commonly used SVM-based classifiers.\n"
   ],
   "doi": "10.21437/Interspeech.2016-620"
  },
  "masmoudi16_interspeech": {
   "authors": [
    [
     "Abir",
     "Masmoudi"
    ],
    [
     "Mariem",
     "Ellouze"
    ],
    [
     "Fethi",
     "Bougares"
    ],
    [
     "Yannick",
     "Esètve"
    ],
    [
     "Lamia",
     "Belguith"
    ]
   ],
   "title": "Conditional Random Fields for the Tunisian Dialect Grapheme-to-Phoneme Conversion",
   "original": "1320",
   "page_count": 5,
   "order": 307,
   "p1": "1457",
   "pn": "1461",
   "abstract": [
    "Conditional Random Fields (CRFs) represent an effective approach for\nmonotone string-to-string translation tasks. In this work, we apply\nthe CRF model to perform grapheme-to-phoneme (G2P) conversion for the\nTunisian Dialect. This choice is motivated by the fact that CRFs give\na long term prediction and assume relaxed state independence conditions\ncompared to HMMs [7]. The CRF model needs to be trained on a 1-to-1\nalignement between graphemes and phonemes. Alignments are generated\nusing Joint-Multigram Model (JMM) and GIZA++ toolkit. We trained CRF\nmodel for each generated alignment. We then compared our models to\nstate-of-the-art G2P systems based on Sequitur G2P and Phonetisaurus\ntoolkit. We also investigate the CRF prediction quality with different\ntraining size. Our results show that CRF perform slightly better using\nJMM alignment and outperform both Sequitur and Phonetisaurus systems\nwith different training size. At the end, our system gets a phone error\nrate of 14.09%.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1320"
  },
  "saychum16_interspeech": {
   "authors": [
    [
     "Sittipong",
     "Saychum"
    ],
    [
     "Sarawoot",
     "Kongyoung"
    ],
    [
     "Anocha",
     "Rugchatjaroen"
    ],
    [
     "Patcharika",
     "Chootrakool"
    ],
    [
     "Sawit",
     "Kasuriya"
    ],
    [
     "Chai",
     "Wutiwiwatchai"
    ]
   ],
   "title": "Efficient Thai Grapheme-to-Phoneme Conversion Using CRF-Based Joint Sequence Modeling",
   "original": "0621",
   "page_count": 5,
   "order": 308,
   "p1": "1462",
   "pn": "1466",
   "abstract": [
    "This paper presents the successful results of applying joint sequence\nmodeling in Thai grapheme-to-phoneme conversion. The proposed method\nutilizes Conditional Random Fields (CRFs) in two-stage prediction.\nThe first CRF is used for textual syllable segmentation and syllable\ntype prediction. Graphemes and their corresponding phonemes are then\naligned using well-designed many-to-many alignment rules and outputs\ngiven by the first CRF. The second CRF, modeling the jointly aligned\nsequences, efficiently predicts phonemes. The proposed method obviously\nimproves the prediction of  linking syllables, normally hidden from\ntheir textual graphemes. Evaluation results show that the prediction\nword error rate (WER) of the proposed method reaches 13.66%, which\nis 11.09% lower than that of the baseline system.\n"
   ],
   "doi": "10.21437/Interspeech.2016-621"
  },
  "jaumardhakoun16_interspeech": {
   "authors": [
    [
     "Aurore",
     "Jaumard-Hakoun"
    ],
    [
     "Kele",
     "Xu"
    ],
    [
     "Clémence",
     "Leboullenger"
    ],
    [
     "Pierre",
     "Roussel-Ragot"
    ],
    [
     "Bruce",
     "Denby"
    ]
   ],
   "title": "An Articulatory-Based Singing Voice Synthesis Using Tongue and Lips Imaging",
   "original": "0385",
   "page_count": 5,
   "order": 309,
   "p1": "1467",
   "pn": "1471",
   "abstract": [
    "Ultrasound imaging of the tongue and videos of lips movements can be\nused to investigate specific articulation in speech or singing voice.\nIn this study, tongue and lips image sequences recorded during singing\nperformance are used to predict vocal tract properties via Line Spectral\nFrequencies (LSF). We focused our work on traditional Corsican singing\n&#8220;Cantu in paghjella&#8221;. A multimodal Deep Autoencoder (DAE)\nextracts salient descriptors directly from tongue and lips images.\nAfterwards, LSF values are predicted from the most relevant of these\nfeatures using a multilayer perceptron. A vocal tract model is derived\nfrom the predicted LSF, while a glottal flow model is computed from\na synchronized electroglottographic recording. Articulatory-based singing\nvoice synthesis is developed using both models. The quality of the\nprediction and singing voice synthesis using this method outperforms\nthe state of the art method.\n"
   ],
   "doi": "10.21437/Interspeech.2016-385"
  },
  "li16d_interspeech": {
   "authors": [
    [
     "Xu",
     "Li"
    ],
    [
     "Zhiyong",
     "Wu"
    ],
    [
     "Helen",
     "Meng"
    ],
    [
     "Jia",
     "Jia"
    ],
    [
     "Xiaoyan",
     "Lou"
    ],
    [
     "Lianhong",
     "Cai"
    ]
   ],
   "title": "Phoneme Embedding and its Application to Speech Driven Talking Avatar Synthesis",
   "original": "0363",
   "page_count": 5,
   "order": 310,
   "p1": "1472",
   "pn": "1476",
   "abstract": [
    "Word embedding has made great achievements in many natural language\nprocessing tasks. However, the attempt to apply word embedding to the\nfield of speech got few breakthroughs. The reason is that word vectors\nmainly contain semantic and syntactic information. Such high level\nfeatures are difficult to be directly incorporated in speech related\ntasks compared to acoustic or phoneme related features. In this paper,\nwe investigate the method for phoneme embedding to generate phoneme\nvectors carrying acoustic information for speech related tasks. One-hot\nrepresentations of phoneme labels are fed into embedding layer to generate\nphoneme vectors that are then passed through bidirectional long short-term\nmemory (BLSTM) recurrent neural network to predict acoustic features.\nWeights in embedding layer are updated through backpropagation during\ntraining. Analyses indicate that phonemes with similar acoustic pronunciations\nare close to each other in cosine distance in the generated phoneme\nvector space, and tend to be in the same category after k-means clustering.\nWe evaluate the phoneme embedding by applying the generated phoneme\nvector into speech driven talking avatar synthesis. Experimental results\nindicate that adding phoneme vector as features can achieve 10.2% relative\nimprovement in objective test.\n"
   ],
   "doi": "10.21437/Interspeech.2016-363"
  },
  "li16e_interspeech": {
   "authors": [
    [
     "Xu",
     "Li"
    ],
    [
     "Zhiyong",
     "Wu"
    ],
    [
     "Helen",
     "Meng"
    ],
    [
     "Jia",
     "Jia"
    ],
    [
     "Xiaoyan",
     "Lou"
    ],
    [
     "Lianhong",
     "Cai"
    ]
   ],
   "title": "Expressive Speech Driven Talking Avatar Synthesis with DBLSTM Using Limited Amount of Emotional Bimodal Data",
   "original": "0364",
   "page_count": 5,
   "order": 311,
   "p1": "1477",
   "pn": "1481",
   "abstract": [
    "One of the essential problems in synthesizing expressive talking avatar\nis how to model the interactions between emotional facial expressions\nand lip movements. Traditional methods either simplify such interactions\nthrough separately modeling lip movements and facial expressions, or\nrequire substantial high quality emotional audio-visual bimodal training\ndata which are usually difficult to collect. This paper proposes several\nmethods to explore different possibilities in capturing the interactions\nusing a large-scale neutral corpus in addition to a small size emotional\ncorpus with limited amount of data. To incorporate contextual influences,\ndeep bidirectional long short-term memory (DBLSTM) recurrent neural\nnetwork is adopted as the regression model to predict facial features\nfrom acoustic features, emotional states as well as contexts. Experimental\nresults indicate that the method by concatenating neutral facial features\nwith emotional acoustic features as the input of DBLSTM model achieves\nthe best performance in both objective and subjective evaluations.\n"
   ],
   "doi": "10.21437/Interspeech.2016-364"
  },
  "taylor16_interspeech": {
   "authors": [
    [
     "Sarah",
     "Taylor"
    ],
    [
     "Akihiro",
     "Kato"
    ],
    [
     "Iain",
     "Matthews"
    ],
    [
     "Ben",
     "Milner"
    ]
   ],
   "title": "Audio-to-Visual Speech Conversion Using Deep Neural Networks",
   "original": "0483",
   "page_count": 5,
   "order": 312,
   "p1": "1482",
   "pn": "1486",
   "abstract": [
    "We study the problem of mapping from acoustic to visual speech with\nthe goal of generating accurate, perceptually natural speech animation\nautomatically from an audio speech signal. We present a sliding window\ndeep neural network that learns a mapping from a window of acoustic\nfeatures to a window of visual features from a large audio-visual speech\ndataset. Overlapping visual predictions are averaged to generate continuous,\nsmoothly varying speech animation. We outperform a baseline HMM inversion\napproach in both objective and subjective evaluations and perform a\nthorough analysis of our results.\n"
   ],
   "doi": "10.21437/Interspeech.2016-483"
  },
  "nakashika16_interspeech": {
   "authors": [
    [
     "Toru",
     "Nakashika"
    ],
    [
     "Yasuhiro",
     "Minami"
    ]
   ],
   "title": "Generative Acoustic-Phonemic-Speaker Model Based on Three-Way Restricted Boltzmann Machine",
   "original": "1105",
   "page_count": 5,
   "order": 313,
   "p1": "1487",
   "pn": "1491",
   "abstract": [
    "In this paper, we argue the way of modeling speech signals based on\nthree-way restricted Boltzmann machine (3WRBM) for separating phonetic-related\ninformation and speaker-related information from an observed signal\nautomatically. The proposed model is an energy-based probabilistic\nmodel that includes three-way potentials of three variables: acoustic\nfeatures, latent phonetic features, and speaker-identity features.\nWe train the model so that it automatically captures the undirected\nrelationships among the three variables. Once the model is trained,\nit can be applied to many tasks in speech signal processing. For example,\ngiven a speech signal, estimating speaker-identity features is equivalent\nto speaker recognition; on the other hand, estimated latent phonetic\nfeatures may be helpful for speech recognition because they contain\nmore phonetic-related information than the acoustic features. Since\nthe model is generative, we can also apply it to voice conversion;\ni.e., we just estimate acoustic features from the phonetic features\nthat were estimated given the source speakers acoustic features along\nwith the desired speaker-identity features. In our experiments, we\ndiscuss the effectiveness of the speech modeling through a speaker\nrecognition, a speech (continuous phone) recognition, and a voice conversion\ntasks.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1105"
  },
  "toutios16_interspeech": {
   "authors": [
    [
     "Asterios",
     "Toutios"
    ],
    [
     "Tanner",
     "Sorensen"
    ],
    [
     "Krishna",
     "Somandepalli"
    ],
    [
     "Rachel",
     "Alexander"
    ],
    [
     "Shrikanth S.",
     "Narayanan"
    ]
   ],
   "title": "Articulatory Synthesis Based on Real-Time Magnetic Resonance Imaging Data",
   "original": "0596",
   "page_count": 5,
   "order": 314,
   "p1": "1492",
   "pn": "1496",
   "abstract": [
    "This paper presents a methodology for articulatory synthesis of running\nspeech in American English driven by real-time magnetic resonance imaging\n(rtMRI) mid-sagittal vocal-tract data. At the core of the methodology\nis a time-domain simulation of the propagation of sound in the vocal\ntract developed previously by Maeda. The first step of the methodology\nis the automatic derivation of air-tissue boundaries from the rtMRI\ndata. These articulatory outlines are then modified in a systematic\nway in order to introduce additional precision in the formation of\nconsonantal vocal-tract constrictions. Other elements of the methodology\ninclude a previously reported set of empirical rules for setting the\ntime-varying characteristics of the glottis and the velopharyngeal\nport, and a revised sagittal-to-area conversion. Results are promising\ntowards the development of a full-fledged text-to-speech synthesis\nsystem leveraging directly observed vocal-tract dynamics.\n"
   ],
   "doi": "10.21437/Interspeech.2016-596"
  },
  "xie16c_interspeech": {
   "authors": [
    [
     "Xurong",
     "Xie"
    ],
    [
     "Xunying",
     "Liu"
    ],
    [
     "Lan",
     "Wang"
    ]
   ],
   "title": "Deep Neural Network Based Acoustic-to-Articulatory Inversion Using Phone Sequence Information",
   "original": "0659",
   "page_count": 5,
   "order": 315,
   "p1": "1497",
   "pn": "1501",
   "abstract": [
    "In recent years, neural network based acoustic-to-articulatory inversion\napproaches have achieved the state-of-the-art performance. One major\nissue associated with these approaches is the lack of phone sequence\ninformation during inversion. In order to address this issue, this\npaper proposes an improved architecture hierarchically concatenating\nphone classification and articulatory inversion component DNNs to improve\narticulatory movement generation. On a Mandarin Chinese speech inversion\ntask, the proposed technique consistently outperformed a range of baseline\nDNN and RNN inversion systems constructed using no phone sequence information,\na mixture density parameter output layer, additional phone features\nat the input layer, or multi-task learning with additional monophone\noutput layer target labels, measured in terms of electromagnetic articulography\n(EMA) root mean square error (RMSE) and correlation. Further improvements\nwere obtained using the bottleneck features extracted from the proposed\nhierarchical articulatory inversion systems as auxiliary features in\ngeneralized variable parameter HMMs (GVP-HMMs) based inversion systems.\n"
   ],
   "doi": "10.21437/Interspeech.2016-659"
  },
  "liu16g_interspeech": {
   "authors": [
    [
     "Zheng-Chen",
     "Liu"
    ],
    [
     "Zhen-Hua",
     "Ling"
    ],
    [
     "Li-Rong",
     "Dai"
    ]
   ],
   "title": "Articulatory-to-Acoustic Conversion with Cascaded Prediction of Spectral and Excitation Features Using Neural Networks",
   "original": "0715",
   "page_count": 5,
   "order": 316,
   "p1": "1502",
   "pn": "1506",
   "abstract": [
    "This paper presents an articulatory-to-acoustic conversion method using\nelectromagnetic midsagittal articulography (EMA) measurements as input\nfeatures. Neural networks, including feed-forward deep neural networks\n(DNNs) and recurrent neural networks (RNNs) with long short-term term\nmemory (LSTM) cells, are adopted to map EMA features towards not only\nspectral features (i.e. mel-cepstra) but also excitation features (i.e.\npower, U/V flag and F0). Then speech waveforms are reconstructed using\nthe predicted spectral and excitation features. A cascaded prediction\nstrategy is proposed to utilize the predicted spectral features as\nauxiliary input to boost the prediction of excitation features. Experimental\nresults show that LSTM-RNN models can achieve better objective and\nsubjective performance in articulatory-to-spectral conversion than\nDNNs and Gaussian mixture models (GMMs). The strategy of cascaded prediction\ncan increase the accuracy of excitation feature prediction and the\nneural network-based methods also outperform the GMM-based approach\nwhen predicting power features.\n"
   ],
   "doi": "10.21437/Interspeech.2016-715"
  },
  "liberatore16_interspeech": {
   "authors": [
    [
     "Christopher",
     "Liberatore"
    ],
    [
     "Ricardo",
     "Gutierrez-Osuna"
    ]
   ],
   "title": "Generating Gestural Scores from Acoustics Through a Sparse Anchor-Based Representation of Speech",
   "original": "1336",
   "page_count": 5,
   "order": 317,
   "p1": "1507",
   "pn": "1511",
   "abstract": [
    "We present a procedure for generating gestural scores from speech acoustics.\nThe procedure is based on our recent SABR (sparse, anchor-based representation)\nalgorithm, which models the speech signal as a linear combination of\nacoustic anchors. We present modifications to SABR that encourage temporal\nsmoothness by restricting the number of anchors that can be active\nover an analysis window. We propose that peaks in the SABR weights\ncan be interpreted as &#8220;keyframes&#8221; that determine when vocal\ntract articulations occur. We validate the approach in two ways. First,\nwe compare SABR keyframes to maxima in the velocity of electromagnetic\narticulography (EMA) pellets from an articulatory corpus. Second, we\nuse keyframes and SABR weights to build a gestural score for the VocalTractLab\n(VTL) model, and compare synthetic EMA trajectories generated by VTL\nagainst those in the articulatory corpus. We find that SABR keyframes\noccur within 15&#8211;20 ms (on average) of EMA maxima, suggesting\nthat SABR keyframes can be used to identify articulatory phenomena.\nHowever, comparison of synthetic and real EMA pellets show moderate\ncorrelation on tongue pellets but weak correlation on lip pellets,\na result that may be due to differences between the VTL speaker model\nand the source speaker in our corpus.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1336"
  },
  "guennec16_interspeech": {
   "authors": [
    [
     "David",
     "Guennec"
    ],
    [
     "Damien",
     "Lolive"
    ]
   ],
   "title": "On the Suitability of Vocalic Sandwiches in a Corpus-Based TTS Engine",
   "original": "1222",
   "page_count": 5,
   "order": 318,
   "p1": "1512",
   "pn": "1516",
   "abstract": [
    "Unit selection speech synthesis systems generally rely on target and\nconcatenation costs for selecting the best unit sequence. The role\nof the concatenation cost is to insure that joining two voice segments\nwill not cause any acoustic artefact to appear. For this task, acoustic\ndistances (MFCC, F<SUB>0</SUB>) are typically used but in many cases,\nthis is not enough to prevent concatenation artefacts. Among other\nstrategies, the improvement of corpus covering by favoring units that\nnaturally support well the joining process (vocalic sandwiches) seems\nto be effective on TTS. In this paper, we investigate if vocalic sandwiches\ncan be used directly in the unit selection engine when the corpus was\nnot created using that principle. First, the sandwich approach is directly\ntransposed in the unit selection engine with a penalty that greatly\nfavors concatenation on sandwich boundaries. Second, a derived fuzzy\nversion is proposed to relax the penalty based on the concatenation\ncost, with respect to the cost distribution. We show that the sandwich\napproach, very efficient at the corpus creation step, seems to be inefficient\nwhen directly transposed in the unit selection engine. However, we\nobserve that the fuzzy approach enhances synthesis quality, especially\non sentences with high concatenation costs.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1222"
  },
  "moungsri16_interspeech": {
   "authors": [
    [
     "Decha",
     "Moungsri"
    ],
    [
     "Tomoki",
     "Koriyama"
    ],
    [
     "Takao",
     "Kobayashi"
    ]
   ],
   "title": "Unsupervised Stress Information Labeling Using Gaussian Process Latent Variable Model for Statistical Speech Synthesis",
   "original": "0273",
   "page_count": 5,
   "order": 319,
   "p1": "1517",
   "pn": "1521",
   "abstract": [
    "In Thai language, stress is an important prosodic feature that not\nonly affects naturalness but also has a crucial role in meaning of\nphrase-level utterance. It is seen that a speech synthesis model that\nis trained with lack of stress and phrase-level information causes\nincorrect tones and ambiguity in meaning of synthetic speech. Our previous\nwork has shown that manually annotated stress information improves\nnaturalness of synthetic speech. However, a high time consumption is\na drawback of the manual annotation. In this paper, we utilize an unsupervised\nlearning technique called Bayesian Gaussian process latent variable\nmodel (Bayesian GP-LVM) to automatically put stress annotation on the\ngiven training data. Stress related features are projected onto a latent\nspace in which syllables are easier classified into stressed/unstressed\nclasses. We use the stressed/unstressed information as an additional\ncontext in GPR-based speech synthesis. Experimental results show that\nthe proposed technique improves naturalness of synthetic speech as\nwell as accuracy of stressed/unstressed classification. Moreover, the\nproposed technique enables us to avoid ambiguity in meaning of synthetic\nspeech by providing intended stress position into context label sequence\nto be synthesized.\n"
   ],
   "doi": "10.21437/Interspeech.2016-273"
  },
  "ni16_interspeech": {
   "authors": [
    [
     "Jinfu",
     "Ni"
    ],
    [
     "Yoshinori",
     "Shiga"
    ],
    [
     "Hisashi",
     "Kawai"
    ]
   ],
   "title": "Using Zero-Frequency Resonator to Extract Multilingual Intonation Structure",
   "original": "1607",
   "page_count": 5,
   "order": 320,
   "p1": "1522",
   "pn": "1526",
   "abstract": [
    "Human uses expressive intonation to convey linguistic and paralinguistic\nmeaning, especially making focal prominence to give emphasis that highlights\nthe focus of speech. Automatic extraction of dynamic intonation feature\nfrom a speech corpus and representing it in a continuous form are desired\nin multilingual speech synthesis. This paper presents a method to extract\ndynamic prosodic structure from speech signal using zero-frequency\nresonator to detect glottal cycle epoch and filter both voice amplitude\nand fundamental frequency (F0) contours. We choose stable voice F0\nsegments free from micro-prosodic effect to recover relevant F0 trajectory\nof an utterance, taking into consideration of inter-correlation of\nmicro-prosody with phonetic segments and syllable structure of the\nutterance, and further filter out long-term global pitch movements.\nThe method is evaluated by objective tests upon multilingual speech\ncorpora including Chinese, Japanese, Korean, and Myanmar. Our experiment\nresults show that the extracted intonation contour can match F0 contour\nby conventional approach in very high accuracy and the estimated long-term\npitch movements demonstrate regular characteristics of intonation across\nlanguages. The proposed method is language-independent and robust to\nnoisy speech.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1607"
  },
  "yu16b_interspeech": {
   "authors": [
    [
     "Jia",
     "Yu"
    ],
    [
     "Xiong",
     "Xiao"
    ],
    [
     "Lei",
     "Xie"
    ],
    [
     "Eng Siong",
     "Chng"
    ],
    [
     "Haizhou",
     "Li"
    ]
   ],
   "title": "A DNN-HMM Approach to Story Segmentation",
   "original": "0873",
   "page_count": 5,
   "order": 321,
   "p1": "1527",
   "pn": "1531",
   "abstract": [
    "Hidden Markov model (HMM) is one of the popular techniques for story\nsegmentation, where hidden Markov states represent the topics, and\nthe emission distributions of n-gram language model (LM) are dependent\non the states. Given a text document, a Viterbi decoder finds the hidden\nstory sequence, with a change of topic indicating a story boundary.\nIn this paper, we propose a discriminative approach to story boundary\ndetection. In the HMM framework, we use deep neural network (DNN) to\nestimate the posterior probability of topics given the bag-of-words\nin the local context. We call it the DNN-HMM approach. We consider\nthe topic dependent LM as a generative modeling technique, and the\nDNN-HMM as the discriminative solution. Experiments on topic detection\nand tracking (TDT2) task show that DNN-HMM outperforms traditional\nn-gram LM approach significantly and achieves state-of-the-art performance.\n"
   ],
   "doi": "10.21437/Interspeech.2016-873"
  },
  "goldman16_interspeech": {
   "authors": [
    [
     "Jean-Philippe",
     "Goldman"
    ],
    [
     "Pierre-Edouard",
     "Honnet"
    ],
    [
     "Rob",
     "Clark"
    ],
    [
     "Philip N.",
     "Garner"
    ],
    [
     "Maria",
     "Ivanova"
    ],
    [
     "Alexandros",
     "Lazaridis"
    ],
    [
     "Hui",
     "Liang"
    ],
    [
     "Tiago",
     "Macedo"
    ],
    [
     "Beat",
     "Pfister"
    ],
    [
     "Manuel Sam",
     "Ribeiro"
    ],
    [
     "Eric",
     "Wehrli"
    ],
    [
     "Junichi",
     "Yamagishi"
    ]
   ],
   "title": "The SIWIS Database: A Multilingual Speech Database with Acted Emphasis",
   "original": "1003",
   "page_count": 4,
   "order": 322,
   "p1": "1532",
   "pn": "1535",
   "abstract": [
    "We describe here a collection of speech data of bilingual and trilingual\nspeakers of English, French, German and Italian. In the context of\nspeech to speech translation (S2ST), this database is designed for\nseveral purposes and studies: training CLSA systems (cross-language\nspeaker adaptation), conveying emphasis through S2ST systems, and evaluating\nTTS systems. More precisely, 36 speakers judged as accentless (22 bilingual\nand 14 trilingual speakers) were recorded for a set of 171 prompts\nin two or three languages, amounting to a total of 24 hours of speech.\nThese sets of prompts include 100 sentences from news, 25 sentences\nfrom Europarl, the same 25 sentences with one acted emphasised word,\n20 semantically unpredictable sentences, and finally a 240-word long\ntext. All in all, it yielded 64 bilingual session pairs of the six\npossible combinations of the four languages. The database is freely\navailable for non-commercial use and scientific research purposes.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1003"
  },
  "ylmaz16b_interspeech": {
   "authors": [
    [
     "Emre",
     "Yılmaz"
    ],
    [
     "Henk van den",
     "Heuvel"
    ],
    [
     "Jelske",
     "Dijkstra"
    ],
    [
     "Hans Van de",
     "Velde"
    ],
    [
     "Frederik",
     "Kampstra"
    ],
    [
     "Jouke",
     "Algra"
    ],
    [
     "David Van",
     "Leeuwen"
    ]
   ],
   "title": "Open Source Speech and Language Resources for Frisian",
   "original": "0048",
   "page_count": 5,
   "order": 323,
   "p1": "1536",
   "pn": "1540",
   "abstract": [
    "In this paper, we present several open source speech and language resources\nfor the under-resourced Frisian language. Frisian is mostly spoken\nin the province of Frysl&#226;n which is located in the north of the\nNetherlands. The native speakers of Frisian are Frisian-Dutch bilingual\nand often code-switch in daily conversations. The resources presented\nin this paper include a code-switching speech database containing radio\nbroadcasts, a phonetic lexicon with more than 70k words and a language\nmodel trained on a text corpus with more than 38M words. With this\ncontribution, we aim to share the Frisian resources we have collected\nin the scope of the FAME! project, in which a spoken document retrieval\nsystem is built for the disclosure of the regional broadcaster&#8217;s\nradio archives. These resources enable research on code-switching and\nlongitudinal speech and language change. Moreover, a sample automatic\nspeech recognition (ASR) recipe for the Kaldi toolkit will also be\nprovided online to facilitate the Frisian ASR research.\n"
   ],
   "doi": "10.21437/Interspeech.2016-48"
  },
  "kathol16_interspeech": {
   "authors": [
    [
     "Andreas",
     "Kathol"
    ],
    [
     "Elizabeth",
     "Shriberg"
    ],
    [
     "Massimilano de",
     "Zambotti"
    ]
   ],
   "title": "The SRI CLEO Speaker-State Corpus",
   "original": "1141",
   "page_count": 4,
   "order": 324,
   "p1": "1541",
   "pn": "1544",
   "abstract": [
    "We introduce the SRI CLEO (Conversational Language about Everyday Objects)\nSpeaker-State Corpus of speech, video, and biosignals. The goal of\nthe corpus is providing insight on the speech and physiological changes\nresulting from subtle, context-based influences on affect and cognition.\nSpeakers were prompted by collections of pictures of neutral everyday\nobjects and were instructed to provide speech related to any subset\nof the objects for a preset period of time (120 or 180 seconds depending\non task).<br style=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  The corpus provides signals for 43 speakers under four different\nspeaker-state conditions: (1) neutral and emotionally charged audiovisual\nbackground; (2) cognitive load; (3) time pressure; and (4) various\nacted emotions. Unlike previous studies that have linked speaker state\nto the content of the speaking task itself, the CLEO prompts remain\nlargely pragmatically, semantically, and affectively neutral across\nall conditions. This framework enables for more direct comparisons\nacross both conditions and speakers. The corpus also includes more\ntraditional speaker tasks involving reading and free-form reporting\nof neutral and emotionally charged content. The explored biosignals\ninclude skin conductance, respiration, blood pressure, and ECG. The\ncorpus is in the final stages of processing and will be made available\nto the research community.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1141"
  },
  "chen16g_interspeech": {
   "authors": [
    [
     "Nancy F.",
     "Chen"
    ],
    [
     "Rong",
     "Tong"
    ],
    [
     "Darren",
     "Wee"
    ],
    [
     "Peixuan",
     "Lee"
    ],
    [
     "Bin",
     "Ma"
    ],
    [
     "Haizhou",
     "Li"
    ]
   ],
   "title": "SingaKids-Mandarin: Speech Corpus of Singaporean Children Speaking Mandarin Chinese",
   "original": "0139",
   "page_count": 5,
   "order": 325,
   "p1": "1545",
   "pn": "1549",
   "abstract": [
    "We present  SingaKids-Mandarin, a speech corpus of 255 Singaporean\nchildren aged 7 to 12 reading Mandarin Chinese, for a total of 125\nhours of data (75 hours of speech) and 79,843 utterances. This corpus\nis phonetically balanced and detailed in human annotations, including\nphonetic transcriptions, lexical tone markings, and proficiency scoring\nat the utterance level. The reading scripts span a diverse set of utterance\nstyles, covering syllable-level minimal pairs, words, phrases, sentences,\nand short stories. We analyze the acoustic properties of Singaporean\nchildren. We also observe that while the lack of the neutral tone is\nthe same for Singaporean adults and children, the phonetic pronunciation\npatterns in these two age groups differ: although Singaporean adults\ntend to front their retroflex, nasal, and palatal consonants, Singaporean\nchildren show both fronting and backing in these consonants. For future\nwork, we plan to develop computer-assisted pronunciation training (CAPT)\nsystems with  SingaKids-Mandarin.\n"
   ],
   "doi": "10.21437/Interspeech.2016-139"
  },
  "richey16_interspeech": {
   "authors": [
    [
     "Colleen",
     "Richey"
    ],
    [
     "Cynthia",
     "D’Angelo"
    ],
    [
     "Nonye",
     "Alozie"
    ],
    [
     "Harry",
     "Bratt"
    ],
    [
     "Elizabeth",
     "Shriberg"
    ]
   ],
   "title": "The SRI Speech-Based Collaborative Learning Corpus",
   "original": "1541",
   "page_count": 5,
   "order": 326,
   "p1": "1550",
   "pn": "1554",
   "abstract": [
    "We introduce the SRI speech-based collaborative learning corpus, a\nnovel collection designed for the investigation and measurement of\nhow students collaborate together in small groups. This is a multi-speaker\ncorpus containing high-quality audio recordings of middle school students\nworking in groups of three to solve mathematical problems. Each student\nwas recorded via a head-mounted noise-cancelling microphone. Each group\nwas also recorded via a stereo microphone placed nearby. A total of\n80 sessions were collected with the participation of 134 students.\nThe average duration of a session was 20 minutes. All students spoke\nEnglish; for some students, English was a second language. Sessions\nhave been annotated with time stamps to indicate which mathematical\nproblem the students were solving and which student was speaking. Sessions\nhave also been hand annotated with common indicators of collaboration\nfor each speaker (e.g., inviting others to contribute, planning) and\nthe overall collaboration quality for each problem. The corpus will\nbe useful to education researchers interested in collaborative learning\nand to speech researchers interested in children&#8217;s speech, speech\nanalytics, and speech diarization. The corpus, both audio and annotation,\nwill be made available to researchers.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1541"
  },
  "ramakrishna16_interspeech": {
   "authors": [
    [
     "Anil",
     "Ramakrishna"
    ],
    [
     "Rahul",
     "Gupta"
    ],
    [
     "Ruth B.",
     "Grossman"
    ],
    [
     "Shrikanth S.",
     "Narayanan"
    ]
   ],
   "title": "An Expectation Maximization Approach to Joint Modeling of Multidimensional Ratings Derived from Multiple Annotators",
   "original": "0270",
   "page_count": 5,
   "order": 327,
   "p1": "1555",
   "pn": "1559",
   "abstract": [
    "Ratings from multiple human annotators are often pooled in applications\nwhere the ground truth is hidden. Examples include annotating perceived\nemotions and assessing quality metrics for speech and image. These\nratings are not restricted to a single dimension and can be multidimensional.\nIn this paper, we propose an Expectation-Maximization based algorithm\nto model such ratings. Our model assumes that there exists a latent\nmultidimensional ground truth that can be determined from the observation\nfeatures and that the ratings provided by the annotators are noisy\nversions of the ground truth. We test our model on a study conducted\non children with autism to predict a four dimensional rating of expressivity,\nnaturalness, pronunciation goodness and engagement. Our goal in this\napplication is to reliably predict the individual annotator ratings\nwhich can be used to address issues of cognitive load on the annotators\nas well as the rating cost. We initially train a baseline directly\npredicting annotator ratings from the features and compare it to our\nmodel under three different settings assuming: (i) each entry in the\nmultidimensional rating is independent of others, (ii) a joint distribution\namong rating dimensions exists, (iii) a partial set of ratings to predict\nthe remaining entries is available.\n"
   ],
   "doi": "10.21437/Interspeech.2016-270"
  },
  "matousek16_interspeech": {
   "authors": [
    [
     "Jindřich",
     "Matoušek"
    ],
    [
     "Daniel",
     "Tihelka"
    ]
   ],
   "title": "Voting Detector: A Combination of Anomaly Detectors to Reveal Annotation Errors in TTS Corpora",
   "original": "0442",
   "page_count": 5,
   "order": 328,
   "p1": "1560",
   "pn": "1564",
   "abstract": [
    "Anomaly detection techniques were shown to help in detecting word-level\nannotation errors in read-speech corpora for text-to-speech synthesis.\nIn this framework, correctly annotated words are considered as normal\nexamples on which the detection methods are trained. Misannotated words\nare then taken as anomalous examples which do not conform to normal\npatterns of the trained detection models. In this paper we propose\na concept of a voting detector &#8212; a combination of anomaly detectors\nin which each &#8220;single&#8221; detector &#8220;votes&#8221; on\nwhether a testing word is annotated correctly or not. The final decision\nis then made by aggregating the votes. Our experiments show that voting\ndetector has a potential to overcome each of the single anomaly detectors.\n"
   ],
   "doi": "10.21437/Interspeech.2016-442"
  },
  "corralesastorgano16_interspeech": {
   "authors": [
    [
     "Mario",
     "Corrales-Astorgano"
    ],
    [
     "David",
     "Escudero-Mancebo"
    ],
    [
     "César",
     "González-Ferreras"
    ],
    [
     "Yurena",
     "Gutiérrez-González"
    ],
    [
     "Valle",
     "Flores-Lucas"
    ],
    [
     "Valentín",
     "Cardeñoso-Payo"
    ],
    [
     "Lourdes",
     "Aguilar-Cuevas"
    ]
   ],
   "title": "The Magic Stone: A Video Game to Improve Communication Skills of People with Intellectual Disabilities",
   "original": "2017",
   "page_count": 2,
   "order": 329,
   "p1": "1565",
   "pn": "1566",
   "abstract": [
    "&#8220;The Magic Stone&#8221; is a video game whose main aim is to\nhelp people with Down syndrome to improve communication skills that\nhave been affected due to their disability, especially those related\nwith prosody. The interface of the video game includes a number of\nelements to motivate the users to practice and train their pronunciation.\nThe usability tests of the system have reported high degrees of satisfaction\nof users and trainers. Perception tests have permitted to confirm that\nplayers improve the use of prosody with the use.\n"
   ]
  },
  "kelly16_interspeech": {
   "authors": [
    [
     "Finnian",
     "Kelly"
    ],
    [
     "Anil",
     "Alexander"
    ],
    [
     "Oscar",
     "Forth"
    ],
    [
     "Samuel",
     "Kent"
    ],
    [
     "Jonas",
     "Lindh"
    ],
    [
     "Joel",
     "Åkesson"
    ]
   ],
   "title": "Identifying Perceptually Similar Voices with a Speaker Recognition System Using Auto-Phonetic Features",
   "original": "2018",
   "page_count": 2,
   "order": 330,
   "p1": "1567",
   "pn": "1568",
   "abstract": [
    "Assessing the perceptual similarity of voices is necessary for the\ncreation of voice parades, along with media applications such as voice\ncasting. These applications are normally prohibitively expensive to\nadminister, requiring significant amounts of &#8216;expert listening&#8217;.\nThe ability to automatically assess voice similarity could benefit\nthese applications by increasing efficiency and reducing subjectivity,\nwhile enabling the use of a much larger search space of candidate voices.\nIn this paper, the use of automatically extracted phonetic features\nwithin an i-vector speaker recognition system is proposed as a means\nof identifying cohorts of perceptually similar voices. Features considered\ninclude formants (F1-F4), fundamental frequency (F0), semitones of\nF0, and their derivatives. To demonstrate the viability of this approach,\na subset of the Interspeech 2016 special session &#8216;Speakers In\nThe Wild&#8217; (SITW) dataset is used in a pilot study comparing subjective\nlistener ratings of similarity with the output of the automatic system.\nIt is observed that the automatic system can locate cohorts of male\nvoices with good perceptual similarity. In addition to these experiments,\nthis proposal will be demonstrated with an application allowing a user\nto retrieve voices perceptually similar to their own from a large dataset.\n"
   ]
  },
  "james16_interspeech": {
   "authors": [
    [
     "Kristy",
     "James"
    ],
    [
     "Alexander",
     "Hewer"
    ],
    [
     "Ingmar",
     "Steiner"
    ],
    [
     "Stefanie",
     "Wuhrer"
    ]
   ],
   "title": "A Real-Time Framework for Visual Feedback of Articulatory Data Using Statistical Shape Models",
   "original": "2019",
   "page_count": 2,
   "order": 331,
   "p1": "1569",
   "pn": "1570",
   "abstract": [
    "We present a novel open-source framework for visualizing electromagnetic\narticulography (EMA) data in real-time, with a modular framework and\nanatomically accurate tongue and palate models derived by multilinear\nsubspace learning.\n"
   ]
  },
  "marin16_interspeech": {
   "authors": [
    [
     "Alex",
     "Marin"
    ],
    [
     "Paul",
     "Crook"
    ],
    [
     "Omar Zia",
     "Khan"
    ],
    [
     "Vasiliy",
     "Radostev"
    ],
    [
     "Khushboo",
     "Aggarwal"
    ],
    [
     "Ruhi",
     "Sarikaya"
    ]
   ],
   "title": "Flexible, Rapid Authoring of Goal-Orientated, Multi-Turn Dialogues Using the Task Completion Platform",
   "original": "2020",
   "page_count": 2,
   "order": 332,
   "p1": "1571",
   "pn": "1572",
   "abstract": [
    "The Task Completion Platform (TCP) is a multi-domain, multi-modal dialogue\nsystem that can host and execute large numbers of goal-orientated dialogue\ntasks. TCP is comprised of a task configuration language, TaskForm,\nand a task-independent dialogue runtime, allowing task definitions\nto be decoupled from the global dialogue policy used by the platform\nto execute the tasks. This separation enables scenario developers to\nrapidly develop new dialogue systems, by eliminating the need to re-implement\nthe policy from scratch for each new task. In this paper, we introduce\nsupport for authoring tasks in a variety of dialogue styles, ranging\nfrom entirely flexible to fully system-initiative. This flexibility\nis enabled by a set of task-level policy override constructs, which\naugment or constrain the default platform-level policy to achieve the\ndesired system behavior. We demonstrate the use of the TaskForm language\nto define complex, multi-turn tasks in a variety of domains and add\ndifferent task-specific policy constructs to demonstrate the flexibility\nof the task authoring process.\n"
   ]
  },
  "delcroix16_interspeech": {
   "authors": [
    [
     "Marc",
     "Delcroix"
    ],
    [
     "Keisuke",
     "Kinoshita"
    ],
    [
     "Atsunori",
     "Ogawa"
    ],
    [
     "Takuya",
     "Yoshioka"
    ],
    [
     "Dung T.",
     "Tran"
    ],
    [
     "Tomohiro",
     "Nakatani"
    ]
   ],
   "title": "Context Adaptive Neural Network for Rapid Adaptation of Deep CNN Based Acoustic Models",
   "original": "0203",
   "page_count": 5,
   "order": 333,
   "p1": "1573",
   "pn": "1577",
   "abstract": [
    "Using auxiliary input features has been seen as one of the most effective\nways to adapt deep neural network (DNN)-based acoustic models to speaker\nor environment. However, this approach has several limitations. It\nonly performs compensation of the bias term of the hidden layer and\ntherefore does not fully exploit the network capabilities. Moreover,\nit may not be well suited for certain types of architectures such as\nconvolutional neural networks (CNNs) because the auxiliary features\nhave different time-frequency structures from speech features. This\npaper resolves these problems by extending the recently proposed context\nadaptive DNN (CA-DNN) framework to CNN architectures. A CA-DNN is a\nDNN with one or several layers factorized in sub-layers associated\nwith an acoustic context class representing speaker or environment.\nThe output of the factorized layer is obtained as the weighted sum\nof the contributions of each sub-layer, weighted by acoustic context\nweights that are derived from auxiliary features such as i-vectors.\nImportantly, a CA-DNN can compensate both bias and weight matrices.\nIn this paper, we investigate the use of CA-DNN for deep CNN-based\narchitectures. We demonstrate consistent performance gains for utterance\nlevel rapid adaptation on the AURORA4 task over a strong network-in-network\nbased deep CNN architecture.\n"
   ],
   "doi": "10.21437/Interspeech.2016-203"
  },
  "lim16_interspeech": {
   "authors": [
    [
     "Boon Pang",
     "Lim"
    ],
    [
     "Faith",
     "Wong"
    ],
    [
     "Yuyao",
     "Li"
    ],
    [
     "Jia Wei",
     "Bay"
    ]
   ],
   "title": "Transfer Learning with Bottleneck Feature Networks for Whispered Speech Recognition",
   "original": "0250",
   "page_count": 5,
   "order": 334,
   "p1": "1578",
   "pn": "1582",
   "abstract": [
    "Previous work on whispered speech recognition has shown that acoustic\nmodels (AM) trained on whispered speech can somewhat classify unwhispered\n(neutral) speech sounds, but not vice versa. In fact, AMs trained purely\non neutral speech completely fail to recognize whispered speech. Meanwhile,\nrecipes used to train neutral AMs will work just as well for whispered\nspeech, but such methods require a large volume of transcribed whispered\nspeech which is expensive to gather. In this work, we propose and investigate\nthe use of bottleneck feature networks to normalize differences between\nwhispered and neutral speech modes. Our extensive experiments show\nthat this type of speech variability can be effectively normalized.\nWe also show that it is possible to transfer this knowledge from two\nsource languages with whispered speech (Mandarin and English), to a\nnew target language (Malay) without whispered speech. Furthermore,\nwe report a substantial reduction in word error rate for cross-mode\nspeech recognition, effectively demonstrate that it is possible to\ntrain acoustic models capable of classifying both types of speech without\nneeding any additional whispered speech.\n"
   ],
   "doi": "10.21437/Interspeech.2016-250"
  },
  "nagamine16b_interspeech": {
   "authors": [
    [
     "Tasha",
     "Nagamine"
    ],
    [
     "Zhuo",
     "Chen"
    ],
    [
     "Nima",
     "Mesgarani"
    ]
   ],
   "title": "Adaptation of Neural Networks Constrained by Prior Statistics of Node Co-Activations",
   "original": "0600",
   "page_count": 5,
   "order": 335,
   "p1": "1583",
   "pn": "1587",
   "abstract": [
    "We propose a novel unsupervised model adaptation framework in which\na neural network uses prior knowledge of the statistics of its output\nand hidden layer activations to update its parameters online to improve\nperformance in mismatched environments. This idea is inspired by biological\nneural networks, which use feedback to dynamically adapt their computation\nwhen faced with unexpected inputs. Here, we introduce an adaptation\ncriterion for deep neural networks based on the observation that in\nmatched testing and training conditions, the node co-activation statistics\nof each layer in a neural network are relatively stable over time.\nThe proposed method thus adapts the model layer by layer to minimize\nthe distance between the co-activation statistics of nodes in matched\nversus mismatched conditions. In phoneme classification experiments,\nwe show that such node co-activation constrained adaptation in a deep\nneural network model significantly improves the recognition accuracy\nover baseline performance when the system is tested in various novel\nnoises not included in the training.\n"
   ],
   "doi": "10.21437/Interspeech.2016-600"
  },
  "suzuki16_interspeech": {
   "authors": [
    [
     "Masayuki",
     "Suzuki"
    ],
    [
     "Ryuki",
     "Tachibana"
    ],
    [
     "Samuel",
     "Thomas"
    ],
    [
     "Bhuvana",
     "Ramabhadran"
    ],
    [
     "George",
     "Saon"
    ]
   ],
   "title": "Domain Adaptation of CNN Based Acoustic Models Under Limited Resource Settings",
   "original": "1161",
   "page_count": 5,
   "order": 336,
   "p1": "1588",
   "pn": "1592",
   "abstract": [
    "Adaptation of Automatic Speech Recognition (ASR) systems to a new domain\n(channel, speaker, topic, etc.) remains a significant challenge, as\noften, only a limited amount of target domain data for adaptation of\nAcoustic Models (AMs) is available. However, unlike GMMs, to date,\nthere has not been an established, efficient method for adapting current\nstate-of-the-art Convolutional Neural Network (CNN)-based AMs. In this\npaper, we explore various training algorithms for domain adaptation\nof CNN based speech recognition systems with limited acoustic training\ndata resources. Our investigations illustrate the following three main\ncontributions. First, introducing a weight decay based regularizer\nalong with the standard cross entropy criteria can significantly improve\nrecognition performances with as little as one hour of adaptation data.\nSecond, the observed gains can be improved further with the state-level\nMinimum Bayes Risk (sMBR) based sequence training technique. In addition\nto supervised training with limited amounts of data, we also study\nthe effect of introducing unsupervised data at both the initial cross-entropy\nand subsequent sequence training stages. Our experiments show that\nunsupervised data helps with cross-entropy and sequence training criteria.\nThird, the effect of speaker diversity in the adaptation data is also\ninvestigated where our experiments show that although there can be\nlarge variance in final performance depending on the speakers selected,\nregularization is required to obtain significant gains. Overall, we\ndemonstrate that with adaptation of neural network based acoustic models,\nwe can obtain performance improvements of up to 24.8% relative.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1161"
  },
  "samarakoon16_interspeech": {
   "authors": [
    [
     "Lahiru",
     "Samarakoon"
    ],
    [
     "Khe Chai",
     "Sim"
    ]
   ],
   "title": "Subspace LHUC for Fast Adaptation of Deep Neural Network Acoustic Models",
   "original": "1249",
   "page_count": 5,
   "order": 337,
   "p1": "1593",
   "pn": "1597",
   "abstract": [
    "Recently, the learning hidden unit contributions (LHUC) method is proposed\nfor the adaptation of deep neural network (DNN) based acoustic models\nfor automatic speech recognition (ASR). In LHUC, a set of speaker dependent\n(SD) parameters is estimated to linearly recombine the hidden units\nin an unsupervised fashion. Although LHUC performs considerably well,\nthe gains diminish when the availability of the adaptation data amount\ndecreases. Moreover, the per-speaker footprint of LHUC adaptation is\nin thousands and it is not desirable. Therefore, in this work, we propose\nthe subspace LHUC, where the SD parameters are estimated in a subspace\nand connected to various layers through a new set of adaptively trained\nweights. We evaluate the subspace LHUC in the Aurora4 and AMI IHM tasks.\nExperimental results show that the subspace LHUC outperforms standard\nLHUC adaptation. With utterance-level fast adaptation, the subspace\nLHUC achieved 11.3% and 4.5% relative improvements over the standard\nLHUC for the Aurora4 and AMI IHM tasks respectively. Furthermore, the\nsubspace LHUC reduces the per-speaker footprint by 94% over the standard\nLHUC adaptation.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1249"
  },
  "fainberg16_interspeech": {
   "authors": [
    [
     "Joachim",
     "Fainberg"
    ],
    [
     "Peter",
     "Bell"
    ],
    [
     "Mike",
     "Lincoln"
    ],
    [
     "Steve",
     "Renals"
    ]
   ],
   "title": "Improving Children&#8217;s Speech Recognition Through Out-of-Domain Data Augmentation",
   "original": "1348",
   "page_count": 5,
   "order": 338,
   "p1": "1598",
   "pn": "1602",
   "abstract": [
    "Children&#8217;s speech poses challenges to speech recognition due\nto strong age-dependent anatomical variations and a lack of large,\npublicly-available corpora. In this paper we explore data augmentation\nfor children&#8217;s speech recognition using stochastic feature mapping\n(SFM) to transform out-of-domain adult data for both GMM-based and\nDNN-based acoustic models. We performed experiments on the English\nPF-STAR corpus, augmenting using WSJCAM0 and ABI. Our experimental\nresults indicate that a DNN acoustic model for childrens speech can\nmake use of adult data, and that out-of-domain SFM is more accurate\nthan in-domain SFM.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1348"
  },
  "metze16_interspeech": {
   "authors": [
    [
     "Florian",
     "Metze"
    ],
    [
     "Eric",
     "Riebling"
    ],
    [
     "Anne S.",
     "Warlaumont"
    ],
    [
     "Elika",
     "Bergelson"
    ]
   ],
   "title": "Virtual Machines and Containers as a Platform for Experimentation",
   "original": "0997",
   "page_count": 5,
   "order": 339,
   "p1": "1603",
   "pn": "1607",
   "abstract": [
    "Research on computational speech processing has traditionally relied\non the availability of a relatively large and complex infrastructure,\nwhich encompasses data (text and audio), tools (feature extraction,\nmodel training, scoring, possibly on-line and off-line, etc.), glue\ncode, and computing. Traditionally, it has been very hard to move experiments\nfrom one site to another, and to replicate experiments. With the increasing\navailability of shared platforms such as commercial cloud computing\nplatforms or publicly funded super-computing centers, there is a need\nand an opportunity to abstract the experimental environment from the\nhardware, and distribute complete setups as a virtual machine, a container,\nor some other shareable resource, that can be deployed and worked with\nanywhere.<br style=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  In this paper, we discuss our experience with this concept and\npresent some tools that the community might find useful. We outline,\nas a case study, how such tools can be applied to a naturalistic language\nacquisition audio corpus.\n"
   ],
   "doi": "10.21437/Interspeech.2016-997"
  },
  "green16_interspeech": {
   "authors": [
    [
     "Phil",
     "Green"
    ],
    [
     "Ricard",
     "Marxer"
    ],
    [
     "Stuart",
     "Cunningham"
    ],
    [
     "Heidi",
     "Christensen"
    ],
    [
     "Frank",
     "Rudzicz"
    ],
    [
     "Maria",
     "Yancheva"
    ],
    [
     "André",
     "Coy"
    ],
    [
     "Massimiliano",
     "Malavasi"
    ],
    [
     "Lorenzo",
     "Desideri"
    ],
    [
     "Fabio",
     "Tamburini"
    ]
   ],
   "title": "CloudCAST &#8212; Remote Speech Technology for Speech Professionals",
   "original": "0148",
   "page_count": 5,
   "order": 340,
   "p1": "1608",
   "pn": "1612",
   "abstract": [
    "Recent advances in speech technology are potentially of great benefit\nto the professionals who help people with speech problems: therapists,\npathologists, educators and clinicians. There are 3 obstacles to progress\nwhich we seek to address in the CloudCAST project: &#8226; the design\nof applications deploying the technology should be user-driven; &#8226;\nthe computing resource should be available remotely; &#8226; the software\nshould be capable of personalisation: clinical applications demand\nindividual solutions.<br style=\"mso-data-placement:same-cell;\" /><br\nstyle=\"mso-data-placement:same-cell;\" />\n  CloudCAST aims to provide\nsuch a resource, and in addition to gather the data produced as the\napplications are used, to underpin the machine learning required for\nfurther progress.\n"
   ],
   "doi": "10.21437/Interspeech.2016-148"
  },
  "hain16_interspeech": {
   "authors": [
    [
     "Thomas",
     "Hain"
    ],
    [
     "Jeremy",
     "Christian"
    ],
    [
     "Oscar",
     "Saz"
    ],
    [
     "Salil",
     "Deena"
    ],
    [
     "Madina",
     "Hasan"
    ],
    [
     "Raymond W.M.",
     "Ng"
    ],
    [
     "Rosanna",
     "Milner"
    ],
    [
     "Mortaza",
     "Doulaty"
    ],
    [
     "Yulan",
     "Liu"
    ]
   ],
   "title": "webASR 2 &#8212; Improved Cloud Based Speech Technology",
   "original": "0700",
   "page_count": 5,
   "order": 341,
   "p1": "1613",
   "pn": "1617",
   "abstract": [
    "This paper presents the most recent developments of the webASR service\n(www.webasr.org), the world&#8217;s first web-based fully functioning\nautomatic speech recognition platform for scientific use. Initially\nreleased in 2008, the functionalities of webASR have recently been\nexpanded with 3 main goals in mind: Facilitate access through a RESTful\narchitecture, that allows for easy use through either the web interface\nor an API; allow the use of input metadata when available by the user\nto improve system performance; and increase the coverage of available\nsystems beyond speech recognition. Several new systems for transcription,\ndiarisation, lightly supervised alignment and translation are currently\navailable through webASR. The results in a series of well-known benchmarks\n(RT&#8217;09, IWSLT&#8217;12 and MGB&#8217;15 evaluations) show how\nthese webASR systems provides state-of-the-art performances across\nthese tasks.\n"
   ],
   "doi": "10.21437/Interspeech.2016-700"
  },
  "plummer16_interspeech": {
   "authors": [
    [
     "Andrew R.",
     "Plummer"
    ],
    [
     "Mary E.",
     "Beckman"
    ]
   ],
   "title": "Sharing Speech Synthesis Software for Research and Education Within Low-Tech and Low-Resource Communities",
   "original": "1540",
   "page_count": 5,
   "order": 342,
   "p1": "1618",
   "pn": "1622",
   "abstract": [
    "Parametric speech synthesis has played an integral role in speech research\nsince the 1950s. However, software sharing is unwieldy, making replication\nof experiments difficult, creating obstacles to communication between\nlaboratories, and hindering entry into research. This paper describes\nour use of the Speech Recognition Virtual Kitchen environment (www.speechkitchen.org)\nto develop an infrastructure for sharing synthesis software for research\nand education. We tested the infrastructure by using it in teaching\na seminar on &#8220;the speech science of speech synthesis&#8221; to\nstudents from several of the graduate programs in linguistics at the\nOhio State University. Using the virtual machines that we developed\nfor Klatt&#8217;s formant synthesis program and Kawahara&#8217;s STRAIGHT\nspeech analysis, modification, and synthesis system enabled the students\nto advance much further in their understanding of the basic principles\nunderlying these acoustic-domain models by comparison to the students\nenrolled in a similar seminar that we taught previously without the\nvirtual machines. At the same time, implementing these and two other\nvirtual machines for the course did not live up to our expectations\nfor the course, in ways that highlight the need to adapt both the Speech\nKitchen environment and the synthesis software systems to the needs\nof low-tech, low-resource users.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1540"
  },
  "sprouse16_interspeech": {
   "authors": [
    [
     "Ronald L.",
     "Sprouse"
    ],
    [
     "Keith",
     "Johnson"
    ]
   ],
   "title": "The Berkeley Phonetics Machine",
   "original": "0524",
   "page_count": 4,
   "order": 343,
   "p1": "1623",
   "pn": "1626",
   "abstract": [
    "The Berkeley Phonetics Machine is a Linux virtual machine image produced\nand used by the UC Berkeley Phonology Lab as a platform for phonetic\nresearch. It contains a full data analysis stack based on Python and\nR and also specialized tools for phonetic research. The machine is\ndesigned as a flexible and productive platform for established and\nnovel research agendas that can be easily shared and reproduced. We\nlist the software available in the machine, which includes many command-line\ntools for acoustic analysis and media file manipulation, as well as\nspecialized Python libraries. We also discuss the use of this machine\nin the Phonology Lab and in phonetics courses. The overall experience\nwith the machine has been positive, as faculty and graduate students\nare able to share and execute scripts in a common working environment.\nUndergraduate students have less opportunity to master the virtual\nmachine environment but benefit from simplified instructions and fewer\ninstallation and operating problems. The primary difficulty that we\nhave encountered has been with a few underpowered student computers\nthat cannot run the virtual machine or do not run it well.\n"
   ],
   "doi": "10.21437/Interspeech.2016-524"
  },
  "bates16_interspeech": {
   "authors": [
    [
     "Rebecca",
     "Bates"
    ],
    [
     "Eric",
     "Fosler-Lussier"
    ],
    [
     "Florian",
     "Metze"
    ],
    [
     "Martha",
     "Larson"
    ],
    [
     "Gina-Anne",
     "Levow"
    ],
    [
     "Emily Mower",
     "Provost"
    ]
   ],
   "title": "Experiences with Shared Resources for Research and Education in Speech and Language Processing",
   "original": "1223",
   "page_count": 5,
   "order": 344,
   "p1": "1627",
   "pn": "1631",
   "abstract": [
    "Resource barriers can prevent capable researchers from participating\nin the speech and language community and can make it difficult to support\nlearning and participation in our field at a wide variety of institutions.\nSharing resources, whether software, processed data, experimental methodologies\nor virtual machines, can reduce the barrier to entry and potentially\nbroaden participation in speech and language research and improve workforce\ndevelopment. As an introduction to the special session on Sharing Research\nand Education Resources for Understanding Speech Processing, we outline\ncurrent trends and requirements for expanding participation in speech\nprocessing research. A qualitative research approach was used. Faculty\nat a variety of institutions have been interviewed and have participated\nin reflection writing about needs, tools, challenges, and successes.\nThemes from reflections were generated using a grounded theory approach\nand were used to code interviews for related evidence. This paper describes\nthe educational and research challenges experienced by faculty as users\nof resources, rather than the details of specific resources provided.\nThe goal is to engage in a stronger dialog between users and providers\nso that needs and resources are better aligned. A case study of a shared\nresource used at several universities highlights this dialog.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1223"
  },
  "toda16_interspeech": {
   "authors": [
    [
     "Tomoki",
     "Toda"
    ],
    [
     "Ling-Hui",
     "Chen"
    ],
    [
     "Daisuke",
     "Saito"
    ],
    [
     "Fernando",
     "Villavicencio"
    ],
    [
     "Mirjam",
     "Wester"
    ],
    [
     "Zhizheng",
     "Wu"
    ],
    [
     "Junichi",
     "Yamagishi"
    ]
   ],
   "title": "The Voice Conversion Challenge 2016",
   "original": "1066",
   "page_count": 5,
   "order": 345,
   "p1": "1632",
   "pn": "1636",
   "abstract": [
    "This paper describes the Voice Conversion Challenge 2016 devised by\nthe authors to better understand different voice conversion (VC) techniques\nby comparing their performance on a common dataset. The task of the\nchallenge was speaker conversion, i.e., to transform the voice identity\nof a source speaker into that of a target speaker while preserving\nthe linguistic content. Using a common dataset consisting of 162 utterances\nfor training and 54 utterances for evaluation from each of 5 source\nand 5 target speakers, 17 groups working in VC around the world developed\ntheir own VC systems for every combination of the source and target\nspeakers, i.e., 25 systems in total, and generated voice samples converted\nby the developed systems. These samples were evaluated in terms of\ntarget speaker similarity and naturalness by 200 listeners in a controlled\nenvironment. This paper summarizes the design of the challenge, its\nresult, and a future plan to share views about unsolved problems and\nchallenges faced by the current VC techniques.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1066"
  },
  "wester16_interspeech": {
   "authors": [
    [
     "Mirjam",
     "Wester"
    ],
    [
     "Zhizheng",
     "Wu"
    ],
    [
     "Junichi",
     "Yamagishi"
    ]
   ],
   "title": "Analysis of the Voice Conversion Challenge 2016 Evaluation Results",
   "original": "1331",
   "page_count": 5,
   "order": 346,
   "p1": "1637",
   "pn": "1641",
   "abstract": [
    "The Voice Conversion Challenge 2016 is the first Voice Conversion Challenge\nin which different voice conversion systems and approaches using the\nsame voice data were compared. This paper describes the design of the\nevaluation, it presents the results and statistical analyses of the\nresults.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1331"
  },
  "chen16h_interspeech": {
   "authors": [
    [
     "Ling-Hui",
     "Chen"
    ],
    [
     "Li-Juan",
     "Liu"
    ],
    [
     "Zhen-Hua",
     "Ling"
    ],
    [
     "Yuan",
     "Jiang"
    ],
    [
     "Li-Rong",
     "Dai"
    ]
   ],
   "title": "The USTC System for Voice Conversion Challenge 2016: Neural Network Based Approaches for Spectrum, Aperiodicity and F<SUB>0</SUB> Conversion",
   "original": "0456",
   "page_count": 5,
   "order": 347,
   "p1": "1642",
   "pn": "1646",
   "abstract": [
    "This paper introduces the methods we adopt to build our system for\nthe evaluation event of Voice Conversion Challenge (VCC) 2016. We propose\nto use neural network-based approaches to convert both spectral and\nexcitation features. First, the generatively trained deep neural network\n(GTDNN) is adopted for spectral envelope conversion after the spectral\nenvelopes have been pre-processed by frequency warping. Second, we\npropose to use a recurrent neural network (RNN) with long short-term\nmemory (LSTM) cells for F0 trajectory conversion. In addition, we adopt\na DNN for band aperiodicity conversion. Both internal tests and formal\nVCC evaluation results demonstrate the effectiveness of the proposed\nmethods.\n"
   ],
   "doi": "10.21437/Interspeech.2016-456"
  },
  "mohammadi16_interspeech": {
   "authors": [
    [
     "Seyed Hamidreza",
     "Mohammadi"
    ],
    [
     "Alexander",
     "Kain"
    ]
   ],
   "title": "A Voice Conversion Mapping Function Based on a Stacked Joint-Autoencoder",
   "original": "1437",
   "page_count": 5,
   "order": 348,
   "p1": "1647",
   "pn": "1651",
   "abstract": [
    "In this study, we propose a novel method for training a regression\nfunction and apply it to a voice conversion task. The regression function\nis constructed using a Stacked Joint-Autoencoder (SJAE). Previously,\nwe have used a more primitive version of this architecture for pre-training\na Deep Neural Network (DNN). Using objective evaluation criteria, we\nshow that the lower levels of the SJAE perform best with a low degree\nof jointness, and higher levels with a higher degree of jointness.\nWe demonstrate that our proposed approach generates features that do\nnot suffer from the averaging effect inherent in back-propagation training.\nWe also carried out subjective listening experiments to evaluate speech\nquality and speaker similarity. Our results show that the SJAE approach\nhas both higher quality and similarity than a SJAE+DNN approach, where\nthe SJAE is used for pre-training a DNN, and the fine-tuned DNN is\nthen used for mapping. We also present the system description and results\nof our submission to Voice Conversion Challenge 2016.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1437"
  },
  "wu16c_interspeech": {
   "authors": [
    [
     "Yi-Chiao",
     "Wu"
    ],
    [
     "Hsin-Te",
     "Hwang"
    ],
    [
     "Chin-Cheng",
     "Hsu"
    ],
    [
     "Yu",
     "Tsao"
    ],
    [
     "Hsin-Min",
     "Wang"
    ]
   ],
   "title": "Locally Linear Embedding for Exemplar-Based Spectral Conversion",
   "original": "0567",
   "page_count": 5,
   "order": 349,
   "p1": "1652",
   "pn": "1656",
   "abstract": [
    "This paper describes a novel exemplar-based spectral conversion (SC)\nsystem developed by the AST (Academia Sinica, Taipei) team for the\n2016 voice conversion challenge (vcc2016). The key feature of our system\nis that it integrates the locally linear embedding (LLE) algorithm,\na manifold learning algorithm that has been successfully applied for\nthe super-resolution task in image processing, with the conventional\nexemplar-based SC method. To further improve the quality of the converted\nspeech, our system also incorporates (1) the maximum likelihood parameter\ngeneration (MLPG) algorithm, (2) the postfiltering-based global variance\n(GV) compensation method, and (3) a high-resolution feature extraction\nprocess. The results of subjective evaluation conducted by the vcc2016\norganizer show that our LLE-exemplar-based SC system notably outperforms\nthe baseline GMM-based system (implemented by the vcc2016 organizer).\nMoreover, our own internal evaluation results confirm the effectiveness\nof the major LLE-exemplar-based SC method and the three additional\napproaches with improved speech quality.\n"
   ],
   "doi": "10.21437/Interspeech.2016-567"
  },
  "villavicencio16_interspeech": {
   "authors": [
    [
     "Fernando",
     "Villavicencio"
    ],
    [
     "Junichi",
     "Yamagishi"
    ],
    [
     "Jordi",
     "Bonada"
    ],
    [
     "Felipe",
     "Espic"
    ]
   ],
   "title": "Applying Spectral Normalisation and Efficient Envelope Estimation and Statistical Transformation for the Voice Conversion Challenge 2016",
   "original": "0305",
   "page_count": 5,
   "order": 350,
   "p1": "1657",
   "pn": "1661",
   "abstract": [
    "In this work we present our entry for the Voice Conversion Challenge\n2016, denoting new features to previous work on GMM-based voice conversion.\nWe incorporate frequency warping and pitch transposition strategies\nto perform a normalisation of the spectral conditions, with benefits\nconfirmed by objective and perceptual means. Moreover, the results\nof the challenge showed our entry among the highest performing systems\nin terms of perceived naturalness while maintaining the target similarity\nperformance of GMM-based conversion. \n"
   ],
   "doi": "10.21437/Interspeech.2016-305"
  },
  "erro16_interspeech": {
   "authors": [
    [
     "D.",
     "Erro"
    ],
    [
     "A.",
     "Alonso"
    ],
    [
     "L.",
     "Serrano"
    ],
    [
     "D.",
     "Tavarez"
    ],
    [
     "I.",
     "Odriozola"
    ],
    [
     "Xabier",
     "Sarasola"
    ],
    [
     "Eder del",
     "Blanco"
    ],
    [
     "J.",
     "Sanchez"
    ],
    [
     "I.",
     "Saratxaga"
    ],
    [
     "Eva",
     "Navas"
    ],
    [
     "Inma",
     "Hernaez"
    ]
   ],
   "title": "ML Parameter Generation with a Reformulated MGE Training Criterion &#8212; Participation in the Voice Conversion Challenge 2016",
   "original": "0219",
   "page_count": 5,
   "order": 351,
   "p1": "1662",
   "pn": "1666",
   "abstract": [
    "This paper describes our entry to the Voice Conversion Challenge 2016.\nBased on the maximum likelihood parameter generation algorithm, the\nmethod is a reformulation of the minimum generation error training\ncriterion. It uses a GMM for soft classification, a Mel-cepstral vocoder\nfor acoustic analysis and an improved dynamic time warping procedure\nfor source-target alignment. To compensate the oversmoothing effect,\nthe generated parameters are filtered through a speaker-independent\npostfilter implemented as a linear transform in cepstral domain. The\nprocess is completed with mean and variance adaptation of the log-\nfundamental frequency and duration modification by a constant factor.\nThe results of the evaluation show that the proposed system achieves\na high conversion accuracy in comparison with other systems, while\nits naturalness scores are intermediate.\n"
   ],
   "doi": "10.21437/Interspeech.2016-219"
  },
  "kobayashi16_interspeech": {
   "authors": [
    [
     "Kazuhiro",
     "Kobayashi"
    ],
    [
     "Shinnosuke",
     "Takamichi"
    ],
    [
     "Satoshi",
     "Nakamura"
    ],
    [
     "Tomoki",
     "Toda"
    ]
   ],
   "title": "The NU-NAIST Voice Conversion System for the Voice Conversion Challenge 2016",
   "original": "0970",
   "page_count": 5,
   "order": 352,
   "p1": "1667",
   "pn": "1671",
   "abstract": [
    "This paper presents the NU-NAIST voice conversion (VC) system for the\nVoice Conversion Challenge 2016 (VCC 2016) developed by a joint team\nof Nagoya University and Nara Institute of Science and Technology.\nStatistical VC based on a Gaussian mixture model makes it possible\nto convert speaker identity of a source speaker&#8217; voice into that\nof a target speaker by converting several speech parameters. However,\nvarious factors such as parameterization errors and over-smoothing\neffects usually cause speech quality degradation of the converted voice.\nTo address this issue, we have proposed a direct waveform modification\ntechnique based on spectral differential filtering and have successfully\napplied it to singing voice conversion where excitation features are\nnot necessary converted. In this paper, we propose a method to apply\nthis technique to a standard voice conversion task where excitation\nfeature conversion is needed. The result of VCC 2016 demonstrates that\nthe NU-NAIST VC system developed by the proposed method yields the\nbest conversion accuracy for speaker identity (more than 70% of the\ncorrect rate) and quite high naturalness score (more than 3 of the\nmean opinion score). This paper presents detail descriptions of the\nNU-NAIST VC system and additional results of its performance evaluation.\n"
   ],
   "doi": "10.21437/Interspeech.2016-970"
  },
  "landerportnoy16_interspeech": {
   "authors": [
    [
     "Maury",
     "Lander-Portnoy"
    ]
   ],
   "title": "Release from Energetic Masking Caused by Repeated Patterns of Glimpsing Windows",
   "original": "1571",
   "page_count": 5,
   "order": 353,
   "p1": "1672",
   "pn": "1676",
   "abstract": [
    "The study of auditory masking not only provides data for how healthy\nand impaired listeners perform in adverse listening conditions, and\nthereby approximates their ability to perceive speech in the noisy\nenvironments of everyday life, but also provides insights into the\nmechanisms that underly the detection and perception of speech. Previous\nstudies, (Pollack 1955) (Festen &amp; Plomp 1990) (Cooper et al. 2015),\nhave manipulated noise maskers in an attempt to observe the relationship\nbetween modulation of the type or characteristics of masking noise\nto subjects ability to detect or recognize a target signal. In this\nexperiment, long term average spectrum speech shaped noise maskers\nwere modulated to allow either short or long glimpsing (Cooke 2005)\nwindows, during which the target signal was unmasked, in one second\nlong morse code patterns of eight windows. The results from 60 participants\nwith normal hearing showed that subjects performed significantly better\non trials of an open set word recognition task when the pattern of\nglimpsing windows repeated twice before presentation of the masked\nsignal than a control with the same glimpsing windows during the signal\nbut different beforehand and one with the same amount of noise masking\nin random patterns before and during the target.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1571"
  },
  "gibbs16_interspeech": {
   "authors": [
    [
     "Bobby",
     "Gibbs"
    ],
    [
     "Daniel",
     "Fogerty"
    ]
   ],
   "title": "Glimpsing Predictions for Natural and Vocoded Sentence Intelligibility During Modulation Masking: Effect of the Glimpse Cutoff Criterion",
   "original": "1587",
   "page_count": 5,
   "order": 354,
   "p1": "1677",
   "pn": "1681",
   "abstract": [
    "This study varied the signal-to-noise ratio (SNR) cutoff criterion\nfor acoustically defining usable perceptual glimpses that contribute\nto speech intelligibility. Criterion-dependent effects were determined\nby examining the correlation of three different acoustic glimpse metrics\nwith intelligibility. Glimpse properties change depending on the acoustic\ninteractions between the speech and competing noise. Therefore, these\nmeasures were investigated with different rates of competing speech\nthat were varied using time compression or expansion. Finally, effects\nof temporal modulation masking and spectral segregation were examined\nby comparison between unprocessed (natural) and vocoded speech. Results\nrevealed a range of SNR cutoffs that were associated with correlations\nbetween the different acoustic glimpse metrics and intelligibility.\nChanging the glimpse criterion strongly influenced the associations\nbetween intelligibility and two of the acoustic glimpse metrics for\nthe different masker modulation rates. However, the proportion of target\nspeech above the SNR cutoff was less affected by altering the cutoff\ncriterion. These results suggest that intelligibility models should\naccount for the perceptual contribution of different glimpse metrics\nor limit glimpse cutoff criteria to an SNR region (1&#8211;3 dB based\non this data) that captures the perceptual utility of multiple glimpse\nmechanisms.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1587"
  },
  "xu16b_interspeech": {
   "authors": [
    [
     "Li",
     "Xu"
    ]
   ],
   "title": "Temporal Envelopes in Sine-Wave Speech Recognition",
   "original": "0171",
   "page_count": 5,
   "order": 355,
   "p1": "1682",
   "pn": "1686",
   "abstract": [
    "There is a long debate on the relative importance of spectral and temporal\ncues in speech perception theories. On the one hand, the highly-intelligible\nsine-wave speech (SWS) has been viewed as a representation of the global\nspectral structure of the speech signal. On the other hand, there is\naccumulating evidence showing that the temporal aspects of speech without\nspectral details provide sufficient speech understanding. The present\nstudy explored whether the temporal envelopes imbedded in the SWS contribute\nto its intelligibility. In the experiments, both SWS and natural speech\nsignals were processed with noise and tone vocoders to remove the spectral\ndetails but to preserve the temporal envelopes. Twenty-two normal-hearing,\nnative English-speaking adult listeners participated in sentence recognition\ntasks. Speech recognition performance of vocoder-processed SWS was\nslightly inferior to that of vocoder-processed natural speech but both\nreached plateau performance at 6&#8211;8 channels. Acoustic analysis\nfurther indicated that the temporal envelopes of the SWS were almost\nidentical to those of the natural speech, with a mean correlation coefficient\nr = 0.949 across all sentences. The results provide strong evidence\nthat the SWS represents both spectral and temporal structures of the\nspeech and that the temporal envelopes imbedded in SWS carry important\ninformation for speech recognition.\n"
   ],
   "doi": "10.21437/Interspeech.2016-171"
  },
  "liu16h_interspeech": {
   "authors": [
    [
     "Jing",
     "Liu"
    ],
    [
     "Rosanna H.N.",
     "Tong"
    ],
    [
     "Fei",
     "Chen"
    ]
   ],
   "title": "Understanding Periodically Interrupted Mandarin Speech",
   "original": "0176",
   "page_count": 5,
   "order": 356,
   "p1": "1687",
   "pn": "1691",
   "abstract": [
    "This study investigated the effects of two parameters (i.e., interruption\nrate, and duty cycle of interruption) on the perception of periodically\ninterrupted Mandarin speech. Normal-hearing listeners were instructed\nto identify consonant/vowel/tone/word from isolated Mandarin words\nand recognize Mandarin sentences when they were temporally interrupted\nby square wave. Results showed that consistent with earlier findings\nobtained with English speech, interruption with a large rate or duty\ncycle favored the perception of periodically interrupted Mandarin speech.\nIn addition, for isolated Mandarin word, the perception of vowel or\ntone was less affected by periodical interruption than that of consonant,\nand under periodical interruption the perception of consonant could\nlargely account for the recognition of Mandarin word. For Mandarin\nsentence, the tonal characteristics and the simpler syllable structure\nin Mandarin might facilitate spectral-temporal integration of the target\nwords, which contributed to a sentence intelligibility advantage of\nMandarin over English under interrupted conditions.\n"
   ],
   "doi": "10.21437/Interspeech.2016-176"
  },
  "chen16i_interspeech": {
   "authors": [
    [
     "Fei",
     "Chen"
    ],
    [
     "Daniel",
     "Fogerty"
    ]
   ],
   "title": "Factors Affecting the Intelligibility of Sine-Wave Speech",
   "original": "0004",
   "page_count": 4,
   "order": 357,
   "p1": "1692",
   "pn": "1695",
   "abstract": [
    "Studies on sine-wave speech (SWS) perception suggest that formants\ncontain sufficient information for sentence intelligibility. This study\nfurther investigated the effects of amplitude modulation, number of\nsine-waves, and vowel resonance in SWS recognition. Results showed\nthat Mandarin sentences synthesized using frequency trajectories of\nthe first two formants were highly intelligible with additional contributions\nfrom formant amplitude modulation. However, amplitude modulation significantly\ncontributed to intelligibility when only the vowels were preserved.\nThe present work demonstrates that the intelligibility of Mandarin\nSWS can be largely attributed to the frequency transition of the first\ntwo formants and is susceptible to temporal interruption.\n"
   ],
   "doi": "10.21437/Interspeech.2016-4"
  },
  "hodoshima16_interspeech": {
   "authors": [
    [
     "Nao",
     "Hodoshima"
    ]
   ],
   "title": "Effects of Urgent Speech and Preceding Sounds on Speech Intelligibility in Noisy and Reverberant Environments",
   "original": "1618",
   "page_count": 4,
   "order": 358,
   "p1": "1696",
   "pn": "1699",
   "abstract": [
    "Public-address (PA) announcements are used to convey emergency information;\nhowever, noise and reverberation sometimes make announcements in public\nspaces unintelligible. Therefore, the present study investigated how\ncombinations of speech spoken in an urgent style and preceding sounds\naffect speech intelligibility and perceived urgency in noisy and reverberant\nenvironments. Sentences were spoken in normal or urgent styles and\npreceded by either two sounds (siren sound or ocean wave-like sound)\nor no sounds. Eighteen young participants carried out word identification\ntest and rated perceived urgency on five-point scales in noisy and\nreverberant environments. The results showed that the urgently spoken\nspeech had significantly higher speech intelligibility than the normal\nspeech. The urgently spoken speech preceded by the wave-like sound\nshowed significantly higher speech intelligibility than normal speech\nwithout sounds, normal speech preceded by the siren sound, and urgently\nspoken speech preceded by the siren sound. The results also demonstrated\nthat the perceived urgency was rated higher for the urgently spoken\nspeech than that for the normal speech, regardless of the types of\npreceding sounds. These results suggest that appropriate combinations\nof speaking styles and alerting sounds will increase the intelligibility\nof emergency PA announcements.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1618"
  },
  "sahidullah16_interspeech": {
   "authors": [
    [
     "Md.",
     "Sahidullah"
    ],
    [
     "Héctor",
     "Delgado"
    ],
    [
     "Massimiliano",
     "Todisco"
    ],
    [
     "Hong",
     "Yu"
    ],
    [
     "Tomi",
     "Kinnunen"
    ],
    [
     "Nicholas",
     "Evans"
    ],
    [
     "Zheng-Hua",
     "Tan"
    ]
   ],
   "title": "Integrated Spoofing Countermeasures and Automatic Speaker Verification: An Evaluation on ASVspoof 2015",
   "original": "1280",
   "page_count": 5,
   "order": 359,
   "p1": "1700",
   "pn": "1704",
   "abstract": [
    "It is well known that automatic speaker verification (ASV) systems\ncan be vulnerable to spoofing. The community has responded to the threat\nby developing dedicated countermeasures aimed at detecting spoofing\nattacks. Progress in this area has accelerated over recent years, partly\nas a result of the first standard evaluation, ASVspoof 2015, which\nfocused on spoofing detection in isolation from ASV. This paper investigates\nthe integration of state-of-the-art spoofing countermeasures in combination\nwith ASV. Two general strategies to countermeasure integration are\nreported: cascaded and parallel. The paper reports the first comparative\nevaluation of each approach performed with the ASVspoof 2015 corpus.\nResults indicate that, even in the case of varying spoofing attack\nalgorithms, ASV performance remains robust when protected with a diverse\nset of integrated countermeasures.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1280"
  },
  "korshunov16_interspeech": {
   "authors": [
    [
     "Pavel",
     "Korshunov"
    ],
    [
     "Sébastien",
     "Marcel"
    ]
   ],
   "title": "Cross-Database Evaluation of Audio-Based Spoofing Detection Systems",
   "original": "1326",
   "page_count": 5,
   "order": 360,
   "p1": "1705",
   "pn": "1709",
   "abstract": [
    "Since automatic speaker verification (ASV) systems are highly vulnerable\nto spoofing attacks, it is important to develop mechanisms that can\ndetect such attacks. To be practical, however, a spoofing attack detection\napproach should have (i) high accuracy, (ii) be well-generalized for\npractical attacks, and (iii) be simple and efficient. Several audio-based\nspoofing detection methods have been proposed recently but their evaluation\nis limited to less realistic databases containing homogeneous data.\nIn this paper, we consider eight existing presentation attack detection\n(PAD) methods and evaluate their performance using two major publicly\navailable speaker databases with spoofing attacks: AVspoof and ASVspoof.\nWe first show that realistic presentation attacks (speech is replayed\nto PAD system) are significantly more challenging for the considered\nPAD methods compared to the so called &#8216;logical access&#8217;\nattacks (speech is presented to PAD system directly). Then, via a cross-database\nevaluation, we demonstrate that the existing methods generalize poorly\nwhen different databases or different types of attacks are used for\ntraining and testing. The results question the efficiency and practicality\nof the existing PAD systems, as well as, call for creation of databases\nwith larger variety of realistic speech presentation attacks.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1326"
  },
  "sriskandaraja16_interspeech": {
   "authors": [
    [
     "Kaavya",
     "Sriskandaraja"
    ],
    [
     "Vidhyasaharan",
     "Sethu"
    ],
    [
     "Phu Ngoc",
     "Le"
    ],
    [
     "Eliathamby",
     "Ambikairajah"
    ]
   ],
   "title": "Investigation of Sub-Band Discriminative Information Between Spoofed and Genuine Speech",
   "original": "0844",
   "page_count": 5,
   "order": 361,
   "p1": "1710",
   "pn": "1714",
   "abstract": [
    "A speaker verification system should include effective precautions\nagainst malicious spoofing attacks, and although some initial countermeasures\nhave been recently proposed, this remains a challenging research problem.\nThis paper investigates discrimination between spoofed and genuine\nspeech, as a function of frequency bands, across the speech bandwidth.\nFindings from our investigation inform some proposed filter bank design\napproaches for discrimination of spoofed speech. Experiments are conducted\non the Spoofing and Anti-Spoofing (SAS) corpus using the proposed frequency-selective\napproach demonstrates an 11% relative improvement in terms of equal\nerror rate compared with a conventional mel filter bank.\n"
   ],
   "doi": "10.21437/Interspeech.2016-844"
  },
  "tian16_interspeech": {
   "authors": [
    [
     "Xiaohai",
     "Tian"
    ],
    [
     "Zhizheng",
     "Wu"
    ],
    [
     "Xiong",
     "Xiao"
    ],
    [
     "Eng Siong",
     "Chng"
    ],
    [
     "Haizhou",
     "Li"
    ]
   ],
   "title": "An Investigation of Spoofing Speech Detection Under Additive Noise and Reverberant Conditions",
   "original": "0743",
   "page_count": 5,
   "order": 362,
   "p1": "1715",
   "pn": "1719",
   "abstract": [
    "Spoofing detection for automatic speaker verification (ASV), which\nis to discriminate between live and artificial speech, has received\nincreasing attentions recently. However, the previous studies have\nbeen done on the clean data without significant noise. It is still\nnot clear whether the spoofing detectors trained on clean speech can\ngeneralise well under noisy conditions. In this work, we perform an\ninvestigation of spoofing detection under additive noise and reverberant\nconditions. In particular, we consider five difference additive noises\nat three different signal-to-noise ratios (SNR), and a reverberation\nnoise with different reverberation time (RT). Our experimental results\nreveal that additive noises degrade the spoofing detectors trained\non clean speech significantly. However, the reverberation does not\nhurt the performance too much.\n"
   ],
   "doi": "10.21437/Interspeech.2016-743"
  },
  "sahidullah16b_interspeech": {
   "authors": [
    [
     "Md.",
     "Sahidullah"
    ],
    [
     "Rosa Gonzalez",
     "Hautamäki"
    ],
    [
     "Dennis Alexander Lehmann",
     "Thomsen"
    ],
    [
     "Tomi",
     "Kinnunen"
    ],
    [
     "Zheng-Hua",
     "Tan"
    ],
    [
     "Ville",
     "Hautamäki"
    ],
    [
     "Robert",
     "Parts"
    ],
    [
     "Martti",
     "Pitkänen"
    ]
   ],
   "title": "Robust Speaker Recognition with Combined Use of Acoustic and Throat Microphone Speech",
   "original": "1153",
   "page_count": 5,
   "order": 363,
   "p1": "1720",
   "pn": "1724",
   "abstract": [
    "Accuracy of automatic speaker recognition (ASV) systems degrades severely\nin the presence of background noise. In this paper, we study the use\nof additional side information provided by a body-conducted sensor,\nthroat microphone. Throat microphone signal is much less affected by\nbackground noise in comparison to acoustic microphone signal. This\nmakes throat microphones potentially useful for feature extraction\nor speech activity detection. This paper, firstly, proposes a new prototype\nsystem for simultaneous data-acquisition of acoustic and throat microphone\nsignals. Secondly, we study the use of this additional information\nfor both speech activity detection, feature extraction and fusion of\nthe acoustic and throat microphone signals. We collect a pilot database\nconsisting of 38 subjects including both clean and noisy sessions.\nWe carry out speaker verification experiments using Gaussian mixture\nmodel with universal background model (GMM-UBM) and i-vector based\nsystem. We have achieved considerable improvement in recognition accuracy\neven in highly degraded conditions.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1153"
  },
  "meng16b_interspeech": {
   "authors": [
    [
     "Zhong",
     "Meng"
    ],
    [
     "Biing-Hwang",
     "Juang"
    ]
   ],
   "title": "Statistical Modeling of Speaker&#8217;s Voice with Temporal Co-Location for Active Voice Authentication",
   "original": "0650",
   "page_count": 5,
   "order": 364,
   "p1": "1725",
   "pn": "1729",
   "abstract": [
    "Active voice authentication (AVA) is a new mode of talker authentication,\nin which the authentication is performed continuously on very short\nsegments of the voice signal, which may have instantaneously undergone\nchange of talker. AVA is necessary in providing real-time monitoring\nof a device authorized for a particular user. The authentication test\nthus cannot rely on a long history of the voice data nor any past decisions.\nMost conventional voice authentication techniques that operate on the\nassumption that the entire test utterance is from only one talker with\na claimed identity (including i-vector) fail to meet this stringent\nrequirement. This paper presents a different signal modeling technique,\nwithin a conditional vector-quantization framework and with matching\nshort-time statistics that take into account the co-located speech\ncodes to meet the new challenge. As one variation, the temporally co-located\nVQ (TC-VQ) associates each codeword with a set of Gaussian mixture\nmodels to account for the co-located distributions and a temporally\nco-located hidden Markov model (TC-HMM) is built upon the TC-VQ. The\nproposed technique achieves an window-based equal error rate in the\nrange of 3&#8211;5% and a relative gain of 4&#8211;25% over a baseline\nsystem using traditional HMMs on the AVA database.\n"
   ],
   "doi": "10.21437/Interspeech.2016-650"
  },
  "fischer16_interspeech": {
   "authors": [
    [
     "Johannes",
     "Fischer"
    ],
    [
     "Tom",
     "Bäckström"
    ]
   ],
   "title": "Joint Enhancement and Coding of Speech by Incorporating Wiener Filtering in a CELP Codec",
   "original": "0245",
   "page_count": 5,
   "order": 365,
   "p1": "1730",
   "pn": "1734",
   "abstract": [
    "The performance of speech communication applications in the field of\nmobile devices is often hampered by background noises and distortions.\nTherefore, noise attenuation methods are commonly used as a pre-processing\nmethod, cascaded with the speech-codec. We demonstrate that the performance\nof such combinations of speech enhancement and coding methods can be\nimproved by joining the two methods into a single block. The proposed\nmethod is based on incorporating Wiener filtering into the objective\nfunction used for optimization of the quantization in code excited\nlinear prediction (CELP)-based codecs. The benefits are that 1) the\nnon-linear components of CELP codecs, including quantization and error\nfeedback, are taken into account in the joint minimization function\nthereby improving quality and 2) by merging blocks both delay and computational\ncomplexity can be minimized. Our experiments demonstrate that the proposed\njoint enhancement and coding approach consistently improves subjective\nand objective quality. The proposed method is compatible with any CELP-based\ncodecs without changing the bit-stream, whereby it can be readily applied\nin mobile phones or speech communication devices applying the concepts\nof CELP codecs for improving perceptual quality in adverse conditions.\n"
   ],
   "doi": "10.21437/Interspeech.2016-245"
  },
  "liu16i_interspeech": {
   "authors": [
    [
     "Hong",
     "Liu"
    ],
    [
     "Xiuling",
     "Wang"
    ],
    [
     "Miao",
     "Sun"
    ],
    [
     "Cheng",
     "Pang"
    ]
   ],
   "title": "Multi-Channel Linear Prediction Based on Binaural Coherence for Speech Dereverberation",
   "original": "0729",
   "page_count": 5,
   "order": 366,
   "p1": "1735",
   "pn": "1739",
   "abstract": [
    "It has been shown that the multi-channel linear prediction (MCLP) can\nachieve blind speech dereverberation effectively. However, it always\ndegrades the binaural cues which are exploited for human sound localization,\ni.e., interaural time differences (ITD) and interaural level differences\n(ILD). To overcome this problem, the multiple input-single output structure\nof conventional MCLP is modified to a binaural input-output structure\nfor suppressing reverberation and preserving binaural cues simultaneously.\nFirst, by employing a binaural coherence model with head shadowing\neffects, the variance of desired signal can be estimated the same to\nboth ears, which can ensure no modification of ILD. Then, the variance\nis utilized to calculate the prediction coefficients in a maximum-likelihood\n(ML) sense. Finally, the desired signals can be obtained as the prediction\nerrors in MCLP. And since the algorithm does not disturb the phase\nof input signal, the ITD cue is kept. Evaluations with measured binaural\nroom impulse responses (BRIRs) show that the proposed method yields\na good performance on both speech dereverberation and binaural cues\npreservation.\n"
   ],
   "doi": "10.21437/Interspeech.2016-729"
  },
  "blass16_interspeech": {
   "authors": [
    [
     "Martin",
     "Blass"
    ],
    [
     "Pejman",
     "Mowlaee"
    ],
    [
     "W. Bastiaan",
     "Kleijn"
    ]
   ],
   "title": "Single-Channel Speech Enhancement Using Double Spectrum",
   "original": "0234",
   "page_count": 5,
   "order": 367,
   "p1": "1740",
   "pn": "1744",
   "abstract": [
    "Single-channel speech enhancement is often formulated in the  Short-Time\nFourier Transform (STFT) domain. As an alternative, several previous\nstudies have reported advantages of speech processing using pitch-synchronous\nanalysis and filtering in the modulation transform domain. We propose\nto use the  Double Spectrum (DS) obtained by combining pitch-synchronous\ntransform followed by modulation transform. The linearity and sparseness\nproperties of DS domain are beneficial for single-channel speech enhancement.\nThe effectiveness of the proposed DS-based speech enhancement is demonstrated\nby comparing it with STFT-based and modulation-based benchmarks. In\ncontrast to the benchmark methods, the proposed method does not exploit\nany statistical information nor does it use temporal smoothing. The\nproposed method leads to an improvement of 0.3 PESQ on average for\nbabble noise.\n"
   ],
   "doi": "10.21437/Interspeech.2016-234"
  },
  "drude16_interspeech": {
   "authors": [
    [
     "Lukas",
     "Drude"
    ],
    [
     "Bhiksha",
     "Raj"
    ],
    [
     "Reinhold",
     "Haeb-Umbach"
    ]
   ],
   "title": "On the Appropriateness of Complex-Valued Neural Networks for Speech Enhancement",
   "original": "0300",
   "page_count": 5,
   "order": 368,
   "p1": "1745",
   "pn": "1749",
   "abstract": [
    "Although complex-valued neural networks (CVNNs) &#8212; networks which\ncan operate with complex arithmetic &#8212; have been around for a\nwhile, they have not been given reconsideration since the breakthrough\nof deep network architectures. This paper presents a critical assessment\nwhether the novel tool set of deep neural networks (DNNs) should be\nextended to complex-valued arithmetic. Indeed, with DNNs making inroads\nin speech enhancement tasks, the use of complex-valued input data,\nspecifically the short-time Fourier transform coefficients, is an obvious\nconsideration. In particular when it comes to performing tasks that\nheavily rely on phase information, such as acoustic beamforming, complex-valued\nalgorithms are omnipresent. In this contribution we recapitulate backpropagation\nin CVNNs, develop complex-valued network elements, such as the split-rectified\nnon-linearity, and compare real- and complex-valued networks on a beamforming\ntask. We find that CVNNs hardly provide a performance gain and conclude\nthat the effort of developing the complex-valued counterparts of the\nbuilding blocks of modern deep or recurrent neural networks can hardly\nbe justified.\n"
   ],
   "doi": "10.21437/Interspeech.2016-300"
  },
  "zeiler16_interspeech": {
   "authors": [
    [
     "Steffen",
     "Zeiler"
    ],
    [
     "Hendrik",
     "Meutzner"
    ],
    [
     "Ahmed Hussen",
     "Abdelaziz"
    ],
    [
     "Dorothea",
     "Kolossa"
    ]
   ],
   "title": "Introducing the Turbo-Twin-HMM for Audio-Visual Speech Enhancement",
   "original": "0350",
   "page_count": 5,
   "order": 369,
   "p1": "1750",
   "pn": "1754",
   "abstract": [
    "Models for automatic speech recognition (ASR) hold detailed information\nabout spectral and spectro-temporal characteristics of clean speech\nsignals. Using these models for speech enhancement is desirable and\nhas been the target of past research efforts. In such model-based speech\nenhancement systems, a powerful ASR is imperative. To increase the\nrecognition rates especially in low-SNR conditions, we suggest the\nuse of the additional visual modality, which is mostly unaffected by\ndegradations in the acoustic channel. An optimal integration of acoustic\nand visual information is achievable by joint inference in both modalities\nwithin the turbo-decoding framework. Thus combining turbo-decoding\nwith Twin-HMMs for speech enhancement, notable improvements can be\nachieved, not only in terms of instrumental estimates of speech quality,\nbut also in actual speech intelligibility. This is verified through\nlistening tests, which show that in highly challenging noise conditions,\naverage human recognition accuracy can be improved from 64% without\nsignal processing to 80% when using the presented architecture.\n"
   ],
   "doi": "10.21437/Interspeech.2016-350"
  },
  "spille16_interspeech": {
   "authors": [
    [
     "Constantin",
     "Spille"
    ],
    [
     "Hendrik",
     "Kayser"
    ],
    [
     "Hynek",
     "Hermansky"
    ],
    [
     "Bernd T.",
     "Meyer"
    ]
   ],
   "title": "Assessing Speech Quality in Speech-Aware Hearing Aids Based on Phoneme Posteriorgrams",
   "original": "1318",
   "page_count": 5,
   "order": 370,
   "p1": "1755",
   "pn": "1759",
   "abstract": [
    "Current behind-the-ear hearing aids (HA) allow to perform spatial filtering\nto enhance localized sound sources; however, they often lack processing\nstrategies that are tailored to spoken language. Hence, without a feedback\nabout speech quality achieved by the system, spatial filtering potentially\nremains unused, in case of a conservative enhancement strategy, or\ncan even be detrimental to the speech intelligibility of the output\nsignal. In this paper we apply phoneme posteriorgrams obtained from\nHA signals processed with deep neural networks to measure the quality\nof speech representations in spatial scenes. Inverse entropy of phoneme\nprobabilities is proposed as a measure that allows to evaluate if current\nhearing aid parameters are optimal for the given acoustic condition.\nWe investigate how varying noise levels and wrong estimates of the\nto-be-enhanced direction affect this measure in anechoic and reverberant\nconditions and show our measure to provide a high reliability when\nvarying each parameter. Experiments show that entropy as a function\nof the beam angle has a distinct minimum at the speaker&#8217;s true\nposition and its immediate vicinity. Thus, it can be used to determine\nthe beam angle which optimizes the speech representation. Further,\nvariations of the SNR cause a consistent offset of the entropy.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1318"
  },
  "gowda16_interspeech": {
   "authors": [
    [
     "Dhananjaya",
     "Gowda"
    ],
    [
     "Paavo",
     "Alku"
    ]
   ],
   "title": "Time-Varying Quasi-Closed-Phase Weighted Linear Prediction Analysis of Speech for Accurate Formant Detection and Tracking",
   "original": "0153",
   "page_count": 5,
   "order": 371,
   "p1": "1760",
   "pn": "1764",
   "abstract": [
    "In this paper, we propose a new method for accurate detection, estimation\nand tracking of formants in speech signals using time-varying quasi-closed\nphase analysis (TVQCP). The proposed method combines two different\nmethods of analysis namely, the time-varying linear prediction (TVLP)\nand quasi-closed phase (QCP) analysis. TVLP helps in better tracking\nof formant frequencies by imposing a time-continuity constraint on\nthe linear prediction (LP) coefficients. QCP analysis, a type of weighted\nLP (WLP), improves the estimation accuracies of the formant frequencies\nby using a carefully designed weight function on the error signal that\nis minimized. The QCP weight function emphasizes the closed-phase region\nof the glottal cycle, and also weights down the regions around the\nmain excitations. This results in reduced coupling of the subglottal\ncavity and the excitation source. Experimental results on natural speech\nsignals show that the proposed method performs considerably better\nthan the detect-and-track approach used in popular tools like Wavesurfer\nor Praat.\n"
   ],
   "doi": "10.21437/Interspeech.2016-153"
  },
  "lim16b_interspeech": {
   "authors": [
    [
     "Yongwan",
     "Lim"
    ],
    [
     "Sajan Goud",
     "Lingala"
    ],
    [
     "Asterios",
     "Toutios"
    ],
    [
     "Shrikanth S.",
     "Narayanan"
    ],
    [
     "Krishna S.",
     "Nayak"
    ]
   ],
   "title": "Improved Depiction of Tissue Boundaries in Vocal Tract Real-Time MRI Using Automatic Off-Resonance Correction",
   "original": "0664",
   "page_count": 5,
   "order": 372,
   "p1": "1765",
   "pn": "1769",
   "abstract": [
    "Real-time magnetic resonance imaging (RT-MRI) is a powerful tool to\nstudy the dynamics of vocal tract shaping during speech production.\nThe dynamic articulators of interest include the surfaces of the lips,\ntongue, hard palate, soft palate, and pharyngeal airway. All of these\nare located at air-tissue interfaces and are vulnerable to MRI off-resonance\neffect due to magnetic susceptibility. In RT-MRI using spiral or radial\nscanning, this appears as a signal loss or blurring in images and may\nimpair the analysis of dynamic speech data. We apply an automatic off-resonance\nartifact correction method to speech RT-MRI data in order to enhance\nthe sharpness of air-tissue boundaries. We demonstrate the improvement\nqualitatively and using an image sharpness metric offering an improved\ntool for speech science research.\n"
   ],
   "doi": "10.21437/Interspeech.2016-664"
  },
  "blaauw16_interspeech": {
   "authors": [
    [
     "Merlijn",
     "Blaauw"
    ],
    [
     "Jordi",
     "Bonada"
    ]
   ],
   "title": "Modeling and Transforming Speech Using Variational Autoencoders",
   "original": "1183",
   "page_count": 5,
   "order": 373,
   "p1": "1770",
   "pn": "1774",
   "abstract": [
    "Latent generative models can learn higher-level underlying factors\nfrom complex data in an unsupervised manner. Such models can be used\nin a wide range of speech processing applications, including synthesis,\ntransformation and classification. While there have been many advances\nin this field in recent years, the application of the resulting models\nto speech processing tasks is generally not explicitly considered.\nIn this paper we apply the variational autoencoder (VAE) to the task\nof modeling frame-wise spectral envelopes. The VAE model has many attractive\nproperties such as continuous latent variables, prior probability over\nthese latent variables, a tractable lower bound on the marginal log\nlikelihood, both generative and recognition models, and end-to-end\ntraining of deep models. We consider different aspects of training\nsuch models for speech data and compare them to more conventional models\nsuch as the Restricted Boltzmann Machine (RBM). While evaluating generative\nmodels is difficult, we try to obtain a balanced picture by considering\nboth performance in terms of reconstruction error and when applying\nthe model to a series of modeling and transformation tasks to get an\nidea of the quality of the learned features.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1183"
  },
  "seelamantula16_interspeech": {
   "authors": [
    [
     "Chandra Sekhar",
     "Seelamantula"
    ]
   ],
   "title": "Phase-Encoded Speech Spectrograms",
   "original": "1600",
   "page_count": 5,
   "order": 374,
   "p1": "1775",
   "pn": "1779",
   "abstract": [
    "Spectrograms of speech and audio signals are time-frequency densities,\nand by construction, they are non-negative and do not have phase associated\nwith them. Under certain conditions on the amount of overlap between\nconsecutive frames and frequency sampling, it is possible to reconstruct\nthe signal from the spectrogram. Deviating from this requirement, we\ndevelop a new technique to incorporate the phase of the signal in the\nspectrogram by satisfying what we call as the  delta dominance condition,\nwhich in general is different from the well known minimum-phase condition.\nIn fact, there are signals that are delta dominant but not minimum-phase\nand vice versa. The delta dominance condition can be satisfied in multiple\nways, for example by placing a Kronecker impulse of the right amplitude\nor by choosing a suitable window function. A direct consequence of\nthis novel way of constructing the spectrograms is that the phase of\nthe signal is directly encoded or embedded in the spectrogram. We also\ndevelop a reconstruction methodology that takes such phase-encoded\nspectrograms and obtains the signal using the discrete Fourier transform\n(DFT). It is envisaged that the new class of phase-encoded spectrogram\nrepresentations would find applications in various speech processing\ntasks such as analysis, synthesis, enhancement, and recognition.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1600"
  },
  "birkholz16_interspeech": {
   "authors": [
    [
     "Peter",
     "Birkholz"
    ],
    [
     "Petko",
     "Bakardjiev"
    ],
    [
     "Steffen",
     "Kürbis"
    ],
    [
     "Rico",
     "Petrick"
    ]
   ],
   "title": "Towards Minimally Invasive Velar State Detection in Normal and Silent Speech",
   "original": "0771",
   "page_count": 5,
   "order": 375,
   "p1": "1780",
   "pn": "1784",
   "abstract": [
    "We present a portable minimally invasive system to determine the state\nof the velum (raised or lowered) at a sampling rate of 40 Hz that works\nboth during normal and silent speech. The system consists of a small\ncapsule containing a miniature loudspeaker and a miniature microphone.\nThe capsule is inserted into one nostril by about 10 mm. The loudspeaker\nemits chirps with a power band from 12&#8211;24 kHz into the nostril\nand the microphone records the signal reflected from the nasal cavity.\nThe chirp response differs between raised and lowered velar positions,\nbecause the velar position determines the shape of the nasal cavity\nin the posterior part and hence its acoustic behaviour. Reference chirp\nresponses for raised and lowered velar positions in combination with\na spectral distance measure are used to infer the state of the velum.\nHere we discuss critical design aspects of the system and outline future\nimprovements. Possible applications of the device include the detection\nof the velar state during silent speech recognition, medical assessment\nof velar mobility and speech production research.\n"
   ],
   "doi": "10.21437/Interspeech.2016-771"
  },
  "zhang16d_interspeech": {
   "authors": [
    [
     "Jianshu",
     "Zhang"
    ],
    [
     "Jian",
     "Tang"
    ],
    [
     "Li-Rong",
     "Dai"
    ]
   ],
   "title": "RNN-BLSTM Based Multi-Pitch Estimation",
   "original": "0117",
   "page_count": 5,
   "order": 376,
   "p1": "1785",
   "pn": "1789",
   "abstract": [
    "Multi-pitch estimation is critical in many applications, including\ncomputational auditory scene analysis (CASA), speech enhancement/separation\nand mixed speech analysis; however, despite much effort, it remains\na challenging problem. This paper uses the PEFAC algorithm to extract\nfeatures and proposes the use of recurrent neural networks with bidirectional\nLong Short-Term Memory (RNN-BLSTM) to model the two pitch contours\nof a mixture of two speech signals. Compared with feed-forward deep\nneural networks (DNN), which are trained on static frame-level acoustic\nfeatures, RNN-BLSTM is trained on sequential frame-level features and\nhas more power to learn pitch contour temporal dynamics. The results\nof evaluations using a speech dataset containing mixtures of two speech\nsignals demonstrate that RNN-BLSTM can substantially outperform DNN\nin multi-pitch estimation of mixed speech signals.\n"
   ],
   "doi": "10.21437/Interspeech.2016-117"
  },
  "morise16_interspeech": {
   "authors": [
    [
     "Masanori",
     "Morise"
    ],
    [
     "Hideki",
     "Kawahara"
    ]
   ],
   "title": "TUSK: A Framework for Overviewing the Performance of F0 Estimators",
   "original": "0140",
   "page_count": 5,
   "order": 377,
   "p1": "1790",
   "pn": "1794",
   "abstract": [
    "This article presents a framework for overviewing the performance of\nfundamental frequency (F0) estimators and evaluates its effectiveness.\nOver the past few decades, many F0 estimators and evaluation indices\nhave been proposed and have been evaluated using various speech databases.\nIn speech analysis/ synthesis research, modern estimators are used\nas the algorithm to fulfill the demand for high-quality speech synthesis,\nbut at the same time, they are competing with one another on minor\nissues. Specifically, while all of them meet the demands for high-quality\nspeech synthesis, the result depends on the speech database used in\nthe evaluation. Since there are various types of speech, it is inadvisable\nto discuss the effectiveness of each estimator on the basis of minor\ndifferences. It would be better to select the appropriate F0 estimator\nin accordance with the speech characteristics. The framework we propose,\nTUSK, does not rank the estimators but rather attempts to overview\nthem. In TUSK, six parameters are introduced to observe the trends\nin the characteristics in each F0 estimator. The signal is artificially\ngenerated so that six parameters can be controllable independently.\nIn this article, we introduce the concept of TUSK and determine its\neffectiveness using several modern F0 estimators.\n"
   ],
   "doi": "10.21437/Interspeech.2016-140"
  },
  "rengaswamy16_interspeech": {
   "authors": [
    [
     "Pradeep",
     "Rengaswamy"
    ],
    [
     "Gurunath Reddy",
     "M."
    ],
    [
     "K. Sreenivasa",
     "Rao"
    ],
    [
     "Pallab",
     "Dasgupta"
    ]
   ],
   "title": "A Robust Non-Parametric and Filtering Based Approach for Glottal Closure Instant Detection",
   "original": "0369",
   "page_count": 5,
   "order": 378,
   "p1": "1795",
   "pn": "1799",
   "abstract": [
    "In this paper, a novel non-parametric based glottal closure instant\n(GCI) detection method after filtering the speech signal through a\npulse shaping filter is proposed. The pulse shaping filter essentially\nde-emphasises the vocal tract resonances by emphasising the frequency\ncomponents containing the pitch information. The filtered signal is\nsubjected to non-linear processing to emphasise the GCI locations.\nThe GCI locations are finally obtained by a non-parametric histograms\nbased approach in the detected voiced regions from the filtered speech\nsignal. The proposed method is compared with the two state-of-the-art\nepoch extraction methods : Zero frequency filtering (ZFF) and SEDREAMS\n(both of which requires upfront knowledge of average pitch period).\nThe performance of the method is evaluated on the complete CMU-ARCTIC\ndataset consisting of both speech and Electroglottograph (EGG) signals.\nThe robustness of the proposed method to the additive white noise is\nevaluated with several degradation levels. The experimental results\nshowed that the proposed method is indeed immune to noise and the obtained\nresults are comparably better than the two state-of-the-art methods.\n"
   ],
   "doi": "10.21437/Interspeech.2016-369"
  },
  "saeidi16_interspeech": {
   "authors": [
    [
     "Rahim",
     "Saeidi"
    ],
    [
     "Ilkka",
     "Huhtakallio"
    ],
    [
     "Paavo",
     "Alku"
    ]
   ],
   "title": "Analysis of Face Mask Effect on Speaker Recognition",
   "original": "0518",
   "page_count": 5,
   "order": 379,
   "p1": "1800",
   "pn": "1804",
   "abstract": [
    "Wearing a face mask affects the speech production. On top of that,\nthe frequency response and radiation characteristics of the face mask\n&#8212; depending on the material and shape of the mask &#8212; adds\nto the complexity of analyzing speech under face mask. Our target is\nto separate the effect of muscle constriction and increased vocal effort\nin speech produced under face mask from sound transmission and radiation\nproperties of face mask. In this paper, we measure up the far-field\neffects of wearing four different face masks; motorcycle helmet, rubber\nmask, surgical mask and scarf inside anechoic chamber. The measurement\nsetup follows the recording configuration of a speech corpus used for\nspeaker recognition experiments. In matching speech under face mask\nwith speech under no mask, the frequency response of the respective\nface mask is accounted for and compensated for before acoustic feature\nextraction. The speaker recognition performance is reported using the\nstate-of-the-art i-vector method for mismatched and compensated conditions\nin order to demonstrate the significance of knowing the type of mask\nand accounting for its sound transmission properties.\n"
   ],
   "doi": "10.21437/Interspeech.2016-518"
  },
  "singer16_interspeech": {
   "authors": [
    [
     "Elliot",
     "Singer"
    ],
    [
     "Tyler",
     "Campbell"
    ],
    [
     "Douglas",
     "Reynolds"
    ]
   ],
   "title": "Data Selection for Within-Class Covariance Estimation",
   "original": "1282",
   "page_count": 5,
   "order": 380,
   "p1": "1805",
   "pn": "1809",
   "abstract": [
    "Methods for performing channel and session compensation in conjunction\nwith subspace techniques have been a focus of considerable study recently\nand have led to significant gains in speaker recognition performance.\nWhile developers have typically exploited the vast archive of speaker\nlabeled data available from earlier NIST evaluations to train the within-class\nand across-class covariance matrices required by these techniques,\nlittle attention has been paid to the characteristics of the data required\nto perform the training efficiently. This paper focuses on within-class\ncovariance normalization (WCCN) and shows that a reduction in training\ndata requirements can be achieved by proper data selection. In particular,\nit is shown that the key variables are the total amount of data and\nthe degree of handset variability, with total calls per handset playing\na smaller role. The study offers insight into efficient WCCN training\ndata collection in real world applications.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1282"
  },
  "ferras16_interspeech": {
   "authors": [
    [
     "M.",
     "Ferras"
    ],
    [
     "Srikanth",
     "Madikeri"
    ],
    [
     "S.",
     "Dey"
    ],
    [
     "Petr",
     "Motlicek"
    ],
    [
     "Hervé",
     "Bourlard"
    ]
   ],
   "title": "Inter-Task System Fusion for Speaker Recognition",
   "original": "1179",
   "page_count": 5,
   "order": 381,
   "p1": "1810",
   "pn": "1814",
   "abstract": [
    "Fusion is a common approach to improving the performance of speaker\nrecognition systems. Multiple systems using different data, features\nor algorithms tend to bring complementary contributions to the final\ndecisions being made. It is known that factors such as native language\nor accent contribute to speaker identity. In this paper, we explore\ninter-task fusion approaches to incorporating side information from\naccent and language identification systems to improve the performance\nof a speaker verification system. We explore both score level and model\nlevel approaches, linear logistic regression and linear discriminant\nanalysis respectively, reporting significant gains on accented and\nmulti-lingual data sets of the NIST Speaker Recognition Evaluation\n2008 data. Equal error rate and expected rank metrics are reported\nfor speaker verification and speaker identification tasks.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1179"
  },
  "lei16_interspeech": {
   "authors": [
    [
     "Zhenchun",
     "Lei"
    ],
    [
     "Yanhong",
     "Wan"
    ],
    [
     "Jian",
     "Luo"
    ],
    [
     "Yingen",
     "Yang"
    ]
   ],
   "title": "Mahalanobis Metric Scoring Learned from Weighted Pairwise Constraints in I-Vector Speaker Recognition System",
   "original": "1071",
   "page_count": 5,
   "order": 382,
   "p1": "1815",
   "pn": "1819",
   "abstract": [
    "The i-vector model is widely used by the state-of-the-art speaker recognition\nsystem. We proposed a new Mahalanobis metric scoring learned from weighted\npairwise constraints (WPCML), which use the different weights for the\nempirical error of the similar and dissimilar pairs. In the new i-vector\nspace described by the metric, the distance between the same speaker&#8217;s\ni-vectors is small, while that of the different speakers&#8217; is\nlarge. In forming the training set, we use the traditional way in random\nfashion and develop a new nearest distance based way. The results on\nthe NIST 2008 telephone data shown that our model can get better performance\nthan the classical cosine similarity scoring. When using the nearest\ndistance based way to form the training set, our model is better than\nthe state-of-the-art PLDA. And the results on the NIST 2014 i-vector\nchallenge show that our model is also better than the PLDA.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1071"
  },
  "soni16_interspeech": {
   "authors": [
    [
     "Meet H.",
     "Soni"
    ],
    [
     "Tanvina B.",
     "Patel"
    ],
    [
     "Hemant A.",
     "Patil"
    ]
   ],
   "title": "Novel Subband Autoencoder Features for Detection of Spoofed Speech",
   "original": "0668",
   "page_count": 5,
   "order": 383,
   "p1": "1820",
   "pn": "1824",
   "abstract": [
    "Deep Neural Network (DNN) have been extensively used in Automatic Speech\nRecognition (ASR) applications. Very recently, DNNs have also found\napplication in detecting natural vs. spoofed speech at ASV spoof challenge\nheld at INTERSPEECH 2015. Along the similar lines, in this work, we\npropose a new feature extraction architecture of DNN called the subband\nautoencoder (SBAE) for spoof detection task. The SBAE is inspired by\nthe human auditory system and extracts features from the speech spectrum\nin an unsupervised manner. The features derived from SBAE are compared\nwith state-of-the-art Mel Frequency Cepstral Coefficient (MFCC) features.\nThe experiments were performed on ASV spoof challenge database and\nthe performance was evaluated using Equal Error Rate (EER). It was\nobserved that on the evaluation set, MFCC features with 36-dimensional\n(static+&#916;+&#916;&#916;) features gave 4.32% EER which reduced\nto 2.9% when 36-dimensional SBAE features were used. Further on fusing\nSBAE features at score-level with MFCC, a further reduction till 1.93%\nEER was observed. This improvement in EER was due to the fact that\nthe dynamics of SBAE features captured significant spoof specific characteristics\nleading to detect significantly even vocoder-independent speech, which\nis not the case for MFCC.\n"
   ],
   "doi": "10.21437/Interspeech.2016-668"
  },
  "mclaren16c_interspeech": {
   "authors": [
    [
     "Mitchell",
     "McLaren"
    ],
    [
     "Diego",
     "Castan"
    ],
    [
     "Luciana",
     "Ferrer"
    ],
    [
     "Aaron",
     "Lawson"
    ]
   ],
   "title": "On the Issue of Calibration in DNN-Based Speaker Recognition Systems",
   "original": "1134",
   "page_count": 5,
   "order": 384,
   "p1": "1825",
   "pn": "1829",
   "abstract": [
    "This article is concerned with the issue of calibration in the context\nof Deep Neural Network (DNN) based approaches to speaker recognition.\nDNNs have provided a new standard in technology when used in place\nof the traditional universal background model (UBM) for feature alignment,\nor to augment traditional features with those extracted from a bottleneck\nlayer of the DNN. These techniques provide extremely good performance\nfor constrained trial conditions that are well matched to development\nconditions. However, when applied to unseen conditions or a wide variety\nof conditions, some DNN-based techniques offer poor calibration performance.\nThrough analysis on both PRISM and the recently released Speakers in\nthe Wild (SITW) corpora, we illustrate that bottleneck features hinder\ncalibration if used in the calculation of first-order Baum Welch statistics\nduring i-vector extraction. We propose a hybrid alignment framework,\nwhich stems from our previous work in DNN senone alignment, that uses\nthe bottleneck features only for the alignment of features during statistics\ncalculation. This framework not only addresses the issue of calibration,\nbut provides a more computationally efficient system based on bottleneck\nfeatures with improved discriminative power.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1134"
  },
  "kheder16b_interspeech": {
   "authors": [
    [
     "Waad Ben",
     "Kheder"
    ],
    [
     "Driss",
     "Matrouf"
    ],
    [
     "Moez",
     "Ajili"
    ],
    [
     "Jean-François",
     "Bonastre"
    ]
   ],
   "title": "Probabilistic Approach Using Joint Long and Short Session i-Vectors Modeling to Deal with Short Utterances for Speaker Recognition",
   "original": "1302",
   "page_count": 5,
   "order": 385,
   "p1": "1830",
   "pn": "1834",
   "abstract": [
    "Speaker recognition with short utterance is highly challenging. The\nuse of i-vectors in SR systems became a standard in the last years\nand many algorithms were developed to deal with the short utterances\nproblem. We present in this paper a new technique based on modeling\njointly the i-vectors corresponding to short utterances and those of\nlong utterances. The joint distribution is estimated using a large\nnumber of i-vectors pairs (coming from short and long utterances) corresponding\nto the same session. The obtained distribution is then integrated in\nan MMSE estimator in the test phase to compute an &#8220;improved&#8221;\nversion of short utterance i-vectors. We show that this technique can\nbe used to deal with duration mismatch and that it achieves up to 40%\nof relative improvement in EER(%) when used on NIST data. We also apply\nthis technique on the recently published SITW database and show that\nit yields 25% of EER(%) improvement compared to a regular PLDA scoring.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1302"
  },
  "kanagasundaram16_interspeech": {
   "authors": [
    [
     "Ahilan",
     "Kanagasundaram"
    ],
    [
     "David",
     "Dean"
    ],
    [
     "Sridha",
     "Sridharan"
    ],
    [
     "Clinton",
     "Fookes"
    ],
    [
     "Ivan",
     "Himawan"
    ]
   ],
   "title": "Short Utterance Variance Modelling and Utterance Partitioning for PLDA Speaker Verification",
   "original": "0778",
   "page_count": 4,
   "order": 386,
   "p1": "1835",
   "pn": "1838",
   "abstract": [
    "This paper analyses the short utterance probabilistic linear discriminant\nanalysis (PLDA) speaker verification with utterance partitioning and\nshort utterance variance (SUV) modelling approaches. Experimental studies\nhave found that instead of using single long-utterance as enrolment\ndata, if long enrolled-utterance is partitioned into multiple short\nutterances and average of short utterance i-vectors is used as enrolled\ndata, that improves the Gaussian PLDA (GPLDA) speaker verification.\nThis is because short utterance i-vectors have speaker, session and\nutterance variations, and utterance-partitioning approach compensates\nthe utterance variation. Subsequently, SUV-PLDA is also studied with\nutterance partitioning approach, and utterance-partitioning-based SUV-GPLDA\nsystem shows relative improvement of 9% and 16% in EER for NIST 2008\nand NIST 2010 truncated 10sec-10sec evaluation condition as utterance-partitioning\napproach compensates the utterance variation and SUV modelling approach\ncompensates the mismatch between full-length development data and short-length\nevaluation data.\n"
   ],
   "doi": "10.21437/Interspeech.2016-778"
  },
  "thomsen16_interspeech": {
   "authors": [
    [
     "Nicolai Bæk",
     "Thomsen"
    ],
    [
     "Dennis Alexander Lehmann",
     "Thomsen"
    ],
    [
     "Zheng-Hua",
     "Tan"
    ],
    [
     "Børge",
     "Lindberg"
    ],
    [
     "Søren Holdt",
     "Jensen"
    ]
   ],
   "title": "Speaker-Dependent Dictionary-Based Speech Enhancement for Text-Dependent Speaker Verification",
   "original": "0763",
   "page_count": 5,
   "order": 387,
   "p1": "1839",
   "pn": "1843",
   "abstract": [
    "The problem of text-dependent speaker verification under noisy conditions\nis becoming ever more relevant, due to increased usage for authentication\nin real-world applications. Classical methods for noise reduction such\nas spectral subtraction and Wiener filtering introduce distortion and\ndo not perform well in this setting. In this work we compare the performance\nof different noise reduction methods under different noise conditions\nin terms of speaker verification when the text is known and the system\nis trained on clean data (mis-matched conditions). We furthermore propose\na new approach based on dictionary-based noise reduction and compare\nit to the baseline methods.\n"
   ],
   "doi": "10.21437/Interspeech.2016-763"
  },
  "yu16c_interspeech": {
   "authors": [
    [
     "Chengzhu",
     "Yu"
    ],
    [
     "Chunlei",
     "Zhang"
    ],
    [
     "Finnian",
     "Kelly"
    ],
    [
     "Abhijeet",
     "Sangwan"
    ],
    [
     "John H.L.",
     "Hansen"
    ]
   ],
   "title": "Text-Available Speaker Recognition System for Forensic Applications",
   "original": "1520",
   "page_count": 4,
   "order": 388,
   "p1": "1844",
   "pn": "1847",
   "abstract": [
    "This paper examines a text-available speaker recognition approach targeting\nscenarios where the transcripts of test utterances are either available\nor obtainable through manual transcription. Forensic speaker recognition\nis one of such applications where the human supervision can be expected.\nIn our study, we extend an existing Deep Neural Network (DNN) i-vector-based\nspeaker recognition system to effectively incorporate text information\nassociated with test utterances. We first show experimentally that\nspeaker recognition performance drops significantly if the DNN output\nposteriors are directly replaced with their target  senone, obtained\nfrom force alignment. The cause of such performance drops can be attributed\nto the fact that forced alignment selects only the single most probable\n senone as their output, which is not desirable in a current speaker\nrecognition framework. To resolve this problem, we propose a posterior\nmapping approach where the relationship between forced aligned  senones\nand its corresponding DNN posteriors are modeled. By replacing DNN\noutput posteriors with  senone mapped posteriors, a robust text-available\nspeaker recognition system can be obtained in mismatched environments.\nExperiments using the proposed approach are performed on the Aurora-4\ndataset.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1520"
  },
  "hong16_interspeech": {
   "authors": [
    [
     "Qingyang",
     "Hong"
    ],
    [
     "Lin",
     "Li"
    ],
    [
     "Lihong",
     "Wan"
    ],
    [
     "Jun",
     "Zhang"
    ],
    [
     "Feng",
     "Tong"
    ]
   ],
   "title": "Transfer Learning for Speaker Verification on Short Utterances",
   "original": "0432",
   "page_count": 5,
   "order": 389,
   "p1": "1848",
   "pn": "1852",
   "abstract": [
    "Short utterance lacks enough discriminative information and its duration\nvariation will propagate uncertainty into a probability linear discriminant\nanalysis (PLDA) classifier. For speaker verification on short utterances,\nit can be considered as a domain with limited amount of long utterances.\nTherefore, transfer learning of PLDA can be adopted to learn discriminative\ninformation from other domain with a large amount of long utterances.\nIn this paper, we explore the effectiveness of transfer learning based\nPLDA (TL-PLDA) on the NIST SRE and Switchboard (SWB) corpus. Experimental\nresults showed that it could produce the largest gain of performance\ncompared with the traditional PLDA, especially for short utterances\nwith the duration of 5s and 10s.\n"
   ],
   "doi": "10.21437/Interspeech.2016-432"
  },
  "ma16b_interspeech": {
   "authors": [
    [
     "Jianbo",
     "Ma"
    ],
    [
     "Vidhyasaharan",
     "Sethu"
    ],
    [
     "Eliathamby",
     "Ambikairajah"
    ],
    [
     "Kong Aik",
     "Lee"
    ]
   ],
   "title": "Twin Model G-PLDA for Duration Mismatch Compensation in Text-Independent Speaker Verification",
   "original": "0683",
   "page_count": 5,
   "order": 390,
   "p1": "1853",
   "pn": "1857",
   "abstract": [
    "Short duration speaker verification is a challenging problem partly\ndue to utterance duration mismatch. This paper proposes a novel method\nthat modifies the standard Gaussian probabilistic linear discriminant\nanalysis (G-PLDA) to use two separate generative models for i-vectors\nfrom long and short utterances which are jointly trained. The proposed\ntwin model G-PLDA employs distinct models for i-vectors corresponding\nto different durations from the same speaker but shares the same latent\nvariables. Unlike the standard G-PLDA, this twin model G-PLDA takes\nthe differences between utterances of varying durations into account.\nHyper-parameter estimation and scoring formulae for the twin model\nG-PLDA are presented. Experimental results obtained using NIST 2010\ndata show that the proposed technique leads to relative improvements\nof 8.5% and 15.6% when tested on utterances of 5 second and 3 second\ndurations respectively.\n"
   ],
   "doi": "10.21437/Interspeech.2016-683"
  },
  "zhang16e_interspeech": {
   "authors": [
    [
     "Xiao-Lei",
     "Zhang"
    ]
   ],
   "title": "Universal Background Sparse Coding and Multilayer Bootstrap Network for Speaker Clustering",
   "original": "0065",
   "page_count": 5,
   "order": 391,
   "p1": "1858",
   "pn": "1862",
   "abstract": [
    "We apply multilayer bootstrap network (MBN) to speaker clustering.\nThe proposed method first extracts supervectors by a universal background\nmodel, then reduces the dimension of the high-dimensional supervectors\nby MBN, and finally conducts speaker clustering by clustering the low-dimensional\ndata. We also propose an MBN-based universal background model, named\nuniversal background sparse coding. The comparison results demonstrate\nthe effectiveness and robustness of the proposed method.\n"
   ],
   "doi": "10.21437/Interspeech.2016-65"
  },
  "tian16b_interspeech": {
   "authors": [
    [
     "Yao",
     "Tian"
    ],
    [
     "Meng",
     "Cai"
    ],
    [
     "Liang",
     "He"
    ],
    [
     "Wei-Qiang",
     "Zhang"
    ],
    [
     "Jia",
     "Liu"
    ]
   ],
   "title": "Improving Deep Neural Networks Based Speaker Verification Using Unlabeled Data",
   "original": "0614",
   "page_count": 5,
   "order": 392,
   "p1": "1863",
   "pn": "1867",
   "abstract": [
    "Recently, deep neural networks (DNNs) trained to predict senones have\nbeen incorporated into the conventional i-vector based speaker verification\nsystems to provide soft frame alignments and show promising results.\nHowever, the data mismatch problem may degrade the performance since\nthe DNN requires transcribed data (out-domain data) while the data\nsets (in-domain data) used for i-vector training and extraction are\nmostly untranscribed. In this paper, we try to address this problem\nby exploiting the unlabeled in-domain data during the training of the\nDNN, hoping the DNN can provide a more robust basis for the in-domain\ndata. In this work, we first explore the impact of using in-domain\ndata during the unsupervised DNN pre-training process. In addition,\nwe decode the in-domain data using a hybrid DNN-HMM system to get its\ntranscription, and then we retrain the DNN model with the &#8220;labeled&#8221;\nin-domain data. Experimental results on the NIST SRE 2008 and the NIST\nSRE 2010 databases demonstrate the effectiveness of the proposed methods.\n"
   ],
   "doi": "10.21437/Interspeech.2016-614"
  },
  "kanda16b_interspeech": {
   "authors": [
    [
     "Naoyuki",
     "Kanda"
    ],
    [
     "Xugang",
     "Lu"
    ],
    [
     "Hisashi",
     "Kawai"
    ]
   ],
   "title": "Maximum a posteriori Based Decoding for CTC Acoustic Models",
   "original": "0071",
   "page_count": 5,
   "order": 393,
   "p1": "1868",
   "pn": "1872",
   "abstract": [
    "This paper presents a novel decoding framework for connectionist temporal\nclassification (CTC)-based acoustic models (AM). Although CTC-based\nAM inherently has the property of a language model (LM) in itself,\nan external LM trained with a large text corpus is still essential\nto obtain the best results. In the previous literatures, a naive interpolation\nof the CTC-based AM score and the external LM score was used, although\nthere is no theoretical justification for it. In this paper, we propose\na theoretically more sound decoding framework derived from a maximization\nof the posterior probability of a word sequence given an observation.\nIn our decoding framework, a subword LM (SLM) is newly introduced to\ncoordinate the CTC-based AM score and the word-level LM score. In experiments\nwith the Wall Street Journal (WSJ) corpus and Corpus of Spontaneous\nJapanese (CSJ), our proposed framework consistently achieved improvements\nof 7.4&#8211;15.3% over the conventional interpolation-based framework.\nIn the CSJ experiment, given 586 hours of training data, the CTC-based\nAM finally achieved a 6.7% better word error rate than the baseline\nmethod with deep neural networks and hidden Markov models.\n"
   ],
   "doi": "10.21437/Interspeech.2016-71"
  },
  "asaei16_interspeech": {
   "authors": [
    [
     "Afsaneh",
     "Asaei"
    ],
    [
     "Gil",
     "Luyet"
    ],
    [
     "Milos",
     "Cernak"
    ],
    [
     "Hervé",
     "Bourlard"
    ]
   ],
   "title": "Phonetic and Phonological Posterior Search Space Hashing Exploiting Class-Specific Sparsity Structures",
   "original": "0938",
   "page_count": 5,
   "order": 394,
   "p1": "1873",
   "pn": "1877",
   "abstract": [
    "This paper shows that exemplar-based speech processing using class-conditional\nposterior probabilities admits a highly effective search strategy relying\non posteriors&#8217; intrinsic sparsity structures. The posterior probabilities\nare estimated for phonetic and phonological classes using deep neural\nnetwork (DNN) computational framework. Exploiting the class-specific\nsparsity leads to a simple quantized posterior hashing procedure to\nreduce the search space of posterior exemplars. To that end, small\nnumber of quantized posteriors are regarded as representatives of the\nposterior space and used as hash keys to index subsets of neighboring\nexemplars. The k nearest neighbor (kNN) method is applied for posterior\nbased classification problems. The phonetic posterior probabilities\nare used as exemplars for phonetic classification whereas the phonological\nposteriors are used as exemplars for automatic prosodic event detection.\nExperimental results demonstrate that posterior hashing improves the\nefficiency of kNN classification drastically. This work encourages\nthe use of posteriors as discriminative exemplars appropriate for large\nscale speech classification tasks.\n"
   ],
   "doi": "10.21437/Interspeech.2016-938"
  },
  "tucker16_interspeech": {
   "authors": [
    [
     "George",
     "Tucker"
    ],
    [
     "Minhua",
     "Wu"
    ],
    [
     "Ming",
     "Sun"
    ],
    [
     "Sankaran",
     "Panchapagesan"
    ],
    [
     "Gengshen",
     "Fu"
    ],
    [
     "Shiv",
     "Vitaladevuni"
    ]
   ],
   "title": "Model Compression Applied to Small-Footprint Keyword Spotting",
   "original": "1393",
   "page_count": 5,
   "order": 395,
   "p1": "1878",
   "pn": "1882",
   "abstract": [
    "Several consumer speech devices feature voice interfaces that perform\non-device keyword spotting to initiate user interactions. Accurate\non-device keyword spotting within a tight CPU budget is crucial for\nsuch devices. Motivated by this, we investigated two ways to improve\ndeep neural network (DNN) acoustic models for keyword spotting without\nincreasing CPU usage. First, we used low-rank weight matrices throughout\nthe DNN. This allowed us to increase representational power by increasing\nthe number of hidden nodes per layer without changing the total number\nof multiplications. Second, we used knowledge distilled from an ensemble\nof much larger DNNs used only during training. We systematically evaluated\nthese two approaches on a massive corpus of far-field utterances. Alone\nboth techniques improve performance and together they combine to give\nsignificant reductions in false alarms and misses without increasing\nCPU or memory usage.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1393"
  },
  "martinez16_interspeech": {
   "authors": [
    [
     "Angel Mario Castro",
     "Martinez"
    ],
    [
     "Marc René",
     "Schädler"
    ]
   ],
   "title": "Why do ASR Systems Despite Neural Nets Still Depend on Robust Features",
   "original": "1552",
   "page_count": 5,
   "order": 396,
   "p1": "1883",
   "pn": "1887",
   "abstract": [
    "To which extent can neural nets learn traditional signal processing\nstages of current robust ASR front-ends? Will neural nets replace the\nclassical, often auditory-inspired feature extraction in the near future?\nTo answer these questions, a DNN-based ASR system was trained and tested\non the Aurora4 robust ASR task using various (intermediate) processing\nstages. Additionally, the training set was divided into several fractions\nto reveal the amount of data needed to account for a missing processing\nstep on the input signal or prior knowledge about the auditory system.\nThe DNN system was able to learn from ordinary spectrograms representations\noutperforming MFCC using 75% of the training set and almost as good\nas log-Mel-spectrograms with the full set; on the other hand, it was\nunable to compensate the robustness of auditory-based Gabor features,\nwhich even using 40% of the training data outperformed every other\nrepresentation. The study concludes that even with deep learning approaches,\ncurrent ASR systems still benefit from a suitable feature extraction.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1552"
  },
  "he16b_interspeech": {
   "authors": [
    [
     "Qing",
     "He"
    ],
    [
     "Gregory W.",
     "Wornell"
    ],
    [
     "Wei",
     "Ma"
    ]
   ],
   "title": "An Adaptive Multi-Band System for Low Power Voice Command Recognition",
   "original": "1562",
   "page_count": 5,
   "order": 397,
   "p1": "1888",
   "pn": "1892",
   "abstract": [
    "A complete voice-driven experience in applications such as wearable\nelectronics requires always-on keyword monitoring, which is prohibitively\npower consuming using current speech recognition methods. In this work,\nwe propose an ultra-low power voice command recognition system that\nis designed to recognize short commands such as &#8216;Hi Galaxy&#8217;.\nTo achieve power-efficient designs, the system uses adaptive feature\npre-selection such that only a subset of all available features are\nselected and extracted based on the noise spectrum. The back-end classifier,\nsupporting adaptive feature selection, is enabled by a novel multi-band\ndeep neural networks (DNNs) model that processes only the selected\nfeatures at each decision. In experiments, our adaptive scheme achieves\ncomparable accuracy and improved efficiency using an average of 5 spectral\nfeature bands, than a generic fully-connected DNNs model using the\nfull speech spectrum. The system makes a recognition decision every\n40ms on 1.2s of buffered speech and consumes &#126;230&#181;W of power,\nthus promising low-power, low-complexity and robust application-specific\nvoice recognition.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1562"
  },
  "price16_interspeech": {
   "authors": [
    [
     "Michael",
     "Price"
    ],
    [
     "Anantha",
     "Chandrakasan"
    ],
    [
     "James",
     "Glass"
    ]
   ],
   "title": "Memory-Efficient Modeling and Search Techniques for Hardware ASR Decoders",
   "original": "0287",
   "page_count": 5,
   "order": 398,
   "p1": "1893",
   "pn": "1897",
   "abstract": [
    "This paper gives an overview of acoustic modeling and search techniques\nfor low-power embedded ASR decoders. Our design decisions prioritize\nmemory bandwidth, which is the main driver in system power consumption.\nWe evaluate three acoustic modeling approaches &#8212; Gaussian mixture\nmodel (GMM), subspace GMM (SGMM) and deep neural network (DNN) &#8212;\nand identify tradeoffs between memory bandwidth and recognition accuracy.\nWe also present an HMM search scheme with WFST compression and caching,\npredictive beam width control, and a word lattice. Our results apply\nto embedded system implementations using microcontrollers, DSPs, FPGAs,\nor ASICs.\n"
   ],
   "doi": "10.21437/Interspeech.2016-287"
  },
  "yang16c_interspeech": {
   "authors": [
    [
     "J.",
     "Yang"
    ],
    [
     "Anton",
     "Ragni"
    ],
    [
     "Mark J.F.",
     "Gales"
    ],
    [
     "Kate M.",
     "Knill"
    ]
   ],
   "title": "Log-Linear System Combination Using Structured Support Vector Machines",
   "original": "0377",
   "page_count": 5,
   "order": 399,
   "p1": "1898",
   "pn": "1902",
   "abstract": [
    "Building high accuracy speech recognition systems with limited language\nresources is a highly challenging task. Although the use of multi-language\ndata for acoustic models yields improvements, performance is often\nunsatisfactory with highly limited acoustic training data. In these\nsituations, it is possible to consider using multiple well trained\nacoustic models and combine the system outputs together. Unfortunately,\nthe computational cost associated with these approaches is high as\nmultiple decoding runs are required. To address this problem, this\npaper examines schemes based on log-linear score combination. This\nhas a number of advantages over standard combination schemes. Even\nwith limited acoustic training data, it is possible to train, for example,\nphone-specific combination weights, allowing detailed relationships\nbetween the available well trained models to be obtained. To ensure\nrobust parameter estimation, this paper casts log-linear score combination\ninto a structured support vector machine (SSVM) learning task. This\nyields a method to train model parameters with good generalisation\nproperties. Here the SSVM feature space is a set of scores from well-trained\nindividual systems. The SSVM approach is compared to lattice rescoring\nand confusion network combination using language packs released within\nthe IARPA Babel program.\n"
   ],
   "doi": "10.21437/Interspeech.2016-377"
  },
  "tang16b_interspeech": {
   "authors": [
    [
     "Hao",
     "Tang"
    ],
    [
     "Weiran",
     "Wang"
    ],
    [
     "Kevin",
     "Gimpel"
    ],
    [
     "Karen",
     "Livescu"
    ]
   ],
   "title": "Efficient Segmental Cascades for Speech Recognition",
   "original": "1298",
   "page_count": 5,
   "order": 400,
   "p1": "1903",
   "pn": "1907",
   "abstract": [
    "Discriminative segmental models offer a way to incorporate flexible\nfeature functions into speech recognition. However, their appeal has\nbeen limited by their computational requirements, due to the large\nnumber of possible segments to consider. Multi-pass cascades of segmental\nmodels introduce features of increasing complexity in different passes,\nwhere in each pass a segmental model rescores lattices produced by\na previous (simpler) segmental model. In this paper, we explore several\nways of making segmental cascades efficient and practical: reducing\nthe feature set in the first pass, frame subsampling, and various pruning\napproaches. In experiments on phonetic recognition, we find that with\na combination of such techniques, it is possible to maintain competitive\nperformance while greatly reducing decoding, pruning, and training\ntime.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1298"
  },
  "xu16c_interspeech": {
   "authors": [
    [
     "Sirui",
     "Xu"
    ],
    [
     "Eric",
     "Fosler-Lussier"
    ]
   ],
   "title": "A WFST Framework for Single-Pass Multi-Stream Decoding",
   "original": "1307",
   "page_count": 5,
   "order": 401,
   "p1": "1908",
   "pn": "1912",
   "abstract": [
    "Combining disparate automatic speech recognition systems has long been\nan important strategy to improve recognition accuracy. Typically, each\nsystem requires a separate decoder; final results are derived by combining\nhypotheses from multiple lattices, necessitating multiple passes of\ndecoding. We propose a novel Weighted Finite State Transducer (WFST)\nframework for integrating disparate systems. Our framework is different\nfrom the current popular system combination techniques in that the\ncombination is done in one-pass decoding and allows the flexibility\nto combine systems at different levels of the decoding pipeline. Initial\nexperiments with the framework achieved comparable performance as MBR-based\ncombination which is reported to outperform ROVER and Confusion Network\nCombination (CNC). In this paper, we describe our methodology and present\npilot study results for combining systems that use different sets of\nacoustic models, 1) gender-dependent GMM models, 2) MFCC and PLP features\nwith GMM models, 3) MFCC, PLP and Filter Bank features with DNN models,\nand 4) SNR-specific DNN acoustic models. For each experiment, we also\ncompared the computation time of the combined systems with their corresponding\nbaseline systems. Our results show encouraging benefits of using the\nproposed framework to improve recognition performance while reducing\ncomputation time.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1307"
  },
  "hartmann16_interspeech": {
   "authors": [
    [
     "William",
     "Hartmann"
    ],
    [
     "Le",
     "Zhang"
    ],
    [
     "Kerri",
     "Barnes"
    ],
    [
     "Roger",
     "Hsiao"
    ],
    [
     "Stavros",
     "Tsakalidis"
    ],
    [
     "Richard",
     "Schwartz"
    ]
   ],
   "title": "Comparison of Multiple System Combination Techniques for Keyword Spotting",
   "original": "1381",
   "page_count": 5,
   "order": 402,
   "p1": "1913",
   "pn": "1917",
   "abstract": [
    "System combination is a common approach to improving results for both\nspeech transcription and keyword spotting &#8212; especially in the\ncontext of low-resourced languages where building multiple complementary\nmodels requires less computational effort. Using state-of-the-art CNN\nand DNN acoustic models, we analyze the performance, cost, and trade-offs\nof four system combination approaches: feature combination, joint decoding,\nhitlist combination, and a novel lattice combination method. Previous\nwork has focused solely on accuracy comparisons. We show that joint\ndecoding, lattice combination, and hitlist combination perform comparably,\nsignificantly better than feature combination. However, for practical\nsystems, earlier combination reduces computational cost and storage\nrequirements. Results are reported on four languages from the IARPA\nBabel dataset.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1381"
  },
  "obara16_interspeech": {
   "authors": [
    [
     "Masato",
     "Obara"
    ],
    [
     "Kazunori",
     "Kojima"
    ],
    [
     "Kazuyo",
     "Tanaka"
    ],
    [
     "Shi-wook",
     "Lee"
    ],
    [
     "Yoshiaki",
     "Itoh"
    ]
   ],
   "title": "Rescoring by Combination of Posteriorgram Score and Subword-Matching Score for Use in Query-by-Example",
   "original": "0309",
   "page_count": 5,
   "order": 403,
   "p1": "1918",
   "pn": "1922",
   "abstract": [
    "There has been much discussion recently regarding spoken term detection\n(STD) in speech processing communities. Query-by-Example (QbE) has\nalso been an important topic in spoken-term detection (STD), where\na query is issued using a speech signal. This paper proposes a rescoring\nmethod using a posteriorgram, which is a sequence of posterior probabilities\nobtained by a deep neural network (DNN) to be matched against both\na speech signal of a query and spoken documents. Because direct matching\nbetween two posteriorgrams requires significant computation time, we\nfirst apply a conventional STD method that performs matching at a subword\nor state level, where the subword denotes an acoustic model, and the\nstate composes a hidden Markov model of the acoustic model. Both the\nspoken query and the spoken documents are converted to subword sequences,\nusing an automatic speech recognizer. After obtaining scores of candidates\nby subword/state matching, matching at the frame level using the posteriorgram\nis performed with continuous dynamic programming (CDP) verification\nfor the top N candidates acquired by the subword/state matching. The\nscore of the subword/state matching and the score of the posteriorgram\nmatching are integrated and rescored, using a weighting coefficient.\nTo reduce computation time, the proposed method is restricted to only\ntop candidates for rescoring. Experiments for evaluation have been\ncarried out using open test collections (Spoken-Doc tasks of NTCIR-10\nworkshops), and the results have demonstrated the effectiveness of\nthe proposed method.\n"
   ],
   "doi": "10.21437/Interspeech.2016-309"
  },
  "chen16j_interspeech": {
   "authors": [
    [
     "Zhehuai",
     "Chen"
    ],
    [
     "Wei",
     "Deng"
    ],
    [
     "Tao",
     "Xu"
    ],
    [
     "Kai",
     "Yu"
    ]
   ],
   "title": "Phone Synchronous Decoding with CTC Lattice",
   "original": "0831",
   "page_count": 5,
   "order": 404,
   "p1": "1923",
   "pn": "1927",
   "abstract": [
    "Connectionist Temporal Classification (CTC) has recently shown improved\nefficiency in LVCSR decoding. One popular implementation is to use\na CTC model to predict the phone posteriors at each frame which are\nthen used for Viterbi beam search on a modified WFST network. This\nis still within the traditional frame synchronous decoding framework.\nIn this paper, the peaky posterior property of a CTC model is carefully\ninvestigated and it is found that ignoring blank frames will not introduce\nadditional search errors. Based on this phenomenon, a novel  phone\nsynchronous decoding framework is proposed. Here, a phone-level CTC\nlattice is constructed purely using the CTC acoustic model. The resultant\nCTC lattice is highly compact and removes tremendous search redundancy\ndue to blank frames. Then, the CTC lattice can be composed with the\nstandard WFST to yield the final decoding result. The proposed approach\neffectively separates the acoustic evidence calculation and the search\noperation. This not only significantly improves online search efficiency,\nbut also allows flexible acoustic/linguistic resources to be used.\nExperiments on LVCSR tasks show that phone synchronous decoding can\nyield an extra 2&#8211;3 times speed up compared to the traditional\nframe synchronous CTC decoding implementation.\n"
   ],
   "doi": "10.21437/Interspeech.2016-831"
  },
  "sahu16_interspeech": {
   "authors": [
    [
     "Saurabh",
     "Sahu"
    ],
    [
     "Carol",
     "Espy-Wilson"
    ]
   ],
   "title": "Speech Features for Depression Detection",
   "original": "1566",
   "page_count": 5,
   "order": 405,
   "p1": "1928",
   "pn": "1932",
   "abstract": [
    "In this paper we discuss speech features that are useful in the detection\nof depression. Neuro-physiological changes associated with depression\naffect motor coordination and can disrupt articulatory precision in\nspeech. We use the Mundt database and focus on six speakers in the\ndatabase that transitioned between being depressed and not depressed\nbased on their Hamilton depression scores. We quantify the degree of\nbreathiness, jitter and shimmer computed from an AMDF based parameter.\nMeasures from sustained vowels spoken in isolation show that all of\nthese attributes can increase when a person is depressed. In this study,\nwe focused on using features from free-flowing speech to classify the\ndepressed state of an individual. To do so we looked at vowel regions\nthat look the most like sustained vowels. We train an SVM for each\nspeaker and do a speaker dependent classification of the test speech\nframes. Using the AMDF based feature we got a better accuracy (62&#8211;87%\nframe-wise accuracy for 5 out of 6 speakers) for most speakers than\n13 dimensional MFCC along with its velocity and acceleration coefficients.\nUsing the AMDF based feature, we also trained a speaker independent\nSVM which gave an average accuracy of 77.8% for utterance based classification.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1566"
  },
  "ariasvergara16_interspeech": {
   "authors": [
    [
     "T.",
     "Arias-Vergara"
    ],
    [
     "J.C.",
     "Vasquez-Correa"
    ],
    [
     "Juan Rafael",
     "Orozco-Arroyave"
    ],
    [
     "J.F.",
     "Vargas-Bonilla"
    ],
    [
     "Elmar",
     "Nöth"
    ]
   ],
   "title": "Parkinson&#8217;s Disease Progression Assessment from Speech Using GMM-UBM",
   "original": "1122",
   "page_count": 5,
   "order": 406,
   "p1": "1933",
   "pn": "1937",
   "abstract": [
    "The Gaussian Mixture Model Universal Background Model (GMM-UBM) approach\nis used to assess the Parkinson&#8217;s disease (PD) progression per\nspeaker. The disease progression is assessed individually per patient\nfollowing a user modeling-approach. Voiced and unvoiced segments are\nextracted and grouped separately to train the models. Additionally,\nthe Bhattacharyya distance is used to estimate the difference between\nthe UBM and the user model. Speech recordings from 62 PD patients (34\nmale and 28 female) were captured from 2012 to 2015 in four recording\nsessions. The validation of the models is performed with recordings\nof 7 patients. All of the patients were diagnosed by a neurologist\nexpert according to the MDS-UPDRS-III scale. The features used to model\nthe speech of the patients are validated by doing a regression based\non a Support Vector Regressor (SVR). According to the results, it is\npossible to track the disease progression with a Pearson&#8217;s correlation\nof up to 0.60 with respect to the MDS-UPDRS-III labels.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1122"
  },
  "weiner16_interspeech": {
   "authors": [
    [
     "Jochen",
     "Weiner"
    ],
    [
     "Christian",
     "Herff"
    ],
    [
     "Tanja",
     "Schultz"
    ]
   ],
   "title": "Speech-Based Detection of Alzheimer&#8217;s Disease in Conversational German",
   "original": "0100",
   "page_count": 5,
   "order": 407,
   "p1": "1938",
   "pn": "1942",
   "abstract": [
    "The worldwide population is aging. With a larger population of elderly\npeople, the numbers of people affected by cognitive impairment such\nas Alzheimer&#8217;s disease are growing. Unfortunately, there is no\nknown cure for Alzheimer&#8217;s disease. The only way to alleviate\nit&#8217;s serious effects is to start therapy very early before the\ndisease has wrought too much irreversible damage. Current diagnostic\nprocedures are neither cost nor time efficient and therefore do not\nmeet the demands for frequent mass screening required to mitigate the\nconsequences of cognitive impairments on the global scale.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  We present an experiment\nto detect Alzheimer&#8217;s disease using spontaneous conversational\nspeech. The speech data was recorded during biographic interviews in\nthe Interdisciplinary Longitudinal Study on Adult Development and Aging\n(ILSE), a large data resource on healthy and satisfying aging in middle\nadulthood and later life in Germany. From these recordings we extract\nten speech-based features using voice activity detection and transcriptions.\nIn an experimental setup with 98 data samples we train a linear discriminant\nanalysis classifier to distinguish subjects with Alzheimer&#8217;s\ndisease from the control group. This setup results in an F-score of\n0.8 for the detection of Alzheimer&#8217;s disease, clearly showing\nour approach detects dementia well.\n"
   ],
   "doi": "10.21437/Interspeech.2016-100"
  },
  "alghowinem16_interspeech": {
   "authors": [
    [
     "Sharifa",
     "Alghowinem"
    ],
    [
     "Roland",
     "Goecke"
    ],
    [
     "Julien",
     "Epps"
    ],
    [
     "Michael",
     "Wagner"
    ],
    [
     "Jeffrey",
     "Cohn"
    ]
   ],
   "title": "Cross-Cultural Depression Recognition from Vocal Biomarkers",
   "original": "1339",
   "page_count": 5,
   "order": 408,
   "p1": "1943",
   "pn": "1947",
   "abstract": [
    "No studies have investigated cross-cultural and cross-language characteristics\nof depressed speech. We investigated the generalisability of a vocal\nbiomarker-based approach to depression detection in clinical interviews\nrecorded in three countries (Australia, the USA and Germany), two languages\n(German and English) and different accents (Australian and American).\nSeveral approaches to training and testing within and between datasets\nwere evaluated. Using the same experimental protocol separately within\neach dataset, (cross-classification) accuracy was high. Combining datasets,\nhigh accuracy was high again and consistent across language, recording\nenvironment, and culture. Training and testing between datasets, however,\nattenuated accuracy. These finding emphasize the importance of heterogeneous\ntraining sets for robust depression detection.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1339"
  },
  "zhou16_interspeech": {
   "authors": [
    [
     "Luke",
     "Zhou"
    ],
    [
     "Kathleen C.",
     "Fraser"
    ],
    [
     "Frank",
     "Rudzicz"
    ]
   ],
   "title": "Speech Recognition in Alzheimer&#8217;s Disease and in its Assessment",
   "original": "1228",
   "page_count": 5,
   "order": 409,
   "p1": "1948",
   "pn": "1952",
   "abstract": [
    "Narrative, spontaneous speech can provide a valuable source of information\nabout an individual&#8217;s cognitive state. Unfortunately, clinical\ntranscription of this type of data is typically done by hand, which\nis prohibitively time-consuming. In order to automate the entire process,\nwe optimize automatic speech recognition (ASR) for participants with\nAlzheimer&#8217;s disease (AD) in a relatively large clinical database.\nWe extract text features from the resulting transcripts and use these\nfeatures to identify AD with an SVM classifier. While the accuracy\nof automatic assessment decreases with increased WER, this is weakly\ncorrelated (-0.31). This relative robustness to ASR error is aided\nby selecting features that are resilient to ASR error.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1228"
  },
  "pokorny16_interspeech": {
   "authors": [
    [
     "Florian B.",
     "Pokorny"
    ],
    [
     "Peter B.",
     "Marschik"
    ],
    [
     "Christa",
     "Einspieler"
    ],
    [
     "Björn",
     "Schuller"
    ]
   ],
   "title": "Does She Speak RTT? Towards an Earlier Identification of Rett Syndrome Through Intelligent Pre-Linguistic Vocalisation Analysis",
   "original": "0520",
   "page_count": 5,
   "order": 410,
   "p1": "1953",
   "pn": "1957",
   "abstract": [
    "For many years, an apparently normal early development has been regarded\nas a main characteristic of Rett syndrome (RTT), a severe progressive\nneurodevelopmental disorder almost exclusively affecting girls/females.\nThe speech-language domain represents a key domain for the clinical\ndiagnosis of RTT, which usually happens around three years of age.\nRecent studies have built upon the assumption that this domain is already\naffected in the prodromal period. Aiming to find RTT-specific speech-language\natypicalities on signal level as early acoustic markers, we analysed\nmore than 16 hours of home video recordings of 4 girls later diagnosed\nwith RTT and 4 typically developing girls aged 6 to 12 months. We segmented\na total of 4 678 pre-linguistic vocalisations. A comprehensive set\nof acoustic features was extracted from the vocalisations as basis\nfor the classification paradigm RTT versus typical development. A promising\nmean unweighted recognition accuracy of 76.5% was achieved using linear\nkernel support vector machines and 4-fold leave-one-speaker-pair-out\ncross-validation. To the best of our knowledge, this is the first approach\nto automatically identify infants later diagnosed with RTT based on\nacoustic characteristics of pre-linguistic vocalisations. Our findings\nmay build the basis for facilitating earlier identification and thus\nan avenue for an earlier entry into intervention.\n"
   ],
   "doi": "10.21437/Interspeech.2016-520"
  },
  "pettorino16_interspeech": {
   "authors": [
    [
     "Massimo",
     "Pettorino"
    ],
    [
     "Maria Grazia",
     "Busà"
    ],
    [
     "Elisa",
     "Pellegrino"
    ]
   ],
   "title": "Speech Rhythm in Parkinson&#8217;s Disease: A Study on Italian",
   "original": "0074",
   "page_count": 4,
   "order": 411,
   "p1": "1958",
   "pn": "1961",
   "abstract": [
    "Experimental studies on different languages have shown that neurogenetic\ndisorders connected with Parkinson&#8217;s disease (PD) determine a\nseries of variations in the speech rhythm. This study aims at verifying\nwhether the speech of PD patients presents rhythmic abnormalities compared\nto healthy speakers also in Italian. The read speech of 15 healthy\nspeakers and of 11 patients with mild PD was segmented in consonantal\nand vocalic portions. After extracting the durations of all segments,\nthe vowel percentage (%V) and the interval between two consecutive\nvowel onset points (VtoV) were calculated. The results show that %V\nhas significantly different values in mildly affected patients as compared\nto controls. For Italian, %V spans between 44% and 50% for healthy\nsubjects and between 51% and 58% for PD subjects. A positive correlation\nwas found between %V and the number of years of PD since its insurgence.\nThe correlation with the age at which the disease insurges is weak.\nWith regard to VtoV, PD subjects do not speak at a significantly slower\nrate than healthy controls, though a trend in this direction was found.\nThe data suggest that %V could be used as a more reliable parameter\nfor the early diagnosis of PD than speech rate.\n"
   ],
   "doi": "10.21437/Interspeech.2016-74"
  },
  "anguera16_interspeech": {
   "authors": [
    [
     "Xavier",
     "Anguera"
    ],
    [
     "Vu",
     "Van"
    ]
   ],
   "title": "English Language Speech Assistant",
   "original": "2023",
   "page_count": 2,
   "order": 412,
   "p1": "1962",
   "pn": "1963",
   "abstract": [
    "This show&amp;tell demo presentation showcases ELSA Speak, an app for\nEnglish Language pronunciation and intonation improvement that uses\nspeech technology to assess the users speech and to offer consistent\nfeedback on the errors the students make.\n"
   ]
  },
  "guo16_interspeech": {
   "authors": [
    [
     "Allen",
     "Guo"
    ],
    [
     "Arlo",
     "Faria"
    ],
    [
     "Korbinian",
     "Riedhammer"
    ]
   ],
   "title": "Remeeting &#8212; Deep Insights to Conversations",
   "original": "2024",
   "page_count": 2,
   "order": 413,
   "p1": "1964",
   "pn": "1965",
   "abstract": [
    "Remeeting is a cloud service that helps you get insights to (spoken)\nconversations. Audio and video data such as recorded meetings, online\nconferences, sales or customer success calls are processed using speaker\nseparation and identification, speech recognition and indexing, and\nan automated keyword analysis. The resulting annotated &#8220;documents&#8221;\ncan be shared with others and reviewed using a web app that acts as\na visual index to the meeting. Furthermore, the extracted metadata\nis index by a search engine to allow for efficient cross-document search.\nA powerful query DSL allows the user to make sophisticated queries\nsuch as &#8220;what did X say about topic Y in the first quarter of\nthis year&#8221; or &#8220;show me the keywords for all meetings where\nX and Y attended&#8221;. Similar to retrieval, a watchdog can be set\nto deliver real-time insights to operations. Use cases include productivity\nin meetings, compliance and policies, real time callcenter analytics\nand better accessibility of large archives.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  Remeeting is leveraging,\npromoting and contributing to open source projects including Kaldi,\nElasticsearch and Docker.\n"
   ]
  },
  "chan16b_interspeech": {
   "authors": [
    [
     "Paul Yaozhu",
     "Chan"
    ],
    [
     "Minghui",
     "Dong"
    ],
    [
     "Grace Xue Hui",
     "Ho"
    ],
    [
     "Haizhou",
     "Li"
    ]
   ],
   "title": "SERAPHIM  Live! &#8212; Singing Synthesis for the Performer, the Composer, and the 3D Game Developer",
   "original": "2025",
   "page_count": 2,
   "order": 414,
   "p1": "1966",
   "pn": "1967",
   "abstract": [
    "The human singing voice is highly expressive instrument capable of\nproducing a variety of complex timbres. Singing synthesis today is\npopular amongst composers and studio musicians accessing the technology\nby means of offline sequencing platforms. Only a couple of singing\nsynthesizers are known to be equipped with both the real-time capability\nand the user interface to successfully target live performances. These\nare LIMSI&#8217;s Cantor Digitalis and Yamaha&#8217;s VOCALOID Keyboard.\nHowever, both systems have their own shortcomings. The former is limited\nto vowels and does not synthesize complete words or syllables. The\nlatter is only real-time to the syllable level and thus requires specifications\nof the entire syllable before it commences in the performance. A demand\nremains for a singing synthesis system that truly solves the problem\nof real-time synthesis &#8212; a system capable of synthesizing both\nvowels and consonants to form entire words while being capable of synthesizing\nin real-time to the sub-frame level. Such a system has to be versatile\nenough to exhaustively present all acoustic options possible to the\nuser for maximal control while being intelligent enough to fill in\nacoustic details that are too fine for human reflexes to control.<br\nstyle=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  SERAPHIM is a real-time singing synthesizer developed in answer\nto this demand. This paper presents the implementation of SERAPHIM\nfor performing musicians and studio musicians, together with how 3D\ngame developers may use Seraphim to deploy singing in their games.\n"
   ]
  },
  "malfrere16_interspeech": {
   "authors": [
    [
     "Fabrice",
     "Malfrere"
    ],
    [
     "Olivier",
     "Deroo"
    ],
    [
     "Emmanuelle",
     "Franques"
    ],
    [
     "Jonathan",
     "Hourez"
    ],
    [
     "Nicolas",
     "Mazars"
    ],
    [
     "Vincent",
     "Pagel"
    ],
    [
     "Geoffrey",
     "Wilfart"
    ]
   ],
   "title": " My-Own-Voice: A Web Service That Allows You to Create a Text-to-Speech Voice From Your Own Voice",
   "original": "2010",
   "page_count": 2,
   "order": 415,
   "p1": "1968",
   "pn": "1969",
   "abstract": [
    " My-Own-Voice is a service that provides a tool to end-users who want\nto have their voices synthesized by a high-quality commercial-grade\nText-to-Speech system without the need to install, configure or manage\nspeech-processing software and equipment. The system records and validates\nusers&#8217; utterances with Automatic Speech Recognition (ASR), to\nbuild an HMM or a Unit Selection synthetic voice. All the procedures\nare automated to avoid human intervention. We describe here the system\nfor particular end-users about to lose the ability to speak with their\nown voice, who can now synthetically recreate it with the help of their\nspeech therapist, enabling them to preserve this essential part of\ntheir identity.\n"
   ]
  },
  "fernald16_interspeech": {
   "authors": [
    [
     "Anne",
     "Fernald"
    ]
   ],
   "title": "Talking with Kids Really Matters: Early Language Experience Shapes Later Life Chances",
   "original": "3003",
   "page_count": 1,
   "order": 416,
   "p1": "1970",
   "pn": "1970",
   "abstract": [
    "The foundation for lifelong literacy is built through a child&#8217;s\nexperience with language in the first five years. Integrating research\nfrom biological, psycholinguistic, and sociocultural perspectives,\nI will examine why millions of children fail to reach their developmental\npotential in the early years and enter school without a strong foundation\nfor learning, resulting in enormous loss of human potential.\n"
   ]
  },
  "sainath16b_interspeech": {
   "authors": [
    [
     "Tara N.",
     "Sainath"
    ],
    [
     "Arun",
     "Narayanan"
    ],
    [
     "Ron J.",
     "Weiss"
    ],
    [
     "Ehsan",
     "Variani"
    ],
    [
     "Kevin W.",
     "Wilson"
    ],
    [
     "Michiel",
     "Bacchiani"
    ],
    [
     "Izhak",
     "Shafran"
    ]
   ],
   "title": "Reducing the Computational Complexity of Multimicrophone Acoustic Models with Integrated Feature Extraction",
   "original": "0092",
   "page_count": 5,
   "order": 417,
   "p1": "1971",
   "pn": "1975",
   "abstract": [
    "Recently, we presented a multichannel neural network model trained\nto perform speech enhancement jointly with acoustic modeling [1], directly\nfrom raw waveform input signals. While this model achieved over a 10%\nrelative improvement compared to a single channel model, it came at\na large cost in computational complexity, particularly in the convolutions\nused to implement a time-domain filterbank. In this paper we present\nseveral different approaches to reduce the complexity of this model\nby reducing the stride of the convolution operation and by implementing\nfilters in the frequency domain. These optimizations reduce the computational\ncomplexity of the model by a factor of 3 with no loss in accuracy on\na 2,000 hour Voice Search task.\n"
   ],
   "doi": "10.21437/Interspeech.2016-92"
  },
  "li16f_interspeech": {
   "authors": [
    [
     "Bo",
     "Li"
    ],
    [
     "Tara N.",
     "Sainath"
    ],
    [
     "Ron J.",
     "Weiss"
    ],
    [
     "Kevin W.",
     "Wilson"
    ],
    [
     "Michiel",
     "Bacchiani"
    ]
   ],
   "title": "Neural Network Adaptive Beamforming for Robust Multichannel Speech Recognition",
   "original": "0173",
   "page_count": 5,
   "order": 418,
   "p1": "1976",
   "pn": "1980",
   "abstract": [
    "Joint multichannel enhancement and acoustic modeling using neural networks\nhas shown promise over the past few years. However, one shortcoming\nof previous work [1, 2, 3] is that the filters learned during training\nare fixed for decoding, potentially limiting the ability of these models\nto adapt to previously unseen or changing conditions. In this paper\nwe explore a neural network adaptive beamforming (NAB) technique to\naddress this issue. Specifically, we use LSTM layers to predict time\ndomain beamforming filter coefficients at each input frame. These filters\nare convolved with the framed time domain input signal and summed across\nchannels, essentially performing FIR filter-and-sum beamforming using\nthe dynamically adapted filter. The beamformer output is passed into\na waveform CLDNN acoustic model [4] which is trained jointly with the\nfilter prediction LSTM layers. We find that the proposed NAB model\nachieves a 12.7% relative improvement in WER over a single channel\nmodel [4] and reaches similar performance to a &#8220;factored&#8221;\nmodel architecture which utilizes several fixed spatial filters [3]\non a 2,000-hour Voice Search task, with a 17.9% decrease in computational\ncost.\n"
   ],
   "doi": "10.21437/Interspeech.2016-173"
  },
  "erdogan16_interspeech": {
   "authors": [
    [
     "Hakan",
     "Erdogan"
    ],
    [
     "John R.",
     "Hershey"
    ],
    [
     "Shinji",
     "Watanabe"
    ],
    [
     "Michael I.",
     "Mandel"
    ],
    [
     "Jonathan Le",
     "Roux"
    ]
   ],
   "title": "Improved MVDR Beamforming Using Single-Channel Mask Prediction Networks",
   "original": "0552",
   "page_count": 5,
   "order": 419,
   "p1": "1981",
   "pn": "1985",
   "abstract": [
    "Recent studies on multi-microphone speech databases indicate that it\nis beneficial to perform beamforming to improve speech recognition\naccuracies, especially when there is a high level of background noise.\nMinimum variance distortionless response (MVDR) beamforming is an important\nbeamforming method that performs quite well for speech recognition\npurposes especially if the steering vector is known. However, steering\nthe beamformer to focus on speech in unknown acoustic conditions remains\na challenging problem. In this study, we use single-channel speech\nenhancement deep networks to form masks that can be used for noise\nspatial covariance estimation, which steers the MVDR beamforming toward\nthe speech. We analyze how mask prediction affects performance and\nalso discuss various ways to use masks to obtain the speech and noise\nspatial covariance estimates in a reliable way. We show that using\na single mask across microphones for covariance prediction with minima-limited\npost-masking yields the best result in terms of signal-level quality\nmeasures and speech recognition word error rates in a mismatched training\ncondition.\n"
   ],
   "doi": "10.21437/Interspeech.2016-552"
  },
  "guerrero16_interspeech": {
   "authors": [
    [
     "Cristina",
     "Guerrero"
    ],
    [
     "Georgina",
     "Tryfou"
    ],
    [
     "Maurizio",
     "Omologo"
    ]
   ],
   "title": "Channel Selection for Distant Speech Recognition Exploiting Cepstral Distance",
   "original": "0865",
   "page_count": 5,
   "order": 420,
   "p1": "1986",
   "pn": "1990",
   "abstract": [
    "In a multi-microphone distant speech recognition task, the redundancy\nof information that results from the availability of multiple instances\nof the same source signal can be exploited through channel selection.\nIn this work, we propose the use of cepstral distance as a means of\nassessment of the available channels, in an informed and a blind fashion.\nIn the informed approach the distances between the close-talk and all\nof the channels are calculated. In the blind method, the cepstral distances\nare computed using an estimated reference signal, assumed to represent\nthe average distortion among the available channels. Furthermore, we\npropose a new evaluation methodology that better illustrates the strengths\nand weaknesses of a channel selection method, in comparison to the\nsole use of word error rate. The experimental results suggest that\nthe proposed blind method successfully selects the least distorted\nchannel, when sufficient room coverage is provided by the microphone\nnetwork. As a result, improved recognition rates are obtained in a\ndistant speech recognition task, both in a simulated and a real context.\n"
   ],
   "doi": "10.21437/Interspeech.2016-865"
  },
  "mandel16b_interspeech": {
   "authors": [
    [
     "Michael I.",
     "Mandel"
    ],
    [
     "Jon",
     "Barker"
    ]
   ],
   "title": "Multichannel Spatial Clustering for Robust Far-Field Automatic Speech Recognition in Mismatched Conditions",
   "original": "1275",
   "page_count": 5,
   "order": 421,
   "p1": "1991",
   "pn": "1995",
   "abstract": [
    "Recent automatic speech recognition (ASR) results are quite good when\nthe training data is matched to the test data, but much worse when\nthey differ in some important regard, like the number and arrangement\nof microphones or differences in reverberation and noise conditions.\nThis paper proposes an unsupervised spatial clustering approach to\nmicrophone array processing that can overcome such train-test mismatches.\nThis approach, known as Model-based EM Source Separation and Localization\n(MESSL), clusters spectrogram points based on the relative differences\nin phase and level between pairs of microphones. Here it is used for\nthe first time to drive minimum variance distortionless response (MVDR)\nbeamforming in several ways. We compare it to a standard delay-and-sum\nbeamformer on the CHiME-3 noisy test set (real recordings), using each\nsystem as a pre-processor for the same recognizer trained on the AMI\nmeeting corpus. We find that the spatial clustering front end reduces\nword error rates by between 9.9 and 17.1% relative to the baseline.\n",
    ""
   ],
   "doi": "10.21437/Interspeech.2016-1275"
  },
  "peddinti16_interspeech": {
   "authors": [
    [
     "Vijayaditya",
     "Peddinti"
    ],
    [
     "Vimal",
     "Manohar"
    ],
    [
     "Yiming",
     "Wang"
    ],
    [
     "Daniel",
     "Povey"
    ],
    [
     "Sanjeev",
     "Khudanpur"
    ]
   ],
   "title": "Far-Field ASR Without Parallel Data",
   "original": "1475",
   "page_count": 5,
   "order": 422,
   "p1": "1996",
   "pn": "2000",
   "abstract": [
    "In far-field speech recognition systems, training acoustic models with\nalignments generated from parallel close-talk microphone data provides\nsignificant improvements. However it is not practical to assume the\navailability of large corpora of parallel close-talk microphone data,\nfor training. In this paper we explore methods to reduce the performance\ngap between far-field ASR systems trained with alignments from distant\nmicrophone data and those trained with alignments from parallel close-talk\nmicrophone data. These methods include the use of a lattice-free sequence\nobjective function which tolerates minor mis-alignment errors; and\nthe use of data selection techniques to discard badly aligned data.\nWe present results on single distant microphone and multiple distant\nmicrophone scenarios of the AMI LVCSR task. We identify prominent causes\nof alignment errors in AMI data.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1475"
  },
  "schuller16_interspeech": {
   "authors": [
    [
     "Björn",
     "Schuller"
    ],
    [
     "Stefan",
     "Steidl"
    ],
    [
     "Anton",
     "Batliner"
    ],
    [
     "Julia",
     "Hirschberg"
    ],
    [
     "Judee K.",
     "Burgoon"
    ],
    [
     "Alice",
     "Baird"
    ],
    [
     "Aaron",
     "Elkins"
    ],
    [
     "Yue",
     "Zhang"
    ],
    [
     "Eduardo",
     "Coutinho"
    ],
    [
     "Keelan",
     "Evanini"
    ]
   ],
   "title": "The INTERSPEECH 2016 Computational Paralinguistics Challenge: Deception, Sincerity &amp; Native Language",
   "original": "0129",
   "page_count": 5,
   "order": 423,
   "p1": "2001",
   "pn": "2005",
   "abstract": [
    "The INTERSPEECH 2016 Computational Paralinguistics Challenge addresses\nthree different problems for the first time in research competition\nunder well-defined conditions: classification of deceptive vs. non-deceptive\nspeech, the estimation of the degree of sincerity, and the identification\nof the native language out of eleven L1 classes of English L2 speakers.\nIn this paper, we describe these sub-challenges, their conditions,\nthe baseline feature extraction and classifiers, and the resulting\nbaselines, as provided to the participants.\n"
   ],
   "doi": "10.21437/Interspeech.2016-129"
  },
  "schuller16b_interspeech": {
   "authors": [
    [
     "Björn",
     "Schuller"
    ],
    [
     "Stefan",
     "Steidl"
    ],
    [
     "Anton",
     "Batliner"
    ],
    [
     "Julia",
     "Hirschberg"
    ],
    [
     "Judee K.",
     "Burgoon"
    ],
    [
     "Alice",
     "Baird"
    ],
    [
     "Aaron",
     "Elkins"
    ],
    [
     "Yue",
     "Zhang"
    ],
    [
     "Eduardo",
     "Coutinho"
    ],
    [
     "Keelan",
     "Evanini"
    ]
   ],
   "title": "The Deception Sub-Challenge: The Data",
   "original": "abs5",
   "page_count": 0,
   "order": 424,
   "p1": "0",
   "pn": "",
   "abstract": [
    "(No abstract available at the time of publication)\n"
   ]
  },
  "levitan16b_interspeech": {
   "authors": [
    [
     "Sarah Ita",
     "Levitan"
    ],
    [
     "Guozhen",
     "An"
    ],
    [
     "Min",
     "Ma"
    ],
    [
     "Rivka",
     "Levitan"
    ],
    [
     "Andrew",
     "Rosenberg"
    ],
    [
     "Julia",
     "Hirschberg"
    ]
   ],
   "title": "Combining Acoustic-Prosodic, Lexical, and Phonotactic Features for Automatic Deception Detection",
   "original": "1519",
   "page_count": 5,
   "order": 425,
   "p1": "2006",
   "pn": "2010",
   "abstract": [
    "Improving methods of automatic deception detection is an important\ngoal of many researchers from a variety of disciplines, including psychology,\ncomputational linguistics, and criminology. We present a system to\nautomatically identify deceptive utterances using acoustic-prosodic,\nlexical, syntactic, and phonotactic features. We train and test our\nsystem on the Interspeech 2016 ComParE challenge corpus, and find that\nour combined features result in performance well above the challenge\nbaseline on the development data. We also perform feature ranking experiments\nto evaluate the usefulness of each of our feature sets. Finally, we\nconduct a cross-corpus evaluation by training on another deception\ncorpus and testing on the ComParE corpus.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1519"
  },
  "amiriparian16_interspeech": {
   "authors": [
    [
     "Shahin",
     "Amiriparian"
    ],
    [
     "Jouni",
     "Pohjalainen"
    ],
    [
     "Erik",
     "Marchi"
    ],
    [
     "Sergey",
     "Pugachevskiy"
    ],
    [
     "Björn",
     "Schuller"
    ]
   ],
   "title": "Is Deception Emotional? An Emotion-Driven Predictive Approach",
   "original": "0565",
   "page_count": 5,
   "order": 426,
   "p1": "2011",
   "pn": "2015",
   "abstract": [
    "In this paper, we propose a method for automatically detecting deceptive\nspeech by relying on predicted scores derived from emotion dimensions\nsuch as arousal, valence, regulation, and emotion categories. The scores\nare derived from task-dependent models trained on the GEMEP emotional\nspeech database. Inputs from the INTERSPEECH 2016 Computational Paralinguistics\nDeception sub-challenge are processed to obtain predictions of emotion\nattributes and associated scores that are then used as features in\ndetecting deception. We show that using the new emotion-related features,\nit is possible to improve upon the challenge baseline.\n"
   ],
   "doi": "10.21437/Interspeech.2016-565"
  },
  "montacie16_interspeech": {
   "authors": [
    [
     "Claude",
     "Montacié"
    ],
    [
     "Marie-José",
     "Caraty"
    ]
   ],
   "title": "Prosodic Cues and Answer Type Detection for the Deception Sub-Challenge",
   "original": "0033",
   "page_count": 5,
   "order": 427,
   "p1": "2016",
   "pn": "2020",
   "abstract": [
    "Deception is a deliberate act to deceive interlocutor by transmitting\na message containing false or misleading information. Detection of\ndeception consists in the search for reliable differences between liars\nand truth-tellers. In this paper, we used the Deceptive Speech Database\n(DSD) provided for the Deception sub-challenge. DSD consists of deceptive\nand non-deceptive answers to a set of unknown questions. We have investigated\nlinguistic cues: prosodic cues (pauses and phone duration, speech segmentation)\nand answer types (e.g., opinion, self-report, offense denial). These\ncues were automatically detected using the CMU-Sphinx toolkit for speech\nrecognition (acoustic-phonetic decoding, isolated word recognition\nand keyword spotting). Two kinds of prosodic features were computed\nfrom the speech transcriptions (phoneme, silent pause, filled pause,\nand breathing): the usual speech rate measures and the audio feature\nbased on the multi-resolution paradigm. The answer type features were\nintroduced. A set of answer types was chosen from the transcription\nof the Training set and each answer type was modeled by a bag-of-words.\nExperiments have shown improvements of 13.0% and 3.8% on the Development\nand Test sets respectively, compared to the official baseline Unweighted\nAverage Recall.\n"
   ],
   "doi": "10.21437/Interspeech.2016-33"
  },
  "schuller16c_interspeech": {
   "authors": [
    [
     "Björn",
     "Schuller"
    ],
    [
     "Stefan",
     "Steidl"
    ],
    [
     "Anton",
     "Batliner"
    ],
    [
     "Julia",
     "Hirschberg"
    ],
    [
     "Judee K.",
     "Burgoon"
    ],
    [
     "Alice",
     "Baird"
    ],
    [
     "Aaron",
     "Elkins"
    ],
    [
     "Yue",
     "Zhang"
    ],
    [
     "Eduardo",
     "Coutinho"
    ],
    [
     "Keelan",
     "Evanini"
    ]
   ],
   "title": "The Sincerity Sub-Challenge: The Data",
   "original": "abs6",
   "page_count": 0,
   "order": 428,
   "p1": "0",
   "pn": "",
   "abstract": [
    "(No abstract available at the time of publication)\n"
   ]
  },
  "booth16_interspeech": {
   "authors": [
    [
     "Brandon M.",
     "Booth"
    ],
    [
     "Rahul",
     "Gupta"
    ],
    [
     "Pavlos",
     "Papadopoulos"
    ],
    [
     "Ruchir",
     "Travadi"
    ],
    [
     "Shrikanth S.",
     "Narayanan"
    ]
   ],
   "title": "Automatic Estimation of Perceived Sincerity from Spoken Language",
   "original": "1537",
   "page_count": 5,
   "order": 429,
   "p1": "2021",
   "pn": "2025",
   "abstract": [
    "Sincerity is important in everyday human communication and perception\nof genuineness can greatly affect emotions and outcomes in social interactions.\nIn this paper, submitted for the INTERSPEECH 2016 Sincerity Challenge,\nwe examine a corpus of six different types of apologetic utterances\nfrom a variety of English speakers articulated in different prosodic\nstyles, and we rate the sincerity of each remark. Since the utterances\nand semantic meaning in the examined database are controlled, we focus\non tone of voice by exploring a plethora of acoustic and paralinguistic\nfeatures not present in the baseline model and how well they contribute\nto human assessment of sincerity. We show that these additional features\nimprove the performance using the baseline model, and furthermore that\nconditioning learning models on the prosody of utterances boosts the\nprediction accuracy. Our best system outperforms the challenge baseline\nand in principle can generalize well to other corpora.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1537"
  },
  "gosztolya16b_interspeech": {
   "authors": [
    [
     "Gábor",
     "Gosztolya"
    ],
    [
     "Tamás",
     "Grósz"
    ],
    [
     "György",
     "Szaszák"
    ],
    [
     "László",
     "Tóth"
    ]
   ],
   "title": "Estimating the Sincerity of Apologies in Speech by DNN Rank Learning and Prosodic Analysis",
   "original": "0956",
   "page_count": 5,
   "order": 430,
   "p1": "2026",
   "pn": "2030",
   "abstract": [
    "In the Sincerity Sub-Challenge of the Interspeech ComParE 2016 Challenge,\nthe task is to estimate user-annotated sincerity scores for speech\nsamples. We interpret this challenge as a rank-learning regression\ntask, since the evaluation metric (Spearman&#8217;s correlation) is\ncalculated from the rank of the instances. As a first approach, Deep\nNeural Networks are used by introducing a novel error criterion which\nmaximizes the correlation metric directly. We obtained the best performance\nby combining the proposed error function with the conventional MSE\nerror. This approach yielded results that outperform the baseline on\nthe Challenge test set. Furthermore, we introduce a compact prosodic\nfeature set based on a dynamic representation of F0, energy and sound\nduration. We extract syllable-based prosodic features which are used\nas the basis of another machine learning step. We show that a small\nset of prosodic features is capable of yielding a result very close\nto the baseline one and that by combining the predictions yielded by\nDNN and the prosodic feature set, further improvement can be reached,\nsignificantly outperforming the baseline SVR on the Challenge test\nset.\n"
   ],
   "doi": "10.21437/Interspeech.2016-956"
  },
  "lee16c_interspeech": {
   "authors": [
    [
     "Hung-Shin",
     "Lee"
    ],
    [
     "Yu",
     "Tsao"
    ],
    [
     "Chi-Chun",
     "Lee"
    ],
    [
     "Hsin-Min",
     "Wang"
    ],
    [
     "Wei-Cheng",
     "Lin"
    ],
    [
     "Wei-Chen",
     "Chen"
    ],
    [
     "Shan-Wen",
     "Hsiao"
    ],
    [
     "Shyh-Kang",
     "Jeng"
    ]
   ],
   "title": "Minimization of Regression and Ranking Losses with Shallow Neural Networks on Automatic Sincerity Evaluation",
   "original": "0756",
   "page_count": 5,
   "order": 431,
   "p1": "2031",
   "pn": "2035",
   "abstract": [
    "To estimate the degree of sincerity conveyed by a speech utterance\nand received by listeners, we propose an instance-based learning framework\nwith shallow neural networks. The framework plays as not only a regressor\nthat intends to fit the predicted value to the actual value but also\na ranker that preserves the relative target magnitude between each\npair of utterances, in an attempt to derive a higher Spearman&#8217;s\nrank correlation coefficient. In addition to describing how to simultaneously\nminimize regression and ranking losses, the issue of how utterance\npairs work in the training and evaluation phases is also addressed\nby two kinds of realizations. The intuitive one is related to random\nsampling while the other seeks for representative utterances, named\nanchors, to form non-stochastic pairs. Our system outperforms the baseline\nby more than 25% relative improvement in the development set.\n"
   ],
   "doi": "10.21437/Interspeech.2016-756"
  },
  "herms16_interspeech": {
   "authors": [
    [
     "Robert",
     "Herms"
    ]
   ],
   "title": "Prediction of Deception and Sincerity from Speech Using Automatic Phone Recognition-Based Features",
   "original": "0971",
   "page_count": 5,
   "order": 432,
   "p1": "2036",
   "pn": "2040",
   "abstract": [
    "As part of the Interspeech 2016 COMPARE challenge, the two different\nsub-challenges Deception and Sincerity are addressed. The former refers\nto the identification of deceptive speech whereas the degree of perceived\nsincerity of speakers has to be estimated in the latter. In this paper,\nwe investigate the potential of automatic phone recognition-based features\nfor these use case scenarios. The speech transcriptions were used to\nprocess the appearing tokens (phoneme, silent pause, filled pause)\nand the corresponding durations. We designed a high-level feature set\nincluding the four groups: vowels, phones, pseudo syllables, and pauses.\nAdditionally, we selected suitable predefined acoustic feature sets\nand fused them with our introduced features showing a positive effect\non the prediction. Moreover, the performance is further boosted by\nrefining these fused features using the ReliefF feature selection method.\nExperiments show that the final systems outperform the baseline results\nof both sub-challenges.\n"
   ],
   "doi": "10.21437/Interspeech.2016-971"
  },
  "zhang16f_interspeech": {
   "authors": [
    [
     "Yue",
     "Zhang"
    ],
    [
     "Felix",
     "Weninger"
    ],
    [
     "Zhao",
     "Ren"
    ],
    [
     "Björn",
     "Schuller"
    ]
   ],
   "title": "Sincerity and Deception in Speech: Two Sides of the Same Coin? A Transfer- and Multi-Task Learning Perspective",
   "original": "1305",
   "page_count": 5,
   "order": 433,
   "p1": "2041",
   "pn": "2045",
   "abstract": [
    "In this work, we investigate the coherence between inferable deception\nand perceived sincerity in speech, as featured in the Deception and\nSincerity tasks of the INTERSPEECH 2016 Computational Paralinguistics\nChallengE (ComParE). We demonstrate an effective approach that combines\nthe corpora of both Challenge tasks to achieve higher classification\naccuracy. We show that the na&#239;ve label mapping method based on\nthe assumption that sincerity and deception are just &#8216;two sides\nof the same coin&#8217;, i. e., taking deceptive speech as equivalent\nto non-sincere speech and vice versa, does not yield satisfactory results.\nHowever, we can exploit the interplay and synergies between these characteristics.\nTo achieve this, we combine our previously introduced approach for\ndata aggregation by semi-supervised cross-task label completion with\nmulti-task learning, and knowledge-based instance selection. In the\nresult, our approach achieves significant error rate reductions compared\nto the official Challenge baseline.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1305"
  },
  "kaya16_interspeech": {
   "authors": [
    [
     "Heysem",
     "Kaya"
    ],
    [
     "Alexey A.",
     "Karpov"
    ]
   ],
   "title": "Fusing Acoustic Feature Representations for Computational Paralinguistics Tasks",
   "original": "0995",
   "page_count": 5,
   "order": 434,
   "p1": "2046",
   "pn": "2050",
   "abstract": [
    "The field of Computational Paralinguistics is rapidly growing and is\nof interest in various application domains ranging from biomedical\nengineering to forensics. The INTERSPEECH ComParE challenge series\nhas a field-leading role, introducing novel problems with a common\nbenchmark protocol for comparability. In this work, we tackle all three\nComParE 2016 Challenge corpora (Native Language, Sincerity and Deception)\nbenefiting from multi-level normalization on features followed by fast\nand robust kernel learning methods. Moreover, we employ computer vision\ninspired low level descriptor representation methods such as the Fisher\nvector encoding. After non-linear preprocessing, obtained Fisher vectors\nare kernelized and mapped to target variables by classifiers based\non Kernel Extreme Learning Machines and Partial Least Squares regression.\nWe finally combine predictions of models trained on popularly used\nfunctional based descriptor encoding (openSMILE features) with those\nobtained from the Fisher vector encoding. In the preliminary experiments,\nour approach has significantly outperformed the baseline systems for\nNative Language and Sincerity sub-challenges both in the development\nand test sets.\n"
   ],
   "doi": "10.21437/Interspeech.2016-995"
  },
  "harte16_interspeech": {
   "authors": [
    [
     "Naomi",
     "Harte"
    ],
    [
     "Peter",
     "Jančovič"
    ],
    [
     "Karl-L.",
     "Schuchmann"
    ]
   ],
   "title": "Introduction",
   "original": "abs7",
   "page_count": 0,
   "order": 435,
   "p1": "0",
   "pn": "",
   "abstract": [
    "(No abstract available at the time of publication)\n"
   ]
  },
  "harte16b_interspeech": {
   "authors": [
    [
     "Naomi",
     "Harte"
    ],
    [
     "Peter",
     "Jančovič"
    ],
    [
     "Karl-L.",
     "Schuchmann"
    ]
   ],
   "title": "Poster Overview Presentations",
   "original": "abs8",
   "page_count": 0,
   "order": 436,
   "p1": "0",
   "pn": "",
   "abstract": [
    "(No abstract available at the time of publication)\n"
   ]
  },
  "harte16c_interspeech": {
   "authors": [
    [
     "Naomi",
     "Harte"
    ],
    [
     "Peter",
     "Jančovič"
    ],
    [
     "Karl-L.",
     "Schuchmann"
    ]
   ],
   "title": "Discussion",
   "original": "abs9",
   "page_count": 0,
   "order": 437,
   "p1": "0",
   "pn": "",
   "abstract": [
    "(No abstract available at the time of publication)\n"
   ]
  },
  "harte16d_interspeech": {
   "authors": [
    [
     "Naomi",
     "Harte"
    ],
    [
     "Peter",
     "Jančovič"
    ],
    [
     "Karl-L.",
     "Schuchmann"
    ]
   ],
   "title": "Closing Remarks",
   "original": "abs10",
   "page_count": 0,
   "order": 438,
   "p1": "0",
   "pn": "",
   "abstract": [
    "(No abstract available at the time of publication)\n"
   ]
  },
  "barlier16_interspeech": {
   "authors": [
    [
     "Merwan",
     "Barlier"
    ],
    [
     "Romain",
     "Laroche"
    ],
    [
     "Olivier",
     "Pietquin"
    ]
   ],
   "title": "A Stochastic Model for Computer-Aided Human-Human Dialogue",
   "original": "0479",
   "page_count": 5,
   "order": 439,
   "p1": "2051",
   "pn": "2055",
   "abstract": [
    "In this paper we introduce a novel model for computer-aided human-human\ndialogue. In this context, the computer aims at improving the outcome\nof a human-human task-oriented dialogue by intervening during the course\nof the interaction. While dialogue state and topic tracking in human-human\ndialogue have already been studied, few work has been devoted to the\nsequential part of the problem, where the impact of the system&#8217;s\nactions on the future of the conversation is taken into account. This\npaper addresses this issue by first modelling human-human dialogue\nas a Markov Reward Process. The task of purposely taking part into\nthe conversation is then optimised within the Linearly Solvable Markov\nDecision Process framework. Utterances of the Conversational Agent\nare seen as perturbations in this process, which aim at satisfying\nthe user&#8217;s long-term goals while keeping the conversation natural.\nFinally, results obtained by simulation suggest that such an approach\nis suitable for computer-aided human-human dialogue and is a first\nstep towards three-party dialogue.\n"
   ],
   "doi": "10.21437/Interspeech.2016-479"
  },
  "lejeune16_interspeech": {
   "authors": [
    [
     "Gaël",
     "Lejeune"
    ],
    [
     "François",
     "Rioult"
    ],
    [
     "Bruno",
     "Crémilleux"
    ]
   ],
   "title": "Highlighting Psychological Features for Predicting Child Interjections During Story Telling",
   "original": "0527",
   "page_count": 4,
   "order": 440,
   "p1": "2056",
   "pn": "2059",
   "abstract": [
    "Conversational agents are more and more investigated by the community\nbut their ability to keep the user committed in the interaction is\nlimited. Predicting the behavior of children in a human-machine interaction\nsetting is a key issue for the success of narrative conversational\nagents. In this paper, we investigate solutions to evaluate the child&#8217;s\ncommitment in the story and to detect when the child is likely to react\nduring the story. We show that the conversational agent cannot solely\ncount on questions and requests for attention to stimulate the child.\nWe assess how (1) psychological features allow to improve the prediction\nof children interjections and how (2) exploiting these features with\nPattern Mining techniques offers better results. Experiments show that\npsychological features improves the predictions and furthermore help\nto produce robust dialog models.\n"
   ],
   "doi": "10.21437/Interspeech.2016-527"
  },
  "sun16c_interspeech": {
   "authors": [
    [
     "Kai",
     "Sun"
    ],
    [
     "Su",
     "Zhu"
    ],
    [
     "Lu",
     "Chen"
    ],
    [
     "Siqiu",
     "Yao"
    ],
    [
     "Xueyang",
     "Wu"
    ],
    [
     "Kai",
     "Yu"
    ]
   ],
   "title": "Hybrid Dialogue State Tracking for Real World Human-to-Human Dialogues",
   "original": "0949",
   "page_count": 5,
   "order": 441,
   "p1": "2060",
   "pn": "2064",
   "abstract": [
    "Dialogue state tracking is a key sub-task of dialogue management. The\nfourth Dialog State Tracking Challenge (DSTC-4) focuses on dialogue\nstate tracking for real world human-to-human dialogues. The task is\nmore challenging than previous challenges because of more complex domain\nand coreferences, more synonyms and abbreviations, sub-dialogue level\nlabelled utterances, and no spoken language understanding output provided.\nTo deal with these challenges, this paper proposes a novel hybrid dialogue\nstate tracking method, which can take advantage of the strength of\nboth rule-based and statistical methods. Thousands of rules are first\nautomatically generated using a template-based rule generation approach\nand then combined together with several manually designed rules to\nyield the output of the rule-based method. In parallel, a statistical\nmethod is applied to track the state. The tracker finally takes the\nunion of the outputs of the two methods. In DSTC-4 evaluation, the\nproposed hybrid tracker obtained state-of-the-art results. It ranked\nthe second and significantly outperformed the baseline system and most\nsubmissions.\n"
   ],
   "doi": "10.21437/Interspeech.2016-949"
  },
  "fotedar16_interspeech": {
   "authors": [
    [
     "Gaurav",
     "Fotedar"
    ],
    [
     "Aditya Gaonkar",
     "P."
    ],
    [
     "Saikat",
     "Chatterjee"
    ],
    [
     "Prasanta Kumar",
     "Ghosh"
    ]
   ],
   "title": "Automatic Recognition of Social Roles Using Long Term Role Transitions in Small Group Interactions",
   "original": "0202",
   "page_count": 5,
   "order": 442,
   "p1": "2065",
   "pn": "2069",
   "abstract": [
    "Recognition of social roles in small group interactions is challenging\nbecause of the presence of disfluency in speech, frequent overlaps\nbetween speakers, short speaker turns and the need for reliable data\nannotation. In this work, we consider the problem of recognizing four\nroles, namely Gatekeeper, Protagonist, Neutral, and Supporter in small\ngroup interactions in AMI corpus. In general, Gatekeeper and Protagonist\nroles occur less frequently compared to Neutral, and Supporter. In\nthis work, we exploit role transitions across segments in a meeting\nby incorporating role transition probabilities and formulating the\nrole recognition as a decoding problem over the sequence of segments\nin an interaction. Experiments are performed in a five fold cross validation\nsetup using acoustic, lexical and structural features with precision,\nrecall and F-score as the performance metrics. The results reveal that\nprecision averaged across all folds and different feature combinations\nimproves in the case of Gatekeeper and Protagonist by 13.64% and 12.75%\nwhen the role transition information is used which in turn improves\nthe F-score for Gatekeeper by 6.58% while the F-scores for the rest\nof the roles do not change significantly.\n"
   ],
   "doi": "10.21437/Interspeech.2016-202"
  },
  "eecke16_interspeech": {
   "authors": [
    [
     "Paul Van",
     "Eecke"
    ],
    [
     "Raquel",
     "Fernández"
    ]
   ],
   "title": "On the Influence of Gender on Interruptions in Multiparty Dialogue",
   "original": "0951",
   "page_count": 5,
   "order": 443,
   "p1": "2070",
   "pn": "2074",
   "abstract": [
    "During conversations, participants do not always alternate turns smoothly.\nOne cause of disturbance particularly prominent in multiparty dialogue\nis the presence of interruptions: interventions that prevent current\nspeakers from finishing their turns. Previous work, mostly within the\nfield of sociolinguistics, has suggested that the gender of the dialogue\nparticipants plays an important role in their interruptive behaviour.\nWe investigate existing hypotheses in this respect by systematically\nanalysing interruptions in a corpus of spoken multiparty meetings that\ninclude a minimum of two male and two female participants. We find\na number of significant differences, including the fact that women\nare more often interrupted overall and that men interrupt more often\nwomen than other men, in particular using speech overlap to grab the\nfloor. We do not find evidence for the hypothesis that women interrupt\nother women more frequently than they interrupt men.\n"
   ],
   "doi": "10.21437/Interspeech.2016-951"
  },
  "beaver16_interspeech": {
   "authors": [
    [
     "Ian",
     "Beaver"
    ],
    [
     "Cynthia",
     "Freeman"
    ]
   ],
   "title": "Detection of User Escalation in Human-Computer Interactions",
   "original": "0535",
   "page_count": 5,
   "order": 444,
   "p1": "2075",
   "pn": "2079",
   "abstract": [
    "Detection of virtual agent conversations where a user requests an alternative\nchannel for the completion of a task, known as an escalation request,\nis necessary for the improvement of language models and for a better\nuser experience. Although methods exist for proactive escalation, we\ninstead wish to explicitly detect escalation requests. In addition,\nthese proactive methods depend on features that do not correlate highly\nwith open-ended chats found in many modern virtual agents. We propose\na strategy that can apply to both bounded and open-ended systems since\nour method has no assumptions on the implementation of the underlying\nlanguage model. By combining classifiers with several conversation\nfeatures, we successfully detect escalation requests in real world\ndata.\n"
   ],
   "doi": "10.21437/Interspeech.2016-535"
  },
  "barnaud16_interspeech": {
   "authors": [
    [
     "Marie-Lou",
     "Barnaud"
    ],
    [
     "Julien",
     "Diard"
    ],
    [
     "Pierre",
     "Bessière"
    ],
    [
     "Jean-Luc",
     "Schwartz"
    ]
   ],
   "title": "Assessing Idiosyncrasies in a Bayesian Model of Speech Communication",
   "original": "0396",
   "page_count": 5,
   "order": 445,
   "p1": "2080",
   "pn": "2084",
   "abstract": [
    "Although speakers of one specific language share the same phoneme representations,\ntheir productions can differ. We propose to investigate the development\nof these differences in production, called idiosyncrasies, by using\na Bayesian model of communication. Supposing that idiosyncrasies appear\nduring the development of the motor system, we present two versions\nof the motor learning phase, both based on the guidance of an agent\nmaster: &#8220;a repetition model&#8221; where agents try to imitate\nthe  sounds produced by the master and &#8220;a communication model&#8221;\nwhere agents try to replicate the  phonemes produced by the master.\nOur experimental results show that only the &#8220;communication model&#8221;\nprovides production idiosyncrasies, suggesting that idiosyncrasies\nare a natural output of a motor learning process based on a communicative\ngoal.\n"
   ],
   "doi": "10.21437/Interspeech.2016-396"
  },
  "wolters16_interspeech": {
   "authors": [
    [
     "Maria K.",
     "Wolters"
    ],
    [
     "Najoung",
     "Kim"
    ],
    [
     "Jung-Ho",
     "Kim"
    ],
    [
     "Sarah E.",
     "MacPherson"
    ],
    [
     "Jong C.",
     "Park"
    ]
   ],
   "title": "Prosodic and Linguistic Analysis of Semantic Fluency Data: A Window into Speech Production and Cognition",
   "original": "0420",
   "page_count": 5,
   "order": 446,
   "p1": "2085",
   "pn": "2089",
   "abstract": [
    "Semantic fluency is a commonly used task in psychology that provides\ndata about executive function and semantic memory. Performance on the\ntask is affected by conditions ranging from depression to dementia.\nThe task involves participants naming as many members of a given category\n(e.g. animals) as possible in sixty seconds. Most of the analyses reported\nin the literature only rely on word counts and transcribed data, and\ndo not take into account the evidence of utterance planning present\nin the speech signal. Using data from Korean, we show how prosodic\nanalyses can be combined with computational linguistic analyses of\nthe words produced to provide further insights into the processes involved\nin producing fluency data. We compare our analyses to an established\nanalysis method for semantic fluency data, manual determination of\nlexically coherent clusters of words.\n"
   ],
   "doi": "10.21437/Interspeech.2016-420"
  },
  "katz16_interspeech": {
   "authors": [
    [
     "William F.",
     "Katz"
    ],
    [
     "Divya",
     "Prabhakaran"
    ]
   ],
   "title": "Sensorimotor Response to Visual Imagery of Tongue Displacement",
   "original": "1594",
   "page_count": 5,
   "order": 447,
   "p1": "2090",
   "pn": "2094",
   "abstract": [
    "To better understand audiovisual speech processing, we investigated\nthe effects of viewing time-synchronized videos of a 3D tongue avatar\non vowel production by healthy individuals. A group of 15 American\nEnglish-speaking subjects heard pink noise over headphones and produced\nthe word  head under four viewing conditions: First, while viewing\nrepetitions of the same vowel, /&#949;/ (baseline phase), then during\na series of &#8220;morphed&#8221; videos shifting gradually from /&#949;/\nto /&#230;/ (ramp phase), followed by repetitions of /&#230;/ (maximum\nhold phase), and finally repetitions of /&#949;/ (after effects phase).\nResults of a formant frequency (F1) analysis indicated that the visual\nmismatch phases (ramp and maximum hold) caused all subjects to align\ntheir productions to the visually-presented vowel, /&#230;/. No subjects\nreported being aware that their vowel quality had changed. We conclude\nthat the visual moving tongue stimuli produced entrainment to the viewed\nvowel category, rather than adaptation in the opposite direction of\nthe perturbation. Further experimentation is needed to determine whether\nthese effects are due to inherent imitation behaviors or subjects&#8217;\nlack of agency with the tongue avatar.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1594"
  },
  "caudrelier16_interspeech": {
   "authors": [
    [
     "Tiphaine",
     "Caudrelier"
    ],
    [
     "Pascal",
     "Perrier"
    ],
    [
     "Jean-Luc",
     "Schwartz"
    ],
    [
     "Amélie",
     "Rochet-Capellan"
    ]
   ],
   "title": "Does Auditory-Motor Learning of Speech Transfer from the CV Syllable to the CVCV Word?",
   "original": "0262",
   "page_count": 5,
   "order": 448,
   "p1": "2095",
   "pn": "2099",
   "abstract": [
    "Speech is often described as a sequence of units associating linguistic,\nsensory and motor representations. Is the connection between these\nrepresentations preferentially maintained at a specific level in terms\nof a linguistic unit? In the present study, we contrasted the possibility\nof a link at the level of the syllable (CV) and the word (CVCV). We\nmodified the production of the syllable /be/ in French speakers using\nan auditory-motor adaptation paradigm that consists of altering the\nspeakers&#8217; auditory feedback. After stopping the perturbation,\nwe studied to what extent this modification would transfer to the production\nof the disyllabic word /bebe/ and compared it to the after-effect on\n/be/.<br style=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  The results show that changes in /be/ transfer partially to /bebe/.\nThe partial influence of the somatosensory and motor representations\nassociated with the syllable on the production of the disyllabic word\nsuggests that both units may contribute to the specification of the\nmotor goals in speech sequences. In addition, the transfer occurs to\na larger extent in the first syllable of /bebe/ than in the second\none. It raises new questions about a possible interaction between the\ntransfer of auditory-motor learning and serial control processes.\n"
   ],
   "doi": "10.21437/Interspeech.2016-262"
  },
  "schweitzer16b_interspeech": {
   "authors": [
    [
     "Antje",
     "Schweitzer"
    ],
    [
     "Michael",
     "Walsh"
    ]
   ],
   "title": "Exemplar Dynamics in Phonetic Convergence of Speech Rate",
   "original": "0373",
   "page_count": 5,
   "order": 449,
   "p1": "2100",
   "pn": "2104",
   "abstract": [
    "We motivate and test an exemplar-theoretic view of phonetic convergence,\nin which convergence effects arise because exemplars just perceived\nin a conversation are stored in a speaker&#8217;s memory, and used\nsubsequently in speech production. Most exemplar models assume that\nproduction targets are established using stored exemplars, taking into\naccount their frequency- and recency-influenced level of activation.\nThus, convergence effects are expected to arise because the exemplars\njust perceived from a partner have a comparably high activation. However,\nin the case of frequent exemplars, this effect should be countered\nby the high frequency of already stored, older exemplars. We test this\nassumption by examining speech rate convergence in spontaneous speech\nby female German speakers. We fit two linear mixed models, calculating\nspeech rate on the basis of either infrequent, or frequent, syllables,\nand predict a speaker&#8217;s speech rate in a phrase by the partner&#8217;s\nspeech rate in the preceding phrase. As anticipated, we find a significant\nmain effect indicating convergence only for the infrequent syllables.\nWe also find an unexpected significant interaction of the partner&#8217;s\nspeech rate and the speaker&#8217;s assessment of the partner in terms\nof likeability, indicating divergence, but again, only for the infrequent\ncase.\n"
   ],
   "doi": "10.21437/Interspeech.2016-373"
  },
  "tuomainen16_interspeech": {
   "authors": [
    [
     "Outi",
     "Tuomainen"
    ],
    [
     "Valerie",
     "Hazan"
    ]
   ],
   "title": "Articulation Rate in Adverse Listening Conditions in Younger and Older Adults",
   "original": "0843",
   "page_count": 5,
   "order": 450,
   "p1": "2105",
   "pn": "2109",
   "abstract": [
    "Speech communication becomes increasingly difficult with age, especially\nin adverse listening conditions. We compared speech adaptations made\nby &#8216;older adult&#8217; (65&#8211;84 years) and &#8216;younger\nadult&#8217; (19&#8211;26 years) talkers when speech is produced with\ncommunicative intent. The aim was to investigate how articulation rate\nis affected by the type of adverse listening condition and by the change\nin task demands. Articulation rate was recorded in 35 older and 18\nyounger adult talkers when they were reading and repeating BKB-sentences\nand when they were doing an interactive &#8216;spot-the-difference&#8217;\ngame in a good and three adverse listening conditions (Hearing Loss\nSimulation, one speaker in noise, both speakers in noise). Similar\nto younger adults, older adults reduced their articulation rate in\nthe cognitively simpler sentence repetition task in response to adverse\nconditions. However, in spontaneous speech, only older adult women\ndecreased their articulation rate to counter the effect of the adverse\nconditions to the same degree as the younger adult talkers. Older men\ndid not reduce their articulation rate in any of the three adverse\nconditions. These sex differences were not due to differences in the\ntask difficulty experienced by men and women nor were they associated\nwith sensory or cognitive factors.\n"
   ],
   "doi": "10.21437/Interspeech.2016-843"
  },
  "olcoz16_interspeech": {
   "authors": [
    [
     "Julia",
     "Olcoz"
    ],
    [
     "Oscar",
     "Saz"
    ],
    [
     "Thomas",
     "Hain"
    ]
   ],
   "title": "Error Correction in Lightly Supervised Alignment of Broadcast Subtitles",
   "original": "0056",
   "page_count": 5,
   "order": 451,
   "p1": "2110",
   "pn": "2114",
   "abstract": [
    "This paper presents a range of error correction techniques aimed at\nimproving the accuracy of a lightly supervised alignment task for broadcast\nsubtitles. Lightly supervised approaches are frequently used in the\nmultimedia domain, either for subtitling purposes or for providing\na more reliable source for training speech-based systems. The proposed\nmethods focus on directly correcting of the alignment output using\ndifferent techniques to infer word insertions and words with inaccurate\ntime boundaries. The features used by the classification models are\nthe outputs from the alignment system, such as confidence measures,\nand word or segment duration. Experiments in this paper are based on\nbroadcast material provided by the BBC to the Multi-Genre Broadcast\n(MGB) challenge participants. Results, show that the order alignment\nF-measure improves up to 2.6% absolute (15.8% relative) when combining\ninsertion and word-boundary correction.\n"
   ],
   "doi": "10.21437/Interspeech.2016-56"
  },
  "doulaty16_interspeech": {
   "authors": [
    [
     "Mortaza",
     "Doulaty"
    ],
    [
     "Oscar",
     "Saz"
    ],
    [
     "Raymond W.M.",
     "Ng"
    ],
    [
     "Thomas",
     "Hain"
    ]
   ],
   "title": "Automatic Genre and Show Identification of Broadcast Media",
   "original": "0472",
   "page_count": 5,
   "order": 452,
   "p1": "2115",
   "pn": "2119",
   "abstract": [
    "Huge amounts of digital videos are being produced and broadcast every\nday, leading to giant media archives. Effective techniques are needed\nto make such data accessible further. Automatic meta-data labelling\nof broadcast media is an essential task for multimedia indexing, where\nit is standard to use multi-modal input for such purposes. This paper\ndescribes a novel method for automatic detection of media genre and\nshow identities using acoustic features, textual features or a combination\nthereof. Furthermore the inclusion of available meta-data, such as\ntime of broadcast, is shown to lead to very high performance. Latent\nDirichlet Allocation is used to model both acoustics and text, yielding\nfixed dimensional representations of media recordings that can then\nbe used in Support Vector Machines based classification. Experiments\nare conducted on more than 1200 hours of TV broadcasts from the British\nBroadcasting Corporation (BBC), where the task is to categorise the\nbroadcasts into 8 genres or 133 show identities. On a 200-hour test\nset, accuracies of 98.6% and 85.7% were achieved for genre and show\nidentification respectively, using a combination of acoustic and textual\nfeatures with meta-data.\n"
   ],
   "doi": "10.21437/Interspeech.2016-472"
  },
  "chao16_interspeech": {
   "authors": [
    [
     "Guan-Lin",
     "Chao"
    ],
    [
     "William",
     "Chan"
    ],
    [
     "Ian",
     "Lane"
    ]
   ],
   "title": "Speaker-Targeted Audio-Visual Models for Speech Recognition in Cocktail-Party Environments",
   "original": "0599",
   "page_count": 5,
   "order": 453,
   "p1": "2120",
   "pn": "2124",
   "abstract": [
    "Speech recognition in cocktail-party environments remains a significant\nchallenge for state-of-the-art speech recognition systems, as it is\nextremely difficult to extract an acoustic signal of an individual\nspeaker from a background of overlapping speech with similar frequency\nand temporal characteristics. We propose the use of speaker-targeted\nacoustic and audio-visual models for this task. We complement the acoustic\nfeatures in a hybrid DNN-HMM model with information of the target speaker&#8217;s\nidentity as well as visual features from the mouth region of the target\nspeaker. Experimentation was performed using simulated cocktail-party\ndata generated from the GRID audio-visual corpus by overlapping two\nspeakers&#8217;s speech on a single acoustic channel. Our audio-only\nbaseline achieved a WER of 26.3%. The audio-visual model improved the\nWER to 4.4%. Introducing speaker identity information had an even more\npronounced effect, improving the WER to 3.6%. Combining both approaches,\nhowever, did not significantly improve performance further. Our work\ndemonstrates that speaker-targeted models can significantly improve\nthe speech recognition in cocktail-party environments.\n"
   ],
   "doi": "10.21437/Interspeech.2016-599"
  },
  "aides16_interspeech": {
   "authors": [
    [
     "Amit",
     "Aides"
    ],
    [
     "Hagai",
     "Aronowitz"
    ]
   ],
   "title": "Text-Dependent Audiovisual Synchrony Detection for Spoofing Detection in Mobile Person Recognition",
   "original": "0196",
   "page_count": 5,
   "order": 454,
   "p1": "2125",
   "pn": "2129",
   "abstract": [
    "Liveness detection is an important countermeasure against spoofing\nattacks on biometric authentication systems. In the context of audiovisual\nbiometrics, synchrony detection is a proposed method for liveness confirmation.\nThis paper presents a novel, text-dependent scheme for checking audiovisual\nsynchronization in a video sequence. We present custom visual features\nlearned using a unique deep learning framework and show that they outperform\nother commonly used visual features. We tested our system on two testing\nsets representing realistic spoofing attack approaches. On our mobile\ndataset of short video clips of people talking, we obtained equal error\nrates of 0.8% and 2.7% for liveness detection of photos and video attacks,\nrespectively.\n"
   ],
   "doi": "10.21437/Interspeech.2016-196"
  },
  "tao16b_interspeech": {
   "authors": [
    [
     "Fei",
     "Tao"
    ],
    [
     "John H.L.",
     "Hansen"
    ],
    [
     "Carlos",
     "Busso"
    ]
   ],
   "title": "Improving Boundary Estimation in Audiovisual Speech Activity Detection Using Bayesian Information Criterion",
   "original": "0406",
   "page_count": 5,
   "order": 455,
   "p1": "2130",
   "pn": "2134",
   "abstract": [
    "A key preprocessing step in multimodal interfaces is to detect when\na user is speaking to the system. While push-to-talk approaches are\neffective, its use limits the flexibility of the system. Solutions\nbased on  speech activity detection (SAD) offer more intuitive and\nuser-friendly alternatives. A limitation in current SAD solutions is\nthe drop in performance observed in noisy environments or when the\nspeech mode differs from neutral speech (e.g., whisper speech). Emerging\naudiovisual solutions provide a principled framework to improve detection\nof speech boundaries by incorporating lip activity detection. In our\nprevious work, we proposed an unsupervised  visual speech activity\ndetection (V-SAD) system that combines temporal and dynamic facial\nfeatures. The key limitation of the system was the precise detection\nof boundaries between speech and non-speech regions due to anticipatory\nfacial movements and low video resolution (29.97fps). This study builds\nupon this system by (a) combining speech and facial features creating\nan unsupervised  audiovisual speech activity detection (AV-SAD) system,\n(b) refining the decision boundary with the  Bayesian information criterion\n(BIC) algorithm, resulting in improved speech boundary detection. The\nevaluation considers the challenging case of whisper speech, where\nthe proposed AV-SAD achieves a 10% absolute improvement over a state-of-the-art\naudio SAD.\n"
   ],
   "doi": "10.21437/Interspeech.2016-406"
  },
  "gergen16_interspeech": {
   "authors": [
    [
     "Sebastian",
     "Gergen"
    ],
    [
     "Steffen",
     "Zeiler"
    ],
    [
     "Ahmed Hussen",
     "Abdelaziz"
    ],
    [
     "Robert",
     "Nickel"
    ],
    [
     "Dorothea",
     "Kolossa"
    ]
   ],
   "title": "Dynamic Stream Weighting for Turbo-Decoding-Based Audiovisual ASR",
   "original": "0166",
   "page_count": 5,
   "order": 456,
   "p1": "2135",
   "pn": "2139",
   "abstract": [
    "Automatic speech recognition (ASR) enables very intuitive human-machine\ninteraction. However, signal degradations due to reverberation or noise\nreduce the accuracy of audio-based recognition. The introduction of\na second signal stream that is not affected by degradations in the\naudio domain (e.g., a video stream) increases the robustness of ASR\nagainst degradations in the original domain. Here, depending on the\nsignal quality of audio and video at each point in time, a dynamic\nweighting of both streams can optimize the recognition performance.\nIn this work, we introduce a strategy for estimating optimal weights\nfor the audio and video streams in turbo-decoding-based ASR using a\ndiscriminative cost function. The results show that turbo decoding\nwith this maximally discriminative dynamic weighting of information\nyields higher recognition accuracy than turbo-decoding-based recognition\nwith fixed stream weights or optimally dynamically weighted audiovisual\ndecoding using coupled hidden Markov models.\n"
   ],
   "doi": "10.21437/Interspeech.2016-166"
  },
  "kruspe16_interspeech": {
   "authors": [
    [
     "Anna M.",
     "Kruspe"
    ]
   ],
   "title": "Retrieval of Textual Song Lyrics from Sung Inputs",
   "original": "1272",
   "page_count": 5,
   "order": 457,
   "p1": "2140",
   "pn": "2144",
   "abstract": [
    "Retrieving the lyrics of a sung recording from a database of text documents\nis a research topic that has not received attention so far. Such a\nretrieval system has many practical applications, e.g. for karaoke\napplications or for indexing large song databases by their lyric content.<br\nstyle=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  In this paper, we present such a lyrics retrieval system. In a\nfirst step, phoneme posteriorgrams are extracted from sung recordings\nusing various acoustic models trained on  TIMIT and a variation thereof,\nand on subsets of a large database of recordings of unaccompanied singing\n( DAMP). On the other side, we generate binary templates from the available\ntextual lyrics. Since these lyrics do not have any temporal information,\nwe then employ an approach based on Dynamic Time Warping to retrieve\nthe most likely lyrics document for each recording.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  The approach is tested\non a different subset of the unaccompanied singing database which includes\n601 recordings of 301 different songs (12000 lines of lyrics). The\napproach is evaluated both on a song-wise and on a line-wise scale.<br\nstyle=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  The results are highly encouraging and could be used further to\nperform automatic lyrics alignment and keyword spotting for large databases\nof songs.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1272"
  },
  "yuan16b_interspeech": {
   "authors": [
    [
     "Jiahong",
     "Yuan"
    ],
    [
     "Mark",
     "Liberman"
    ]
   ],
   "title": "Phoneme, Phone Boundary, and Tone in Automatic Scoring of Mandarin Proficiency",
   "original": "0510",
   "page_count": 5,
   "order": 458,
   "p1": "2145",
   "pn": "2149",
   "abstract": [
    "Not every phone, word, or sentence is equally good for assessing language\nproficiency. We investigated three phonetic factors that may affect\nautomatic scoring of Mandarin proficiency &#8212; phoneme, phone boundary,\nand tone. Results showed that phone boundaries performed the best,\nand within-syllable boundaries were better than cross-syllable boundaries.\nThe retroflex consonants as well as the vowel following these consonants\noutperformed the other phonemes. Tone0 and Tone3 outperformed the other\ntones, and ditone models significantly improved the performance of\nTone0. These results suggest that phone boundary models and phoneme-\nand tone- dependent scoring algorithms should be employed in automatic\nassessment of Mandarin proficiency. It may also be helpful to separate\nphoneme and tone scoring prior to the combination of individual scores,\nas we found that the worst phoneme and the best tone, with respect\nto automatic scoring of Mandarin proficiency, appeared in the same\nword.\n"
   ],
   "doi": "10.21437/Interspeech.2016-510"
  },
  "chen16k_interspeech": {
   "authors": [
    [
     "Charles",
     "Chen"
    ],
    [
     "Razvan",
     "Bunescu"
    ],
    [
     "Li",
     "Xu"
    ],
    [
     "Chang",
     "Liu"
    ]
   ],
   "title": "Tone Classification in Mandarin Chinese Using Convolutional Neural Networks",
   "original": "0528",
   "page_count": 5,
   "order": 459,
   "p1": "2150",
   "pn": "2154",
   "abstract": [
    "In tone languages, different tone patterns of the same syllable may\nconvey different meanings. Tone perception is important for sentence\nrecognition in noise conditions, especially for children with cochlear\nimplants (CI). We propose a method that fully automates tone classification\nof syllables in Mandarin Chinese. Our model takes as input the raw\ntone data and uses convolutional neural networks to classify syllables\ninto one of the four tones in Mandarin. When evaluated on syllables\nrecorded from normal-hearing children, our method achieves substantially\nhigher accuracy compared with previous tone classification techniques\nbased on manually edited F<SUB>0</SUB>. The new approach is also more\nefficient, as it does not require manual checking of F<SUB>0</SUB>.\nThe new tone classification system could have significant clinical\napplications in the speech evaluation of the hearing impaired population.\n"
   ],
   "doi": "10.21437/Interspeech.2016-528"
  },
  "pannala16_interspeech": {
   "authors": [
    [
     "Vishala",
     "Pannala"
    ],
    [
     "G.",
     "Aneeja"
    ],
    [
     "Sudarsana Reddy",
     "Kadiri"
    ],
    [
     "B.",
     "Yegnanarayana"
    ]
   ],
   "title": "Robust Estimation of Fundamental Frequency Using Single Frequency Filtering Approach",
   "original": "1401",
   "page_count": 5,
   "order": 460,
   "p1": "2155",
   "pn": "2159",
   "abstract": [
    "A new method for robust estimation of fundamental frequency (F<SUB>0</SUB>)\nfrom speech signal is proposed in this paper. The method exploits the\nhigh SNR regions of speech in time and frequency domains in the outputs\nof single frequency filtering (SFF) of speech signal. The high resolution\nin the frequency domain brings out the harmonic characteristics of\nspeech clearly. The harmonic spacing in the high SNR regions of spectrum\ndetermine the F<SUB>0</SUB>. The concept of root cepstrum is used to\nreduce the effects of vocal tract resonances in the F<SUB>0</SUB> estimation.\nThe proposed method is evaluated for clean speech and noisy speech\nsimulated for 15 different degradations at different noise levels.\nPerformance of the proposed method is compared with four other standard\nmethods of F<SUB>0</SUB> extraction. From the results it is evident\nthat the proposed method is robust for most types of degradations.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1401"
  },
  "daido16_interspeech": {
   "authors": [
    [
     "Ryunosuke",
     "Daido"
    ],
    [
     "Yuji",
     "Hisaminato"
    ]
   ],
   "title": "A Fast and Accurate Fundamental Frequency Estimator Using Recursive Moving Average Filters",
   "original": "0394",
   "page_count": 5,
   "order": 461,
   "p1": "2160",
   "pn": "2164",
   "abstract": [
    "We propose a fundamental frequency (F0) estimation method which is\nfast, accurate and suitable for real-time use. While the proposed method\nis based on the same framework as DIO [1, 2], it has two clear differences:\nit uses RMA (Recursive Moving Average) filters for attenuating high\norder harmonics, and the period detector is designed to work well even\nfor signals which contain some higher harmonics. Effect of trace-back\nduration of post-processing was also examined. Evaluation experiments\nusing natural speech databases showed that the accuracy of the proposed\nmethod was better than DIO, SWIPE&#39; [3] and YIN [4] and computation\nspeed was the fastest compared to those existing methods.\n"
   ],
   "doi": "10.21437/Interspeech.2016-394"
  },
  "verma16_interspeech": {
   "authors": [
    [
     "Prateek",
     "Verma"
    ],
    [
     "Ronald W.",
     "Schafer"
    ]
   ],
   "title": "Frequency Estimation from Waveforms Using Multi-Layered Neural Networks",
   "original": "0679",
   "page_count": 5,
   "order": 462,
   "p1": "2165",
   "pn": "2169",
   "abstract": [
    "For frequency estimation in noisy speech or music signals, time domain\nmethods based on signal processing techniques such as autocorrelation\nor average magnitude difference, often do not perform well. As deep\nneural networks (DNNs) have become feasible, some researchers have\nattempted with some success to improve the performance of signal processing\nbased methods by learning on autocorrelation, Fourier transform or\nconstant-Q filter bank based representations. In our approach, blocks\nof signal samples are input  directly to a neural network to perform\nend to end learning. The emergence of sub-harmonic structure in the\nposterior vector of the output layer, along with analysis of the filter-like\nstructures emerging in the DNN shows strong correlations with some\nsignal processing based approaches. These NNs appear to learn a nonlinearly-spaced\nfrequency representation in the first layer followed by comb-like filters.\nWe find that learning representations from raw time-domain signals\ncan achieve performance on par with the current state of the art algorithms\nfor frequency estimation in noisy and polyphonic settings. The emergence\nof sub-harmonic structure in the posterior vector suggests that existing\npost-processing techniques such as harmonic product spectra and salience\nmapping may further improve the performance.\n"
   ],
   "doi": "10.21437/Interspeech.2016-679"
  },
  "sturim16_interspeech": {
   "authors": [
    [
     "Douglas E.",
     "Sturim"
    ],
    [
     "William M.",
     "Campbell"
    ]
   ],
   "title": "Speaker Linking and Applications Using Non-Parametric Hashing Methods",
   "original": "0468",
   "page_count": 5,
   "order": 463,
   "p1": "2170",
   "pn": "2174",
   "abstract": [
    "Large unstructured audio data sets have become ubiquitous and present\na challenge for organization and search. One logical approach for structuring\ndata is to find common speakers and link occurrences across different\nrecordings. Prior approaches to this problem have focused on basic\nmethodology for the linking task. In this paper, we introduce a novel\ntrainable non-parametric hashing method for indexing large speaker\nrecording data sets. This approach leads to tunable computational complexity\nmethods for speaker linking. We focus on a scalable clustering method\nbased on hashing &#8212; canopy-clustering. We apply this method to\na large corpus of speaker recordings, demonstrate performance tradeoffs,\nand compare to other hashing methods.\n"
   ],
   "doi": "10.21437/Interspeech.2016-468"
  },
  "lan16_interspeech": {
   "authors": [
    [
     "Gaël Le",
     "Lan"
    ],
    [
     "Delphine",
     "Charlet"
    ],
    [
     "Anthony",
     "Larcher"
    ],
    [
     "Sylvain",
     "Meignier"
    ]
   ],
   "title": "Iterative PLDA Adaptation for Speaker Diarization",
   "original": "0572",
   "page_count": 5,
   "order": 464,
   "p1": "2175",
   "pn": "2179",
   "abstract": [
    "This paper investigates iterative PLDA adaptation for cross-show speaker\ndiarization applied to small collections of French TV archives based\non an i-vector framework. Using the target collection itself for unsupervised\nadaptation, PLDA parameters are iteratively tuned while score normalization\nis applied for convergence. Performances are compared, using combinations\nof target and external data for training and adaptation. The experiments\non two distinct target corpora show that the proposed framework can\ngradually improve an existing system trained on external annotated\ndata. Such results indicate that performing speaker diarization on\nsmall collections of unlabeled audio archives should only rely on the\navailability of a sufficient bootstrap system, which can be incrementally\nadapted to every target collection. The proposed framework also widens\nthe range of acceptable speaker clustering thresholds for a given performance\nobjective.\n"
   ],
   "doi": "10.21437/Interspeech.2016-572"
  },
  "dubey16_interspeech": {
   "authors": [
    [
     "Harishchandra",
     "Dubey"
    ],
    [
     "Lakshmish",
     "Kaushik"
    ],
    [
     "Abhijeet",
     "Sangwan"
    ],
    [
     "John H.L.",
     "Hansen"
    ]
   ],
   "title": "A Speaker Diarization System for Studying Peer-Led Team Learning Groups",
   "original": "1497",
   "page_count": 5,
   "order": 465,
   "p1": "2180",
   "pn": "2184",
   "abstract": [
    "Peer-led team learning (PLTL) is a model for teaching STEM courses\nwhere small student groups meet periodically to collaboratively discuss\ncoursework. Automatic analysis of PLTL sessions would help education\nresearchers to get insight into how learning outcomes are impacted\nby individual participation, group behavior, team dynamics,  etc..\nTowards this, speech and language technology can help, and speaker\ndiarization technology will lay the foundation for analysis. In this\nstudy, a new corpus is established called CRSS-PLTL, that contains\nspeech data from 5 PLTL teams over a semester (10 sessions per team\nwith 5-to-8 participants in each team). In CRSS-PLTL, every participant\nwears a LENA device (portable audio recorder) that provides multiple\naudio recordings of the event. Our proposed solution is unsupervised\nand contains a new online speaker change detection algorithm, termed\nG<SUP>3</SUP> algorithm in conjunction with Hausdorff-distance based\nclustering to provide improved detection accuracy. Additionally, we\nalso exploit cross channel information to refine our diarization hypothesis.\nThe proposed system provides good improvements in diarization error\nrate (DER) over the baseline LIUM system. We also present higher level\nanalysis such as the number of conversational turns taken in a session,\nand speaking-time duration (participation) for each speaker.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1497"
  },
  "milner16_interspeech": {
   "authors": [
    [
     "Rosanna",
     "Milner"
    ],
    [
     "Thomas",
     "Hain"
    ]
   ],
   "title": "DNN-Based Speaker Clustering for Speaker Diarisation",
   "original": "0126",
   "page_count": 5,
   "order": 466,
   "p1": "2185",
   "pn": "2189",
   "abstract": [
    "Speaker diarisation, the task of answering &#8220;who spoke when?&#8221;,\nis often considered to consist of three independent stages: speech\nactivity detection, speaker segmentation and speaker clustering. These\nrepresent the separation of speech and non-speech, the splitting into\nspeaker homogeneous speech segments, followed by grouping together\nthose which belong to the same speaker. This paper is concerned with\nspeaker clustering, which is typically performed by bottom-up clustering\nusing the Bayesian information criterion (BIC). We present a novel\nsemi-supervised method of speaker clustering based on a deep neural\nnetwork (DNN) model. A speaker separation DNN trained on independent\ndata is used to iteratively relabel the test data set. This is achieved\nby reconfiguration of the output layer, combined with fine tuning in\neach iteration. A stopping criterion involving posteriors as confidence\nscores is investigated. Results are shown on a meeting task (RT07)\nfor single distant microphones and compared with standard diarisation\napproaches. The new method achieves a diarisation error rate (DER)\nof 14.8%, compared to a baseline of 19.9%.\n"
   ],
   "doi": "10.21437/Interspeech.2016-126"
  },
  "lapidot16_interspeech": {
   "authors": [
    [
     "Itshak",
     "Lapidot"
    ],
    [
     "Jean-François",
     "Bonastre"
    ]
   ],
   "title": "On the Importance of Efficient Transition Modeling for Speaker Diarization",
   "original": "0503",
   "page_count": 4,
   "order": 467,
   "p1": "2190",
   "pn": "2193",
   "abstract": [
    "In recent years speaker diarization becomes an important issue. In\nprevious works, we presented the Hidden Distortion Model (HDM) approach,\nin order to overcome the limitations of traditional HMMs in terms of\nemission and transition modeling. In this work, we show that HDM allows\nto build more efficient speaker diarization systems both in terms of\ndiarization error rated and in terms of memory footprint. The best\ndiarization performance is obtained using smaller than usual emission\nmodels which constitutes potentially a key advantage for embedded applications\nwith limited memory resources and computational power. A significant\nmemory size reduction was observed using LDC CALLHOME (American) for\nboth SOM- and GMM-based emission probability models.\n"
   ],
   "doi": "10.21437/Interspeech.2016-503"
  },
  "sell16_interspeech": {
   "authors": [
    [
     "Gregory",
     "Sell"
    ],
    [
     "Alan",
     "McCree"
    ],
    [
     "Daniel",
     "Garcia-Romero"
    ]
   ],
   "title": "Priors for Speaker Counting and Diarization with AHC",
   "original": "1380",
   "page_count": 5,
   "order": 468,
   "p1": "2194",
   "pn": "2198",
   "abstract": [
    "Estimating the number of speakers in an audio segment is a necessary\nstep in the process of speaker diarization, but current diarization\nalgorithms do not explicitly define a prior probability on this estimation.\nThis work proposes a process for including priors in speaker diarization\nwith agglomerative hierarchical clustering (AHC). It is also shown\nthat the exclusion of a prior with AHC is itself implicitly a prior,\nwhich is found to be geometric growth in the number of speakers. By\nusing more sensible priors, we are able to demonstrate significantly\nimproved robustness to calibration error for speaker counting and speaker\ndiarization.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1380"
  },
  "dawalatabad16_interspeech": {
   "authors": [
    [
     "Nauman",
     "Dawalatabad"
    ],
    [
     "Srikanth",
     "Madikeri"
    ],
    [
     "C Chandra",
     "Sekhar"
    ],
    [
     "Hema A.",
     "Murthy"
    ]
   ],
   "title": "Two-Pass IB Based Speaker Diarization System Using Meeting-Specific ANN Based Features",
   "original": "0714",
   "page_count": 5,
   "order": 469,
   "p1": "2199",
   "pn": "2203",
   "abstract": [
    "In this paper, we present a two-pass Information Bottleneck (IB) based\nsystem for speaker diarization which uses meeting-specific artificial\nneural network (ANN) based features. We first use IB based speaker\ndiarization system to get the labelled speaker segments. These segments\nare re-segmented using Kullback-Leibler Hidden Markov Model (KL-HMM)\nbased re-segmentation. The multi-layer ANN is then trained to discriminate\nthese speakers using the re-segmented output labels and the spectral\nfeatures. We then extract the bottleneck features from the trained\nANN and perform principal component analysis (PCA) on these features.\nAfter performing PCA, these bottleneck features are used along with\nthe different spectral features in the second pass using the same IB\nbased system with KL-HMM re-segmentation. Our experiments on NIST RT\nand AMI datasets show that the proposed system performs better than\nthe baseline IB system in terms of speaker error rate (SER) with a\nbest case relative improvement of 28.6% amongst AMI datasets and 27.1%\non NIST RT04eval dataset.\n"
   ],
   "doi": "10.21437/Interspeech.2016-714"
  },
  "oo16_interspeech": {
   "authors": [
    [
     "Zeyan",
     "Oo"
    ],
    [
     "Yuta",
     "Kawakami"
    ],
    [
     "Longbiao",
     "Wang"
    ],
    [
     "Seiichi",
     "Nakagawa"
    ],
    [
     "Xiong",
     "Xiao"
    ],
    [
     "Masahiro",
     "Iwahashi"
    ]
   ],
   "title": "DNN-Based Amplitude and Phase Feature Enhancement for Noise Robust Speaker Identification",
   "original": "0717",
   "page_count": 5,
   "order": 470,
   "p1": "2204",
   "pn": "2208",
   "abstract": [
    "The importance of the phase information of speech signal is gathering\nattention. Many researches indicate system combination of the amplitude\nand phase features is effective for improving speaker recognition performance\nunder noisy environments. On the other hand, speech enhancement approach\nis taken usually to reduce the influence of noises. However, this approach\nonly enhances the amplitude spectrum, therefore noisy phase spectrum\nis used for reconstructing the estimated signal. Recent years, DNN\nbased feature enhancement is studied intensively for robust speech\nprocessing. This approach is expected to be effective also for phase-based\nfeature. In this paper, we propose feature space enhancement of amplitude\nand phase features using deep neural network (DNN) for speaker identification.\nWe used mel-frequency cepstral coefficients as an amplitude feature,\nand modified group delay cepstral coefficients as a phase feature.\nSimultaneous enhancement of amplitude and phase based feature was effective,\nand it achieved about 24% relative error reduction comparing with individual\nfeature enhancement.\n"
   ],
   "doi": "10.21437/Interspeech.2016-717"
  },
  "scherhag16_interspeech": {
   "authors": [
    [
     "Ulrich",
     "Scherhag"
    ],
    [
     "Andreas",
     "Nautsch"
    ],
    [
     "Christian",
     "Rathgeb"
    ],
    [
     "Christoph",
     "Busch"
    ]
   ],
   "title": "Unit-Selection Attack Detection Based on Unfiltered Frequency-Domain Features",
   "original": "0969",
   "page_count": 5,
   "order": 471,
   "p1": "2209",
   "pn": "2213",
   "abstract": [
    "Modern text-to-speech algorithms pose a vital threat to the security\nof speaker identification and verification (SIV) systems, in terms\nof subversive usage, i.e. generating presentation attacks. In order\nto distinguish between presentation attacks and bona fide authentication\nattempts, presentation attack detection (PAD) subsystems are of utmost\nimportance. Until now, the vast majority of introduced spoofing countermeasures\nrely on speech production and perception based features. In this paper,\nwe utilize the complete frequency band without further filter-bank\nprocessing in order to detect non-smooth transitions in the full and\nhigh frequency domain caused by unit-selection attacks. For the purpose\nof especially detecting unit selection attacks, the applicability of\nFast Fourier Transformation (FFT) and Discrete Wavelet Transformation\n(DWT) is examined regarding non-smooth transitions in the full and\nhigh frequency domain, excluding filter-bank analyses. Gaussian Mixture\nModel (GMM) and Support Vector Machine (SVM) classifiers are trained\non the German Speech Data Corpus (GSDC) and validated on the standard\nASVspoof 2015 corpus resulting in EERs of 7.1% and 11.7%, respectively.\nDespite language and data shifts, the proposed unit-selection PAD scheme\nachieves promising biometric performance and hence, introduces a new\ndirection to voice PAD.\n"
   ],
   "doi": "10.21437/Interspeech.2016-969"
  },
  "monteserin16_interspeech": {
   "authors": [
    [
     "Mairym Lloréns",
     "Monteserín"
    ],
    [
     "Jason",
     "Zevin"
    ]
   ],
   "title": "Investigating the Impact of Dialect Prestige on Lexical Decision",
   "original": "1549",
   "page_count": 5,
   "order": 472,
   "p1": "2214",
   "pn": "2218",
   "abstract": [
    "The speech signal encodes both a talker&#8217;s message and indexical\ninformation about a talker&#8217;s identity. Dialectal variation is\none way in which non-linguistic information about a talker is conveyed\nthrough her speech. A talker&#8217;s dialect tends to correlate strongly\nwith her demographic background, and listeners are known to form beliefs\nabout speakers based on their dialect alone: talkers of lower-status\ndialects are consistently downgraded on positively-valued attributes\nrelative to talkers of canonical dialects. Hypothesizing that pre-formed\nbeliefs about a low-status talker might impact optimal perception of\nher speech, this study investigated the influence of the relative prestige\nof talker dialect on listeners&#8217; behavior in three lexical decision\nexperiments. The finding of significantly increased propensity to incorrectly\nreject words uttered in an arguably low-prestige variety of American\nEnglish relative to both normative General American English and British\nEnglish suggests that talker status may play a role in the success\nwith which talker messages are perceived by listeners. These results\nas well as unexpected interactions of dialect and word frequency in\nsome but not all experiments are discussed in the context of signal\ndetection theory.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1549"
  },
  "guo16b_interspeech": {
   "authors": [
    [
     "Jinxi",
     "Guo"
    ],
    [
     "Gary",
     "Yeung"
    ],
    [
     "Deepak",
     "Muralidharan"
    ],
    [
     "Harish",
     "Arsikere"
    ],
    [
     "Amber",
     "Afshan"
    ],
    [
     "Abeer",
     "Alwan"
    ]
   ],
   "title": "Speaker Verification Using Short Utterances with DNN-Based Estimation of Subglottal Acoustic Features",
   "original": "0282",
   "page_count": 4,
   "order": 473,
   "p1": "2219",
   "pn": "2222",
   "abstract": [
    "Speaker verification in real-world applications sometimes deals with\nlimited duration of enrollment and/or test data. MFCC-based i-vector\nsystems have defined the state-of-the-art for speaker verification,\nbut it is well known that they are less effective with short utterances.\nTo address this issue, we propose a method to leverage the speaker\nspecificity and stationarity of subglottal acoustics. First, we present\na deep neural network (DNN) based approach to estimate subglottal features\nfrom speech signals. The approach involves training a DNN-regression\nmodel that maps the log filter-bank coefficients of a given speech\nsignal to those of its corresponding subglottal signal. Cross-validation\nexperiments on the WashU-UCLA corpus (which contains parallel recordings\nof speech and subglottal acoustics) show the effectiveness of our DNN-based\nestimation algorithm. The average correlation coefficient between the\nactual and estimated subglottal filter-bank coefficients is 0.9. A\nscore-level fusion of MFCC and subglottal-feature systems in the i-vector\nPLDA framework yields statistically-significant improvements over the\nMFCC-only baseline. On the NIST SRE 08 truncated 10sec&#8211;10sec\nand 5sec&#8211;5sec core evaluation tasks, the relative reduction in\nequal error rate ranges between 6 and 14% for the conditions tested\nwith both microphone and telephone speech.\n"
   ],
   "doi": "10.21437/Interspeech.2016-282"
  },
  "su16_interspeech": {
   "authors": [
    [
     "Hang",
     "Su"
    ],
    [
     "Steven",
     "Wegmann"
    ]
   ],
   "title": "Factor Analysis Based Speaker Verification Using ASR",
   "original": "1157",
   "page_count": 5,
   "order": 474,
   "p1": "2223",
   "pn": "2227",
   "abstract": [
    "In this paper, we propose to improve speaker verification performance\nby importing better posterior statistics from acoustic models trained\nfor Automatic Speech Recognition (ASR). This approach aims to introduce\nstate-of-the-art techniques in ASR to speaker verification task. We\ncompare statistics collected from several ASR systems, and show that\nthose collected from deep neural networks (DNN) trained with fMLLR\nfeatures can effectively reduce equal error rate (EER) by more than\n30% on NIST SRE 2010 task, compared with those DNN trained without\nfeature transformations. We also present derivation of factor analysis\nusing variational Bayes inference, and illustrate implementation details\nof factor analysis and probabilistic linear discriminant analysis (PLDA)\nin Kaldi recognition toolkit.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1157"
  },
  "zegers16_interspeech": {
   "authors": [
    [
     "Jeroen",
     "Zegers"
    ],
    [
     "Hugo",
     "Van hamme"
    ]
   ],
   "title": "Joint Sound Source Separation and Speaker Recognition",
   "original": "0773",
   "page_count": 5,
   "order": 475,
   "p1": "2228",
   "pn": "2232",
   "abstract": [
    "Non-negative Matrix Factorization (NMF) has already been applied to\nlearn speaker characterizations from single or non-simultaneous speech\nfor speaker recognition applications. It is also known for its good\nperformance in (blind) source separation for simultaneous speech. This\npaper explains how NMF can be used to jointly solve the two problems\nin a multichannel speaker recognizer for simultaneous speech. It is\nshown how state-of-the-art multichannel NMF for blind source separation\ncan be easily extended to incorporate speaker recognition. Experiments\non the CHiME corpus show that this method outperforms the sequential\napproach of first applying source separation, followed by speaker recognition\nthat uses state-of-the-art i-vector techniques.\n"
   ],
   "doi": "10.21437/Interspeech.2016-773"
  },
  "kumar16_interspeech": {
   "authors": [
    [
     "Naveen",
     "Kumar"
    ],
    [
     "Md.",
     "Nasir"
    ],
    [
     "Panayiotis",
     "Georgiou"
    ],
    [
     "Shrikanth S.",
     "Narayanan"
    ]
   ],
   "title": "Robust Multichannel Gender Classification from Speech in Movie Audio",
   "original": "0540",
   "page_count": 5,
   "order": 476,
   "p1": "2233",
   "pn": "2237",
   "abstract": [
    "Speech in the form of scripted dialogues forms an important part of\nthe audio signal in movies. However, it is often masked by background\naudio signals such as music, ambient noise or background chatter. These\nbackground sounds make even otherwise simple tasks, such as gender\nclassification, challenging. Additionally, the variability in this\nnoise across movies renders standard approaches to source separation\nor enhancement inadequate. Instead, we exploit multichannel information\npresent in different language channels (English, Spanish, French) for\neach movie to improve the robustness of our gender classification system.\nWe exploit the fact that the speaker labels of interest in this case\nco-occur in each language channel. We fuse the predictions obtained\nfor each channel using Recognition Output Voting Error Reduction (ROVER)\nand show that this approach improves the gender accuracy by 7% absolute\n(11% relative) compared to the best independent prediction on any single\nchannel. In the case of surround movies, we further investigate fusion\nof mono audio and front center channels which shows 5% and 3% absolute\n(8% and 4% relative) increase in accuracy compared to only using mono\nand front center channel, respectively.\n"
   ],
   "doi": "10.21437/Interspeech.2016-540"
  },
  "gonzalvo16_interspeech": {
   "authors": [
    [
     "Xavi",
     "Gonzalvo"
    ],
    [
     "Siamak",
     "Tazari"
    ],
    [
     "Chun-an",
     "Chan"
    ],
    [
     "Markus",
     "Becker"
    ],
    [
     "Alexander",
     "Gutkin"
    ],
    [
     "Hanna",
     "Silen"
    ]
   ],
   "title": "Recent Advances in Google Real-Time HMM-Driven Unit Selection Synthesizer",
   "original": "0264",
   "page_count": 5,
   "order": 477,
   "p1": "2238",
   "pn": "2242",
   "abstract": [
    "This paper presents advances in Google&#8217;s hidden Markov model\n(HMM)-driven unit selection speech synthesis system. We describe several\nimprovements to the run-time system; these include minimal latency,\nhigh-quality and fast refresh cycle for new voices. Traditionally unit\nselection synthesizers are limited in terms of the amount of data they\ncan handle and the real applications they are built for. That is even\nmore critical for real-life large-scale applications where high-quality\nis expected and low latency is required given the available computational\nresources. In this paper we present an optimized engine to handle a\nlarge database at runtime, a composite unit search approach for combining\ndiphones and phrase-based units. In addition a new voice building strategy\nfor handling big databases and keeping the building times low is presented.\n"
   ],
   "doi": "10.21437/Interspeech.2016-264"
  },
  "wang16e_interspeech": {
   "authors": [
    [
     "Wenfu",
     "Wang"
    ],
    [
     "Shuang",
     "Xu"
    ],
    [
     "Bo",
     "Xu"
    ]
   ],
   "title": "First Step Towards End-to-End Parametric TTS Synthesis: Generating Spectral Parameters with Neural Attention",
   "original": "0134",
   "page_count": 5,
   "order": 478,
   "p1": "2243",
   "pn": "2247",
   "abstract": [
    "In conventional neural networks (NN) based parametric text-to-speech\n(TTS) synthesis frameworks, text analysis and acoustic modeling are\ntypically processed separately, leading to some limitations. On one\nhand, much significant human expertise is normally required in text\nanalysis, which presents a laborious task for researchers; on the other\nhand, training of the NN-based acoustic models still relies on the\nhidden Markov model (HMM) to obtain frame-level alignments. This acquisition\nprocess normally goes through multiple complicated stages. The complex\npipeline makes constructing a NN-based parametric TTS system a challenging\ntask. This paper attempts to bypass these limitations using a novel\nend-to-end parametric TTS synthesis framework, i.e. the text analysis\nand acoustic modeling are integrated together employing an attention-based\nrecurrent neural network. Thus the alignments can be learned automatically.\nPreliminary experimental results show that the proposed system can\ngenerate moderately smooth spectral parameters and synthesize fairly\nintelligible speech on short utterances (less than 8 Chinese characters).\n"
   ],
   "doi": "10.21437/Interspeech.2016-134"
  },
  "wen16b_interspeech": {
   "authors": [
    [
     "Zhengqi",
     "Wen"
    ],
    [
     "Ya",
     "Li"
    ],
    [
     "Jianhua",
     "Tao"
    ]
   ],
   "title": "The Parameterized Phoneme Identity Feature as a Continuous Real-Valued Vector for Neural Network Based Speech Synthesis",
   "original": "0222",
   "page_count": 5,
   "order": 479,
   "p1": "2248",
   "pn": "2252",
   "abstract": [
    "In the speech synthesis systems, the phoneme identity feature indicated\nas the pronunciation unit is influenced by external contexts like the\nneighboring words and phonemes. This paper proposes to encode such\nrelatedness and parameterize the pronunciation of the phoneme identity\nfeature as a continuous real-valued vector. The vector, composed by\na phoneme embedded vector (PEV) and a word embedded vector (WEV), is\napplied to substitute the binary vector whose representation is one-hot.\nIt is realized in the word embedding model with the joint training\nstructure where the PEV and WEV are learned together. The effectiveness\nof the proposed technique was evaluated by comparing it with the binary\nvector in the bidirectional long short term memory recurrent neural\nnetwork (BLSTM-RNN) based speech synthesis systems. Improvement on\nthe quality of the synthesized speech has been achieved from the proposed\nsystem, which proves the effectiveness of replacing the binary vector\nwith the continuous real-valued vector in describing the phoneme identity\nfeature.\n"
   ],
   "doi": "10.21437/Interspeech.2016-222"
  },
  "song16_interspeech": {
   "authors": [
    [
     "Eunwoo",
     "Song"
    ],
    [
     "Frank K.",
     "Soong"
    ],
    [
     "Hong-Goo",
     "Kang"
    ]
   ],
   "title": "Improved Time-Frequency Trajectory Excitation Vocoder for DNN-Based Speech Synthesis",
   "original": "0230",
   "page_count": 5,
   "order": 480,
   "p1": "2253",
   "pn": "2257",
   "abstract": [
    "We investigate an improved time-frequency trajectory excitation (ITFTE)\nvocoder for deep neural network (DNN)-based statistical parametric\nspeech synthesis (SPSS) systems. The ITFTE is a linear predictive coding-based\nvocoder, where a pitch-dependent excitation signal is represented by\na periodicity distribution in a time-frequency domain. The proposed\nmethod significantly improves the parameterization efficiency of ITFTE\nvocoder for the DNN-based SPSS system, even if its dimension changes\ndue to the inherent nature of pitch variation. By utilizing an orthogonality\nproperty of discrete cosine transform, we not only accurately reconstruct\nthe ITFTE parameters but also improve the perceptual quality of synthesized\nspeech. Objective and subjective test results confirm that the proposed\nmethod provides superior synthesized speech compared to the previous\nsystem.\n"
   ],
   "doi": "10.21437/Interspeech.2016-230"
  },
  "ohtani16_interspeech": {
   "authors": [
    [
     "Yamato",
     "Ohtani"
    ],
    [
     "Koichiro",
     "Mori"
    ],
    [
     "Masahiro",
     "Morita"
    ]
   ],
   "title": "Voice Quality Control Using Perceptual Expressions for Statistical Parametric Speech Synthesis Based on Cluster Adaptive Training",
   "original": "0290",
   "page_count": 5,
   "order": 481,
   "p1": "2258",
   "pn": "2262",
   "abstract": [
    "This paper describes novel voice quality control of synthetic speech\nusing cluster adaptive training (CAT). In this method, we model voice\nquality factors labeled with perceptual expressions such as &#8220;Gender,&#8221;\n&#8220;Age&#8221; and &#8220;Brightness.&#8221; In advance, we obtain\nthe intensity scores of the perceptual expressions by conducting a\nlistening test, which evaluates differences of voice qualities between\nsynthetic speech of average voice and that of the target. Then we build\nperceptual expression (PE) clusters that we call PE models (PEM) under\nthe conditions that the average voice model is used as the bias cluster\nand the PE intensity scores are employed as the CAT weights. In synthesis,\nwe can generate controlled synthetic speech by the linear combination\nof PEMs and the existing speaker&#8217;s model. Subjective results\ndemonstrate that the proposed method can control the voice qualities\nwith PEs in many cases and the target synthetic speech modified by\nPEMs achieves comparatively good speech quality.\n"
   ],
   "doi": "10.21437/Interspeech.2016-290"
  },
  "espic16_interspeech": {
   "authors": [
    [
     "Felipe",
     "Espic"
    ],
    [
     "Cassia",
     "Valentini-Botinhao"
    ],
    [
     "Zhizheng",
     "Wu"
    ],
    [
     "Simon",
     "King"
    ]
   ],
   "title": "Waveform Generation Based on Signal Reshaping for Statistical Parametric Speech Synthesis",
   "original": "0487",
   "page_count": 5,
   "order": 482,
   "p1": "2263",
   "pn": "2267",
   "abstract": [
    "We propose a new paradigm of waveform generation for Statistical Parametric\nSpeech Synthesis that is based on neither source-filter separation\nnor sinusoidal modelling. We suggest that one of the main problems\nof current vocoding techniques is that they perform an extreme decomposition\nof the speech signal into source and filter, which is an underlying\ncause of &#8220;buzziness&#8221;, &#8220;musical artifacts&#8221;,\nor &#8220;muffled sound&#8221; in the synthetic speech. The proposed\nmethod avoids making unnecessary assumptions and decompositions as\nfar as possible, and uses only the spectral envelope and F0 as parameters.\nPre-recorded speech is used as a base signal, which is &#8220;reshaped&#8221;\nto match the acoustic specification predicted by the statistical model,\nwithout any source-filter decomposition. A detailed description of\nthe method is presented, including implementation details and adjustments.\nSubjective listening test evaluations of complete DNN-based text-to-speech\nsystems were conducted for two voices: one female and one male. The\nresults show that the proposed method tends to outperform the state-of-the-art\nstandard vocoder STRAIGHT, whilst using fewer acoustic parameters.\n"
   ],
   "doi": "10.21437/Interspeech.2016-487"
  },
  "zhao16b_interspeech": {
   "authors": [
    [
     "Yi",
     "Zhao"
    ],
    [
     "Daisuke",
     "Saito"
    ],
    [
     "Nobuaki",
     "Minematsu"
    ]
   ],
   "title": "Speaker Representations for Speaker Adaptation in Multiple Speakers&#8217; BLSTM-RNN-Based Speech Synthesis",
   "original": "0506",
   "page_count": 5,
   "order": 483,
   "p1": "2268",
   "pn": "2272",
   "abstract": [
    "Training a high quality acoustic model with a limited database and\nsynthesizing a new speaker&#8217;s voice with a few utterances have\nbeen hot topics in deep neural network (DNN) based statistical parametric\nspeech synthesis (SPSS). To solve these problems, we built a unified\nframework for speaker adaptive training as well as speaker adaptation\non Bidirectional Long Short-Term Memory with Recurrent Neural Network\n(BLSTM-RNN) acoustic model. In this paper, we mainly focus on speaker\nidentity control at the input layer of our framework. We have investigated\ni-vector and speaker code as different speaker representations when\nused in an augmented input vector, and also propose two approaches\nto estimate a new speaker&#8217;s code. Experimental results show that\nthe speaker representations input to the first layer of acoustic model\ncan effectively control speaker identity during speaker adaptive training,\nthus improving the synthesized speech quality of speakers included\nin training phase. For speaker adaptation, speaker code estimated from\nMFCCs can achieve higher preference than other speaker representations.\n"
   ],
   "doi": "10.21437/Interspeech.2016-506"
  },
  "zen16_interspeech": {
   "authors": [
    [
     "Heiga",
     "Zen"
    ],
    [
     "Yannis",
     "Agiomyrgiannakis"
    ],
    [
     "Niels",
     "Egberts"
    ],
    [
     "Fergus",
     "Henderson"
    ],
    [
     "Przemysław",
     "Szczepaniak"
    ]
   ],
   "title": "Fast, Compact, and High Quality LSTM-RNN Based Statistical Parametric Speech Synthesizers for Mobile Devices",
   "original": "0522",
   "page_count": 5,
   "order": 484,
   "p1": "2273",
   "pn": "2277",
   "abstract": [
    "Acoustic models based on long short-term memory recurrent neural networks\n(LSTM-RNNs) were applied to statistical parametric speech synthesis\n(SPSS) and showed significant improvements in naturalness and latency\nover those based on hidden Markov models (HMMs). This paper describes\nfurther optimizations of LSTM-RNN-based SPSS for deployment on mobile\ndevices; weight quantization, multi-frame inference, and robust inference\nusing an &#1013;-contaminated Gaussian loss function. Experimental\nresults in subjective listening tests show that these optimizations\ncan make LSTM-RNN-based SPSS comparable to HMM-based SPSS in runtime\nspeed while maintaining naturalness. Evaluations between LSTM-RNN-based\nSPSS and HMM-driven unit selection speech synthesis are also presented.\n"
   ],
   "doi": "10.21437/Interspeech.2016-522"
  },
  "hojo16_interspeech": {
   "authors": [
    [
     "Nobukatsu",
     "Hojo"
    ],
    [
     "Yusuke",
     "Ijima"
    ],
    [
     "Hideyuki",
     "Mizuno"
    ]
   ],
   "title": "An Investigation of DNN-Based Speech Synthesis Using Speaker Codes",
   "original": "0589",
   "page_count": 5,
   "order": 485,
   "p1": "2278",
   "pn": "2282",
   "abstract": [
    "Recent studies have shown that DNN-based speech synthesis can produce\nmore natural synthesized speech than the conventional HMM-based speech\nsynthesis. However, an open problem remains as to whether the synthesized\nspeech quality can be improved by utilizing a multi-speaker speech\ncorpus. To address this problem, this paper proposes DNN-based speech\nsynthesis using speaker codes as a simple method to improve the performance\nof the conventional speaker dependent DNN-based method. In order to\nmodel speaker variation in the DNN, the augmented feature (speaker\ncodes) is fed to the hidden layer(s) of the conventional DNN. The proposed\nmethod trains connection weights of the whole DNN using a multi-speaker\nspeech corpus. When synthesizing a speech parameter sequence, a target\nspeaker is chosen from the corpus and the speaker code corresponding\nto the selected target speaker is fed to the DNN to generate the speaker&#8217;s\nvoice. We investigated the relationship between the prediction performance\nand architecture of the DNNs by changing the input hidden layer for\nspeaker codes. Experimental results showed that the proposed model\noutperformed the conventional speaker-dependent DNN when the model\narchitecture was set at optimal for the amount of training data of\nthe selected target speaker.\n"
   ],
   "doi": "10.21437/Interspeech.2016-589"
  },
  "juvela16b_interspeech": {
   "authors": [
    [
     "Lauri",
     "Juvela"
    ],
    [
     "Xin",
     "Wang"
    ],
    [
     "Shinji",
     "Takaki"
    ],
    [
     "Manu",
     "Airaksinen"
    ],
    [
     "Junichi",
     "Yamagishi"
    ],
    [
     "Paavo",
     "Alku"
    ]
   ],
   "title": "Using Text and Acoustic Features in Predicting Glottal Excitation Waveforms for Parametric Speech Synthesis with Recurrent Neural Networks",
   "original": "0712",
   "page_count": 5,
   "order": 486,
   "p1": "2283",
   "pn": "2287",
   "abstract": [
    "This work studies the use of deep learning methods to directly model\nglottal excitation waveforms from context dependent text features in\na text-to-speech synthesis system. Glottal vocoding is integrated into\na deep neural network-based text-to-speech framework where text and\nacoustic features can be flexibly used as both network inputs or outputs.\nLong short-term memory recurrent neural networks are utilised in two\nstages: first, in mapping text features to acoustic features and second,\nin predicting glottal waveforms from the text and/or acoustic features.\nResults show that using the text features directly yields similar quality\nto the prediction of the excitation from acoustic features, both outperforming\na baseline system based on using a fixed glottal pulse for excitation\ngeneration.\n"
   ],
   "doi": "10.21437/Interspeech.2016-712"
  },
  "tachibana16_interspeech": {
   "authors": [
    [
     "Kentaro",
     "Tachibana"
    ],
    [
     "Tomoki",
     "Toda"
    ],
    [
     "Yoshinori",
     "Shiga"
    ],
    [
     "Hisashi",
     "Kawai"
    ]
   ],
   "title": "Model Integration for HMM- and DNN-Based Speech Synthesis Using Product-of-Experts Framework",
   "original": "1006",
   "page_count": 5,
   "order": 487,
   "p1": "2288",
   "pn": "2292",
   "abstract": [
    "In this paper, we propose a model integration method for hidden Markov\nmodel (HMM) and deep neural network (DNN) based acoustic models using\na product-of-experts (PoE) framework in statistical parametric speech\nsynthesis. In speech parameter generation, DNN predicts a mean vector\nof the probability density function of speech parameters frame by frame\nwhile keeping its covariance matrix constant over all frames. On the\nother hand, HMM predicts the covariance matrix as well as the mean\nvector but they are fixed within the same HMM state, i.e., they can\nactually vary state by state. To make it possible to predict a better\nprobability density function by leveraging advantages of individual\nmodels, the proposed method integrates DNN and HMM as PoE, generating\na new probability density function satisfying conditions of both DNN\nand HMM. Furthermore, we propose a joint optimization method of DNN\nand HMM within the PoE framework by effectively using additional latent\nvariables. We conducted objective and subjective evaluations, demonstrating\nthat the proposed method significantly outperforms the DNN-based speech\nsynthesis as well as the HMM-based speech synthesis.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1006"
  },
  "potard16_interspeech": {
   "authors": [
    [
     "Blaise",
     "Potard"
    ],
    [
     "Matthew P.",
     "Aylett"
    ],
    [
     "David A.",
     "Baude"
    ],
    [
     "Petr",
     "Motlicek"
    ]
   ],
   "title": "Idlak Tangle: An Open Source Kaldi Based Parametric Speech Synthesiser Based on DNN",
   "original": "1188",
   "page_count": 5,
   "order": 488,
   "p1": "2293",
   "pn": "2297",
   "abstract": [
    "This paper presents a text to speech (TTS) extension to Kaldi &#8212;\na liberally licensed open source speech recognition system. The system,\nIdlak Tangle, uses recent deep neural network (DNN) methods for modelling\nspeech, the Idlak XML based text processing system as the front end,\nand a newly released open source mixed excitation MLSA vocoder included\nin Idlak. The system has none of the licensing restrictions of current\nfreely available HMM style systems, such as the HTS toolkit. To date\nno alternative open source DNN systems are available. Tangle combines\nthe Idlak front-end and vocoder, with two DNNs modelling respectively\nthe units duration and acoustic parameters, providing a fully functional\nend-to-end TTS system.<br style=\"mso-data-placement:same-cell;\" /><br\nstyle=\"mso-data-placement:same-cell;\" />\n  Experimental results using\nthe freely available SLT speaker from CMU ARCTIC, reveal that the speech\noutput is rated in a MUSHRA test as significantly more natural than\nthe output of HTS-demo, the only other free to download HMM system\navailable with no commercially restricted or proprietary IP. The tools,\naudio database and recipe required to reproduce the results presented\nin these paper are fully available online.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1188"
  },
  "lazaridis16_interspeech": {
   "authors": [
    [
     "Alexandros",
     "Lazaridis"
    ],
    [
     "Milos",
     "Cernak"
    ],
    [
     "Philip N.",
     "Garner"
    ]
   ],
   "title": "Probabilistic Amplitude Demodulation Features in Speech Synthesis for Improving Prosody",
   "original": "0258",
   "page_count": 5,
   "order": 489,
   "p1": "2298",
   "pn": "2302",
   "abstract": [
    "Amplitude demodulation (AM) is a signal decomposition technique by\nwhich a signal can be decomposed to a product of two signals, i.e,\na quickly varying carrier and a slowly varying modulator. In this work,\nthe probabilistic amplitude demodulation (PAD) features are used to\nimprove prosody in speech synthesis. The PAD is applied iteratively\nfor generating syllable and stress amplitude modulations in a cascade\nmanner. The PAD features are used as a secondary input scheme along\nwith the standard text-based input features in statistical parametric\nspeech synthesis. Specifically, deep neural network (DNN)-based speech\nsynthesis is used to evaluate the importance of these features. Objective\nevaluation has shown that the proposed system using the PAD features\nhas improved mainly prosody modelling; it outperforms the baseline\nsystem by approximately 5% in terms of relative reduction in root mean\nsquare error (RMSE) of the fundamental frequency (F0). The significance\nof this improvement is validated by subjective evaluation of the overall\nspeech quality, achieving 38.6% over 19.5% preference score in respect\nto the baseline system, in an ABX test.\n"
   ],
   "doi": "10.21437/Interspeech.2016-258"
  },
  "chiang16_interspeech": {
   "authors": [
    [
     "Chen-Yu",
     "Chiang"
    ]
   ],
   "title": "On Smoothing and Enhancing Dynamics of Pitch Contours Represented by Discrete Orthogonal Polynomials for Prosody Generation",
   "original": "0409",
   "page_count": 5,
   "order": 490,
   "p1": "2303",
   "pn": "2307",
   "abstract": [
    "This paper presents a new pitch contour generation algorithm for statistical\nsyllable-based logF0 generation models which represent logF0 contours\nof syllables by coefficients of discrete orthogonal polynomials, i.e.\northogonal expansion coefficients (OECs). The conventional statistical\nlogF0 models can generate smooth pitch contour within a syllable because\nof the continuity property of polynomials. However, the models do not\nensure to produce continuous and smooth logF0 contours in the proximity\nof syllable junctures. Besides, dynamic range of the generated logF0\ncontours is generally smaller than the one of real speech. The above\ntwo shortcomings would result in unnatural and monotonous prosody.\nTo overcome these shortcomings, juncture-smooth and dynamics-enhancing\nOEC generation algorithms are hence proposed in this paper. Analysis\non the generated logF0 contours by the proposed algorithm shows some\nimprovements in logF0 smoothness at syllable junctures and enhanced\nlogF0 dynamic range. In addition, a perceptual evaluation of the logF0\ncontour generated by the proposed algorithm shows an improvement in\nnaturalness of the synthesized speech.\n"
   ],
   "doi": "10.21437/Interspeech.2016-409"
  },
  "vadapalli16_interspeech": {
   "authors": [
    [
     "Anandaswarup",
     "Vadapalli"
    ],
    [
     "Suryakanth V.",
     "Gangashetty"
    ]
   ],
   "title": "An Investigation of Recurrent Neural Network Architectures Using Word Embeddings for Phrase Break Prediction",
   "original": "0885",
   "page_count": 5,
   "order": 491,
   "p1": "2308",
   "pn": "2312",
   "abstract": [
    "This paper presents our investigations of recurrent neural networks\n(RNNs) for the phrase break prediction task. With the advent of deep\nlearning, there have been attempts to apply deep neural networks (DNNs)\nto phrase break prediction. While deep neural networks are able to\neffectively capture dependencies across features, they lack the ability\nto capture long-term relations that are spread over time. On the other\nhand, RNNs are able to capture long-term temporal relations and thus\nare better suited for tasks where sequences have to be modeled. We\nmodel the phrase break prediction task as a sequence labeling task,\nand show by means of experimental results that RNNs perform better\nat phrase break prediction as compared to conventional DNN systems.\n"
   ],
   "doi": "10.21437/Interspeech.2016-885"
  },
  "liu16j_interspeech": {
   "authors": [
    [
     "Hao",
     "Liu"
    ],
    [
     "Heng",
     "Lu"
    ],
    [
     "Xu",
     "Shao"
    ],
    [
     "Yi",
     "Xu"
    ]
   ],
   "title": "Model-Based Parametric Prosody Synthesis with Deep Neural Network",
   "original": "1325",
   "page_count": 5,
   "order": 492,
   "p1": "2313",
   "pn": "2317",
   "abstract": [
    "Conventional statistical parametric speech synthesis (SPSS) captures\nonly frame-wise acoustic observations and computes probability densities\nat HMM state level to obtain statistical acoustic models combined with\ndecision trees, which is therefore a purely statistical data-driven\napproach without explicit integration of any articulatory mechanisms\nfound in speech production research. The present study explores an\nalternative paradigm, namely, model-based parametric prosody synthesis\n(MPPS), which integrates dynamic mechanisms of human speech production\nas a core component of F0 generation. In this paradigm, contextual\nvariations in prosody are processed in two separate yet integrated\nstages: linguistic to motor, and motor to acoustic. Here the motor\nmodel is target approximation (TA), which generates syllable-sized\nF0 contours with only three motor parameters that are associated to\nlinguistic functions. In this study, we simulate this two-stage process\nby linking the TA model to a deep neural network (DNN), which learns\nthe &#8220;linguistic-motor&#8221; mapping given the &#8220;motor-acoustic&#8221;\nmapping provided by TA-based syllable-wise F0 production. The proposed\nprosody modeling system outperforms the HMM-based baseline system in\nboth objective and subjective evaluations.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1325"
  },
  "drugman16_interspeech": {
   "authors": [
    [
     "Thomas",
     "Drugman"
    ],
    [
     "Janne",
     "Pylkkönen"
    ],
    [
     "Reinhard",
     "Kneser"
    ]
   ],
   "title": "Active and Semi-Supervised Learning in ASR: Benefits on the Acoustic and Language Models",
   "original": "1382",
   "page_count": 5,
   "order": 493,
   "p1": "2318",
   "pn": "2322",
   "abstract": [
    "The goal of this paper is to simulate the benefits of jointly applying\nactive learning (AL) and semi-supervised training (SST) in a new speech\nrecognition application. Our data selection approach relies on confidence\nfiltering, and its impact on both the acoustic and language models\n(AM and LM) is studied. While AL is known to be beneficial to AM training,\nwe show that it also carries out substantial improvements to the LM\nwhen combined with SST. Sophisticated confidence models, on the other\nhand, did not prove to yield any data selection gain. Our results indicate\nthat, while SST is crucial at the beginning of the labeling process,\nits gains degrade rapidly as AL is set in place. The final simulation\nreports that AL allows a transcription cost reduction of about 70%\nover random selection. Alternatively, for a fixed transcription budget,\nthe proposed approach improves the word error rate by about 12.5% relative.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1382"
  },
  "kuznetsov16_interspeech": {
   "authors": [
    [
     "Vitaly",
     "Kuznetsov"
    ],
    [
     "Hank",
     "Liao"
    ],
    [
     "Mehryar",
     "Mohri"
    ],
    [
     "Michael",
     "Riley"
    ],
    [
     "Brian",
     "Roark"
    ]
   ],
   "title": "Learning N-Gram Language Models from Uncertain Data",
   "original": "1093",
   "page_count": 5,
   "order": 494,
   "p1": "2323",
   "pn": "2327",
   "abstract": [
    "We present a new algorithm for efficiently training n-gram language\nmodels on uncertain data, and illustrate its use for semi-supervised\nlanguage model adaptation. We compute the probability that an n-gram\noccurs k times in the sample of uncertain data, and use the resulting\nhistograms to derive a generalized Katz back-off model. We compare\nthree approaches to semi-supervised adaptation of language models for\nspeech recognition of selected YouTube video categories: (1) using\njust the one-best output from the baseline speech recognizer or (2)\nusing samples from lattices with standard algorithms versus (3) using\nfull lattices with our new algorithm. Unlike the other methods, our\nnew algorithm provides models that yield solid improvements over the\nbaseline on the full test set, and, further, achieves these gains without\nhurting performance on any of the set of video categories. We show\nthat categories with the most data yielded the largest gains. The algorithm\nhas been released as part of the OpenGrm n-gram library [1].\n"
   ],
   "doi": "10.21437/Interspeech.2016-1093"
  },
  "oguz16_interspeech": {
   "authors": [
    [
     "Barlas",
     "Oğuz"
    ],
    [
     "Issac",
     "Alphonso"
    ],
    [
     "Shuangyu",
     "Chang"
    ]
   ],
   "title": "Entropy Based Pruning for Non-Negative Matrix Based Language Models with Contextual Features",
   "original": "0130",
   "page_count": 5,
   "order": 495,
   "p1": "2328",
   "pn": "2332",
   "abstract": [
    "Non-negative matrix based language models have been recently introduced\n[1] as a computationally efficient alternative to other feature-based\nmodels such as maximum-entropy models. We present a new entropy based\npruning algorithm for this class of language models, which is fast\nand scalable. We present perplexity and word error rate results and\ncompare these against regular n-gram pruning. We also train models\nwith location and personalization features and report results at various\npruning thresholds. We demonstrate that contextual features are helpful\nover the vanilla model even after pruning to a similar size.\n"
   ],
   "doi": "10.21437/Interspeech.2016-130"
  },
  "gangireddy16_interspeech": {
   "authors": [
    [
     "Siva Reddy",
     "Gangireddy"
    ],
    [
     "Pawel",
     "Swietojanski"
    ],
    [
     "Peter",
     "Bell"
    ],
    [
     "Steve",
     "Renals"
    ]
   ],
   "title": "Unsupervised Adaptation of Recurrent Neural Network Language Models",
   "original": "1342",
   "page_count": 5,
   "order": 496,
   "p1": "2333",
   "pn": "2337",
   "abstract": [
    "Recurrent neural network language models (RNNLMs) have been shown to\nconsistently improve Word Error Rates (WERs) of large vocabulary speech\nrecognition systems employing n-gram LMs. In this paper we investigate\nsupervised and unsupervised discriminative adaptation of RNNLMs in\na broadcast transcription task to target domains defined by either\ngenre or show. We have explored two approaches based on (1) scaling\nforward-propagated hidden activations (Learning Hidden Unit Contributions\n(LHUC) technique) and (2) direct fine-tuning of the parameters of the\nwhole RNNLM. To investigate the effectiveness of the proposed methods\nwe carry out experiments on multi-genre broadcast (MGB) data following\nthe MGB-2015 challenge protocol. We observe small but significant improvements\nin WER compared to a strong unadapted RNNLM model.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1342"
  },
  "halpern16_interspeech": {
   "authors": [
    [
     "Yoni",
     "Halpern"
    ],
    [
     "Keith",
     "Hall"
    ],
    [
     "Vlad",
     "Schogol"
    ],
    [
     "Michael",
     "Riley"
    ],
    [
     "Brian",
     "Roark"
    ],
    [
     "Gleb",
     "Skobeltsyn"
    ],
    [
     "Martin",
     "Bäuml"
    ]
   ],
   "title": "Contextual Prediction Models for Speech Recognition",
   "original": "1358",
   "page_count": 5,
   "order": 497,
   "p1": "2338",
   "pn": "2342",
   "abstract": [
    "We introduce an approach to biasing language models towards known contexts\nwithout requiring separate language models or explicit contextually-dependent\nconditioning contexts. We do so by presenting an alternative ASR objective,\nwhere we predict the acoustics and words given the contextual cue,\nsuch as the geographic location of the speaker. A simple factoring\nof the model results in an additional  biasing term, which effectively\nindicates how correlated a hypothesis is with the contextual cue (e.g.,\ngiven the hypothesized transcript, how likely is the user&#8217;s known\nlocation). We demonstrate that this factorization allows us to train\nrelatively small contextual models which are effective in speech recognition.\nAn experimental analysis shows a perplexity reduction of up to 35%\nand a relative reduction in word error rate of 1.6% on a targeted voice\nsearch dataset when using the user&#8217;s coarse location as a contextual\ncue.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1358"
  },
  "deena16_interspeech": {
   "authors": [
    [
     "Salil",
     "Deena"
    ],
    [
     "Madina",
     "Hasan"
    ],
    [
     "Mortaza",
     "Doulaty"
    ],
    [
     "Oscar",
     "Saz"
    ],
    [
     "Thomas",
     "Hain"
    ]
   ],
   "title": "Combining Feature and Model-Based Adaptation of RNNLMs for Multi-Genre Broadcast Speech Recognition",
   "original": "0480",
   "page_count": 5,
   "order": 498,
   "p1": "2343",
   "pn": "2347",
   "abstract": [
    "Recurrent neural network language models (RNNLMs) have consistently\noutperformed n-gram language models when used in automatic speech recognition\n(ASR). This is because RNNLMs provide robust parameter estimation through\nthe use of a continuous-space representation of words, and can generally\nmodel longer context dependencies than n-grams. The adaptation of RNNLMs\nto new domains remains an active research area and the two main approaches\nare: feature-based adaptation, where the input to the RNNLM is augmented\nwith auxiliary features; and model-based adaptation, which includes\nmodel fine-tuning and introduction of adaptation layer(s) in the network.\nThis paper explores the properties of both types of adaptation on multi-genre\nbroadcast speech recognition. Two hybrid adaptation techniques are\nproposed, namely the fine-tuning of feature-based RNNLMs and the use\nof a feature-based adaptation layer. A method for the semi-supervised\nadaptation of RNNLMs, using topic model-based genre classification,\nis also presented and investigated. The gains obtained with RNNLM adaptation\non a system trained on 700h. of speech are consistent using both RNNLMs\ntrained on a small (10Mwords) and large set (660M words), with 10%\nperplexity and 2% word error rate improvements on a 28.3h. test set.\n"
   ],
   "doi": "10.21437/Interspeech.2016-480"
  },
  "brady16_interspeech": {
   "authors": [
    [
     "Michael C.",
     "Brady"
    ]
   ],
   "title": "A Low Cost Desktop Robot and Tele-Presence Device for Interactive Speech Research",
   "original": "2022",
   "page_count": 2,
   "order": 499,
   "p1": "2348",
   "pn": "2349",
   "abstract": [
    "In building robotic systems that interact with people through speech,\nmany robotics engineers are obliged to treat artificial speech recognition\nand synthesis as a black-box problem best left to speech engineers\nto solve. Yet speech engineers today typically do not have access to\nthe kinds of expensive robots needed for this development. Progress\non the human-robot speech interface thus suffers from something of\na diffusion of responsibility. In an attempt to remedy the situation,\nwe have developed a low-cost interactive embodied speech device. The\ndevice is constructed from off-the-shelf components and from 3D-printed\nand laser-cut parts. We make the files for the 3D and laser-cut parts\nfreely available for download. In addition to offering basic assembled\ndevices and kits for self-assembly, we provide an assembly guide and\na shopping list of components a user will need in order to build, maintain,\nand customize their own device. We supply a basic software framework\n(in both Matlab and in C/C++), and template code for a ROS node for\ninterfacing with the device. The idea is to establish a standard and\naccessible hardware platform with an open-source foundation for the\nsharing of ideas and research.\n"
   ]
  },
  "stone16_interspeech": {
   "authors": [
    [
     "Simon",
     "Stone"
    ],
    [
     "Peter",
     "Birkholz"
    ]
   ],
   "title": "Silent-Speech Command Word Recognition Using Electro-Optical Stomatography",
   "original": "2005",
   "page_count": 2,
   "order": 500,
   "p1": "2350",
   "pn": "2351",
   "abstract": [
    "In this paper that accompanies a live Show &amp; Tell demonstration\nat  Interspeech 2016, we present our current speaker-dependent silent-speech\nrecognition system. Silent-speech recognition refers to the recognition\nof speech without any acoustic data. To that end, our system uses a\nnovel technique called electro-optical stomatography to record the\ntongue and lip movements of a subject during the articulation of a\nset of isolated words in real-time. Based on these data, simple articulatory\nmodels are learned. The system then classifies unseen articulatory\ndata of learned isolated words spoken by the same subject. This paper\npresents the system components and showcases the silent-speech recognition\nprocess with a set of the 30 most common German words. Since the system\nis language-independent and easy to train, the demonstration will also\nshow both training and recognition of any other words on demand.\n"
   ]
  },
  "stanislav16b_interspeech": {
   "authors": [
    [
     "Petr",
     "Stanislav"
    ],
    [
     "Jan",
     "Švec"
    ],
    [
     "Pavel",
     "Ircing"
    ]
   ],
   "title": "An Engine for Online Video Search in Large Archives of the Holocaust Testimonies",
   "original": "2016",
   "page_count": 2,
   "order": 501,
   "p1": "2352",
   "pn": "2353",
   "abstract": [
    "In this paper we present an online system for cross-lingual lexical\n(full-text) searching in the large archive of the Holocaust testimonies.<br\nstyle=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  Video interviews recorded in two languages (English and Czech)\nwere automatically transcribed and indexed in order to provide efficient\naccess to the lexical content of the recordings. The engine takes advantage\nof the state-of-the-art speech recognition system and performs fast\nspoken term detection (STD), providing direct access to the segments\nof interviews containing queried words or short phrases.\n"
   ]
  },
  "zmolikova16_interspeech": {
   "authors": [
    [
     "Kateřina",
     "Žmolíková"
    ],
    [
     "Martin",
     "Karafiát"
    ],
    [
     "Karel",
     "Veselý"
    ],
    [
     "Marc",
     "Delcroix"
    ],
    [
     "Shinji",
     "Watanabe"
    ],
    [
     "Lukáš",
     "Burget"
    ],
    [
     "Jan",
     "Černocký"
    ]
   ],
   "title": "Data Selection by Sequence Summarizing Neural Network in Mismatch Condition Training",
   "original": "0741",
   "page_count": 5,
   "order": 502,
   "p1": "2354",
   "pn": "2358",
   "abstract": [
    "Data augmentation is a simple and efficient technique to improve the\nrobustness of a speech recognizer when deployed in mismatched training-test\nconditions. Our paper proposes a new approach for selecting data with\nrespect to similarity of acoustic conditions. The similarity is computed\nbased on a sequence summarizing neural network which extracts vectors\ncontaining acoustic summary (e.g. noise and reverberation characteristics)\nof an utterance. Several configurations of this network and different\nmethods of selecting data using these &#8220;summary-vectors&#8221;\nwere explored. The results are reported on a mismatched condition using\nAMI training set with the proposed data selection and CHiME3 test set.\n"
   ],
   "doi": "10.21437/Interspeech.2016-741"
  },
  "kundu16_interspeech": {
   "authors": [
    [
     "Souvik",
     "Kundu"
    ],
    [
     "Khe Chai",
     "Sim"
    ],
    [
     "Mark J.F.",
     "Gales"
    ]
   ],
   "title": "Incorporating a Generative Front-End Layer to Deep Neural Network for Noise Robust Automatic Speech Recognition",
   "original": "0760",
   "page_count": 5,
   "order": 503,
   "p1": "2359",
   "pn": "2363",
   "abstract": [
    "It is difficult to apply well-formulated model-based noise adaptation\napproaches to Deep Neural Network (DNN) due to the lack of interpretability\nof the model parameters. In this paper, we propose incorporating a\ngenerative front-end layer (GFL), which is parameterised by Gaussian\nMixture Model (GMM), into the DNN. A GFL can be easily adapted to different\nnoise conditions by applying the model-based Vector Taylor Series (VTS)\nto the underlying GMM. We show that incorporating a GFL to DNN yields\n12.1% relative improvement over a baseline multi-condition DNN. We\nalso show that the proposed system performs significantly better than\nthe noise aware training method, where the per-utterance estimated\nnoise parameters are appended to the acoustic features.\n"
   ],
   "doi": "10.21437/Interspeech.2016-760"
  },
  "markov16_interspeech": {
   "authors": [
    [
     "Konstantin",
     "Markov"
    ],
    [
     "Tomoko",
     "Matsui"
    ]
   ],
   "title": "Robust Speech Recognition Using Generalized Distillation Framework",
   "original": "0852",
   "page_count": 5,
   "order": 504,
   "p1": "2364",
   "pn": "2368",
   "abstract": [
    "In this paper, we propose a noise robust speech recognition system\nbuilt using generalized distillation framework. It is assumed that\nduring training, in addition to the training data, some kind of &#8220;privileged&#8221;\ninformation is available and can be used to guide the training process.\nThis allows to obtain a system which at test time outperforms those\nbuilt on regular training data alone. In the case of noisy speech recognition\ntask, the privileged information is obtained from a model, called &#8220;teacher&#8221;,\ntrained on clean speech only. The regular model, called &#8220;student&#8221;,\nis trained on noisy utterances and uses teacher&#8217;s output for\nthe corresponding clean utterances. Thus, for this framework a parallel\nclean/noisy speech data are required. We experimented on the Aurora2\ndatabase which provides such kind of data. Our system uses hybrid DNN-HMM\nacoustic model where neural networks provide HMM state probabilities\nduring decoding. The teacher DNN is trained on the clean data, while\nthe student DNN is trained using multi-condition (various SNRs) data.\nThe student DNN loss function combines the targets obtained from forced\nalignment of the training data and the outputs of the teacher DNN when\nfed with the corresponding clean features. Experimental results clearly\nshow that distillation framework is effective and allows to achieve\nsignificant reduction in the word error rate.\n"
   ],
   "doi": "10.21437/Interspeech.2016-852"
  },
  "shinohara16b_interspeech": {
   "authors": [
    [
     "Yusuke",
     "Shinohara"
    ]
   ],
   "title": "Adversarial Multi-Task Learning of Deep Neural Networks for Robust Speech Recognition",
   "original": "0879",
   "page_count": 4,
   "order": 505,
   "p1": "2369",
   "pn": "2372",
   "abstract": [
    "A method of learning deep neural networks (DNNs) for noise robust speech\nrecognition is proposed. It is widely known that representations (activations)\nof well-trained DNNs are highly invariant to noise, especially in higher\nlayers, and such invariance leads to the noise robustness of DNNs.\nHowever, little is known about how to enhance such invariance of representations,\nwhich is a key for improving robustness. In this paper, we propose\nadversarial multi-task learning of DNNs for explicitly enhancing the\ninvariance of representations. Specifically, a primary task of senone\nclassification and a secondary task of domain (noise condition) classification\nare jointly solved. What is different from the standard multi-task\nlearning is that the representation is learned adversarially to the\nsecondary task, so that representation with low domain-classification\naccuracy is induced. As a result, senone-discriminative and domain-invariant\nrepresentation is obtained, which leads to an improved robustness of\nDNNs. Experimental results on a noise-corrupted Wall Street Journal\ndata set show the effectiveness of the proposed method.\n"
   ],
   "doi": "10.21437/Interspeech.2016-879"
  },
  "poblete16_interspeech": {
   "authors": [
    [
     "Víctor",
     "Poblete"
    ],
    [
     "Juan Pablo",
     "Escudero"
    ],
    [
     "Josué",
     "Fredes"
    ],
    [
     "José",
     "Novoa"
    ],
    [
     "Richard M.",
     "Stern"
    ],
    [
     "Simon",
     "King"
    ],
    [
     "Néstor Becerra",
     "Yoma"
    ]
   ],
   "title": "The Use of Locally Normalized Cepstral Coefficients (LNCC) to Improve Speaker Recognition Accuracy in Highly Reverberant Rooms",
   "original": "1277",
   "page_count": 5,
   "order": 506,
   "p1": "2373",
   "pn": "2377",
   "abstract": [
    "We describe the ability of LNCC features (Locally Normalized Cepstral\nCoefficients) to improve speaker recognition accuracy in highly reverberant\nenvironments. We used a realistic test environment, in which we changed\nthe number and nature of reflective surfaces in the room, creating\nfour increasingly reverberant times from approximately 1 to 9 seconds.\nIn this room, we re-recorded reverberated versions of the Yoho speaker\nverification corpus. The recordings were made using four speaker-to-microphone\ndistances, from 0.32m to 2.56m. Experimental results for a speaker\nverification task suggest that LNCC features are an attractive alternative\nto MFCC features under such reverberant conditions, as they were observed\nto improve verification accuracy compared to baseline MFCC features\nin all cases where the reverberation time exceeded 1 second or with\na greater speaker-microphone distance (i.e. 2.56 m).\n"
   ],
   "doi": "10.21437/Interspeech.2016-1277"
  },
  "hartmann16b_interspeech": {
   "authors": [
    [
     "William",
     "Hartmann"
    ],
    [
     "Tim",
     "Ng"
    ],
    [
     "Roger",
     "Hsiao"
    ],
    [
     "Stavros",
     "Tsakalidis"
    ],
    [
     "Richard",
     "Schwartz"
    ]
   ],
   "title": "Two-Stage Data Augmentation for Low-Resourced Speech Recognition",
   "original": "1386",
   "page_count": 5,
   "order": 507,
   "p1": "2378",
   "pn": "2382",
   "abstract": [
    "Low resourced languages suffer from limited training data and resources.\nData augmentation is a common approach to increasing the amount of\ntraining data. Additional data is synthesized by manipulating the original\ndata with a variety of methods. Unlike most previous work that focuses\non a single technique, we combine multiple, complementary augmentation\napproaches. The first stage adds noise and perturbs the speed of additional\ncopies of the original audio. The data is further augmented in a second\nstage, where a novel fMLLR-based augmentation is applied to bottleneck\nfeatures to further improve performance. A reduction in word error\nrate is demonstrated on four languages from the IARPA Babel program.\nWe present an analysis exploring why these techniques are beneficial.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1386"
  },
  "schuller16d_interspeech": {
   "authors": [
    [
     "Björn",
     "Schuller"
    ],
    [
     "Stefan",
     "Steidl"
    ],
    [
     "Anton",
     "Batliner"
    ],
    [
     "Julia",
     "Hirschberg"
    ],
    [
     "Judee K.",
     "Burgoon"
    ],
    [
     "Alice",
     "Baird"
    ],
    [
     "Aaron",
     "Elkins"
    ],
    [
     "Yue",
     "Zhang"
    ],
    [
     "Eduardo",
     "Coutinho"
    ],
    [
     "Keelan",
     "Evanini"
    ]
   ],
   "title": "The Native Language Sub-Challenge: The Data",
   "original": "abs11",
   "page_count": 0,
   "order": 508,
   "p1": "0",
   "pn": "",
   "abstract": [
    "(No abstract available at the time of publication)\n"
   ]
  },
  "rajpal16_interspeech": {
   "authors": [
    [
     "Avni",
     "Rajpal"
    ],
    [
     "Tanvina B.",
     "Patel"
    ],
    [
     "Hardik B.",
     "Sailor"
    ],
    [
     "Maulik C.",
     "Madhavi"
    ],
    [
     "Hemant A.",
     "Patil"
    ],
    [
     "Hiroya",
     "Fujisaki"
    ]
   ],
   "title": "Native Language Identification Using Spectral and Source-Based Features",
   "original": "1100",
   "page_count": 5,
   "order": 509,
   "p1": "2383",
   "pn": "2387",
   "abstract": [
    "The task of native language (L1) identification from non-native language\n(L2) can be thought of as the task of identifying the common traits\nthat each group of L1 speakers maintains while speaking L2 irrespective\nof the dialect or region. Under the assumption that speakers are L1\nproficient, non-native cues in terms of segmental and prosodic aspects\nare investigated in our work. In this paper, we propose the use of\nlonger duration cepstral features, namely, Mel frequency cepstral coefficients\n(MFCC) and auditory filterbank features learnt from the database using\nConvolutional Restricted Boltzmann Machine (ConvRBM) along with their\ndelta and shifted delta features. MFCC and ConvRBM gave accuracy of\n38.2% and 36.8%, respectively, on the development set provided for\nthe ComParE 2016 Nativeness Task using Gaussian Mixture Model (GMM)\nclassifier. To add complementary information about the prosodic and\nexcitation source features, phrase information and its dynamics extracted\nfrom the log(F<SUB>0</SUB>) contour of the speech was explored. The\naccuracy obtained using score-level fusion between system features\n(MFCC and ConvRBM) and phrase features were 39.6% and 38.3%, respectively,\nindicating that phrase information and MFCC capture complementary information\nthan ConvRBM alone. Furthermore, score-level fusion of MFCC, ConvRBM\nand phrase improves the accuracy to 40.2%.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1100"
  },
  "jiao16_interspeech": {
   "authors": [
    [
     "Yishan",
     "Jiao"
    ],
    [
     "Ming",
     "Tu"
    ],
    [
     "Visar",
     "Berisha"
    ],
    [
     "Julie",
     "Liss"
    ]
   ],
   "title": "Accent Identification by Combining Deep Neural Networks and Recurrent Neural Networks Trained on Long and Short Term Features",
   "original": "1148",
   "page_count": 5,
   "order": 510,
   "p1": "2388",
   "pn": "2392",
   "abstract": [
    "Automatic identification of foreign accents is valuable for many speech\nsystems, such as speech recognition, speaker identification, voice\nconversion, etc. The INTERSPEECH 2016 Native Language Sub-Challenge\nis to identify the native languages of non-native English speakers\nfrom eleven countries. Since differences in accent are due to both\nprosodic and articulation characteristics, a combination of long-term\nand short-term training is proposed in this paper. Each speech sample\nis processed into multiple speech segments with equal length. For each\nsegment, deep neural networks (DNNs) are used to train on long-term\nstatistical features, while recurrent neural networks (RNNs) are used\nto train on short-term acoustic features. The result for each speech\nsample is calculated by linearly fusing the results from the two sets\nof networks on all segments. The performance of the proposed system\ngreatly surpasses the provided baseline system. Moreover, by fusing\nthe results with the baseline system, the performance can be further\nimproved.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1148"
  },
  "keren16_interspeech": {
   "authors": [
    [
     "Gil",
     "Keren"
    ],
    [
     "Jun",
     "Deng"
    ],
    [
     "Jouni",
     "Pohjalainen"
    ],
    [
     "Björn",
     "Schuller"
    ]
   ],
   "title": "Convolutional Neural Networks with Data Augmentation for Classifying Speakers&#8217; Native Language",
   "original": "0261",
   "page_count": 5,
   "order": 511,
   "p1": "2393",
   "pn": "2397",
   "abstract": [
    "We use a feedforward Convolutional Neural Network to classify speakers&#8217;\nnative language for the INTERSPEECH 2016 Computational Paralinguistic\nChallenge Native Language Sub-Challenge, using no specialized features\nfor computational paralinguistics tasks, but only MFCCs with their\nfirst and second order deltas. In addition, we augment the training\ndata by replacing the original examples with shorter overlapping samples\nextracted from them, thus multiplying the number of training examples\nby almost 40. With the augmented training dataset and enhancements\nto neural network models such as Batch Normalization, Dropout, and\nMaxout activation function, we managed to improve upon the challenge\nbaseline by a large margin, both for the development and the test set.\n"
   ],
   "doi": "10.21437/Interspeech.2016-261"
  },
  "senoussaoui16_interspeech": {
   "authors": [
    [
     "Mohammed",
     "Senoussaoui"
    ],
    [
     "Patrick",
     "Cardinal"
    ],
    [
     "Najim",
     "Dehak"
    ],
    [
     "Alessandro L.",
     "Koerich"
    ]
   ],
   "title": "Native Language Detection Using the I-Vector Framework",
   "original": "1473",
   "page_count": 5,
   "order": 512,
   "p1": "2398",
   "pn": "2402",
   "abstract": [
    "Native-language identification is the task of determining a speaker&#8217;s\nnative language based only on their speeches in a second language.\nIn this paper we propose the use of the well-known i-vector representation\nof the speech signal to detect the native language of an English speaker.\nThe i-vector representation has shown an excellent performance on the\nquite similar task of distinguishing between different languages. We\nhave evaluated different ways to extract i-vectors in order to adapt\nthem to the specificities of the native language detection task. The\nexperimental results on the 2016 ComParE Native language sub-challenge\ntest set have shown that the proposed system based on a conventional\ni-vector extractor outperforms the baseline system with a 42% relative\nimprovement.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1473"
  },
  "huckvale16_interspeech": {
   "authors": [
    [
     "Mark",
     "Huckvale"
    ]
   ],
   "title": "Within-Speaker Features for Native Language Recognition in the Interspeech 2016 Computational Paralinguistics Challenge",
   "original": "1466",
   "page_count": 5,
   "order": 513,
   "p1": "2403",
   "pn": "2407",
   "abstract": [
    "The Interspeech 2016 Native Language recognition challenge was to identify\nthe first language of 867 speakers from their spoken English. Effectively\nthis was an L2 accent recognition task where the L1 was one of eleven\nlanguages. The lack of transcripts of the spontaneous speech recordings\nmeant that the currently best performing accent recognition approach\n(ACCDIST) developed by the author could not be applied. Instead, the\nobjectives of this study were to explore whether within-speaker features\nfound to be effective in ACCDIST would also have value within a contemporary\nGMM-based accent recognition approach. We show that while Gaussian\nmean supervectors provide the best performance on this task, small\ngains may be had by fusing the mean supervector system with a system\nbased on within-speaker Gaussian mixture distances.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1466"
  },
  "shivakumar16_interspeech": {
   "authors": [
    [
     "Prashanth Gurunath",
     "Shivakumar"
    ],
    [
     "Sandeep Nallan",
     "Chakravarthula"
    ],
    [
     "Panayiotis",
     "Georgiou"
    ]
   ],
   "title": "Multimodal Fusion of Multirate Acoustic, Prosodic, and Lexical Speaker Characteristics for Native Language Identification",
   "original": "1312",
   "page_count": 5,
   "order": 514,
   "p1": "2408",
   "pn": "2412",
   "abstract": [
    "Native language identification from acoustic signals of L2 speakers\ncan be useful in a range of applications such as informing automatic\nspeech recognition (ASR), speaker recognition, and speech biometrics.\nIn this paper we follow a multi-stream and multi-rate approach, for\nnative language identification, in feature extraction, classification,\nand fusion. On the feature front we employ acoustic features such as\nMFCC and PLP features, at different time scales and different transformations;\nwe evaluate speaker normalization as a feature and as a transform;\ninvestigate phonemic confusability and its interplay with paralinguistic\ncues at both the frame and phone-level temporal scales; and automatically\nextract lexical features; in addition to baseline features. On the\nclassification side we employ SVM, i-Vector, DNN and bottleneck features,\nand maximum-likelihood models. Finally we employ fusion for system\ncombination and analyze the complementarity of the individual systems.\nOur proposed system significantly outperforms the baseline system on\nboth development and test sets.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1312"
  },
  "abad16_interspeech": {
   "authors": [
    [
     "Alberto",
     "Abad"
    ],
    [
     "Eugénio",
     "Ribeiro"
    ],
    [
     "Fábio",
     "Kepler"
    ],
    [
     "Ramon",
     "Astudillo"
    ],
    [
     "Isabel",
     "Trancoso"
    ]
   ],
   "title": "Exploiting Phone Log-Likelihood Ratio Features for the Detection of the Native Language of Non-Native English Speakers",
   "original": "1491",
   "page_count": 5,
   "order": 515,
   "p1": "2413",
   "pn": "2417",
   "abstract": [
    "Detecting the native language (L1) of non-native English speakers may\nbe of great relevance in some applications, such as computer assisted\nlanguage learning or IVR services. In fact, the L1 detection problem\nclosely resembles the problem of spoken language and dialect recognition.\nIn particular, log-likelihood ratios of phone posterior probabilities,\nknown as Phone LogLikelihood Ratios (PLLR), have been recently introduced\nas features for spoken language recognition systems. This representation\nhas proven to be an effective way of retrieving acoustic-phonotactic\ninformation at frame-level, which allows for its use in state-of-the-art\nsystems, that is, in i-vector systems. In this paper, we explore the\nuse of PLLR-based i-vector systems for L1 native language detection.\nWe also investigate several linear and non-linear L1 classification\nschemes on top of the PLLR i-vector front-ends. Moreover, we compare\nPLLR based systems with both conventional phonotactic systems based\non n-gram modelling of phoneme sequences and acoustic-based i-vector\nsystems. Finally, the potential complementarity of the different approaches\nis investigated based on a set of system fusion experiments.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1491"
  },
  "gosztolya16c_interspeech": {
   "authors": [
    [
     "Gábor",
     "Gosztolya"
    ],
    [
     "Tamás",
     "Grósz"
    ],
    [
     "Róbert",
     "Busa-Fekete"
    ],
    [
     "László",
     "Tóth"
    ]
   ],
   "title": "Determining Native Language and Deception Using Phonetic Features and Classifier Combination",
   "original": "0962",
   "page_count": 5,
   "order": 516,
   "p1": "2418",
   "pn": "2422",
   "abstract": [
    "For several years, the Interspeech ComParE Challenge has focused on\nparalinguistic tasks of various kinds. In this paper we focus on the\nNative Language and the Deception sub-challenges of ComParE 2016, where\nthe goal is to identify the native language of the speaker, and to\nrecognize deceptive speech. As both tasks can be treated as classification\nones, we experiment with several state-of-the-art machine learning\nmethods (Support-Vector Machines, AdaBoost. MH and Deep Neural Networks),\nand also test a simple-yet-robust combination method. Furthermore,\nwe will assume that the native language of the speaker affects the\npronunciation of specific phonemes in the language he is currently\nusing. To exploit this, we extract phonetic features for the Native\nLanguage task. Moreover, for the Deception Sub-Challenge we compensate\nfor the highly unbalanced class distribution by instance re-sampling.\nWith these techniques we are able to significantly outperform the baseline\nSVM on the unpublished test set.\n"
   ],
   "doi": "10.21437/Interspeech.2016-962"
  },
  "schuller16e_interspeech": {
   "authors": [
    [
     "Björn",
     "Schuller"
    ],
    [
     "Stefan",
     "Steidl"
    ],
    [
     "Anton",
     "Batliner"
    ],
    [
     "Julia",
     "Hirschberg"
    ],
    [
     "Judee K.",
     "Burgoon"
    ],
    [
     "Alice",
     "Baird"
    ],
    [
     "Aaron",
     "Elkins"
    ],
    [
     "Yue",
     "Zhang"
    ],
    [
     "Eduardo",
     "Coutinho"
    ],
    [
     "Keelan",
     "Evanini"
    ]
   ],
   "title": "The INTERSPEECH 2016 Computational Paralinguistics Challenge: A Summary of Results",
   "original": "abs12",
   "page_count": 0,
   "order": 517,
   "p1": "0",
   "pn": "",
   "abstract": [
    "(No abstract available at the time of publication)\n"
   ]
  },
  "schuller16f_interspeech": {
   "authors": [
    [
     "Björn",
     "Schuller"
    ],
    [
     "Stefan",
     "Steidl"
    ],
    [
     "Anton",
     "Batliner"
    ],
    [
     "Julia",
     "Hirschberg"
    ],
    [
     "Judee K.",
     "Burgoon"
    ],
    [
     "Alice",
     "Baird"
    ],
    [
     "Aaron",
     "Elkins"
    ],
    [
     "Yue",
     "Zhang"
    ],
    [
     "Eduardo",
     "Coutinho"
    ],
    [
     "Keelan",
     "Evanini"
    ]
   ],
   "title": "Discussion",
   "original": "abs13",
   "page_count": 0,
   "order": 518,
   "p1": "0",
   "pn": "",
   "abstract": [
    "(No abstract available at the time of publication)\n"
   ]
  },
  "tabain16_interspeech": {
   "authors": [
    [
     "Marija",
     "Tabain"
    ],
    [
     "Richard",
     "Beare"
    ]
   ],
   "title": "A Preliminary Ultrasound Study of Nasal and Lateral Coronals in Arrernte",
   "original": "0568",
   "page_count": 5,
   "order": 519,
   "p1": "2423",
   "pn": "2427",
   "abstract": [
    "Ultrasound tongue image data are presented for two female speakers\nof the Central Australian language Arrernte, focusing on the nasal\nand lateral coronal consonants. These coronal places of articulation\nare: dental, alveolar, retroflex and palatal. It is shown that the\ntongue back is particularly far forward for the palatal consonant,\nand to a lesser extent also for the retroflex consonant. There is a\ngeneral flattening of the tongue for the dental consonants. In addition,\nthe back of the tongue is consistently further forward for the nasal\nconsonants than for the laterals &#8212; this is true for all places\nof articulation. Finally, a double-pivot pattern for the retroflex\narticulations is observed for one speaker, but not for the other.\n"
   ],
   "doi": "10.21437/Interspeech.2016-568"
  },
  "toutios16b_interspeech": {
   "authors": [
    [
     "Asterios",
     "Toutios"
    ],
    [
     "Sajan Goud",
     "Lingala"
    ],
    [
     "Colin",
     "Vaz"
    ],
    [
     "Jangwon",
     "Kim"
    ],
    [
     "John",
     "Esling"
    ],
    [
     "Patricia",
     "Keating"
    ],
    [
     "Matthew",
     "Gordon"
    ],
    [
     "Dani",
     "Byrd"
    ],
    [
     "Louis",
     "Goldstein"
    ],
    [
     "Krishna S.",
     "Nayak"
    ],
    [
     "Shrikanth S.",
     "Narayanan"
    ]
   ],
   "title": "Illustrating the Production of the International Phonetic Alphabet Sounds Using Fast Real-Time Magnetic Resonance Imaging",
   "original": "0605",
   "page_count": 5,
   "order": 520,
   "p1": "2428",
   "pn": "2432",
   "abstract": [
    "Recent advances in real-time magnetic resonance imaging (rtMRI) of\nthe upper airway for acquiring speech production data provide unparalleled\nviews of the dynamics of a speaker&#8217;s vocal tract at very high\nframe rates (83 frames per second and even higher). This paper introduces\nan effort to collect and make available on-line rtMRI data corresponding\nto a large subset of the sounds of the world&#8217;s languages as encoded\nin the International Phonetic Alphabet, with supplementary English\nwords and phonetically-balanced texts, produced by four prominent phoneticians,\nusing the latest rtMRI technology. The technique images oral as well\nas laryngeal articulator movements in the production of each sound\ncategory. This resource is envisioned as a teaching tool in pronunciation\ntraining, second language acquisition, and speech therapy.\n"
   ],
   "doi": "10.21437/Interspeech.2016-605"
  },
  "renwick16_interspeech": {
   "authors": [
    [
     "Margaret E.L.",
     "Renwick"
    ],
    [
     "Ioana",
     "Vasilescu"
    ],
    [
     "Camille",
     "Dutrey"
    ],
    [
     "Lori",
     "Lamel"
    ],
    [
     "Bianca",
     "Vieru"
    ]
   ],
   "title": "Marginal Contrast Among Romanian Vowels: Evidence from ASR and Functional Load",
   "original": "0762",
   "page_count": 5,
   "order": 521,
   "p1": "2433",
   "pn": "2437",
   "abstract": [
    "This work quantifies the phonological contrast between the Romanian\ncentral vowels [&#652;] and [&#616;], which are considered separate\nphonemes, although they are historical allophones with few minimal\npairs. We consider the vowels&#8217; functional load within the Romanian\ninventory and the usefulness of the contrast for automatic speech recognition\n(ASR). Using a 7 hour corpus of automatically aligned broadcast speech,\nthe relative frequencies of vowels are compared across phonological\ncontexts. Results indicate a near complementary distribution of [&#652;]\nand [&#616;]: the contrast scores lowest of all pairwise comparisons\non measures of functional load, and shows the highest Kullback-Leibler\ndivergence, suggesting that few lexical distinctions depend on the\ncontrast. Thereafter, forced alignment is performed using an existing\nASR system. The system selects among [&#616;], [&#652;], &#248; for\nlexical /&#616;/, testing for its reduction in continuous speech. The\nsame data is transcribed using the ASR system where [&#652;]/[&#616;]\nare merged, testing the hypothesis that loss of a marginal contrast\nhas little impact on ASR error rates. Both results are consistent with\nfunctional load calculations, indicating that the /&#652;/-/&#616;/\ncontrast is lexically and phonetically weak. These results show how\nautomatic transcription tools can help test phonological predictions\nusing continuous speech.\n"
   ],
   "doi": "10.21437/Interspeech.2016-762"
  },
  "fan16_interspeech": {
   "authors": [
    [
     "Shuanglin",
     "Fan"
    ],
    [
     "Kiyoshi",
     "Honda"
    ],
    [
     "Jianwu",
     "Dang"
    ],
    [
     "Hui",
     "Feng"
    ]
   ],
   "title": "Effects of Subglottal-Coupling and Interdental-Space on Formant Trajectories During Front-to-Back Vowel Transitions in Chinese",
   "original": "1054",
   "page_count": 5,
   "order": 522,
   "p1": "2438",
   "pn": "2442",
   "abstract": [
    "Discontinuity of the second formant (F2 discontinuity) is often found\nduring back-to-front vowel transitions, and it has been thought due\nto two possible effects: acoustic coupling with the subglottal tract\n(subglottal-coupling effect, SCE) and traveling anti-resonance of the\ninterdental space (interdental-space effect, ISE). Although both are\npossible to appear together, either of the two is common to find in\nmany spectrographic observations, and how to distinguish from one another\nis often puzzling. This study aims at exploring manifestations of the\ntwo effects in Chinese triphthongs through acoustic analysis on front-to-back\nvowel sequences. Test utterances were recorded from five Chinese speakers\nwith simultaneous measurement of subglottal resonance via a vibration\nsensor adhered to their necks. Results revealed that F2 discontinuity\noccurs near the second subglottal formant (SgF2) but not always, and\ndiscontinuity of both F2 and F3 is more common that occurs with a short\ntime lag, suggesting predominance of ISE rather than SCE in the data.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1054"
  },
  "monteserin16b_interspeech": {
   "authors": [
    [
     "Mairym Lloréns",
     "Monteserín"
    ],
    [
     "Shrikanth S.",
     "Narayanan"
    ],
    [
     "Louis",
     "Goldstein"
    ]
   ],
   "title": "Perceptual Lateralization of Coda Rhotic Production in Puerto Rican Spanish",
   "original": "1498",
   "page_count": 5,
   "order": 523,
   "p1": "2443",
   "pn": "2447",
   "abstract": [
    "When speakers of Puerto Rican Spanish (PRS) produce phonemic coda taps,\nSpanish-speakers of other dialects often perceive these as laterals.\nWe observed production of phonemic coda laterals and taps by a male\nPRS speaker in real-time MRI. Temporal and spatial characteristics\nof tongue tip movements during coda liquid production are inconsistent\nwith accounts positing a categorical change from rhotic to lateral\nin coda for this speaker. Perceptual coding of coda tap production\nby na&#239;ve listeners suggests that both preceding vowel type and\nthe relative strength of a proximal prosodic boundary may impact the\nproportion of the subject&#8217;s phonemic taps that received a lateral\npercept. Results are discussed in the context of persistent difficulties\nin modeling the gestural representation of liquid consonants.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1498"
  },
  "yi16_interspeech": {
   "authors": [
    [
     "Hao",
     "Yi"
    ],
    [
     "Sam",
     "Tilsen"
    ]
   ],
   "title": "Interaction Between Lexical Tone and Intonation: An EMA Study",
   "original": "0662",
   "page_count": 5,
   "order": 524,
   "p1": "2448",
   "pn": "2452",
   "abstract": [
    "This paper aims to examine the interaction of intonation and lexical\ntone within the framework of Articulatory Phonology, by investigating\nthe timing relationship between oral articulatory gestures and tone-related/intonation-related\nF0 dynamics. Specifically, we compared the consonant-vowel-F0 (C-V-T)\ncoordinative patterns at phrase-final position and at phrase-medial\nposition. We found that the C-V-T coordination was altered by the presence\nof boundary tones, which is in line with the sequential model in which\ntone and intonation are conceptualized as events that interact at the\nphonological level before the phonetic implementation. However, the\neffect of boundary tone on the C-V-T coordination seemed to be tone-specific.\nMoreover, the presence of pitch accents also influenced the intra-syllabic\nC-V-T coordinative patterns. By presenting evidence from the coordinative\npatterns between articulatory gestures and F0 dynamics, the current\nstudy lent support to the sequential model of the interaction between\nintonation and lexical tone from a gestural perspective.\n"
   ],
   "doi": "10.21437/Interspeech.2016-662"
  },
  "ming16_interspeech": {
   "authors": [
    [
     "Huaiping",
     "Ming"
    ],
    [
     "Dongyan",
     "Huang"
    ],
    [
     "Lei",
     "Xie"
    ],
    [
     "Jie",
     "Wu"
    ],
    [
     "Minghui",
     "Dong"
    ],
    [
     "Haizhou",
     "Li"
    ]
   ],
   "title": "Deep Bidirectional LSTM Modeling of Timbre and Prosody for Emotional Voice Conversion",
   "original": "1053",
   "page_count": 5,
   "order": 525,
   "p1": "2453",
   "pn": "2457",
   "abstract": [
    "Emotional voice conversion aims at converting speech from one emotion\nstate to another. This paper proposes to model timbre and prosody features\nusing a deep bidirectional long short-term memory (DBLSTM) for emotional\nvoice conversion. A continuous wavelet transform (CWT) representation\nof fundamental frequency (F0) and energy contour are used for prosody\nmodeling. Specifically, we use CWT to decompose F0 into a five-scale\nrepresentation, and decompose energy contour into a ten-scale representation,\nwhere each feature scale corresponds to a temporal scale. Both spectrum\nand prosody (F0 and energy contour) features are simultaneously converted\nby a sequence to sequence conversion method with DBLSTM model, which\ncaptures both frame-wise and long-range relationship between source\nand target voice. The converted speech signals are evaluated both objectively\nand subjectively, which confirms the effectiveness of the proposed\nmethod.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1053"
  },
  "thangthai16_interspeech": {
   "authors": [
    [
     "Ausdang",
     "Thangthai"
    ],
    [
     "Ben",
     "Milner"
    ],
    [
     "Sarah",
     "Taylor"
    ]
   ],
   "title": "Visual Speech Synthesis Using Dynamic Visemes, Contextual Features and DNNs",
   "original": "1084",
   "page_count": 5,
   "order": 526,
   "p1": "2458",
   "pn": "2462",
   "abstract": [
    "This paper examines methods to improve visual speech synthesis from\na text input using a deep neural network (DNN). Two representations\nof the input text are considered, namely into phoneme sequences or\ndynamic viseme sequences. From these sequences, contextual features\nare extracted that include information at varying linguistic levels,\nfrom frame level down to the utterance level. These are extracted from\na broad sliding window that captures context and produces features\nthat are input into the DNN to estimate visual features. Experiments\nfirst compare the accuracy of these visual features against an HMM\nbaseline method which establishes that both the phoneme and dynamic\nviseme systems perform better with best performance obtained by a combined\nphoneme-dynamic viseme system. An investigation into the features then\nreveals the importance of the frame level information which is able\nto avoid discontinuities in the visual feature sequence and produces\na smooth and realistic output.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1084"
  },
  "ronanki16_interspeech": {
   "authors": [
    [
     "Srikanth",
     "Ronanki"
    ],
    [
     "Gustav Eje",
     "Henter"
    ],
    [
     "Zhizheng",
     "Wu"
    ],
    [
     "Simon",
     "King"
    ]
   ],
   "title": "A Template-Based Approach for Speech Synthesis Intonation Generation Using LSTMs",
   "original": "0096",
   "page_count": 5,
   "order": 527,
   "p1": "2463",
   "pn": "2467",
   "abstract": [
    "The absence of convincing intonation makes current parametric speech\nsynthesis systems sound dull and lifeless, even when trained on expressive\nspeech data. Typically, these systems use regression techniques to\npredict the fundamental frequency (F0) frame-by-frame. This approach\nleads to overly-smooth pitch contours and fails to construct an appropriate\nprosodic structure across the full utterance. In order to capture and\nreproduce larger-scale pitch patterns, this paper proposes a template-based\napproach for automatic F0 generation, where per-syllable pitch-contour\ntemplates (from a small, automatically learned set) are predicted by\na recurrent neural network (RNN). The use of syllable templates mitigates\nthe over-smoothing problem and is able to reproduce pitch patterns\nobserved in the data. The use of an RNN, paired with connectionist\ntemporal classification (CTC), enables the prediction of structure\nin the pitch contour spanning the entire utterance. This novel F0 prediction\nsystem is used alongside separate LSTMs for predicting phone durations\nand the other acoustic features, to construct a complete text-to-speech\nsystem. We report the results of objective and subjective tests on\nan expressive speech corpus of children&#8217;s audiobooks, and include\ncomparisons to a conventional baseline that predicts F0 directly at\nthe frame level.\n"
   ],
   "doi": "10.21437/Interspeech.2016-96"
  },
  "li16g_interspeech": {
   "authors": [
    [
     "Bo",
     "Li"
    ],
    [
     "Heiga",
     "Zen"
    ]
   ],
   "title": "Multi-Language Multi-Speaker Acoustic Modeling for LSTM-RNN Based Statistical Parametric Speech Synthesis",
   "original": "0172",
   "page_count": 5,
   "order": 528,
   "p1": "2468",
   "pn": "2472",
   "abstract": [
    "Building text-to-speech (TTS) systems requires large amounts of high\nquality speech recordings and annotations, which is a challenge to\ncollect especially considering the variation in spoken languages around\nthe world. Acoustic modeling techniques that could utilize inhomogeneous\ndata are hence important as they allow us to pool more data for training.\nThis paper presents a long short-term memory (LSTM) recurrent neural\nnetwork (RNN) based statistical parametric speech synthesis system\nthat uses data from multiple languages and speakers. It models language\nvariation through cluster adaptive training and speaker variation with\nspeaker dependent output layers. Experimental results have shown that\nthe proposed multilingual TTS system can synthesize speech in multiple\nlanguages from a single model while maintaining naturalness. Furthermore,\nit can be adapted to new languages with only a small amount of data.\n"
   ],
   "doi": "10.21437/Interspeech.2016-172"
  },
  "airaksinen16b_interspeech": {
   "authors": [
    [
     "Manu",
     "Airaksinen"
    ],
    [
     "Bajibabu",
     "Bollepalli"
    ],
    [
     "Lauri",
     "Juvela"
    ],
    [
     "Zhizheng",
     "Wu"
    ],
    [
     "Simon",
     "King"
    ],
    [
     "Paavo",
     "Alku"
    ]
   ],
   "title": "GlottDNN &#8212; A Full-Band Glottal Vocoder for Statistical Parametric Speech Synthesis",
   "original": "0342",
   "page_count": 5,
   "order": 529,
   "p1": "2473",
   "pn": "2477",
   "abstract": [
    "GlottHMM is a previously developed vocoder that has been successfully\nused in HMM-based synthesis by parameterizing speech into two parts\n(glottal flow, vocal tract) according to the functioning of the real\nhuman voice production mechanism. In this study, a new glottal vocoding\nmethod, GlottDNN, is proposed. The GlottDNN vocoder is built on the\nprinciples of its predecessor, GlottHMM, but the new vocoder introduces\nthree main improvements: GlottDNN (1) takes advantage of a new, more\naccurate glottal inverse filtering method, (2) uses a new method of\ndeep neural network (DNN) -based glottal excitation generation, and\n(3) proposes a new approach of band-wise processing of full-band speech.<br\nstyle=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  The proposed GlottDNN vocoder was evaluated as part of a full-band\nstate-of-the-art DNN-based text-to-speech (TTS) synthesis system, and\ncompared against the release version of the original GlottHMM vocoder,\nand the well-known STRAIGHT vocoder. The results of the subjective\nlistening test indicate that GlottDNN improves the TTS quality over\nthe compared methods.\n"
   ],
   "doi": "10.21437/Interspeech.2016-342"
  },
  "nishimura16_interspeech": {
   "authors": [
    [
     "Masanari",
     "Nishimura"
    ],
    [
     "Kei",
     "Hashimoto"
    ],
    [
     "Keiichiro",
     "Oura"
    ],
    [
     "Yoshihiko",
     "Nankaku"
    ],
    [
     "Keiichi",
     "Tokuda"
    ]
   ],
   "title": "Singing Voice Synthesis Based on Deep Neural Networks",
   "original": "1027",
   "page_count": 5,
   "order": 530,
   "p1": "2478",
   "pn": "2482",
   "abstract": [
    "Singing voice synthesis techniques have been proposed based on a hidden\nMarkov model (HMM). In these approaches, the spectrum, excitation,\nand duration of singing voices are simultaneously modeled with context-dependent\nHMMs and waveforms are generated from the HMMs themselves. However,\nthe quality of the synthesized singing voices still has not reached\nthat of natural singing voices. Deep neural networks (DNNs) have largely\nimproved on conventional approaches in various research areas including\nspeech recognition, image recognition, speech synthesis, etc. The DNN-based\ntext-to-speech (TTS) synthesis can synthesize high quality speech.\nIn the DNN-based TTS system, a DNN is trained to represent the mapping\nfunction from contextual features to acoustic features, which are modeled\nby decision tree-clustered context dependent HMMs in the HMM-based\nTTS system. In this paper, we propose singing voice synthesis based\non a DNN and evaluate its effectiveness. The relationship between the\nmusical score and its acoustic features is modeled in frames by a DNN.\nFor the sparseness of pitch context in a database, a musical-note-level\npitch normalization and linear-interpolation techniques are used to\nprepare the excitation features. Subjective experimental results show\nthat the DNN-based system outperformed the HMM-based system in terms\nof naturalness.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1027"
  },
  "backstrom16_interspeech": {
   "authors": [
    [
     "Tom",
     "Bäckström"
    ],
    [
     "Florin",
     "Ghido"
    ],
    [
     "Johannes",
     "Fischer"
    ]
   ],
   "title": "Blind Recovery of Perceptual Models in Distributed Speech and Audio Coding",
   "original": "0027",
   "page_count": 5,
   "order": 531,
   "p1": "2483",
   "pn": "2487",
   "abstract": [
    "A central part of speech and audio codecs are their perceptual models,\nwhich describe the relative perceptual importance of errors in different\nelements of the signal representation. In practice, the perceptual\nmodels consists of signal-dependent weighting factors which are used\nin quantization of each element. For optimal performance, we would\nlike to use the same perceptual model at the decoder. While the perceptual\nmodel is signal-dependent, however, it is not known in advance at the\ndecoder, whereby audio codecs generally transmit this model explicitly,\nat the cost of increased bit-consumption. In this work we present an\nalternative method which recovers the perceptual model at the decoder\nfrom the transmitted signal without any side-information. The approach\nwill be especially useful in distributed sensor-networks and the Internet\nof things, where the added cost on bit-consumption from transmitting\na perceptual model increases with the number of sensors.\n"
   ],
   "doi": "10.21437/Interspeech.2016-27"
  },
  "tang16c_interspeech": {
   "authors": [
    [
     "Yan",
     "Tang"
    ],
    [
     "Martin",
     "Cooke"
    ]
   ],
   "title": "Glimpse-Based Metrics for Predicting Speech Intelligibility in Additive Noise Conditions",
   "original": "0014",
   "page_count": 5,
   "order": 532,
   "p1": "2488",
   "pn": "2492",
   "abstract": [
    "The glimpsing model of speech perception in noise operates by recognising\nthose speech-dominant spectro-temporal regions, or glimpses, that survive\nenergetic masking; hence, a speech recognition component is an integral\npart of the model. The current study evaluates whether a simpler family\nof metrics based solely on quantifying the amount of supra-threshold\ntarget speech available after energetic masking can account for subjective\nintelligibility. The predictive power of glimpse-based metrics is compared\nfor natural, processed and synthetic speech in the presence of stationary\nand fluctuating maskers. These metrics are raw glimpse proportion,\nextended glimpse proportion, and two further refinements: one, FMGP,\nincorporates a component simulating the effect of forward masking;\nthe other, HEGP, selects speech-dominant spectro-temporal regions with\nabove-average energy on the noisy speech. The metrics are compared\nalongside a state-of-the-art non-glimpsing metric, using three large\ndatasets of listener scores. Both FMGP and HEGP equal or improve upon\nthe predictive power of the raw and extended metrics, with across-masker\ncorrelations ranging from 0.81&#8211;0.92; both metrics equal or exceed\nthe state-of-the-art metric in all conditions. These outcomes suggests\nthat easily-computed measures of unmasked, supra-threshold speech can\nserve as robust proxies for intelligibility across a range of speech\nstyles and additive masking conditions.\n"
   ],
   "doi": "10.21437/Interspeech.2016-14"
  },
  "koster16_interspeech": {
   "authors": [
    [
     "Friedemann",
     "Köster"
    ],
    [
     "Sebastian",
     "Möller"
    ]
   ],
   "title": "Analyzing the Relation Between Overall Quality and the Quality of Individual Phases in a Telephone Conversation",
   "original": "0255",
   "page_count": 5,
   "order": 533,
   "p1": "2493",
   "pn": "2497",
   "abstract": [
    "Assessing and analyzing the quality of transmitted speech in a conversational\nsituation is an important topic in current research. For this, a conversation\nhas been separated into three individual conversational phases (listening,\nspeaking, and interaction), and for each phase corresponding quality-relevant\nperceptual dimensions have been identified. The dimensions can be used\nto determine the quality of each phase, and the qualities of all phases,\nin turn, can be combined for overall conversational quality estimation.\nIn this article we present the work that has been conducted to identify\nthe weights of the individual phases for the overall quality. For this,\nwe conducted an experiment that allows the participants to perceive\neach phase separately and to gather the overall quality as well as\nthe quality ratings for each individual phase. The results enable to\ncreate a linear model to predict the overall quality on the basis of\nthe three phases. This allows to draw first conclusions regarding the\nrelation between the individual phases and the overall quality and\nprovides a major landmark towards a diagnostic assessment of conversational\nquality.\n"
   ],
   "doi": "10.21437/Interspeech.2016-255"
  },
  "jokinen16_interspeech": {
   "authors": [
    [
     "Emma",
     "Jokinen"
    ],
    [
     "Paavo",
     "Alku"
    ]
   ],
   "title": "Intelligibility Enhancement at the Receiving End of the Speech Transmission System &#8212; Effects of Far-End Noise Reduction",
   "original": "0144",
   "page_count": 5,
   "order": 534,
   "p1": "2498",
   "pn": "2502",
   "abstract": [
    "Post-processing methods can be used in mobile communications to improve\nthe intelligibility of speech in adverse near-end background noise\nconditions. Generally, it is assumed that the input of the post-processing\ncontains quantization noise only, that is to say, no far-end noise\nis present. However, this assumption is not entirely realistic. Therefore,\nthe effect of far-end noise with and without noise reduction on the\nperformance of three post-processing methods is studied in this investigation.\nThe performance evaluation is done using subjective intelligibility\nand quality tests in several far-end and near-end noise conditions.\nThe results suggest that although the noise reduction generally improves\nperformance in stationary far-end noise, the noise reduction does not\nimprove intelligibility in unstationary far-end noise conditions but\nhas a positive impact on perceptual quality for some of the post-processing\nmethods.\n"
   ],
   "doi": "10.21437/Interspeech.2016-144"
  },
  "ganzeboom16_interspeech": {
   "authors": [
    [
     "Mario",
     "Ganzeboom"
    ],
    [
     "Marjoke",
     "Bakker"
    ],
    [
     "Catia",
     "Cucchiarini"
    ],
    [
     "Helmer",
     "Strik"
    ]
   ],
   "title": "Intelligibility of Disordered Speech: Global and Detailed Scores",
   "original": "1448",
   "page_count": 5,
   "order": 535,
   "p1": "2503",
   "pn": "2507",
   "abstract": [
    "Measuring the intelligibility of disordered speech is a common practice\nin both clinical and research contexts. Over the years various methods\nhave been proposed and studied, including methods relying on subjective\nratings by human judges, and objective methods based on speech technology.\nMany of these methods measure speech intelligibility at the speaker\nor utterance level. While this may be satisfactory for some purposes,\nmore detailed evaluations might be required in other cases such as\ndiagnosis and measuring or comparing the outcomes of different types\nof therapy (by humans or computer programs). In the current paper we\ninvestigate intelligibility ratings at three different levels of granularity:\nutterance, word, and subword level. In a web experiment 50 speech fragments\nproduced by seven dysarthric speakers were rated by 36 listeners in\nthree ways: a score per utterance on a Visual Analogue and a Likert\nscale, and an orthographic transcription. The latter was used to obtain\nword and subword (grapheme and phoneme) level ratings using automatic\nalignment and conversion methods. The implemented phoneme scoring method\nproved feasible, reliable, and provided a more sensitive and informative\nmeasure of intelligibility. Possible implications for clinical practice\nand research are discussed.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1448"
  },
  "koutsogiannaki16_interspeech": {
   "authors": [
    [
     "Maria",
     "Koutsogiannaki"
    ],
    [
     "Yannis",
     "Stylianou"
    ]
   ],
   "title": "Modulation Enhancement of Temporal Envelopes for Increasing Speech Intelligibility in Noise",
   "original": "0500",
   "page_count": 5,
   "order": 536,
   "p1": "2508",
   "pn": "2512",
   "abstract": [
    "In this paper, speech intelligibility is enhanced by manipulating the\nmodulation spectrum of the signal. First, the signal is decomposed\ninto Amplitude Modulation (AM) and Frequency Modulation (FM) components\nusing a high resolution adaptive quasi-harmonic model of speech. Then,\nthe AM part of midrange frequencies of speech spectrum is modified\nby applying a transforming function which follows the characteristics\nof the clear style of speaking. This results in increasing the modulation\ndepth of the temporal envelopes of casual speech as in clear speech.\nThe modified AM components of speech are then combined with the original\nFM parts to synthesize the final processed signal. Subjective listening\ntests evaluating the intelligibility of speech in noise showed that\nthe suggested approach increases the intelligibility of speech by 40%\non average, while it is comparable with recently suggested state-of-the-art\nalgorithms of intelligibility boosters.\n"
   ],
   "doi": "10.21437/Interspeech.2016-500"
  },
  "niehues16_interspeech": {
   "authors": [
    [
     "Jan",
     "Niehues"
    ],
    [
     "Thai Son",
     "Nguyen"
    ],
    [
     "Eunah",
     "Cho"
    ],
    [
     "Thanh-Le",
     "Ha"
    ],
    [
     "Kevin",
     "Kilgour"
    ],
    [
     "Markus",
     "Müller"
    ],
    [
     "Matthias",
     "Sperber"
    ],
    [
     "Sebastian",
     "Stüker"
    ],
    [
     "Alex",
     "Waibel"
    ]
   ],
   "title": "Dynamic Transcription for Low-Latency Speech Translation",
   "original": "0154",
   "page_count": 5,
   "order": 537,
   "p1": "2513",
   "pn": "2517",
   "abstract": [
    "Latency is one of the main challenges in the task of simultaneous spoken\nlanguage translation. While significant improvements in recent years\nhave led to high quality automatic translations, their usefulness in\nreal-time settings is still severely limited due to the large delay\nbetween the input speech and the delivered translation.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  In this paper, we\npresent a novel scheme which reduces the latency of a large scale speech\ntranslation system drastically. Within this scheme, the transcribed\ntext and its translation can be updated when more context is available,\neven after they are presented to the user. Thereby, this scheme allows\nus to display an initial transcript and its translation to the user\nwith a very low latency. If necessary, both transcript and translation\ncan later be updated to better, more accurate versions until eventually\nthe final versions are displayed. Using this framework, we are able\nto reduce the latency of the source language transcript into half.\nFor the translation, an average delay of 3.3s was achieved, which is\nmore than twice as fast as our initial system.\n"
   ],
   "doi": "10.21437/Interspeech.2016-154"
  },
  "adams16_interspeech": {
   "authors": [
    [
     "Oliver",
     "Adams"
    ],
    [
     "Graham",
     "Neubig"
    ],
    [
     "Trevor",
     "Cohn"
    ],
    [
     "Steven",
     "Bird"
    ]
   ],
   "title": "Learning a Translation Model from Word Lattices",
   "original": "0862",
   "page_count": 5,
   "order": 538,
   "p1": "2518",
   "pn": "2522",
   "abstract": [
    "Translation models have been used to improve automatic speech recognition\nwhen speech input is paired with a written translation, primarily for\nthe task of computer-aided translation. Existing approaches require\nlarge amounts of parallel text for training the translation models,\nbut for many language pairs this data is not available. We propose\na model for learning lexical translation parameters directly from the\nword lattices for which a transcription is sought. The model is expressed\nthrough composition of each lattice with a weighted finite-state transducer\nrepresenting the translation model, where inference is performed by\nsampling paths through the composed finite-state transducer. We show\nconsistent word error rate reductions in two datasets, using between\njust 20 minutes and 4 hours of speech input, additionally outperforming\na translation model trained on the 1-best path.\n"
   ],
   "doi": "10.21437/Interspeech.2016-862"
  },
  "zayats16_interspeech": {
   "authors": [
    [
     "Vicky",
     "Zayats"
    ],
    [
     "Mari",
     "Ostendorf"
    ],
    [
     "Hannaneh",
     "Hajishirzi"
    ]
   ],
   "title": "Disfluency Detection Using a Bidirectional LSTM",
   "original": "1247",
   "page_count": 5,
   "order": 539,
   "p1": "2523",
   "pn": "2527",
   "abstract": [
    "We introduce a new approach for disfluency detection using a Bidirectional\nLong-Short Term Memory neural network (BLSTM). In addition to the word\nsequence, the model takes as input pattern match features that were\ndeveloped to reduce sensitivity to vocabulary size in training, which\nlead to improved performance over the word sequence alone. The BLSTM\ntakes advantage of explicit repair states in addition to the standard\nreparandum states. The final output leverages integer linear programming\nto incorporate constraints of disfluency structure. In experiments\non the Switchboard corpus, the model achieves state-of-the-art performance\nfor both the standard disfluency detection task and the correction\ndetection task. Analysis shows that the model has better detection\nof non-repetition disfluencies, which tend to be much harder to detect.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1247"
  },
  "che16_interspeech": {
   "authors": [
    [
     "Xiaoyin",
     "Che"
    ],
    [
     "Sheng",
     "Luo"
    ],
    [
     "Haojin",
     "Yang"
    ],
    [
     "Christoph",
     "Meinel"
    ]
   ],
   "title": "Sentence Boundary Detection Based on Parallel Lexical and Acoustic Models",
   "original": "0257",
   "page_count": 5,
   "order": 540,
   "p1": "2528",
   "pn": "2532",
   "abstract": [
    "In this paper we propose a solution that detects sentence boundary\nfrom speech transcript. First we train a pure lexical model with deep\nneural network, which takes word vectors as the only input feature.\nThen a simple acoustic model is also prepared. Because the models work\nindependently, they can be trained with different data. In next step,\nthe posterior probabilities of both lexical and acoustic models will\nbe involved in a heuristic 2-stage joint decision scheme to classify\nthe sentence boundary positions. This approach ensures that the models\ncan be updated or switched freely in actual use. Evaluation on TED\nTalks shows that the proposed lexical model can achieve good results:\n75.5% accuracy on error-involved ASR transcripts and 82.4% on error-free\nmanual references. The joint decision scheme can further improve the\naccuracy by 3&#126;10% when acoustic data is available.\n"
   ],
   "doi": "10.21437/Interspeech.2016-257"
  },
  "do16_interspeech": {
   "authors": [
    [
     "Quoc Truong",
     "Do"
    ],
    [
     "Sakriani",
     "Sakti"
    ],
    [
     "Graham",
     "Neubig"
    ],
    [
     "Satoshi",
     "Nakamura"
    ]
   ],
   "title": "Transferring Emphasis in Speech Translation Using Hard-Attentional Neural Network Models",
   "original": "0898",
   "page_count": 5,
   "order": 541,
   "p1": "2533",
   "pn": "2537",
   "abstract": [
    "While traditional speech translation systems are oblivious to paralinguistic\ninformation, there has been a recent focus on speech translation systems\nthat transfer not only the linguistic content but also emphasis information\nacross languages. A recent work has tried to tackle this task by developing\na method for mapping emphasis between languages utilizing conditional\nrandom fields (CRFs). Although CRFs allow for consideration of rich\nfeatures and local context, they have difficulty in handling continuous\nvariables, and cannot capture long-distance dependencies easily. In\nthis paper, we propose a new model for emphasis transfer in speech\ntranslation using an approach based on neural networks. The proposed\nmodel can handle long-distance dependencies by using long short-term\nmemory (LSTM) neural networks, and is able to handle continuous emphasis\nvalues through a novel hard-attention mechanism, which uses word alignments\nto decide which emphasis values to map from the source to the target\nsentence. Our experiments on the emphasis translation task showed a\nsignificant improvement of the proposed model over the previous state-of-the-art\nmodel by 4% target-language emphasis prediction F-measure according\nto objective evaluation and 2% F-measure according to subjective evaluation.\n"
   ],
   "doi": "10.21437/Interspeech.2016-898"
  },
  "le16_interspeech": {
   "authors": [
    [
     "Ngoc-Tien",
     "Le"
    ],
    [
     "Christophe",
     "Servan"
    ],
    [
     "Benjamin",
     "Lecouteux"
    ],
    [
     "Laurent",
     "Besacier"
    ]
   ],
   "title": "Better Evaluation of ASR in Speech Translation Context Using Word Embeddings",
   "original": "0464",
   "page_count": 5,
   "order": 542,
   "p1": "2538",
   "pn": "2542",
   "abstract": [
    "This paper investigates the evaluation of ASR in spoken language translation\ncontext. More precisely, we propose a simple extension of WER metric\nin order to penalize differently substitution errors according to their\ncontext using word embeddings. For instance, the proposed metric should\ncatch near matches (mainly morphological variants) and penalize less\nthis kind of error which has a more limited impact on translation performance.\nOur experiments show that the correlation of the new proposed metric\nwith SLT performance is better than the one of WER. Oracle experiments\nare also conducted and show the ability of our metric to find better\nhypotheses (to be translated) in the ASR N-best. Finally, a preliminary\nexperiment where ASR tuning is based on our new metric shows encouraging\nresults. For reproducible experiments, the code allowing to call our\nmodified WER and the corpora used are made available to the research\ncommunity.\n"
   ],
   "doi": "10.21437/Interspeech.2016-464"
  },
  "korse16_interspeech": {
   "authors": [
    [
     "Srikanth",
     "Korse"
    ],
    [
     "Tobias",
     "Jähnel"
    ],
    [
     "Tom",
     "Bäckström"
    ]
   ],
   "title": "Entropy Coding of Spectral Envelopes for Speech and Audio Coding Using Distribution Quantization",
   "original": "0055",
   "page_count": 5,
   "order": 543,
   "p1": "2543",
   "pn": "2547",
   "abstract": [
    "Speech and audio codecs model the overall shape of the signal spectrum\nusing envelope models. In speech coding the predominant approach is\nlinear predictive coding, which offers high coding efficiency at the\ncost of computational complexity and a rigid systems design. Audio\ncodecs are usually based on scale factor bands, whose calculation and\ncoding is simple, but whose coding efficiency is lower than that of\nlinear prediction. In the current work we propose an entropy coding\napproach for scale factor bands, with the objective of reaching the\nsame coding efficiency as linear prediction, but simultaneously retaining\na low computational complexity. The proposed method is based on quantizing\nthe distribution of spectral mass using beta-distributions. Our experiments\nshow that the perceptual quality achieved with the proposed method\nis similar to that of linear predictive models with the same bit rate,\nwhile the design simultaneously allows variable bit-rate coding and\ncan easily be scaled to different sampling rates. The algorithmic complexity\nof the proposed method is less than one third of traditional multi-stage\nvector quantization of linear predictive envelopes.\n"
   ],
   "doi": "10.21437/Interspeech.2016-55"
  },
  "villette16_interspeech": {
   "authors": [
    [
     "Stéphane",
     "Villette"
    ],
    [
     "Sen",
     "Li"
    ],
    [
     "Pravin",
     "Ramadas"
    ],
    [
     "Daniel J.",
     "Sinder"
    ]
   ],
   "title": "An Objective Evaluation Methodology for Blind Bandwidth Extension",
   "original": "1595",
   "page_count": 5,
   "order": 544,
   "p1": "2548",
   "pn": "2552",
   "abstract": [
    "In this paper we introduce an objective evaluation methodology for\nBlind Bandwidth Extension (BBE) algorithms. The methodology combines\nan objective method, POLQA, with a bandwidth requirement, based on\na frequency mask. We compare its results to subjective test data, and\nshow that it gives consistent results across several bandwidth extension\nalgorithms. Additionally, we show that our latest BBE algorithm achieves\nquality similar to AMR-WB at 8.85 kbps, using both subjective and objective\nevaluation methods.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1595"
  },
  "ramo16_interspeech": {
   "authors": [
    [
     "Anssi",
     "Rämö"
    ],
    [
     "Antti",
     "Kurittu"
    ],
    [
     "Henri",
     "Toukomaa"
    ]
   ],
   "title": "EVS Channel Aware Mode Robustness to Frame Erasures",
   "original": "0917",
   "page_count": 5,
   "order": 545,
   "p1": "2553",
   "pn": "2557",
   "abstract": [
    "This paper discusses the voice and audio quality characteristics of\nthe EVS, the recently standardized 3GPP Enhanced Voice Services codec.\nEspecially frame erasure conditions with and without channel aware\nmode were evaluated. The test consisted of two extended range MOS listening\ntests. The tests contained both clean and noisy speech in clean channel\nas well as with four frame erasure rates (5%, 10%, 20% and 30%) for\nselected codecs and bitrates. In addition to subjective test results\nsome additional objective results are presented. The results show that\nEVS channel aware mode performs better than EVS native mode in high\nFER rates. For comparison also AMR, AMR-WB and Opus codecs were included\nto the listening tests.\n"
   ],
   "doi": "10.21437/Interspeech.2016-917"
  },
  "pirhosseinloo16_interspeech": {
   "authors": [
    [
     "Shadi",
     "Pirhosseinloo"
    ],
    [
     "Kostas",
     "Kokkinakis"
    ]
   ],
   "title": "An Interaural Magnification Algorithm for Enhancement of Naturally-Occurring Level Differences",
   "original": "1049",
   "page_count": 4,
   "order": 546,
   "p1": "2558",
   "pn": "2561",
   "abstract": [
    "In this work, we describe an interaural magnification algorithm for\nspeech enhancement in noise and reverberation. The proposed algorithm\noperates by magnifying the interaural level differences corresponding\nto the interfering sound source. The enhanced signal outputs are estimated\nby processing the signal inputs with the interaurally-magnified head-related\ntransfer functions. Experimental results with speech masked by a single\ninterfering source in anechoic and reverberant scenarios indicate that\nthe proposed algorithm yields an increased benefit due to spatial release\nfrom masking and a much higher perceived speech quality.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1049"
  },
  "kayser16_interspeech": {
   "authors": [
    [
     "Hendrik",
     "Kayser"
    ],
    [
     "Niko",
     "Moritz"
    ],
    [
     "Jörn",
     "Anemüller"
    ]
   ],
   "title": "Probabilistic Spatial Filter Estimation for Signal Enhancement in Multi-Channel Automatic Speech Recognition",
   "original": "1340",
   "page_count": 5,
   "order": 547,
   "p1": "2562",
   "pn": "2566",
   "abstract": [
    "Speech recognition in multi-channel environments requires target speaker\nlocalization, multi-channel signal enhancement and robust speech recognition.\nWe here propose a system that addresses these problems: Localization\nis performed with a recently introduced probabilistic localization\nmethod that is based on support-vector machine learning of GCC-PHAT\nweights and that estimates a spatial source probability map. The main\ncontribution of the present work is the introduction of a probabilistic\napproach to (re-)estimation of location-specific steering vectors based\non weighting of observed inter-channel phase differences with the spatial\nsource probability map derived in the localization step. Subsequent\nspeech recognition is carried out with a DNN-HMM system using amplitude\nmodulation filter bank (AMFB) acoustic features which are robust to\nspectral distortions introduced during spatial filtering.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  The system has been\nevaluated on the CHIME-3 multi-channel ASR dataset. Recognition was\ncarried out with and without probabilistic steering vector re-estimation\nand with MVDR and delay-and-sum beamforming, respectively. Results\nindicate that the system attains on real-world evaluation data a relative\nimprovement of 31.98% over the baseline and of 21.44% over a modified\nbaseline. We note that this improvement is achieved without exploiting\noracle knowledge about speech/non-speech intervals for noise covariance\nestimation (which is, however, assumed for baseline processing).\n"
   ],
   "doi": "10.21437/Interspeech.2016-1340"
  },
  "ji16_interspeech": {
   "authors": [
    [
     "Youna",
     "Ji"
    ],
    [
     "Young-cheol",
     "Park"
    ]
   ],
   "title": "Improved  a priori SAP Estimator in Complex Noisy Environment for Dual Channel Microphone System",
   "original": "0894",
   "page_count": 5,
   "order": 548,
   "p1": "2567",
   "pn": "2571",
   "abstract": [
    "In this paper,  a priori speech absence probability (SAP) estimator\nis proposed for accurately obtaining the speech presence probability\n(SPP) in a complex noise field. Unlike previous techniques, the proposed\nestimator considers a complex noise sound field where the target speech\nis corrupted by a coherent interference with diffuse noise around.\nThe proposed algorithm estimates  a priori SAP based on the normalized\nspeech to interference plus diffuse noise ratio (SINR) being expressed\nin terms of the speech to interference ratio (SIR) and the directional\nto diffuse noise ratio (DDR). The SIR is obtained from a quadratic\nequation of the magnitude-squared coherence (MSC) between two microphone\nsignals. A performance comparison with several advanced  a priori SAP\nestimators was conducted in terms of the receiver operating characteristic\n(ROC) curve. The proposed algorithm attains a correct detection rate\nat a given false-alarm rate that is higher than those attained by conventional\nalgorithms.\n"
   ],
   "doi": "10.21437/Interspeech.2016-894"
  },
  "cheong16_interspeech": {
   "authors": [
    [
     "Kah-Meng",
     "Cheong"
    ],
    [
     "Yuh-Yuan",
     "Wang"
    ],
    [
     "Tai-Shih",
     "Chi"
    ]
   ],
   "title": "A Spectral Modulation Sensitivity Weighted Pre-Emphasis Filter for Active Noise Control System",
   "original": "0757",
   "page_count": 5,
   "order": 549,
   "p1": "2572",
   "pn": "2576",
   "abstract": [
    "Psychoacoustic active noise control (ANC) systems by considering human\nhearing thresholds in different frequency bands were developed in the\npast. Besides the frequency sensitivity, human hearing also shows different\nsensitivity to spectral and temporal modulations of the sound. In this\npaper, we propose a new psychoacoustic active noise control system\nby further considering the spectral modulation sensitivity of human\nhearing. In addition to the sound pressure level (SPL), the loudness\nlevel is also objectively assessed to evaluate the noise reduction\nperformance of the proposed ANC system. Simulation results demonstrate\nthe proposed system outperforms two compared systems under test conditions\nof narrowband and broadband noise in terms of the loudness level. The\nproposed algorithm has been validated on TI C6713 DSP platform for\nreal-time process.\n"
   ],
   "doi": "10.21437/Interspeech.2016-757"
  },
  "sreeram16_interspeech": {
   "authors": [
    [
     "Ganji",
     "Sreeram"
    ],
    [
     "Rohit",
     "Sinha"
    ]
   ],
   "title": "Semi-Coupled Dictionary Based Automatic Bandwidth Extension Approach for Enhancing Children&#8217;s ASR",
   "original": "0798",
   "page_count": 5,
   "order": 550,
   "p1": "2577",
   "pn": "2581",
   "abstract": [
    "The work presented in this paper is motivated by our earlier work exploring\nsparse representation based approach for automatic bandwidth extension\n(ABWE) of speech signals. In that work, two dictionaries one for voiced\nand the other for unvoiced speech frames are created using KSVD algorithm\non wideband data. Each of the atoms of these dictionaries is then decimated\nand interpolated by a factor of 2 to generate narrowband interpolated\n(NBI) dictionaries whose atoms have one-to-one correspondence with\nthose of the WB dictionaries. The given narrowband speech frames are\nalso interpolated to generated NBI targets and those are sparse coded\nover the NBI dictionaries. The resulting sparse codes are then applied\nto the WB dictionaries to estimate the WB target data. In this work,\nwe extend the said approach by making use of an existing semi-coupled\ndictionary learning (SCDL) algorithm. Unlike the direct dictionary\nlearning, the SCDL algorithm also learns a set of bidirectional transforms\ncoupling the dictionaries more flexibly. The bandwidth enhanced speech\nobtained employing the SCDL approach and a modified high/low band gain\nadjustment yields significant improvements in terms of speech quality\nmeasures as well as in the context of children&#8217;s mismatched speech\nrecognition.\n"
   ],
   "doi": "10.21437/Interspeech.2016-798"
  },
  "bonada16b_interspeech": {
   "authors": [
    [
     "Jordi",
     "Bonada"
    ],
    [
     "Robert",
     "Lachlan"
    ],
    [
     "Merlijn",
     "Blaauw"
    ]
   ],
   "title": "Bird Song Synthesis Based on Hidden Markov Models",
   "original": "1110",
   "page_count": 5,
   "order": 551,
   "p1": "2582",
   "pn": "2586",
   "abstract": [
    "This paper focuses on the synthesis of bird songs using Hidden Markov\nModels (HMM). This technique has been widely used for speech modeling\nand synthesis. However, features and contextual factors typically used\nfor human speech are not appropriate for modeling bird songs. Moreover,\nwhile for speech we can easily control the content of the recordings,\nthis is not the case for bird songs, where we have to rely on the spontaneous\nsinging of the animal. In this work we briefly overview the characteristics\nof bird songs, compare them to speech, and propose strategies for adapting\nthe widely-used HTS (HMM-based Speech Synthesis System) framework to\nmodel and synthesize bird songs. In particular, we focus on Chaffinch\nspecies and a database of recordings of several song bouts of one male\nbird. At the end we discuss the synthesis results obtained.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1110"
  },
  "kaewtip16_interspeech": {
   "authors": [
    [
     "Kantapon",
     "Kaewtip"
    ],
    [
     "Charles",
     "Taylor"
    ],
    [
     "Abeer",
     "Alwan"
    ]
   ],
   "title": "Noise-Robust Hidden Markov Models for Limited Training Data for Within-Species Bird Phrase Classification",
   "original": "1360",
   "page_count": 5,
   "order": 552,
   "p1": "2587",
   "pn": "2591",
   "abstract": [
    "Hidden Markov Models (HMMs) have been studied and used extensively\nin speech and birdsong recognition, but they are not robust to limited\ntraining data and noise. This paper presents two novel approaches to\ntraining continuous and discrete HMMs with extremely limited data.\nFirst, the algorithm learns the global Gaussian Mixture Models (GMMs)\nfor all training phrases available. GMM parameters are then used to\ninitialize state parameters of each individual model. For the GMM-HMM\nframework, the number of states and the mixture components for each\nstate are determined by the acoustic variation of each phrase type.\nThe (high-energy) time-frequency prominent regions are used to compute\nthe state emitting probability to increase noise-robustness. For the\ndiscrete HMM framework, the probability distribution of each state\nis initialized by the global GMMs in training. In testing, the probability\nof each codebook is estimated using the prominent regions of each state\nto increase noise-robustness. In Cassins Vireo phrase classification\nusing 75 phrase types, the new GMM-HMM approach achieves 79.5% and\n87% classification accuracy using 1 and 2 phrases, respectively, while\nHTK&#8217;s GMM-HMM framework makes guess predictions resulting in\n1.33% accuracy. The performance of the other algorithm is presented\nin the paper.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1360"
  },
  "wisler16_interspeech": {
   "authors": [
    [
     "Alan",
     "Wisler"
    ],
    [
     "Laura J.",
     "Brattain"
    ],
    [
     "Rogier",
     "Landman"
    ],
    [
     "Thomas F.",
     "Quatieri"
    ]
   ],
   "title": "A Framework for Automated Marmoset Vocalization Detection and Classification",
   "original": "1410",
   "page_count": 5,
   "order": 553,
   "p1": "2592",
   "pn": "2596",
   "abstract": [
    "This paper describes a novel framework for automated marmoset vocalization\ndetection and classification from within long audio streams recorded\nin a noisy animal room, where multiple marmosets are housed. To overcome\nthe challenge of limited manually annotated data, we implemented a\ndata augmentation method using only a small number of labeled vocalizations.\nThe feature sets chosen have the desirable property of capturing characteristics\nof the signals that are useful in both identifying and distinguishing\nmarmoset vocalizations. Unlike many previous methods, feature extraction,\ncall detection, and call classification in our system are completely\nautomated. The system maintains a good performance of 20% equal error\ndetection rate using data with high number of noise events and 15%\nof classification error. Performance can be further improved with additional\nlabeled training data. Because this extensible system is capable of\nidentifying both positive and negative welfare indicators, it provides\na powerful framework for non-human primate welfare monitoring as well\nas behavior assessment.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1410"
  },
  "aihara16b_interspeech": {
   "authors": [
    [
     "Ikkyu",
     "Aihara"
    ],
    [
     "Takeshi",
     "Mizumoto"
    ],
    [
     "Hiromitsu",
     "Awano"
    ],
    [
     "Hiroshi G.",
     "Okuno"
    ]
   ],
   "title": "Call Alternation Between Specific Pairs of Male Frogs Revealed by a Sound-Imaging Method in Their Natural Habitat",
   "original": "0336",
   "page_count": 5,
   "order": 554,
   "p1": "2597",
   "pn": "2601",
   "abstract": [
    "Male frogs vocalize calls to attract conspecific females as well as\nto announce their own territories to other male frogs. In the choruses,\nacoustic interaction allows the male frogs to alternate their calls\nwith each other. Such call alternation is reported in various species\nof frogs including Japanese tree frogs ( Hyla japonica). During call\nalternation, both male and female frogs are likely to discriminate\ncalls of the male frogs because of small amount of call overlaps. Here,\nwe show that call alternation is observed in natural choruses of male\nJapanese tree frogs especially between neighboring pairs. First, we\ndemonstrate that caller positions and call timings can be estimated\nby a sound-imaging method. Second, the occurrence of call alternation\nis detected on the basis of statistical tests on phase differences\nof calls between respective pairs. Although our previous study revealed\na global synchronization pattern in natural choruses of the male frogs,\nlocal chorus structures were not examined well. Through the observation\nof call alternation between specific pairs, this study suggests the\nexistence of selective attention in the frog choruses. \n"
   ],
   "doi": "10.21437/Interspeech.2016-336"
  },
  "guyot16_interspeech": {
   "authors": [
    [
     "Patrice",
     "Guyot"
    ],
    [
     "Alice",
     "Eldridge"
    ],
    [
     "Ying Chen",
     "Eyre-Walker"
    ],
    [
     "Alison",
     "Johnston"
    ],
    [
     "Thomas",
     "Pellegrini"
    ],
    [
     "Mika",
     "Peck"
    ]
   ],
   "title": "Sinusoidal Modelling for Ecoacoustics",
   "original": "0361",
   "page_count": 5,
   "order": 555,
   "p1": "2602",
   "pn": "2606",
   "abstract": [
    "Biodiversity assessment is a central and urgent task, necessary to\nmonitoring the changes to ecological systems and understanding the\nfactors which drive these changes. Technological advances are providing\nnew approaches to monitoring, which are particularly useful in remote\nregions. Situated within the framework of the emerging field of ecoacoustics,\nthere is growing interest in the possibility of extracting ecological\ninformation from digital recordings of the acoustic environment. Rather\nthan focusing on identification of individual species, an increasing\nnumber of automated indices attempt to summarise acoustic activity\nat the community level, in order to provide a proxy for biodiversity.\nOriginally designed for speech processing, sinusoidal modelling has\npreviously been used as a bioacoustic tool, for example to detect particular\nbird species. In this paper, we demonstrate the use of sinusoidal modelling\nas a proxy for bird abundance. Using data from acoustic surveys made\nduring the breeding season in UK woodland, the number of extracted\nsinusoidal tracks is shown to correlate with estimates of bird abundance\nmade by expert ornithologists listening to the recordings. We also\nreport ongoing work exploring a new approach to investigate the composition\nof calls in spectro-temporal space that constitutes a promising new\nmethod for Ecoaoustic biodiversity assessment.\n"
   ],
   "doi": "10.21437/Interspeech.2016-361"
  },
  "stowell16_interspeech": {
   "authors": [
    [
     "Dan",
     "Stowell"
    ],
    [
     "Veronica",
     "Morfi"
    ],
    [
     "Lisa F.",
     "Gill"
    ]
   ],
   "title": "Individual Identity in Songbirds: Signal Representations and Metric Learning for Locating the Information in Complex Corvid Calls",
   "original": "0465",
   "page_count": 5,
   "order": 556,
   "p1": "2607",
   "pn": "2611",
   "abstract": [
    "Bird calls range from simple tones to rich dynamic multi-harmonic structures.\nThe more complex calls are very poorly understood at present, such\nas those of the scientifically important corvid family (jackdaws, crows,\nravens, etc.). Individual birds can recognise familiar individuals\nfrom calls, but where in the signal is this identity encoded? We studied\nthe question by applying a combination of feature representations to\na dataset of jackdaw calls, including linear predictive coding (LPC)\nand adaptive discrete Fourier transform (aDFT). We demonstrate through\na classification paradigm that we can strongly outperform a standard\nspectrogram representation for identifying individuals, and we apply\nmetric learning to determine which time-frequency regions contribute\nmost strongly to robust individual identification. Computational methods\ncan help to direct our search for understanding of these complex biological\nsignals.\n"
   ],
   "doi": "10.21437/Interspeech.2016-465"
  },
  "jancovic16_interspeech": {
   "authors": [
    [
     "Peter",
     "Jančovič"
    ],
    [
     "Münevver",
     "Köküer"
    ]
   ],
   "title": "Recognition of Multiple Bird Species Based on Penalised Maximum Likelihood and HMM-Based Modelling of Individual Vocalisation Elements",
   "original": "0669",
   "page_count": 5,
   "order": 557,
   "p1": "2612",
   "pn": "2616",
   "abstract": [
    "This paper presents an extension of our recent work on recognition\nof multiple bird species from their vocalisations by incorporating\nan improved acoustic modelling. The acoustic scene is segmented into\nspectro-temporal isolated segments by employing a sinusoidal detection\nalgorithm, which is able to handle multiple simultaneous bird vocalisations.\nEach segment is represented as a temporal sequence of frequencies of\nthe detected sinusoid. Each bird species is represented by a set of\nhidden Markov models (HMMs), each HMM modelling a particular vocalisation\nelement. A set of elements is discovered in an unsupervised manner\nusing a partial dynamic time warping algorithm and agglomerative hierarchical\nclustering. Recognition of multiple bird species is performed based\non maximising the likelihood of the set of detected segments on a subset\nof bird species models, with a penalisation applied for increasing\nthe number of bird species. Experimental evaluations used audio field\nrecordings containing 30 bird species. Detected segments from several\nbird species are joined to simulate the presence of multiple bird species.\nIt is demonstrated that the use of improved acoustic modelling in conjunction\nwith the maximum likelihood score combination method provides considerable\nimprovements over previous results and the use of majority voting.\n"
   ],
   "doi": "10.21437/Interspeech.2016-669"
  },
  "maina16_interspeech": {
   "authors": [
    [
     "Ciira wa",
     "Maina"
    ]
   ],
   "title": "Cost Effective Acoustic Monitoring of Bird Species",
   "original": "0746",
   "page_count": 4,
   "order": 558,
   "p1": "2617",
   "pn": "2620",
   "abstract": [
    "Climate change and human encroachment are some of the major threats\nfacing several natural ecosystems around the world. To ensure the protection\nof ecosystems under threat, it is important to monitor the biodiversity\nwithin these ecosystems to determine when conservation efforts are\nnecessary. For this to be achieved, technologies that allow large areas\nto be monitored in a cost effective manner are essential. In this work\nwe investigate the use of acoustic recordings obtained using a low\ncost Raspberry Pi based recorder to monitor the Hartlaub&#8217;s Turaco\nin central Kenya. This species is endemic to East Africa and faces\nhabitat loss due to climate change. Using simple features derived from\nthe spectrograms of the recordings, a Gaussian mixture model classifier\nis able to accurately screen large data sets for presence of the Hartlaub&#8217;s\nTuraco call. In addition, we present a method based on musical note\nonset detection to determine the number of calls within a recording.\n"
   ],
   "doi": "10.21437/Interspeech.2016-746"
  },
  "kohlsdorf16_interspeech": {
   "authors": [
    [
     "Daniel",
     "Kohlsdorf"
    ],
    [
     "Denise",
     "Herzing"
    ],
    [
     "Thad",
     "Starner"
    ]
   ],
   "title": "Feature Learning and Automatic Segmentation for Dolphin Communication Analysis",
   "original": "0748",
   "page_count": 5,
   "order": 559,
   "p1": "2621",
   "pn": "2625",
   "abstract": [
    "The study of dolphin cognition involves intensive research of animal\nvocalizations recorded in the field. We address the automated analysis\nof audible dolphin communication and propose a system that automatically\ndiscovers patterns in dolphin signals. These patterns are invariant\nto frequency shifts and time warping transformations. The discovery\nalgorithm is based on feature learning and unsupervised time series\nsegmentation using hidden Markov models. Researchers can inspect the\npatterns visually and interactively run comparative statistics between\nthe distribution of dolphin signals in different behavioral contexts.\nOur results indicate that our system provides meaningful patterns to\nthe marine biologist and that the comparative statistics are aligned\nwith the biologists domain knowledge.\n"
   ],
   "doi": "10.21437/Interspeech.2016-748"
  },
  "suzuki16b_interspeech": {
   "authors": [
    [
     "Reiji",
     "Suzuki"
    ],
    [
     "Shiho",
     "Matsubayashi"
    ],
    [
     "Kazuhiro",
     "Nakadai"
    ],
    [
     "Hiroshi G.",
     "Okuno"
    ]
   ],
   "title": "Localizing Bird Songs Using an Open Source Robot Audition System with a Microphone Array",
   "original": "0782",
   "page_count": 5,
   "order": 560,
   "p1": "2626",
   "pn": "2630",
   "abstract": [
    "Auditory scene analysis is critical in observing bio-diversity and\nunderstanding social behavior of animals in natural habitats because\nmany animals and birds sing or call and environmental sounds are made.\nTo understand acoustic interactions among songbirds, we need to collect\nspatiotemporal data for a long period of time during which multiple\nindividuals and species are singing simultaneously. We are developing\nHARKBird, which is an easily-available and portable system to record,\nlocalize, and analyze bird songs. It is composed of a laptop PC with\nan open source robot audition system HARK (Honda Research Institute\nJapan Audition for Robots with Kyoto University) and a commercially\navailable low-cost microphone array. HARKBird helps us annotate bird\nsongs and grasp the soundscape around the microphone array by providing\nthe direction of arrival (DOA) of each localized source and its separated\nsound automatically. In this paper, we briefly introduce our system\nand show an example analysis of a track recorded at the experimental\nforest of Nagoya University, in central Japan. We demonstrate that\nHARKBird can extract birdsongs successfully by combining multiple localization\nresults with appropriate parameter settings that took account of ecological\nproperties of environment around a microphone array and species-specific\nproperties of bird songs.\n"
   ],
   "doi": "10.21437/Interspeech.2016-782"
  },
  "kurth16_interspeech": {
   "authors": [
    [
     "Frank",
     "Kurth"
    ]
   ],
   "title": "Robust Detection of Multiple Bioacoustic Events with Repetitive Structures",
   "original": "0083",
   "page_count": 5,
   "order": 561,
   "p1": "2631",
   "pn": "2635",
   "abstract": [
    "In this paper we address the task of robustly detecting multiple bioacoustic\nevents with repetitive structures in outdoor monitoring recordings.\nFor this, we propose to use the shift-autocorrelation (shift-ACF) that\nwas previously successfully applied to F0 estimation in speech processing\nand has subsequently led to a robust technique for speech activity\ndetection. As a first contribution, we illustrate the potentials of\nvarious shift-ACF-based time-frequency representations adapted to repeated\nsignal components in the context of bioacoustic pattern detection.\nSecondly, we investigate a method for automatically detecting multiple\nrepeated events and present an application to a concrete bioacoustic\nmonitoring scenario. As a third contribution, we provide a systematic\nevaluation of the shift-ACF-based feature extraction in representing\nmultiple overlapping repeated events.\n"
   ],
   "doi": "10.21437/Interspeech.2016-83"
  },
  "moore16_interspeech": {
   "authors": [
    [
     "Roger K.",
     "Moore"
    ]
   ],
   "title": "A Real-Time Parametric General-Purpose Mammalian Vocal Synthesiser",
   "original": "0841",
   "page_count": 5,
   "order": 562,
   "p1": "2636",
   "pn": "2640",
   "abstract": [
    "Although R&amp;D into &#8216;speech synthesis&#8217; has received a\nconsiderable amount of attention over many years, there has been remarkably\nlittle effort devoted to constructing vocal synthesisers for non-human\nanimals. Of course, interest in synthesising human speech has been\ndriven by the demand for practical applications such as reading machines\nfor the blind or voice-operated assistants. Nevertheless, there are\npotential uses for non-human vocal synthesis:  e.g. in education, robotics\nor ecological fieldwork. The latter is of particular interest, since\nit is common practice to use &#8216;playback&#8217; methods (based\non recorded samples) that do not easily facilitate parametric control\nover key experimental variables. Therefore, this paper presents the\ndesign and implementation of a real-time parametric general-purpose\nmammalian vocal synthesiser. The approach taken has been to decompose\nthe overall sound production system into the relevant anatomical components\n(such as the lungs, vocal folds, tongue and mouth), and to implement\na real-time simulation in &#8216;Pure Data&#8217; &#8212; an open-source\ndataflow programming language. The software was successfully used to\ndesign an appropriate mammalian voice for the  MiRo biomimetic robot,\nbut there are potential applications in a number of areas. The software\nis available for free download at  http://www.dcs.shef.ac.uk/&#126;roger/downloads.html.\n"
   ],
   "doi": "10.21437/Interspeech.2016-841"
  },
  "oreilly16_interspeech": {
   "authors": [
    [
     "Colm",
     "O’Reilly"
    ],
    [
     "Nicola M.",
     "Marples"
    ],
    [
     "David J.",
     "Kelly"
    ],
    [
     "Naomi",
     "Harte"
    ]
   ],
   "title": "YIN-Bird: Improved Pitch Tracking for Bird Vocalisations",
   "original": "0090",
   "page_count": 5,
   "order": 563,
   "p1": "2641",
   "pn": "2645",
   "abstract": [
    "Pitch is an important property of birdsong. Accurate and automatic\ntracking of pitch for large numbers of recordings would be useful for\nautomatic analysis of birdsong. Currently, pitch trackers such as YIN\ncan work with carefully tuned parameters but the characteristics of\nbirdsong mean those optimal parameters can change quickly even within\na single song. This paper presents YIN-bird, a modified version of\nYIN which exploits spectrogram properties to automatically set a minimum\nfundamental frequency parameter for YIN. This parameter is continuously\nupdated without user intervention. A ground truth dataset of synthetic\nbirdsong with known fundamental frequency is generated for evaluation\nof YIN-bird. Listener tests from expert birders described the synthetic\nsamples as &#8220;sounding like original &amp; can hardly tell it is\nsynthetic&#8221;. Gross pitch error on whistles and trills were reduced\nby up to 4%. An analysis of nasal sounds shows the challenge in accurate\npitch tracking for this syllable type.\n"
   ],
   "doi": "10.21437/Interspeech.2016-90"
  },
  "hsu16c_interspeech": {
   "authors": [
    [
     "Yao-Chi",
     "Hsu"
    ],
    [
     "Ming-Han",
     "Yang"
    ],
    [
     "Hsiao-Tsung",
     "Hung"
    ],
    [
     "Berlin",
     "Chen"
    ]
   ],
   "title": "Mispronunciation Detection Leveraging Maximum Performance Criterion Training of Acoustic Models and Decision Functions",
   "original": "1602",
   "page_count": 5,
   "order": 564,
   "p1": "2646",
   "pn": "2650",
   "abstract": [
    "Mispronunciation detection is part and parcel of a computer assisted\npronunciation training (CAPT) system, facilitating second-language\n(L2) learners to pinpoint erroneous pronunciations in a given utterance\nso as to improve their spoken proficiency. This paper presents a continuation\nof such a general line of research and the major contributions are\ntwofold. First, we present an effective training approach that estimates\nthe deep neural network based acoustic models involved in the mispronunciation\ndetection process by optimizing an objective directly linked to the\nultimate evaluation metric. Second, along the same vein, two disparate\nlogistic sigmoid based decision functions with either phone- or senone-dependent\nparameterization are also inferred and used for enhanced mispronunciation\ndetection. A series of experiments on a Mandarin mispronunciation detection\ntask seem to show the performance merits of the proposed method.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1602"
  },
  "heeman16_interspeech": {
   "authors": [
    [
     "Peter A.",
     "Heeman"
    ],
    [
     "Rebecca",
     "Lunsford"
    ],
    [
     "Andy",
     "McMillin"
    ],
    [
     "J. Scott",
     "Yaruss"
    ]
   ],
   "title": "Using Clinician Annotations to Improve Automatic Speech Recognition of Stuttered Speech",
   "original": "1388",
   "page_count": 5,
   "order": 565,
   "p1": "2651",
   "pn": "2655",
   "abstract": [
    "In treating people who stutter, clinicians often have their clients\nread a story in order to determine their stuttering frequency. As the\nclient is speaking, the clinician annotates each disfluency. For further\nanalysis of the client&#8217;s speech, it is useful to have a word\ntranscription of what was said. However, as these are real-time annotations,\nthey are not always correct, and they usually lag where the actual\ndisfluency occurred. We have built a tool that rescores a word lattice\ntaking into account the clinician&#8217;s annotations. In the paper,\nwe describe how we incorporate the clinician&#8217;s annotations, and\nthe improvement over a baseline version. This approach of leveraging\nclinician annotations can be used for other clinical tasks where a\nword transcription is useful for further or richer analysis.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1388"
  },
  "xie16d_interspeech": {
   "authors": [
    [
     "Simin",
     "Xie"
    ],
    [
     "Nan",
     "Yan"
    ],
    [
     "Ping",
     "Yu"
    ],
    [
     "Manwa L.",
     "Ng"
    ],
    [
     "Lan",
     "Wang"
    ],
    [
     "Zhuanzhuan",
     "Ji"
    ]
   ],
   "title": "Deep Neural Networks for Voice Quality Assessment Based on the GRBAS Scale",
   "original": "0986",
   "page_count": 5,
   "order": 566,
   "p1": "2656",
   "pn": "2660",
   "abstract": [
    "In the field of voice therapy, perceptual evaluation is widely used\nby expert listeners as a way to evaluate pathological and normal voice\nquality. This approach is understandably subjective as it is subject\nto listeners&#8217; bias which high inter- and intra-listeners variability\ncan be found. As such, research on automatic assessment of pathological\nvoices using a combination of subjective and objective analyses emerged.\nThe present study aimed to develop a complementary automatic assessment\nsystem for voice quality based on the well-known GRBAS scale by using\na battery of multidimensional acoustical measures through Deep Neural\nNetworks. A total of 44 dimensionality parameters including Mel-frequency\nCepstral Coefficients, Smoothed Cepstral Peak Prominence and Long-Term\nAverage Spectrum was adopted. In addition, the state-of-the-art automatic\nassessment system based on Modulation Spectrum (MS) features and GMM\nclassifiers was used as comparison system. The classification results\nusing the proposed method revealed a moderate correlation with subjective\nGRBAS scores of dysphonic severity, and yielded a better performance\nthan MS-GMM system, with the best accuracy around 81.53%. The findings\nindicate that such assessment system can be used as an appropriate\nevaluation tool in determining the presence and severity of voice disorders.\n"
   ],
   "doi": "10.21437/Interspeech.2016-986"
  },
  "ward16_interspeech": {
   "authors": [
    [
     "Lauren",
     "Ward"
    ],
    [
     "Alessandro",
     "Stefani"
    ],
    [
     "Daniel",
     "Smith"
    ],
    [
     "Andreas",
     "Duenser"
    ],
    [
     "Jill",
     "Freyne"
    ],
    [
     "Barbara",
     "Dodd"
    ],
    [
     "Angela",
     "Morgan"
    ]
   ],
   "title": "Automated Screening of Speech Development Issues in Children by Identifying Phonological Error Patterns",
   "original": "0850",
   "page_count": 5,
   "order": 567,
   "p1": "2661",
   "pn": "2665",
   "abstract": [
    "A proof of concept system is developed to provide a broad assessment\nof speech development issues in children. It has been designed to enable\nnon-experts to complete an initial screening of children&#8217;s speech\nwith the aim of reducing the workload on Speech Language Pathology\nservices. The system was composed of an acoustic model trained by neural\nnetworks with split temporal context features and a constrained HMM\nencoded with the knowledge of Speech Language Pathologists. Results\ndemonstrated the system was able to improve PER by 33% compared with\nstandard HMM decoders, with a minimum PER of 19.03% achieved. Identification\nof Phonological Error Patterns with up to 94% accuracy was achieved\ndespite utilizing only a small corpus of disordered speech from Australian\nchildren. These results indicate the proposed system is viable and\nthe direction of further development are outlined in the paper.\n"
   ],
   "doi": "10.21437/Interspeech.2016-850"
  },
  "lin16_interspeech": {
   "authors": [
    [
     "Ju",
     "Lin"
    ],
    [
     "Yanlu",
     "Xie"
    ],
    [
     "Jinsong",
     "Zhang"
    ]
   ],
   "title": "Automatic Pronunciation Evaluation of Non-Native Mandarin Tone by Using Multi-Level Confidence Measures",
   "original": "1162",
   "page_count": 5,
   "order": 568,
   "p1": "2666",
   "pn": "2670",
   "abstract": [
    "Automatic evaluation of tonal production plays an important role in\na tonal language Computer-Assisted Pronunciation Training (CAPT) system.\nIn this paper, we propose an automatic evaluation method for non-native\nMandarin tones. The method applied multi-level confidence measures\ngenerated from Deep Neural Network (DNN). The confidence measures consisted\nof Log Posterior Ratios (LPR), Average Frame-level Log Posteriors (AFLP)\nand Segment-level Log Posteriors (SLP). The LPR was calculated between\nthe correct tone model and competing tone models. The AFLP and LPR\nwere obtained from frame-level scores. And the SLP was directly derived\nfrom segment-level scores. The multi-level confidence measures were\nmodeled with a support vector machine (SVM) classifier. For comparison,\nthree experiments were conducted according to different features: AFLP+LPR,\nSLP only and AFLP+LPR+SLP. The experimental results showed that the\nperformance of the system which used multi-level confidence measures\nwas the best, achieving a FRR of 5.63% and a DA of 82.45%, which demonstrated\nthe efficiency of the proposed method.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1162"
  },
  "kim16c_interspeech": {
   "authors": [
    [
     "Myungjong",
     "Kim"
    ],
    [
     "Jun",
     "Wang"
    ],
    [
     "Hoirin",
     "Kim"
    ]
   ],
   "title": "Dysarthric Speech Recognition Using Kullback-Leibler Divergence-Based Hidden Markov Model",
   "original": "0776",
   "page_count": 5,
   "order": 569,
   "p1": "2671",
   "pn": "2675",
   "abstract": [
    "Dysarthria is a neuro-motor speech disorder that impedes the physical\nproduction of speech. Patients with dysarthria often have trouble in\npronouncing certain sounds, resulting in undesirable phonetic variation.\nCurrent automatic speech recognition systems designed for the general\npublic are ineffective for dysarthric sufferers due to the phonetic\nvariation. In this paper, we investigate dysarthric speech recognition\nusing Kullback-Leibler divergence-based hidden Markov models. In the\nmodel, the emission probability of state is modeled by a categorical\ndistribution using phoneme posterior probabilities from a deep neural\nnetwork, and therefore, it can effectively capture the phonetic variation\nof dysarthric speech. Experimental evaluation on a database of several\nhundred words uttered by 30 speakers consisting of 12 mildly dysarthric,\n8 moderately dysarthric, and 10 control speakers showed that our approach\nprovides substantial improvement over the conventional Gaussian mixture\nmodel and deep neural network based speech recognition systems.\n"
   ],
   "doi": "10.21437/Interspeech.2016-776"
  },
  "warlaumont16_interspeech": {
   "authors": [
    [
     "Anne S.",
     "Warlaumont"
    ],
    [
     "Heather L.",
     "Ramsdell-Hudock"
    ]
   ],
   "title": "Detection of Total Syllables and Canonical Syllables in Infant Vocalizations",
   "original": "1518",
   "page_count": 5,
   "order": 570,
   "p1": "2676",
   "pn": "2680",
   "abstract": [
    "During the first two years of life, human infants produce increasing\nnumbers of speech-like (canonical) syllables. Both basic research on\nchild speech development and clinical work assessing a child&#8217;s\npre-speech capabilities stand to benefit from efficient, accurate,\nand consistent methods for counting the syllables present in a given\ninfant utterance. To date, there have been only a few attempts to perform\nsyllable counting in infant vocalizations automatically, and thorough\ncomparisons to human listener counts are lacking. We apply four existing,\nopenly available systems for detecting syllabic, consonant, or vowel\nelements in vocalizations and apply them to a set of infant utterances\nindividually and in combination. With the automated methods, we obtain\ncanonical syllable counts that correlate well enough with trained human\nlistener counts to replicate the pattern of increasing canonical syllable\nfrequency as infants get older. However, agreement between the automated\nmethods and human listener canonical syllable counts is considerably\nweaker than human listeners&#8217; agreement with each other. On the\nother hand, automatic identification of syllable-like units of any\ntype (canonical and non-canonical both included) match human listeners&#8217;\njudgments quite well. Interestingly, these total syllable counts also\nincrease with infant age.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1518"
  },
  "le16b_interspeech": {
   "authors": [
    [
     "Duc",
     "Le"
    ],
    [
     "Emily Mower",
     "Provost"
    ]
   ],
   "title": "Improving Automatic Recognition of Aphasic Speech with AphasiaBank",
   "original": "0213",
   "page_count": 5,
   "order": 571,
   "p1": "2681",
   "pn": "2685",
   "abstract": [
    "Automatic recognition of aphasic speech is challenging due to various\nspeech-language impairments associated with aphasia as well as a scarcity\nof training data appropriate for this speaker population. AphasiaBank,\na shared database of multimedia interactions primarily used by clinicians\nto study aphasia, offers a promising source of data for Deep Neural\nNetwork acoustic modeling. In this paper, we establish the first large-vocabulary\ncontinuous speech recognition baseline on AphasiaBank and study recognition\naccuracy as a function of diagnoses. We investigate several out-of-domain\nadaptation methods and show that AphasiaBank data can be leveraged\nto significantly improve the recognition rate on a smaller aphasic\nspeech corpus. This work helps broaden the understanding of aphasic\nspeech recognition, demonstrates the potential of AphasiaBank, and\nguides researchers who wish to use this database for their own work.\n"
   ],
   "doi": "10.21437/Interspeech.2016-213"
  },
  "laborde16_interspeech": {
   "authors": [
    [
     "Vincent",
     "Laborde"
    ],
    [
     "Thomas",
     "Pellegrini"
    ],
    [
     "Lionel",
     "Fontan"
    ],
    [
     "Julie",
     "Mauclair"
    ],
    [
     "Halima",
     "Sahraoui"
    ],
    [
     "Jérôme",
     "Farinas"
    ]
   ],
   "title": "Pronunciation Assessment of Japanese Learners of French with GOP Scores and Phonetic Information",
   "original": "0513",
   "page_count": 5,
   "order": 572,
   "p1": "2686",
   "pn": "2690",
   "abstract": [
    "In this paper, we report automatic pronunciation assessment experiments\nat phone-level on a read speech corpus in French, collected from 23\nJapanese speakers learning French as a foreign language. We compare\nthe standard approach based on Goodness Of Pronunciation (GOP) scores\nand phone-specific score thresholds to the use of logistic regressions\n(LR) models. French native speech corpus, in which artificial pronunciation\nerrors were introduced, was used as training set. Two typical errors\nof Japanese speakers were considered: /&#640;/ and /v/ often mispronounced\nas [l] and [b], respectively. The LR classifier achieved a 64.4% accuracy\nsimilar to the 63.8% accuracy of the baseline threshold method, when\nusing GOP scores and the expected phone identity as input features\nonly. A significant performance gain of 20.8% relative was obtained\nby adding phonetic and phonological features as input to the LR model,\nleading to a 77.1% accuracy. This LR model also outperformed another\nbaseline approach based on linear discriminant models trained on raw\nf-BANK coefficient features.\n"
   ],
   "doi": "10.21437/Interspeech.2016-513"
  },
  "robertson16_interspeech": {
   "authors": [
    [
     "Sean",
     "Robertson"
    ],
    [
     "Cosmin",
     "Munteanu"
    ],
    [
     "Gerald",
     "Penn"
    ]
   ],
   "title": "Pronunciation Error Detection for New Language Learners",
   "original": "0539",
   "page_count": 5,
   "order": 573,
   "p1": "2691",
   "pn": "2695",
   "abstract": [
    "Existing pronunciation error detection research assumes that second\nlanguage learners&#8217; speech is advanced enough that its segments\nare generally well articulated. However, learners just beginning their\nstudies, especially when those studies are organized according to western,\ndialogue-driven pedagogies, are unlikely to abide by those assumptions.\nThis paper presents an evaluation of pronunciation error detectors\non the utterances of second language learners just beginning their\nstudies. A corpus of nonnative speech data is collected through an\nexperimental application teaching beginner French. Word-level binary\nlabels are acquired through successive pairwise comparisons made by\nlanguage experts with years of experience teaching. Six error detectors\nare trained to classify these data: a classifier inspired by phonetic\ndistance algorithms; the Goodness of Pronunciation classifier [1];\nand four GMM-based discriminative classifiers modelled after [2]. Three\npartitioning strategies for 4-fold cross-validation are tested: one\nbased on corpus distribution, another leaving speakers out, and another\nleaving annotators out. The best error detector, a log-likelihood ratio\nof native versus nonnative GMMs, achieved detector-annotator agreement\nof up to &#954; = .41, near the expected between-annotator agreement.\n"
   ],
   "doi": "10.21437/Interspeech.2016-539"
  },
  "ding16_interspeech": {
   "authors": [
    [
     "Hongwei",
     "Ding"
    ],
    [
     "Xinping",
     "Xu"
    ]
   ],
   "title": "L2 English Rhythm in Read Speech by Chinese Students",
   "original": "0427",
   "page_count": 5,
   "order": 574,
   "p1": "2696",
   "pn": "2700",
   "abstract": [
    "L2 English speech produced by Mandarin Chinese speakers is usually\nperceived to be intermediate between stress-timed and syllable-timed\nin rhythm. However, previous studies seldom employed comparable data\nof target language, source language and L2 interlanguage in one investigation,\nwhich may lead to discrepant results. Thus, in this study we conducted\na contrastive investigation of 10 Chinese students and 10 native English\nspeakers. We measured the rhythmic correlates in passage readings of\nMandarin and L2 English produced by the native Chinese subjects, and\nthose of English by the native British speakers. Comparison of the\nwidely used rhythmic metrics  %V, &#916; C, &#916; V, nPVI, rPVI, VarcoV,\nand  VarcoC confirmed that Mandarin Chinese is a highly syllable-timed\nlanguage. Results suggested that vowel-related metrics were better\nindexes to classify L2 English rhythm produced by Chinese speakers\nas being more syllable-timed than stress-timed. Analysis showed that\nvowel epenthesis, non-reduction of vowels, and no stressed/unstressed\ncontrast could contribute to the auditory impression of syllable-timed\nrhythm of their L2 English. This investigation could shed some light\non the Chinese accent of L2 English and provided support to facilitate\nthe rhythmic acquisition of stress-timed languages for Chinese students.\n"
   ],
   "doi": "10.21437/Interspeech.2016-427"
  },
  "li16h_interspeech": {
   "authors": [
    [
     "Miao",
     "Li"
    ],
    [
     "Zhipeng",
     "Chen"
    ],
    [
     "Ji",
     "Wu"
    ]
   ],
   "title": "Improving the Probabilistic Framework for Representing Dialogue Systems with User Response Model",
   "original": "0810",
   "page_count": 5,
   "order": 575,
   "p1": "2701",
   "pn": "2705",
   "abstract": [
    "A probabilistic framework for goal-driven spoken dialogue systems (SDSs)\nhas been proposed by us in a previous work. In the framework, a target\ndistribution, instead of the frame structure, is used to represent\nthe dialogue state at each turn. The target-based state tracking algorithm\nenables the system to handle uncertainties in the dialogue. By summarizing\nthe target-based state, information from the back-end database can\nbe exploited to develop efficient dialogue strategies. To extend our\nprobabilistic framework and adapt our approach to real application\nscenarios, a user response model is investigated and integrated into\nthe probabilistic framework to enhance the dialogue policy in this\npaper. Experiments in both ideal setting and real user test setting\nare conducted to test the enhanced dialogue policy. The results show\nthat despite an unavoidable mismatch between the user response model\nbased on prior knowledge and real users&#8217; behaviors in the experiment,\nthe enhanced dialogue policy works robustly and efficiently. The results\nfurther demonstrate that the probabilistic framework is quite flexible\nand amenable to the integration of additional factors and models of\nreal-world dialogue problems.\n"
   ],
   "doi": "10.21437/Interspeech.2016-810"
  },
  "song16b_interspeech": {
   "authors": [
    [
     "Yiping",
     "Song"
    ],
    [
     "Lili",
     "Mou"
    ],
    [
     "Rui",
     "Yan"
    ],
    [
     "Li",
     "Yi"
    ],
    [
     "Zinan",
     "Zhu"
    ],
    [
     "Xiaohua",
     "Hu"
    ],
    [
     "Ming",
     "Zhang"
    ]
   ],
   "title": "Dialogue Session Segmentation by Embedding-Enhanced TextTiling",
   "original": "1234",
   "page_count": 5,
   "order": 576,
   "p1": "2706",
   "pn": "2710",
   "abstract": [
    "In human-computer conversation systems, the context of a user-issued\nutterance is particularly important because it provides useful background\ninformation of the conversation. However, it is unwise to track all\nprevious utterances in the current session as not all of them are equally\nimportant. In this paper, we address the problem of session segmentation.\nWe propose an embedding-enhanced TextTiling approach, inspired by the\nobservation that conversation utterances are highly noisy, and that\nword embeddings provide a robust way of capturing semantics. Experimental\nresults show that our approach achieves better performance than the\nTextTiling, MMD approaches.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1234"
  },
  "li16i_interspeech": {
   "authors": [
    [
     "Miao",
     "Li"
    ],
    [
     "Zhiyang",
     "He"
    ],
    [
     "Ji",
     "Wu"
    ]
   ],
   "title": "Target-Based State and Tracking Algorithm for Spoken Dialogue System",
   "original": "0800",
   "page_count": 5,
   "order": 577,
   "p1": "2711",
   "pn": "2715",
   "abstract": [
    "Conventional spoken dialogue systems use frame structure to represent\ndialogue state. In this paper, we argue that using target distribution\nto represent dialogue state is much better than using frame structure.\nBased on the proposed target-based state, two target-based state tracking\nalgorithms are introduced. Experiments in an end-to-end spoken dialogue\nsystem with real users are conducted to compare the performance between\nthe target-based state trackers and frame-based state trackers. The\nexperimental results show that the proposed target-based state tracker\nachieve 97% of dialogue success rate, comparing to 81% of frame-based\nstate tracker, which suggests the advantage of target-based state.\n"
   ],
   "doi": "10.21437/Interspeech.2016-800"
  },
  "shen16_interspeech": {
   "authors": [
    [
     "Sheng-syun",
     "Shen"
    ],
    [
     "Hung-Yi",
     "Lee"
    ]
   ],
   "title": "Neural Attention Models for Sequence Classification: Analysis and Application to Key Term Extraction and Dialogue Act Detection",
   "original": "1359",
   "page_count": 5,
   "order": 578,
   "p1": "2716",
   "pn": "2720",
   "abstract": [
    "Recurrent neural network architectures combining with attention mechanism,\nor neural attention model, have shown promising performance recently\nfor the tasks including speech recognition, image caption generation,\nvisual question answering and machine translation. In this paper, neural\nattention model is applied on two sequence labeling tasks, dialogue\nact detection and key term extraction. In the sequence labeling tasks,\nthe model input is a sequence, and the output is the label of the input\nsequence. The major difficulty of sequence labeling is that when the\ninput sequence is long, it can include many noisy or irrelevant part.\nIf the information in the whole sequence is treated equally, the noisy\nor irrelevant part may degrade the classification performance. The\nattention mechanism is helpful for sequence classification task because\nit is capable of highlighting important part among the entire sequence\nfor the classification task. The experimental results show that with\nthe attention mechanism, discernible improvements were achieved in\nthe sequence labeling task considered here. The roles of the attention\nmechanism in the tasks are further analyzed and visualized in this\npaper.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1359"
  },
  "kumar16b_interspeech": {
   "authors": [
    [
     "Manoj",
     "Kumar"
    ],
    [
     "Rahul",
     "Gupta"
    ],
    [
     "Daniel",
     "Bone"
    ],
    [
     "Nikolaos",
     "Malandrakis"
    ],
    [
     "Somer",
     "Bishop"
    ],
    [
     "Shrikanth S.",
     "Narayanan"
    ]
   ],
   "title": "Objective Language Feature Analysis in Children with Neurodevelopmental Disorders During Autism Assessment",
   "original": "0563",
   "page_count": 5,
   "order": 579,
   "p1": "2721",
   "pn": "2725",
   "abstract": [
    "Lexical planning is an important part of communication and is reflective\nof a speaker&#8217;s internal state that includes aspects of affect,\nmood, as well as mental health. Within the study of developmental disorders\nsuch as autism spectrum disorder (ASD), language acquisition and language\nuse have been studied to assess disorder severity and expressive capability\nas well as to support diagnosis. In this paper, we perform a language\nanalysis of children focusing on word usage, social and cognitive linguistic\nword counts, and a few recently proposed psycho-linguistic norms. We\nuse data from conversational samples of verbally fluent children obtained\nduring Autism Diagnostic Observation Schedule (ADOS) sessions. We extract\nthe aforementioned lexical cues from transcripts of session recordings\nand demonstrate their role in differentiating children diagnosed with\nAutism Spectrum Disorder from the rest. Further, we perform a correlation\nanalysis between the lexical norms and ASD symptom severity. The analysis\nreveals an increased affinity by the interlocutor towards use of words\nwith greater feminine association and negative valence.\n"
   ],
   "doi": "10.21437/Interspeech.2016-563"
  },
  "casanueva16_interspeech": {
   "authors": [
    [
     "Iñigo",
     "Casanueva"
    ],
    [
     "Thomas",
     "Hain"
    ],
    [
     "Phil",
     "Green"
    ]
   ],
   "title": "Improving Generalisation to New Speakers in Spoken Dialogue State Tracking",
   "original": "0404",
   "page_count": 5,
   "order": 580,
   "p1": "2726",
   "pn": "2730",
   "abstract": [
    "Users with disabilities can greatly benefit from personalised voice-enabled\nenvironmental-control interfaces, but for users with speech impairments\n(e.g. dysarthria) poor ASR performance poses a challenge to successful\ndialogue. Statistical dialogue management has shown resilience against\nhigh ASR error rates, hence making it useful to improve the performance\nof these interfaces. However, little research was devoted to dialogue\nmanagement personalisation to specific users so far. Recently, data\ndriven discriminative models have been shown to yield the best performance\nin dialogue state tracking (the inference of the user goal from the\ndialogue history). However, due to the unique characteristics of each\nspeaker, training a system for a new user when user specific data is\nnot available can be challenging due to the mismatch between training\nand working conditions. This work investigates two methods to improve\nthe performance with new speakers of a LSTM-based personalised state\ntracker: The use of speaker specific acoustic and ASR-related features;\nand dropout regularisation. It is shown that in an environmental control\nsystem for dysarthric speakers, the combination of both techniques\nyields improvements of 3.5% absolute in state tracking accuracy. Further\nanalysis explores the effect of using different amounts of speaker\nspecific data to train the tracking system.\n"
   ],
   "doi": "10.21437/Interspeech.2016-404"
  },
  "tseng16b_interspeech": {
   "authors": [
    [
     "Bo-Hsiang",
     "Tseng"
    ],
    [
     "Sheng-syun",
     "Shen"
    ],
    [
     "Hung-Yi",
     "Lee"
    ],
    [
     "Lin-Shan",
     "Lee"
    ]
   ],
   "title": "Towards Machine Comprehension of Spoken Content: Initial TOEFL Listening Comprehension Test by Machine",
   "original": "0876",
   "page_count": 5,
   "order": 581,
   "p1": "2731",
   "pn": "2735",
   "abstract": [
    "Multimedia or spoken content presents more attractive information than\nplain text content, but it&#8217;s more difficult to display on a screen\nand be selected by a user. As a result, accessing large collections\nof the former is much more difficult and time-consuming than the latter\nfor humans. It&#8217;s highly attractive to develop a machine which\ncan automatically understand spoken content and summarize the key information\nfor humans to browse over. In this endeavor, we propose a new task\nof machine comprehension of spoken content. We define the initial goal\nas the listening comprehension test of TOEFL, a challenging academic\nEnglish examination for English learners whose native language is not\nEnglish. We further propose an Attention-based Multi-hop Recurrent\nNeural Network (AMRNN) architecture for this task, achieving encouraging\nresults in the initial tests. Initial results also have shown that\nword-level attention is probably more robust than sentence-level attention\nfor this task with ASR errors.\n"
   ],
   "doi": "10.21437/Interspeech.2016-876"
  },
  "ravuri16_interspeech": {
   "authors": [
    [
     "Suman",
     "Ravuri"
    ],
    [
     "Steven",
     "Wegmann"
    ]
   ],
   "title": "How Neural Network Depth Compensates for HMM Conditional Independence Assumptions in DNN-HMM Acoustic Models",
   "original": "0283",
   "page_count": 5,
   "order": 582,
   "p1": "2736",
   "pn": "2740",
   "abstract": [
    "While DNN-HMM acoustic models have replaced GMM-HMMs in the standard\nASR pipeline due to performance improvements, one unrealistic assumption\nthat remains in these models is the conditional independence assumption\nof the Hidden Markov Model (HMM). In this work, we explore the extent\nto which depth of neural networks helps compensate for these poor conditional\nindependence assumptions. Using a bootstrap resampling framework that\nallows us to control the amount of data dependence in the test set\nwhile still using real observations from the data, we can determine\nhow robust neural networks, and particularly deeper models, are to\ndata dependence. Our conclusions are that if the data were to match\nthe conditional independence assumptions of the HMM, there would be\nlittle benefit from using deeper models. It is only when data become\nmore dependent that depth improves ASR performance. That performance\nsubstantially degrades, however, as the data becomes more realistic\nsuggests that better temporal modeling is still needed for ASR.\n"
   ],
   "doi": "10.21437/Interspeech.2016-283"
  },
  "palaz16_interspeech": {
   "authors": [
    [
     "Dimitri",
     "Palaz"
    ],
    [
     "Gabriel",
     "Synnaeve"
    ],
    [
     "Ronan",
     "Collobert"
    ]
   ],
   "title": "Jointly Learning to Locate and Classify Words Using Convolutional Networks",
   "original": "0968",
   "page_count": 5,
   "order": 583,
   "p1": "2741",
   "pn": "2745",
   "abstract": [
    "In this paper, we propose a novel approach for weakly-supervised word\nrecognition. Most state of the art automatic speech recognition systems\nare based on frame-level labels obtained through forced alignments\nor through a sequential loss. Recently, weakly-supervised trained models\nhave been proposed in vision, that can learn which part of the input\nis relevant for classifying a given pattern [1]. Our system is composed\nof a convolutional neural network and a temporal score aggregation\nmechanism. For each sentence, it is trained using as supervision only\nsome of the words (most frequent) that are present in a given sentence,\nwithout knowing their order nor quantity. We show that our proposed\nsystem is able to jointly classify and localize words. We also evaluate\nthe system on a keyword spotting task, and show that it can yield similar\nperformance to strong supervised HMM/GMM baseline.\n"
   ],
   "doi": "10.21437/Interspeech.2016-968"
  },
  "alvarez16_interspeech": {
   "authors": [
    [
     "Raziel",
     "Alvarez"
    ],
    [
     "Rohit",
     "Prabhavalkar"
    ],
    [
     "Anton",
     "Bakhtin"
    ]
   ],
   "title": "On the Efficient Representation and Execution of Deep Acoustic Models",
   "original": "0128",
   "page_count": 5,
   "order": 584,
   "p1": "2746",
   "pn": "2750",
   "abstract": [
    "In this paper we present a simple and computationally efficient quantization\nscheme that enables us to reduce the resolution of the parameters of\na neural network from 32-bit floating point values to 8-bit integer\nvalues. The proposed quantization scheme leads to significant memory\nsavings and enables the use of optimized hardware instructions for\ninteger arithmetic, thus significantly reducing the cost of inference.\nFinally, we propose a &#8216;quantization aware&#8217; training process\nthat applies the proposed scheme during network training and find that\nit allows us to recover most of the loss in accuracy introduced by\nquantization. We validate the proposed techniques by applying them\nto a long short-term memory-based acoustic model on an open-ended large\nvocabulary speech recognition task.\n"
   ],
   "doi": "10.21437/Interspeech.2016-128"
  },
  "povey16_interspeech": {
   "authors": [
    [
     "Daniel",
     "Povey"
    ],
    [
     "Vijayaditya",
     "Peddinti"
    ],
    [
     "Daniel",
     "Galvez"
    ],
    [
     "Pegah",
     "Ghahremani"
    ],
    [
     "Vimal",
     "Manohar"
    ],
    [
     "Xingyu",
     "Na"
    ],
    [
     "Yiming",
     "Wang"
    ],
    [
     "Sanjeev",
     "Khudanpur"
    ]
   ],
   "title": "Purely Sequence-Trained Neural Networks for ASR Based on Lattice-Free MMI",
   "original": "0595",
   "page_count": 5,
   "order": 585,
   "p1": "2751",
   "pn": "2755",
   "abstract": [
    "In this paper we describe a method to perform sequence-discriminative\ntraining of neural network acoustic models without the need for frame-level\ncross-entropy pre-training. We use the lattice-free version of the\nmaximum mutual information (MMI) criterion: LF-MMI. To make its computation\nfeasible we use a phone n-gram language model, in place of the word\nlanguage model. To further reduce its space and time complexity we\ncompute the objective function using neural network outputs at one\nthird the standard frame rate. These changes enable us to perform the\ncomputation for the forward-backward algorithm on GPUs. Further the\nreduced output frame-rate also provides a significant speed-up during\ndecoding.<br style=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  We present results on 5 different LVCSR tasks with training data\nranging from 100 to 2100 hours. Models trained with LF-MMI provide\na relative word error rate reduction of &#126;11.5%, over those trained\nwith cross-entropy objective function, and &#126;8%, over those trained\nwith cross-entropy and sMBR objective functions. A further reduction\nof &#126;2.5%, relative, can be obtained by fine tuning these models\nwith the word-lattice based sMBR objective function.\n"
   ],
   "doi": "10.21437/Interspeech.2016-595"
  },
  "ratajczak16_interspeech": {
   "authors": [
    [
     "Martin",
     "Ratajczak"
    ],
    [
     "Sebastian",
     "Tschiatschek"
    ],
    [
     "Franz",
     "Pernkopf"
    ]
   ],
   "title": "Virtual Adversarial Training Applied to Neural Higher-Order Factors for Phone Classification",
   "original": "0832",
   "page_count": 5,
   "order": 586,
   "p1": "2756",
   "pn": "2760",
   "abstract": [
    "We explore virtual adversarial training (VAT) applied to neural higher-order\nconditional random fields for sequence labeling. VAT is a recently\nintroduced regularization method promoting local distributional smoothness:\nIt counteracts the problem that predictions of many state-of-the-art\nclassifiers are unstable to adversarial perturbations. Unlike random\nnoise, adversarial perturbations are minimal and bounded perturbations\nthat flip the predicted label. We utilize VAT to regularize neural\nhigher-order factors in conditional random fields. These factors are\nfor example important for phone classification where phone representations\nstrongly depend on the context phones. However, without using VAT for\nregularization, the use of such factors was limited as they were prone\nto overfitting. In extensive experiments, we successfully apply VAT\nto improve performance on the TIMIT phone classification task. In particular,\nwe achieve a phone error rate of 13.0%, exceeding the state-of-the-art\nperformance by a wide margin.\n"
   ],
   "doi": "10.21437/Interspeech.2016-832"
  },
  "wong16_interspeech": {
   "authors": [
    [
     "Jeremy H.M.",
     "Wong"
    ],
    [
     "Mark J.F.",
     "Gales"
    ]
   ],
   "title": "Sequence Student-Teacher Training of Deep Neural Networks",
   "original": "0911",
   "page_count": 5,
   "order": 587,
   "p1": "2761",
   "pn": "2765",
   "abstract": [
    "The performance of automatic speech recognition can often be significantly\nimproved by combining multiple systems together. Though beneficial,\nensemble methods can be computationally expensive, often requiring\nmultiple decoding runs. An alternative approach, appropriate for deep\nlearning schemes, is to adopt student-teacher training. Here, a student\nmodel is trained to reproduce the outputs of a teacher model, or ensemble\nof teachers. The standard approach is to train the student model on\nthe frame posterior outputs of the teacher. This paper examines the\ninteraction between student-teacher training schemes and sequence training\ncriteria, which have been shown to yield significant performance gains\nover frame-level criteria. There are several possible options for integrating\nsequence training, including training of the ensemble and further training\nof the student. This paper also proposes an extension to the student-teacher\nframework, where the student is trained to emulate the hypothesis posterior\ndistribution of the teacher, or ensemble of teachers. This sequence\nstudent-teacher training approach allows the benefit of student-teacher\ntraining to be directly combined with sequence training schemes. These\napproaches are evaluated on two speech recognition tasks: a Wall Street\nJournal based task and a low-resource Tok Pisin conversational telephone\nspeech task from the IARPA Babel programme.\n"
   ],
   "doi": "10.21437/Interspeech.2016-911"
  },
  "hansen16_interspeech": {
   "authors": [
    [
     "John H.L.",
     "Hansen"
    ],
    [
     "Hynek",
     "Bořil"
    ]
   ],
   "title": "Robustness in Speech, Speaker, and Language Recognition: &#8220;You&#8217;ve Got to Know Your Limitations&#8221;",
   "original": "1395",
   "page_count": 5,
   "order": 588,
   "p1": "2766",
   "pn": "2770",
   "abstract": [
    "In the field of speech, speaker and language recognition, significant\ngains have and are being made with new machine learning strategies\nalong with the availability of new and emerging speech corpora. However,\nmany of the core scientific principles required for effective speech\nprocessing research appear to be drifting to the sidelines with the\nassumptions that access to larger amounts of data can address a growing\nrange of issues relating to new speech/speaker/language recognition\nscenarios. This study focuses on exploring several challenging domains\nin formulating effective solutions in realistic speech data, and in\nparticular the notion of using naturalistic data to better reflect\nthe potential effectiveness of new algorithms. Our main focus is on\nmismatch/speech variability issues due to (i) differences in noisy\nspeech with and without Lombard effect and a communication factor,\n(ii) realistic field data in noisy/increased cognitive load conditions,\nand (iii) dialect identification using found data. Finally, we study\nspeaker&#8211;noise and speaker&#8211;speaker interactions in a newly\nestablished, fully naturalistic Prof-Life-Log corpus. The specific\noutcomes from this study include an analysis of the strengths and weaknesses\nof simulated vs. actual speech data collection for research.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1395"
  },
  "jokinen16b_interspeech": {
   "authors": [
    [
     "Emma",
     "Jokinen"
    ],
    [
     "Ulpu",
     "Remes"
    ],
    [
     "Paavo",
     "Alku"
    ]
   ],
   "title": "The Use of Read versus Conversational Lombard Speech in Spectral Tilt Modeling for Intelligibility Enhancement in Near-End Noise Conditions",
   "original": "0143",
   "page_count": 5,
   "order": 589,
   "p1": "2771",
   "pn": "2775",
   "abstract": [
    "Intelligibility of speech in adverse near-end noise conditions can\nbe enhanced with post-processing. Recently, a post-processing method\nbased on statistical mapping of the spectral tilt of normal speech\nto that of Lombard speech was proposed. However, previous intelligibility\nimprovement studies utilizing Lombard speech have mainly gathered data\nfrom read sentences which might result in a less pronounced Lombard\neffect. Having a mild Lombard effect in the training data weakens the\nstatistical normal-to-Lombard mapping of the spectral tilt which in\nturn deteriorates performance of intelligibility enhancement. Therefore,\na database containing both conversational and read Lombard speech was\nrecorded in several background noise conditions in this study. Statistical\nmodels for normal-to-Lombard mapping of the spectral tilt were then\ntrained using the obtained conversational and read speech data and\nevaluated using an objective intelligibility metric. The results suggest\nthat the conversational data contains a more pronounced Lombard effect\nand could be used to obtain better statistical models for intelligibility\nenhancement.\n"
   ],
   "doi": "10.21437/Interspeech.2016-143"
  },
  "sturim16b_interspeech": {
   "authors": [
    [
     "Douglas E.",
     "Sturim"
    ],
    [
     "Pedro A.",
     "Torres-Carrasquillo"
    ],
    [
     "Joseph P.",
     "Campbell"
    ]
   ],
   "title": "Corpora for the Evaluation of Robust Speaker Recognition Systems",
   "original": "1609",
   "page_count": 5,
   "order": 590,
   "p1": "2776",
   "pn": "2780",
   "abstract": [
    "The goal of this paper is to describe significant corpora available\nto support speaker recognition research and evaluation, along with\ndetails about the corpora collection and design. We describe the attributes\nof high-quality speaker recognition corpora. Considerations of the\napplication, domain, and performance metrics are also discussed. Additionally,\na literature survey of corpora used in speaker recognition research\nover the last 10 years is presented. Finally we show the most common\ncorpora used in the research community and review them on their success\nin enabling meaningful speaker recognition research.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1609"
  },
  "bertin16_interspeech": {
   "authors": [
    [
     "Nancy",
     "Bertin"
    ],
    [
     "Ewen",
     "Camberlein"
    ],
    [
     "Emmanuel",
     "Vincent"
    ],
    [
     "Romain",
     "Lebarbenchon"
    ],
    [
     "Stéphane",
     "Peillon"
    ],
    [
     "Éric",
     "Lamande"
    ],
    [
     "Sunit",
     "Sivasankaran"
    ],
    [
     "Frédéric",
     "Bimbot"
    ],
    [
     "Irina",
     "Illina"
    ],
    [
     "Ariane",
     "Tom"
    ],
    [
     "Sylvain",
     "Fleury"
    ],
    [
     "Éric",
     "Jamet"
    ]
   ],
   "title": "A French Corpus for Distant-Microphone Speech Processing in Real Homes",
   "original": "1384",
   "page_count": 5,
   "order": 591,
   "p1": "2781",
   "pn": "2785",
   "abstract": [
    "We introduce a new corpus for distant-microphone speech processing\nin domestic environments. This corpus includes reverberated, noisy\nspeech signals spoken by native French talkers in a lounge and recorded\nby an 8-microphone device at various angles and distances and in various\nnoise conditions. Room impulse responses and noise-only signals recorded\nin various real rooms and homes and baseline speaker localization and\nenhancement software are also provided. This corpus stands apart from\nother corpora in the field by the number of rooms and homes considered\nand by the fact that it is publicly available at no cost. We describe\nthe corpus specifications and annotations and the data recorded so\nfar. We report baseline results.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1384"
  },
  "ravanelli16_interspeech": {
   "authors": [
    [
     "Mirco",
     "Ravanelli"
    ],
    [
     "Piergiorgio",
     "Svaizer"
    ],
    [
     "Maurizio",
     "Omologo"
    ]
   ],
   "title": "Realistic Multi-Microphone Data Simulation for Distant Speech Recognition",
   "original": "0731",
   "page_count": 5,
   "order": 592,
   "p1": "2786",
   "pn": "2790",
   "abstract": [
    "The availability of realistic simulated corpora is of key importance\nfor the future progress of distant speech recognition technology. The\nreliability, flexibility and low computational cost of a data simulation\nprocess may ultimately allow researchers to train, tune and test different\ntechniques in a variety of acoustic scenarios, avoiding the laborious\neffort of directly recording real data from the targeted environment.<br\nstyle=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  In the last decade, several simulated corpora have been released\nto the research community, including the data-sets distributed in the\ncontext of projects and international challenges, such as CHiME and\nREVERB. These efforts were extremely useful to derive baselines and\ncommon evaluation frameworks for comparison purposes. At the same time,\nin many cases they highlighted the need of a better coherence between\nreal and simulated conditions.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  In this paper, we\nexamine this issue and we describe our approach to the generation of\nrealistic corpora in a domestic context. Experimental validation, conducted\nin a multi-microphone scenario, shows that a comparable performance\ntrend can be observed with both real and simulated data across different\nrecognition frameworks, acoustic models, as well as multi-microphone\nprocessing techniques.\n"
   ],
   "doi": "10.21437/Interspeech.2016-731"
  },
  "gamper16_interspeech": {
   "authors": [
    [
     "Hannes",
     "Gamper"
    ],
    [
     "Mark R.P.",
     "Thomas"
    ],
    [
     "Lyle",
     "Corbin"
    ],
    [
     "Ivan",
     "Tashev"
    ]
   ],
   "title": "Synthesis of Device-Independent Noise Corpora for Realistic ASR Evaluation",
   "original": "0978",
   "page_count": 5,
   "order": 593,
   "p1": "2791",
   "pn": "2795",
   "abstract": [
    "In order to effectively evaluate the accuracy of automatic speech recognition\n(ASR) with a novel capture device, it is important to create a realistic\ntest data corpus that is representative of real-world noise conditions.\nTypically, this involves either recording the output of a device under\ntest (DUT) in a noisy environment, or synthesizing an environment over\nloudspeakers in a way that simulates realistic signal-to-noise ratios\n(SNRs), reverberation times, and spatial noise distributions. Here\nwe propose a method that aims at combining the realism of in-situ recordings\nwith the convenience and repeatability of synthetic corpora. A device-independent\nspatial recording containing noise and speech is combined with the\nmeasured directivity pattern of a DUT to generate a synthetic test\ncorpus for evaluating the performance of an ASR system. This is achieved\nby a spherical harmonic decomposition of both the sound field and the\nDUT&#8217;s directivity patterns. Experimental results suggest that\nthe proposed method can be a viable alternative to costly and cumbersome\ndevice-dependent measurements. The proposed simulation method predicted\nthe SNR of the DUT response to within about 3 dB and the word error\nrate (WER) to within about 20%, across a range of test SNRs, target\nsource directions, and noise types.\n"
   ],
   "doi": "10.21437/Interspeech.2016-978"
  },
  "richardson16_interspeech": {
   "authors": [
    [
     "Fred",
     "Richardson"
    ],
    [
     "Michael",
     "Brandstein"
    ],
    [
     "Jennifer",
     "Melot"
    ],
    [
     "Douglas",
     "Reynolds"
    ]
   ],
   "title": "Speaker Recognition Using Real vs Synthetic Parallel Data for DNN Channel Compensation",
   "original": "0544",
   "page_count": 5,
   "order": 594,
   "p1": "2796",
   "pn": "2800",
   "abstract": [
    "Recent work has shown large performance gains using denoising DNNs\nfor speech processing tasks under challenging acoustic conditions.\nHowever, training these DNNs requires large amounts of parallel multichannel\nspeech data which can be impractical or expensive to collect. The effective\nuse of synthetic parallel data as an alternative has been demonstrated\nfor several speech technologies including automatic speech recognition\nand speaker recognition (SR). This paper demonstrates that denoising\nDNNs trained with real Mixer 2 multichannel data perform only slightly\nbetter than DNNs trained with synthetic multichannel data for microphone\nSR on Mixer 6. Large reductions in pooled error rates of 50% EER and\n30% min DCF are achieved using DNNs trained on real Mixer 2 data. Nearly\nthe same performance gains are achieved using synthetic data generated\nwith a limited number of room impulse responses (RIRs) and noise sources\nderived from Mixer 2. Using RIRs from three publicly available sources\nused in the Kaldi ASpIRE recipe yields somewhat lower pooled gains\nof 34% EER and 25% min DCF. These results confirm the effective use\nof synthetic parallel data for DNN channel compensation even when the\nRIRs used for synthesizing the data are not particularly well matched\nto the task.\n"
   ],
   "doi": "10.21437/Interspeech.2016-544"
  },
  "ribas16_interspeech": {
   "authors": [
    [
     "Dayana",
     "Ribas"
    ],
    [
     "Emmanuel",
     "Vincent"
    ],
    [
     "John H.L.",
     "Hansen"
    ],
    [
     "Emma",
     "Jokinen"
    ],
    [
     "Mirco",
     "Ravanelli"
    ],
    [
     "Hannes",
     "Gamper"
    ],
    [
     "Fred",
     "Richardson"
    ]
   ],
   "title": "Discussion",
   "original": "abs14",
   "page_count": 0,
   "order": 595,
   "p1": "0",
   "pn": "",
   "abstract": [
    "(No abstract available at the time of publication)\n"
   ]
  },
  "bosch16_interspeech": {
   "authors": [
    [
     "Louis ten",
     "Bosch"
    ],
    [
     "Lou",
     "Boves"
    ],
    [
     "M.",
     "Ernestus"
    ]
   ],
   "title": "Combining Data-Oriented and Process-Oriented Approaches to Modeling Reaction Time Data",
   "original": "1072",
   "page_count": 5,
   "order": 596,
   "p1": "2801",
   "pn": "2805",
   "abstract": [
    "This paper combines two different approaches to modeling reaction time\ndata from lexical decision experiments, viz. a data-oriented statistical\nanalysis by means of a linear mixed effects model, and a process-oriented\ncomputational model of human speech comprehension.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  The linear mixed effect\nmodel is implemented by lmer in R. As computational model we apply\nDIANA, an end-to-end computational model which aims at modeling the\ncognitive processes underlying speech comprehension. DIANA takes as\ninput the speech signal, and provides as output the orthographic transcription\nof the stimulus, a word/non-word judgment and the associated reaction\ntime. Previous studies have shown that DIANA shows good results for\nlarge-scale lexical decision experiments in Dutch and North-American\nEnglish.<br style=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  We investigate whether predictors that appear significant in an\nlmer analysis and processes implemented in DIANA can be related and\ninform both approaches. Predictors such as &#8216;previous reaction\ntime&#8217; can be related to a process description; other predictors,\nsuch as &#8216;lexical neighborhood&#8217; are hard-coded in lmer and\nemergent in DIANA. The analysis focuses on the interaction between\nsubject variables and task variables in lmer, and the ways in which\nthese interactions can be implemented in DIANA.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1072"
  },
  "mcauliffe16_interspeech": {
   "authors": [
    [
     "Michael",
     "McAuliffe"
    ],
    [
     "Molly",
     "Babel"
    ],
    [
     "Charlotte",
     "Vaughn"
    ]
   ],
   "title": "Do Listeners Learn Better from Natural Speech?",
   "original": "0610",
   "page_count": 5,
   "order": 597,
   "p1": "2806",
   "pn": "2810",
   "abstract": [
    "Perceptual learning of novel pronunciations is a seemingly robust and\nefficient process for adapting to unfamiliar speech patterns. In this\nstudy we compare perceptual learning of /s/ words where a medially\noccurring /s/ is substituted with /&#643;/, rendering, for example,\n castle as /k&#230;&#643;l/ instead of /k&#230;sl/. Exposure to the\nnovel pronunciations is presented in the guise of a lexical decision\ntask. Perceptual learning is assessed in a categorization task where\nlisteners are presented with minimal pair continua (e.g.,  sock-shock).\nGiven recent suggestions that perceptual learning may be more robust\nwith natural as opposed to synthesized speech, we compare perceptual\nlearning in groups that either receive natural /s/-to-/&#643;/ words\nor resynthesized /s/-to-/&#643;/ words. Despite low word endorsement\nrates in the lexical decision task, both groups of listeners show robust\ngeneralization in perceptual learning to the novel minimal pair continua,\nthereby indicating that at least with high quality resynthesis, perceptual\nlearning in natural and synthesized speech is roughly equivalent.\n"
   ],
   "doi": "10.21437/Interspeech.2016-610"
  },
  "drozdova16_interspeech": {
   "authors": [
    [
     "Polina",
     "Drozdova"
    ],
    [
     "Roeland van",
     "Hout"
    ],
    [
     "Odette",
     "Scharenborg"
    ]
   ],
   "title": "Processing and Adaptation to Ambiguous Sounds during the Course of Perceptual Learning",
   "original": "0814",
   "page_count": 5,
   "order": 598,
   "p1": "2811",
   "pn": "2815",
   "abstract": [
    "Listeners use their lexical knowledge to interpret ambiguous sounds,\nand retune their phonetic categories to include this ambiguous sound.\nAlthough there is ample evidence for lexically-guided retuning, the\nadaptation process is not fully understood. Using a lexical decision\ntask with an embedded auditory semantic priming task, the present study\ninvestigates whether words containing an ambiguous sound are processed\nin the same way as &#8220;natural&#8221; words and whether adaptation\nto the ambiguous sound tends to equalize the processing of &#8220;ambiguous&#8221;\nand natural words. Analyses of the yes/no responses and reaction times\nto natural and &#8220;ambiguous&#8221; words showed that words containing\nan ambiguous sound were accepted as words less often and were processed\nslower than the same words without ambiguity. The difference in acceptance\ndisappeared after exposure to approximately 15 ambiguous items. Interestingly,\nlower acceptance rates and slower processing did not have an effect\non the processing of semantic information of the following word. However,\nlower acceptance rates of ambiguous primes predict slower reaction\ntimes of these primes, suggesting an important role of stimulus-specific\ncharacteristics in triggering lexically-guided perceptual learning.\n"
   ],
   "doi": "10.21437/Interspeech.2016-814"
  },
  "hintz16_interspeech": {
   "authors": [
    [
     "Florian",
     "Hintz"
    ],
    [
     "Odette",
     "Scharenborg"
    ]
   ],
   "title": "The Effect of Background Noise on the Activation of Phonological and Semantic Information During Spoken-Word Recognition",
   "original": "0882",
   "page_count": 5,
   "order": 599,
   "p1": "2816",
   "pn": "2820",
   "abstract": [
    "During spoken-word recognition, listeners experience phonological competition\nbetween multiple word candidates, which increases, relative to optimal\nlistening conditions, when speech is masked by noise. Moreover, listeners\nactivate semantic word knowledge during the word&#8217;s unfolding.\nHere, we replicated the effect of background noise on phonological\ncompetition and investigated to which extent noise affects the activation\nof semantic information in phonological competitors. Participants&#8217;\neye movements were recorded when they listened to sentences containing\na target word and looked at three types of displays. The displays either\ncontained a picture of the target word, or a picture of a phonological\nonset competitor, or a picture of a word semantically related to the\nonset competitor, each along with three unrelated distractors. The\nanalyses revealed that, in noise, fixations to the target and to the\nphonological onset competitor were delayed and smaller in magnitude\ncompared to the clean listening condition, most likely reflecting enhanced\nphonological competition. No evidence for the activation of semantic\ninformation in the phonological competitors was observed in noise and,\nsurprisingly, also not in the clear. We discuss the implications of\nthe lack of an effect and differences between the present and earlier\nstudies.\n"
   ],
   "doi": "10.21437/Interspeech.2016-882"
  },
  "kang16_interspeech": {
   "authors": [
    [
     "Shinae",
     "Kang"
    ],
    [
     "Clara",
     "Cohen"
    ]
   ],
   "title": "Relationships Between Functional Load and Auditory Confusability Under Different Speech Environments",
   "original": "0906",
   "page_count": 5,
   "order": 600,
   "p1": "2821",
   "pn": "2825",
   "abstract": [
    "Functional load (FL) is an information-theoretic measure that captures\na phoneme&#8217;s contribution to successful word identification. Experimental\nfindings have shown that it can help explain patterns in perceptual\naccuracy. Here, we ask whether the relationship between FL and perception\nhas larger consequences for the structure of a language&#8217;s lexicon.\nSince reducing FL minimizes the risk of misidentifying a word in the\ncase where a listener inaccurately perceives the initial phoneme, we\npredicted that in spoken language, where perceptual accuracy is important\nfor successful communication, the lexicon will be structured to reduce\nFL in auditorily confusable initial phonemes more than in written language.\nTo test this prediction, we compared FL of all initial phonemes in\nspoken and academic written genres of the COCA corpus. We found that\nFL in phoneme pairs in the spoken corpus is overall higher and more\nvariable than in the academic corpus, a natural consequence of the\nsmaller lexical inventory characteristic of spoken language. In auditorily\nconfusable pairs, however, this difference is relatively reduced, such\nthat spoken FL decreases relative to academic FL. We argue that this\nreflects a pressure in spoken language to use words for which inaccurate\nperception does minimal damage to word identification.\n"
   ],
   "doi": "10.21437/Interspeech.2016-906"
  },
  "kanwal16_interspeech": {
   "authors": [
    [
     "Jasmeen",
     "Kanwal"
    ],
    [
     "Amanda",
     "Ritchart"
    ]
   ],
   "title": "The Role of Pitch in Punjabi Word Identification",
   "original": "1445",
   "page_count": 5,
   "order": 601,
   "p1": "2826",
   "pn": "2830",
   "abstract": [
    "Previous work has argued that one class of consonants in Punjabi &#8212;\nthose thought to be historically voiced aspirated &#8212; have now\nlost aspiration in all contexts and voicing in certain contexts. Word\ninitially, these consonants are realized as voiceless unaspirated and\nare differentiated from other voiceless unaspirated consonants by a\nfalling pitch on the following vowel. In this study, we investigate,\nusing a two-alternative forced choice task, whether listeners make\nuse of a falling pitch word-initially to distinguish between these\ntwo types of consonants that are otherwise phonetically identical.\nOur results show that, regardless of talker or listener, differences\nin falling pitch on the vowel following an unaspirated voiceless consonant\nare indeed sufficient for listeners to distinguish between words beginning\nwith these consonants. These results provide further evidence that,\nin word-initial contexts, pitch may be in the process of phonologization\nin at least some dialects of Punjabi.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1445"
  },
  "tahon16_interspeech": {
   "authors": [
    [
     "Marie",
     "Tahon"
    ],
    [
     "Raheel",
     "Qader"
    ],
    [
     "Gwénolé",
     "Lecorvé"
    ],
    [
     "Damien",
     "Lolive"
    ]
   ],
   "title": "Improving TTS with Corpus-Specific Pronunciation Adaptation",
   "original": "0864",
   "page_count": 5,
   "order": 602,
   "p1": "2831",
   "pn": "2835",
   "abstract": [
    "Text-to-speech (TTS) systems are built on speech corpora which are\nlabeled with carefully checked and segmented phonemes. However, phoneme\nsequences generated by automatic grapheme-to-phoneme converters during\nsynthesis are usually inconsistent with those from the corpus, thus\nleading to poor quality synthetic speech signals. To solve this problem,\nthe present work aims at adapting automatically generated pronunciations\nto the corpus. The main idea is to train corpus-specific phoneme-to-phoneme\nconditional random fields with a large set of linguistic, phonological,\narticulatory and acoustic-prosodic features. Features are first selected\nin cross-validation condition, then combined to produce the final best\nfeature set. Pronunciation models are evaluated in terms of phoneme\nerror rate and through perceptual tests. Experiments carried out on\na French speech corpus show an improvement in the quality of speech\nsynthesis when pronunciation models are included in the phonetization\nprocess. Apart from improving TTS quality, the presented pronunciation\nadaptation method also brings interesting perspectives in terms of\nexpressive speech synthesis.\n"
   ],
   "doi": "10.21437/Interspeech.2016-864"
  },
  "mousa16_interspeech": {
   "authors": [
    [
     "Amr El-Desoky",
     "Mousa"
    ],
    [
     "Björn",
     "Schuller"
    ]
   ],
   "title": "Deep Bidirectional Long Short-Term Memory Recurrent Neural Networks for Grapheme-to-Phoneme Conversion Utilizing Complex Many-to-Many Alignments",
   "original": "1229",
   "page_count": 5,
   "order": 603,
   "p1": "2836",
   "pn": "2840",
   "abstract": [
    "Efficient grapheme-to-phoneme (G2P) conversion models are considered\nindispensable components to achieve the state-of-the-art performance\nin modern automatic speech recognition (ASR) and text-to-speech (TTS)\nsystems. The role of these models is to provide such systems with a\nmeans to generate accurate pronunciations for unseen words. Recent\nwork in this domain is based on recurrent neural networks (RNN) that\nare capable of translating grapheme sequences into phoneme sequences\ntaking into account the full context of graphemes. To achieve high\nperformance with these models, utilizing explicit alignment information\nis found essential. The quality of the G2P model heavily depends on\nthe imposed alignment constraints.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  In this paper, a novel\napproach is proposed using complex many-to-many G2P alignments to improve\nthe performance of G2P models based on deep bidirectional long short-term\nmemory (BLSTM) RNNs. Extensive experiments cover models with different\nnumbers of hidden layers, projection layer, input splicing windows,\nand varying alignment schemes. One observes that complex alignments\nsignificantly improve the performance on the publicly available CMUDict\nUS English dataset. We compare our results with previously published\nresults.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1229"
  },
  "esch16_interspeech": {
   "authors": [
    [
     "Daan van",
     "Esch"
    ],
    [
     "Mason",
     "Chua"
    ],
    [
     "Kanishka",
     "Rao"
    ]
   ],
   "title": "Predicting Pronunciations with Syllabification and Stress with Recurrent Neural Networks",
   "original": "1419",
   "page_count": 5,
   "order": 604,
   "p1": "2841",
   "pn": "2845",
   "abstract": [
    "Word pronunciations, consisting of phoneme sequences and the associated\nsyllabification and stress patterns, are vital for both speech recognition\nand text-to-speech (TTS) systems. For speech recognition phoneme sequences\nfor words may be learned from audio data. We train recurrent neural\nnetwork (RNN) based models to predict the syllabification and stress\npattern for such pronunciations making them usable for TTS. We find\nthese RNN models significantly outperform naive rule-based models for\nalmost all languages we tested. Further, we find additional improvements\nto the stress prediction model by using the spelling as features in\naddition to the phoneme sequence. Finally, we train a single RNN model\nto predict the phoneme sequence, syllabification and stress for a given\nword. For several languages, this single RNN outperforms similar models\ntrained specifically for either phoneme sequence or stress prediction.\nWe report an exhaustive comparison of these approaches for twenty languages.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1419"
  },
  "pouget16_interspeech": {
   "authors": [
    [
     "Maël",
     "Pouget"
    ],
    [
     "Olha",
     "Nahorna"
    ],
    [
     "Thomas",
     "Hueber"
    ],
    [
     "Gérard",
     "Bailly"
    ]
   ],
   "title": "Adaptive Latency for Part-of-Speech Tagging in Incremental Text-to-Speech Synthesis",
   "original": "0165",
   "page_count": 5,
   "order": 605,
   "p1": "2846",
   "pn": "2850",
   "abstract": [
    "Incremental text-to-speech systems aim at synthesizing a text &#8216;on-the-fly&#8217;,\nwhile the user is typing a sentence. In this context, this article\naddresses the problem of the part-of-speech tagging (POS, i.e. lexical\ncategory) which is a critical step for accurate grapheme-to-phoneme\nconversion and prosody estimation. Here, the main challenge is to estimate\nthe POS of a given word without knowing its &#8216;right context&#8217;\n(i.e. the following words which are not available yet). To address\nthis issue, we propose a method based on a set of decision trees estimating\nonline whether a given POS tag is likely to be modified when more right-contextual\ninformation becomes available. In such a case, the synthesis is delayed\nuntil POS stability is guaranteed. This results in delivering the synthetic\nvoice in word chunks of variable length. Objective evaluation on French\nshows that the proposed method is able to estimate POS tags with more\nthan a 92% accuracy (compared to a non-incremental system) while minimizing\nthe synthesis latency (between 1 and 4 words). Perceptual evaluation\n(ranking test) is then carried in the context of HMM-based speech synthesis.\nExperimental results show that the word grouping resulting from the\nproposed method is rated more acceptable than word-by-word incremental\nsynthesis.\n"
   ],
   "doi": "10.21437/Interspeech.2016-165"
  },
  "dall16_interspeech": {
   "authors": [
    [
     "Rasmus",
     "Dall"
    ],
    [
     "Kei",
     "Hashimoto"
    ],
    [
     "Keiichiro",
     "Oura"
    ],
    [
     "Yoshihiko",
     "Nankaku"
    ],
    [
     "Keiichi",
     "Tokuda"
    ]
   ],
   "title": "Redefining the Linguistic Context Feature Set for HMM and DNN TTS Through Position and Parsing",
   "original": "0399",
   "page_count": 5,
   "order": 606,
   "p1": "2851",
   "pn": "2855",
   "abstract": [
    "In this paper we present an investigation of a number of alternative\nlinguistic feature context sets for HMM and DNN text-to-speech synthesis.\nThe representation of positional values is explored through two alternatives\nto the standard set of absolute values, namely relational and categorical\nvalues. In a preference test the categorical representation was found\nto be preferred for both HMM and DNN synthesis. Subsequently, features\nbased on probabilistic context free grammar and dependency parsing\nare presented. These features represent the phrase level relations\nbetween words in the sentences, and in a preference evaluation it was\nfound that these features all improved upon the base set, with a combination\nof both parsing methods best overall. As the features primarily affected\nthe F0 prediction, this illustrates the potential of syntactic structure\nto improve prosody in TTS.\n"
   ],
   "doi": "10.21437/Interspeech.2016-399"
  },
  "wang16f_interspeech": {
   "authors": [
    [
     "Xin",
     "Wang"
    ],
    [
     "Shinji",
     "Takaki"
    ],
    [
     "Junichi",
     "Yamagishi"
    ]
   ],
   "title": "Enhance the Word Vector with Prosodic Information for the Recurrent Neural Network Based TTS System",
   "original": "0390",
   "page_count": 5,
   "order": 607,
   "p1": "2856",
   "pn": "2860",
   "abstract": [
    "Word embedding, which is a dense and low-dimensional vector representation\nof word, is recently used to replace of the conventional prosodic context\nas an input feature to the acoustic model of a TTS system. However,\nthese word vectors trained from text data may encode insufficient information\nrelated to speech. This paper presents a post-filtering approach to\nenhance the raw word vectors with prosodic information for the TTS\ntask. Based on a publicly available speech corpus with manual prosodic\nannotation, a post-filter can be trained to transform the raw word\nvectors. Experiment shows that using the enhanced word vectors as an\ninput to the neural network-based acoustic model improves the accuracy\nof the predicted F0 trajectory. Besides, we also show that the enhanced\nvectors provide better initial values than the raw vectors for error\nback-propagation of the network, which results in further improvement.\n"
   ],
   "doi": "10.21437/Interspeech.2016-390"
  },
  "jeon16_interspeech": {
   "authors": [
    [
     "Kwang Myung",
     "Jeon"
    ],
    [
     "Hong Kook",
     "Kim"
    ]
   ],
   "title": "Local Sparsity Based Online Dictionary Learning for Environment-Adaptive Speech Enhancement with Nonnegative Matrix Factorization",
   "original": "0586",
   "page_count": 5,
   "order": 608,
   "p1": "2861",
   "pn": "2865",
   "abstract": [
    "In this paper, a nonnegative matrix factorization (NMF)-based speech\nenhancement method robust to real and diverse noise is proposed by\nonline NMF dictionary learning without relying on prior knowledge of\nnoise. Conventional NMF-based methods have used a fixed noise dictionary,\nwhich often results in performance degradation when the NMF noise dictionary\ncannot cover noise types that occur in real-life recording. Thus, the\nnoise dictionary needs to be learned from noises according to the variation\nof recording environments. To this end, the proposed method first estimates\nnoise spectra and then performs online noise dictionary learning by\na discriminative NMF learning framework. In particular, the noise spectra\nare estimated from minimum mean squared error filtering, which is based\non the local sparsity defined by a posteriori signal-to-noise ratio\n(SNR) estimated from the NMF separation of the previous analysis frame.\nThe effectiveness of the proposed speech enhancement method is demonstrated\nby adding six different realistic noises to clean speech signals with\nvarious SNRs. Consequently, it is shown that the proposed method outperforms\ncomparative methods in terms of signal-to-distortion ratio (SDR) and\nperceptual evaluation of speech quality (PESQ) for all kinds of simulated\nnoise and SNR conditions.\n"
   ],
   "doi": "10.21437/Interspeech.2016-586"
  },
  "papadopoulos16_interspeech": {
   "authors": [
    [
     "Pavlos",
     "Papadopoulos"
    ],
    [
     "Colin",
     "Vaz"
    ],
    [
     "Shrikanth S.",
     "Narayanan"
    ]
   ],
   "title": "Noise Aware and Combined Noise Models for Speech Denoising in Unknown Noise Conditions",
   "original": "0501",
   "page_count": 4,
   "order": 609,
   "p1": "2866",
   "pn": "2869",
   "abstract": [
    "Traditional denoising schemes require prior knowledge or statistics\nof the noise corrupting the signal, or estimate the noise from noise-only\nportions of the signal, which requires knowledge of speech boundaries.\nExtending denoising methods to perform well in unknown noise conditions\ncan facilitate processing of data captured in different real life environments,\nand relax rigid data acquisition protocols. In this paper we propose\ntwo methods for denoising speech signals in unknown noise conditions.\nThe first method has two stages. In the first stage we use Long Term\nSignal Variability features to decide which noise model to use from\na pool of available models. Once we determine the noise type, we use\nNonnegative Matrix Factorization with a dictionary trained on that\nnoise to denoise the signal. In the second method, we create a combined\nnoise dictionary from different types of noise, and use that dictionary\nin the denoising phase. Both of our systems improve signal quality,\nas measured by PESQ scores, for all the noise types we tested, and\nfor different Signal to Noise Ratio levels.\n"
   ],
   "doi": "10.21437/Interspeech.2016-501"
  },
  "mirsamadi16_interspeech": {
   "authors": [
    [
     "Seyedmahdad",
     "Mirsamadi"
    ],
    [
     "Ivan",
     "Tashev"
    ]
   ],
   "title": "Causal Speech Enhancement Combining Data-Driven Learning and Suppression Rule Estimation",
   "original": "0437",
   "page_count": 5,
   "order": 610,
   "p1": "2870",
   "pn": "2874",
   "abstract": [
    "The problem of single-channel speech enhancement has been traditionally\naddressed by using statistical signal processing algorithms that are\ndesigned to suppress time-frequency regions affected by noise. We study\nan alternative data-driven approach which uses deep neural networks\n(DNNs) to learn the transformation from noisy and reverberant speech\nto clean speech, with a focus on real-time applications which require\nlow-latency causal processing. We examine several structures in which\ndeep learning can be used within an enhancement system. These include\nend-to-end DNN regression from noisy to clean spectra, as well as less\nintervening approaches which estimate a suppression gain for each time-frequency\nbin instead of directly recovering the clean spectral features. We\nalso propose a novel architecture in which the general structure of\na conventional noise suppressor is preserved, but the sub-tasks are\nindependently learned and carried out by separate networks. It is shown\nthat DNN-based suppression gain estimation outperforms the regression\napproach in the causal processing mode and for noise types that are\nnot seen during DNN training.\n"
   ],
   "doi": "10.21437/Interspeech.2016-437"
  },
  "brutti16_interspeech": {
   "authors": [
    [
     "Alessio",
     "Brutti"
    ],
    [
     "Antigoni",
     "Tsiami"
    ],
    [
     "Athanasios",
     "Katsamanis"
    ],
    [
     "Petros",
     "Maragos"
    ]
   ],
   "title": "A Phase-Based Time-Frequency Masking for Multi-Channel Speech Enhancement in Domestic Environments",
   "original": "0150",
   "page_count": 5,
   "order": 611,
   "p1": "2875",
   "pn": "2879",
   "abstract": [
    "This paper introduces a novel time-frequency masking approach for speech\nenhancement, based on the consistency of the phase of the cross-spectrum\nobserved at multiple microphones. The proposed approach is derived\nfrom solutions commonly adopted in spatial source separation and can\nbe used as a post-filter in traditional multi-channel speech enhancement\nschemes. Since it is not based on a modeling of the coherence of diffuse\nnoise, the proposed method complements traditional post-filters implementations,\ntargeting non diffuse/coherent sources. It is particularly effective\nin domestic scenarios where microphones in a given room capture interfering\ncoherent sources active in adjacent rooms.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  An experimental analysis\non the DIRHA-GRID corpus shows that the proposed method considerably\nimproves the signal-to-interference-ratio and can be used on top of\nstate-of-the-art multi-channel speech enhancement methods.\n"
   ],
   "doi": "10.21437/Interspeech.2016-150"
  },
  "petkov16b_interspeech": {
   "authors": [
    [
     "Petko N.",
     "Petkov"
    ],
    [
     "Yannis",
     "Stylianou"
    ]
   ],
   "title": "Generalizing Steady State Suppression for Enhanced Intelligibility Under Reverberation",
   "original": "1026",
   "page_count": 5,
   "order": 612,
   "p1": "2880",
   "pn": "2884",
   "abstract": [
    "Speech intelligibility in reverberant environments decreases due to\noverlap-masking. Unlike additive noise, the masking signal is not independent\nfrom the information bearing signal. A mathematical framework for intelligibility-enhancing\nsignal modification prior to presentation in reverberant environments\nis presented in this paper. The optimal solution generalizes steady\nstate suppression and adjusts the short-term signal power as a function\nof late reverberation power and signal importance. The signal modification\noperates in a full-band setting and preserves the time scale of the\nunmodified signal. Gain smoothing based on an adaptive rate-of-change\nconstraint reduces processing artifacts and enhances performance. Subjective\nvalidation shows that the proposed method effectively reduces the impact\nof overlap-masking. Speech intelligibility at a reverberation time\nof 1.8 s was improved significantly compared to unmodified and steady-state-suppressed\nspeech.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1026"
  },
  "yamamoto16_interspeech": {
   "authors": [
    [
     "Katsuhiko",
     "Yamamoto"
    ],
    [
     "Toshio",
     "Irino"
    ],
    [
     "Toshie",
     "Matsui"
    ],
    [
     "Shoko",
     "Araki"
    ],
    [
     "Keisuke",
     "Kinoshita"
    ],
    [
     "Tomohiro",
     "Nakatani"
    ]
   ],
   "title": "Speech Intelligibility Prediction Based on the Envelope Power Spectrum Model with the Dynamic Compressive Gammachirp Auditory Filterbank",
   "original": "0652",
   "page_count": 5,
   "order": 613,
   "p1": "2885",
   "pn": "2889",
   "abstract": [
    "In this study, we develop a new method to realize speech intelligibility\nprediction of synthetic sounds processed by nonlinear speech enhancement\nalgorithms. A speech envelope power spectrum model (sEPSM) was proposed\nto account for subjective results on a spectral subtraction, but it\nis untested by recent state-of-the-art speech enhancement algorithms.\nWe introduce a dynamic compressive gammachirp auditory filterbank as\nthe front-end of the sEPSM (dcGC-sEPSM) to improve the predictability.\nWe perform subjective experiments on speech intelligibility (SI) of\nnoise-reduced sounds processed by the spectral subtraction and a recently\ndeveloped Wiener filter algorithm. We compare the subjective SI scores\nwith the objective SI scores predicted by the proposed dcGC-sEPSM,\nthe original GT-sEPSM, the three-level coherence SII (CSII), and the\nshort-time objective intelligibility (STOI). The results show that\nthe proposed dcGC-sEPSM performs better than the conventional models.\n"
   ],
   "doi": "10.21437/Interspeech.2016-652"
  },
  "kawahara16b_interspeech": {
   "authors": [
    [
     "Tatsuya",
     "Kawahara"
    ],
    [
     "Takashi",
     "Yamaguchi"
    ],
    [
     "Koji",
     "Inoue"
    ],
    [
     "Katsuya",
     "Takanashi"
    ],
    [
     "Nigel",
     "Ward"
    ]
   ],
   "title": "Prediction and Generation of Backchannel Form for Attentive Listening Systems",
   "original": "0118",
   "page_count": 5,
   "order": 614,
   "p1": "2890",
   "pn": "2894",
   "abstract": [
    "In human-human dialogue, especially in attentive listening such as\ncounseling, backchannels are important not only for smooth communication\nbut also for establishing rapport. Despite several studies on when\nto backchannel, most of the current spoken dialogue systems generate\nthe same pattern of backchannels, giving monotonous impressions to\nusers. In this work, we investigate generation of a variety of backchannel\nforms according to the dialogue context. We first show the feasibility\nof choosing appropriate backchannel forms based on machine learning,\nand the synergy of using linguistic and prosodic features. For generation\nof backchannels, a framework based on a set of binary classifiers is\nadopted to effectively make a &#8220;not-to-generate&#8221; decision.\nThe proposed model achieved better prediction accuracy than a baseline\nwhich always outputs the same backchannel form and another baseline\nwhich randomly generates backchannels. Finally, evaluations by human\nsubjects demonstrate that the proposed method generates backchannels\nas naturally as human choices, giving impressions of understanding\nand empathy.\n"
   ],
   "doi": "10.21437/Interspeech.2016-118"
  },
  "lunsford16_interspeech": {
   "authors": [
    [
     "Rebecca",
     "Lunsford"
    ],
    [
     "Peter A.",
     "Heeman"
    ],
    [
     "Emma",
     "Rennie"
    ]
   ],
   "title": "Measuring Turn-Taking Offsets in Human-Human Dialogues",
   "original": "1350",
   "page_count": 5,
   "order": 615,
   "p1": "2895",
   "pn": "2899",
   "abstract": [
    "This paper examines the pauses, gaps and overlaps associated with turn-taking\nin order to better understand how people engage in this activity, which\nshould lead to more natural and effective spoken dialogue systems.\nThis paper makes three advances in studying these durations. First,\nwe take into account the type of turn-taking event, carefully treating\ninterruptions, dual starts, and delayed backchannels, as these can\nmake it appear that turn-taking is more disorderly than it really is.\nSecond, we do not view turn-transitions in isolation, but consider\nturn-transitions and turn-continuations together, as equal alternatives\nof what could have occurred. Third, we use the distributions of turn-transition\nand turn-continuation offsets (gaps, overlaps, and pauses) to shed\nlight on the extent to which turn-taking is negotiated by the two conversants\nversus controlled by the current speaker.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1350"
  },
  "meshorer16_interspeech": {
   "authors": [
    [
     "Tomer",
     "Meshorer"
    ],
    [
     "Peter A.",
     "Heeman"
    ]
   ],
   "title": "Using Past Speaker Behavior to Better Predict Turn Transitions",
   "original": "1409",
   "page_count": 5,
   "order": 616,
   "p1": "2900",
   "pn": "2904",
   "abstract": [
    "This paper explores using a summary of past speaker behavior to better\npredict turn transitions. We computed two types of summary features\nthat represent the current speaker&#8217;s past turn-taking behavior:\nrelative turn length and relative floor control. Relative turn length\nmeasures the current turn length so far (in time and words) relative\nto the speaker&#8217;s average turn length. Relative floor control\nmeasures the speaker&#8217;s control of the conversation floor (in\ntime and words) relative to the total conversation length. The features\nare recomputed for each dialog act based on past turns of the speaker\nwithin the current conversation. Using the switchboard corpus, we trained\ntwo models to predict turn transitions: one with just local features\n(e.g., current speech act, previous speech act) and one that added\nthe summary features. Our results shows that using the summary features\nimprove turn transitions prediction.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1409"
  },
  "bailly16_interspeech": {
   "authors": [
    [
     "Gérard",
     "Bailly"
    ],
    [
     "Frédéric",
     "Elisei"
    ],
    [
     "Alexandra",
     "Juphard"
    ],
    [
     "Olivier",
     "Moreaud"
    ]
   ],
   "title": "Quantitative Analysis of Backchannels Uttered by an Interviewer During Neuropsychological Tests",
   "original": "0022",
   "page_count": 5,
   "order": 617,
   "p1": "2905",
   "pn": "2909",
   "abstract": [
    "This paper examines in detail the backchannels uttered by a French\nprofessional interviewer during a neuropsychological test of verbal\nmemories. These backchannels are short utterances such as  oui, d&#8217;accord,\nuhm, etc. They are mainly produced here to encourage subjects to retrieve\na set of words after their controlled encoding. We show that the choice\nof lexical items, their production rates and their associated prosodic\ncontours are influenced by the subject performance and conditioned\nby the protocol.\n"
   ],
   "doi": "10.21437/Interspeech.2016-22"
  },
  "chowdhury16_interspeech": {
   "authors": [
    [
     "Shammur Absar",
     "Chowdhury"
    ],
    [
     "Evgeny A.",
     "Stepanov"
    ],
    [
     "Giuseppe",
     "Riccardi"
    ]
   ],
   "title": "Predicting User Satisfaction from Turn-Taking in Spoken Conversations",
   "original": "0859",
   "page_count": 5,
   "order": 618,
   "p1": "2910",
   "pn": "2914",
   "abstract": [
    "User satisfaction is an important aspect of the user experience while\ninteracting with objects, systems or people. Traditionally user satisfaction\nis evaluated a-posteriori via spoken or written questionnaires or interviews.\nIn automatic behavioral analysis we aim at measuring the user emotional\nstates and its descriptions as they unfold during the interaction.\nIn our approach,  user satisfaction is modeled as the final state of\na sequence of emotional states and given ternary values  positive,\nnegative, neutral. In this paper, we investigate the discriminating\npower of turn-taking in predicting user satisfaction in spoken conversations.\nTurn-taking is used for discourse organization of a conversation by\nmeans of explicit phrasing, intonation, and pausing. In this paper,\nwe train different characterization of turn-taking, such as competitiveness\nof the speech overlaps. To extract turn-taking features we design a\nturn segmentation and labeling system that incorporates lexical and\nacoustic information. Given a human-human spoken dialog, our system\nautomatically infers any of the three values of the state of the user\nsatisfaction. We evaluate the classification system on real-life call-center\nhuman-human dialogs. The comparative performance analysis shows that\nthe contribution of the turn-taking features outperforms both prosodic\nand lexical features.\n"
   ],
   "doi": "10.21437/Interspeech.2016-859"
  },
  "oertel16_interspeech": {
   "authors": [
    [
     "Catharine",
     "Oertel"
    ],
    [
     "Joakim",
     "Gustafson"
    ],
    [
     "Alan W.",
     "Black"
    ]
   ],
   "title": "Towards Building an Attentive Artificial Listener: On the Perception of Attentiveness in Feedback Utterances",
   "original": "1274",
   "page_count": 5,
   "order": 619,
   "p1": "2915",
   "pn": "2919",
   "abstract": [
    "Current speech synthesizers typically lack backchannel tokens. Those\nsynthesiser, which include backchannels, typically only support a limited\nset of stereotypical functions. However, this does not mirror the subtleties\nof backchannels in spontaneous conversations. If we want to be able\nto build an artificial listener, that can display degrees of attentiveness,\nwe need a speech synthesizer with more fine-grained control of the\nprosodic realisations of its backchannels.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  In the current study\nwe used a corpus of three-party face-to-face discussions to sample\nbackchannels produced under varying conversational dynamics. We wanted\nto understand i) which prosodic cues are relevant for the perception\nof varying degrees of attentiveness ii) how much of a difference is\nnecessary for people to perceive a difference in attentiveness iii)\nwhether a preliminary classifier could be trained to distinguish between\nmore and less attentive backchannel token.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1274"
  },
  "gwon16_interspeech": {
   "authors": [
    [
     "Youngjune L.",
     "Gwon"
    ],
    [
     "William M.",
     "Campbell"
    ],
    [
     "Douglas E.",
     "Sturim"
    ],
    [
     "H.T.",
     "Kung"
    ]
   ],
   "title": "Language Recognition via Sparse Coding",
   "original": "0881",
   "page_count": 5,
   "order": 620,
   "p1": "2920",
   "pn": "2924",
   "abstract": [
    "Spoken language recognition requires a series of signal processing\nsteps and learning algorithms to model distinguishing characteristics\nof different languages. In this paper, we present a sparse discriminative\nfeature learning framework for language recognition. We use sparse\ncoding, an unsupervised method, to compute efficient representations\nfor spectral features from a speech utterance while learning basis\nvectors for language models. Differentiated from existing approaches\nin sparse representation classification, we introduce a maximum a posteriori\n(MAP) adaptation scheme based on online learning that further optimizes\nthe discriminative quality of sparse-coded speech features. We empirically\nvalidate the effectiveness of our approach using the NIST LRE 2015\ndataset.\n"
   ],
   "doi": "10.21437/Interspeech.2016-881"
  },
  "fernando16_interspeech": {
   "authors": [
    [
     "Sarith",
     "Fernando"
    ],
    [
     "Vidhyasaharan",
     "Sethu"
    ],
    [
     "Eliathamby",
     "Ambikairajah"
    ]
   ],
   "title": "A Feature Normalisation Technique for PLLR Based Language Identification Systems",
   "original": "0560",
   "page_count": 5,
   "order": 621,
   "p1": "2925",
   "pn": "2929",
   "abstract": [
    "Phone log-likelihood ratio (PLLR) features have been shown to be effective\nin language identification systems. However, PLLR feature distributions\nare bounded and this may contradict assumptions of Gaussianity and\nconsequently lead to reduced language recognition rates. In this paper,\nwe propose a feature normalisation technique for the PLLR feature space\nand demonstrate that it can outperform conventional normalisation and\ndecorrelation techniques such as mean-variance normalisation, feature\nwarping, discrete cosine transform and principal component analysis.\nExperimental results on the NIST LRE 2007 and the NIST LRE 2015 databases\nshow that the proposed method outperforms other normalisation methods\nby at least 9.3% in terms of %Cavg. Finally, unlike PCA which needs\nto be estimated from all the training data, the proposed technique\ncan be applied on each utterance independently.\n"
   ],
   "doi": "10.21437/Interspeech.2016-560"
  },
  "kv16_interspeech": {
   "authors": [
    [
     "Mounika",
     "K.V."
    ],
    [
     "Sivanand",
     "Achanta"
    ],
    [
     "Lakshmi H.",
     "R."
    ],
    [
     "Suryakanth V.",
     "Gangashetty"
    ],
    [
     "Anil Kumar",
     "Vuppala"
    ]
   ],
   "title": "An Investigation of Deep Neural Network Architectures for Language Recognition in Indian Languages",
   "original": "0910",
   "page_count": 4,
   "order": 622,
   "p1": "2930",
   "pn": "2933",
   "abstract": [
    "In this paper, deep neural networks are investigated for language identification\nin Indian languages. Deep neural networks (DNN) have been recently\nproposed for this task. However many architectural choices and training\naspects that have been made while building such systems have not been\nstudied carefully. We perform several experiments on a dataset consisting\nof 12 Indian languages with a total training data of about 120 hours\nin evaluating the effect of such choices.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  While DNN based approach\nis inherently a frame based one, we propose an attention mechanism\nbased DNN architecture for utterance level classification there by\nefficiently making use of the context. Evaluation of models were performed\non 30 hours of testing data with 2.5 hours for each language. In our\nresults, we find that deeper architectures outperform shallower counterparts.\nAlso, DNN with attention mechanism outperforms the regular DNN models\nindicating the effectiveness of attention mechanism.\n"
   ],
   "doi": "10.21437/Interspeech.2016-910"
  },
  "ali16_interspeech": {
   "authors": [
    [
     "Ahmed",
     "Ali"
    ],
    [
     "Najim",
     "Dehak"
    ],
    [
     "Patrick",
     "Cardinal"
    ],
    [
     "Sameer",
     "Khurana"
    ],
    [
     "Sree Harsha",
     "Yella"
    ],
    [
     "James",
     "Glass"
    ],
    [
     "Peter",
     "Bell"
    ],
    [
     "Steve",
     "Renals"
    ]
   ],
   "title": "Automatic Dialect Detection in Arabic Broadcast Speech",
   "original": "1297",
   "page_count": 5,
   "order": 623,
   "p1": "2934",
   "pn": "2938",
   "abstract": [
    "In this paper, we investigate different approaches for dialect identification\nin Arabic broadcast speech. These methods are based on phonetic and\nlexical features obtained from a speech recognition system, and bottleneck\nfeatures using the i-vector framework. We studied both generative and\ndiscriminative classifiers, and we combined these features using a\nmulti-class Support Vector Machine (SVM). We validated our results\non an Arabic/English language identification task, with an accuracy\nof 100%. We also evaluated these features in a binary classifier to\ndiscriminate between Modern Standard Arabic (MSA) and Dialectal Arabic,\nwith an accuracy of 100%. We further reported results using the proposed\nmethods to discriminate between the five most widely used dialects\nof Arabic: namely Egyptian, Gulf, Levantine, North African, and MSA,\nwith an accuracy of 59.2%. We discuss dialect identification errors\nin the context of dialect code-switching between Dialectal Arabic and\nMSA, and compare the error pattern between manually labeled data, and\nthe output from our classifier. All the data used on our experiments\nhave been released to the public as a language identification corpus.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1297"
  },
  "ng16_interspeech": {
   "authors": [
    [
     "Raymond W.M.",
     "Ng"
    ],
    [
     "Bhusan",
     "Chettri"
    ],
    [
     "Thomas",
     "Hain"
    ]
   ],
   "title": "Combining Weak Tokenisers for Phonotactic Language Recognition in a Resource-Constrained Setting",
   "original": "0630",
   "page_count": 5,
   "order": 624,
   "p1": "2939",
   "pn": "2943",
   "abstract": [
    "In the phonotactic approach for language recognition, a phone tokeniser\nis normally used to transform the audio signal into acoustic tokens.\nThe language identity of the speech is modelled by the occurrence statistics\nof the decoded tokens. The performance of this approach depends heavily\non the quality of the audio tokeniser. A high-quality tokeniser in\nmatched condition is not always available for a language recognition\ntask. This study investigated into the performance of a phonotactic\nlanguage recogniser in a resource-constrained setting, following NIST\nLRE 2015 specification. An ensemble of phone tokenisers was constructed\nby applying unsupervised sequence training on different target languages\nfollowed by a score-based fusion. This method gave 5&#8211;7% relative\nperformance improvement to baseline system on LRE 2015 eval set. This\ngain was retained when the ensemble phonotactic system was further\nfused with an acoustic iVector system.\n"
   ],
   "doi": "10.21437/Interspeech.2016-630"
  },
  "geng16_interspeech": {
   "authors": [
    [
     "Wang",
     "Geng"
    ],
    [
     "Wenfu",
     "Wang"
    ],
    [
     "Yuanyuan",
     "Zhao"
    ],
    [
     "Xinyuan",
     "Cai"
    ],
    [
     "Bo",
     "Xu"
    ]
   ],
   "title": "End-to-End Language Identification Using Attention-Based Recurrent Neural Networks",
   "original": "0686",
   "page_count": 5,
   "order": 625,
   "p1": "2944",
   "pn": "2948",
   "abstract": [
    "This paper proposes a novel attention-based recurrent neural network\n(RNN) to build an end-to-end automatic language identification (LID)\nsystem. Inspired by the success of attention mechanism on a range of\nsequence-to-sequence tasks, this work introduces the attention mechanism\nwith long short term memory (LSTM) encoder to the sequence-to-tag LID\ntask. This unified architecture extends the end-to-end training method\nto LID system and dramatically boosts the system performance. Firstly,\na language category embedding module is used to provide attentional\nvector which guides the derivation of the utterance level representation.\nSecondly, two attention approaches are explored: a soft attention which\nattends all source frames and a hard one that focuses on a subset of\nthe sequential input. Thirdly, a hybrid test method which traverses\nall gold labels is adopted in the inference phase. Experimental results\nshow that 8.2% relative equal error rate (EER) reduction is obtained\ncompared with the LSTM-based frame level system by the soft approach\nand 34.33% performance improvement is observed compared to the conventional\ni-Vector system.\n"
   ],
   "doi": "10.21437/Interspeech.2016-686"
  },
  "sagha16_interspeech": {
   "authors": [
    [
     "Hesam",
     "Sagha"
    ],
    [
     "Pavel",
     "Matějka"
    ],
    [
     "Maryna",
     "Gavryukova"
    ],
    [
     "Filip",
     "Povolny"
    ],
    [
     "Erik",
     "Marchi"
    ],
    [
     "Björn",
     "Schuller"
    ]
   ],
   "title": "Enhancing Multilingual Recognition of Emotion in Speech by Language Identification",
   "original": "0333",
   "page_count": 5,
   "order": 626,
   "p1": "2949",
   "pn": "2953",
   "abstract": [
    "We investigate, for the first time, if applying model selection based\non automatic language identification (LID) can improve multilingual\nrecognition of emotion in speech. Six emotional speech corpora from\nthree language families (Germanic, Romance, Sino-Tibetan) are evaluated.\nThe emotions are represented by the quadrants in the arousal/valence\nplane, i. e., positive/ negative arousal/valence. Four selection approaches\nfor choosing an optimal training set depending on the current language\nare compared: within the same language family, across language family,\nuse of all available corpora, and selection based on the automatic\nLID. We found that, on average, the proposed LID approach for selecting\ntraining corpora is superior to using all the available corpora when\nthe spoken language is not known.\n"
   ],
   "doi": "10.21437/Interspeech.2016-333"
  },
  "mun16_interspeech": {
   "authors": [
    [
     "Seongkyu",
     "Mun"
    ],
    [
     "Suwon",
     "Shon"
    ],
    [
     "Wooil",
     "Kim"
    ],
    [
     "Hanseok",
     "Ko"
    ]
   ],
   "title": "Deep Neural Network Bottleneck Features for Acoustic Event Recognition",
   "original": "1112",
   "page_count": 4,
   "order": 627,
   "p1": "2954",
   "pn": "2957",
   "abstract": [
    "Bottleneck features have been shown to be effective in improving the\naccuracy of speaker recognition, language identification and automatic\nspeech recognition. However, few works have focused on bottleneck features\nfor acoustic event recognition. This paper proposes a novel acoustic\nevent recognition framework using bottleneck features derived from\na Deep Neural Network (DNN). In addition to conventional features (MFCC,\nMel-spectrum, etc.), this paper employs rhythm, timbre, and spectrum-statistics\nfeatures for effectively extracting acoustic characteristics from audio\nsignals. The effectiveness of the proposed method is demonstrated on\na database of real life recordings via experiments, and its robust\nperformance is verified by comparing to conventional methods.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1112"
  },
  "origlia16_interspeech": {
   "authors": [
    [
     "Antonio",
     "Origlia"
    ],
    [
     "Francesco",
     "Cutugno"
    ]
   ],
   "title": "Combining Energy and Cross-Entropy Analysis for Nuclear Segments Detection",
   "original": "1345",
   "page_count": 5,
   "order": 628,
   "p1": "2958",
   "pn": "2962",
   "abstract": [
    "Features related to rhythmic patterns are involved in the representation\nof the intonational content for spoken language analysis. Among others,\nspeech rate is one of the most used measures extracted by systems using\nprosodic analysis and is typically measured in syllables per second.\nAutomatic approaches designed to estimate this measure in absence of\nmanual annotations usually mark the position of syllable nuclei as\na single point in time. Approaches extracting duration features using\nautomatic segmentation in units shorter than words but larger than\nphones tend to detect syllables. To represent the prosodic contents\nof an utterance, especially from the rhythmic point of view, automatic\npositioning of nuclear boundaries may, however, be more informative\nthan syllable boundaries. In this paper we present a method combining\nthe analysis of the energy envelope and of the cross-entropy profile\nto obtain a segmentation into nuclear and inter-nuclear segments, showing\nthat the proposed method can be used to obtain a reliable estimate\nof speech rate and that accuracy in nuclear boundary positioning allows\nthe extraction of segmental features useful for automatic prosodic\nanalysis.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1345"
  },
  "maas16_interspeech": {
   "authors": [
    [
     "Roland",
     "Maas"
    ],
    [
     "Sree Hari Krishnan",
     "Parthasarathi"
    ],
    [
     "Brian",
     "King"
    ],
    [
     "Ruitong",
     "Huang"
    ],
    [
     "Björn",
     "Hoffmeister"
    ]
   ],
   "title": "Anchored Speech Detection",
   "original": "1346",
   "page_count": 5,
   "order": 629,
   "p1": "2963",
   "pn": "2967",
   "abstract": [
    "We propose two new methods of speech detection in the context of voice-controlled\nfar-field appliances. While conventional detection methods are designed\nto differentiate between speech and nonspeech, we aim at distinguishing\n desired speech, which we define as speech originating from the person\ninteracting with the device, from background noise and interfering\ntalkers. Our two proposed methods use the first word spoken by the\ndesired talker, the &#8220;anchor&#8221; word, as a reference to learn\ncharacteristics about that speaker. In the first method, we estimate\nthe mean of the anchor word segment and subtract it from the subsequent\nfeature vectors. In the second, we use an encoder-decoder network with\nfeatures that are normalized by applying conventional log amplitude\ncausal mean subtraction. The experimental results reveal that both\ntechniques achieve around 10% relative reduction in frame classification\nerror rate over a baseline feed-forward network with conventionally\nnormalized features.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1346"
  },
  "nandwana16_interspeech": {
   "authors": [
    [
     "Mahesh Kumar",
     "Nandwana"
    ],
    [
     "Taufiq",
     "Hasan"
    ]
   ],
   "title": "Towards Smart-Cars That Can Listen: Abnormal Acoustic Event Detection on the Road",
   "original": "1366",
   "page_count": 4,
   "order": 630,
   "p1": "2968",
   "pn": "2971",
   "abstract": [
    "Even with the recent technological advancements in smart-cars, safety\nis still a major challenge in autonomous driving. State-of-the-art\nself-driving vehicles mostly rely on visual, ultrasonic and radar sensors\nto assess the surroundings and make decisions. However, in certain\ndriving scenarios, the best modality for context awareness is environmental\nsound. In this study, we propose an acoustic event recognition framework\nfor detecting abnormal audio events on the road. We consider five classes\nof audio events, namely, ambulance siren, railroad crossing bell, tire\nscreech, car honk, and glass break. We explore various generative and\ndiscriminative back-end classifiers, utilizing Gaussian Mixture Models\n(GMM), GMM mean supervectors and the I-vector framework. Evaluation\nresults using the proposed strategy validate the effectiveness of the\nproposed system.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1366"
  },
  "girish16_interspeech": {
   "authors": [
    [
     "K.V. Vijay",
     "Girish"
    ],
    [
     "A.G.",
     "Ramakrishnan"
    ],
    [
     "T.V.",
     "Ananthapadmanabha"
    ]
   ],
   "title": "Hierarchical Classification of Speaker and Background Noise and Estimation of SNR Using Sparse Representation",
   "original": "0175",
   "page_count": 5,
   "order": 631,
   "p1": "2972",
   "pn": "2976",
   "abstract": [
    "In the analysis of recordings of conversations, one of the motivations\nis to be able to identify the nature of background noise as a means\nof identifying the possible geographical location of a speaker. In\na high noise environment, to minimize manual analysis of the recording,\nit is also desirable to automatically locate only the segments of the\nrecording, which contain speech. The next task is to identify if the\nspeech is from one of the known people. A dictionary learning and block\nsparsity based source recovery approach has been used to estimate the\nSNR of a noisy speech recording, simulated at different SNRs using\nten different noise sources. Given a test utterance, a noise label\nis assigned using block sparsity approach, and subsequently, the speaker\nis classified using sum of weights recovered from the concatenation\nof speaker dictionaries and the identified noise source dictionary.\nUsing the dictionaries of the identified speaker and noise sources,\nframewise speech and noise energy are estimated using a source recovery\nmethod. The energy estimates are then used to identify the segments,\nwhere speech is present. We obtain 100% accuracy for background classification\nand around 90% for speaker classification at a SNR of 10 dB.\n"
   ],
   "doi": "10.21437/Interspeech.2016-175"
  },
  "zhang16g_interspeech": {
   "authors": [
    [
     "Haomin",
     "Zhang"
    ],
    [
     "Ian",
     "McLoughlin"
    ],
    [
     "Yan",
     "Song"
    ]
   ],
   "title": "Robust Sound Event Detection in Continuous Audio Environments",
   "original": "0392",
   "page_count": 5,
   "order": 632,
   "p1": "2977",
   "pn": "2981",
   "abstract": [
    "Sound event detection in real world environments has attracted significant\nresearch interest recently because of it&#8217;s applications in popular\nfields such as machine hearing and automated surveillance, as well\nas in sound scene understanding. This paper considers continuous robust\nsound event detection, which means multiple overlapped sound events\nin different types of interfering noise. First, a standard evaluation\ntask is outlined based upon existing testing data sets for the sound\nevent classification of isolated sounds. This paper then proposes and\nevaluates the use of spectrogram image features employing an energy\ndetector to segment sound events, before developing a novel segmentation\nmethod making use of a Bayesian inference criteria. At the back end,\na convolutional neural network is used to classify detected regions,\nand this combination is compared to several alternative approaches.\nThe proposed method is shown capable of achieving very good performance\ncompared with current state-of-the-art techniques.\n"
   ],
   "doi": "10.21437/Interspeech.2016-392"
  },
  "takahashi16b_interspeech": {
   "authors": [
    [
     "Naoya",
     "Takahashi"
    ],
    [
     "Michael",
     "Gygli"
    ],
    [
     "Beat",
     "Pfister"
    ],
    [
     "Luc Van",
     "Gool"
    ]
   ],
   "title": "Deep Convolutional Neural Networks and Data Augmentation for Acoustic Event Recognition",
   "original": "0805",
   "page_count": 5,
   "order": 633,
   "p1": "2982",
   "pn": "2986",
   "abstract": [
    "We propose a novel method for Acoustic Event Recognition (AER). In\ncontrast to speech, sounds coming from acoustic events may be produced\nby a wide variety of sources. Furthermore, distinguishing them often\nrequires analyzing an extended time period due to the lack of a clear\nsub-word unit. In order to incorporate the long-time frequency structure\nfor AER, we introduce a convolutional neural network (CNN) with a large\ninput field. In contrast to previous works, this enables to train audio\nevent detection end-to-end. Our architecture is inspired by the success\nof VGGNet [1] and uses small, 3&#215;3 convolutions, but more depth\nthan previous methods in AER. In order to prevent over-fitting and\nto take full advantage of the modeling capabilities of our network,\nwe further propose a novel data augmentation method to introduce data\nvariation. Experimental results show that our CNN significantly outperforms\nstate of the art methods including Bag of Audio Words (BoAW) and classical\nCNNs, achieving a 16% absolute improvement.\n"
   ],
   "doi": "10.21437/Interspeech.2016-805"
  },
  "meier16_interspeech": {
   "authors": [
    [
     "Stefan",
     "Meier"
    ],
    [
     "Walter",
     "Kellermann"
    ]
   ],
   "title": "Artificial Neural Network-Based Feature Combination for Spatial Voice Activity Detection",
   "original": "1184",
   "page_count": 5,
   "order": 634,
   "p1": "2987",
   "pn": "2991",
   "abstract": [
    "For many applications in speech communications and speech-based human-machine\ninteraction, a reliable Voice Activity Detection (VAD) is crucial.\nConventional methods for VAD typically differentiate between a target\nspeaker and background noise by exploiting characteristic properties\nof speech signals. If a target speaker should be distinguished from\nother speech sources, these conventional concepts are no longer applicable,\nand other methods, typically exploiting the spatial diversity of the\nindividual sources, are required. Often, it is beneficial to combine\nseveral features in order to improve the overall decision. Optimum\ncombinations of features, however, depend strongly on the scenario,\nespecially on the position of the target source, the characteristics\nof noise and interference and the Signal-to-Interference Ratio (SIR).\nMoreover, choosing detection thresholds which are robust to changing\nscenarios is often a difficult problem. In this paper, these issues\nare addressed by introducing Artificial Neural Networks (ANNs) for\nspatial voice activity detection, which allow to combine several features\nwith background information. The experimental results show that already\nsmall ANNs can significantly and robustly improve the detection rates,\noffering a valuable tool for VAD.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1184"
  },
  "kinnunen16b_interspeech": {
   "authors": [
    [
     "Tomi",
     "Kinnunen"
    ],
    [
     "Alexey",
     "Sholokhov"
    ],
    [
     "Elie",
     "Khoury"
    ],
    [
     "Dennis Alexander Lehmann",
     "Thomsen"
    ],
    [
     "Md.",
     "Sahidullah"
    ],
    [
     "Zheng-Hua",
     "Tan"
    ]
   ],
   "title": "HAPPY Team Entry to NIST OpenSAD Challenge: A Fusion of Short-Term Unsupervised and Segment i-Vector Based Speech Activity Detectors",
   "original": "1281",
   "page_count": 5,
   "order": 635,
   "p1": "2992",
   "pn": "2996",
   "abstract": [
    "Speech activity detection (SAD), the task of locating speech segments\nfrom a given recording, remains challenging under acoustically degraded\nconditions. In 2015, National Institute of Standards and Technology\n(NIST) coordinated OpenSAD bench-mark. We summarize &#8220;HAPPY&#8221;\nteam effort to OpenSAD. SADs come in both unsupervised and supervised\nflavors, the latter requiring a labeled training set. Our solution\nfuses six base SADs (2 supervised and 4 unsupervised). The individually\nbest SAD, in terms of detection cost function (DCF), is supervised\nand uses adaptive segmentation with i-vectors to represent the segments.\nFusion of the six base SADs yields a relative decrease of 9.3% in DCF\nover this SAD. Further, relative decrease of 17.4% is obtained by incorporating\nchannel detection side information.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1281"
  },
  "pokorny16b_interspeech": {
   "authors": [
    [
     "Florian B.",
     "Pokorny"
    ],
    [
     "Robert",
     "Peharz"
    ],
    [
     "Wolfgang",
     "Roth"
    ],
    [
     "Matthias",
     "Zöhrer"
    ],
    [
     "Franz",
     "Pernkopf"
    ],
    [
     "Peter B.",
     "Marschik"
    ],
    [
     "Björn",
     "Schuller"
    ]
   ],
   "title": "Manual versus Automated: The Challenging Routine of Infant Vocalisation Segmentation in Home Videos to Study Neuro(mal)development",
   "original": "1341",
   "page_count": 5,
   "order": 636,
   "p1": "2997",
   "pn": "3001",
   "abstract": [
    "In recent years, voice activity detection has been a highly researched\nfield, due to its importance as input stage in many real-world applications.\nAutomated detection of vocalisations in the very first year of life\nis still a stepchild of this field. On our quest defining acoustic\nparameters in pre-linguistic vocalisations as markers for neuro(mal)development,\nwe are confronted with the challenge of manually segmenting and annotating\nhours of variable quality home video material for sequences of infant\nvoice/vocalisations. While in total our corpus comprises video footage\nof typically developing infants and infants with various neurodevelopmental\ndisorders of more than a year running time, only a small proportion\nhas been processed so far. This calls for automated assistance tools\nfor detecting and/or segmenting infant utterances from real-live video\nrecordings. In this paper, we investigated several approaches of infant\nvoice detection and segmentation, including a rule-based voice activity\ndetector, hidden Markov models with Gaussian mixture observation models,\nsupport vector machines, and random forests. Results indicate that\nthe applied methods could be well applied in a semi-automated retrieval\nof infant utterances from highly non-standardised footage. At the same\ntime, our results show that, a fully automated approach for this problem\nis yet to come.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1341"
  },
  "ferrer16_interspeech": {
   "authors": [
    [
     "Luciana",
     "Ferrer"
    ],
    [
     "Martin",
     "Graciarena"
    ]
   ],
   "title": "Minimizing Annotation Effort for Adaptation of Speech-Activity Detection Systems",
   "original": "0247",
   "page_count": 5,
   "order": 637,
   "p1": "3002",
   "pn": "3006",
   "abstract": [
    "Annotating audio data for the presence and location of speech is a\ntime-consuming and therefore costly task. This is mostly because annotation\nprecision greatly affects the performance of the speech-activity detection\n(SAD) systems trained with this data, which means that the annotation\nprocess must be careful and detailed. Although significant amounts\nof data are already annotated for speech presence and are available\nto train SAD systems, these systems are known to perform poorly on\nchannels that are not well-represented by the training data. However\nobtaining representative audio samples from a new channel is relative\neasy and this data can be used for training a new SAD system or adapting\none trained with larger amounts of mismatched data. This paper focuses\non the problem of selecting the best-possible subset of available audio\ndata given a budgeted time for annotation. We propose simple approaches\nfor selection that lead to significant gains over na&#239;ve methods\nthat merely select N full files at random. An approach that uses the\nframe-level scores from a baseline system to select regions such that\nthe score distribution is uniformly sampled gives the best trade-off\nacross a variety of channel groups.\n"
   ],
   "doi": "10.21437/Interspeech.2016-247"
  },
  "moore16b_interspeech": {
   "authors": [
    [
     "Roger K.",
     "Moore"
    ],
    [
     "Hui",
     "Li"
    ],
    [
     "Shih-Hao",
     "Liao"
    ]
   ],
   "title": "Progress and Prospects for Spoken Language Technology: What Ordinary People Think",
   "original": "0874",
   "page_count": 5,
   "order": 638,
   "p1": "3007",
   "pn": "3011",
   "abstract": [
    "Arguably the most significant milestone (so far) in the spoken language\ntechnology field was the appearance in November 2011 of  Siri &#8212;\nApple&#8217;s voice-based &#8216;personal assistant and knowledge navigator&#8217;\nfor the iPhone.  Siri brought the potential of spoken language technology\nto the attention of the wider general public, and speech finally became\n&#8220; mainstream&#8221;. This meant that ordinary people suddenly\nhad an informed opinion about the merits (or otherwise) of using their\nvoice to access information, send messages and control their smart\ndevices. So, this paper presents the results of two surveys that were\nconducted in order to find out what ordinary people think about contemporary\nspoken language technology. The first used a modified version of the\nsurveys conducted every six years at the IEEE ASRU series of workshops,\nand the second addressed questions about the awareness and usage of\nspeech technology by members of the general public. The overall results\nsuggest that ordinary people are more optimistic than the experts about\nwhat spoken language technology might have to offer, but usage patterns\nreveal that the majority of end users still prefer typing to talking,\nwith accuracy, privacy and online accessibility cited as the main impediments\nto wider take-up.\n"
   ],
   "doi": "10.21437/Interspeech.2016-874"
  },
  "moore16c_interspeech": {
   "authors": [
    [
     "Roger K.",
     "Moore"
    ],
    [
     "Ricard",
     "Marxer"
    ]
   ],
   "title": "Progress and Prospects for Spoken Language Technology: Results from Four Sexennial Surveys",
   "original": "0948",
   "page_count": 5,
   "order": 639,
   "p1": "3012",
   "pn": "3016",
   "abstract": [
    "Since 1997, a survey has been conducted every six years at the IEEE\nworkshop on  Automatic Speech Recognition and Understanding (ASRU)\nin order to ascertain the research community&#8217;s perspective on\nfuture progress and prospects in spoken language technology. These\nsurveys have been based on a set of &#8216;statements&#8217;, each\nof which portray a possible future scenario, and respondents are asked\nto estimate the year in which each given scenario might become true.\nMany of the statements have appeared in several of the surveys, hence\nit is possible to track changes in opinion over time. This paper presents\nthe combined results of all four surveys, the most recent of which\nwas conducted at ASRU-2015. The results give an insight into the key\ntrends that are taking place in the spoken language technology field,\nand reveal the realism that pervades the research community. They also\nsuggest that there is growing confidence that some of the scenarios\nwill indeed be realised at some point in the future.\n"
   ],
   "doi": "10.21437/Interspeech.2016-948"
  },
  "radadia16_interspeech": {
   "authors": [
    [
     "Purushotam",
     "Radadia"
    ],
    [
     "Rahul",
     "Kumar"
    ],
    [
     "Kanika",
     "Kalra"
    ],
    [
     "Shirish",
     "Karande"
    ],
    [
     "Sachin",
     "Lodha"
    ]
   ],
   "title": "On Employing a Highly Mismatched Crowd for Speech Transcription",
   "original": "0673",
   "page_count": 5,
   "order": 640,
   "p1": "3017",
   "pn": "3021",
   "abstract": [
    "Crowd sourcing provides a cheap and fast way to obtain speech transcriptions.\nThe crowd size available for a task is inversely proportional to the\nskill requirements. Hence, there has been recent interest in studying\nthe utility of mismatched crowd workers, who provide transcriptions\neven without knowing the source language. Nevertheless, these studies\nhave required that the worker be capable of providing a transcription\nin Roman script. We believe that if the script constraint is removed,\nthen countries like India can provide significantly larger crowd base.\nWith this as a motivation, in this paper, we consider transcription\nof spoken Russian words by a rural Indian crowd that is unfamiliar\nwith Russian and has very limited knowledge of English. The crowd we\nemploy knew Gujarati, Marathi, Telugu and used the scripts of these\nlanguages to provide their transcriptions. We utilized an insertion-deletion-substitution\nchannel to model the transcription errors. With a parallel channel\nmodel we can easily combine the crowd inputs. We show that the 4 transcriptions\nin Indic scripts (2 Gujarati, 1 Marathi, 1 Telugu) provide an accuracy\nof 73.77 (vs. 47% for ROVER algorithm) and a 4-best accuracy of 86.48%,\neven without employing any worker filtering.\n"
   ],
   "doi": "10.21437/Interspeech.2016-673"
  },
  "hsiao16_interspeech": {
   "authors": [
    [
     "Roger",
     "Hsiao"
    ],
    [
     "Ralf",
     "Meermeier"
    ],
    [
     "Tim",
     "Ng"
    ],
    [
     "Zhongqiang",
     "Huang"
    ],
    [
     "Maxwell",
     "Jordan"
    ],
    [
     "Enoch",
     "Kan"
    ],
    [
     "Tanel",
     "Alumäe"
    ],
    [
     "Jan",
     "Silovsky"
    ],
    [
     "William",
     "Hartmann"
    ],
    [
     "Francis",
     "Keith"
    ],
    [
     "Omer",
     "Lang"
    ],
    [
     "Manhung",
     "Siu"
    ],
    [
     "Owen",
     "Kimball"
    ]
   ],
   "title": "Sage: The New BBN Speech Processing Platform",
   "original": "1031",
   "page_count": 5,
   "order": 641,
   "p1": "3022",
   "pn": "3026",
   "abstract": [
    "To capitalize on the rapid development of Speech-to-Text (STT) technologies\nand the proliferation of open source machine learning toolkits, BBN\nhas developed Sage, a new speech processing platform that integrates\ntechnologies from multiple sources, each of which has particular strengths.\nIn this paper, we describe the design of Sage, which allows the easy\ninterchange of STT components from different sources. We also describe\nour approach for fast prototyping with new machine learning toolkits,\nand a framework for sharing STT components across different applications.\nFinally, we report Sage&#8217;s state-of-the-art performance on different\nSTT tasks.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1031"
  },
  "lee16d_interspeech": {
   "authors": [
    [
     "Kang Hyun",
     "Lee"
    ],
    [
     "Tae Gyoon",
     "Kang"
    ],
    [
     "Woo Hyun",
     "Kang"
    ],
    [
     "Nam Soo",
     "Kim"
    ]
   ],
   "title": "DNN-Based Feature Enhancement Using Joint Training Framework for Robust Multichannel Speech Recognition",
   "original": "0105",
   "page_count": 5,
   "order": 642,
   "p1": "3027",
   "pn": "3031",
   "abstract": [
    "Ever since the deep neural network (DNN) appeared in the speech signal\nprocessing society, the recognition performance of automatic speech\nrecognition (ASR) has been greatly improved. Due to this achievement,\nthe demands on various applications in distant-talking environment\nalso have been increased. However, ASR performance in such environments\nis still far from that in close-talking environments due to various\nproblems. In this paper, we propose a novel multichannel-based feature\nmapping technique combining conventional beamformer, DNN and its joint\ntraining scheme. Through the experiments using multichannel wall street\njournal audio visual (MC-WSJ-AV) corpus, it has been shown that the\nproposed technique models the complicated relationship between the\narray inputs and clean speech features effectively via employing intermediate\ntarget. The proposed method outperformed the conventional DNN system.\n"
   ],
   "doi": "10.21437/Interspeech.2016-105"
  },
  "wand16_interspeech": {
   "authors": [
    [
     "Michael",
     "Wand"
    ],
    [
     "Jürgen",
     "Schmidhuber"
    ]
   ],
   "title": "Deep Neural Network Frontend for Continuous EMG-Based Speech Recognition",
   "original": "0340",
   "page_count": 5,
   "order": 643,
   "p1": "3032",
   "pn": "3036",
   "abstract": [
    "We report on a  Deep Neural Network frontend for a continuous speech\nrecognizer based on Surface Electromyography (EMG). Speech data is\nobtained by facial electrodes capturing the electric activity generated\nby the articulatory muscles, thus allowing speech processing without\nmaking use of the acoustic signal. The electromyographic signal is\npreprocessed and fed into the neural network, which is trained on framewise\ntargets; the output layer activations are further processed by a Hidden\nMarkov sequence classifier. We show that such a neural network frontend\ncan be trained on EMG data and yields substantial improvements over\nprevious systems, despite the fact that the available amount of data\nis very small, just amounting to a few tens of sentences: on the  EMG-UKA\ncorpus, we obtain average evaluation set Word Error Rate improvements\nof more than 32% relative on context-independent phone models and 13%\nrelative on versatile  Bundled Phonetic feature (BDPF) models, compared\nto a conventional system using Gaussian Mixture Models. In particular,\non simple context-independent phone models, the new system yields results\nwhich are almost as good as with BDPF models, which were specifically\ndesigned to cope with small amounts of training data.\n"
   ],
   "doi": "10.21437/Interspeech.2016-340"
  },
  "abraham16b_interspeech": {
   "authors": [
    [
     "Basil",
     "Abraham"
    ],
    [
     "S.",
     "Umesh"
    ],
    [
     "Neethu Mariam",
     "Joy"
    ]
   ],
   "title": "Overcoming Data Sparsity in Acoustic Modeling of Low-Resource Language by Borrowing Data and Model Parameters from High-Resource Languages",
   "original": "0963",
   "page_count": 5,
   "order": 644,
   "p1": "3037",
   "pn": "3041",
   "abstract": [
    "In this paper, we propose two techniques to improve the acoustic model\nof a low-resource language by: (i) Pooling data from closely related\nlanguages using a phoneme mapping algorithm to build acoustic models\nlike subspace Gaussian mixture model (SGMM), phone cluster adaptive\ntraining (Phone-CAT), deep neural network (DNN) and convolutional neural\nnetwork (CNN). Using the low-resource language data, we then adapt\nthe afore mentioned models towards that language. (ii) Using models\nbuilt from high-resource languages, we first borrow subspace model\nparameters from SGMM/Phone-CAT; or hidden layers from DNN/CNN. The\nlanguage specific parameters are then estimated using the low-resource\nlanguage data. The experiments were performed on four Indian languages\nnamely Assamese, Bengali, Hindi and Tamil. Relative improvements of\n10 to 30% were obtained over corresponding monolingual models in each\ncase.\n"
   ],
   "doi": "10.21437/Interspeech.2016-963"
  },
  "ragni16_interspeech": {
   "authors": [
    [
     "Anton",
     "Ragni"
    ],
    [
     "Edgar",
     "Dakin"
    ],
    [
     "Xie",
     "Chen"
    ],
    [
     "Mark J.F.",
     "Gales"
    ],
    [
     "Kate M.",
     "Knill"
    ]
   ],
   "title": "Multi-Language Neural Network Language Models",
   "original": "0371",
   "page_count": 5,
   "order": 645,
   "p1": "3042",
   "pn": "3046",
   "abstract": [
    "In recent years there has been considerable interest in neural network\nbased language models. These models typically consist of vocabulary\ndependent input and output layers and one, or more, hidden layers.\nA standard problem with these networks is that large quantities of\ntraining data are needed to robustly estimate the model parameters.\nThis poses a challenge when only limited data is available for the\ntarget language. One way to address this issue is to make use of overlapping\nvocabularies between related languages. However this is only applicable\nto a small set of languages, and the impact is expected to be limited\nfor more general applications. This paper describes a general solution\nthat allows data from any language to be used. Here, only the input\nand output layers are vocabulary dependent whilst hidden layers are\nshared, language independent. This multi-task training set-up allows\nthe quantity of data available to train the hidden layers to be increased.\nThis multi-language network can be used in a range of configurations,\nincluding as initialisation for previously unseen languages. As a proof\nof concept this paper examines multilingual recurrent neural network\nlanguage models. Experiments are conducted using language packs released\nwithin the IARPA Babel program.\n"
   ],
   "doi": "10.21437/Interspeech.2016-371"
  },
  "tilk16_interspeech": {
   "authors": [
    [
     "Ottokar",
     "Tilk"
    ],
    [
     "Tanel",
     "Alumäe"
    ]
   ],
   "title": "Bidirectional Recurrent Neural Network with Attention Mechanism for Punctuation Restoration",
   "original": "1517",
   "page_count": 5,
   "order": 646,
   "p1": "3047",
   "pn": "3051",
   "abstract": [
    "Automatic speech recognition systems generally produce unpunctuated\ntext which is difficult to read for humans and degrades the performance\nof many downstream machine processing tasks. This paper introduces\na bidirectional recurrent neural network model with attention mechanism\nfor punctuation restoration in unsegmented text. The model can utilize\nlong contexts in both directions and direct attention where necessary\nenabling it to outperform previous state-of-the-art on English (IWSLT2011)\nand Estonian datasets by a large margin.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1517"
  },
  "enarvi16_interspeech": {
   "authors": [
    [
     "Seppo",
     "Enarvi"
    ],
    [
     "Mikko",
     "Kurimo"
    ]
   ],
   "title": "TheanoLM &#8212; An Extensible Toolkit for Neural Network Language Modeling",
   "original": "0618",
   "page_count": 5,
   "order": 647,
   "p1": "3052",
   "pn": "3056",
   "abstract": [
    "We present a new tool for training neural network language models (NNLMs),\nscoring sentences, and generating text. The tool has been written using\nPython library Theano, which allows researcher to easily extend it\nand tune any aspect of the training process. Regardless of the flexibility,\nTheano is able to generate extremely fast native code that can utilize\na GPU or multiple CPU cores in order to parallelize the heavy numerical\ncomputations. The tool has been evaluated in difficult Finnish and\nEnglish conversational speech recognition tasks, and significant improvement\nwas obtained over our best back-off n-gram models. The results that\nwe obtained in the Finnish task were compared to those from existing\nRNNLM and RWTHLM toolkits, and found to be as good or better, while\ntraining times were an order of magnitude shorter.\n"
   ],
   "doi": "10.21437/Interspeech.2016-618"
  },
  "lanchantin16_interspeech": {
   "authors": [
    [
     "P.",
     "Lanchantin"
    ],
    [
     "Mark J.F.",
     "Gales"
    ],
    [
     "Penny",
     "Karanasou"
    ],
    [
     "X.",
     "Liu"
    ],
    [
     "Y.",
     "Qian"
    ],
    [
     "L.",
     "Wang"
    ],
    [
     "P.C.",
     "Woodland"
    ],
    [
     "C.",
     "Zhang"
    ]
   ],
   "title": "Selection of Multi-Genre Broadcast Data for the Training of Automatic Speech Recognition Systems",
   "original": "0462",
   "page_count": 5,
   "order": 648,
   "p1": "3057",
   "pn": "3061",
   "abstract": [
    "This paper compares schemes for the selection of multi-genre broadcast\ndata and corresponding transcriptions for speech recognition model\ntraining. Selections of the same amount of data (700 hours) from lightly\nsupervised alignments based on the same original subtitle transcripts\nare compared. Data segments were selected according to a maximum phone\nmatched error rate between the lightly supervised decoding and the\noriginal transcript. The data selected with an improved lightly supervised\nsystem yields lower word error rates (WERs). Detailed comparisons of\nthe data selected on carefully transcribed development data show how\nthe selected portions match the true phone error rate for each genre.\nFrom a broader perspective, it is shown that for different genres,\neither the original subtitles or the lightly supervised output should\nbe used for model training and a suitable combination yields further\nreductions in final WER.\n"
   ],
   "doi": "10.21437/Interspeech.2016-462"
  },
  "gaur16_interspeech": {
   "authors": [
    [
     "Yashesh",
     "Gaur"
    ],
    [
     "Florian",
     "Metze"
    ],
    [
     "Jeffrey P.",
     "Bigham"
    ]
   ],
   "title": "Manipulating Word Lattices to Incorporate Human Corrections",
   "original": "0660",
   "page_count": 4,
   "order": 649,
   "p1": "3062",
   "pn": "3065",
   "abstract": [
    "Automatic Speech Recognition (ASR) is not perfect and even advanced\nstatistical models make errors that render its output difficult to\nunderstand. We are therefore interested in having Humans correct ASR\noutput efficiently. A naive approach, in which Humans manually &#8220;edit&#8221;\nthe ASR output, may work when the recognition is done offline, but\nfails in on-line scenarios when Humans cannot keep up. To address this\nproblem, our prior work introduced an approach that combines ASR and\nkeyword search (KWS) to allow Humans to simply type corrections for\nthe errors they observe, while the system positioned each correction\nusing KWS and then &#8220;stitches&#8221; in the correction. In this\npaper, we present an improved &#8220;stitching&#8221; algorithm that\nworks at the lattice level (rather than on the first-best string).\nWe show that this algorithm drastically improves the word error rate\n(WER) of a TED system when applied to a new corpus of CS lectures that\nhas not been carefully prepared for ASR experiments. We also show that\nthe system can fix annoying repeat errors from just a single correction,\nmaking it suitable for post-processing of large amounts of data from\nlimited corrections.\n"
   ],
   "doi": "10.21437/Interspeech.2016-660"
  },
  "fischer16b_interspeech": {
   "authors": [
    [
     "Philipp",
     "Fischer"
    ],
    [
     "Cornelius Styp von",
     "Rekowski"
    ],
    [
     "Andreas",
     "Nürnberger"
    ]
   ],
   "title": "Context-Aware Restaurant Recommendation for Natural Language Queries: A Formative User Study in the Automotive Domain",
   "original": "1503",
   "page_count": 5,
   "order": 650,
   "p1": "3066",
   "pn": "3070",
   "abstract": [
    "In this paper, the authors describe an extension to an approach previously\ndiscussed for personalization of a natural language system in the automotive\ndomain that allows reasoning under uncertainty with incomplete preference\nstructures. Therefore, the concept of an &#8220;information stream&#8221;\nis defined as an underlying model for real-time recommendation learned\nfrom previous speech queries. The stream captures contextual data based\non implicit feedback from the user&#8217;s speech utterances.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  Furthermore, a formative\nuser study is discussed. Each study iteration has been based on a prototype\nthat allows the user to utter natural language queries in the restaurant\ndomain. The system responds with a ranked list of restaurant recommendations\nin relation to the user&#8217;s context. Several driving scenarios\nwith varying contexts have been analyzed (e.g. weekday/ weekend, route\ndestinations, traffic). Users could inspect the result lists and indicate\nthe most preferred item. In addition to quantitative data gained from\nthis interaction, feedback on relevance of context features and on\nthe UI concept was collected in a post-study interview for each iteration.\nBased on the study findings, we outline the contextual features found\nto be most relevant for speech-based interaction in automotive applications.\nThese findings will be integrated into an existing hybrid recommendation\nmodel.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1503"
  },
  "pancoast16_interspeech": {
   "authors": [
    [
     "Stephanie",
     "Pancoast"
    ],
    [
     "Murat",
     "Akbacak"
    ]
   ],
   "title": "Teaming Up: Making the Most of Diverse Representations for a Novel Personalized Speech Retrieval Application",
   "original": "1589",
   "page_count": 5,
   "order": 651,
   "p1": "3071",
   "pn": "3075",
   "abstract": [
    "In addition to the increasing number of publicly available multimedia\ndocuments generated and searched every day, there is also a large corpora\nof personalized videos, images and spoken recordings, stored on users&#8217;\nprivate devices and/or in their personal accounts in the cloud. Retrieving\nspoken items via voice commonly involves supervised indexing approaches\nsuch as large vocabulary speech recognition. When these items are personalized\nrecordings, diverse and personalized content causes recognition systems\nto experience mis-matches mostly in vocabulary and language model components,\nand sometimes even in the language users use. All of these contribute\nto retrieval task performing very poorly. Alternatively, common audio\npatterns can be captured and used for exampler-based retrieval in an\nunsupervised fashion but this approach has its limitations as well.\nIn this work we explore supervised, unsupervised and fusion techniques\nto perform the retrieval of short personalized spoken utterances. On\na small collection of personal recordings, we find that when fusing\nword, phoneme and unsupervised frame based systems, we can improve\naccuracy on the top retrieved item approximately 3% above the best\nperforming individual system. Besides demonstrating this improvement\non our initial collection, we hope to attract community&#8217;s interest\nto such novel personalized retrieval applications.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1589"
  },
  "mitra16b_interspeech": {
   "authors": [
    [
     "Vikramjit",
     "Mitra"
    ],
    [
     "Andreas",
     "Kathol"
    ],
    [
     "Jonathan D.",
     "Amith"
    ],
    [
     "Rey Castillo",
     "García"
    ]
   ],
   "title": "Automatic Speech Transcription for Low-Resource Languages &#8212; The Case of Yolox&#243;chitl Mixtec (Mexico)",
   "original": "0546",
   "page_count": 5,
   "order": 652,
   "p1": "3076",
   "pn": "3080",
   "abstract": [
    "The rate at which endangered languages can be documented has been highly\nconstrained by human factors. Although digital recording of natural\nspeech in endangered languages may proceed at a fairly robust pace,\ntranscription of this material is not only time consuming but severely\nlimited by the lack of native-speaker personnel proficient in the orthography\nof their mother tongue. Our NSF-funded project in the Documenting Endangered\nLanguages (DEL) program proposes to tackle this problem from two sides:\nfirst via a tool that helps native speakers become proficient in the\northographic conventions of their language, and second by using automatic\nspeech recognition (ASR) output that assists in the transcription effort\nfor newly recorded audio data. In the present study, we focus exclusively\non progress in developing speech recognition for the language of interest,\nYolox&#243;chitl Mixtec (YM), an Oto-Manguean language spoken by fewer\nthan 5000 speakers on the Pacific coast of Guerrero, Mexico. In particular,\nwe present results from an initial set of experiments and discuss future\ndirections through which better and more robust acoustic models for\nendangered languages with limited resources can be created.\n"
   ],
   "doi": "10.21437/Interspeech.2016-546"
  },
  "asadi16_interspeech": {
   "authors": [
    [
     "Reza",
     "Asadi"
    ],
    [
     "Harriet J.",
     "Fell"
    ],
    [
     "Timothy",
     "Bickmore"
    ],
    [
     "Ha",
     "Trinh"
    ]
   ],
   "title": "Real-Time Presentation Tracking Using Semantic Keyword Spotting",
   "original": "0617",
   "page_count": 5,
   "order": 653,
   "p1": "3081",
   "pn": "3085",
   "abstract": [
    "Given presentation slides with detailed written speaking notes, automatic\ntracking of oral presentations can help speakers ensure they cover\ntheir planned content, and can reduce their anxiety during the speech.\nTracking is a more complex problem than speech-to-text alignment, since\npresenters rarely follow their exact presentation notes, and it must\nbe performed in real-time. In this paper, we propose a novel system\nthat can track the current degree of coverage of each slide&#8217;s\ncontents. To do this, the presentation notes for each slide are segmented\ninto sentences, and the words are filtered into keyword candidates.\nThese candidates are then scored based on word specificity and semantic\nsimilarity measures to find the most useful keywords for the tracking\ntask. Real-time automatic speech recognition results are matched against\nthe keywords and their synonyms. Sentences are scored based on detected\nkeywords, and the ones with scores higher than a threshold are tagged\nas covered. We manually and automatically annotated 150 slide presentation\nrecordings to evaluate the system. A simple tracking method, matching\nspeech recognition results against the notes, was used as the baseline.\nThe results show that our approach led to higher accuracy measures\ncompared to the baseline method.\n"
   ],
   "doi": "10.21437/Interspeech.2016-617"
  },
  "wilkinson16_interspeech": {
   "authors": [
    [
     "Andrew",
     "Wilkinson"
    ],
    [
     "Tiancheng",
     "Zhao"
    ],
    [
     "Alan W.",
     "Black"
    ]
   ],
   "title": "Deriving Phonetic Transcriptions and Discovering Word Segmentations for Speech-to-Speech Translation in Low-Resource Settings",
   "original": "1319",
   "page_count": 5,
   "order": 654,
   "p1": "3086",
   "pn": "3090",
   "abstract": [
    "We investigate speech-to-speech translation where one language does\nnot have a well-defined written form. We use English-Spanish and Mandarin-English\nbitext corpora in order to provide both gold-standard text-based translations\nand experimental results for different levels of automatically derived\nsymbolic representations from speech. We constrain our experiments\nsuch that the methods developed can be extended to low-resource languages.\nWe derive different phonetic representations of the source texts in\norder to model the kinds of transcriptions that can be learned from\nlow-resource-language speech data. We experiment with different methods\nof clustering the elements of the phonetic representations together\ninto word-like units. We train MT models on the resulting texts, and\nreport BLEU scores for the different representations and clustering\nmethods in order to compare their effectiveness. Finally, we discuss\nour findings and suggest avenues for future research.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1319"
  },
  "tsujioka16_interspeech": {
   "authors": [
    [
     "Satoshi",
     "Tsujioka"
    ],
    [
     "Sakriani",
     "Sakti"
    ],
    [
     "Koichiro",
     "Yoshino"
    ],
    [
     "Graham",
     "Neubig"
    ],
    [
     "Satoshi",
     "Nakamura"
    ]
   ],
   "title": "Unsupervised Joint Estimation of Grapheme-to-Phoneme Conversion Systems and Acoustic Model Adaptation for Non-Native Speech Recognition",
   "original": "0919",
   "page_count": 5,
   "order": 655,
   "p1": "3091",
   "pn": "3095",
   "abstract": [
    "Non-native speech differs significantly from native speech, often resulting\nin a degradation of the performance of automatic speech recognition\n(ASR). Hand-crafted pronunciation lexicons used in standard ASR systems\ngenerally fail to cover non-native pronunciations, and design of new\nones by linguistic experts is time consuming and costly. In this work,\nwe propose acoustic data-driven iterative pronunciation learning for\nnon-native speech recognition, which automatically learns non-native\npronunciations directly from speech using an iterative estimation procedure.\nGrapheme-to-Phoneme (G2P) conversion is used to predict multiple candidate\npronunciations for each word, occurrence frequency of pronunciation\nvariations is estimated from the acoustic data of non-native speakers,\nand these automatically estimated pronunciation variations are used\nto perform acoustic model adaptation. We investigate various cases\nsuch as learning (1) without knowledge of non-native pronunciation,\nand (2) when we adapt to the speaker&#8217;s proficiency level. In\nexperiments on speech from non-native speakers of various levels, the\nproposed method was able to achieve an 8.9% average improvement in\naccuracy.\n"
   ],
   "doi": "10.21437/Interspeech.2016-919"
  },
  "bruguier16_interspeech": {
   "authors": [
    [
     "Antoine",
     "Bruguier"
    ],
    [
     "Fuchun",
     "Peng"
    ],
    [
     "Françoise",
     "Beaufays"
    ]
   ],
   "title": "Learning Personalized Pronunciations for Contact Name Recognition",
   "original": "0537",
   "page_count": 5,
   "order": 656,
   "p1": "3096",
   "pn": "3100",
   "abstract": [
    "Automatic speech recognition that involves people&#8217;s names is\ndifficult because names follow a long-tail distribution and they have\nno commonly accepted spelling or pronunciation. This poses significant\nchallenges to contact dialing by voice. We propose using personalized\npronunciation learning: people can use their own pronunciations for\ntheir contact names. We achieve this by implicitly learning from users&#8217;\ncorrections and within minutes making that pronunciation available\nfor the next voice dialing. We show that personalized pronunciations\nsignificantly reduce word error for difficult contact names by 15%\nrelatively.\n"
   ],
   "doi": "10.21437/Interspeech.2016-537"
  },
  "ge16_interspeech": {
   "authors": [
    [
     "Zhenhao",
     "Ge"
    ],
    [
     "Aravind",
     "Ganapathiraju"
    ],
    [
     "Ananth N.",
     "Iyer"
    ],
    [
     "Scott A.",
     "Randal"
    ],
    [
     "Felix I.",
     "Wyss"
    ]
   ],
   "title": "Generation and Pruning of Pronunciation Variants to Improve ASR Accuracy",
   "original": "1375",
   "page_count": 5,
   "order": 657,
   "p1": "3101",
   "pn": "3105",
   "abstract": [
    "Speech recognition, especially name recognition, is widely used in\nphone services such as company directory dialers, stock quote providers\nor location finders. It is usually challenging due to pronunciation\nvariations. This paper proposes an efficient and robust data-driven\ntechnique which automatically learns acceptable word pronunciations\nand updates the pronunciation dictionary to build a better lexicon\nwithout affecting recognition of other words similar to the target\nword. It generalizes well on datasets with various sizes, and reduces\nthe error rate on a database with 13000+ human names by 42%, compared\nto a baseline with regular dictionaries already covering canonical\npronunciations of 97%+ words in names, plus a well-trained spelling-to-pronunciation\n(STP) engine.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1375"
  },
  "pylkkonen16_interspeech": {
   "authors": [
    [
     "Janne",
     "Pylkkönen"
    ],
    [
     "Thomas",
     "Drugman"
    ],
    [
     "Max",
     "Bisani"
    ]
   ],
   "title": "Optimizing Speech Recognition Evaluation Using Stratified Sampling",
   "original": "1364",
   "page_count": 5,
   "order": 658,
   "p1": "3106",
   "pn": "3110",
   "abstract": [
    "Producing large enough quantities of high-quality transcriptions for\naccurate and reliable evaluation of an automatic speech recognition\n(ASR) system can be costly. It is therefore desirable to minimize the\nmanual transcription work for producing metrics with an agreed precision.\nIn this paper we demonstrate how to improve ASR evaluation precision\nusing stratified sampling. We show that by altering the sampling, the\ndeviations observed in the error metrics can be reduced by up to 30%\ncompared to random sampling, or alternatively, the same precision can\nbe obtained on about 30% smaller datasets. We compare different variants\nfor conducting stratified sampling, including a novel sample allocation\nscheme tailored for word error rate. Experimental evidence is provided\nto assess the effect of different sampling schemes to evaluation precision.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1364"
  },
  "jurafsky16_interspeech": {
   "authors": [
    [
     "Dan",
     "Jurafsky"
    ]
   ],
   "title": "Ketchup, Interdisciplinarity, and the Spread of Innovation in Speech and Language Processing",
   "original": "3004",
   "page_count": 1,
   "order": 659,
   "p1": "3111",
   "pn": "3111",
   "abstract": [
    "I show how natural language processing can help model the spread of\ninnovation through scientific communities, with special focus on the\nhistory of speech and language processing, and the important role of\ninterdisciplinarity.\n"
   ]
  },
  "scheffer16_interspeech": {
   "authors": [
    [
     "Nicolas",
     "Scheffer"
    ],
    [
     "Korbinian",
     "Riedhammer"
    ],
    [
     "Alexandre",
     "Lebrun"
    ],
    [
     "David",
     "Suendermann-Oeft"
    ]
   ],
   "title": "Speech Ventures",
   "original": "abs15",
   "page_count": 0,
   "order": 660,
   "p1": "0",
   "pn": "",
   "abstract": [
    "Interspeech 2016, the world&#8217;s largest conference on speech technologies\nto be held in San Francisco, the heart of Silicon Valley, provides\na unique opportunity to present the most recent developments and ideas\nof both academia and industry. Located at the cross-section of the\ntwo, startups that are interested in using speech in their products\nor that want to share their experience in doing so, are invited to\nparticipate in the speech venture special event. This event provides\na platform for participants to interact with the brightest speech researchers\nand present and discover new trends in spoken language technology.<br\nstyle=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  The objective of this special event are two-fold: <ul> <li>Leverage\nthe experience and stories of startups as they adopt speech in their\nproducts </li><li>Enable startups to attend a day of the largest conference\nin speech and meet with researchers, companies, and research institutions\n</ul></li>\n"
   ]
  },
  "tong16_interspeech": {
   "authors": [
    [
     "Rong",
     "Tong"
    ],
    [
     "Nancy F.",
     "Chen"
    ],
    [
     "Bin",
     "Ma"
    ],
    [
     "Haizhou",
     "Li"
    ]
   ],
   "title": "Context Aware Mispronunciation Detection for Mandarin Pronunciation Training",
   "original": "0289",
   "page_count": 5,
   "order": 661,
   "p1": "3112",
   "pn": "3116",
   "abstract": [
    "Mispronunciation detection is an important component in a computer-assisted\nlanguage learning (CALL) system. Many CALL systems only provide pronunciation\ncorrectness as the single feedback, which is not very informative for\nlanguage learners. This paper proposes a context aware multilayer framework\nfor Mandarin mispronunciation detection. The proposed framework incorporates\nthe context information in the detection process and providing phonetic,\ntonal and syllabic level feedback. In particular, the contribution\nof this work is twofold: 1) we propose to use a multilayer mispronunciation\ndetection architecture to detect and provide mispronunciation feedback\nat the phonetic, tonal and syllabic levels. 2) we propose to incorporate\nthe phonetic and tone context information in mispronunciation detection\nusing vector space modelling. Our experiment results show that the\nproposed framework improves the mispronunciation detection performance\nin all three levels.\n"
   ],
   "doi": "10.21437/Interspeech.2016-289"
  },
  "tao16c_interspeech": {
   "authors": [
    [
     "Jidong",
     "Tao"
    ],
    [
     "Lei",
     "Chen"
    ],
    [
     "Chong Min",
     "Lee"
    ]
   ],
   "title": "DNN Online with iVectors Acoustic Modeling and Doc2Vec Distributed Representations for Improving Automated Speech Scoring",
   "original": "1457",
   "page_count": 5,
   "order": 662,
   "p1": "3117",
   "pn": "3121",
   "abstract": [
    "When applying automated speech-scoring technology to the rating of\nglobally administered real assessments, there are several practical\nchallenges: (a) ASR accuracy on non-native spontaneous speech is generally\nlow; (b) due to the data mismatch between an ASR systems training stage\nand its final usage, the recognition accuracy obtained in practice\nis even lower; (c) content-relevance was not widely used in the scoring\nmodels in operation due to various technical and logistical issues.\nFor this paper, an ASR in a deep neural network (DNN) architecture\nof multi-splice with iVectors was trained and resulted in a performance\nat 19.1% word error rate (WER). Secondly, we applied language model\n(LM) adaptation for the prompts that were not covered in ASR training\nby using the spoken responses acquired from previous operational tests,\nand we were able to reduce the relative WER by more than 8%. The boosted\nASR performance improves the scoring performance without any extra\nhuman annotation cost. Finally, the developed ASR system allowed us\nto apply content features in practice. Besides the conventional frequency-based\napproach, content vector analysis (CVA), we also explored distributed\nrepresentations with Doc2Vec and found an improvement on content measurement.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1457"
  },
  "qian16_interspeech": {
   "authors": [
    [
     "Yao",
     "Qian"
    ],
    [
     "Xinhao",
     "Wang"
    ],
    [
     "Keelan",
     "Evanini"
    ],
    [
     "David",
     "Suendermann-Oeft"
    ]
   ],
   "title": "Self-Adaptive DNN for Improving Spoken Language Proficiency Assessment",
   "original": "0291",
   "page_count": 5,
   "order": 663,
   "p1": "3122",
   "pn": "3126",
   "abstract": [
    "Automated assessment of language proficiency of a test taker&#8217;s\nspoken response regarding its content, vocabulary, grammar and context\ndepends largely upon how well the input speech can be recognized. While\nstate-of-the-art, deep neural net based acoustic models have significantly\nimproved the recognition performance of native speaker&#8217;s speech,\ngood recognition is still challenging when the input speech consists\nof non-native spontaneous utterances. In this paper, we investigate\nhow to train a DNN based ASR with a fairly large non-native English\ncorpus and make it self-adaptive to a test speaker and a new task,\nnamely a simulated conversation, which is different from them monologic\nspeech in the training data. Automated assessment of language proficiency\nis evaluated according to both task completion (TC) and pragmatic competence\n(PC) rubrics. Experimental results show that self-adaptive DNNs trained\nwith i-vectors can reduce absolute word error rate by 11.7% and deliver\nmore accurate recognized word sequences for language proficiency assessment.\nAlso, the recognition accuracy gain translates into a gain of automatic\nassessment performance on the test data. The correlations between automated\nscoring and expert scoring could be increased by 0.07 (TC) and 0.15\n(PC), respectively.\n"
   ],
   "doi": "10.21437/Interspeech.2016-291"
  },
  "li16j_interspeech": {
   "authors": [
    [
     "Wei",
     "Li"
    ],
    [
     "Kehuang",
     "Li"
    ],
    [
     "Sabato Marco",
     "Siniscalchi"
    ],
    [
     "Nancy F.",
     "Chen"
    ],
    [
     "Chin-Hui",
     "Lee"
    ]
   ],
   "title": "Detecting Mispronunciations of L2 Learners and Providing Corrective Feedback Using Knowledge-Guided and Data-Driven Decision Trees",
   "original": "0517",
   "page_count": 5,
   "order": 664,
   "p1": "3127",
   "pn": "3131",
   "abstract": [
    "We propose a novel decision tree based framework to detect phonetic\nmispronunciations produced by L2 learners caused by using inaccurate\nspeech attributes, such as manner and place of articulation. Compared\nwith conventional score-based CAPT (computer assisted pronunciation\ntraining) systems, our proposed framework has three advantages: (1)\neach mispronunciation in a tree can be interpreted and communicated\nto the L2 learners by traversing the corresponding path from a leaf\nnode to the root node; (2) corrective feedback based on speech attribute\nfeatures, which are directly used to describe how consonants and vowels\nare produced using related articulators, can be provided to the L2\nlearners; and (3) by building the phone-dependent decision tree, the\nrelative importance of the speech attribute features of a target phone\ncan be automatically learned and used to distinguish itself from other\nphones. This information can provide L2 learners speech attribute feedback\nthat is ranked in order of importance. In addition to the abovementioned\nadvantages, experimental results confirm that the proposed approach\ncan detect most pronunciation errors and provide accurate diagnostic\nfeedback.\n"
   ],
   "doi": "10.21437/Interspeech.2016-517"
  },
  "wang16g_interspeech": {
   "authors": [
    [
     "Xiaoyun",
     "Wang"
    ],
    [
     "Tsuneo",
     "Kato"
    ],
    [
     "Seiichi",
     "Yamamoto"
    ]
   ],
   "title": "Phoneme Set Design Considering Integrated Acoustic and Linguistic Features of Second Language Speech",
   "original": "0663",
   "page_count": 5,
   "order": 665,
   "p1": "3132",
   "pn": "3136",
   "abstract": [
    "Recognition of second language speech is still a challenging task even\nfor state-of-the-art automatic speech recognition (ASR) systems. Considering\nthat second language speech usually includes less fluent pronunciation\nand mispronunciation even when it is grammatically correct, we propose\na novel phonetic decision tree (PDT) method considering integrated\nacoustic and linguistic features to derive the phoneme set for second\nlanguage speech recognition. We verify the efficacy of the proposed\nmethod using second language speech collected with a translation game\ntype dialogue-based English CALL system. Experimental results demonstrated\nthat the derived phoneme set achieved higher accuracy recognition performance\nthan the canonical one.\n"
   ],
   "doi": "10.21437/Interspeech.2016-663"
  },
  "rasipuram16_interspeech": {
   "authors": [
    [
     "Ramya",
     "Rasipuram"
    ],
    [
     "Milos",
     "Cernak"
    ],
    [
     "Mathew",
     "Magimai-Doss"
    ]
   ],
   "title": "HMM-Based Non-Native Accent Assessment Using Posterior Features",
   "original": "0750",
   "page_count": 5,
   "order": 666,
   "p1": "3137",
   "pn": "3141",
   "abstract": [
    "Automatic non-native accent assessment has potential benefits in language\nlearning and speech technologies. The three fundamental challenges\nin automatic accent assessment are to characterize, model and assess\nindividual variation in speech of the non-native speaker. In our recent\nwork, accentedness score was automatically obtained by comparing two\nphone probability sequences obtained through instances of non-native\nand native speech. Although automatic accentedness ratings of the approach\ncorrelated well with human accent ratings, the approach is critically\nconstrained because of the requirement of native speech instance. In\nthis paper, we build on the previous work and obtain the native latent\nsymbol probability sequence through the word hypothesis modeled as\na hidden Markov model (HMM). The latent symbols are either context-independent\nphonemes or clustered context-dependent phonemes. The advantage of\nthe proposed approach is that it requires just reference text transcription\ninstead of native speech recordings. Using the HMMs trained on an auxiliary\nnative speech corpus, the proposed approach achieves a correlation\nof 0.68 with human accent ratings on the ISLE corpus. This is further\ninteresting considering that the approach does not use any non-native\ndata and human accent ratings at any stage of the system development.\n"
   ],
   "doi": "10.21437/Interspeech.2016-750"
  },
  "shi16_interspeech": {
   "authors": [
    [
     "Shuju",
     "Shi"
    ],
    [
     "Yosuke",
     "Kashiwagi"
    ],
    [
     "Shohei",
     "Toyama"
    ],
    [
     "Junwei",
     "Yue"
    ],
    [
     "Yutaka",
     "Yamauchi"
    ],
    [
     "Daisuke",
     "Saito"
    ],
    [
     "Nobuaki",
     "Minematsu"
    ]
   ],
   "title": "Automatic Assessment and Error Detection of Shadowing Speech: Case of English Spoken by Japanese Learners",
   "original": "0915",
   "page_count": 5,
   "order": 667,
   "p1": "3142",
   "pn": "3146",
   "abstract": [
    "Shadowing is a task where the subject is required to repeat the presented\nspeech as s/he hears it. Although shadowing is cognitively a challenging\ntask, it is considered as an efficient way of language training since\nit includes processes of listening, speaking and comprehension simultaneously.\nOur previous study realized automatic assessment of shadowing speech\nusing the average of Goodness of Pronunciation (GOP) scores. But the\nfact that shadowing often includes broken utterances makes this approach\ninsufficient. This study attempts to improve automatic assessment and,\nat the same time, give corrective feedbacks to learners based on error\ndetection. We first manually labeled shadowing speech of 10 female\nand 10 male speakers and defined ten typical error types including\nword omission, substitution etc.. Forced alignment with adjusted grammar\nand GOP scores are adopted to detect word omission errors and poorly\npronounced words. In the experiments, GOP scores, Word Recognition\nRate (WRR), silence ratio, forced alignment log-likelihood scores,\nword omission rate are used to predict the overall proficiency of the\nindividual speakers. The mean correlation coefficient between automatic\nscores and the speaker&#8217;s TOEIC scores is 0.81, improved by 13%\nrelatively. The detection accuracy of word omission is 73%.\n"
   ],
   "doi": "10.21437/Interspeech.2016-915"
  },
  "hejna16b_interspeech": {
   "authors": [
    [
     "Míša",
     "Hejná"
    ]
   ],
   "title": "Multiplicity of the Acoustic Correlates of the Fortis-Lenis Contrast: Plosives in Aberystwyth English",
   "original": "1101",
   "page_count": 5,
   "order": 668,
   "p1": "3147",
   "pn": "3151",
   "abstract": [
    "Using evidence from Aberystwyth English, this study shows two points\nrelevant for the phonetic implementation of the fortis-lenis contrast\nin plosives and two points concerning the diachronic scenarios proposed\nas ways in which pre-aspiration (one of the correlates of this contrast)\ninnovates. Firstly, a wide range of acoustic features distinguishes\nthe fortis and the lenis series. Release duration is a correlate of\nthe contrast in three foot-positions (initial:  tot vs  dot; medial:\nco tter vs co dder; final: co t vs co d), as is vowel duration and\nthe presence of voicing. Furthermore, pre-aspiration and breathiness\ndifferentiate the two series foot-medially and foot-finally. For one\nspeaker, glottalisation rather than pre-aspiration distinguishes the\nseries foot-finally. Secondly, whilst the plosives are frequently post-aspirated\nfoot-initially, the release of /t/ and /d/ is realised variably with\naffrication and/or post-aspiration in all three positions: rather than\npresence or absence of affrication or post-aspiration then, it is release\nduration that distinguishes the series. Thirdly, the data is not supportive\nof the suggestion that pre-aspiration innovates in the fortis series\nas a consequence of the loss of voicing in the lenis series or the\nother way round [1] and [2] or, fourthly, as a step on a degemination\ntrajectory [3], [4].\n"
   ],
   "doi": "10.21437/Interspeech.2016-1101"
  },
  "adi16_interspeech": {
   "authors": [
    [
     "Yossi",
     "Adi"
    ],
    [
     "Joseph",
     "Keshet"
    ],
    [
     "Olga",
     "Dmitrieva"
    ],
    [
     "Matt",
     "Goldrick"
    ]
   ],
   "title": "Automatic Measurement of Voice Onset Time and Prevoicing Using Recurrent Neural Networks",
   "original": "0893",
   "page_count": 4,
   "order": 669,
   "p1": "3152",
   "pn": "3155",
   "abstract": [
    "Voice onset time (VOT) is defined as the time difference between the\nonset of the burst and the onset of voicing. When voicing begins preceding\nthe burst, the stop is called prevoiced, and the VOT is negative. When\nvoicing begins following the burst the VOT is positive. While most\nof the work on automatic measurement of VOT has focused on positive\nVOT mostly evident in American English, in many languages the VOT can\nbe negative. We propose an algorithm that estimates if the stop is\nprevoiced, and measures either positive or negative VOT, respectively.\nMore specifically, the input to the algorithm is a speech segment of\nan arbitrary length containing a single stop consonant, and the output\nis the time of the burst onset, the duration of the burst, and the\ntime of the prevoicing onset with a confidence. Manually labeled data\nis used to train a recurrent neural network that can model the dynamic\ntemporal behavior of the input signal, and outputs the events&#8217;\nonset and duration. Results suggest that the proposed algorithm is\nsuperior to the current state-of-the-art both in terms of the VOT measurement\nand in terms of prevoicing detection.\n"
   ],
   "doi": "10.21437/Interspeech.2016-893"
  },
  "ghosh16_interspeech": {
   "authors": [
    [
     "Sucheta",
     "Ghosh"
    ],
    [
     "Camille",
     "Fauth"
    ],
    [
     "Aghilas",
     "Sini"
    ],
    [
     "Yves",
     "Laprie"
    ]
   ],
   "title": "L1-L2 Interference: The Case of Final Devoicing of French Voiced Fricatives in Final Position by German Learners",
   "original": "0954",
   "page_count": 5,
   "order": 670,
   "p1": "3156",
   "pn": "3160",
   "abstract": [
    "This work is dealing with a case of L1-L2 interference in language\nlearning. The Germans learning French as a second language frequently\nproduce unvoiced fricatives in word-final position instead of the expected\nvoiced fricatives. We investigated the production of French fricatives\nfor 16 non-native (8 beginner- and 8 advanced-learners) and 8 native\nspeakers, and designed auditory feedback to help them realize the right\nvoicing feature. The productions of all speakers were categorized either\nas voiced or unvoiced by experts. The same fricatives were also evaluated\nby non-experts in a perception experiment targeting VCs. We compare\nthe ratings by experts and non-experts with the feature-based analysis.\nThe ratio of locally unvoiced frames in the consonantal segment and\nalso the ratio between consonantal duration and V1 duration were measured.\nThe acoustic cues of neighboring sounds and pitch-based features play\na significant role in the voicing judgment. As expected, we found that\nbeginners face more difficulties to produce voiced fricatives than\nadvanced learners. Also, the production becomes easier for the learners,\nespecially for the beginners, if they practice repetition after a native\nspeaker. We use these findings to design and develop feedback via speech\nanalysis/synthesis technique TD-PSOLA using the learner&#8217;s own\nvoice.\n"
   ],
   "doi": "10.21437/Interspeech.2016-954"
  },
  "yanushevskaya16_interspeech": {
   "authors": [
    [
     "Irena",
     "Yanushevskaya"
    ],
    [
     "Andy",
     "Murphy"
    ],
    [
     "Christer",
     "Gobl"
    ],
    [
     "Ailbhe Ní",
     "Chasaide"
    ]
   ],
   "title": "Perceptual Salience of Voice Source Parameters in Signaling Focal Prominence",
   "original": "1160",
   "page_count": 5,
   "order": 671,
   "p1": "3161",
   "pn": "3165",
   "abstract": [
    "This paper describes listening tests investigating the perceptual role\nof voice source parameters (other than F0) in signaling focal prominence.\nSynthesized stimuli were constructed on the basis of an inverse filtered\nutterance &#8216;We were away a year ago&#8217;. Voice source parameters\nwere manipulated in the two potentially accentable syllables WAY and\nYEAR (in terms of the absolute magnitude and alignment of peaks) and\nto provide source deaccentuation of post-focal material. Participants\nin the first listening test were asked to decide whether the syllable\nWAY, YEAR or neither was deemed the most prominent: judgments on the\ndegree of prominence and naturalness were also indicated on a continuous\nvisual analogue scale. In the second test listeners indicated the degree\nof prominence for every syllable in the phrase. For WAY, voice source\nmanipulations can cue focal accentuation, and both the magnitude of\nthe source manipulation of the syllable and the presence of source\ndeaccentuation contribute to the effect. However, for YEAR, listeners&#8217;\nperception of focal accentuation tended to show relatively minor increases\nin perceived prominence regardless of the source manipulations involved.\nIt therefore appears that the source expression of focus is sensitive\nto the location of focus in the intonational phrase.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1160"
  },
  "borsky16_interspeech": {
   "authors": [
    [
     "Michal",
     "Borsky"
    ],
    [
     "Daryush D.",
     "Mehta"
    ],
    [
     "Julius P.",
     "Gudjohnsen"
    ],
    [
     "Jon",
     "Gudnason"
    ]
   ],
   "title": "Classification of Voice Modality Using Electroglottogram Waveforms",
   "original": "1194",
   "page_count": 5,
   "order": 672,
   "p1": "3166",
   "pn": "3170",
   "abstract": [
    "It has been proven that the improper function of the vocal folds can\nresult in perceptually distorted speech that is typically identified\nwith various speech pathologies or even some neurological diseases.\nAs a consequence, researchers have focused on finding quantitative\nvoice characteristics to objectively assess and automatically detect\nnon-modal voice types. The bulk of the research has focused on classifying\nthe speech modality by using the features extracted from the speech\nsignal. This paper proposes a different approach that focuses on analyzing\nthe signal characteristics of the electroglottogram (EGG) waveform.\nThe core idea is that modal and different kinds of non-modal voice\ntypes produce EGG signals that have distinct spectral/cepstral characteristics.\nAs a consequence, they can be distinguished from each other by using\nstandard cepstral-based features and a simple multivariate Gaussian\nmixture model. The practical usability of this approach has been verified\nin the task of classifying among modal, breathy, rough, pressed and\nsoft voice types. We have achieved 83% frame-level accuracy and 91%\nutterance-level accuracy by training a speaker-dependent system.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1194"
  },
  "maekawa16_interspeech": {
   "authors": [
    [
     "Kikuo",
     "Maekawa"
    ],
    [
     "Hiroki",
     "Mori"
    ]
   ],
   "title": "Voice-Quality Difference Between the Vowels in Filled Pauses and Ordinary Lexical Items",
   "original": "1309",
   "page_count": 5,
   "order": 673,
   "p1": "3171",
   "pn": "3175",
   "abstract": [
    "Acoustic differences between the vowels in filled pauses and ordinary\nlexical items such as nouns and verbs were examined to know if there\nwas systematic difference of voice-quality. Statistical test of material\ntaken from the Corpus of Spontaneous Japanese showed that, in most\ncases, there was significant difference of acoustic features like F0,\nF1, F2, intensity, jitter, shimmer, TL, H1-H2, H1-A2, duration, etc.\nbetween the two classes of vowels. Random forest classification of\nopen data sets showed higher than 0.8 F-values on average. It turned\nout intensity, F0, F1, jitter, and H1-H2 were the most important acoustic\nfeatures for the expected voice-quality difference.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1309"
  },
  "chen16l_interspeech": {
   "authors": [
    [
     "Yan-You",
     "Chen"
    ],
    [
     "Chung-Hsien",
     "Wu"
    ],
    [
     "Yu-Fong",
     "Huang"
    ]
   ],
   "title": "Generation of Emotion Control Vector Using MDS-Based Space Transformation for Expressive Speech Synthesis",
   "original": "0815",
   "page_count": 5,
   "order": 674,
   "p1": "3176",
   "pn": "3180",
   "abstract": [
    "In control vector-based expressive speech synthesis, the emotion/style\ncontrol vector defined in the categorical (CAT) emotion space is uneasy\nto be precisely defined by the user to synthesize the speech with the\ndesired emotion/style. This paper applies the arousal-valence (AV)\nspace to the multiple regression hidden semi-Markov model (MRHSMM)-based\nsynthesis framework for expressive speech synthesis. In this study,\nthe user can designate a specific emotion by defining the AV values\nin the AV space. The multidimensional scaling (MDS) method is adopted\nto project the AV emotion space and the categorical (CAT) emotion space\nonto their corresponding orthogonal coordinate systems. A transformation\napproach is thus proposed to transform the AV values to the emotion\ncontrol vector in CAT emotion space for MRHSMM-based expressive speech\nsynthesis. In the synthesis phase given the input text and desired\nemotion, with the transformed emotion control vector, the speech with\nthe desired emotion is generated from the MRHSMMs. Experimental result\nshows the proposed method is helpful for the user to easily and precisely\ndetermine the desired emotion for expressive speech synthesis.\n"
   ],
   "doi": "10.21437/Interspeech.2016-815"
  },
  "jauk16_interspeech": {
   "authors": [
    [
     "Igor",
     "Jauk"
    ],
    [
     "Antonio",
     "Bonafonte"
    ]
   ],
   "title": "Direct Expressive Voice Training Based on Semantic Selection",
   "original": "0979",
   "page_count": 5,
   "order": 675,
   "p1": "3181",
   "pn": "3185",
   "abstract": [
    "This work aims at creating expressive voices from audiobooks using\nsemantic selection. First, for each utterance of the audiobook an acoustic\nfeature vector is extracted, including iVectors built on MFCC and on\nF0 basis. Then, the transcription is projected into a semantic vector\nspace. A seed utterance is projected to the semantic vector space and\nthe N nearest neighbors are selected. The selection is then filtered\nby selecting only acoustically similar data.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  The proposed technique\ncan be used to train emotional voices by using emotional keywords or\nphrases as seeds, obtaining training data semantically similar to the\nseed. It can also be used to read larger texts in an expressive manner,\ncreating specific voices for each sentence. That later application\nis compared to a DNN predictor, which predicts acoustic features from\nsemantic features. The selected data is used to adapt statistical speech\nsynthesis models. The performance of the technique is analyzed objectively\nand in a perceptive experiment. In the first part of the experiment,\nsubjects clearly show preference for particular expressive voices to\nsynthesize semantically expressive utterances. In the second part,\nthe proposed method is shown to achieve similar or better performance\nthan the DNN based prediction.\n"
   ],
   "doi": "10.21437/Interspeech.2016-979"
  },
  "ribeiro16_interspeech": {
   "authors": [
    [
     "Manuel Sam",
     "Ribeiro"
    ],
    [
     "Oliver",
     "Watts"
    ],
    [
     "Junichi",
     "Yamagishi"
    ]
   ],
   "title": "Syllable-Level Representations of Suprasegmental Features for DNN-Based Text-to-Speech Synthesis",
   "original": "1034",
   "page_count": 5,
   "order": 676,
   "p1": "3186",
   "pn": "3190",
   "abstract": [
    "A top-down hierarchical system based on deep neural networks is investigated\nfor the modeling of prosody in speech synthesis. Suprasegmental features\nare processed separately from segmental features and a compact distributed\nrepresentation of high-level units is learned at syllable-level. The\nsuprasegmental representation is then integrated into a frame-level\nnetwork. Objective measures show that balancing segmental and suprasegmental\nfeatures can be useful for the frame-level network. Additional features\nincorporated into the hierarchical system are then tested. At the syllable-level,\na bag-of-phones representation is proposed and, at the word-level,\nembeddings learned from text sources are used. It is shown that the\nhierarchical system is able to leverage new features at higher-levels\nmore efficiently than a system which exploits them directly at the\nframe-level. A perceptual evaluation of the proposed systems is conducted\nand followed by a discussion of the results.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1034"
  },
  "braunschweiler16_interspeech": {
   "authors": [
    [
     "Norbert",
     "Braunschweiler"
    ],
    [
     "Ranniery",
     "Maia"
    ]
   ],
   "title": "Pause Prediction from Text for Speech Synthesis with User-Definable Pause Insertion Likelihood Threshold",
   "original": "0752",
   "page_count": 5,
   "order": 677,
   "p1": "3191",
   "pn": "3195",
   "abstract": [
    "Predicting the location of pauses from text is an important aspect\nfor speech synthesizers. The accuracy of pause prediction can significantly\ninfluence both naturalness and intelligibility. Pauses which help listeners\nto better parse the synthesized speech into meaningful units are deemed\nto increase naturalness and intelligibility ratings, while pauses in\nunexpected or incorrect locations can reduce these ratings and cause\nconfusion. This paper presents a multi-stage pause prediction approach\nincluding first prosodic chunk prediction, followed by a feature scoring\nalgorithm and finally a pause sequence evaluation module. Preference\ntests showed that the new method outperformed a pauses-at-punctuation\nbaseline while not yet matching human performance. In addition, the\napproach includes two more functionalities: (1) a user-specifiable\npause insertion rate and (2) multiple output formats in the form of\nbinary pauses, multi-level pauses or as a score reflecting pause strength.\n"
   ],
   "doi": "10.21437/Interspeech.2016-752"
  },
  "do16b_interspeech": {
   "authors": [
    [
     "Quoc Truong",
     "Do"
    ],
    [
     "Tomoki",
     "Toda"
    ],
    [
     "Graham",
     "Neubig"
    ],
    [
     "Sakriani",
     "Sakti"
    ],
    [
     "Satoshi",
     "Nakamura"
    ]
   ],
   "title": "A Hybrid System for Continuous Word-Level Emphasis Modeling Based on HMM State Clustering and Adaptive Training",
   "original": "0930",
   "page_count": 5,
   "order": 678,
   "p1": "3196",
   "pn": "3200",
   "abstract": [
    "Emphasis is an important aspect of speech that conveys the focus of\nutterances, and modeling of this emphasis has been an active research\nfield. Previous work has modeled emphasis using state clustering with\nan emphasis contextual factor indicating whether or not a word is emphasized.\nIn addition, cluster adaptive training (CAT) makes it possible to directly\noptimize model parameters for clusters with different characteristics.\nIn this paper, we first make a straightforward extension of CAT to\nemphasis adaptive training using continuous emphasis representations.\nWe then compare it to state clustering, and propose a hybrid approach\nthat combines both the emphasis contextual factor and adaptive training.\nExperiments demonstrated the effectiveness of adaptive training both\nstand-alone or combined with the state clustering approach (hybrid\nsystem) with it improving emphasis estimation by 2&#8211;5% F-measure\nand producing more natural audio.\n"
   ],
   "doi": "10.21437/Interspeech.2016-930"
  },
  "zheng16_interspeech": {
   "authors": [
    [
     "Yibin",
     "Zheng"
    ],
    [
     "Ya",
     "Li"
    ],
    [
     "Zhengqi",
     "Wen"
    ],
    [
     "Xingguang",
     "Ding"
    ],
    [
     "Jianhua",
     "Tao"
    ]
   ],
   "title": "Improving Prosodic Boundaries Prediction for Mandarin Speech Synthesis by Using Enhanced Embedding Feature and Model Fusion Approach",
   "original": "1060",
   "page_count": 5,
   "order": 679,
   "p1": "3201",
   "pn": "3205",
   "abstract": [
    "Hierarchical prosody structure generation is an important but challenging\ncomponent for speech synthesis systems. In this paper, we investigate\nthe use of enhanced embedding (joint learning of character and word\nembedding (CWE)) features and different model fusion approaches at\nboth character and word level for Mandarin prosodic boundaries prediction.\nFor CWE module, the internal structures of words and non-compositional\nwords are considered in the word embedding, while the character ambiguity\nis addressed by multiple-prototype character embedding. For model fusion\nmodule, linear function (LF) and gradient boosting decision tree (GBDT),\nare investigated at the decision level respectively, with the important\nfeatures selected by feature ranking module used as its input. Experiment\nresults show the effectiveness of the proposed enhanced embedding features\nand the two model fusion approaches at both character and word level.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1060"
  },
  "zhao16c_interspeech": {
   "authors": [
    [
     "Hui",
     "Zhao"
    ],
    [
     "Désiré",
     "Bansé"
    ],
    [
     "George",
     "Doddington"
    ],
    [
     "Craig",
     "Greenberg"
    ],
    [
     "Jaime",
     "Hernández-Cordero"
    ],
    [
     "John",
     "Howard"
    ],
    [
     "Lisa",
     "Mason"
    ],
    [
     "Alvin",
     "Martin"
    ],
    [
     "Douglas",
     "Reynolds"
    ],
    [
     "Elliot",
     "Singer"
    ],
    [
     "Audrey",
     "Tong"
    ]
   ],
   "title": "Results of The 2015 NIST Language Recognition Evaluation",
   "original": "0169",
   "page_count": 5,
   "order": 680,
   "p1": "3206",
   "pn": "3210",
   "abstract": [
    "In 2015, NIST conducted the most recent in an ongoing series of Language\nRecognition Evaluations (LRE) meant to foster research in language\nrecognition. The 2015 Language Recognition Evaluation featured 20 target\nlanguages grouped into 6 language clusters. The evaluation was focused\non distinguishing languages within each cluster, without disclosing\nwhich cluster a test language belongs to.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  The 2015 evaluation\nintroduced several new aspects, such as using limited and specified\ntraining data and a wider range of durations for test segments. Unlike\nin past LRE&#8217;s, systems were not required to output hard decisions\nfor each test language and test segment, instead systems were required\nto provide a vector of log likelihood ratios to indicate the likelihood\na test segment matches a target language. A total of 24 research organizations\nparticipated in this four-month long evaluation and combined they submitted\n167 systems to be evaluated. The evaluation results showed that top-performing\nsystems exhibited similar performance and there were wide variations\nin performance based on language clusters and within cluster language\npairs. Among the 6 clusters, the French cluster was the hardest to\nrecognize, with near random performance, and the Slavic cluster was\nthe easiest to recognize.\n"
   ],
   "doi": "10.21437/Interspeech.2016-169"
  },
  "lee16e_interspeech": {
   "authors": [
    [
     "Kong Aik",
     "Lee"
    ],
    [
     "Haizhou",
     "Li"
    ],
    [
     "Li",
     "Deng"
    ],
    [
     "Ville",
     "Hautamäki"
    ],
    [
     "Wei",
     "Rao"
    ],
    [
     "Xiong",
     "Xiao"
    ],
    [
     "Anthony",
     "Larcher"
    ],
    [
     "Hanwu",
     "Sun"
    ],
    [
     "Trung Hieu",
     "Nguyen"
    ],
    [
     "Guangsen",
     "Wang"
    ],
    [
     "Aleksandr",
     "Sizov"
    ],
    [
     "Jianshu",
     "Chen"
    ],
    [
     "Ivan",
     "Kukanov"
    ],
    [
     "Amir Hossein",
     "Poorjam"
    ],
    [
     "Trung Ngo",
     "Trong"
    ],
    [
     "Cheng-Lin",
     "Xu"
    ],
    [
     "Haihua",
     "Xu"
    ],
    [
     "Bin",
     "Ma"
    ],
    [
     "Eng Siong",
     "Chng"
    ],
    [
     "Sylvain",
     "Meignier"
    ]
   ],
   "title": "The 2015 NIST Language Recognition Evaluation: The Shared View of I2R, Fantastic4 and SingaMS",
   "original": "0624",
   "page_count": 5,
   "order": 681,
   "p1": "3211",
   "pn": "3215",
   "abstract": [
    "The series of  language recognition evaluations (LRE&#8217;s) conducted\nby the National Institute of Standards and Technology (NIST) have been\none of the driving forces in advancing spoken language recognition\ntechnology. This paper presents a shared view of five institutions\nresulting from our collaboration toward LRE 2015 submissions under\nthe names of I2R, Fantastic4, and SingaMS. Among others, LRE&#8217;15\nemphasizes on language detection in the context of closely related\nlanguages, which is different from previous LRE&#8217;s. From the perspective\nof language recognition system design, we have witnessed a major paradigm\nshift in adopting deep neural network (DNN) for both feature extraction\nand classifier. In particular, deep bottleneck features (DBF) have\na significant advantage in replacing the shifted-delta-cepstral (SDC)\nwhich has been the only option in the past. We foresee deep learning\nis going to serve as a major driving force in advancing spoken language\nrecognition system in the coming years.\n"
   ],
   "doi": "10.21437/Interspeech.2016-624"
  },
  "lu16c_interspeech": {
   "authors": [
    [
     "Xugang",
     "Lu"
    ],
    [
     "Peng",
     "Shen"
    ],
    [
     "Yu",
     "Tsao"
    ],
    [
     "Hisashi",
     "Kawai"
    ]
   ],
   "title": "Pair-Wise Distance Metric Learning of Neural Network Model for Spoken Language Identification",
   "original": "0722",
   "page_count": 5,
   "order": 682,
   "p1": "3216",
   "pn": "3220",
   "abstract": [
    "The i-vector representation and modeling technique has been successfully\napplied in spoken language identification (SLI). In modeling, a discriminative\ntransform or classifier must be applied to emphasize variations correlated\nto language identity since the i-vector representation encodes most\nof the acoustic variations (e.g., speaker variation, transmission channel\nvariation, etc.). Due to the strong nonlinear discriminative power\nof neural network (NN) modeling (including its deep form DNN), the\nNN has been directly used to learn the mapping function between the\ni-vector representation and language identity labels. In most studies,\nonly the point-wise feature-label information is feeded to NN for parameter\nlearning which may result in model overfitting, particularly when with\nlimited training data. In this study, we propose to integrate pair-wise\ndistance metric learning in NN parameter optimization. In the representation\nspace of nonlinear transforms of hidden layers, a distance metric learning\nis explicitly designed for minimizing the pair-wise intra-class variation\nand maximizing the inter-class variation. With the distance metric\nas a constraint in the point-wise learning, the i-vectors are transformed\nto a new feature space which are much more discriminative for samples\nbelonging to different languages while are much more similar for samples\nbelonging to the same language. We tested the algorithm on a SLI task,\nencouraging results were obtained with more than 20% relative improvement\non identification error rate.\n"
   ],
   "doi": "10.21437/Interspeech.2016-722"
  },
  "travadi16_interspeech": {
   "authors": [
    [
     "Ruchir",
     "Travadi"
    ],
    [
     "Shrikanth S.",
     "Narayanan"
    ]
   ],
   "title": "Non-Iterative Parameter Estimation for Total Variability Model Using Randomized Singular Value Decomposition",
   "original": "0293",
   "page_count": 5,
   "order": 683,
   "p1": "3221",
   "pn": "3225",
   "abstract": [
    "In this paper, we address the problem of parameter estimation for the\nTotal Variability Model (TVM) [1]. Typically, the estimation of the\nTotal Variability Matrix requires several iterations of the Expectation\nMaximization (EM) algorithm [2], and can be considerably demanding\ncomputationally. As a result, fast and efficient parameter estimation\nremains a key challenge facing the model. We show that it is possible\nto reduce the Maximum Likelihood parameter estimation problem for TVM\ninto a Singular Value Decomposition (SVD) problem by making some suitably\njustified approximations in the likelihood function. By using randomized\nalgorithms for efficient computation of the SVD, it becomes possible\nto accelerate the parameter estimation task remarkably. In addition,\nwe show that this method is able to increase the efficiency of the\nivector extraction procedure, and also lends some interpretability\nto the extracted ivectors.\n"
   ],
   "doi": "10.21437/Interspeech.2016-293"
  },
  "garciaromero16_interspeech": {
   "authors": [
    [
     "Daniel",
     "Garcia-Romero"
    ],
    [
     "Alan",
     "McCree"
    ]
   ],
   "title": "Stacked Long-Term TDNN for Spoken Language Recognition",
   "original": "1334",
   "page_count": 5,
   "order": 684,
   "p1": "3226",
   "pn": "3230",
   "abstract": [
    "This paper introduces a stacked architecture that uses a time delay\nneural network (TDNN) to model long-term patterns for spoken language\nidentification. The first component of the architecture is a feed-forward\nneural network with a bottleneck layer that is trained to classify\ncontext-dependent phone states (senones). The second component is a\nTDNN that takes the output of the bottleneck, concatenated over a long\ntime span, and produces a posterior probability over the set of languages.\nThe use of a TDNN architecture provides an efficient model to capture\ndiscriminative patterns over a wide temporal context. Experimental\nresults are presented using the audio data from the language i-vector\nchallenge (IVC) recently organized by NIST. The proposed system outperforms\na state-of-the-art shifted delta cepstra i-vector system and provides\ncomplementary information to fuse with the new generation of bottleneck-based\ni-vector systems that model short-term dependencies.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1334"
  },
  "gelly16_interspeech": {
   "authors": [
    [
     "G.",
     "Gelly"
    ],
    [
     "Jean-Luc",
     "Gauvain"
    ],
    [
     "V.B.",
     "Le"
    ],
    [
     "A.",
     "Messaoudi"
    ]
   ],
   "title": "A Divide-and-Conquer Approach for Language Identification Based on Recurrent Neural Networks",
   "original": "0180",
   "page_count": 5,
   "order": 685,
   "p1": "3231",
   "pn": "3235",
   "abstract": [
    "This paper describes the design of an acoustic language recognition\nsystem based on BLSTM that can discriminate closely related languages\nand dialects of the same language. We introduce a  Divide-and-Conquer\n(D&amp;C) method to quickly and successfully train an RNN-based multi-language\nclassifier. Experiments compare this approach to the straightforward\ntraining of the same RNN, as well as to two widely used LID techniques:\na phonotactic system using DNN acoustic models and an i-vector system.\nResults are reported on two different data sets: the 14 languages of\nNIST LRE07 and the 20 closely related languages and dialects of NIST\nOpenLRE15. In addition to reporting the NIST Cavg metric which served\nas the primary metric for the LRE07 and OpenLRE15 evaluations, the\nEER and LER are provided. When used with BLSTM, the D&amp;C training\nscheme significantly outperformed the classical training method for\nmulti-class RNNs. On the OpenLRE15 data set, this method also outperforms\nclassical LID techniques and combines very well with a phonotactic\nsystem.\n"
   ],
   "doi": "10.21437/Interspeech.2016-180"
  },
  "hori16_interspeech": {
   "authors": [
    [
     "Chiori",
     "Hori"
    ],
    [
     "Takaaki",
     "Hori"
    ],
    [
     "Shinji",
     "Watanabe"
    ],
    [
     "John R.",
     "Hershey"
    ]
   ],
   "title": "Context-Sensitive and Role-Dependent Spoken Language Understanding Using Bidirectional and Attention LSTMs",
   "original": "1171",
   "page_count": 5,
   "order": 686,
   "p1": "3236",
   "pn": "3240",
   "abstract": [
    "To understand speaker intentions accurately in a dialog, it is important\nto consider the context of the surrounding sequence of dialog turns.\nFurthermore, each speaker may play a different role in the conversation,\nsuch as agent versus client, and thus features related to these roles\nmay be important to the context. In previous work, we proposed context-sensitive\nspoken language understanding (SLU) using role-dependent long short-term\nmemory (LSTM) recurrent neural networks (RNNs), and showed improved\nperformance at predicting concept tags representing the intentions\nof agent and client in a human-human hotel reservation task. In the\npresent study, we use bidirectional and attention-based LSTMs to train\na role-dependent context-sensitive model to jointly represent both\nthe local word-level context within each utterance, and the left and\nright context within the dialog. The different roles of client and\nagent are modeled by switching between role-dependent layers. We evaluated\nlabel accuracies in the hotel reservation task using a variety of models,\nincluding logistic regression, RNNs, LSTMs, and the proposed bidirectional\nand attention-based LSTMs. The bidirectional and attention-based LSTMs\nyield significantly better performance in this task.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1171"
  },
  "vukotic16_interspeech": {
   "authors": [
    [
     "Vedran",
     "Vukotić"
    ],
    [
     "Christian",
     "Raymond"
    ],
    [
     "Guillaume",
     "Gravier"
    ]
   ],
   "title": "A Step Beyond Local Observations with a Dialog Aware Bidirectional GRU Network for Spoken Language Understanding",
   "original": "1301",
   "page_count": 4,
   "order": 687,
   "p1": "3241",
   "pn": "3244",
   "abstract": [
    "Architectures of Recurrent Neural Networks (RNN) recently become a\nvery popular choice for Spoken Language Understanding (SLU) problems;\nhowever, they represent a big family of different architectures that\ncan furthermore be combined to form more complex neural networks. In\nthis work, we compare different recurrent networks, such as simple\nRecurrent Neural Networks (RNN), Long Short-Term Memory (LSTM) networks,\nGated Memory Units (GRU) and their bidirectional versions, on the popular\nATIS dataset and on MEDIA, a more complex French dataset. Additionally,\nwe propose a novel method where information about the presence of relevant\nword classes in the dialog history is combined with a bidirectional\nGRU, and we show that combining relevant word classes from the dialog\nhistory improves the performance over recurrent networks that work\nby solely analyzing the current sentence.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1301"
  },
  "chen16m_interspeech": {
   "authors": [
    [
     "Yun-Nung",
     "Chen"
    ],
    [
     "Dilek",
     "Hakkani-Tür"
    ],
    [
     "Gokhan",
     "Tur"
    ],
    [
     "Jianfeng",
     "Gao"
    ],
    [
     "Li",
     "Deng"
    ]
   ],
   "title": "End-to-End Memory Networks with Knowledge Carryover for Multi-Turn Spoken Language Understanding",
   "original": "0312",
   "page_count": 5,
   "order": 688,
   "p1": "3245",
   "pn": "3249",
   "abstract": [
    "Spoken language understanding (SLU) is a core component of a spoken\ndialogue system. In the traditional architecture of dialogue systems,\nthe SLU component treats each utterance independent of each other,\nand then the following components aggregate the multi-turn information\nin the separate phases. However, there are two challenges: 1) errors\nfrom previous turns may be propagated and then degrade the performance\nof the current turn; 2) knowledge mentioned in the long history may\nnot be carried into the current turn. This paper addresses the above\nissues by proposing an architecture using end-to-end memory networks\nto model knowledge carryover in multi-turn conversations, where utterances\nencoded with intents and slots can be stored as embeddings in the memory\nand the decoding phase applies an attention model to leverage previously\nstored semantics for intent prediction and slot tagging simultaneously.\nThe experiments on Microsoft Cortana conversational data show that\nthe proposed memory network architecture can effectively extract salient\nsemantics for modeling knowledge carryover in the multi-turn conversations\nand outperform the results using the state-of-the-art recurrent neural\nnetwork framework (RNN) designed for single-turn SLU.\n"
   ],
   "doi": "10.21437/Interspeech.2016-312"
  },
  "vu16_interspeech": {
   "authors": [
    [
     "Ngoc Thang",
     "Vu"
    ]
   ],
   "title": "Sequential Convolutional Neural Networks for Slot Filling in Spoken Language Understanding",
   "original": "0395",
   "page_count": 5,
   "order": 689,
   "p1": "3250",
   "pn": "3254",
   "abstract": [
    "We investigate the usage of convolutional neural networks (CNNs) for\nthe slot filling task in spoken language understanding. We propose\na novel CNN architecture for sequence labeling which takes into account\nthe previous context words with preserved order information and pays\nspecial attention to the current word with its surrounding context.\nMoreover, it combines the information from the past and the future\nwords for classification. Our proposed CNN architecture outperforms\neven the previously best ensembling recurrent neural network model\nand achieves state-of-the-art results with an F1-score of 95.61% on\nthe ATIS benchmark dataset without using any additional linguistic\nknowledge and resources.\n"
   ],
   "doi": "10.21437/Interspeech.2016-395"
  },
  "celikyilmaz16_interspeech": {
   "authors": [
    [
     "Asli",
     "Celikyilmaz"
    ],
    [
     "Ruhi",
     "Sarikaya"
    ],
    [
     "Dilek",
     "Hakkani-Tür"
    ],
    [
     "Xiaohu",
     "Liu"
    ],
    [
     "Nikhil",
     "Ramesh"
    ],
    [
     "Gokhan",
     "Tur"
    ]
   ],
   "title": "A New Pre-Training Method for Training Deep Learning Models with Application to Spoken Language Understanding",
   "original": "0512",
   "page_count": 5,
   "order": 690,
   "p1": "3255",
   "pn": "3259",
   "abstract": [
    "We propose a simple and efficient approach for pre-training deep learning\nmodels with application to slot filling tasks in spoken language understanding.\nThe proposed approach leverages unlabeled data to train the models\nand is generic enough to work with any deep learning model. In this\nstudy, we consider the CNN2CRF architecture that contains Convolutional\nNeural Network (CNN) with Conditional Random Fields (CRF) as top layer,\nsince it has shown great potential for learning useful representations\nfor supervised sequence learning tasks. The proposed pre-training approach\nwith this architecture learns the feature representations from both\nlabeled and unlabeled data at the CNN layer, covering features that\nwould not be observed in limited labeled data. At the CRF layer, the\nunlabeled data uses predicted classes of words as latent sequence labels\ntogether with labeled sequences. Latent labeled sequences, in principle,\nhas the regularization effect on the labeled sequences, yielding a\nbetter generalized model. This allows the network to learn representations\nthat are useful for not only slot tagging using labeled data but also\nlearning dependencies both within and between latent clusters of unseen\nwords. The proposed pre-training method with the CRF2CNN architecture\nachieves significant gains with respect to the strongest semi-supervised\nbaseline.\n"
   ],
   "doi": "10.21437/Interspeech.2016-512"
  },
  "tafforeau16_interspeech": {
   "authors": [
    [
     "Jeremie",
     "Tafforeau"
    ],
    [
     "Frederic",
     "Bechet"
    ],
    [
     "Thierry",
     "Artiere"
    ],
    [
     "Benoit",
     "Favre"
    ]
   ],
   "title": "Joint Syntactic and Semantic Analysis with a Multitask Deep Learning Framework for Spoken Language Understanding",
   "original": "0851",
   "page_count": 5,
   "order": 691,
   "p1": "3260",
   "pn": "3264",
   "abstract": [
    "Spoken Language Understanding (SLU) models have to deal with Automatic\nSpeech Recognition outputs which are prone to contain errors. Most\nof SLU models overcome this issue by directly predicting semantic labels\nfrom words without any deep linguistic analysis. This is acceptable\nwhen enough training data is available to train SLU models in a supervised\nway. However for open-domain SLU, such annotated corpus is not easily\navailable or very expensive to obtain, and generic syntactic and semantic\nmodels, such as dependency parsing, Semantic Role Labeling (SRL) or\nFrameNet parsing are good candidates if they can be applied to noisy\nASR transcriptions with enough robustness. To tackle this issue we\npresent in this paper an RNN-based architecture for performing joint\nsyntactic and semantic parsing tasks on noisy ASR outputs. Experiments\ncarried on a corpus of French spoken conversations collected in a telephone\ncall-centre are reported and show that our strategy brings an improvement\nover the standard pipeline approach while allowing a lot more flexibility\nin the model design and optimization.\n"
   ],
   "doi": "10.21437/Interspeech.2016-851"
  },
  "li16k_interspeech": {
   "authors": [
    [
     "Ruizhi",
     "Li"
    ],
    [
     "Sri Harish",
     "Mallidi"
    ],
    [
     "Lukáš",
     "Burget"
    ],
    [
     "Oldřich",
     "Plchot"
    ],
    [
     "Najim",
     "Dehak"
    ]
   ],
   "title": "Exploiting Hidden-Layer Responses of Deep Neural Networks for Language Recognition",
   "original": "1584",
   "page_count": 5,
   "order": 692,
   "p1": "3265",
   "pn": "3269",
   "abstract": [
    "The most popular way to apply Deep Neural Network (DNN) for Language\nIDentification (LID) involves the extraction of bottleneck features\nfrom a network that was trained on automatic speech recognition task.\nThese features are modeled using a classical I-vector system. Recently,\na more direct DNN approach was proposed, it consists of estimating\nthe language posteriors directly from a stacked frames input. The final\ndecision score is based on averaging the scores for all the frames\nfor a given speech segment. In this paper, we extended the direct DNN\napproach by modeling all hidden-layer activations rather than just\naveraging the output scores. One super-vector per utterance is formed\nby concatenating all hidden-layer responses. The dimensionality of\nthis vector is then reduced using a Principal Component Analysis (PCA).\nThe obtained reduce vector summarizes the most discriminative features\nfor language recognition based on the trained DNNs. We evaluated this\napproach in NIST 2015 language recognition evaluation. The performances\nachieved by the proposed approach are very competitive to the classical\nI-vector baseline.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1584"
  },
  "irtza16_interspeech": {
   "authors": [
    [
     "Saad",
     "Irtza"
    ],
    [
     "Vidhyasaharan",
     "Sethu"
    ],
    [
     "Sarith",
     "Fernando"
    ],
    [
     "Eliathamby",
     "Ambikairajah"
    ],
    [
     "Haizhou",
     "Li"
    ]
   ],
   "title": "Out of Set Language Modelling in Hierarchical Language Identification",
   "original": "0558",
   "page_count": 5,
   "order": 693,
   "p1": "3270",
   "pn": "3274",
   "abstract": [
    "This paper proposes a novel approach to the open set language identification\ntask by introducing out of set (OOS) language modelling in a Hierarchical\nLanguage Identification (HLID) framework. Most recent language identification\nsystems make use of data sources from other than target languages to\nmodel OOS languages. The proposed approach does not require such data\nto model OOS languages, instead it only uses data from target languages.\nAdditionally, a diverse language selection method is incorporated to\nfurther improve OOS language modelling. This work also proposes the\nuse of a new training data selection method to develop compact models\nin a hierarchical framework. Experiments are conducted on the recent\nNIST LRE 2015 data set. The overall results show relative improvements\nof 32.9% and 30.1% in terms of C<SUB>avg</SUB> with and without the\ndiverse language selection method respectively over the corresponding\nbaseline systems, when using the proposed hierarchical OOS modelling.\n"
   ],
   "doi": "10.21437/Interspeech.2016-558"
  },
  "masumura16_interspeech": {
   "authors": [
    [
     "Ryo",
     "Masumura"
    ],
    [
     "Taichi",
     "Asami"
    ],
    [
     "Hirokazu",
     "Masataki"
    ],
    [
     "Yushi",
     "Aono"
    ],
    [
     "Sumitaka",
     "Sakauchi"
    ]
   ],
   "title": "Language Identification Based on Generative Modeling of Posteriorgram Sequences Extracted from Frame-by-Frame DNNs and LSTM-RNNs",
   "original": "0719",
   "page_count": 5,
   "order": 694,
   "p1": "3275",
   "pn": "3279",
   "abstract": [
    "This paper aims to enhance spoken language identification methods based\non direct discriminative modeling of language labels using deep neural\nnetworks (DNNs) and long short-term memory recurrent neural networks\n(LSTM-RNNs). In conventional methods, frame-by-frame DNNs or LSTM-RNNs\nare used for utterance-level classification. Although they have strong\nframe-level classification performance and real-time efficiency, they\nare not optimized for variable length utterance-level classification\nsince the classification is conducted by simply averaging frame-level\nprediction results. In addition, the simple classification methodology\ncannot fully utilize the combination of DNNs and LSTM-RNNs. To address\nthese issues, our idea is to combine the frame-by-frame DNNs and LSTM-RNNs\nwith a sequential generative model based classifier. In the proposed\nmethod, we regard posteriorgram sequences generated from a frame-by-frame\nclassifier as feature sequences, and model them with respect to each\nlanguage using language modeling technologies. The generative model\nbased classifier does not model an identification boundary, so we can\nflexibly deal with variable length utterances without loss of conventional\nadvantages. Furthermore, the proposed method can support the combination\nof DNNs and LSTMs using joint posteriorgram sequences, those of generative\nmodeling can capture differences between two posteriorgram sequences.\nExperiments conducted using the GlobalPhone database demonstrate the\nproposed method&#8217;s effectiveness.\n"
   ],
   "doi": "10.21437/Interspeech.2016-719"
  },
  "geng16b_interspeech": {
   "authors": [
    [
     "Wang",
     "Geng"
    ],
    [
     "Yuanyuan",
     "Zhao"
    ],
    [
     "Wenfu",
     "Wang"
    ],
    [
     "Xinyuan",
     "Cai"
    ],
    [
     "Bo",
     "Xu"
    ]
   ],
   "title": "Gating Recurrent Enhanced Memory Neural Networks on Language Identification",
   "original": "0684",
   "page_count": 5,
   "order": 695,
   "p1": "3280",
   "pn": "3284",
   "abstract": [
    "This paper proposes a novel memory neural network structure, namely\ngating recurrent enhanced memory network (GREMN), to model long-range\ndependency in temporal series on language identification (LID) task\nat the acoustic frame level. The proposed GREMN is a stacking gating\nrecurrent neural network (RNN) equipped with a learnable enhanced memory\nblock near the classifier. It aims at capturing the long-span history\nand certain future contextual information of the sequential input.\nIn addition, two optimization strategies of coherent SortaGrad-like\ntraining mechanism and a hard sample score acquisition approach are\nproposed. The proposed optimization policies drastically boost this\nmemory network based LID system, especially on the large disparity\ntraining materials. It is confirmed by the experimental results that\nthe proposed GREMN possesses strong ability of sequential modeling\nand generalization, where about 5% relative equal error rate (EER)\nreduction is obtained comparing with the approximate-sized gating RNNs\nand 38.5% performance improvements is observed compared to conventional\ni-Vector based LID system.\n"
   ],
   "doi": "10.21437/Interspeech.2016-684"
  },
  "pesan16_interspeech": {
   "authors": [
    [
     "Jan",
     "Pešán"
    ],
    [
     "Lukáš",
     "Burget"
    ],
    [
     "Jan",
     "Černocký"
    ]
   ],
   "title": "Sequence Summarizing Neural Networks for Spoken Language Recognition",
   "original": "0764",
   "page_count": 4,
   "order": 696,
   "p1": "3285",
   "pn": "3288",
   "abstract": [
    "This paper explores the use of Sequence Summarizing Neural Networks\n(SSNNs) as a variant of deep neural networks (DNNs) for classifying\nsequences. In this work, it is applied to the task of spoken language\nrecognition. Unlike other classification tasks in speech processing\nwhere the DNN needs to produce a per-frame output, language is considered\nconstant during an utterance. We introduce a summarization component\ninto the DNN structure producing one set of language posteriors per\nutterance. The training of the DNN is performed by an appropriately\nmodified gradient-descent algorithm. In our initial experiments, the\nSSNN results are compared to a single state-of-the-art i-vector based\nbaseline system with a similar complexity (i.e. no system fusion, etc.).\nFor some conditions, SSNNs is able to provide performance comparable\nto the baseline system. Relative improvement up to 30% is obtained\nwith the score level fusion of the baseline and the SSNN systems.\n"
   ],
   "doi": "10.21437/Interspeech.2016-764"
  },
  "kapolowicz16_interspeech": {
   "authors": [
    [
     "Michelle R.",
     "Kapolowicz"
    ],
    [
     "Vahid",
     "Montazeri"
    ],
    [
     "Peter F.",
     "Assmann"
    ]
   ],
   "title": "The Role of Spectral Resolution in Foreign-Accented Speech Perception",
   "original": "1585",
   "page_count": 5,
   "order": 697,
   "p1": "3289",
   "pn": "3293",
   "abstract": [
    "Several studies have shown that diminished spectral resolution leads\nto poorer speech recognition in adverse listening conditions such as\ncompeting background noise or in cochlear implants. Although intelligibility\nis also reduced when the talker has a foreign accent, it is unknown\nhow limited spectral resolution interacts with foreign-accent perception.\nIt is hypothesized that limited spectral resolution will further impair\nperception of foreign-accented speech. To test this, we assessed the\ncontribution of spectral resolution to the intelligibility of foreign-accented\nspeech by varying the number of spectral channels in a tone vocoder.\nWe also examined listeners&#8217; abilities to discriminate between\nnative and foreign-accented speech in each condition to determine the\neffect of reduced spectral resolution on accent detection. Results\nshowed that increasing the spectral resolution improves intelligibility\nfor foreign-accented speech while also improving listeners&#8217; ability\nto detect a foreign accent but not to the level of accuracy for broadband\nspeech. Results also reveal a correlation between intelligibility and\naccent detection. Overall, results suggest that greater spectral resolution\nis needed for perception of foreign-accented speech compared to native\nspeech.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1585"
  },
  "he16c_interspeech": {
   "authors": [
    [
     "Liang",
     "He"
    ],
    [
     "Yao",
     "Tian"
    ],
    [
     "Yi",
     "Liu"
    ],
    [
     "Jiaming",
     "Xu"
    ],
    [
     "Weiwei",
     "Liu"
    ],
    [
     "Cai",
     "Meng"
    ],
    [
     "Jia",
     "Liu"
    ]
   ],
   "title": "THU-EE System Description for NIST LRE 2015",
   "original": "0791",
   "page_count": 5,
   "order": 698,
   "p1": "3294",
   "pn": "3298",
   "abstract": [
    "This paper describes the systems developed by the Department of Electronic\nEngineering of Tsinghua University for the NIST Language Recognition\nEvaluation 2015. We submitted one primary and three alternative systems\nfor the fixed training data evaluation and didn&#8217;t take part in\nthe open training data evaluation for our limited data resources and\ncomputation capability. Both the primary system and three alternative\nsystems are fusions of multiple subsystems. The primary system and\nalternative systems are identical except for the training, development\nand fusion data. The subsystems are different in feature, statistical\nmodeling or backend approach. The features of our subsystems include\nMFCC, PLP, TFC, PNCC and Fbank. The statistical modeling of our subsystems\ncan be roughly categorized into four types: i-vector, deep neural network,\nmultiple coordinate sequence kernel (MCSK) and phoneme recognizer followed\nby vector space models (PR-VSM). The backend approach includes LDA-Gaussian,\nSVM and extreme learning machine (ELM). Finally, these subsystems are\nfused by the FoCal toolkit. Our primary system is presented and briefly\ndiscussed. Post-key analyses are also addressed, including comparison\nof different features, modeling backend approaches and a study of their\ncontribution to the whole performance. The processing speed for each\nsubsystem is also given in the paper.\n"
   ],
   "doi": "10.21437/Interspeech.2016-791"
  },
  "jokinen16c_interspeech": {
   "authors": [
    [
     "Kristiina",
     "Jokinen"
    ],
    [
     "Trung Ngo",
     "Trong"
    ],
    [
     "Ville",
     "Hautamäki"
    ]
   ],
   "title": "Variation in Spoken North Sami Language",
   "original": "1438",
   "page_count": 5,
   "order": 699,
   "p1": "3299",
   "pn": "3303",
   "abstract": [
    "The paper sets to investigate the amount of variation between the North\nSami speakers living in two different majority language contexts: Finnish,\nspoken in Finland, and Norwegian Bokm&#229;l, spoken in Norway. We\nhypothesize that the majority language is a significant factor in recognizing\nvariation of the North Sami language. Although North Sami is the biggest\nof the nine currently spoken Sami languages and it has become a lingua\nfranca among the Sami speakers, there are clear differences in the\npronunciation of the North Sami spoken in Finland and Norway, so that\nthe difference can be used to recognize which majority language region\nthe speaker comes from. Using a corpus of spoken North Sami collected\nin locations in Finland and Norway, we experimented in classifying\nthe speech samples into categories based on the two majority languages.\nWe used the i-vector methodology to model both intra- and between-dialect\nvariations, and achieved the average recognition of about 17.31% EER\nfor classifying the Sami speech samples. The results support our hypothesis\nthat the variation is due to the majority language, i.e. Finnish or\nNorwegian, spoken in the given context, rather than individual variation.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1438"
  },
  "zhang16h_interspeech": {
   "authors": [
    [
     "Weibin",
     "Zhang"
    ],
    [
     "Wenkang",
     "Lei"
    ],
    [
     "Xiangmin",
     "Xu"
    ],
    [
     "Xiaofeng",
     "Xing"
    ]
   ],
   "title": "Improved Music Genre Classification with Convolutional Neural Networks",
   "original": "1236",
   "page_count": 5,
   "order": 700,
   "p1": "3304",
   "pn": "3308",
   "abstract": [
    "In recent years, deep neural networks have been shown to be effective\nin many classification tasks, including music genre classification.\nIn this paper, we proposed two ways to improve music genre classification\nwith convolutional neural networks: 1) combining max- and average-pooling\nto provide more statistical information to higher level neural networks;\n2) using shortcut connections to skip one or more layers, a method\ninspired by residual learning method. The input of the CNN is simply\nthe short time Fourier transforms of the audio signal. The output of\nthe CNN is fed into another deep neural network to do classification.\nBy comparing two different network topologies, our preliminary experimental\nresults on the GTZAN data set show that the above two methods can effectively\nimprove the classification accuracy, especially the second one.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1236"
  },
  "m16_interspeech": {
   "authors": [
    [
     "Gurunath Reddy",
     "M."
    ],
    [
     "K. Sreenivasa",
     "Rao"
    ]
   ],
   "title": "Enhanced Harmonic Content and Vocal Note Based Predominant Melody Extraction from Vocal Polyphonic Music Signals",
   "original": "0856",
   "page_count": 5,
   "order": 701,
   "p1": "3309",
   "pn": "3313",
   "abstract": [
    "A method based on the production mechanism of the vocals in the composite\nvocal polyphonic music signal is proposed for vocal melody extraction.\nIn the proposed method, initially the non-pitched percussive source\nis suppressed by observing its wideband spectral characteristics to\nemphasise the harmonic content in the mixture signal. Further, the\nharmonic enhanced signal is segmented into vocal and non-vocal regions\nby thresholding the salience energy contour. The vocal regions are\nfurther divided into vocal note like regions by their spectral transition\ncues in the frequency domain. The melody contour in each vocal note\nis extracted by detecting the locations of instant of significant excitation\nby passing it through adaptive zero frequency filtering (ZFF) in the\ntime domain. The experimental results showed that the proposed method\nis indeed comparable to the state-of-the-art saliency based melody\nextraction method. \n"
   ],
   "doi": "10.21437/Interspeech.2016-856"
  },
  "chen16n_interspeech": {
   "authors": [
    [
     "Jitong",
     "Chen"
    ],
    [
     "DeLiang",
     "Wang"
    ]
   ],
   "title": "Long Short-Term Memory for Speaker Generalization in Supervised Speech Separation",
   "original": "0551",
   "page_count": 5,
   "order": 702,
   "p1": "3314",
   "pn": "3318",
   "abstract": [
    "Speech separation can be formulated as a supervised learning problem\nwhere a time-frequency mask is estimated by a learning machine from\nacoustic features of noisy speech. Deep neural networks (DNNs) have\nbeen successful for noise generalization in supervised separation.\nHowever, real world applications desire a trained model to perform\nwell with both unseen speakers and unseen noises. In this study we\ninvestigate speaker generalization for noise-independent models and\npropose a separation model based on long short-term memory to account\nfor the temporal dynamics of speech. Our experiments show that the\nproposed model significantly outperforms a DNN in terms of objective\nspeech intelligibility for both seen and unseen speakers. Compared\nto feedforward networks, the proposed model is more capable of modeling\na large number of speakers, and represents an effective approach for\nspeaker- and noise-independent speech separation.\n"
   ],
   "doi": "10.21437/Interspeech.2016-551"
  },
  "kruspe16b_interspeech": {
   "authors": [
    [
     "Anna M.",
     "Kruspe"
    ]
   ],
   "title": "Phonotactic Language Identification for Singing",
   "original": "0131",
   "page_count": 5,
   "order": 703,
   "p1": "3319",
   "pn": "3323",
   "abstract": [
    "In the past decades, many successful approaches for language identification\nhave been published. However, almost none of these approaches were\ndeveloped with singing in mind. Singing has a lot of characteristics\nthat differ from speech, such as a wider variance of fundamental frequencies\nand phoneme durations, vibrato, pronunciation differences, and different\nsemantic content.<br style=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  We present a new phonotactic language identification system for\nsinging based on phoneme posteriorgrams. These posteriorgrams were\nextracted using acoustic models trained on English speech ( TIMIT)\nand on an unannotated English-language a-capella singing dataset (\nDAMP). SVM models were then trained on phoneme statistics.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  The models are evaluated\non a set of amateur singing recordings from  YouTube, and, for comparison,\non the  OGI Multilanguage corpus.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  While the results\non a-capella singing are somewhat worse than the ones previously obtained\nusing i-vector extraction, this approach is easier to implement. Phoneme\nposteriorgrams need to be extracted for many applications, and can\neasily be employed for language identification using this approach.\nThe results on singing improve significantly when the utilized acoustic\nmodels have also been trained on singing. Interestingly, the best results\non the  OGI speech corpus are also obtained when acoustic models trained\non singing are used.\n"
   ],
   "doi": "10.21437/Interspeech.2016-131"
  },
  "bentsen16_interspeech": {
   "authors": [
    [
     "Thomas",
     "Bentsen"
    ],
    [
     "Tobias",
     "May"
    ],
    [
     "Abigail A.",
     "Kressner"
    ],
    [
     "Torsten",
     "Dau"
    ]
   ],
   "title": "Comparing the Influence of Spectro-Temporal Integration in Computational Speech Segregation",
   "original": "1025",
   "page_count": 5,
   "order": 704,
   "p1": "3324",
   "pn": "3328",
   "abstract": [
    "The goal of computational speech segregation systems is to automatically\nsegregate a target speaker from interfering maskers. Typically, these\nsystems include a feature extraction stage in the front-end and a classification\nstage in the back-end. A spectro-temporal integration strategy can\nbe applied in either the front-end, using the so-called delta features,\nor in the back-end, using a second classifier that exploits the posterior\nprobability of speech from the first classifier across a spectro-temporal\nwindow. This study systematically analyzes the influence of such stages\non segregation performance, the error distributions and intelligibility\npredictions. Results indicated that it could be problematic to exploit\ncontext in the back-end, even though such a spectro-temporal integration\nstage improves the segregation performance. Also, the results emphasized\nthe potential need of a single metric that comprehensively predicts\ncomputational segregation performance and correlates well with intelligibility.\nThe outcome of this study could help to identify the most effective\nspectro-temporal integration strategy for computational segregation\nsystems.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1025"
  },
  "wood16_interspeech": {
   "authors": [
    [
     "Sean U.N.",
     "Wood"
    ],
    [
     "Jean",
     "Rouat"
    ]
   ],
   "title": "Blind Speech Separation with GCC-NMF",
   "original": "1449",
   "page_count": 5,
   "order": 705,
   "p1": "3329",
   "pn": "3333",
   "abstract": [
    "We introduce a blind source separation algorithm named GCC-NMF that\ncombines unsupervised dictionary learning via non-negative matrix factorization\n(NMF) with spatial localization via the generalized cross correlation\n(GCC) method. Dictionary learning is performed on the mixture signal,\nwith separation subsequently achieved by grouping dictionary atoms,\nover time, according to their spatial origins. Separation quality is\nevaluated using publicly available data from the SiSEC signal separation\nevaluation campaign consisting of stereo recordings of 3 and 4 concurrent\nspeakers in reverberant environments. Performance is quantified using\nperceptual and SNR-based measures with the PEASS and BSS Eval toolkits,\nrespectively. We compare our approach with other NMF-based speech separation\nalgorithms including unsupervised and semi-supervised approaches. GCC-NMF\noutperforms the unsupervised model-based approach that combines NMF\nwith spatial covariance mixture models, and compares favourably to\nsemi-supervised approaches that leverage prior knowledge and information,\ndespite being purely unsupervised itself.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1449"
  },
  "montazeri16_interspeech": {
   "authors": [
    [
     "Vahid",
     "Montazeri"
    ],
    [
     "Shaikat",
     "Hossain"
    ],
    [
     "Peter F.",
     "Assmann"
    ]
   ],
   "title": "Effects of Cochlear Hearing Loss on the Benefits of Ideal Binary Masking",
   "original": "1555",
   "page_count": 5,
   "order": 706,
   "p1": "3334",
   "pn": "3338",
   "abstract": [
    "Ideal Binary Masking (IdBM) is considered as the primary goal of computational\nauditory scene analysis. This binary masking criterion provides a time-frequency\nrepresentation of noisy speech and retains regions where the speech\ndominates the noise while discarding regions where the noise is dominant.\nSeveral studies have shown the benefits of IdBM for normal hearing\nand hearing-impaired listeners as well as cochlear implant recipients.\nIn this study, we evaluate the effects of simulated moderate and severe\nhearing loss on the masking release resulting from IdBM. Speech-shaped\nnoise was added to IEEE sentences; the stimuli were processed using\na tone-vocoder with 32 bandpass filters. The bandwidths of the filters\nwere adjusted to account for impaired frequency selectivity observed\nin individuals with moderate and severe hearing loss. Following envelope\nextraction, the IdBM processing was then applied to the envelopes.\nThe processed stimuli were presented to nineteen normal hearing listeners\nand their intelligibility scores were measured. Statistical analysis\nindicated that participants&#8217; benefit from IdBM was significantly\nreduced with impaired frequency selectivity (spectral smearing). Results\nshow that the masking release obtained from IdBM is highly dependent\non the listeners&#8217; hearing loss.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1555"
  },
  "grais16_interspeech": {
   "authors": [
    [
     "Emad M.",
     "Grais"
    ],
    [
     "Gerard",
     "Roma"
    ],
    [
     "Andrew J.R.",
     "Simpson"
    ],
    [
     "Mark D.",
     "Plumbley"
    ]
   ],
   "title": "Combining Mask Estimates for Single Channel Audio Source Separation Using Deep Neural Networks",
   "original": "0216",
   "page_count": 5,
   "order": 707,
   "p1": "3339",
   "pn": "3343",
   "abstract": [
    "Deep neural networks (DNNs) are usually used for single channel source\nseparation to predict either soft or binary time frequency masks. The\nmasks are used to separate the sources from the mixed signal. Binary\nmasks produce separated sources with more distortion and less interference\nthan soft masks. In this paper, we propose to use another DNN to combine\nthe estimates of binary and soft masks to achieve the advantages and\navoid the disadvantages of using each mask individually. We aim to\nachieve separated sources with low distortion and low interference\nbetween each other. Our experimental results show that combining the\nestimates of binary and soft masks using DNN achieves lower distortion\nthan using each estimate individually and achieves as low interference\nas the binary mask.\n"
   ],
   "doi": "10.21437/Interspeech.2016-216"
  },
  "riday16_interspeech": {
   "authors": [
    [
     "Cosimo",
     "Riday"
    ],
    [
     "Saurabh",
     "Bhargava"
    ],
    [
     "Richard H.R.",
     "Hahnloser"
    ],
    [
     "Shih-Chii",
     "Liu"
    ]
   ],
   "title": "Monaural Source Separation Using a Random Forest Classifier",
   "original": "0252",
   "page_count": 5,
   "order": 708,
   "p1": "3344",
   "pn": "3348",
   "abstract": [
    "We address the problem of separating two audio sources from a single\nchannel mixture recording. A novel method called Multi Layered Random\nForest (MLRF) that learns a binary mask for both the sources is presented.\nRandom Forest (RF) classifiers are trained for each frequency band\nof a source spectrogram. A specialized set of linear transformations\nare applied to a local time-frequency (T-F) neighborhood of the mixture\nthat captures relevant local statistics. A sampling method is presented\nthat efficiently samples T-F training bins in each frequency band.\nWe draw equal numbers of dominant (more power) training samples from\nthe two sources for RF classifiers that estimate the Ideal Binary Mask\n(IBM). An estimated IBM in a given layer is used to train a RF classifier\nin the next higher layer of the MLRF hierarchy. On average, MLRF performs\nbetter than deep Recurrent Neural Networks (RNNs) and Non-Negative\nSparse Coding (NNSC) in signal-to-noise ratio (SNR) of reconstructed\naudio, overall T-F bin classification accuracy, as well as PESQ and\nSTOI scores. Additionally, we demonstrate the ability of the MLRF to\ncorrectly reconstruct T-F bins of the target even when the latter has\nlower power in that frequency band.\n"
   ],
   "doi": "10.21437/Interspeech.2016-252"
  },
  "li16l_interspeech": {
   "authors": [
    [
     "Xu",
     "Li"
    ],
    [
     "Ziteng",
     "Wang"
    ],
    [
     "Xiaofei",
     "Wang"
    ],
    [
     "Qiang",
     "Fu"
    ],
    [
     "Yonghong",
     "Yan"
    ]
   ],
   "title": "Adaptive Group Sparsity for Non-Negative Matrix Factorization with Application to Unsupervised Source Separation",
   "original": "0321",
   "page_count": 5,
   "order": 709,
   "p1": "3349",
   "pn": "3353",
   "abstract": [
    "Non-negative matrix factorization (NMF) is an appealing technique for\nmany audio applications, such as automatic music transcription, source\nseparation and speech enhancement. Sparsity constraints are commonly\nused on the NMF model to discover a small number of dominant patterns.\nRecently, group sparsity has been proposed for NMF based methods, in\nwhich basis vectors belonging to a same group are permitted to activate\ntogether, while activations across groups are suppressed. However,\nmost group sparsity models penalize all groups using a same parameter\nwithout considering the relative importance of different groups for\nmodeling the input data. In this paper, we propose adaptive group sparsity\nto model the relative importance of different groups with adaptive\npenalty parameters and investigate its potential benefit to separate\nspeech from other sound sources. Experimental results show that the\nproposed adaptive group sparsity improves the performance over regular\ngroup sparsity in unsupervised settings where neither the speaker identity\nnor the type of noise is known in advance.\n"
   ],
   "doi": "10.21437/Interspeech.2016-321"
  },
  "guo16c_interspeech": {
   "authors": [
    [
     "Yanmeng",
     "Guo"
    ],
    [
     "Xiaofei",
     "Wang"
    ],
    [
     "Chao",
     "Wu"
    ],
    [
     "Qiang",
     "Fu"
    ],
    [
     "Ning",
     "Ma"
    ],
    [
     "Guy J.",
     "Brown"
    ]
   ],
   "title": "A Robust Dual-Microphone Speech Source Localization Algorithm for Reverberant Environments",
   "original": "1063",
   "page_count": 5,
   "order": 710,
   "p1": "3354",
   "pn": "3358",
   "abstract": [
    "Speech source localization (SSL) using a microphone array aims to estimate\nthe direction-of-arrival (DOA) of the speech source. However, its performance\noften degrades rapidly in reverberant environments. In this paper,\na novel dual-microphone SSL algorithm is proposed to address this problem.\nFirst, the time-frequency regions dominated by direct sound are extracted\nby tracking the envelopes of speech, reverberation and background noise.\nThe time-difference-of-arrival (TDOA) is then estimated by considering\nonly these reliable regions. Second, a bin-wise de-aliasing strategy\nis introduced to make better use of the DOA information carried at\nhigh frequencies, where the spatial resolution is higher and there\nis typically less corruption by diffuse noise. Our experiments show\nthat when compared with other widely-used algorithms, the proposed\nalgorithm produces more reliable performance in realistic reverberant\nenvironments.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1063"
  },
  "ma16c_interspeech": {
   "authors": [
    [
     "Ning",
     "Ma"
    ],
    [
     "Guy J.",
     "Brown"
    ]
   ],
   "title": "Speech Localisation in a Multitalker Mixture by Humans and Machines",
   "original": "1149",
   "page_count": 5,
   "order": 711,
   "p1": "3359",
   "pn": "3363",
   "abstract": [
    "Speech localisation in multitalker mixtures is affected by the listener&#8217;s\nexpectations about the spatial arrangement of the sound sources. This\neffect was investigated via experiments with human listeners and a\nmachine system, in which the task was to localise a female-voice target\namong four spatially distributed male-voice maskers. Two configurations\nwere used: either the masker locations were fixed or the locations\nvaried from trial-to-trial. The machine system uses deep neural networks\n(DNNs) to learn the relationship between binaural cues and source azimuth,\nand exploits top-down knowledge about the spectral characteristics\nof the target source. Performance was examined in both anechoic and\nreverberant conditions. Our experiments show that the machine system\noutperformed listeners in some conditions. Both the machine and listeners\nwere able to make use of  a priori knowledge about the spatial configuration\nof the sources, but the effect for headphone listening was smaller\nthan that previously reported for listening in a real room.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1149"
  },
  "sundar16_interspeech": {
   "authors": [
    [
     "Harshavardhan",
     "Sundar"
    ],
    [
     "Gokul Deepak",
     "Manavalan"
    ],
    [
     "T.V.",
     "Sreenivas"
    ],
    [
     "Chandra Sekhar",
     "Seelamantula"
    ]
   ],
   "title": "Reverberation-Robust One-Bit TDOA Based Moving Source Localization for Automatic Camera Steering",
   "original": "0575",
   "page_count": 5,
   "order": 712,
   "p1": "3364",
   "pn": "3368",
   "abstract": [
    "We address the problem of moving acoustic source localization and automatic\ncamera steering using one-bit measurement of the time-difference of\narrival (TDOA) between two microphones in a given array. Given that\nthe camera has a finite field of view (FoV), an algorithm with a coarse\nestimate of the source location would suffice for the purpose. We use\na microphone array and develop an algorithm to obtain a coarse estimate\nof the source using only one-bit information of the TDOA, the sign\nof it, to be precise. One advantage of the one-bit approach is that\nthe computational complexity is lower, which aids in real-time adaptation\nand localization of the moving source. We carried out experiments in\na reverberant enclosure with a 60 dB reverberation time of 600 ms (RT60\n= 600 ms). We analyzed the performance of the proposed approach using\na circular microphone array. We report comparisons with a point source\nlocalization-based automatic camera steering algorithm proposed in\nthe literature. The proposed algorithm turned out to be more accurate\nin terms of always having the moving speaker within the field of view.\n"
   ],
   "doi": "10.21437/Interspeech.2016-575"
  },
  "ochi16b_interspeech": {
   "authors": [
    [
     "Keiko",
     "Ochi"
    ],
    [
     "Nobutaka",
     "Ono"
    ],
    [
     "Shigeki",
     "Miyabe"
    ],
    [
     "Shoji",
     "Makino"
    ]
   ],
   "title": "Multi-Talker Speech Recognition Based on Blind Source Separation with ad hoc Microphone Array Using Smartphones and Cloud Storage",
   "original": "0758",
   "page_count": 5,
   "order": 713,
   "p1": "3369",
   "pn": "3373",
   "abstract": [
    "In this paper, we present a multi-talker speech recognition system\nbased on blind source separation with an ad hoc microphone array, which\nconsists of smartphones and cloud storage. In this system, a mixture\nof voices from multiple speakers is recorded by each speaker&#8217;s\nsmartphone, which is automatically transferred to online cloud storage.\nOur prototype system is realized using iPhone and Dropbox. Although\nthe signals recorded by different iPhones are not synchronized, the\nblind synchronization technique compensates both the differences in\nthe time offset and the sampling frequency mismatch. Then, auxiliary-function-based\nindependent vector analysis separates the synchronized mixture into\neach speaker&#8217;s voice. Finally, automatic speech recognition is\napplied to transcribe the speech. By experimental evaluation of the\nmulti-talker speech recognition system using Julius, we confirm that\nit effectively reduces the speech overlap and improves the speech recognition\nperformance.\n"
   ],
   "doi": "10.21437/Interspeech.2016-758"
  },
  "fahringer16_interspeech": {
   "authors": [
    [
     "Johannes",
     "Fahringer"
    ],
    [
     "Tobias",
     "Schrank"
    ],
    [
     "Johannes",
     "Stahl"
    ],
    [
     "Pejman",
     "Mowlaee"
    ],
    [
     "Franz",
     "Pernkopf"
    ]
   ],
   "title": "Phase-Aware Signal Processing for Automatic Speech Recognition",
   "original": "0823",
   "page_count": 5,
   "order": 714,
   "p1": "3374",
   "pn": "3378",
   "abstract": [
    "Conventional automatic speech recognition (ASR) often neglects the\nspectral phase information in its front-end and feature extraction\nstages. The aim of this paper is to show the impact that enhancement\nof the noisy spectral phase has on ASR accuracy when dealing with speech\nsignals corrupted with additive noise. Apart from proof-of-concept\nexperiments using clean spectral phase, we also present a phase enhancement\nmethod as a phase-aware front-end and modified group delay as a phase-aware\nfeature extractor, and the combination thereof. In experiments, we\ndemonstrate the improved performance for each individual component\nand their combination, compared to the conventional phase-unaware Mel\nFrequency Cepstral Coefficients (MFCCs)-based ASR. We observe that\nthe estimated phase information used in the front-end or feature extraction\ncomponent improves the ASR word accuracy rate (WAR) by 20.98% absolute\nfor noise corrupted speech (averaged over SNRs ranging from 0 to 20\ndB).\n"
   ],
   "doi": "10.21437/Interspeech.2016-823"
  },
  "sailor16_interspeech": {
   "authors": [
    [
     "Hardik B.",
     "Sailor"
    ],
    [
     "Hemant A.",
     "Patil"
    ]
   ],
   "title": "Unsupervised Deep Auditory Model Using Stack of Convolutional RBMs for Speech Recognition",
   "original": "0812",
   "page_count": 5,
   "order": 715,
   "p1": "3379",
   "pn": "3383",
   "abstract": [
    "Recently, we have proposed an unsupervised filterbank learning model\nbased on Convolutional RBM (ConvRBM). This model is able to learn auditory-like\nsubband filters using speech signals as an input. In this paper, we\npropose two-layer Unsupervised Deep Auditory Model (UDAM) by stacking\ntwo ConvRBMs. The first layer ConvRBM learns filterbank from speech\nsignals and hence, it represents early auditory processing. The hidden\nunits&#8217; responses of the first layer are pooled as short-time\nspectral representation to train another ConvRBM using greedy layer-wise\nmethod. The ConvRBM in second layer trained on spectral representation\nlearns Temporal Receptive Field (TRF) which represent temporal properties\nof the auditory cortex in human brain. To show the effectiveness of\nthe proposed UDAM, speech recognition experiments were conducted on\nTIMIT and AURORA 4 databases. We have shown that features extracted\nfrom second layer when added to filterbank features of first layer\nperforms better than first layer features alone (and their delta features\nas well). For both databases, our proposed two-layer deep auditory\nfeatures improve speech recognition performance over Mel filterbank\nfeatures. Further improvements can be achieved by system-level combination\nof both UDAM features and Mel filterbank features.\n"
   ],
   "doi": "10.21437/Interspeech.2016-812"
  },
  "weber16_interspeech": {
   "authors": [
    [
     "Philip",
     "Weber"
    ],
    [
     "Linxue",
     "Bai"
    ],
    [
     "Martin",
     "Russell"
    ],
    [
     "Peter",
     "Jančovič"
    ],
    [
     "Stephen",
     "Houghton"
    ]
   ],
   "title": "Interpretation of Low Dimensional Neural Network Bottleneck Features in Terms of Human Perception and Production",
   "original": "0124",
   "page_count": 5,
   "order": 716,
   "p1": "3384",
   "pn": "3388",
   "abstract": [
    "Low-dimensional &#8216;bottleneck&#8217; features extracted from neural\nnetworks have been shown to give phoneme recognition accuracy similar\nto that obtained with higher-dimensional MFCCs, using GMM-HMM models.\nSuch features have also been shown to preserve well the assumptions\nof speech trajectory dynamics made by dynamic models of speech such\nas Continuous-State HMMs. However, little is understood about how networks\nderive these features and how and whether they can be interpreted in\nterms of human speech perception and production.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  We analyse three-dimensional\nbottleneck features. We show that for vowels, their spatial representation\nis very close to the familiar F<SUB>1</SUB>:F<SUB>2</SUB> vowel quadrilateral.\nFor other classes of phonemes the features can similarly be related\nto phonetic and acoustic spatial representations presented in the literature.\nThis suggests that these networks derive representations specific to\nparticular phonetic categories, with properties similar to those used\nby human perception. The representation of the full set of phonemes\nin the bottleneck space is consistent with a hypothesized comprehensive\nmodel of speech perception and also with models of speech perception\nsuch as prototype theory.\n"
   ],
   "doi": "10.21437/Interspeech.2016-124"
  },
  "zhang16i_interspeech": {
   "authors": [
    [
     "Shiliang",
     "Zhang"
    ],
    [
     "Hui",
     "Jiang"
    ],
    [
     "Shifu",
     "Xiong"
    ],
    [
     "Si",
     "Wei"
    ],
    [
     "Li-Rong",
     "Dai"
    ]
   ],
   "title": "Compact Feedforward Sequential Memory Networks for Large Vocabulary Continuous Speech Recognition",
   "original": "0121",
   "page_count": 5,
   "order": 717,
   "p1": "3389",
   "pn": "3393",
   "abstract": [
    "In acoustic modeling for large vocabulary continuous speech recognition,\nit is essential to model long term dependency within speech signals.\nUsually, recurrent neural network (RNN) architectures, especially the\nlong short term memory (LSTM) models, are the most popular choice.\nRecently, a novel architecture, namely feedforward sequential memory\nnetworks (FSMN), provides a non-recurrent architecture to model long\nterm dependency in sequential data and has achieved better performance\nover RNNs on acoustic modeling and language modeling tasks. In this\nwork, we propose a compact feedforward sequential memory networks (cFSMN)\nby combining FSMN with low-rank matrix factorization. We also make\na slight modification to the encoding method used in FSMNs in order\nto further simplify the network architecture. On the Switchboard task,\nthe proposed new cFSMN structures can reduce the model size by 60%\nand speed up the learning by more than 7 times while the models still\nsignificantly outperform the popular bidirection LSTMs for both frame-level\ncross-entropy (CE) criterion based training and MMI based sequence\ntraining.\n"
   ],
   "doi": "10.21437/Interspeech.2016-121"
  },
  "tang16d_interspeech": {
   "authors": [
    [
     "Jian",
     "Tang"
    ],
    [
     "Shiliang",
     "Zhang"
    ],
    [
     "Si",
     "Wei"
    ],
    [
     "Li-Rong",
     "Dai"
    ]
   ],
   "title": "Future Context Attention for Unidirectional LSTM Based Acoustic Model",
   "original": "0185",
   "page_count": 5,
   "order": 718,
   "p1": "3394",
   "pn": "3398",
   "abstract": [
    "Recently, feedforward sequential memory networks (FSMN) has shown strong\nability to model past and future long-term dependency in speech signals\nwithout using recurrent feedback, and has achieved better performance\nthan BLSTM in acoustic modeling. However, the encoding coefficients\nin FSMN is context-independent while context-dependent weights are\ncommonly supposed to be more reasonable in acoustic modeling. In this\npaper, we propose a novel architecture called attention-based LSTM,\nwhich employs context-dependent scores or context-dependent weights\nto encode temporal future context information with the help of a kind\nof attention mechanism for unidirectional LSTM based acoustic model.\nPreliminary experimental results on TIMIT corpus have shown that the\nproposed attention-based LSTM achieves a phone error rate (PER) of\n20.8% while PER is 20.1% for BLSTM. We have also presented a lot of\nexperiments to evaluate different context attention methods.\n"
   ],
   "doi": "10.21437/Interspeech.2016-185"
  },
  "chien16_interspeech": {
   "authors": [
    [
     "Jen-Tzung",
     "Chien"
    ],
    [
     "Pei-Wen",
     "Huang"
    ],
    [
     "Tan",
     "Lee"
    ]
   ],
   "title": "Hybrid Accelerated Optimization for Speech Recognition",
   "original": "0192",
   "page_count": 5,
   "order": 719,
   "p1": "3399",
   "pn": "3403",
   "abstract": [
    "Optimization procedure is crucial to achieve desirable performance\nfor speech recognition based on deep neural networks (DNNs). Conventionally,\nDNNs are trained by using mini-batch stochastic gradient descent (SGD)\nwhich is stable but prone to be trapped into local optimum. A recent\nwork based on Nesterov&#8217;s accelerated gradient descent (NAG) algorithm\nis developed by merging the current momentum information into correction\nof SGD updating. NAG less likely jumps into local minimum so that convergence\nrate is improved. In general, optimization based on SGD is more stable\nwhile that based on NAG is faster and more accurate. This study aims\nto boost the performance of speech recognition by combining complimentary\nSGD and NAG. A new hybrid optimization is proposed by integrating the\nSGD with momentum and the NAG by using an interpolation scheme which\nis continuously run in each mini-batch according to the change rate\nof cost function in consecutive two learning epochs. Tradeoff between\ntwo algorithms can be balanced for mini-batch optimization. Experiments\non speech recognition using CUSENT and Aurora-4 show the effectiveness\nof the hybrid accelerated optimization in DNN acoustic model.\n"
   ],
   "doi": "10.21437/Interspeech.2016-192"
  },
  "chan16c_interspeech": {
   "authors": [
    [
     "William",
     "Chan"
    ],
    [
     "Ian",
     "Lane"
    ]
   ],
   "title": "On Online Attention-Based Speech Recognition and Joint Mandarin Character-Pinyin Training",
   "original": "0334",
   "page_count": 5,
   "order": 720,
   "p1": "3404",
   "pn": "3408",
   "abstract": [
    "In this paper, we explore the use of attention-based models for online\nspeech recognition without the usage of language models or searching.\nOur model is based on an attention-based neural network which directly\nemits English/Mandarin characters as outputs. The model jointly learns\nthe pronunciation, acoustic and language model. We evaluate the model\nfor online speech recognition on English and Mandarin. On English,\nwe achieve a 33.0% WER on the WSJ task, or a 5.4% absolute reduction\nin WER compared to an online CTC based system. We also introduce a\nnew training method and show how we can learn joint Mandarin Character-Pinyin\nmodels. Our Mandarin character only model achieves a 72% CER on the\nGALE Phase 2 evaluation, and with our joint Mandarin Character-Pinyin\nmodel, we achieve 59.3% CER or 12.7% absolute improvement over the\ncharacter only model.\n"
   ],
   "doi": "10.21437/Interspeech.2016-334"
  },
  "gosztolya16d_interspeech": {
   "authors": [
    [
     "Gábor",
     "Gosztolya"
    ],
    [
     "Tamás",
     "Grósz"
    ],
    [
     "László",
     "Tóth"
    ]
   ],
   "title": "GMM-Free Flat Start Sequence-Discriminative DNN Training",
   "original": "0391",
   "page_count": 5,
   "order": 721,
   "p1": "3409",
   "pn": "3413",
   "abstract": [
    "Recently, attempts have been made to remove Gaussian mixture models\n(GMM) from the training process of deep neural network-based hidden\nMarkov models (HMM/DNN). For the GMM-free training of a HMM/DNN hybrid\nwe have to solve two problems, namely the initial alignment of the\nframe-level state labels and the creation of context-dependent states.\nAlthough flat-start training via iteratively realigning and retraining\nthe DNN using a frame-level error function is viable, it is quite cumbersome.\nHere, we propose to use a sequence-discriminative training criterion\nfor flat start. While sequence-discriminative training is routinely\napplied only in the final phase of model training, we show that with\nproper caution it is also suitable for getting an alignment of context-independent\nDNN models. For the construction of tied states we apply a recently\nproposed KL-divergence-based state clustering method, hence our whole\ntraining process is GMM-free. In the experimental evaluation we found\nthat the sequence-discriminative flat start training method is not\nonly significantly faster than the straightforward approach of iterative\nretraining and realignment, but the word error rates attained are slightly\nbetter as well.\n"
   ],
   "doi": "10.21437/Interspeech.2016-391"
  },
  "miao16_interspeech": {
   "authors": [
    [
     "Yajie",
     "Miao"
    ],
    [
     "Florian",
     "Metze"
    ]
   ],
   "title": "Open-Domain Audio-Visual Speech Recognition: A Deep Learning Approach",
   "original": "0412",
   "page_count": 5,
   "order": 722,
   "p1": "3414",
   "pn": "3418",
   "abstract": [
    "Automatic speech recognition (ASR) on video data naturally has access\nto two modalities: audio and video. In previous work, audio-visual\nASR, which leverages visual features to help ASR, has been explored\non restricted domains of videos. This paper aims to extend this idea\nto open-domain videos, for example videos uploaded to YouTube. We achieve\nthis by adopting a unified deep learning approach. First, for the visual\nfeatures, we propose to apply segment- (utterance-) level features,\ninstead of highly restrictive frame-level features. These visual features\nare extracted using deep learning architectures which have been pre-trained\non computer vision tasks, e.g., object recognition and scene labeling.\nSecond, the visual features are incorporated into ASR under deep learning\nbased acoustic modeling. In addition to simple feature concatenation,\nwe also apply an adaptive training framework to incorporate visual\nfeatures in a more flexible way. On a challenging video transcribing\ntask, audio-visual ASR using our proposed approach gets notable improvements\nin terms of word error rates (WERs), compared to ASR merely using speech\nfeatures.\n"
   ],
   "doi": "10.21437/Interspeech.2016-412"
  },
  "zhao16d_interspeech": {
   "authors": [
    [
     "Yuanyuan",
     "Zhao"
    ],
    [
     "Shuang",
     "Xu"
    ],
    [
     "Bo",
     "Xu"
    ]
   ],
   "title": "Multidimensional Residual Learning Based on Recurrent Neural Networks for Acoustic Modeling",
   "original": "0677",
   "page_count": 5,
   "order": 723,
   "p1": "3419",
   "pn": "3423",
   "abstract": [
    "Theoretical and empirical evidences indicate that the depth of neural\nnetworks is crucial to acoustic modeling in speech recognition tasks.\nUnfortunately, the situation in practice always is that with the depth\nincreasing, the accuracy gets saturated and then degrades rapidly.\nIn this paper, a novel multidimensional residual learning architecture\nis proposed to address this degradation of deep recurrent neural networks\n(RNNs) on acoustic modeling by further exploring the spatial and temporal\ndimensions. In the spatial dimension, shortcut connections are introduced\nto RNNs, along which the information can flow across several layers\nwithout attenuation. In the temporal dimension, we cope with the degradation\nproblem by regulating temporal granularity, namely, splitting the input\nsequence into several parallel sub-sequences, which can ensure information\nflowing across the time axis unimpededly. Finally, we place a row convolution\nlayer on the top of all recurrent layers to comprehend appropriate\ninformation from several parallel sub-sequences to feed to the classifier.\nExperiments are illustrated on two quite different speech recognition\ntasks and 10% relative performance improvements are observed.\n"
   ],
   "doi": "10.21437/Interspeech.2016-677"
  },
  "zeyer16_interspeech": {
   "authors": [
    [
     "Albert",
     "Zeyer"
    ],
    [
     "Ralf",
     "Schlüter"
    ],
    [
     "Hermann",
     "Ney"
    ]
   ],
   "title": "Towards Online-Recognition with Deep Bidirectional LSTM Acoustic Models",
   "original": "0759",
   "page_count": 5,
   "order": 724,
   "p1": "3424",
   "pn": "3428",
   "abstract": [
    "Online-Recognition requires the acoustic model to provide posterior\nprobabilities after a limited time delay given the online input audio\ndata. This necessitates unidirectional modeling and the standard solution\nis to use unidirectional long short-term memory (LSTM) recurrent neural\nnetworks (RNN) or feed-forward neural networks (FFNN).<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  It is known that bidirectional\nLSTMs are more powerful and perform better than unidirectional LSTMs.\nTo demonstrate the performance difference, we start by comparing several\ndifferent bidirectional and unidirectional LSTM topologies.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  Furthermore, we apply\na modification to bidirectional RNNs to enable online-recognition by\nmoving a window over the input stream and perform one forwarding through\nthe RNN on each window. Then, we combine the posteriors of each forwarding\nand we renormalize them. We show in experiments that the performance\nof this online-enabled bidirectional LSTM performs as good as the offline\nbidirectional LSTM and much better than the unidirectional LSTM.\n"
   ],
   "doi": "10.21437/Interspeech.2016-759"
  },
  "sercu16_interspeech": {
   "authors": [
    [
     "Tom",
     "Sercu"
    ],
    [
     "Vaibhava",
     "Goel"
    ]
   ],
   "title": "Advances in Very Deep Convolutional Neural Networks for LVCSR",
   "original": "1033",
   "page_count": 5,
   "order": 725,
   "p1": "3429",
   "pn": "3433",
   "abstract": [
    "Very deep CNNs with small 3&#215;3 kernels have recently been shown\nto achieve very strong performance as acoustic models in hybrid NN-HMM\nspeech recognition systems. In this paper we investigate how to efficiently\nscale these models to larger datasets. Specifically, we address the\ndesign choice of pooling and padding along the time dimension which\nrenders convolutional evaluation of sequences highly inefficient. We\npropose a new CNN design without timepadding and without timepooling,\nwhich is slightly suboptimal for accuracy, but has two significant\nadvantages: it enables sequence training and deployment by allowing\nefficient convolutional evaluation of full utterances, and, it allows\nfor batch normalization to be straightforwardly adopted to CNNs on\nsequence data. Through batch normalization, we recover the lost peformance\nfrom removing the time-pooling, while keeping the benefit of efficient\nconvolutional evaluation.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  We demonstrate the\nperformance of our models both on larger scale data than before, and\nafter sequence training. Our very deep CNN model sequence trained on\nthe 2000h switchboard dataset obtains 9.4 word error rate on the Hub5\ntest-set, matching with a single model the performance of the 2015\nIBM system combination, which was the previous best published result.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1033"
  },
  "ghahremani16_interspeech": {
   "authors": [
    [
     "Pegah",
     "Ghahremani"
    ],
    [
     "Vimal",
     "Manohar"
    ],
    [
     "Daniel",
     "Povey"
    ],
    [
     "Sanjeev",
     "Khudanpur"
    ]
   ],
   "title": "Acoustic Modelling from the Signal Domain Using CNNs",
   "original": "1495",
   "page_count": 5,
   "order": 726,
   "p1": "3434",
   "pn": "3438",
   "abstract": [
    "Most speech recognition systems use spectral features based on fixed\nfilters, such as MFCC and PLP. In this paper, we show that it is possible\nto achieve state of the art results by making the feature extractor\na part of the network and jointly optimizing it with the rest of the\nnetwork. The basic approach is to start with a convolutional layer\nthat operates on the signal (say, with a step size of 1.25 milliseconds),\nand aggregate the filter outputs over a portion of the time axis using\na network in network architecture, and then down-sample to every 10\nmilliseconds for use by the rest of the network. We find that, unlike\nsome previous work on learned feature extractors, the objective function\nconverges as fast as for a network based on traditional features.<br\nstyle=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  Because we found that iVector adaptation is less effective in\nthis framework, we also experiment with a different adaptation method\nthat is part of the network, where activation statistics over a medium\ntime span (around a second) are computed at intermediate layers. We\nfind that the resulting &#8216;direct-from-signal&#8217; network is\ncompetitive with our state of the art networks based on conventional\nfeatures with iVector adaptation.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1495"
  },
  "chebotar16_interspeech": {
   "authors": [
    [
     "Yevgen",
     "Chebotar"
    ],
    [
     "Austin",
     "Waters"
    ]
   ],
   "title": "Distilling Knowledge from Ensembles of Neural Networks for Speech Recognition",
   "original": "1190",
   "page_count": 5,
   "order": 727,
   "p1": "3439",
   "pn": "3443",
   "abstract": [
    "Speech recognition systems that combine multiple types of acoustic\nmodels have been shown to outperform single-model systems. However,\nsuch systems can be complex to implement and too resource-intensive\nto use in production. This paper describes how to use  knowledge distillation\nto combine acoustic models in a way that has the best of many worlds:\nIt improves recognition accuracy significantly, can be implemented\nwith standard training tools, and requires no additional complexity\nduring recognition. First, we identify a simple but particularly strong\ntype of ensemble: a late combination of recurrent neural networks with\ndifferent architectures  and training objectives. To harness such an\nensemble, we use a variant of standard cross-entropy training to distill\nit into a single model and then discriminatively fine-tune the result.\nAn evaluation on 2,000-hour large vocabulary tasks in 5 languages shows\nthat the distilled models provide up to 8.9% relative WER improvement\nover conventionally-trained baselines with an identical number of parameters.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1190"
  },
  "wang16h_interspeech": {
   "authors": [
    [
     "Weiran",
     "Wang"
    ],
    [
     "Hao",
     "Tang"
    ],
    [
     "Karen",
     "Livescu"
    ]
   ],
   "title": "Triphone State-Tying via Deep Canonical Correlation Analysis",
   "original": "1300",
   "page_count": 5,
   "order": 728,
   "p1": "3444",
   "pn": "3448",
   "abstract": [
    "Context-dependent phone models are used in modern speech recognition\nsystems to account for co-articulation effects. Due to the vast number\nof possible context-dependent phones, state-tying is typically used\nto reduce the number of target classes for acoustic modeling. We propose\na novel approach for state-tying which is completely data dependent\nand requires no domain knowledge. Our method first learns low-dimensional\nembeddings of context-dependent phones using deep canonical correlation\nanalysis. The learned embeddings capture similarity between triphones\nand are highly predictable from the acoustics. We then cluster the\nembeddings and use cluster IDs as tied states. The bottleneck features\nof a DNN predicting the tied states achieve competitive recognition\naccuracy on TIMIT.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1300"
  },
  "luyet16_interspeech": {
   "authors": [
    [
     "Gil",
     "Luyet"
    ],
    [
     "Pranay",
     "Dighe"
    ],
    [
     "Afsaneh",
     "Asaei"
    ],
    [
     "Hervé",
     "Bourlard"
    ]
   ],
   "title": "Low-Rank Representation of Nearest Neighbor Posterior Probabilities to Enhance DNN Based Acoustic Modeling",
   "original": "1279",
   "page_count": 5,
   "order": 729,
   "p1": "3449",
   "pn": "3453",
   "abstract": [
    "We hypothesize that optimal deep neural networks (DNN) class-conditional\nposterior probabilities live in a union of low-dimensional subspaces.\nIn real test conditions, DNN posteriors encode uncertainties which\ncan be regarded as a superposition of unstructured sparse noise over\nthe optimal posteriors. We aim to investigate different ways to structure\nthe DNN outputs by exploiting low-rank representation (LRR) techniques.\nUsing a large number of training posterior vectors, the underlying\nlow-dimensional subspace of a test posterior is identified through\nnearest neighbor analysis, and low-rank decomposition enables separation\nof the &#8220;optimal&#8221; posteriors from the spurious uncertainties\nat the DNN output. Experiments demonstrate that by processing subsets\nof posteriors which possess strong subspace similarity, low-rank representation\nenables enhancement of posterior probabilities, and leads to higher\nspeech recognition accuracy based on the hybrid DNN-hidden Markov model\n(HMM) system.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1279"
  },
  "zheng16b_interspeech": {
   "authors": [
    [
     "Hao",
     "Zheng"
    ],
    [
     "Shanshan",
     "Zhang"
    ],
    [
     "Liwei",
     "Qiao"
    ],
    [
     "Jianping",
     "Li"
    ],
    [
     "Wenju",
     "Liu"
    ]
   ],
   "title": "Improving Large Vocabulary Accented Mandarin Speech Recognition with Attribute-Based I-Vectors",
   "original": "0378",
   "page_count": 5,
   "order": 730,
   "p1": "3454",
   "pn": "3458",
   "abstract": [
    "It has been well-recognized that the accent has a great impact on the\nASR of Chinese Mandarin, therefore, how to improve the performance\non the accented speech has become a critical issue in this field. The\nattribute feature has been proven effective on modelling accented speech,\nresulting in a significantly improved performance in accent recognition.\nIn this paper, we propose an attribute-based i-vector to improve the\nperformance of speech recognition system on large vocabulary accented\nMandarin speech task. The system with proposed attribute features works\nwell especially with sufficient training data. To further promote the\nperformance on conditions such as resource limited condition or training\ndata mismatched condition, we also develop Multi-Task Learning Deep\nNeural Networks (MTL-DNNs) with attribute classification as the secondary\ntask to improve the discriminative ability on Mandarin speech. Experiments\non the 450-hour Intel accented Mandarin speech corpus demonstrate that\nthe system with attribute-based i-vectors achieves a significant performance\nimprovement on sufficient training data compared with the baseline\nDNN-HMM system. The MTL-DNNs complement the shortage of attribute-based\ni-vectors on data limited and mismatched conditions and obtain obvious\nCER reductions.\n"
   ],
   "doi": "10.21437/Interspeech.2016-378"
  },
  "shahnawazuddin16_interspeech": {
   "authors": [
    [
     "S.",
     "Shahnawazuddin"
    ],
    [
     "Abhishek",
     "Dey"
    ],
    [
     "Rohit",
     "Sinha"
    ]
   ],
   "title": "Pitch-Adaptive Front-End Features for Robust Children&#8217;s ASR",
   "original": "1020",
   "page_count": 5,
   "order": 731,
   "p1": "3459",
   "pn": "3463",
   "abstract": [
    "In the presented work, we explore some of the challenges in recognizing\nchildren&#8217;s speech on automatic speech recognition (ASR) systems\ndeveloped using adults&#8217; speech. In such mismatched ASR tasks,\na severely degraded recognition performance is observed due to the\ngross mismatch in the acoustic attributes between those two groups\nof speakers. Among the various sources of mismatch, we focus on the\nlarge differences in the average pitch values across the adult and\nchild speakers in this work. Earlier studies have shown that the Mel-filterbank\nemployed in the feature extraction is not able to smooth out the pitch\nharmonics sufficiently in particularly for the high-pitched child speakers.\nAs a result of that, the acoustic features derived for the adult and\nthe child speakers turn out to be significantly mismatched. For addressing\nthis problem, we propose a simple technique based on adaptive-liftering\nfor deriving the pitch-robust features. This enables us to reduce the\nsensitivity of the acoustic features to the gross variations in pitch\nacross the speakers. The proposed features are found to result in improved\nperformance in the context of deep neural network based ASR system.\nFurther with the use of the existing feature normalization techniques,\nadditional gains are noted.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1020"
  },
  "delagua16_interspeech": {
   "authors": [
    [
     "Miguel Ángel",
     "del-Agua"
    ],
    [
     "Santiago",
     "Piqueras"
    ],
    [
     "Adrià",
     "Giménez"
    ],
    [
     "Alberto",
     "Sanchis"
    ],
    [
     "Jorge",
     "Civera"
    ],
    [
     "Alfons",
     "Juan"
    ]
   ],
   "title": "ASR Confidence Estimation with Speaker-Adapted Recurrent Neural Networks",
   "original": "1142",
   "page_count": 5,
   "order": 732,
   "p1": "3464",
   "pn": "3468",
   "abstract": [
    "Confidence estimation for automatic speech recognition has been very\nrecently improved by using Recurrent Neural Networks (RNNs), and also\nby speaker adaptation (on the basis of Conditional Random Fields).\nIn this work, we explore how to obtain further improvements by combining\nRNNs and speaker adaptation. In particular, we explore different speaker-dependent\nand -independent data representations for Bidirectional Long Short\nTerm Memory RNNs of various topologies. Empirical tests are reported\non the LibriSpeech dataset showing that the best results are achieved\nby the proposed combination of RNNs and speaker adaptation.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1142"
  },
  "dharo16_interspeech": {
   "authors": [
    [
     "Luis Fernando",
     "D’Haro"
    ],
    [
     "Rafael E.",
     "Banchs"
    ]
   ],
   "title": "Automatic Correction of ASR Outputs by Using Machine Translation",
   "original": "0299",
   "page_count": 5,
   "order": 733,
   "p1": "3469",
   "pn": "3473",
   "abstract": [
    "One of the main challenges when working with a domain-independent automatic\nspeech recognizers (ASR) is to correctly transcribe rare or out-of-vocabulary\nwords that are not included in the language model or whose probabilities\nare sub-estimated. Although the common solution would be to adapt the\nlanguage models and pronunciation vocabularies, in some conditions,\nlike when using free online recognizers, that is not possible and therefore\nit is necessary to apply post-recognition rectifications. In this paper,\nwe propose an automatic correction procedure based on using a phrase-based\nmachine translation system trained using words and phonetic encoding\nrepresentations to the generated n-best lists of ASR results. Our experiments\non two different datasets: human computer interfaces for robots, and\nhuman to human dialogs about tourism information show that the proposed\nmethodology can provide a quick and robust mechanism to improve the\nperformance of the ASR by reducing the word error rate (WER) and character\nerror rate (CER).\n"
   ],
   "doi": "10.21437/Interspeech.2016-299"
  },
  "mallidi16_interspeech": {
   "authors": [
    [
     "Sri Harish",
     "Mallidi"
    ],
    [
     "Hynek",
     "Hermansky"
    ]
   ],
   "title": "A Framework for Practical Multistream ASR",
   "original": "0619",
   "page_count": 5,
   "order": 734,
   "p1": "3474",
   "pn": "3478",
   "abstract": [
    "Robustness of automatic speech recognition (ASR) to acoustic mismatches\ncan be improved by using multistream architecture. Past multistream\napproaches involve training large number of neural networks, one for\neach possible stream combination. During testing phase, each utterance\nis forward passed through all the neural networks to estimate best\nstream combination. In this work, we propose a new framework to reduce\nthe complexity of multistream architecture. We show that multiple neural\nnetworks, used in the past approaches, can be replaced by a single\nneural network. This results in a significant decrease in the number\nof parameters used in the system. The test time complexity is also\nreduced by organizing the stream combinations in a tree structure,\nwhere each node in the tree represent a stream combination. Instead\nof traversing through all the nodes, we traverse through paths which\nresulted in a increase in the performance monitor score. Compared to\nstate-of-the-art baseline system, the proposed approach resulted in\n13.5% relative improvement word-error-rate (WER) in Aurora4 speech\nrecognition task. We also obtained an average of 0.7% absolute decrease\nin WER in 5 IARPRA-BABEL Year 4 languages.\n"
   ],
   "doi": "10.21437/Interspeech.2016-619"
  },
  "joy16_interspeech": {
   "authors": [
    [
     "Neethu Mariam",
     "Joy"
    ],
    [
     "Murali Karthick",
     "Baskar"
    ],
    [
     "S.",
     "Umesh"
    ],
    [
     "Basil",
     "Abraham"
    ]
   ],
   "title": "DNNs for Unsupervised Extraction of Pseudo FMLLR Features Without Explicit Adaptation Data",
   "original": "0904",
   "page_count": 5,
   "order": 735,
   "p1": "3479",
   "pn": "3483",
   "abstract": [
    "In this paper, we propose the use of deep neural networks (DNN) as\na regression model to estimate feature-space maximum likelihood linear\nregression (FMLLR) features from unnormalized features. During training,\nthe pair of unnormalized features as input and corresponding FMLLR\nfeatures as target are provided and the network is optimized to reduce\nthe mean-square error between output and target FMLLR features. During\ntest, the unnormalized features are passed through this DNN feature\nextractor to obtain FMLLR-like features without any supervision or\nfirst pass decode. Further, the FMLLR-like features are generated frame-by-frame,\nrequiring no explicit adaptation data to extract the features unlike\nin FMLLR or i-vector. Our proposed approach is therefore suitable for\nscenarios where there is little adaptation data. The proposed approach\nprovides sizable improvements over basis-FMLLR and conventional FMLLR\nwhen normalization is done at utterance level on TIMIT and Switchboard-33hour\ndata sets.\n"
   ],
   "doi": "10.21437/Interspeech.2016-904"
  },
  "samarakoon16b_interspeech": {
   "authors": [
    [
     "Lahiru",
     "Samarakoon"
    ],
    [
     "Khe Chai",
     "Sim"
    ]
   ],
   "title": "Multi-Attribute Factorized Hidden Layer Adaptation for DNN Acoustic Models",
   "original": "1233",
   "page_count": 5,
   "order": 736,
   "p1": "3484",
   "pn": "3488",
   "abstract": [
    "Recently, the Factorized Hidden Layer (FHL) adaptation is proposed\nfor speaker adaptation of deep neural network (DNN) based acoustic\nmodels. In addition to the standard affine transformation, an FHL contains\na speaker-dependent (SD) transformation matrix using a linear combination\nof rank-1 matrices and an SD bias using a linear combination of vectors.\nIn this work, we extend the FHL based adaptation to multiple variabilities\nof the speech signal. Experimental results on Aurora4 task show 26.0%\nrelative improvement over the baseline when standard FHL adaptation\nis used for speaker adaptation. The Multi-attribute FHL adaptation\nshows gains over the standard FHL adaptation where improvements reach\nup to 29.0% relative to the baseline.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1233"
  },
  "goo16_interspeech": {
   "authors": [
    [
     "Jahyun",
     "Goo"
    ],
    [
     "Younggwan",
     "Kim"
    ],
    [
     "Hyungjun",
     "Lim"
    ],
    [
     "Hoirin",
     "Kim"
    ]
   ],
   "title": "Speaker Normalization Through Feature Shifting of Linearly Transformed i-Vector",
   "original": "0819",
   "page_count": 5,
   "order": 737,
   "p1": "3489",
   "pn": "3493",
   "abstract": [
    "In this paper, we propose a simple speaker normalization for deep neural\nnetwork (DNN) using i-vectors, the state-of-the-art technique for speaker\nrecognition, for automatic speech recognition. There have been already\nmany techniques using i-vectors for speaker adaptation or speaker variability\nreduction of DNN acoustic models. However, in order to add the speaker\ninformation into the acoustic feature, most of those techniques have\nto train a large number of parameters while dimensionality of the i-vector\nis quite small. We tried to apply a component-wise shift to the acoustic\nfeatures by linearly transformed i-vector, and then achieved the better\nperformance than typical approaches. On top of that, we propose to\nmodify this structure to adapt each frame of the features, reducing\nthe number of parameters. Experiments were conducted on the TED-LIUM\nrelease-1 corpus, and the proposed method showed some performance gains.\n"
   ],
   "doi": "10.21437/Interspeech.2016-819"
  },
  "diab16_interspeech": {
   "authors": [
    [
     "Mona",
     "Diab"
    ],
    [
     "Pascale",
     "Fung"
    ],
    [
     "Julia",
     "Hirschberg"
    ],
    [
     "Thamar",
     "Solorio"
    ]
   ],
   "title": "Computational Approaches to Linguistic Code Switching",
   "original": "abs16",
   "page_count": 0,
   "order": 738,
   "p1": "0",
   "pn": "",
   "abstract": [
    "Code-switching (CS) is the phenomenon by which multilingual speakers\nswitch back and forth between their common languages in written or\nspoken communication. CS may occur at the inter-utterance, intra-utterance\n(mixing of words from multiple languages in the same utterance) and\neven morphological (mixing of morphemes from different languages) levels.\nCS presents serious challenges for language technologies such as Automatic\nSpeech Recognition, Language Modeling, Parsing, Machine Translation\n(MT), Information Retrieval (IR) and Extraction (IE), Keyword Search,\nand semantic processing. A prime example of this is acoustic modeling\nand language modeling in automatic speech recognition (ASR): techniques\ntrained on one language quickly break down when there is mixed language\ninput. The lack of basic tools such as language models, part-of-speech\n(POS) taggers and parsers trained on such mixed language data makes\ndownstream tasks even more challenging. Even for problems that are\nlargely considered solved for monolingual corpora, such as Language\nIdentification, or POS Tagging, performance degrades at a rate proportional\nto the amount and level of mixed-language present in the data.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  This special event\nis to bring together researchers interested in solving the CS problem,\nto raise community awareness of the (limited) resources available and\nthe work currently underway for the study of CS, with particular emphasis\non work in the speech community.  The format will consist of a short\nintroduction from the organizers followed by discussion. We held a\nworkshop in CS in conjunction with EMNLP 2014, developing a shared\ntext-based task for this purpose. We received 18 regular workshop submissions\nand accepted 8. The goal of this event is to engage the speech processing\ncommunity now working in this area and to encourage new research by\nthose now working primarily with monolingual corpora.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  We will solicit participation\nfrom researchers working in speech processing for the analysis and/processing\nof CS data. Topics of relevance to the event will include the following:\n<ul> <li>Methods for improving ASR acoustic and language models in\ncode switched data </li><li>Domain/dialect/genre adaptation techniques\napplied to CS data processing </li><li>Challenges of language identification\nin CS data </li><li>Speech-to-speech translation in CS data </li><li>Keyword\nsearch in CS data </li><li>Cross-lingual approaches to CS </li><li>Development\nof corpora to support research on CS data </li><li>Crowdsourcing approaches\nfor the annotation of code switched data </ul></li>\n"
   ]
  },
  "arisoy16_interspeech": {
   "authors": [
    [
     "Ebru",
     "Arisoy"
    ],
    [
     "Murat",
     "Saraclar"
    ]
   ],
   "title": "Compositional Neural Network Language Models for Agglutinative Languages",
   "original": "1239",
   "page_count": 5,
   "order": 739,
   "p1": "3494",
   "pn": "3498",
   "abstract": [
    "Continuous space language models (CSLMs) have been proven to be successful\nin speech recognition. With proper training of the word embeddings,\nwords that are semantically or syntactically related are expected to\nbe mapped to nearby locations in the continuous space. In agglutinative\nlanguages, words are made up of concatenation of stems and suffixes\nand, as a result, compositional modeling is important. However, when\ntrained on word tokens, CSLMs do not explicitly consider this structure.\nIn this paper, we explore compositional modeling of stems and suffixes\nin a long short-term memory neural network language model. Our proposed\nmodels jointly learn distributed representations for stems and endings\n(concatenation of suffixes) and predict the probability for stem and\nending sequences. Experiments on the Turkish Broadcast news transcription\ntask show that further gains on top of a state-of-the-art stem-ending-based\nn-gram language model can be obtained with the proposed models.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1239"
  },
  "damavandi16_interspeech": {
   "authors": [
    [
     "Babak",
     "Damavandi"
    ],
    [
     "Shankar",
     "Kumar"
    ],
    [
     "Noam",
     "Shazeer"
    ],
    [
     "Antoine",
     "Bruguier"
    ]
   ],
   "title": "NN-Grams: Unifying Neural Network and n-Gram Language Models for Speech Recognition",
   "original": "1295",
   "page_count": 5,
   "order": 740,
   "p1": "3499",
   "pn": "3503",
   "abstract": [
    "We present NN-grams, a novel, hybrid language model integrating n-grams\nand neural networks (NN) for speech recognition. The model takes as\ninput both word histories as well as n-gram counts. Thus, it combines\nthe memorization capacity and scalability of an n-gram model with the\ngeneralization ability of neural networks. We report experiments where\nthe model is trained on 26B words. NN-grams are efficient at runtime\nsince they do not include an output soft-max layer. The model is trained\nusing noise contrastive estimation (NCE), an approach that transforms\nthe estimation problem of neural networks into one of binary classification\nbetween data samples and noise samples. We present results with noise\nsamples derived from either an n-gram distribution or from speech recognition\nlattices. NN-grams outperforms an n-gram model on an Italian speech\nrecognition dictation task.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1295"
  },
  "haidar16_interspeech": {
   "authors": [
    [
     "Md. Akmal",
     "Haidar"
    ],
    [
     "Mikko",
     "Kurimo"
    ]
   ],
   "title": "Recurrent Neural Network Language Model with Incremental Updated Context Information Generated Using Bag-of-Words Representation",
   "original": "0375",
   "page_count": 5,
   "order": 741,
   "p1": "3504",
   "pn": "3508",
   "abstract": [
    "Recurrent neural network language model (RNNLM) is becoming popular\nin the state-of-the-art speech recognition systems. However, it can\nnot remember long term patterns well due to a so-called vanishing gradient\nproblem. Recently, Bag-of-words (BOW) representation of a word sequence\nis frequently used as a context feature to improve the performance\nof a standard feed-forward NNLM. However, the BOW features have not\nbeen shown to benefit RNNLM. In this paper, we introduce a technique\nusing BOW features to remember long term dependencies in RNNLM by creating\na context feature vector in a separate non-linear context layer during\nthe training of RNNLM. The context information is incrementally updated\nbased on the BOW features and processed further in a non-linear context\nlayer. The output of this layer is used as a context feature vector\nand fed into the hidden and output layers of the RNNLM. Experiments\nwith Penn Treebank corpus indicate that our approach can provide lower\nperplexity with fewer parameters and faster training compared to the\nconventional RNNLM. Moreover, we carried out speech recognition experiments\nwith Wall Street Journal corpus and achieved lower word error rate\nthan RNNLM.\n"
   ],
   "doi": "10.21437/Interspeech.2016-375"
  },
  "oualil16_interspeech": {
   "authors": [
    [
     "Youssef",
     "Oualil"
    ],
    [
     "Clayton",
     "Greenberg"
    ],
    [
     "Mittul",
     "Singh"
    ],
    [
     "Dietrich",
     "Klakow"
    ]
   ],
   "title": "Sequential Recurrent Neural Networks for Language Modeling",
   "original": "0422",
   "page_count": 5,
   "order": 742,
   "p1": "3509",
   "pn": "3513",
   "abstract": [
    "Feedforward Neural Network (FNN)-based language models estimate the\nprobability of the next word based on the history of the last N words,\nwhereas Recurrent Neural Networks (RNN) perform the same task based\nonly on the last word and some context information that cycles in the\nnetwork. This paper presents a novel approach, which bridges the gap\nbetween these two categories of networks. In particular, we propose\nan architecture which takes advantage of the explicit, sequential enumeration\nof the word history in FNN structure while enhancing each word representation\nat the projection layer through recurrent context information that\nevolves in the network. The context integration is performed using\nan additional word-dependent weight matrix that is also learned during\nthe training. Extensive experiments conducted on the Penn Treebank\n(PTB) and the Large Text Compression Benchmark (LTCB) corpus showed\na significant reduction of the perplexity when compared to state-of-the-art\nfeedforward as well as recurrent neural network architectures.\n"
   ],
   "doi": "10.21437/Interspeech.2016-422"
  },
  "levit16_interspeech": {
   "authors": [
    [
     "Michael",
     "Levit"
    ],
    [
     "Sarangarajan",
     "Parthasarathy"
    ],
    [
     "Shuangyu",
     "Chang"
    ]
   ],
   "title": "Word-Phrase-Entity Recurrent Neural Networks for Language Modeling",
   "original": "0044",
   "page_count": 5,
   "order": 743,
   "p1": "3514",
   "pn": "3518",
   "abstract": [
    "The recently introduced framework of Word-Phrase-Entity language modeling\nis applied to Recurrent Neural Networks and leads to similar improvements\nas reported for n-gram language models. In the proposed architecture,\nRNN LMs do not operate in terms of lexical items (words), but consume\nsequences of tokens that could be words, word phrases or classes such\nas named entities, with the optimal representation for a particular\ninput sentence determined in an iterative manner. We show how auxiliary\ntechniques previously described for n-gram WPE language models, such\nas token-level interpolation and personalization, can also be realized\nwith recurrent networks and lead to similar perplexity improvements.\n"
   ],
   "doi": "10.21437/Interspeech.2016-44"
  },
  "irie16_interspeech": {
   "authors": [
    [
     "Kazuki",
     "Irie"
    ],
    [
     "Zoltán",
     "Tüske"
    ],
    [
     "Tamer",
     "Alkhouli"
    ],
    [
     "Ralf",
     "Schlüter"
    ],
    [
     "Hermann",
     "Ney"
    ]
   ],
   "title": "LSTM, GRU, Highway and a Bit of Attention: An Empirical Overview for Language Modeling in Speech Recognition",
   "original": "0491",
   "page_count": 5,
   "order": 744,
   "p1": "3519",
   "pn": "3523",
   "abstract": [
    "Popularized by the long short-term memory (LSTM), multiplicative gates\nhave become a standard means to design artificial neural networks with\nintentionally organized information flow. Notable examples of such\narchitectures include gated recurrent units (GRU) and highway networks.\nIn this work, we first focus on the evaluation of each of the classical\ngated architectures for language modeling for large vocabulary speech\nrecognition. Namely, we evaluate the highway network, lateral network,\nLSTM and GRU. Furthermore, the motivation underlying the highway network\nalso applies to LSTM and GRU. An extension specific to the LSTM has\nbeen recently proposed with an additional highway connection between\nthe memory cells of adjacent LSTM layers. In contrast, we investigate\nan approach which can be used with both LSTM and GRU: a highway network\nin which the LSTM or GRU is used as the transformation function. We\nfound that the highway connections enable both standalone feedforward\nand recurrent neural language models to benefit better from the deep\nstructure and provide a slight improvement of recognition accuracy\nafter interpolation with count models. To complete the overview, we\ninclude our initial investigations on the use of the attention mechanism\nfor learning word triggers.\n"
   ],
   "doi": "10.21437/Interspeech.2016-491"
  },
  "das16b_interspeech": {
   "authors": [
    [
     "Amit",
     "Das"
    ],
    [
     "Preethi",
     "Jyothi"
    ],
    [
     "Mark",
     "Hasegawa-Johnson"
    ]
   ],
   "title": "Automatic Speech Recognition Using Probabilistic Transcriptions in Swahili, Amharic, and Dinka",
   "original": "0657",
   "page_count": 5,
   "order": 745,
   "p1": "3524",
   "pn": "3528",
   "abstract": [
    "In this study, we develop automatic speech recognition systems for\nthree sub-Saharan African languages using probabilistic transcriptions\ncollected from crowd workers who neither speak nor have any familiarity\nwith the African languages. The three African languages in consideration\nare Swahili, Amharic, and Dinka. There is a language mismatch in this\nscenario. More specifically, utterances spoken in African languages\nwere transcribed by crowd workers who were mostly native speakers of\nEnglish. Due to this, such transcriptions are highly prone to label\ninaccuracies. First, we use a recently introduced technique called\nmismatched crowdsourcing which processes the raw crowd transcriptions\nto confusion networks. Next, we adapt both multilingual hidden Markov\nmodels (HMM) and deep neural network (DNN) models using the probabilistic\ntranscriptions of the African languages. Finally, we report the results\nusing both deterministic and probabilistic phone error rates (PER).\nAutomatic speech recognition systems developed using this recipe are\nparticularly useful for low resource languages where there is limited\naccess to linguistic resources and/or transcribers in the native language.\n"
   ],
   "doi": "10.21437/Interspeech.2016-657"
  },
  "gauthier16b_interspeech": {
   "authors": [
    [
     "Elodie",
     "Gauthier"
    ],
    [
     "Laurent",
     "Besacier"
    ],
    [
     "Sylvie",
     "Voisin"
    ]
   ],
   "title": "Speed Perturbation and Vowel Duration Modeling for ASR in Hausa and Wolof Languages",
   "original": "0461",
   "page_count": 5,
   "order": 746,
   "p1": "3529",
   "pn": "3533",
   "abstract": [
    "Automatic Speech Recognition (ASR) for (under-resourced) Sub-Saharan\nAfrican languages faces several challenges: small amount of transcribed\nspeech, written language normalization issues, few text resources available\nfor language modeling, as well as specific features (tones, morphology,\netc.) that need to be taken into account seriously to optimize ASR\nperformance. This paper tries to address some of the above challenges\nthrough the development of ASR systems for two Sub-Saharan African\nlanguages: Hausa and Wolof. First, we investigate data augmentation\ntechnique (through speed perturbation) to overcome the lack of resources.\nSecondly, the main contribution is our attempt to model vowel length\ncontrast existing in both languages. For reproducible experiments,\nthe ASR systems developed for Hausa and Wolof are made available to\nthe research community on  github. To our knowledge, the Wolof ASR\nsystem presented in this paper is the first large vocabulary continuous\nspeech recognition system ever developed for this language.\n"
   ],
   "doi": "10.21437/Interspeech.2016-461"
  },
  "heerden16_interspeech": {
   "authors": [
    [
     "Charl van",
     "Heerden"
    ],
    [
     "Neil",
     "Kleynhans"
    ],
    [
     "Marelie",
     "Davel"
    ]
   ],
   "title": "Improving the Lwazi ASR Baseline",
   "original": "1412",
   "page_count": 5,
   "order": 747,
   "p1": "3534",
   "pn": "3538",
   "abstract": [
    "We investigate the impact of recent advances in speech recognition\ntechniques for under-resourced languages. Specifically, we review earlier\nresults published on the Lwazi ASR corpus of South African languages,\nand experiment with additional acoustic modeling approaches. We demonstrate\nlarge gains by applying current state-of-the-art techniques, even if\nthe data itself is neither extended nor improved. We analyze the various\nperformance improvements observed, report on comparative performance\nper technique &#8212; across all eleven languages in the corpus &#8212;\nand discuss the implications of our findings for under-resourced languages\nin general.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1412"
  },
  "godard16_interspeech": {
   "authors": [
    [
     "Pierre",
     "Godard"
    ],
    [
     "Gilles",
     "Adda"
    ],
    [
     "Martine",
     "Adda-Decker"
    ],
    [
     "Alexandre",
     "Allauzen"
    ],
    [
     "Laurent",
     "Besacier"
    ],
    [
     "Hélène",
     "Bonneau-Maynard"
    ],
    [
     "Guy-Noël",
     "Kouarata"
    ],
    [
     "Kevin",
     "Löser"
    ],
    [
     "Annie",
     "Rialland"
    ],
    [
     "François",
     "Yvon"
    ]
   ],
   "title": "Preliminary Experiments on Unsupervised Word Discovery in Mboshi",
   "original": "0886",
   "page_count": 5,
   "order": 748,
   "p1": "3539",
   "pn": "3543",
   "abstract": [
    "The necessity to document thousands of endangered languages encourages\nthe collaboration between linguists and computer scientists in order\nto provide the documentary linguistics community with the support of\nautomatic processing tools. The French-German ANR-DFG project  Breaking\nthe Unwritten Language Barrier (BULB) aims at developing such tools\nfor three mostly unwritten African languages of the Bantu family. For\none of them, Mboshi, a language originating from the &#8220;Cuvette&#8221;\nregion of the Republic of Congo, we investigate unsupervised word discovery\ntechniques from an unsegmented stream of phonemes. We compare different\nmodels and algorithms, both monolingual and bilingual, on a new corpus\nin Mboshi and French, and discuss various ways to represent the data\nwith suitable granularity. An additional French-English corpus allows\nus to contrast the results obtained on Mboshi and to experiment with\nmore data.\n"
   ],
   "doi": "10.21437/Interspeech.2016-886"
  },
  "vetter16_interspeech": {
   "authors": [
    [
     "Marco",
     "Vetter"
    ],
    [
     "Markus",
     "Müller"
    ],
    [
     "Fatima",
     "Hamlaoui"
    ],
    [
     "Graham",
     "Neubig"
    ],
    [
     "Satoshi",
     "Nakamura"
    ],
    [
     "Sebastian",
     "Stüker"
    ],
    [
     "Alex",
     "Waibel"
    ]
   ],
   "title": "Unsupervised Phoneme Segmentation of Previously Unseen Languages",
   "original": "1440",
   "page_count": 5,
   "order": 749,
   "p1": "3544",
   "pn": "3548",
   "abstract": [
    "In this paper we investigate the automatic detection of phoneme boundaries\nin audio recordings of an unknown language. This work is motivated\nby the needs of the project BULB which aims to support linguists in\ndocumenting unwritten languages. The automatic phonemic transcription\nof recordings of the unwritten language is part of this. We cannot\nuse multilingual phoneme recognizers as their phoneme inventory might\nnot completely cover that of the new language. Thus we opted for pursuing\na two step approach which is inspired by work from speech synthesis\nfor previously unknown languages. First, we detect boundaries for phonemes,\nand then we classify the detected segments into phoneme units. In this\npaper we address the first step, i.e. the detection of the phoneme\nboundaries. For this we again used multilingual and crosslingual phoneme\nrecognizers but were only interested in the phoneme boundaries detected\nby them and not the phoneme identities. We measured the quality of\nthe segmentations obtained this way using precision, recall and F-measure.\nWe compared the performance of different configurations of mono- and\nmultilingual phoneme recognizers among each other and against a monolingual\ngold standard. Finally we applied the technique to Basaa, a Bantu language.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1440"
  },
  "manenti16_interspeech": {
   "authors": [
    [
     "Céline",
     "Manenti"
    ],
    [
     "Thomas",
     "Pellegrini"
    ],
    [
     "Julien",
     "Pinquier"
    ]
   ],
   "title": "CNN-Based Phone Segmentation Experiments in a Less-Represented Language",
   "original": "0796",
   "page_count": 5,
   "order": 750,
   "p1": "3549",
   "pn": "3553",
   "abstract": [
    "These last years, there has been a regain of interest in unsupervised\nsub-lexical and lexical unit discovery. Speech segmentation into phone-like\nunits may be a first interesting step for such a task. In this article,\nwe report speech segmentation experiments in Xitsonga, a less-represented\nlanguage spoken in South Africa. We chose to use convolutional neural\nnetworks (CNN) with FBANK static coefficients as input. The models\ntake binary decisions whether a boundary is present or not at each\nsignal sliding frame. We compare the use of a model trained exclusively\non Xitsonga data to the use of a bootstrap model trained on a larger\ncorpus of another language, the BUCKEYE U.S. English corpus. Using\na two-convolution-layer model, a 79% F-measure was obtained on BUCKEYE,\nwith a 20 ms error tolerance. This performance is equal to the human\ninter-annotator agreement rate. We then used this bootstrap model to\nsegment Xitsonga data and compared the results when adapting it with\n1 to 20 minutes of Xitsonga data.\n"
   ],
   "doi": "10.21437/Interspeech.2016-796"
  },
  "schlunz16_interspeech": {
   "authors": [
    [
     "Georg I.",
     "Schlünz"
    ],
    [
     "Nkosikhona",
     "Dlamini"
    ],
    [
     "Rynhardt P.",
     "Kruger"
    ]
   ],
   "title": "Part-of-Speech Tagging and Chunking in Text-to-Speech Synthesis for South African Languages",
   "original": "1040",
   "page_count": 5,
   "order": 751,
   "p1": "3554",
   "pn": "3558",
   "abstract": [
    "Text-to-speech synthesis can be an empowering communication tool in\nthe hands of the print-disabled or augmentative and alternative communication\nuser. In an effort to improve the naturalness of synthesised speech\n&#8212; and thus enhance the communication experience &#8212; we apply\nthe natural language processing tasks of part-of-speech tagging and\nchunking to the text in the synthesis process. We cover the South African\nlanguages of (South African) English, Afrikaans, isiXhosa, isiZulu\nand Sepedi. The part-of-speech tagging delivers positive results for\nmost of the languages; however, the chunking does not give any improvement\nin its current form.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1040"
  },
  "westhuizen16_interspeech": {
   "authors": [
    [
     "Ewald van der",
     "Westhuizen"
    ],
    [
     "Thomas",
     "Niesler"
    ]
   ],
   "title": "The Effect of Postlexical Deletion on Automatic Speech Recognition in Fast Spontaneously Spoken Zulu",
   "original": "0820",
   "page_count": 5,
   "order": 752,
   "p1": "3559",
   "pn": "3563",
   "abstract": [
    "We consider the phenomenon of postlexical deletion in fast spontaneously\nspoken isiZulu speech and its implication for automatic speech recognition\n(ASR). Analysis of hand-crafted transcripts of fast spontaneous speech\nrecorded from broadcast media indicates that postlexical deletion,\nespecially of vowels, is common in isiZulu. We show that ASR performance\ncan be increased by inclusion of pronunciation variants that model\nsuch deletions. We also apply a sequence modelling approach normally\nused for grapheme-to-phoneme (G2P) conversion to generate orthography\ncontaining synthetic deletions. These synthetically generated contacted\nwords are subsequently used to generate accompanying pronunciations\nusing conventional G2P conversion. We evaluate an ASR system using\nthese synthetically generated pronunciations, and compare it to a baseline\nsystem without such variants as well as an oracle system. Augmentation\nwith synthetically generated pronunciations leads to an absolute improvement\nin word error rate (WER) of 2.36% relative to the baseline. Furthermore,\nthe augmented system performs almost as well as the oracle system,\nwith an absolute difference in WER of 0.38%.\n"
   ],
   "doi": "10.21437/Interspeech.2016-820"
  },
  "ramanarayanan16_interspeech": {
   "authors": [
    [
     "Vikram",
     "Ramanarayanan"
    ],
    [
     "Benjamin",
     "Parrell"
    ],
    [
     "Louis",
     "Goldstein"
    ],
    [
     "Srikantan",
     "Nagarajan"
    ],
    [
     "John",
     "Houde"
    ]
   ],
   "title": "A New Model of Speech Motor Control Based on Task Dynamics and State Feedback",
   "original": "1499",
   "page_count": 5,
   "order": 753,
   "p1": "3564",
   "pn": "3568",
   "abstract": [
    "We present a new model of speech motor control (TD-SFC) based on articulatory\ngoals that explicitly incorporates acoustic sensory feedback using\na framework for state-based control. We do this by combining two existing,\ncomplementary models of speech motor control &#8212; the Task Dynamics\nmodel [1] and the State Feedback Control model of speech [2]. We demonstrate\nthe effectiveness of the combined model by simulating a simple formant\nperturbation study, and show that the model qualitatively reproduces\nthe behavior of online compensation for unexpected perturbations reported\nin human subjects.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1499"
  },
  "dabbaghchian16_interspeech": {
   "authors": [
    [
     "Saeed",
     "Dabbaghchian"
    ],
    [
     "Marc",
     "Arnela"
    ],
    [
     "Olov",
     "Engwall"
    ],
    [
     "Oriol",
     "Guasch"
    ],
    [
     "Ian",
     "Stavness"
    ],
    [
     "Pierre",
     "Badin"
    ]
   ],
   "title": "Using a Biomechanical Model and Articulatory Data for the Numerical Production of Vowels",
   "original": "1500",
   "page_count": 5,
   "order": 754,
   "p1": "3569",
   "pn": "3573",
   "abstract": [
    "We introduce a framework to study speech production using a biomechanical\nmodel of the human vocal tract, ArtiSynth. Electromagnetic articulography\ndata was used as input to an inverse tracking simulation that estimates\nmuscle activations to generate 3D jaw and tongue postures corresponding\nto the target articulator positions. For acoustic simulations, the\nvocal tract geometry is needed, but since the vocal tract is a cavity\nrather than a physical object, its geometry does not explicitly exist\nin a biomechanical model. A fully-automatic method to extract the 3D\ngeometry (surface mesh) of the vocal tract by blending geometries of\nthe relevant articulators has therefore been developed. This automatic\nextraction procedure is essential, since a method with manual intervention\nis not feasible for large numbers of simulations or for generation\nof dynamic sounds, such as diphthongs. We then simulated the vocal\ntract acoustics by using the Finite Element Method (FEM). This requires\na high quality vocal tract mesh without irregular geometry or self-intersections.\nWe demonstrate that the framework is applicable to acoustic FEM simulations\nof a wide range of vocal tract deformations. In particular we present\nresults for cardinal vowel production, with muscle activations, vocal\ntract geometry, and acoustic simulations.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1500"
  },
  "wei16_interspeech": {
   "authors": [
    [
     "Jianguo",
     "Wei"
    ],
    [
     "Wendan",
     "Guan"
    ],
    [
     "Darcy Q.",
     "Hou"
    ],
    [
     "Dingyi",
     "Pan"
    ],
    [
     "Wenhuan",
     "Lu"
    ],
    [
     "Jianwu",
     "Dang"
    ]
   ],
   "title": "A New Model for Acoustic Wave Propagation and Scattering in the Vocal Tract",
   "original": "1513",
   "page_count": 5,
   "order": 755,
   "p1": "3574",
   "pn": "3578",
   "abstract": [
    "A new and efficient numerical model is proposed for simulating the\nacoustic wave propagation and scattering problems due to a complex\ngeometry. In this model, the linearized Euler equations are solved\nby the finite-difference time-domain (FDTD) method on an orthogonal\nEulerian grid. The complex wall boundary represented by a series of\nLagrangian points is numerically treated by the immersed boundary method\n(IBM). To represent the interaction between these two systems, a force\nfield is added to the momentum equation, which is calculated on the\nLagrangian points and interpolated to the nearby Eulerian points. The\npressure and velocity fields are then calculated alternatively using\nFDTD. The developed model is verified in the case of acoustic scattering\nby a cylinder, for which the exact solutions exist. The model is then\napplied to sound wave propagation in a 2D vocal tract with area function\nextracted from MRI data. To show the advantage of present model, the\ngrid points are non-aligned with the boundary. The numerical results\nhave good agreements with solutions in literature. A FDTD calculation\nwith boundary condition directly imposed on the grid points closest\nto the wall cannot give a reasonable solution.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1513"
  },
  "szabados16_interspeech": {
   "authors": [
    [
     "Andrew",
     "Szabados"
    ],
    [
     "Pascal",
     "Perrier"
    ]
   ],
   "title": "Uncontrolled Manifolds in Vowel Production: Assessment with a Biomechanical Model of the Tongue",
   "original": "1579",
   "page_count": 5,
   "order": 756,
   "p1": "3579",
   "pn": "3583",
   "abstract": [
    "Motor equivalence is a key feature of speech motor control, since speakers\nmust constantly adapt to various phonetic contexts and speaking conditions.\nThe Uncontrolled Manifold (UCM) idea offers a theoretical framework\nfor considering motor equivalence. In this framework coordination among\nmotor control variables is separated into two subspaces, one in which\nchanges in control variables modify the acoustic output and another\none in which these changes do not influence the output. Our work develops\nthis concept for speech production using a 2D biomechanical model of\nthe tongue, coupled with a jaw and lip model, for vowel production.\nWe first propose a representation of the linearized UCM based on orthogonal\nprojection matrices. Next we characterize the UCMs of various vocal\ntract configurations of the 10 French oral vowels using their perturbation\nresponses. We then investigate whether these UCMs describe phonetic\nclasses like phonemes, front/back vowels, rounded/unrounded vowels,\nor whether they significantly vary across representatives of these\ndifferent classes. We found they clearly differ between rounded and\nunrounded vowels, but are quite similar within each category. This\nsuggests that similar motor equivalence strategies can be implemented\nwithin each of these classes and that UCMs provide a valid characterization\nof an equivalence strategy.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1579"
  },
  "yoshinaga16_interspeech": {
   "authors": [
    [
     "Tsukasa",
     "Yoshinaga"
    ],
    [
     "Kazunori",
     "Nozaki"
    ],
    [
     "Shigeo",
     "Wada"
    ]
   ],
   "title": "Experimental Validation of Sound Generated from Flow in Simplified Vocal Tract Model of Sibilant /s/",
   "original": "1597",
   "page_count": 4,
   "order": 757,
   "p1": "3584",
   "pn": "3587",
   "abstract": [
    "The coupled numerical simulation of flow and sound generation in a\nsimplified vocal tract model of sibilant /s/ were validated with experimental\nmeasurements. The simplified model consists of incisors and four rectangular\nchannels representing a throat, constriction, space behind the incisors,\nand lips. Velocity distribution and far-field sound were measured by\na hot-wire anemometer and an acoustic microphone, respectively. Simulated\namplitude of velocity fluctuation at the flow separation region was\nstabilized by increasing the grid resolution, and agreed with those\nof the measurement. Amplitude of sound pressure simulated by the low-resolution\ngrids was larger than that of the high-resolution grids, indicating\nthat calculation accuracy of velocity fluctuation at the separation\nregion is required to simulate sound generation of the sibilant /s/.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1597"
  },
  "patri16_interspeech": {
   "authors": [
    [
     "Jean-François",
     "Patri"
    ],
    [
     "Pascal",
     "Perrier"
    ],
    [
     "Julien",
     "Diard"
    ]
   ],
   "title": "Bayesian Modeling in Speech Motor Control: A Principled Structure for the Integration of Various Constraints",
   "original": "0441",
   "page_count": 5,
   "order": 758,
   "p1": "3588",
   "pn": "3592",
   "abstract": [
    "Speaking involves sequences of linguistic units that can be produced\nunder different sets of control strategies. For instance, a given phoneme\ncan be achieved with different acoustic properties, and a sequence\nof phonemes can be performed at different speech rates and with different\nprosodies. How does the Central Nervous System select a specific control\nstrategy among all the available ones? In a previously published article\nwe proposed a Bayesian model that addressed this question with respect\nto the multiplicity of acoustic realizations of a sequence of phonemes.\nOne of the strengths of Bayesian modeling is that it is well adapted\nto the combination of multiple constraints. In the present paper we\nillustrate this feature by defining an extension of our previous model\nthat includes force constraints related to the level of effort for\nthe production of phoneme sequences, as it could be the case in clear\nversus casual speech. The integration of this additional constraint\nis used to model the control of articulation clarity. Pertinence of\nthe results is illustrated by controlling a biomechanical model of\nthe vocal tract for speech production.\n"
   ],
   "doi": "10.21437/Interspeech.2016-441"
  },
  "zhang16j_interspeech": {
   "authors": [
    [
     "Zixing",
     "Zhang"
    ],
    [
     "Fabien",
     "Ringeval"
    ],
    [
     "Jing",
     "Han"
    ],
    [
     "Jun",
     "Deng"
    ],
    [
     "Erik",
     "Marchi"
    ],
    [
     "Björn",
     "Schuller"
    ]
   ],
   "title": "Facing Realism in Spontaneous Emotion Recognition from Speech: Feature Enhancement by Autoencoder with LSTM Neural Networks",
   "original": "0998",
   "page_count": 5,
   "order": 759,
   "p1": "3593",
   "pn": "3597",
   "abstract": [
    "During the last decade, speech emotion recognition technology has matured\nwell enough to be used in some real-life scenarios. However, these\nscenarios require an almost silent environment to not compromise the\nperformance of the system. Emotion recognition technology from speech\nthus needs to evolve and face more challenging conditions, such as\nenvironmental additive and convolutional noises, in order to broaden\nits applicability to real-life conditions. This contribution evaluates\nthe impact of a front-end feature enhancement method based on an autoencoder\nwith long short-term memory neural networks, for robust emotion recognition\nfrom speech. Support Vector Regression is then used as a back-end for\ntime- and value-continuous emotion prediction from enhanced features.\nWe perform extensive evaluations on both non-stationary additive noise\nand convolutional noise, on a database of spontaneous and natural emotions.\nResults show that the proposed method significantly outperforms a system\ntrained on raw features, for both arousal and valence dimensions, while\nhaving almost no degradation when applied to clean speech.\n"
   ],
   "doi": "10.21437/Interspeech.2016-998"
  },
  "parthasarathy16_interspeech": {
   "authors": [
    [
     "Srinivas",
     "Parthasarathy"
    ],
    [
     "Carlos",
     "Busso"
    ]
   ],
   "title": "Defining Emotionally Salient Regions Using Qualitative Agreement Method",
   "original": "0429",
   "page_count": 5,
   "order": 760,
   "p1": "3598",
   "pn": "3602",
   "abstract": [
    "Conventional emotion classification methods focus on predefined segments\nsuch as sentences or speaking turns that are labeled and classified\nat the segment level. However, the emotional state dynamically fluctuates\nduring human interactions, so not all the segments have the same relevance.\nWe are interested in detecting regions within the interaction where\nthe emotions are particularly salient, which we refer to as  emotional\nhotspots. A system with this capability can have real applications\nin many domains. A key step towards building such a system is to define\nreliable hotspot labels, which will dictate the performance of machine\nlearning algorithms. Creating ground-truth labels from scratch is both\nexpensive and time consuming. This paper also demonstrates that defining\nthose emotionally salient segments using perceptual evaluation is a\nhard problem resulting in low inter-evaluator agreement. Instead, we\npropose to define emotionally salient regions leveraging existing time-continuous\nemotional labels. The proposed approach relies on the  qualitative\nagreement (QA) method, which dynamically captures increasing or decreasing\ntrends across emotional traces provided by multiple evaluators. The\nproposed method is more reliable than just averaging traces across\nevaluators, providing the flexibility to define hotspots at various\nreliability levels without having to recollect new perceptual evaluations.\n"
   ],
   "doi": "10.21437/Interspeech.2016-429"
  },
  "ghosh16b_interspeech": {
   "authors": [
    [
     "Sayan",
     "Ghosh"
    ],
    [
     "Eugene",
     "Laksana"
    ],
    [
     "Louis-Philippe",
     "Morency"
    ],
    [
     "Stefan",
     "Scherer"
    ]
   ],
   "title": "Representation Learning for Speech Emotion Recognition",
   "original": "0692",
   "page_count": 5,
   "order": 761,
   "p1": "3603",
   "pn": "3607",
   "abstract": [
    "Speech emotion recognition is an important problem with applications\nas varied as human-computer interfaces and affective computing. Previous\napproaches to emotion recognition have mostly focused on extraction\nof carefully engineered features and have trained simple classifiers\nfor the emotion task. There has been limited effort at representation\nlearning for affect recognition, where features are learnt directly\nfrom the signal waveform or spectrum. Prior work also does not investigate\nthe effect of transfer learning from affective attributes such as valence\nand activation to categorical emotions. In this paper, we investigate\nemotion recognition from spectrogram features extracted from the speech\nand glottal flow signals; spectrogram encoding is performed by a stacked\nautoencoder and an RNN (Recurrent Neural Network) is used for classification\nof four primary emotions. We perform two experiments to improve RNN\ntraining : (1) Representation Learning &#8212; Model training on the\nglottal flow signal to investigate the effect of speaker and phonetic\ninvariant features on classification performance (2) Transfer Learning\n&#8212; RNN training on valence and activation, which is adapted to\na four emotion classification task. On the USC-IEMOCAP dataset, our\nproposed approach achieves a performance comparable to the state of\nthe art speech emotion recognition systems.\n"
   ],
   "doi": "10.21437/Interspeech.2016-692"
  },
  "li16m_interspeech": {
   "authors": [
    [
     "Xingfeng",
     "Li"
    ],
    [
     "Masato",
     "Akagi"
    ]
   ],
   "title": "Multilingual Speech Emotion Recognition System Based on a Three-Layer Model",
   "original": "0645",
   "page_count": 5,
   "order": 762,
   "p1": "3608",
   "pn": "3612",
   "abstract": [
    "Speech Emotion Recognition (SER) systems currently are focusing on\nclassifying emotions on each single language. Since optimal acoustic\nsets are strongly language dependent, to achieve a generalized SER\nsystem working for multiple languages, issues of selection of common\nfeatures and retraining are still challenging. In this paper, we therefore\npresent a SER system in a multilingual scenario from perspective of\nhuman perceptual processing. The goal is twofold. Firstly, to predict\nmultilingual emotion dimensions accurately such as human annotations.\nTo this end, a three layered model consist of acoustic features, semantic\nprimitives, emotion dimensions, along with Fuzzy Inference System (FIS)\nwere studied. Secondly, by knowledge of human perception of emotion\namong languages in dimensional space, we adopt direction and distance\nas common features to detect multilingual emotions. Results of estimation\nperformance of emotion dimensions comparable to human evaluation is\nfurnished, and classification rates that are close to monolingual SER\nsystem performed are achieved.\n"
   ],
   "doi": "10.21437/Interspeech.2016-645"
  },
  "kalinli16_interspeech": {
   "authors": [
    [
     "Ozlem",
     "Kalinli"
    ]
   ],
   "title": "Analysis of Multi-Lingual Emotion Recognition Using Auditory Attention Features",
   "original": "1557",
   "page_count": 5,
   "order": 763,
   "p1": "3613",
   "pn": "3617",
   "abstract": [
    "In this paper, we build mono-lingual and cross-lingual emotion recognition\nsystems and report performance on English and German databases. The\nemotion recognition system uses biologically inspired auditory attention\nfeatures together with a neural network for learning the mapping between\nfeatures and emotion classes. We first build mono-lingual systems for\nboth Berlin Database of Emotional Speech (EMO-DB) and LDC&#8217;s Emotional\nProsody (Emo-Prosody) and achieve 82.7% and 56.7% accuracy for five\nclass emotion classification (neutral, sad, angry, happy, and boredom)\nusing leave-one-speaker-out cross validation. When tested with cross-lingual\nsystems, the five-class emotion recognition accuracy drops to 55.1%\nand 41.4% accuracy for EMO-DB and Emo-Prosody, respectively. Finally,\nwe build a bilingual emotion recognition system and report experimental\nresults and their analysis. Bilingual system performs close to the\nperformance of individual mono-lingual systems.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1557"
  },
  "fayek16_interspeech": {
   "authors": [
    [
     "Haytham M.",
     "Fayek"
    ],
    [
     "Margaret",
     "Lech"
    ],
    [
     "Lawrence",
     "Cavedon"
    ]
   ],
   "title": "On the Correlation and Transferability of Features Between Automatic Speech Recognition and Speech Emotion Recognition",
   "original": "0868",
   "page_count": 5,
   "order": 764,
   "p1": "3618",
   "pn": "3622",
   "abstract": [
    "The correlation between Automatic Speech Recognition (ASR) and Speech\nEmotion Recognition (SER) is poorly understood. Studying such correlation\nmay pave the way for integrating both tasks into a single system or\nmay provide insights that can aid in advancing both systems such as\nimproving ASR in dealing with emotional speech or embedding linguistic\ninput into SER. In this paper, we quantify the relation between ASR\nand SER by studying the relevance of features learned between both\ntasks in deep convolutional neural networks using transfer learning.\nExperiments are conducted using the TIMIT and IEMOCAP databases. Results\nreveal an intriguing correlation between both tasks, where features\nlearned in some layers particularly towards initial layers of the network\nfor either task were found to be applicable to the other task with\nvarying degree.\n"
   ],
   "doi": "10.21437/Interspeech.2016-868"
  },
  "valenti16_interspeech": {
   "authors": [
    [
     "Giacomo",
     "Valenti"
    ],
    [
     "Adrien",
     "Daniel"
    ],
    [
     "Nicholas",
     "Evans"
    ]
   ],
   "title": "On the Influence of Text Content on Pass-Phrase Strength for Short-Duration Text-Dependent Automatic Speaker Authentication",
   "original": "1115",
   "page_count": 5,
   "order": 765,
   "p1": "3623",
   "pn": "3627",
   "abstract": [
    "In the context of automatic speaker verification it is well known that\ndifferent speech units offer different levels of speaker discrimination.\nFor short-duration, text-dependent automatic speaker recognition, a\nuser&#8217;s pass-phrase bears influence on how reliably they can be\nrecognized; just as is the case with text passwords, some spoken pass-phrases\nare more secure than others. This paper investigates the influence\nof text or phone content on recognition performance. This work is performed\nusing the shortest duration subset of the standard RSR2015 database.\nWith a thorough statistical analysis, the work shows how significant\nreductions in error rates can be achieved by preventing the use of\nweak passwords and that improvements in performance are consistent\nacross disjoint speaker subsets. The ultimate goal of this work is\nto develop an automated means of enforcing the use of stronger or more\ndiscriminant spoken pass-phrases.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1115"
  },
  "todisco16_interspeech": {
   "authors": [
    [
     "Massimiliano",
     "Todisco"
    ],
    [
     "Héctor",
     "Delgado"
    ],
    [
     "Nicholas",
     "Evans"
    ]
   ],
   "title": "Articulation Rate Filtering of CQCC Features for Automatic Speaker Verification",
   "original": "1140",
   "page_count": 5,
   "order": 766,
   "p1": "3628",
   "pn": "3632",
   "abstract": [
    "This paper introduces a new articulation rate filter and reports its\ncombination with recently proposed constant Q cepstral coefficients\n(CQCCs) in their first application to automatic speaker verification\n(ASV). CQCC features are extracted with the constant Q transform (CQT),\na perceptually-inspired alternative to Fourier-based approaches to\ntime-frequency analysis. The CQT offers greater frequency resolution\nat lower frequencies and greater time resolution at higher frequencies.\nWhen coupled with cepstral analysis and the new articulation rate filter,\nthe resulting CQCC features are readily modelled using conventional\ntechniques. A comparative assessment of CQCCs and mel frequency cepstral\ncoefficients (MFCC) for a short-duration speaker verification scenario\nshows that CQCCs generally outperform MFCCs and that the two feature\nrepresentations are highly complementary; fusion experiments with the\nRSR2015 and RedDots databases show relative reductions in equal error\nrates of as much as 60% compared to an MFCC baseline.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1140"
  },
  "sadjadi16_interspeech": {
   "authors": [
    [
     "Seyed Omid",
     "Sadjadi"
    ],
    [
     "Jason W.",
     "Pelecanos"
    ],
    [
     "Sriram",
     "Ganapathy"
    ]
   ],
   "title": "The IBM Speaker Recognition System: Recent Advances and Error Analysis",
   "original": "1159",
   "page_count": 5,
   "order": 767,
   "p1": "3633",
   "pn": "3637",
   "abstract": [
    "We present the recent advances along with an error analysis of the\nIBM speaker recognition system for conversational speech. Some of the\nkey advancements that contribute to our system include: a nearest-neighbor\ndiscriminant analysis (NDA) approach (as opposed to LDA) for intersession\nvariability compensation in the i-vector space, the application of\nspeaker and channel-adapted features derived from an automatic speech\nrecognition (ASR) system for speaker recognition, and the use of a\nDNN acoustic model with a very large number of output units (&#126;10k\nsenones) to compute the frame-level soft alignments required in the\ni-vector estimation process. We evaluate these techniques on the NIST\n2010 SRE extended core conditions (C1&#8211;C9), as well as the  10sec&#8211;10sec\ncondition. To our knowledge, results achieved by our system represent\nthe best performances published to date on these conditions. For example,\non the extended  tel-tel condition (C5) the system achieves an EER\nof 0.59%. To garner further understanding of the remaining errors (on\nC5), we examine the recordings associated with the low scoring target\ntrials, where various issues are identified for the problematic recordings/trials.\nInterestingly, it is observed that correcting the pathological recordings\nnot only improves the scores for the target trials but also for the\nnon-target trials.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1159"
  },
  "kheder16c_interspeech": {
   "authors": [
    [
     "Waad Ben",
     "Kheder"
    ],
    [
     "Driss",
     "Matrouf"
    ],
    [
     "Moez",
     "Ajili"
    ],
    [
     "Jean-François",
     "Bonastre"
    ]
   ],
   "title": "Probabilistic Approach Using Joint Clean and Noisy i-Vectors Modeling for Speaker Recognition",
   "original": "1292",
   "page_count": 5,
   "order": 768,
   "p1": "3638",
   "pn": "3642",
   "abstract": [
    "Additive noise is one of the main challenges for automatic speaker\nrecognition and several compensation techniques have been proposed\nto deal with this problem. In this paper, we present a new &#8220;data-driven&#8221;\ndenoising technique operating in the i-vector space based on a joint\nmodeling of clean and noisy i-vectors. The joint distribution is estimated\nusing a large set of i-vectors pairs (clean i-vectors and their noisy\nversions generated artificially) then integrated in an MMSE estimator\nin the test phase to compute a &#8220;cleaned-up&#8221; version of\nnoisy test i-vectors. We show that this algorithm achieves up to 80%\nof relative improvement in EER. We also present a version of the proposed\nalgorithm that can be used to compensate multiple &#8220;unseen&#8221;\nnoises. We test this technique on the recently published SITW database\nand show a significant gain compared to the baseline system performance.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1292"
  },
  "bahmaninezhad16_interspeech": {
   "authors": [
    [
     "Fahimeh",
     "Bahmaninezhad"
    ],
    [
     "John H.L.",
     "Hansen"
    ]
   ],
   "title": "Generalized Discriminant Analysis (GDA) for Improved i-Vector Based Speaker Recognition",
   "original": "1523",
   "page_count": 5,
   "order": 769,
   "p1": "3643",
   "pn": "3647",
   "abstract": [
    "In general, the majority of recent speaker recognition systems employ\nan i-Vector configuration as their front-end. Post-processing of i-Vectors\nusually requires a Linear Discriminant Analysis (LDA) phase to reduce\nthe dimensions of the i-Vectors as well as improve discrimination of\nspeaker classes based on the Fisher criterion. Given that channel,\nnoise, and other types of mismatch are generally present in the data,\nit is better to discriminate the speaker&#8217;s data non-linearly.\nGeneralized Discriminant Analysis (GDA) uses kernel functions to map\nthe data into a high dimensional feature-space which leads to non-linear\ndiscriminant analysis. In this study, we replace LDA with GDA in an\ni-Vector based speaker recognition system and study the effectiveness\nof various kernel functions. It is shown, based on equal error rate\n(EER) and minimum of detection cost function, that GDA not only improves\nperformance for regular test utterances, but is also useful for short\nduration test segments. NIST2010 Speaker Recognition Evaluation (SRE)\ncore and extended-core (coreext) conditions are employed for experiments;\nin addition, we evaluate the system for short duration segments on\nthe 10-sec test condition and truncated coreext test data. The relative\nimprovement in EER is 20% for the cosine kernel employed here with\nGDA processing.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1523"
  },
  "qian16b_interspeech": {
   "authors": [
    [
     "Yao",
     "Qian"
    ],
    [
     "Jidong",
     "Tao"
    ],
    [
     "David",
     "Suendermann-Oeft"
    ],
    [
     "Keelan",
     "Evanini"
    ],
    [
     "Alexei V.",
     "Ivanov"
    ],
    [
     "Vikram",
     "Ramanarayanan"
    ]
   ],
   "title": "Noise and Metadata Sensitive Bottleneck Features for Improving Speaker Recognition with Non-Native Speech Input",
   "original": "0548",
   "page_count": 5,
   "order": 770,
   "p1": "3648",
   "pn": "3652",
   "abstract": [
    "Recently, text independent speaker recognition systems with phonetically-aware\nDNNs, which allow the comparison among different speakers with &#8220;soft-aligned&#8221;\nphonetic content, have significantly outperformed standard i-vector\nbased systems [9&#8211;12]. However, when applied to speaker recognition\non a non-native spontaneous corpus, DNN-based speaker recognition does\nnot show its superior performance due to the relatively lower accuracy\nof phonetic content recognition. In this paper, noise-aware features\nand multi-task learning are investigated to improve the alignment of\nspeech feature frames into the sub-phonemic &#8220;senone&#8221; space\nand to &#8220;distill&#8221; the L1 (native language) information of\nthe test takers into bottleneck features (BNFs), which we refer to\nas metadata sensitive BNFs. Experimental results show that the system\nwith metadata sensitive BNFs can improve speaker recognition performance\nby a 23.9% relative reduction in equal error rate (EER) compared to\nthe baseline i-vector system. In addition, L1 info is just used to\ntrain the BNFs extractor, so it is not necessary to be used as input\nfor BNFs extraction, i-vector extraction and scoring for the enrollment\nand evaluation sets, which can avoid the use of erroneous L1s claimed\nby imposters.\n"
   ],
   "doi": "10.21437/Interspeech.2016-548"
  },
  "phan16_interspeech": {
   "authors": [
    [
     "Huy",
     "Phan"
    ],
    [
     "Lars",
     "Hertel"
    ],
    [
     "Marco",
     "Maass"
    ],
    [
     "Alfred",
     "Mertins"
    ]
   ],
   "title": "Robust Audio Event Recognition with 1-Max Pooling Convolutional Neural Networks",
   "original": "0123",
   "page_count": 5,
   "order": 771,
   "p1": "3653",
   "pn": "3657",
   "abstract": [
    "We present in this paper a simple, yet efficient convolutional neural\nnetwork (CNN) architecture for robust audio event recognition. Opposing\nto deep CNN architectures with multiple convolutional and pooling layers\ntopped up with multiple fully connected layers, the proposed network\nconsists of only three layers: convolutional, pooling, and softmax\nlayer. Two further features distinguish it from the deep architectures\nthat have been proposed for the task: varying-size convolutional filters\nat the convolutional layer and 1-max pooling scheme at the pooling\nlayer. In intuition, the network tends to select the most discriminative\nfeatures from the whole audio signals for recognition. Our proposed\nCNN not only shows state-of-the-art performance on the standard task\nof robust audio event recognition but also outperforms other deep architectures\nup to 4.5% in terms of recognition accuracy, which is equivalent to\n76.3% relative error reduction.\n"
   ],
   "doi": "10.21437/Interspeech.2016-123"
  },
  "karamanolakis16_interspeech": {
   "authors": [
    [
     "Giannis",
     "Karamanolakis"
    ],
    [
     "Elias",
     "Iosif"
    ],
    [
     "Athanasia",
     "Zlatintsi"
    ],
    [
     "Aggelos",
     "Pikrakis"
    ],
    [
     "Alexandros",
     "Potamianos"
    ]
   ],
   "title": "Audio-Based Distributional Representations of Meaning Using a Fusion of Feature Encodings",
   "original": "0839",
   "page_count": 5,
   "order": 772,
   "p1": "3658",
   "pn": "3662",
   "abstract": [
    "Recently a &#8220;Bag-of-Audio-Words&#8221; approach was proposed [1]\nfor the combination of lexical features with audio clips in a multimodal\nsemantic representation, i.e., an Audio Distributional Semantic Model\n(ADSM). An important step towards the creation of ADSMs is the estimation\nof the semantic distance between clips in the acoustic space, which\nis especially challenging given the diversity of audio collections.\nIn this work, we investigate the use of different feature encodings\nin order to address this challenge following a two-step approach. First,\nan audio clip is categorized with respect to three classes, namely,\nmusic, speech and other. Next, the feature encodings are fused according\nto the posterior probabilities estimated in the previous step. Using\na collection of audio clips annotated with tags we derive a mapping\nbetween words and audio clips. Based on this mapping and the proposed\naudio semantic distance, we construct an ADSM model in order to compute\nthe distance between words (lexical semantic similarity task). The\nproposed model is shown to significantly outperform (23.6% relative\nimprovement in correlation coefficient) the state-of-the-art results\nreported in the literature.\n"
   ],
   "doi": "10.21437/Interspeech.2016-839"
  },
  "fujita16_interspeech": {
   "authors": [
    [
     "Yuya",
     "Fujita"
    ],
    [
     "Ken-ichi",
     "Iso"
    ]
   ],
   "title": "Robust DNN-Based VAD Augmented with Phone Entropy Based Rejection of Background Speech",
   "original": "0136",
   "page_count": 5,
   "order": 773,
   "p1": "3663",
   "pn": "3667",
   "abstract": [
    "We propose a DNN-based voice activity detector augmented by entropy\nbased frame rejection. DNN-based VAD classifies a frame into speech\nor non-speech and achieves significantly higher VAD performance compared\nto conventional statistical model-based VAD. We observed that many\nof the remaining errors are false alarms caused by background human\nspeech, such as TV/radio or surrounding peoples&#8217; conversations.\nIn order to reject such background speech frames, we introduce an entropy-based\nconfidence measure using the phone posterior probability output by\na DNN-based acoustic model. Compared to the target speaker&#8217;s\nvoice background speech tends to have relatively unclear pronunciation\nor is contaminated by other types of noises so its entropy becomes\nlarger than audio signals with only the target speaker&#8217;s voice.\nCombining DNN-based VAD and the entropy criterion, we reject speech\nframes classified by the DNN-based VAD as having an entropy larger\nthan a threshold value. We have evaluated the proposed approach and\nconfirmed greater than 10% reduction in Sentence Error Rate.\n"
   ],
   "doi": "10.21437/Interspeech.2016-136"
  },
  "zazo16_interspeech": {
   "authors": [
    [
     "Ruben",
     "Zazo"
    ],
    [
     "Tara N.",
     "Sainath"
    ],
    [
     "Gabor",
     "Simko"
    ],
    [
     "Carolina",
     "Parada"
    ]
   ],
   "title": "Feature Learning with Raw-Waveform CLDNNs for Voice Activity Detection",
   "original": "0268",
   "page_count": 5,
   "order": 774,
   "p1": "3668",
   "pn": "3672",
   "abstract": [
    "Voice Activity Detection (VAD) is an important preprocessing step in\nany state-of-the-art speech recognition system. Choosing the right\nset of features and model architecture can be challenging and is an\nactive area of research. In this paper we propose a novel approach\nto VAD to tackle both feature and model selection jointly. The proposed\nmethod is based on a CLDNN (Convolutional, Long Short-Term Memory,\nDeep Neural Networks) architecture fed directly with the raw waveform.\nWe show that using the raw waveform allows the neural network to learn\nfeatures directly for the task at hand, which is more powerful than\nusing log-mel features, specially for noisy environments. In addition,\nusing a CLDNN, which takes advantage of both frequency modeling with\nthe CNN and temporal modeling with LSTM, is a much better model for\nVAD compared to the DNN. The proposed system achieves over 78% relative\nimprovement in False Alarms (FA) at the operating point of 2% False\nRejects (FR) on both clean and noisy conditions compared to a DNN of\ncomparable size trained with log-mel features. In addition, we study\nthe impact of the model size and the learned features to provide a\nbetter understanding of the proposed architecture.\n"
   ],
   "doi": "10.21437/Interspeech.2016-268"
  },
  "graciarena16_interspeech": {
   "authors": [
    [
     "Martin",
     "Graciarena"
    ],
    [
     "Luciana",
     "Ferrer"
    ],
    [
     "Vikramjit",
     "Mitra"
    ]
   ],
   "title": "The SRI System for the NIST OpenSAD 2015 Speech Activity Detection Evaluation",
   "original": "0550",
   "page_count": 5,
   "order": 775,
   "p1": "3673",
   "pn": "3677",
   "abstract": [
    "In this paper, we present the SRI system submission to the NIST OpenSAD\n2015 speech activity detection (SAD) evaluation. We present results\non three different development databases that we created from the provided\ndata. We present system-development results for feature normalization;\nfor feature fusion with acoustic, voicing, and channel bottleneck features;\nand finally for SAD bottleneck-feature fusion. We present a novel technique\ncalled test adaptive calibration, which is designed to improve decision-threshold\nselection for each test waveform. We present unsupervised test adaptation\nof the fusion component and describe its tight synergy to the test\nadaptive calibration component. Finally, we present results on the\nevaluation test data and show how the proposed techniques lead to significant\ngains on channels unseen during training.\n"
   ],
   "doi": "10.21437/Interspeech.2016-550"
  },
  "karakos16_interspeech": {
   "authors": [
    [
     "Damianos",
     "Karakos"
    ],
    [
     "Scott",
     "Novotney"
    ],
    [
     "Le",
     "Zhang"
    ],
    [
     "Richard",
     "Schwartz"
    ]
   ],
   "title": "Model Adaptation and Active Learning in the BBN Speech Activity Detection System for the DARPA RATS Program",
   "original": "0603",
   "page_count": 5,
   "order": 776,
   "p1": "3678",
   "pn": "3682",
   "abstract": [
    "Model adaptation is an important task in many human language technology\nfields, as it allows one to reduce differences that arise due to various\nforms of variability. Here, we focus on the speech activity detection\n(SAD) task, in the context of the DARPA RATS program, where the training\ndata do not cover all channels (transmitter/receiver characteristics)\nthat are encountered at test time. For supervised adaptation, limited\nmanually labeled data from the (novel) channel of interest are used\nto adapt the model; for unsupervised adaptation, the labels are automatically\ngenerated with a baseline model. The modeling is done with long short-term\nmemory neural networks, and we make the case that strong regularization\nis of paramount importance when adapting such models. Results on two\ndifferent datasets show that adaptation gives rise to large gains (at\nleast 27related task, that of active learning, is also considered.\nIn active learning, data to be annotated for supervised adaptation\nare selected automatically, with the ultimate goal of maximizing performance.\nWe investigate an algorithm for active learning that utilizes the output\nof a SAD decoder and show that it performs significantly better (by\n10% relative) than random selection.\n"
   ],
   "doi": "10.21437/Interspeech.2016-603"
  },
  "mitra16c_interspeech": {
   "authors": [
    [
     "Vikramjit",
     "Mitra"
    ],
    [
     "Julien",
     "VanHout"
    ],
    [
     "Wen",
     "Wang"
    ],
    [
     "Chris",
     "Bartels"
    ],
    [
     "Horacio",
     "Franco"
    ],
    [
     "Dimitra",
     "Vergyri"
    ],
    [
     "Abeer",
     "Alwan"
    ],
    [
     "Adam",
     "Janin"
    ],
    [
     "John H.L.",
     "Hansen"
    ],
    [
     "Richard M.",
     "Stern"
    ],
    [
     "Abhijeet",
     "Sangwan"
    ],
    [
     "Nelson",
     "Morgan"
    ]
   ],
   "title": "Fusion Strategies for Robust Speech Recognition and Keyword Spotting for Channel- and Noise-Degraded Speech",
   "original": "0279",
   "page_count": 5,
   "order": 777,
   "p1": "3683",
   "pn": "3687",
   "abstract": [
    "Recognizing speech under high levels of channel and/or noise degradation\nis challenging. Current state-of-the-art automatic speech recognition\nsystems are sensitive to changing acoustic conditions, which can cause\nsignificant performance degradation. Noise-robust acoustic features\ncan improve speech recognition performance under varying background\nconditions, where it is usually observed that robust modeling techniques\nand multiple system fusion can help to improve the performance even\nfurther. This work investigates a wide array of robust acoustic features\nthat have been previously used to successfully improve speech recognition\nrobustness. We use these features to train individual acoustic models,\nand we analyze their individual performance. We investigate and report\nresults for simple feature combination, feature-map combination at\nthe output of convolutional layers, and fusion of deep neural nets\nat the senone posterior level. We report results for speech recognition\non a large-vocabulary, noise- and channel-degraded Levantine Arabic\nspeech corpus distributed through the Defense Advance Research Projects\nAgency (DARPA) Robust Automatic Speech Transcription (RATS) program.\nIn addition, we report keyword spotting results to demonstrate the\neffect of robust features and multiple levels of information fusion.\n"
   ],
   "doi": "10.21437/Interspeech.2016-279"
  },
  "sawada16_interspeech": {
   "authors": [
    [
     "Naoki",
     "Sawada"
    ],
    [
     "Hiromitsu",
     "Nishizaki"
    ]
   ],
   "title": "Recurrent Neural Network-Based Phoneme Sequence Estimation Using Multiple ASR Systems&#8217; Outputs for Spoken Term Detection",
   "original": "0337",
   "page_count": 5,
   "order": 778,
   "p1": "3688",
   "pn": "3692",
   "abstract": [
    "This paper describes a novel correct phoneme sequence estimation method\nthat uses a recurrent neural network (RNN)-based framework for spoken\nterm detection (STD). In an automatic speech recognition (ASR)-based\nSTD framework, ASR performance (word or subword error rate) affects\nSTD performance. Therefore, it is important to reduce ASR errors to\nobtain good STD results. In this study, we use an RNN-based phoneme\nestimator, which estimates a correct phoneme sequence of an utterance\nfrom some sorts of phoneme-based transcriptions produced by multiple\nASR systems in post-processing, to reduce phoneme errors. With two\ntypes of test speech corpora, the proposed phoneme estimator obtained\nphoneme-based N-best transcriptions with fewer phoneme recognition\nerrors than the N-best transcriptions from the best ASR system we prepared.\nIn addition, the STD system with the RNN-based phoneme estimator drastically\nimproved STD performance with two test collections for STD compared\nto our previously proposed STD system with a conditional random fields-based\nphoneme estimator.\n"
   ],
   "doi": "10.21437/Interspeech.2016-337"
  },
  "kane16_interspeech": {
   "authors": [
    [
     "Mark",
     "Kane"
    ],
    [
     "Julie",
     "Carson-Berndsen"
    ]
   ],
   "title": "Enhancing Data-Driven Phone Confusions Using Restricted Recognition",
   "original": "0489",
   "page_count": 5,
   "order": 779,
   "p1": "3693",
   "pn": "3697",
   "abstract": [
    "This paper presents a novel approach to address data sparseness in\nstandard confusion matrices and demonstrates how enhanced matrices,\nwhich capture additional similarities, can impact the performance of\nspoken term detection. Using the same training data as for the standard\nphone confusion matrix, an enhanced confusion matrix is created by\niteratively restricting the recognition process to exclude one acoustic\nmodel per iteration. Since this results in a greater amount of confusion\ndata for each phone, the enhanced confusion matrix encodes more similarities.\nThe enhanced phone confusion matrices perform demonstrably better than\nstandard confusion matrices on a spoken term detection task which uses\nboth HMMs and DNNs.\n"
   ],
   "doi": "10.21437/Interspeech.2016-489"
  },
  "ni16b_interspeech": {
   "authors": [
    [
     "Chongjia",
     "Ni"
    ],
    [
     "Lei",
     "Wang"
    ],
    [
     "Cheung-Chi",
     "Leung"
    ],
    [
     "Feng",
     "Rao"
    ],
    [
     "Li",
     "Lu"
    ],
    [
     "Bin",
     "Ma"
    ],
    [
     "Haizhou",
     "Li"
    ]
   ],
   "title": "Rapid Update of Multilingual Deep Neural Network for Low-Resource Keyword Search",
   "original": "0053",
   "page_count": 5,
   "order": 780,
   "p1": "3698",
   "pn": "3702",
   "abstract": [
    "This paper proposes an approach to rapidly update a multilingual deep\nneural network (DNN) acoustic model for low-resource keyword search\n(KWS). We use submodular data selection to select a small amount of\nmultilingual data which covers diverse acoustic conditions and is acoustically\nclose to a low-resource target language. The selected multilingual\ndata together with a small amount of the target language data are then\nused to rapidly update the readily available multilingual DNN. Moreover,\nthe weighted cross-entropy criterion is applied to update the multilingual\nDNN to obtain the acoustic model for the target language. To verify\nthe proposed approach, experiments were conducted based on four speech\ncorpora (including Cantonese, Pashto, Turkish, and Tagalog) provided\nby the IARPA Babel program and the OpenKWS14 Tamil corpus. The 3-hour\nvery limited language pack (VLLP) of the Tamil corpus is considered\nas the target language, while the other four speech corpora are viewed\nas multilingual sources. Comparing with the traditional cross-lingual\ntransfer approach, the proposed approach achieved a 19% relative improvement\nin actual term weighted value on the 15-hour evaluation set in the\nVLLP condition, when a word-based or word-morph mixed language model\nwas used. Furthermore, the proposed approach was observed to have similar\nperformance as the KWS system based on the acoustic model built using\nthe target language and all multilingual data from scratch, but with\nshorter training time.\n"
   ],
   "doi": "10.21437/Interspeech.2016-53"
  },
  "leung16_interspeech": {
   "authors": [
    [
     "Cheung-Chi",
     "Leung"
    ],
    [
     "Lei",
     "Wang"
    ],
    [
     "Haihua",
     "Xu"
    ],
    [
     "Jingyong",
     "Hou"
    ],
    [
     "Van Tung",
     "Pham"
    ],
    [
     "Hang",
     "Lv"
    ],
    [
     "Lei",
     "Xie"
    ],
    [
     "Xiong",
     "Xiao"
    ],
    [
     "Chongjia",
     "Ni"
    ],
    [
     "Bin",
     "Ma"
    ],
    [
     "Eng Siong",
     "Chng"
    ],
    [
     "Haizhou",
     "Li"
    ]
   ],
   "title": "Toward High-Performance Language-Independent Query-by-Example Spoken Term Detection for MediaEval 2015: Post-Evaluation Analysis",
   "original": "0691",
   "page_count": 5,
   "order": 781,
   "p1": "3703",
   "pn": "3707",
   "abstract": [
    "This paper documents the significant components of a state-of-the-art\nlanguage-independent query-by-example spoken term detection system\ndesigned for the Query by Example Search on Speech Task (QUESST) in\nMediaEval 2015. We developed exact and partial matching DTW systems,\nand WFST based symbolic search systems to handle different types of\nsearch queries. To handle the noisy and reverberant speech in the task,\nwe trained tokenizers using data augmented with different noise and\nreverberation conditions. Our post-evaluation analysis showed that\nthe phone boundary label provided by the improved tokenizers brings\nmore accurate speech activity detection in DTW systems. We argue that\nacoustic condition mismatch is possibly a more important factor than\nlanguage mismatch for obtaining consistent gain from stacked bottleneck\nfeatures. Our post-evaluation system, involving a smaller number of\ncomponent systems, can outperform our submitted systems, which performed\nthe best for the task.\n"
   ],
   "doi": "10.21437/Interspeech.2016-691"
  },
  "soni16b_interspeech": {
   "authors": [
    [
     "Meet H.",
     "Soni"
    ],
    [
     "Hemant A.",
     "Patil"
    ]
   ],
   "title": "Novel Subband Autoencoder Features for Non-Intrusive Quality Assessment of Noise Suppressed Speech",
   "original": "0693",
   "page_count": 5,
   "order": 782,
   "p1": "3708",
   "pn": "3712",
   "abstract": [
    "In this paper, we propose a novel feature extraction architecture of\nDeep Neural Network (DNN), namely, subband autoencoder (SBAE). The\nproposed architecture is inspired by the Human Auditory System (HAS)\nand extracts features from speech spectrum in an unsupervised manner.\nWe have used features extracted by this architecture for non-intrusive\nobjective quality assessment of noise suppressed speech signal. The\nquality assessment problem is posed as a  regression problem in which\nmapping between the acoustic features of speech signal and the corresponding\nsubjective score is found using single layer Artificial Neural Network\n(ANN). We have shown experimentally that proposed features give more\npowerful mapping than Mel filterbank energies, which are state-of-the-art\nacoustic features for various speech technology applications. Moreover,\nproposed method gives more accurate and correlated objective scores\nthan current standard objective quality assessment metric ITU-T P.563.\nExperiments performed on NOIZEUS database for different test conditions\nalso suggest that objective scores predicted using proposed method\nare more robust to different amount and types of noise.\n"
   ],
   "doi": "10.21437/Interspeech.2016-693"
  },
  "gao16_interspeech": {
   "authors": [
    [
     "Tian",
     "Gao"
    ],
    [
     "Jun",
     "Du"
    ],
    [
     "Li-Rong",
     "Dai"
    ],
    [
     "Chin-Hui",
     "Lee"
    ]
   ],
   "title": "SNR-Based Progressive Learning of Deep Neural Network for Speech Enhancement",
   "original": "0224",
   "page_count": 5,
   "order": 783,
   "p1": "3713",
   "pn": "3717",
   "abstract": [
    "In this paper, we propose a novel progressive learning (PL) framework\nfor deep neural network (DNN) based speech enhancement. It aims at\ndecomposing the complicated regression problem of mapping noisy to\nclean speech into a series of subproblems for enhancing system performances\nand reducing model complexities. As an illustration, we design a signal-to-noise\nratio (SNR) based PL architecture by guiding each hidden layer of the\nDNN to learn an intermediate target with gradual SNR gains explicitly.\nFurthermore, post-processing, with the rich set of information from\nthe multiple learning targets, can further be conducted. Experimental\nresults demonstrate that SNR-based progressive learning can effectively\nimprove perceptual evaluation of speech quality and short-time objective\nintelligibility in low SNR environments, and reduce the model parameters\nby 50% when compared with the DNN baseline system. Moreover, when combined\nwith post-processing, the proposed approach can be further improved.\n"
   ],
   "doi": "10.21437/Interspeech.2016-224"
  },
  "sadasivan16_interspeech": {
   "authors": [
    [
     "Jishnu",
     "Sadasivan"
    ],
    [
     "Chandra Sekhar",
     "Seelamantula"
    ]
   ],
   "title": "A Novel Risk-Estimation-Theoretic Framework for Speech Enhancement in Nonstationary and Non-Gaussian Noise Conditions",
   "original": "0151",
   "page_count": 5,
   "order": 784,
   "p1": "3718",
   "pn": "3722",
   "abstract": [
    "We address the problem of suppressing background noise from noisy speech\nwithin a risk estimation framework, where the clean signal is estimated\nfrom the noisy observations by minimizing an unbiased estimate of a\nchosen risk function. For Gaussian noise, such a risk estimate was\nderived by Stein, which eventually went on to be called Stein&#8217;s\nunbiased risk estimate (SURE). Stein&#8217;s formalism is restricted\nto Gaussian noise and exclusive risk estimators have been developed\nfor each noise type. On the other hand, we consider linear denoising\nfunctions and derive an unbiased risk estimate without making any assumption\nabout the noise distribution. The proposed unbiased estimate depends\nonly on the second-order statistics of noise and makes the proposed\nframework applicable to many practical denoising problems where the\nnoise distribution is not known a priori, but one has access only to\nthe samples of noise. We demonstrate the usefulness of the proposed\nmethodology for speech enhancement using subband shrinkage, where the\nshrinkage parameters are obtained by minimizing the newly developed\nrisk estimator. The proposed methodology is also applicable to nonstationary\nnoise conditions. We show that the proposed denoising algorithm outperforms\nthe state-of-the art algorithms in terms of standard speech-quality\nevaluation metrics.\n"
   ],
   "doi": "10.21437/Interspeech.2016-151"
  },
  "samui16_interspeech": {
   "authors": [
    [
     "Suman",
     "Samui"
    ],
    [
     "Indrajit",
     "Chakrabarti"
    ],
    [
     "Soumya Kanti",
     "Ghosh"
    ]
   ],
   "title": "Two-Stage Temporal Processing for Single-Channel Speech Enhancement",
   "original": "0307",
   "page_count": 5,
   "order": 785,
   "p1": "3723",
   "pn": "3727",
   "abstract": [
    "Most of the conventional speech enhancement methods operating in the\nspectral domain often suffer from spurious artifact called  musical\nnoise. Moreover, these methods also incur an extra overhead time for\nnoise power spectral density estimation. In this paper, a speech enhancement\nframework is proposed by cascading two temporal processing stages.\nThe first stage performs excitation source based temporal processing\nthat involves identifying and boosting the excitation source based\nspeech-specific features present at the gross and fine temporal levels,\nwhereas the second stage provides noise reduction by estimating standard\ndeviation of noise in time-domain by using a robust estimator. The\nproposed noise reduction stage is quite simply implementable and computationally\nless complex as it does not require noise estimation in spectral domain\nas a pre-processing phase. The experimental results have established\nthat the proposed scheme produces on an average 60&#8211;65% improvement\nin the speech quality (PESQ scores) and intelligibility (STOI scores)\nat 0 and -5 dB input SNR when compared to existing standard approaches.\n"
   ],
   "doi": "10.21437/Interspeech.2016-307"
  },
  "pm16_interspeech": {
   "authors": [
    [
     "Nazreen",
     "P.M."
    ],
    [
     "A.G.",
     "Ramakrishnan"
    ],
    [
     "Prasanta Kumar",
     "Ghosh"
    ]
   ],
   "title": "A Class-Specific Speech Enhancement for Phoneme Recognition: A Dictionary Learning Approach",
   "original": "0236",
   "page_count": 5,
   "order": 786,
   "p1": "3728",
   "pn": "3732",
   "abstract": [
    "We study the influence of using class-specific dictionaries for enhancement\nover class-independent dictionary in phoneme recognition of noisy speech.\nWe hypothesize that, using class-specific dictionaries would remove\nthe noise more compared to a class-independent dictionary, thereby\nresulting in better phoneme recognition. Experiments are performed\nwith speech data from TIMIT corpus and noise samples from NOISEX-92\ndatabase. Using KSVD, four types of dictionaries have been learned:\nclass-independent, manner-of-articulation-class, place-of-articulation-class\nand 39 phoneme-class. Initially, a set of labels are obtained by recognizing\nthe speech, enhanced using a class-independent dictionary. Using these\napproximate labels, the corresponding class-specific dictionaries are\nused to enhance each frame of the original noisy speech, and this enhanced\nspeech is then recognized. Compared to the results obtained using the\nclass-independent dictionary, the 39 phoneme-class based dictionaries\nprovide a relative phoneme recognition accuracy improvement of 5.5%,\n3.7%, 2.4% and 2.2%, respectively for factory2, m109, leopard and babble\nnoises, when averaged over 0, 5 and 10 dB SNRs.\n"
   ],
   "doi": "10.21437/Interspeech.2016-236"
  },
  "ogawa16_interspeech": {
   "authors": [
    [
     "Atsunori",
     "Ogawa"
    ],
    [
     "Shogo",
     "Seki"
    ],
    [
     "Keisuke",
     "Kinoshita"
    ],
    [
     "Marc",
     "Delcroix"
    ],
    [
     "Takuya",
     "Yoshioka"
    ],
    [
     "Tomohiro",
     "Nakatani"
    ],
    [
     "Kazuya",
     "Takeda"
    ]
   ],
   "title": "Robust Example Search Using Bottleneck Features for Example-Based Speech Enhancement",
   "original": "0671",
   "page_count": 5,
   "order": 787,
   "p1": "3733",
   "pn": "3737",
   "abstract": [
    "Example-based speech enhancement is a promising approach for coping\nwith highly non-stationary noise. Given a noisy speech input, it first\nsearches in noisy speech corpora for the noisy speech examples that\nbest match the input. Then, it concatenates the clean speech examples\nthat are paired with the matched noisy examples to obtain an estimate\nof the underlying clean speech component in the input. This framework\nworks well if the noisy speech corpora contain the noise included in\nthe input. However, it is impossible to prepare corpora that cover\nall types of noisy environments. Moreover, the example search is usually\nperformed using noise sensitive mel-frequency cepstral coefficient\nfeatures (MFCCs). Consequently, a mismatch between an input and the\ncorpora is inevitable. This paper proposes using bottleneck features\n(BNFs) extracted from a deep neural network (DNN) acoustic model for\nthe example search. Since BNFs have good noise robustness (invariance),\nthe mismatch is mitigated and thus a more accurate example search can\nbe performed. Experimental results on the Aurora4 corpus show that\nthe example-based approach using BNFs greatly improves the enhanced\nspeech quality compared with that using MFCCs. It also consistently\noutperforms a conventional DNN-based approach, i.e. a denoising autoencoder.\n"
   ],
   "doi": "10.21437/Interspeech.2016-671"
  },
  "kumar16c_interspeech": {
   "authors": [
    [
     "Anurag",
     "Kumar"
    ],
    [
     "Dinei",
     "Florencio"
    ]
   ],
   "title": "Speech Enhancement in Multiple-Noise Conditions Using Deep Neural Networks",
   "original": "0088",
   "page_count": 5,
   "order": 788,
   "p1": "3738",
   "pn": "3742",
   "abstract": [
    "In this paper we consider the problem of speech enhancement in real-world\nlike conditions where multiple noises can simultaneously corrupt speech.\nMost of the current literature on speech enhancement focus primarily\non presence of single noise in corrupted speech which is far from real-world\nenvironments. Specifically, we deal with improving speech quality in\noffice environment where multiple stationary as well as non-stationary\nnoises can be simultaneously present in speech. We propose several\nstrategies based on Deep Neural Networks (DNN) for speech enhancement\nin these scenarios. We also investigate a DNN training strategy based\non psychoacoustic models from speech coding for enhancement of noisy\nspeech.\n"
   ],
   "doi": "10.21437/Interspeech.2016-88"
  },
  "shivakumar16b_interspeech": {
   "authors": [
    [
     "Prashanth Gurunath",
     "Shivakumar"
    ],
    [
     "Panayiotis",
     "Georgiou"
    ]
   ],
   "title": "Perception Optimized Deep Denoising AutoEncoders for Speech Enhancement",
   "original": "1284",
   "page_count": 5,
   "order": 789,
   "p1": "3743",
   "pn": "3747",
   "abstract": [
    "Speech Enhancement is a challenging and important area of research\ndue to the many applications that depend on improved signal quality.\nIt is a pre-processing step of speech processing systems and used for\nperceptually improving quality of speech for humans. With recent advances\nin Deep Neural Networks (DNN), deep Denoising Auto-Encoders have proved\nto be very successful for speech enhancement. In this paper, we propose\na novel objective loss function, which takes into account the perceptual\nquality of speech. We use that to train Perceptually-Optimized Speech\nDenoising Auto-Encoders (POS-DAE). We demonstrate the effectiveness\nof POS-DAE in a speech enhancement task. Further we introduce a two\nlevel DNN architecture for denoising and enhancement. We show the effectiveness\nof the proposed methods for a high noise subset of the QUT-NOISE-TIMIT\ndatabase under mismatched noise conditions. Experiments are conducted\ncomparing the POS-DAE against the Mean Square Error loss function using\nspeech distortion, noise reduction and Perceptual Evaluation of Speech\nQuality. We find that the proposed loss function and the new 2-stage\narchitecture give significant improvements in perceptual speech quality\nmeasures and the improvements become more significant for higher noise\nconditions.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1284"
  },
  "kato16_interspeech": {
   "authors": [
    [
     "Akihiro",
     "Kato"
    ],
    [
     "Ben",
     "Milner"
    ]
   ],
   "title": "HMM-Based Speech Enhancement Using Sub-Word Models and Noise Adaptation",
   "original": "0928",
   "page_count": 5,
   "order": 790,
   "p1": "3748",
   "pn": "3752",
   "abstract": [
    "This work proposes a method of speech enhancement that uses a network\nof HMMs to first decode noisy speech and to then synthesise a set of\nfeatures that enables a clean speech signal to be reconstructed. Different\nchoices of acoustic model (whole-word, monophone and triphone) and\ngrammars (highly constrained to no constraints) are considered and\nthe effects of introducing or relaxing acoustic and grammar constraints\ninvestigated. For robust operation in noisy conditions it is necessary\nfor the HMMs to model noisy speech and consequently noise adaptation\nis investigated along with its effect on the reconstructed speech.\nSpeech quality and intelligibility analysis find triphone models with\nno grammar, combined with noise adaptation, gives highest performance\nthat outperforms conventional methods of enhancement at low signal-to-noise\nratios.\n"
   ],
   "doi": "10.21437/Interspeech.2016-928"
  },
  "li16n_interspeech": {
   "authors": [
    [
     "Li",
     "Li"
    ],
    [
     "Hirokazu",
     "Kameoka"
    ],
    [
     "Takuya",
     "Higuchi"
    ],
    [
     "Hiroshi",
     "Saruwatari"
    ]
   ],
   "title": "Semi-Supervised Joint Enhancement of Spectral and Cepstral Sequences of Noisy Speech",
   "original": "1286",
   "page_count": 5,
   "order": 791,
   "p1": "3753",
   "pn": "3757",
   "abstract": [
    "While spectral domain speech enhancement algorithms using non-negative\nmatrix factorization (NMF) are powerful in terms of signal recovery\naccuracy (e.g., signal-to-noise ratio), they do not necessarily lead\nto an improvement in the quality of the enhanced speech in the feature\ndomain. This implies that naively using these algorithms as front-end\nprocessing for e.g., speech recognition and speech conversion does\nnot always lead to satisfactory results. To address this problem, this\npaper proposes a novel method that aims to jointly enhance the spectral\nand cepstral sequences of noisy speech, by optimizing a combined objective\nfunction consisting of an NMF-based model-fitting criterion defined\nin the spectral domain and a Gaussian mixture model (GMM)-based probability\ndistribution defined in the cepstral domain. We derive a novel majorizer\nfor this objective function, which allows us to derive a convergence-guaranteed\niterative algorithm based on a majorization-minimization scheme for\nthe optimization. Experimental results revealed that the proposed method\noutperformed the conventional NMF approach in terms of both signal-to-distortion\nratio and cepstral distance.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1286"
  },
  "chinaev16_interspeech": {
   "authors": [
    [
     "Aleksej",
     "Chinaev"
    ],
    [
     "Reinhold",
     "Haeb-Umbach"
    ]
   ],
   "title": "A priori SNR Estimation Using a Generalized Decision Directed Approach",
   "original": "0474",
   "page_count": 5,
   "order": 792,
   "p1": "3758",
   "pn": "3762",
   "abstract": [
    "In this contribution we investigate  a priori signal-to-noise ratio\n(SNR) estimation, a crucial component of a single-channel speech enhancement\nsystem based on spectral subtraction. The majority of the state-of-the\nart  a priori SNR estimators work in the power spectral domain, which\nis, however, not confirmed to be the optimal domain for the estimation.\nMotivated by the generalized spectral subtraction rule, we show how\nthe estimation of the  a priori SNR can be formulated in the so called\ngeneralized SNR domain. This formulation allows to generalize the widely\nused decision directed (DD) approach. An experimental investigation\nwith different noise types reveals the superiority of the generalized\nDD approach over the conventional DD approach in terms of both the\nmean opinion score &#8212; listening quality objective measure and\nthe output global SNR in the medium to high input SNR regime, while\nwe show that the power spectrum is the optimal domain for low SNR.\nWe further develop a parameterization which adjusts the domain of estimation\nautomatically according to the estimated input global SNR.\n"
   ],
   "doi": "10.21437/Interspeech.2016-474"
  },
  "wang16i_interspeech": {
   "authors": [
    [
     "Ziteng",
     "Wang"
    ],
    [
     "Xu",
     "Li"
    ],
    [
     "Xiaofei",
     "Wang"
    ],
    [
     "Qiang",
     "Fu"
    ],
    [
     "Yonghong",
     "Yan"
    ]
   ],
   "title": "A DNN-HMM Approach to Non-Negative Matrix Factorization Based Speech Enhancement",
   "original": "0147",
   "page_count": 5,
   "order": 793,
   "p1": "3763",
   "pn": "3767",
   "abstract": [
    "General speaker-independent models have been used in non-negative matrix\nfactorization (NMF) based speech enhancement algorithms for the practical\napplicability. And additional regulation is necessary when choosing\nthe optimal models for speech reconstruction. In this paper, we propose\na novel utilization of deep neural network (DNN) to select the models\nused for separating speech from noise. Specifically, multiple local\ndictionaries are learned, whereas only one is activated for each block\nin the separation step. Besides, the temporal dependencies between\nblocks are represented by hidden Markov model (HMM), with which it\nturns out a hybrid DNN-HMM framework. The most probable activation\nsequence is then solved by the Viterbi algorithm. Experimental evaluations\nwhich focus on a speech denoising application are carried out. The\nresults confirm that our proposed approach achieves better performance\nwhen compared with some existing methods.\n"
   ],
   "doi": "10.21437/Interspeech.2016-147"
  },
  "fu16_interspeech": {
   "authors": [
    [
     "Szu-Wei",
     "Fu"
    ],
    [
     "Yu",
     "Tsao"
    ],
    [
     "Xugang",
     "Lu"
    ]
   ],
   "title": "SNR-Aware Convolutional Neural Network Modeling for Speech Enhancement",
   "original": "0211",
   "page_count": 5,
   "order": 794,
   "p1": "3768",
   "pn": "3772",
   "abstract": [
    "This paper proposes a signal-to-noise-ratio (SNR) aware convolutional\nneural network (CNN) model for speech enhancement (SE). Because the\nCNN model can deal with local temporal-spectral structures of speech\nsignals, it can effectively disentangle the speech and noise signals\ngiven the noisy speech signals. In order to enhance the generalization\ncapability and accuracy, we propose two SNR-aware algorithms for CNN\nmodeling. The first algorithm employs a multi-task learning (MTL) framework,\nin which restoring clean speech and estimating SNR level are formulated\nas the main and the secondary tasks, respectively, given the noisy\nspeech input. The second algorithm is an SNR adaptive denoising, in\nwhich the SNR level is explicitly predicted in the first step, and\nthen an SNR-dependent CNN model is selected for denoising. Experiments\nwere carried out to test the two SNR-aware algorithms for CNN modeling.\nResults demonstrate that CNN with the two proposed SNR-aware algorithms\noutperform the deep neural network counterpart in terms of standardized\nobjective evaluations when using the same number of layers and nodes.\nMoreover, the SNR-aware algorithms can improve the denoising performance\nwith unseen SNR levels, suggesting their promising generalization capability\nfor real-world applications.\n"
   ],
   "doi": "10.21437/Interspeech.2016-211"
  },
  "li16o_interspeech": {
   "authors": [
    [
     "Kehuang",
     "Li"
    ],
    [
     "Bo",
     "Wu"
    ],
    [
     "Chin-Hui",
     "Lee"
    ]
   ],
   "title": "An Iterative Phase Recovery Framework with Phase Mask for Spectral Mapping with an Application to Speech Enhancement",
   "original": "0494",
   "page_count": 5,
   "order": 795,
   "p1": "3773",
   "pn": "3777",
   "abstract": [
    "We propose an iterative phase recovery framework to improve spectral\nmapping with an application to improving the performance of state-of-the-art\nspeech enhancement systems using magnitude-based spectral mapping with\ndeep neural networks (DNNs). We further propose to use an estimated\ntime-frequency mask to reduce sign uncertainty in the overlap-add waveform\nreconstruction algorithm. In a series of enhancement experiments using\na DNN baseline system, by directly replacing the original phase of\nnoisy speech with the estimated phase obtained with a classical phase\nrecovery algorithm, the proposed iterative technique reduces the log-spectral\ndistortion (LSD) by 0.41 dB from the DNN baseline, and increases the\nperceptual evaluation speech quality (PESQ) by 0.05 over the DNN baseline,\naveraging over a wide range of signal and noise conditions. The proposed\nphase mask mechanism further increases the segmental signal-to-noise\nratio (SegSNR) by 0.44 dB at an expense of a slight degradation in\nLSD and PESQ comparing with the algorithm without using any phase mask.\n"
   ],
   "doi": "10.21437/Interspeech.2016-494"
  },
  "liu16k_interspeech": {
   "authors": [
    [
     "Bin",
     "Liu"
    ],
    [
     "Jianhua",
     "Tao"
    ]
   ],
   "title": "A Novel Research to Artificial Bandwidth Extension Based on Deep BLSTM Recurrent Neural Networks and Exemplar-Based Sparse Representation",
   "original": "0772",
   "page_count": 5,
   "order": 796,
   "p1": "3778",
   "pn": "3782",
   "abstract": [
    "This paper presents a two stages artificial bandwidth extension (ABE)\nframework which combine deep bidirectional Long Short Term Memory (BLSTM)\nrecurrent neural network with exemplar-based sparse representation\nto estimate missing frequency band. It demonstrates the suitability\nof proposed method for modeling log power spectra of speech signals\nin ABE. The BLSTM-RNN which can capture information from anywhere in\nthe feature sequence is used to estimate the log power spectra in the\nhigh-band firstly and the exemplar-based sparse representation which\ncould alleviate the over-smoothing problem is applied to generated\nlog power spectra in the second stage. In addition, rich acoustic features\nin the low-band are considered to reduce the reconstruction error.\nExperimental results demonstrate that the proposed framework can achieve\nsignificant improvements in both objective and subjective measures\nover the different baseline methods.\n"
   ],
   "doi": "10.21437/Interspeech.2016-772"
  },
  "mitra16d_interspeech": {
   "authors": [
    [
     "Vikramjit",
     "Mitra"
    ],
    [
     "Horacio",
     "Franco"
    ]
   ],
   "title": "Coping with Unseen Data Conditions: Investigating Neural Net Architectures, Robust Features, and Information Fusion for Robust Speech Recognition",
   "original": "0966",
   "page_count": 5,
   "order": 797,
   "p1": "3783",
   "pn": "3787",
   "abstract": [
    "The introduction of deep neural networks has significantly improved\nautomatic speech recognition performance. For real-world use, automatic\nspeech recognition systems must cope with varying background conditions\nand unseen acoustic data. This work investigates the performance of\ntraditional deep neural networks under varying acoustic conditions\nand evaluates their performance with speech recorded under realistic\nbackground conditions that are mismatched with respect to the training\ndata. We explore using robust acoustic features, articulatory features,\nand traditional baseline features against both in-domain microphone\nchannel-matched and channel-mismatched conditions as well as out-of-domain\ndata recorded using far- and near-microphone setups containing both\nbackground noise and reverberation distortions. We investigate feature-combination\ntechniques, both outside and inside the neural network, and explore\nneural-network-level combination at the output decision level. Results\nfrom this study indicate that robust features can significantly improve\ndeep neural network performance under mismatched, noisy conditions,\nand that using multiple features reduces speech recognition error rates.\nFurther, we observed that fusing multiple feature sets at the convolutional\nlayer feature-map level was more effective than performing fusion at\nthe input feature level or at the neural-network output decision level.\n"
   ],
   "doi": "10.21437/Interspeech.2016-966"
  },
  "tomashenko16_interspeech": {
   "authors": [
    [
     "Natalia",
     "Tomashenko"
    ],
    [
     "Yuri",
     "Khokhlov"
    ],
    [
     "Yannick",
     "Estève"
    ]
   ],
   "title": "On the Use of Gaussian Mixture Model Framework to Improve Speaker Adaptation of Deep Neural Network Acoustic Models",
   "original": "1230",
   "page_count": 5,
   "order": 798,
   "p1": "3788",
   "pn": "3792",
   "abstract": [
    "In this paper we investigate the Gaussian Mixture Model (GMM) framework\nfor adaptation of context-dependent deep neural network HMM (CD-DNN-HMM)\nacoustic models. In the previous work an initial attempt was introduced\nfor efficient transfer of adaptation algorithms from the GMM framework\nto DNN models. In this work we present an extension, further detailed\nexploration and analysis of the method with respect to state-of-the-art\nspeech recognition DNN setup and propose various novel ways for adaptation\nperformance improvement, such as, using bottleneck features for GMM-derived\nfeature extraction, combination of GMM-derived with conventional features\nat different levels of DNN architecture, moving from monophones to\ntriphones in the auxiliary GMM model in order to extend the number\nof adapted classes, and finally, using lattice-based information and\nconfidence scores in maximum a posteriori adaptation of the auxiliary\nGMM model. Experimental results on the TED-LIUM corpus show that the\nproposed adaptation technique can be effectively integrated into DNN\nsetup at different levels and provide additional gain in recognition\nperformance.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1230"
  },
  "bosch16b_interspeech": {
   "authors": [
    [
     "Louis ten",
     "Bosch"
    ],
    [
     "Bert",
     "Cranen"
    ],
    [
     "Yang",
     "Sun"
    ]
   ],
   "title": "Analytical Assessment of Dual-Stream Merging for Noise-Robust ASR",
   "original": "1050",
   "page_count": 5,
   "order": 799,
   "p1": "3793",
   "pn": "3797",
   "abstract": [
    "In previous studies (on Aurora2), it was found that merging a posteriori\nprobability streams from different classifiers (GMM, MLP, Sparse Coding)\ncan improve the noise robustness of ASR. Maximizing word accuracy required\nthe stream weights to be systematically dependent on the specific input\nstreams and SNR. The tuning of the weights, however, was largely a\nmatter of trial and error and typically involved a laborious grid search.\nIn this paper, we propose two fundamental, analytical methods to better\nunderstand these empirical findings. To that end, we maximize the trustworthiness\nof merged streams as function of the stream weights. Trustworthiness\nis defined as the probability that the winning state in a probability\nvector correctly predicts a golden reference state obtained by a forced\nalignment. Even though our approach is not directly equivalent to optimizing\nword accuracy, both methods appear highly useful to obtain insight\nin stream properties that determine the success of a given merge (or\nthe lack thereof). Furthermore, both methods clearly support the trends\nthat exist in the grid-search based empirical observations.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1050"
  },
  "loweimi16_interspeech": {
   "authors": [
    [
     "Erfan",
     "Loweimi"
    ],
    [
     "Jon",
     "Barker"
    ],
    [
     "Thomas",
     "Hain"
    ]
   ],
   "title": "Use of Generalised Nonlinearity in Vector Taylor Series Noise Compensation for Robust Speech Recognition",
   "original": "1028",
   "page_count": 5,
   "order": 800,
   "p1": "3798",
   "pn": "3802",
   "abstract": [
    "Designing good normalisation to counter the effect of environmental\ndistortions is one of the major challenges for automatic speech recognition\n(ASR). The Vector Taylor series (VTS) method is a powerful and mathematically\nwell principled technique that can be applied to both the feature and\nmodel domains to compensate for both additive and convolutional noises.\nOne of the limitations of this approach, however, is that it is tied\nto MFCC (and log-filterbank) features and does not extend to other\nrepresentations such as PLP, PNCC and phase-based front-ends that use\npower transformation rather than log compression. This paper aims at\nbroadening the scope of the VTS method by deriving a new formulation\nthat assumes a power transformation is used as the non-linearity during\nfeature extraction. It is shown that the conventional VTS, in the log\ndomain, is a special case of the new extended framework. In addition,\nthe new formulation introduces one more degree of freedom which makes\nit possible to tune the algorithm to better fit the data to the statistical\nrequirements of the ASR back-end. Compared with MFCC and conventional\nVTS, the proposed approach provides up to 12.2% and 2.0% absolute performance\nimprovements on average, in Aurora-4 tasks, respectively.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1028"
  },
  "mimura16_interspeech": {
   "authors": [
    [
     "Masato",
     "Mimura"
    ],
    [
     "Shinsuke",
     "Sakai"
    ],
    [
     "Tatsuya",
     "Kawahara"
    ]
   ],
   "title": "Joint Optimization of Denoising Autoencoder and DNN Acoustic Model Based on Multi-Target Learning for Noisy Speech Recognition",
   "original": "0388",
   "page_count": 5,
   "order": 801,
   "p1": "3803",
   "pn": "3807",
   "abstract": [
    "Denoising autoencoders (DAEs) have been investigated for enhancing\nnoisy speech before feeding it to the back-end deep neural network\n(DNN) acoustic model, but there may be a mismatch between the DAE output\nand the expected input of the back-end DNN, and also inconsistency\nbetween the training objective functions of the two networks. In this\npaper, a joint optimization method of the front-end DAE and the back-end\nDNN is proposed based on a multi-target learning scheme. In the first\nstep, the front-end DAE is trained with an additional target of minimizing\nthe errors propagated by the back-end DNN. Then, the unified network\nof DAE and DNN is fine-tuned for the phone state classification target,\nwith an extra target of input speech enhancement imposed to the DAE\npart. The proposed method has been evaluated with the CHiME3 ASR task,\nand demonstrated to improve the baseline DNN as well as the simple\ncoupling of DAE with DNN. The method is also effective as a post-filter\nof a beamformer.\n"
   ],
   "doi": "10.21437/Interspeech.2016-388"
  },
  "higuchi16_interspeech": {
   "authors": [
    [
     "Takuya",
     "Higuchi"
    ],
    [
     "Takuya",
     "Yoshioka"
    ],
    [
     "Tomohiro",
     "Nakatani"
    ]
   ],
   "title": "Optimization of Speech Enhancement Front-End with Speech Recognition-Level Criterion",
   "original": "0681",
   "page_count": 5,
   "order": 802,
   "p1": "3808",
   "pn": "3812",
   "abstract": [
    "This paper concerns the use of speech enhancement to improve automatic\nspeech recognition (ASR) performance in noisy environments. Speech\nenhancement systems are usually designed separately from a back-end\nrecognizer by optimizing the front-end parameters with signal-level\ncriteria. Such a disjoint processing approach is not always useful\nfor ASR. Indeed, time-frequency masking, which is widely used in the\nspeech enhancement community, sometimes degrades the ASR performance\nbecause of the artifacts created by masking. This paper proposes a\nspeech recognition-oriented front-end approach that optimizes the front-end\nparameters with an ASR-level criterion, where we use a complex Gaussian\nmixture model (CGMM) for mask estimation. First, the process of CGMM-based\ntime-frequency masking is reformulated as a computation network. By\nconnecting this CGMM network to the input layer of the acoustic model,\nthe CGMM parameters can be optimized for each test utterance by back\npropagation using an unsupervised acoustic model adaptation scheme.\nExperimental results show that the proposed method achieves a relative\nimprovement of 7.7% on the CHiME-3 evaluation set in terms of word\nerror rate.\n"
   ],
   "doi": "10.21437/Interspeech.2016-681"
  },
  "tran16_interspeech": {
   "authors": [
    [
     "Dung T.",
     "Tran"
    ],
    [
     "Marc",
     "Delroix"
    ],
    [
     "Atsunori",
     "Ogawa"
    ],
    [
     "Tomohiro",
     "Nakatani"
    ]
   ],
   "title": "Factorized Linear Input Network for Acoustic Model Adaptation in Noisy Conditions",
   "original": "0732",
   "page_count": 5,
   "order": 803,
   "p1": "3813",
   "pn": "3817",
   "abstract": [
    "Deep neural network (DNN) based acoustic models have obtained remarkable\nperformance for many speech recognition tasks. However, recognition\nperformance still remains too low in noisy conditions. To address this\nissue, a speech enhancement front-end is often used before recognition.\nSuch a front-end can reduce noise but there may remain a mismatch due\nto the difference in training and testing conditions and the imperfectness\nof the enhancement front-end. Acoustic model adaptation can be used\nto mitigate such a mismatch. In this paper, we investigate an extension\nof the linear input network (LIN) adaptation framework, where the feature\ntransformation is realized as a weighted combination of affine transforms\nof the enhanced input features. The weights are derived from a vector\ncharacterizing the noise conditions. We tested our approach on the\nreal data set of CHiME3 challenge task, confirming the effectiveness\nof our approach.\n"
   ],
   "doi": "10.21437/Interspeech.2016-732"
  },
  "fujita16b_interspeech": {
   "authors": [
    [
     "Yusuke",
     "Fujita"
    ],
    [
     "Ryoich",
     "Takashima"
    ],
    [
     "Takeshi",
     "Homma"
    ],
    [
     "Masahito",
     "Togami"
    ]
   ],
   "title": "Data Augmentation Using Multi-Input Multi-Output Source Separation for Deep Neural Network Based Acoustic Modeling",
   "original": "0733",
   "page_count": 5,
   "order": 804,
   "p1": "3818",
   "pn": "3822",
   "abstract": [
    "We investigate the use of local Gaussian modeling (LGM) based source\nseparation to improve speech recognition accuracy. Previous studies\nhave shown that the LGM based source separation technique has been\nsuccessfully applied to the runtime speech enhancement and the speech\nenhancement of training data for deep neural network (DNN) based acoustic\nmodeling. In this paper, we propose a data augmentation method utilizing\nthe multi-input multi-output (MIMO) characteristic of LGM based source\nseparation. We first investigate the difference between unprocessed\nmulti-microphone signals and multi-channel output signals from LGM\nbased source separation as augmented training data for DNN based acoustic\nmodeling. Experimental results using the third CHiME challenge dataset\nshow that the proposed data augmentation outperforms the conventional\ndata augmentation. In addition, we experiment the beamforming applied\nto the source separated signals as runtime speech enhancement. The\nresults show that the proposed runtime beamforming further improves\nthe speech recognition accuracy.\n"
   ],
   "doi": "10.21437/Interspeech.2016-733"
  },
  "prasad16_interspeech": {
   "authors": [
    [
     "Animesh",
     "Prasad"
    ],
    [
     "Khe Chai",
     "Sim"
    ]
   ],
   "title": "Microphone Distance Adaptation Using Cluster Adaptive Training for Robust Far Field Speech Recognition",
   "original": "0738",
   "page_count": 5,
   "order": 805,
   "p1": "3823",
   "pn": "3827",
   "abstract": [
    "Microphone distance adaptation is an important and challenging problem\nfor far field speech recognition using a single distant microphone.\nThis paper investigates the use of Cluster Adaptive Training (CAT)\nto learn a structured Deep Neural Network (DNN) that can be quickly\nadapted to cope with changes in the distance between the microphone\nand speaker at test time. A speech corpus was created by re-recording\nthe Wall Street Journal (WSJ0) audio using far-field microphones with\n8 different distances from the source. Experimental results show that\nunsupervised adaptation of the CAT-DNN model achieved up to 0.9% absolute\nword error rate reduction compared to the canonical model trained on\nmulti-style data.\n"
   ],
   "doi": "10.21437/Interspeech.2016-738"
  },
  "dimitriadis16_interspeech": {
   "authors": [
    [
     "Dimitrios",
     "Dimitriadis"
    ],
    [
     "Samuel",
     "Thomas"
    ],
    [
     "Sriram",
     "Ganapathy"
    ]
   ],
   "title": "An Investigation on the Use of i-Vectors for Robust ASR",
   "original": "1482",
   "page_count": 5,
   "order": 806,
   "p1": "3828",
   "pn": "3832",
   "abstract": [
    "In this paper we propose two different i-vector representations that\nimprove the noise robustness of automatic speech recognition (ASR).\nThe first kind of i-vectors is derived from &#8220;noise only&#8221;\ncomponents of speech provided by an adaptive denoising algorithm, the\nsecond variant is extracted from mel filterbank energies containing\nboth speech and noise. The effectiveness of both these representations\nis shown by combining them with two different kinds of spectral features\n&#8212; the commonly used log-mel filterbank energies and Teager energy\nspectral coefficients (TESCs). Using two different DNN architectures\nfor acoustic modeling &#8212; a standard state-of-the-art sigmoid-based\nDNN and an advanced architecture using leaky ReLUs, dropout and rescaling,\nwe demonstrate the benefit of the proposed representations. On the\nAurora-4 multi-condition training task the proposed front-end improves\nASR performance by 4%.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1482"
  },
  "liu16l_interspeech": {
   "authors": [
    [
     "Yulan",
     "Liu"
    ],
    [
     "Charles",
     "Fox"
    ],
    [
     "Madina",
     "Hasan"
    ],
    [
     "Thomas",
     "Hain"
    ]
   ],
   "title": "The Sheffield Wargame Corpus &#8212; Day Two and Day Three",
   "original": "0098",
   "page_count": 5,
   "order": 807,
   "p1": "3833",
   "pn": "3837",
   "abstract": [
    "Improving the performance of distant speech recognition is of considerable\ncurrent interest, driven by a desire to bring speech recognition into\npeople&#8217;s homes. Standard approaches to this task aim to enhance\nthe signal prior to recognition, typically using beamforming techniques\non multiple channels. Only few real-world recordings are available\nthat allow experimentation with such techniques. This has become even\nmore pertinent with recent works with deep neural networks aiming to\nlearn beamforming from data. Such approaches require large multi-channel\ntraining sets, ideally with location annotation for moving speakers,\nwhich is scarce in existing corpora. This paper presents a freely available\nand new extended corpus of English speech recordings in a natural setting,\nwith moving speakers. The data is recorded with diverse microphone\narrays, and uniquely, with ground truth location tracking. It extends\nthe 8.0 hour Sheffield Wargames Corpus released in Interspeech 2013,\nwith a further 16.6 hours of fully annotated data, including 6.1 hours\nof female speech to improve gender bias. Additional blog-based language\nmodel data is provided alongside, as well as a Kaldi baseline system.\nResults are reported with a standard Kaldi configuration, and a baseline\nmeeting recognition system.\n"
   ],
   "doi": "10.21437/Interspeech.2016-98"
  },
  "kim16d_interspeech": {
   "authors": [
    [
     "Suyoun",
     "Kim"
    ],
    [
     "Ian",
     "Lane"
    ]
   ],
   "title": "Recurrent Models for Auditory Attention in Multi-Microphone Distant Speech Recognition",
   "original": "0326",
   "page_count": 5,
   "order": 808,
   "p1": "3838",
   "pn": "3842",
   "abstract": [
    "Integration of multiple microphone data is one of the key ways to achieve\nrobust speech recognition in noisy environments or when the speaker\nis located at some distance from the input device. Signal processing\ntechniques such as beamforming are widely used to extract a speech\nsignal of interest from background noise. These techniques, however,\nare highly dependent on prior spatial information about the microphones\nand the environment in which the system is being used. In this work,\nwe present a neural attention network that directly combines multi-channel\naudio to generate phonetic states without requiring any prior knowledge\nof the microphone layout or any explicit signal preprocessing for speech\nenhancement. We embed an attention mechanism within a Recurrent Neural\nNetwork based acoustic model to automatically tune its attention to\na more reliable input source. Unlike traditional multi-channel preprocessing,\nour system can be optimized towards the desired output in one step.\nAlthough attention-based models have recently achieved impressive results\non sequence-to-sequence learning, no attention mechanisms have previously\nbeen applied to learn potentially asynchronous and non-stationary multiple\ninputs. We evaluate our neural attention model on the CHiME-3 task,\nand show that the model achieves comparable performance to beamforming\nusing a purely data-driven method.\n"
   ],
   "doi": "10.21437/Interspeech.2016-326"
  },
  "lee16f_interspeech": {
   "authors": [
    [
     "Wonkyum",
     "Lee"
    ],
    [
     "Kyu J.",
     "Han"
    ],
    [
     "Ian",
     "Lane"
    ]
   ],
   "title": "Semi-Supervised Speaker Adaptation for In-Vehicle Speech Recognition with Deep Neural Networks",
   "original": "1625",
   "page_count": 5,
   "order": 809,
   "p1": "3843",
   "pn": "3847",
   "abstract": [
    "In this paper, we present a new i-vector based speaker adaptation method\nfor automatic speech recognition with deep neural networks, focusing\non in-vehicle scenarios. Our proposed method is, rather than augmenting\ni-vectors to acoustic feature vectors to form concatenated input vectors\nfor adapting neural network acoustic model parameters, is to perform\nfeature-space transformation with smaller  transformation neural networks\ndedicated to acoustic feature vectors and i-vectors, respectively,\nfollowed by a layer of  linear combination of the network outputs.\nThis feature-space transformation is learned via semi-supervised learning\nwithout any parameter change in the original deep neural network acoustic\nmodel. Experimental results show that our proposed method achieves\n18.3% relative improvement in terms of word error rate compared to\nthe speaker independent performance, and verify that it has a potential\nto replace well-known feature-space Maximum Likelihood Linear Regression\n(fMLLR) in in-vehicle speech recognition with deep neural networks.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1625"
  },
  "huang16e_interspeech": {
   "authors": [
    [
     "Yan",
     "Huang"
    ],
    [
     "Yongqiang",
     "Wang"
    ],
    [
     "Yifan",
     "Gong"
    ]
   ],
   "title": "Semi-Supervised Training in Deep Learning Acoustic Model",
   "original": "1596",
   "page_count": 5,
   "order": 810,
   "p1": "3848",
   "pn": "3852",
   "abstract": [
    "We studied semi-supervised training in a fully connected deep neural\nnetwork (DNN), unfolded recurrent neural network (RNN), and long short-term\nmemory recurrent neural network (LSTM-RNN) with respect to transcription\nquality, importance data sampling, and training data amount. We found\nthat DNN, unfolded RNN, and LSTM-RNN exhibit increased sensitivity\nto labeling errors. One point relative WER increase in the training\ntranscription translates to  a half point WER increase in DNN and slightly\nmore in unfolded RNN; while in LSTM-RNN it translates to  one full\npoint WER increase. LSTM-RNN is notably more sensitive to transcription\nerrors. We further found that the importance sampling has similar impact\non all three models. In supervised training, importance sampling yields\n2&#126;3% relative WER reduction against random sampling. The gain\nis reduced in semi-supervised training. Lastly, we compared the model\ncapacity with increased training data. Experimental results suggest\nthat LSTM-RNN can benefit more from enlarged training data comparing\nto unfolded RNN and DNN.<br style=\"mso-data-placement:same-cell;\" /><br\nstyle=\"mso-data-placement:same-cell;\" />\n  We trained a semi-supervised\nLSTM-RNN using 2600 hours of transcribed and 10000 hours of untranscribed\ndata on a mobile speech task. The semi-supervised LSTM-RNN yields 6.56%\nrelative WER reduction against the supervised baseline trained from\n2600 hours of transcribed speech.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1596"
  },
  "thomas16_interspeech": {
   "authors": [
    [
     "Samuel",
     "Thomas"
    ],
    [
     "Kartik",
     "Audhkhasi"
    ],
    [
     "Jia",
     "Cui"
    ],
    [
     "Brian",
     "Kingsbury"
    ],
    [
     "Bhuvana",
     "Ramabhadran"
    ]
   ],
   "title": "Multilingual Data Selection for Low Resource Speech Recognition",
   "original": "0598",
   "page_count": 5,
   "order": 811,
   "p1": "3853",
   "pn": "3857",
   "abstract": [
    "Feature representations extracted from deep neural network-based multilingual\nfrontends provide significant improvements to speech recognition systems\nin low resource settings. To effectively train these frontends, we\nintroduce a data selection technique that discovers language groups\nfrom an available set of training languages. This data selection method\nreduces the required amount of training data and training time by approximately\n40%, with minimal performance degradation. We present speech recognition\nresults on 7 very limited language pack (VLLP) languages from the second\noption period of the IARPA Babel program using multilingual features\ntrained on up to 10 languages. The proposed multilingual features provide\nup to 15% relative improvement over baseline acoustic features on the\nVLLP languages.\n"
   ],
   "doi": "10.21437/Interspeech.2016-598"
  },
  "das16c_interspeech": {
   "authors": [
    [
     "Amit",
     "Das"
    ],
    [
     "Mark",
     "Hasegawa-Johnson"
    ]
   ],
   "title": "An Investigation on Training Deep Neural Networks Using Probabilistic Transcriptions",
   "original": "0655",
   "page_count": 5,
   "order": 812,
   "p1": "3858",
   "pn": "3862",
   "abstract": [
    "In this study, a transfer learning technique is presented for cross-lingual\nspeech recognition in an adverse scenario where there are no natively\ntranscribed transcriptions in the target language. The transcriptions\nthat are available during training are transcribed by crowd workers\nwho neither speak nor have any familiarity with the target language.\nHence, such transcriptions are likely to be inaccurate. Training a\ndeep neural network (DNN) in such a scenario is challenging; previously\nreported results have described DNN error rates exceeding the error\nrate of an adapted Gaussian Mixture Model (GMM). This paper investigates\nmulti-task learning techniques using deep neural networks which are\nsuitable for this scenario. We report, for the first time, absolute\nimprovement in phone error rates (PER) in the range 1.3&#8211;6.2%\nover GMMs adapted to probabilistic transcriptions. Results are reported\nfor Swahili, Hungarian, and Mandarin.\n"
   ],
   "doi": "10.21437/Interspeech.2016-655"
  },
  "do16c_interspeech": {
   "authors": [
    [
     "Van Hai",
     "Do"
    ],
    [
     "Nancy F.",
     "Chen"
    ],
    [
     "Boon Pang",
     "Lim"
    ],
    [
     "Mark",
     "Hasegawa-Johnson"
    ]
   ],
   "title": "Analysis of Mismatched Transcriptions Generated by Humans and Machines for Under-Resourced Languages",
   "original": "0736",
   "page_count": 5,
   "order": 813,
   "p1": "3863",
   "pn": "3867",
   "abstract": [
    "When speech data with native transcriptions are scarce in an under-resourced\nlanguage, automatic speech recognition (ASR) must be trained using\nother methods. Semi-supervised learning first labels the speech using\nASR from other languages, then re-trains the ASR using the generated\nlabels. Mismatched crowdsourcing asks crowd-workers unfamiliar with\nthe language to transcribe it. In this paper, self-training and mismatched\ncrowdsourcing are compared under exactly matched conditions. Specifically,\nspeech data of the target language are decoded by the source language\nASR systems into source language phone/word sequences. We find that\n(1) human mismatched crowdsourcing and cross-lingual ASR have similar\nerror patterns, but different specific errors. (2) These two sources\nof information can be usefully combined in order to train a better\ntarget-language ASR. (3) The differences between the error patterns\nof non-native human listeners and non-native ASR are small, but when\ndifferences are observed, they provide information about the relationship\nbetween the phoneme systems of the annotator/source language (Mandarin)\nand the target language (Vietnamese).\n"
   ],
   "doi": "10.21437/Interspeech.2016-736"
  },
  "nouza16_interspeech": {
   "authors": [
    [
     "Jan",
     "Nouza"
    ],
    [
     "Radek",
     "Safarik"
    ],
    [
     "Petr",
     "Cerva"
    ]
   ],
   "title": "ASR for South Slavic Languages Developed in Almost Automated Way",
   "original": "0747",
   "page_count": 5,
   "order": 814,
   "p1": "3868",
   "pn": "3872",
   "abstract": [
    "Slavic languages pose several specific challenges that need to be addressed\nin an ASR system design. Since we have already built an engine suited\nfor highly-inflected languages, we focus on adopting it for new languages,\nnow. In this case, we present an efficient way to adapt the system\nto all (seven) South Slavic languages, using methods and tools that\nbenefit from language similarities, easily adjustable G2P rules or\ncommon phonetic subsets. We show that it is possible to build accurate\nlanguage and acoustic models in an almost automated way, entirely from\nresources found on the web. The AMs are trained via cross-lingual bootstrapping\nfollowed by lightly supervised retraining from public data, like broadcast\nand parliament archives. Tests done on a set of main broadcast news\nin each language show WER values in range 16.8 to 21.5%, which includes\nalso errors caused by OOL (out-of-language) utterances often occurring\nin this type of spoken programs.\n"
   ],
   "doi": "10.21437/Interspeech.2016-747"
  },
  "razavi16_interspeech": {
   "authors": [
    [
     "Marzieh",
     "Razavi"
    ],
    [
     "Mathew",
     "Magimai-Doss"
    ]
   ],
   "title": "Improving Under-Resourced Language ASR Through Latent Subword Unit Space Discovery",
   "original": "1010",
   "page_count": 5,
   "order": 815,
   "p1": "3873",
   "pn": "3877",
   "abstract": [
    "Development of state-of-the-art automatic speech recognition (ASR)\nsystems requires acoustic resources (i.e., transcribed speech) as well\nas lexical resources (i.e., phonetic lexicons). It has been shown that\nacoustic and lexical resource constraints can be overcome by first\ntraining an acoustic model that captures acoustic-to-multilingual phone\nrelationships on language-independent data; and then training a lexical\nmodel that captures grapheme-to-multilingual phone relationships on\nthe target language data. In this paper, we show that such an approach\ncan be employed to discover a latent space of subword units for under-resourced\nlanguages, and subsequently improve the performance of the ASR system\nthrough both acoustic and lexical model adaptation. Specifically, we\npresent two approaches to discover the latent space: (1) inference\nof a subset of the multilingual phone set based on the learned grapheme-to-multilingual\nphone relationships, and (2) derivation of automatic subword unit space\nbased on clustering of the grapheme-to-multilingual phone relationships.\nExperimental studies on Scottish Gaelic, a truly under-resourced language,\nshow that both approaches lead to significant performance improvements,\nwith the latter approach yielding the best system.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1010"
  },
  "muller16_interspeech": {
   "authors": [
    [
     "Markus",
     "Müller"
    ],
    [
     "Sebastian",
     "Stüker"
    ],
    [
     "Alex",
     "Waibel"
    ]
   ],
   "title": "Language Adaptive DNNs for Improved Low Resource Speech Recognition",
   "original": "1143",
   "page_count": 5,
   "order": 816,
   "p1": "3878",
   "pn": "3882",
   "abstract": [
    "Deep Neural Network (DNN) acoustic models are commonly used in today&#8217;s\nstate-of-the-art speech recognition systems. As neural networks are\na data driven method, the amount of available training data directly\nimpacts the performance. In the past, several studies have shown that\nmultilingual training of DNNs leads to improvements, especially in\nresource constrained tasks in which only limited training data in the\ntarget language is available.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  Previous studies have\nshown speaker adaptation to be successfully performed on DNNs. This\nis achieved by adding speaker information (e.g. i-Vectors) as additional\ninput features. Based on the idea of adding additional features, we\nhere present a method for adding language information to the input\nfeatures of the network. Preliminary experiments have shown improvements\nby providing supervised information about language identity to the\nnetwork.<br style=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  In this work, we extended this approach by training a neural network\nto encode language specific features. We extracted those features unsupervised\nand used them to provide additional cues to the DNN acoustic model\nduring training. Our results show that augmenting acoustic input features\nwith this language code enabled the network to better capture language\nspecific peculiarities. This improved the performance of systems trained\nusing data from multiple languages.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1143"
  },
  "alumae16_interspeech": {
   "authors": [
    [
     "Tanel",
     "Alumäe"
    ],
    [
     "Stavros",
     "Tsakalidis"
    ],
    [
     "Richard",
     "Schwartz"
    ]
   ],
   "title": "Improved Multilingual Training of Stacked Neural Network Acoustic Models for Low Resource Languages",
   "original": "1426",
   "page_count": 5,
   "order": 817,
   "p1": "3883",
   "pn": "3887",
   "abstract": [
    "This paper proposes several improvements to multilingual training of\nneural network acoustic models for speech recognition and keyword spotting\nin the context of low-resource languages. We concentrate on the stacked\narchitecture where the first network is used as a bottleneck feature\nextractor and the second network as the acoustic model. We propose\nto improve multilingual training when the amount of data from different\nlanguages is very different by applying balancing scalers to the training\nexamples. We also explore how to exploit multilingual data to train\nthe second neural network of the stacked architecture. An ensemble\ntraining method that can take advantage of both unsupervised pretraining\nas well as multilingual training is found to give the best speech recognition\nperformance across a wide variety of languages, while system combination\nof differently trained multilingual models results in further improvements\nin keyword search performance.\n"
   ],
   "doi": "10.21437/Interspeech.2016-1426"
  }
 },
 "sessions": [
  {
   "title": "Keynote 1: ISCA Medalist: John Makhoul",
   "papers": [
    "makhoul16_interspeech"
   ]
  },
  {
   "title": "Neural Networks in Speech Recognition",
   "papers": [
    "medennikov16_interspeech",
    "saon16_interspeech",
    "lu16_interspeech",
    "yu16_interspeech",
    "pundak16_interspeech",
    "kurata16_interspeech"
   ]
  },
  {
   "title": "Special Session: Auditory-Visual Expressive Speech and Gesture in Humans and Machines",
   "papers": [
    "chen16_interspeech",
    "chong16_interspeech",
    "yang16_interspeech",
    "ganesh16_interspeech",
    "sadoughi16_interspeech",
    "kim16_interspeech",
    "kim16b_interspeech"
   ]
  },
  {
   "title": "Prosody",
   "papers": [
    "vogel16_interspeech",
    "jugler16_interspeech",
    "ling16_interspeech",
    "ryant16_interspeech",
    "athanasopoulou16_interspeech",
    "lai16_interspeech"
   ]
  },
  {
   "title": "Speech and Language Processing for Clinical Health Applications",
   "papers": [
    "tsai16_interspeech",
    "lee16_interspeech",
    "sluis16_interspeech",
    "gosztolya16_interspeech",
    "gong16_interspeech",
    "cm16_interspeech"
   ]
  },
  {
   "title": "Speech Coding and Audio Processing for Noise Reduction",
   "papers": [
    "guan16_interspeech",
    "zorila16_interspeech",
    "sharma16_interspeech",
    "wang16_interspeech",
    "liu16_interspeech",
    "petkov16_interspeech"
   ]
  },
  {
   "title": "Speech Analysis",
   "papers": [
    "rouas16_interspeech",
    "bhavsar16_interspeech",
    "dumpala16_interspeech",
    "toger16_interspeech",
    "cernak16_interspeech",
    "shahin16_interspeech"
   ]
  },
  {
   "title": "First and Second Language Acquisition",
   "papers": [
    "chen16b_interspeech",
    "wen16_interspeech",
    "katayama16_interspeech",
    "wottawa16_interspeech",
    "luo16_interspeech",
    "shinohara16_interspeech",
    "harper16_interspeech"
   ]
  },
  {
   "title": "Speech and Hearing Disorders &amp; Perception",
   "papers": [
    "hennequin16_interspeech",
    "ylmaz16_interspeech",
    "laaridh16_interspeech",
    "bhat16_interspeech",
    "chen16c_interspeech",
    "nagle16_interspeech",
    "najnin16_interspeech",
    "rong16_interspeech",
    "delvaux16_interspeech",
    "feng16_interspeech",
    "zhu16_interspeech",
    "ochi16_interspeech",
    "shao16_interspeech",
    "takashima16_interspeech",
    "gu16_interspeech"
   ]
  },
  {
   "title": "Speech Synthesis Poster",
   "papers": [
    "xie16_interspeech",
    "aihara16_interspeech",
    "gu16b_interspeech",
    "yang16b_interspeech",
    "hosaka16_interspeech",
    "aryal16_interspeech",
    "sarfjoo16_interspeech",
    "sun16_interspeech",
    "prakash16_interspeech",
    "zhang16_interspeech",
    "ijima16_interspeech",
    "yoshimura16_interspeech",
    "podsiado16_interspeech",
    "valentinibotinhao16_interspeech",
    "cooper16_interspeech"
   ]
  },
  {
   "title": "Topics in Speech Processing",
   "papers": [
    "tao16_interspeech",
    "ghahabi16_interspeech",
    "woubie16_interspeech"
   ]
  },
  {
   "title": "Show &amp; Tell Session 1",
   "papers": [
    "lawson16_interspeech",
    "smidl16_interspeech",
    "gauthier16_interspeech",
    "gruber16_interspeech"
   ]
  },
  {
   "title": "New Trends in Neural Networks for Speech Recognition",
   "papers": [
    "lu16b_interspeech",
    "nussbaumthom16_interspeech",
    "hsu16_interspeech",
    "wu16_interspeech",
    "badino16_interspeech",
    "zhang16b_interspeech"
   ]
  },
  {
   "title": "Special Session: The RedDots Challenge: Towards Characterizing Speakers from Short Utterances",
   "papers": [
    "wang16b_interspeech",
    "alam16_interspeech",
    "sarkar16_interspeech",
    "kinnunen16_interspeech",
    "ma16_interspeech",
    "zeinali16_interspeech",
    "das16_interspeech"
   ]
  },
  {
   "title": "Articulatory Measurements and Analysis",
   "papers": [
    "uchida16_interspeech",
    "sivaraman16_interspeech",
    "lammert16_interspeech",
    "sorensen16_interspeech",
    "labrunie16_interspeech",
    "lingala16_interspeech"
   ]
  },
  {
   "title": "Automatic Assessment of Emotions",
   "papers": [
    "xia16_interspeech",
    "stasak16_interspeech",
    "lotfian16_interspeech",
    "schmitt16_interspeech",
    "chorianopoulou16_interspeech",
    "gupta16_interspeech"
   ]
  },
  {
   "title": "Acoustic and Articulatory Phonetics",
   "papers": [
    "wodarczak16_interspeech",
    "kaland16_interspeech",
    "putzer16_interspeech",
    "strombergsson16_interspeech",
    "he16_interspeech",
    "enzinger16_interspeech"
   ]
  },
  {
   "title": "Source Separation and Spatial Audio",
   "papers": [
    "qi16_interspeech",
    "isik16_interspeech",
    "li16_interspeech",
    "delfarah16_interspeech",
    "hsu16b_interspeech",
    "gang16_interspeech"
   ]
  },
  {
   "title": "Special Session: Auditory-Visual Expressive Speech and Gesture in Humans and Machines",
   "papers": [
    "jin16_interspeech",
    "heckmann16_interspeech",
    "ouni16_interspeech",
    "barbulescu16_interspeech",
    "huang16_interspeech",
    "chaspari16_interspeech",
    "kasisopa16_interspeech",
    "khaki16_interspeech"
   ]
  },
  {
   "title": "Special Session: Intelligibility Under the Microscope",
   "papers": [
    "schadler16_interspeech",
    "exter16_interspeech",
    "toth16_interspeech",
    "karbasi16_interspeech",
    "toth16b_interspeech",
    "eichenauer16_interspeech",
    "lecumberri16_interspeech",
    "varnet16_interspeech",
    "fontan16_interspeech",
    "matsui16_interspeech",
    "mandel16_interspeech"
   ]
  },
  {
   "title": "Spoken Documents, Spoken Understanding and Semantic Analysis",
   "papers": [
    "rondeau16_interspeech",
    "liu16b_interspeech",
    "sheikh16_interspeech",
    "trione16_interspeech",
    "liu16c_interspeech",
    "jaech16_interspeech",
    "ladhak16_interspeech",
    "kesiraju16_interspeech",
    "zhao16_interspeech",
    "morchid16_interspeech",
    "hakkanitur16_interspeech",
    "janod16_interspeech",
    "kurata16b_interspeech",
    "stehwien16_interspeech",
    "tang16_interspeech"
   ]
  },
  {
   "title": "Spoken Term Detection",
   "papers": [
    "oishi16_interspeech",
    "lv16_interspeech",
    "proenca16_interspeech",
    "lee16b_interspeech",
    "panchapagesan16_interspeech",
    "chung16_interspeech",
    "meng16_interspeech",
    "gorin16_interspeech"
   ]
  },
  {
   "title": "Show &amp; Tell Session 2",
   "papers": [
    "verwimp16_interspeech",
    "stanislav16_interspeech",
    "karhila16_interspeech",
    "damnati16_interspeech"
   ]
  },
  {
   "title": "Feature Extraction and Acoustic Modeling Using Neural Networks for ASR",
   "papers": [
    "yuan16_interspeech",
    "liu16d_interspeech",
    "abraham16_interspeech",
    "nagamine16_interspeech",
    "variani16_interspeech",
    "sainath16_interspeech"
   ]
  },
  {
   "title": "Special Session: The Speakers in the Wild (SITW) Speaker Recognition Challenge",
   "papers": [
    "mclaren16_interspeech",
    "mclaren16b_interspeech",
    "novotny16_interspeech",
    "kudashev16_interspeech",
    "ghaemmaghami16_interspeech",
    "khosravani16_interspeech",
    "kheder16_interspeech",
    "liu16e_interspeech"
   ]
  },
  {
   "title": "Non-Native Speech Perception",
   "papers": [
    "scharenborg16_interspeech",
    "scharenborg16b_interspeech",
    "cooke16_interspeech",
    "zhang16c_interspeech",
    "massaro16_interspeech",
    "gong16b_interspeech"
   ]
  },
  {
   "title": "Behavioral Signal Processing and Speaker State and Traits Analytics",
   "papers": [
    "bassiou16_interspeech",
    "nasir16_interspeech",
    "tseng16_interspeech",
    "gallardo16_interspeech",
    "xiao16_interspeech",
    "dang16_interspeech"
   ]
  },
  {
   "title": "Spoken Term Detection",
   "papers": [
    "ram16_interspeech",
    "chen16d_interspeech",
    "torbati16_interspeech",
    "pham16_interspeech",
    "zhuang16_interspeech",
    "wu16b_interspeech"
   ]
  },
  {
   "title": "Co-Inference of Production and Acoustics",
   "papers": [
    "godoy16_interspeech",
    "tobing16_interspeech",
    "dissen16_interspeech",
    "vaz16_interspeech",
    "juvela16_interspeech",
    "wang16c_interspeech"
   ]
  },
  {
   "title": "Acoustic and Articulatory Phonetics",
   "papers": [
    "hu16_interspeech",
    "hu16b_interspeech",
    "cernak16b_interspeech",
    "xia16b_interspeech",
    "turco16_interspeech",
    "galata16_interspeech",
    "chang16_interspeech",
    "pan16_interspeech"
   ]
  },
  {
   "title": "Prosody, Phonation and Voice Quality",
   "papers": [
    "fenwick16_interspeech",
    "zellers16_interspeech",
    "themistocleous16_interspeech",
    "sun16b_interspeech",
    "lai16b_interspeech",
    "airaksinen16_interspeech",
    "park16_interspeech",
    "kalita16_interspeech",
    "garellek16_interspeech",
    "eriksson16_interspeech",
    "schweitzer16_interspeech",
    "vijayan16_interspeech",
    "kakouros16_interspeech",
    "kallay16_interspeech"
   ]
  },
  {
   "title": "Speech Production Analysis and Modeling",
   "papers": [
    "blaylock16_interspeech",
    "son16_interspeech",
    "cohen16_interspeech",
    "arai16_interspeech",
    "fang16_interspeech",
    "tiainen16_interspeech",
    "najnin16b_interspeech",
    "meyer16_interspeech",
    "voigt16_interspeech",
    "graham16_interspeech",
    "geneid16_interspeech",
    "hejna16_interspeech",
    "takahashi16_interspeech"
   ]
  },
  {
   "title": "Spoken Dialogue Systems",
   "papers": [
    "liu16f_interspeech",
    "asri16_interspeech",
    "georgiladakis16_interspeech",
    "khan16_interspeech",
    "levitan16_interspeech",
    "silvervarg16_interspeech"
   ]
  },
  {
   "title": "Show &amp; Tell Session 3",
   "papers": [
    "fung16_interspeech",
    "tejedorgarcia16_interspeech",
    "kawahara16_interspeech",
    "marchi16_interspeech"
   ]
  },
  {
   "title": "Special Event: Mindfulness",
   "papers": [
    "mirghafori16_interspeech"
   ]
  },
  {
   "title": "Keynote 2: Edward Chang",
   "papers": [
    "chang16b_interspeech"
   ]
  },
  {
   "title": "Special Event: Speaker Comparison for Forensic and Investigative Applications II",
   "papers": [
    "bonastre16_interspeech"
   ]
  },
  {
   "title": "Special Session: Clinical and Neuroscience-Inspired Vocal Biomarkers of Neurological and Psychiatric Disorders",
   "papers": [
    "bone16_interspeech",
    "hemmerling16_interspeech",
    "wang16d_interspeech",
    "ciccarelli16_interspeech",
    "horwitzmartin16_interspeech",
    "ringeval16_interspeech",
    "khorram16_interspeech",
    "mirheidari16_interspeech"
   ]
  },
  {
   "title": "Special Session: Singing Synthesis Challenge: Fill-In the Gap",
   "papers": [
    "chan16_interspeech",
    "bonada16_interspeech",
    "perrotin16_interspeech",
    "blanco16_interspeech",
    "feugere16_interspeech",
    "ardaillon16_interspeech",
    "cotescu16_interspeech"
   ]
  },
  {
   "title": "Conversation and Interaction",
   "papers": [
    "hilton16_interspeech",
    "gravano16_interspeech",
    "perez16_interspeech",
    "wodarczak16b_interspeech",
    "rennie16_interspeech",
    "sherrziarko16_interspeech"
   ]
  },
  {
   "title": "Automatic Learning of Representations",
   "papers": [
    "pellegrini16_interspeech",
    "zeghidour16_interspeech",
    "mitra16_interspeech",
    "zhu16b_interspeech",
    "heck16_interspeech",
    "xu16_interspeech"
   ]
  },
  {
   "title": "Language Modeling for Conversational Speech and Confidence Measures",
   "papers": [
    "asami16_interspeech",
    "kanda16_interspeech",
    "ghannay16_interspeech",
    "horndasch16_interspeech",
    "xie16b_interspeech",
    "alshareef16_interspeech"
   ]
  },
  {
   "title": "Topics in Speech Perception",
   "papers": [
    "kuang16_interspeech",
    "chen16e_interspeech",
    "chen16f_interspeech",
    "pinter16_interspeech",
    "frye16_interspeech",
    "mulder16_interspeech",
    "eklund16_interspeech",
    "fogerty16_interspeech"
   ]
  },
  {
   "title": "Behavioral Signal Processing and Speaker State and Traits Analytics",
   "papers": [
    "huang16b_interspeech",
    "li16b_interspeech",
    "trouvain16_interspeech",
    "grzybowska16_interspeech",
    "li16c_interspeech",
    "an16_interspeech",
    "lehman16_interspeech",
    "akira16_interspeech",
    "gupta16b_interspeech",
    "huang16c_interspeech",
    "dumpala16b_interspeech",
    "kawabata16_interspeech",
    "gibson16_interspeech",
    "huang16d_interspeech"
   ]
  },
  {
   "title": "Speech Synthesis Poster",
   "papers": [
    "masmoudi16_interspeech",
    "saychum16_interspeech",
    "jaumardhakoun16_interspeech",
    "li16d_interspeech",
    "li16e_interspeech",
    "taylor16_interspeech",
    "nakashika16_interspeech",
    "toutios16_interspeech",
    "xie16c_interspeech",
    "liu16g_interspeech",
    "liberatore16_interspeech",
    "guennec16_interspeech",
    "moungsri16_interspeech",
    "ni16_interspeech"
   ]
  },
  {
   "title": "Resources and Annotation of Resources",
   "papers": [
    "yu16b_interspeech",
    "goldman16_interspeech",
    "ylmaz16b_interspeech",
    "kathol16_interspeech",
    "chen16g_interspeech",
    "richey16_interspeech",
    "ramakrishna16_interspeech",
    "matousek16_interspeech"
   ]
  },
  {
   "title": "Show &amp; Tell Session 4",
   "papers": [
    "corralesastorgano16_interspeech",
    "kelly16_interspeech",
    "james16_interspeech",
    "marin16_interspeech"
   ]
  },
  {
   "title": "Acoustic Model Adaptation",
   "papers": [
    "delcroix16_interspeech",
    "lim16_interspeech",
    "nagamine16b_interspeech",
    "suzuki16_interspeech",
    "samarakoon16_interspeech",
    "fainberg16_interspeech"
   ]
  },
  {
   "title": "Special Session: Sharing Research and Education Resources for Understanding Speech Processing",
   "papers": [
    "metze16_interspeech",
    "green16_interspeech",
    "hain16_interspeech",
    "plummer16_interspeech",
    "sprouse16_interspeech",
    "bates16_interspeech"
   ]
  },
  {
   "title": "Special Session: Voice Conversion Challenge",
   "papers": [
    "toda16_interspeech",
    "wester16_interspeech",
    "chen16h_interspeech",
    "mohammadi16_interspeech",
    "wu16c_interspeech",
    "villavicencio16_interspeech",
    "erro16_interspeech",
    "kobayashi16_interspeech"
   ]
  },
  {
   "title": "Intelligibility and Masking",
   "papers": [
    "landerportnoy16_interspeech",
    "gibbs16_interspeech",
    "xu16b_interspeech",
    "liu16h_interspeech",
    "chen16i_interspeech",
    "hodoshima16_interspeech"
   ]
  },
  {
   "title": "Robust Speaker Recognition and Anti-Spoofing",
   "papers": [
    "sahidullah16_interspeech",
    "korshunov16_interspeech",
    "sriskandaraja16_interspeech",
    "tian16_interspeech",
    "sahidullah16b_interspeech",
    "meng16b_interspeech"
   ]
  },
  {
   "title": "Speech Enhancement and Applications",
   "papers": [
    "fischer16_interspeech",
    "liu16i_interspeech",
    "blass16_interspeech",
    "drude16_interspeech",
    "zeiler16_interspeech",
    "spille16_interspeech"
   ]
  },
  {
   "title": "Speech Analysis",
   "papers": [
    "gowda16_interspeech",
    "lim16b_interspeech",
    "blaauw16_interspeech",
    "seelamantula16_interspeech",
    "birkholz16_interspeech",
    "zhang16d_interspeech",
    "morise16_interspeech",
    "rengaswamy16_interspeech"
   ]
  },
  {
   "title": "Speaker Recognition",
   "papers": [
    "saeidi16_interspeech",
    "singer16_interspeech",
    "ferras16_interspeech",
    "lei16_interspeech",
    "soni16_interspeech",
    "mclaren16c_interspeech",
    "kheder16b_interspeech",
    "kanagasundaram16_interspeech",
    "thomsen16_interspeech",
    "yu16c_interspeech",
    "hong16_interspeech",
    "ma16b_interspeech",
    "zhang16e_interspeech",
    "tian16b_interspeech"
   ]
  },
  {
   "title": "Decoding, System Combination",
   "papers": [
    "kanda16b_interspeech",
    "asaei16_interspeech",
    "tucker16_interspeech",
    "martinez16_interspeech",
    "he16b_interspeech",
    "price16_interspeech",
    "yang16c_interspeech",
    "tang16b_interspeech",
    "xu16c_interspeech",
    "hartmann16_interspeech",
    "obara16_interspeech",
    "chen16j_interspeech"
   ]
  },
  {
   "title": "Special Session: Clinical and Neuroscience-Inspired Vocal Biomarkers of Neurological and Psychiatric Disorders",
   "papers": [
    "sahu16_interspeech",
    "ariasvergara16_interspeech",
    "weiner16_interspeech",
    "alghowinem16_interspeech",
    "zhou16_interspeech",
    "pokorny16_interspeech",
    "pettorino16_interspeech"
   ]
  },
  {
   "title": "Show &amp; Tell Session 5",
   "papers": [
    "anguera16_interspeech",
    "guo16_interspeech",
    "chan16b_interspeech",
    "malfrere16_interspeech"
   ]
  },
  {
   "title": "Keynote 3: Anne Fernald",
   "papers": [
    "fernald16_interspeech"
   ]
  },
  {
   "title": "Far-Field Speech Processing",
   "papers": [
    "sainath16b_interspeech",
    "li16f_interspeech",
    "erdogan16_interspeech",
    "guerrero16_interspeech",
    "mandel16b_interspeech",
    "peddinti16_interspeech"
   ]
  },
  {
   "title": "Special Session: Interspeech 2016 Computational Paralinguistics Challenge (ComParE): Deception, Sincerity &amp; Native Language",
   "papers": [
    "schuller16_interspeech",
    "schuller16b_interspeech",
    "levitan16b_interspeech",
    "amiriparian16_interspeech",
    "montacie16_interspeech",
    "schuller16c_interspeech",
    "booth16_interspeech",
    "gosztolya16b_interspeech",
    "lee16c_interspeech",
    "herms16_interspeech",
    "zhang16f_interspeech",
    "kaya16_interspeech"
   ]
  },
  {
   "title": "Special Session: Speech, Audio, and Language Processing Techniques Applied to Bird and Animal Vocalizations",
   "papers": [
    "harte16_interspeech",
    "harte16b_interspeech",
    "harte16c_interspeech",
    "harte16d_interspeech"
   ]
  },
  {
   "title": "Dialogue Systems and Analysis of Dialogue",
   "papers": [
    "barlier16_interspeech",
    "lejeune16_interspeech",
    "sun16c_interspeech",
    "fotedar16_interspeech",
    "eecke16_interspeech",
    "beaver16_interspeech"
   ]
  },
  {
   "title": "Interaction between Speech Production and Perception",
   "papers": [
    "barnaud16_interspeech",
    "wolters16_interspeech",
    "katz16_interspeech",
    "caudrelier16_interspeech",
    "schweitzer16b_interspeech",
    "tuomainen16_interspeech"
   ]
  },
  {
   "title": "Multimodal Processing",
   "papers": [
    "olcoz16_interspeech",
    "doulaty16_interspeech",
    "chao16_interspeech",
    "aides16_interspeech",
    "tao16b_interspeech",
    "gergen16_interspeech"
   ]
  },
  {
   "title": "Pitch, Tone, and Music",
   "papers": [
    "kruspe16_interspeech",
    "yuan16b_interspeech",
    "chen16k_interspeech",
    "pannala16_interspeech",
    "daido16_interspeech",
    "verma16_interspeech"
   ]
  },
  {
   "title": "Speaker Diarization and Recognition",
   "papers": [
    "sturim16_interspeech",
    "lan16_interspeech",
    "dubey16_interspeech",
    "milner16_interspeech",
    "lapidot16_interspeech",
    "sell16_interspeech",
    "dawalatabad16_interspeech",
    "oo16_interspeech",
    "scherhag16_interspeech",
    "monteserin16_interspeech",
    "guo16b_interspeech",
    "su16_interspeech",
    "zegers16_interspeech",
    "kumar16_interspeech"
   ]
  },
  {
   "title": "Speech Synthesis Poster",
   "papers": [
    "gonzalvo16_interspeech",
    "wang16e_interspeech",
    "wen16b_interspeech",
    "song16_interspeech",
    "ohtani16_interspeech",
    "espic16_interspeech",
    "zhao16b_interspeech",
    "zen16_interspeech",
    "hojo16_interspeech",
    "juvela16b_interspeech",
    "tachibana16_interspeech",
    "potard16_interspeech",
    "lazaridis16_interspeech",
    "chiang16_interspeech",
    "vadapalli16_interspeech",
    "liu16j_interspeech"
   ]
  },
  {
   "title": "Language Model Adaptation",
   "papers": [
    "drugman16_interspeech",
    "kuznetsov16_interspeech",
    "oguz16_interspeech",
    "gangireddy16_interspeech",
    "halpern16_interspeech",
    "deena16_interspeech"
   ]
  },
  {
   "title": "Show &amp; Tell Session 6",
   "papers": [
    "brady16_interspeech",
    "stone16_interspeech",
    "stanislav16b_interspeech"
   ]
  },
  {
   "title": "Robustness in Speech Processing",
   "papers": [
    "zmolikova16_interspeech",
    "kundu16_interspeech",
    "markov16_interspeech",
    "shinohara16b_interspeech",
    "poblete16_interspeech",
    "hartmann16b_interspeech"
   ]
  },
  {
   "title": "Special Session: Interspeech 2016 Computational Paralinguistics Challenge (ComParE): Deception, Sincerity &amp; Native Language",
   "papers": [
    "schuller16d_interspeech",
    "rajpal16_interspeech",
    "jiao16_interspeech",
    "keren16_interspeech",
    "senoussaoui16_interspeech",
    "huckvale16_interspeech",
    "shivakumar16_interspeech",
    "abad16_interspeech",
    "gosztolya16c_interspeech",
    "schuller16e_interspeech",
    "schuller16f_interspeech"
   ]
  },
  {
   "title": "Acoustic and Articulatory Phonetics",
   "papers": [
    "tabain16_interspeech",
    "toutios16b_interspeech",
    "renwick16_interspeech",
    "fan16_interspeech",
    "monteserin16b_interspeech",
    "yi16_interspeech"
   ]
  },
  {
   "title": "Speech Synthesis Oral I: Neural Networks",
   "papers": [
    "ming16_interspeech",
    "thangthai16_interspeech",
    "ronanki16_interspeech",
    "li16g_interspeech",
    "airaksinen16b_interspeech",
    "nishimura16_interspeech"
   ]
  },
  {
   "title": "Speech Quality &amp; Intelligibility",
   "papers": [
    "backstrom16_interspeech",
    "tang16c_interspeech",
    "koster16_interspeech",
    "jokinen16_interspeech",
    "ganzeboom16_interspeech",
    "koutsogiannaki16_interspeech"
   ]
  },
  {
   "title": "Speech Translation and Metadata for Linguistic/Discourse Structure",
   "papers": [
    "niehues16_interspeech",
    "adams16_interspeech",
    "zayats16_interspeech",
    "che16_interspeech",
    "do16_interspeech",
    "le16_interspeech"
   ]
  },
  {
   "title": "Speech Coding and Audio Processing for Noise Reduction",
   "papers": [
    "korse16_interspeech",
    "villette16_interspeech",
    "ramo16_interspeech",
    "pirhosseinloo16_interspeech",
    "kayser16_interspeech",
    "ji16_interspeech",
    "cheong16_interspeech",
    "sreeram16_interspeech"
   ]
  },
  {
   "title": "Special Session: Speech, Audio, and Language Processing Techniques Applied to Bird and Animal Vocalizations",
   "papers": [
    "bonada16b_interspeech",
    "kaewtip16_interspeech",
    "wisler16_interspeech",
    "aihara16b_interspeech",
    "guyot16_interspeech",
    "stowell16_interspeech",
    "jancovic16_interspeech",
    "maina16_interspeech",
    "kohlsdorf16_interspeech",
    "suzuki16b_interspeech",
    "kurth16_interspeech",
    "moore16_interspeech",
    "oreilly16_interspeech"
   ]
  },
  {
   "title": "Learning, Education and Different Speech",
   "papers": [
    "hsu16c_interspeech",
    "heeman16_interspeech",
    "xie16d_interspeech",
    "ward16_interspeech",
    "lin16_interspeech",
    "kim16c_interspeech",
    "warlaumont16_interspeech",
    "le16b_interspeech",
    "laborde16_interspeech",
    "robertson16_interspeech",
    "ding16_interspeech"
   ]
  },
  {
   "title": "Dialogue Systems and Analysis of Dialogue",
   "papers": [
    "li16h_interspeech",
    "song16b_interspeech",
    "li16i_interspeech",
    "shen16_interspeech",
    "kumar16b_interspeech",
    "casanueva16_interspeech",
    "tseng16b_interspeech"
   ]
  },
  {
   "title": "Topics in Speech Recognition",
   "papers": [
    "ravuri16_interspeech",
    "palaz16_interspeech",
    "alvarez16_interspeech",
    "povey16_interspeech",
    "ratajczak16_interspeech",
    "wong16_interspeech"
   ]
  },
  {
   "title": "Special Session: Realism in Robust Speech Processing",
   "papers": [
    "hansen16_interspeech",
    "jokinen16b_interspeech",
    "sturim16b_interspeech",
    "bertin16_interspeech",
    "ravanelli16_interspeech",
    "gamper16_interspeech",
    "richardson16_interspeech",
    "ribas16_interspeech"
   ]
  },
  {
   "title": "Spoken Word Recognition",
   "papers": [
    "bosch16_interspeech",
    "mcauliffe16_interspeech",
    "drozdova16_interspeech",
    "hintz16_interspeech",
    "kang16_interspeech",
    "kanwal16_interspeech"
   ]
  },
  {
   "title": "Speech Synthesis Oral: High Level Linguistic Features",
   "papers": [
    "tahon16_interspeech",
    "mousa16_interspeech",
    "esch16_interspeech",
    "pouget16_interspeech",
    "dall16_interspeech",
    "wang16f_interspeech"
   ]
  },
  {
   "title": "Speech Enhancement",
   "papers": [
    "jeon16_interspeech",
    "papadopoulos16_interspeech",
    "mirsamadi16_interspeech",
    "brutti16_interspeech",
    "petkov16b_interspeech",
    "yamamoto16_interspeech"
   ]
  },
  {
   "title": "Dialogue: Backchannels and Turntaking",
   "papers": [
    "kawahara16b_interspeech",
    "lunsford16_interspeech",
    "meshorer16_interspeech",
    "bailly16_interspeech",
    "chowdhury16_interspeech",
    "oertel16_interspeech"
   ]
  },
  {
   "title": "Language Recognition",
   "papers": [
    "gwon16_interspeech",
    "fernando16_interspeech",
    "kv16_interspeech",
    "ali16_interspeech",
    "ng16_interspeech",
    "geng16_interspeech",
    "sagha16_interspeech"
   ]
  },
  {
   "title": "Speech and Audio Segmentation and Classification",
   "papers": [
    "mun16_interspeech",
    "origlia16_interspeech",
    "maas16_interspeech",
    "nandwana16_interspeech",
    "girish16_interspeech",
    "zhang16g_interspeech",
    "takahashi16b_interspeech",
    "meier16_interspeech",
    "kinnunen16b_interspeech",
    "pokorny16b_interspeech",
    "ferrer16_interspeech"
   ]
  },
  {
   "title": "New Products and Services",
   "papers": [
    "moore16b_interspeech",
    "moore16c_interspeech",
    "radadia16_interspeech",
    "hsiao16_interspeech",
    "lee16d_interspeech",
    "wand16_interspeech",
    "abraham16b_interspeech",
    "ragni16_interspeech",
    "tilk16_interspeech",
    "enarvi16_interspeech",
    "lanchantin16_interspeech",
    "gaur16_interspeech",
    "fischer16b_interspeech",
    "pancoast16_interspeech",
    "mitra16b_interspeech",
    "asadi16_interspeech"
   ]
  },
  {
   "title": "Low Resource Speech Recognition",
   "papers": [
    "wilkinson16_interspeech",
    "tsujioka16_interspeech",
    "bruguier16_interspeech",
    "ge16_interspeech",
    "pylkkonen16_interspeech"
   ]
  },
  {
   "title": "Keynote 4: Dan Jurafsky",
   "papers": [
    "jurafsky16_interspeech"
   ]
  },
  {
   "title": "Special Event: Speech Ventures",
   "papers": [
    "scheffer16_interspeech"
   ]
  },
  {
   "title": "Special Session: Speech and Language Technologies for Human-Machine Conversation-Based Language Education",
   "papers": [
    "tong16_interspeech",
    "tao16c_interspeech",
    "qian16_interspeech",
    "li16j_interspeech",
    "wang16g_interspeech",
    "rasipuram16_interspeech",
    "shi16_interspeech"
   ]
  },
  {
   "title": "Phonation and Voice Quality",
   "papers": [
    "hejna16b_interspeech",
    "adi16_interspeech",
    "ghosh16_interspeech",
    "yanushevskaya16_interspeech",
    "borsky16_interspeech",
    "maekawa16_interspeech"
   ]
  },
  {
   "title": "Speech Synthesis Oral: Prosody and Expressive Speech",
   "papers": [
    "chen16l_interspeech",
    "jauk16_interspeech",
    "ribeiro16_interspeech",
    "braunschweiler16_interspeech",
    "do16b_interspeech",
    "zheng16_interspeech"
   ]
  },
  {
   "title": "Language Recognition",
   "papers": [
    "zhao16c_interspeech",
    "lee16e_interspeech",
    "lu16c_interspeech",
    "travadi16_interspeech",
    "garciaromero16_interspeech",
    "gelly16_interspeech"
   ]
  },
  {
   "title": "Spoken Language Understanding Systems",
   "papers": [
    "hori16_interspeech",
    "vukotic16_interspeech",
    "chen16m_interspeech",
    "vu16_interspeech",
    "celikyilmaz16_interspeech",
    "tafforeau16_interspeech"
   ]
  },
  {
   "title": "Language Recognition",
   "papers": [
    "li16k_interspeech",
    "irtza16_interspeech",
    "masumura16_interspeech",
    "geng16b_interspeech",
    "pesan16_interspeech",
    "kapolowicz16_interspeech",
    "he16c_interspeech",
    "jokinen16c_interspeech"
   ]
  },
  {
   "title": "Music, Audio, and Source Separation",
   "papers": [
    "zhang16h_interspeech",
    "m16_interspeech",
    "chen16n_interspeech",
    "kruspe16b_interspeech",
    "bentsen16_interspeech",
    "wood16_interspeech",
    "montazeri16_interspeech",
    "grais16_interspeech",
    "riday16_interspeech",
    "li16l_interspeech",
    "guo16c_interspeech",
    "ma16c_interspeech",
    "sundar16_interspeech",
    "ochi16b_interspeech"
   ]
  },
  {
   "title": "Acoustic Modeling with Neural Networks",
   "papers": [
    "fahringer16_interspeech",
    "sailor16_interspeech",
    "weber16_interspeech",
    "zhang16i_interspeech",
    "tang16d_interspeech",
    "chien16_interspeech",
    "chan16c_interspeech",
    "gosztolya16d_interspeech",
    "miao16_interspeech",
    "zhao16d_interspeech",
    "zeyer16_interspeech",
    "sercu16_interspeech",
    "ghahremani16_interspeech",
    "chebotar16_interspeech",
    "wang16h_interspeech",
    "luyet16_interspeech"
   ]
  },
  {
   "title": "Robustness and Adaptation",
   "papers": [
    "zheng16b_interspeech",
    "shahnawazuddin16_interspeech",
    "delagua16_interspeech",
    "dharo16_interspeech",
    "mallidi16_interspeech",
    "joy16_interspeech",
    "samarakoon16b_interspeech",
    "goo16_interspeech"
   ]
  },
  {
   "title": "Special Event: Computational Approaches to Linguistic Code Switching",
   "papers": [
    "diab16_interspeech"
   ]
  },
  {
   "title": "Neural Networks for Language Modeling",
   "papers": [
    "arisoy16_interspeech",
    "damavandi16_interspeech",
    "haidar16_interspeech",
    "oualil16_interspeech",
    "levit16_interspeech",
    "irie16_interspeech"
   ]
  },
  {
   "title": "Special Session: Sub-Saharan African Languages: From Speech Fundamentals to Applications",
   "papers": [
    "das16b_interspeech",
    "gauthier16b_interspeech",
    "heerden16_interspeech",
    "godard16_interspeech",
    "vetter16_interspeech",
    "manenti16_interspeech",
    "schlunz16_interspeech",
    "westhuizen16_interspeech"
   ]
  },
  {
   "title": "Speech Production Models",
   "papers": [
    "ramanarayanan16_interspeech",
    "dabbaghchian16_interspeech",
    "wei16_interspeech",
    "szabados16_interspeech",
    "yoshinaga16_interspeech",
    "patri16_interspeech"
   ]
  },
  {
   "title": "Speaker States and Traits",
   "papers": [
    "zhang16j_interspeech",
    "parthasarathy16_interspeech",
    "ghosh16b_interspeech",
    "li16m_interspeech",
    "kalinli16_interspeech",
    "fayek16_interspeech"
   ]
  },
  {
   "title": "Speaker Recognition",
   "papers": [
    "valenti16_interspeech",
    "todisco16_interspeech",
    "sadjadi16_interspeech",
    "kheder16c_interspeech",
    "bahmaninezhad16_interspeech",
    "qian16b_interspeech"
   ]
  },
  {
   "title": "VAD and Audio Events",
   "papers": [
    "phan16_interspeech",
    "karamanolakis16_interspeech",
    "fujita16_interspeech",
    "zazo16_interspeech",
    "graciarena16_interspeech",
    "karakos16_interspeech"
   ]
  },
  {
   "title": "Spoken Term Detection",
   "papers": [
    "mitra16c_interspeech",
    "sawada16_interspeech",
    "kane16_interspeech",
    "ni16b_interspeech",
    "leung16_interspeech"
   ]
  },
  {
   "title": "Speech Enhancement and Noise Reduction",
   "papers": [
    "soni16b_interspeech",
    "gao16_interspeech",
    "sadasivan16_interspeech",
    "samui16_interspeech",
    "pm16_interspeech",
    "ogawa16_interspeech",
    "kumar16c_interspeech",
    "shivakumar16b_interspeech",
    "kato16_interspeech",
    "li16n_interspeech",
    "chinaev16_interspeech",
    "wang16i_interspeech",
    "fu16_interspeech",
    "li16o_interspeech",
    "liu16k_interspeech"
   ]
  },
  {
   "title": "Far-Field, Robustness and Adaptation",
   "papers": [
    "mitra16d_interspeech",
    "tomashenko16_interspeech",
    "bosch16b_interspeech",
    "loweimi16_interspeech",
    "mimura16_interspeech",
    "higuchi16_interspeech",
    "tran16_interspeech",
    "fujita16b_interspeech",
    "prasad16_interspeech",
    "dimitriadis16_interspeech",
    "liu16l_interspeech",
    "kim16d_interspeech",
    "lee16f_interspeech"
   ]
  },
  {
   "title": "Low Resource Speech Recognition",
   "papers": [
    "huang16e_interspeech",
    "thomas16_interspeech",
    "das16c_interspeech",
    "do16c_interspeech",
    "nouza16_interspeech",
    "razavi16_interspeech",
    "muller16_interspeech",
    "alumae16_interspeech"
   ]
  }
 ],
 "doi": "10.21437/Interspeech.2016"
}