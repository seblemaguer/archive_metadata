{
 "series": "",
 "title": "Spoken Word Access Processes (SWAP)",
 "location": "Nijmegen, The Netherlands",
 "startDate": "29/5/2000",
 "endDate": "31/5/2000",
 "original_url": "http://www.isca-speech.org/archive_open/swap",
 "original_title": "Spoken Word Access Processes (SWAP)",
 "logo": "minertx2.jpg",
 "conf": "SWAP",
 "year": "2000",
 "name": "swap_2000",
 "SIG": "",
 "title1": "Spoken Word Access Processes",
 "title2": "(SWAP)",
 "date": "29-31 May 2000",
 "papers": {
  "bard00_swap": {
   "authors": [
    [
     "Ellen Gurman",
     "Bard"
    ],
    [
     "C.",
     "Sotillo"
    ],
    [
     "M. P.",
     "Aylett"
    ]
   ],
   "title": "Taking the hit: why lexical and phonological processing should not make lexical access too easy",
   "original": "swap_003",
   "page_count": 4,
   "order": 1,
   "p1": "3",
   "pn": "6",
   "abstract": [
    "It seems to be assumed that phonological representation in the mental lexicon and lexical effects on phonological representations of speech input share the burden of overcoming variations in natural pronunciation, producing a percept of roughly equal quality whatever the input. This paper summarizes work which shows why that outcome would be counterproductive.\n",
    "In fact, naturally occurring variations in word pronunciation are not noise, but information [2, 3]. They indicate, for example, whether a word stands alone or forms part of an utterance and whether a nominal refers to a Given or a New entity. Three studies of spontaneous production show that, claims to the contrary notwithstanding, natural variation in pronunciation is not adjusted to sources of knowledge which the listener might apply simply. First, the degree to which words display phonological reductions and consequent intelligibility loss when heard in isolation is not constrained by lexical competition. Second, the differences between introductory and second mentions are not simply a binary categorial contrast between accented New items and deaccented Given items. Third, changes in pronunciation are not attuned to the listener's current knowledge of the discourse and its domain, but to the speaker's [1]. In effect, the listener has a delicate perceptual task for which the key is the speaker's internal representation of the discourse. Since grasping this is the listener's real goal, good design would be served by inducing the listener to find the key information rather than by managing all of lexical access without it.\n",
    "To serve the discourse understanding system, however, lexical access must take the hit: it must be affected but not blocked by degraded word tokens. A series of perception experiments show that variations in the clarity of spoken words both hamper lexical access and enhance access to a record of the discourse. First, tokens with naturally occurring phonological reductions and assimilations yield substantial but substantially less cross-modal identity priming than more canonical tokens. Second, when listeners perform a primed probe task after hearing a portion of a monologue containing a single token of the prime in the same sentence as the probe, both natural and artificial decrements in intelligibility enhance the priming effect. Because the effect is restricted to those monologues where the prime word will ultimately be mentioned again, perceptual difficulty appears to be inducing selective rapid access to important features of the discourse.\n",
    "References \n",
    "[1] Bard, E. G., Anderson, A. H., Sotillo, C., Aylett, M., Doherty-Sneddon, G., and Newlands, A. (2000). Controlling the intelligibility of referring expressions in dialogue. Journal of Memory and Language, 42, 1-22.\n",
    "[2] Fowler, C., & Housum, J. (1987). Talkers' signalling of `new' and `old' words in speech and listeners' perception and use of the distinction. Journal of Memory and Language, 26, 489-504.\n",
    "[3] Lindblom, B. (1990). Explaining variation: a sketch of the H and H theory. In W. Hardcastle & A. Marchal (Eds.), Speech production and speech modelling, (pp. 403-439). Dordrecht, Netherlands: Kluwer Academic Publishers.\n",
    ""
   ]
  },
  "gaskell00_swap": {
   "authors": [
    [
     "Gareth",
     "Gaskell"
    ]
   ],
   "title": "A quick rum picks you up, but is it good for you? Sentence context effects in the identification of spoken words",
   "original": "swap_007",
   "page_count": 4,
   "order": 2,
   "p1": "7",
   "pn": "10",
   "abstract": [
    "Sentential context apparently has a more influential role in the selection of meanings for lexically ambiguous words (e.g., bank) than the identification of spoken words. Priming studies show that the preceding context of an ambiguous word can have an immediate (but often weak) effect on the activation of its meanings, suggesting some interactive processing (Lucas, 1999). On the other hand, sentential context is often viewed as unimportant for the identification of spoken words, with any effects restricted to the integration stage of spoken word recognition (Connine, Blasko & Wang, 1994; Marslen-Wilson, 1989).\n",
    "This distinction between the effects of context in the two processes can be interpreted in different ways. It is normally assumed to reflect an architectural difference, such that lexical identification and meaning retrieval are located at different levels, with different rules for the involvement of semantic information. But it is also plausible that the distinction can be accommodated by a single system, in which the only difference between the domains is in the availability of disambiguating informationsentential context will generally be unnecessary for spoken word identification because bottom-up information will normally resolve any ambiguity, but it will always be needed for the resolution of lexical ambiguity.\n",
    "Three experiments are reported, which examine spoken word identification but in a situation more like lexical ambiguity. The perceptual effects of utterances such as \"A quick rum picks you up\" are examined, in which the underlined sequence is ambiguous. It could be simply the word rum, but it could also be a token of run, in which the /n/ has fully assimilated to /m/ because of the influence of the following segment. In this case, there is no single lexical item (like bank) with two possible meanings, but nonetheless the acoustic input is insufficient to decide between the two lexical alternatives (run and rum). The repetition priming studies show that sentential context does indeed have a significant immediate effect on word activations in this ambiguous case. These results can be explained by a model of spoken word recognition in which sentential cues are integrated with bottom-up sources of information throughout the course of word recognition, but that the probabilistic nature of sentential context effects means that they are normally overridden by more reliable bottom-up cues.\n",
    "References \n",
    "Connine, C. M., Blasko, D. G., & Wang, J. (1994). Vertical similarity in spoken word recognition - multiple lexical activation, individual-differences, and the role of sentence context. Perception & Psychophysics, 56, 624-636.\n",
    "Lucas, M. (1999). Context Effects in Lexical Access: A meta-analysis. Memory and Cognition, 27, 385-398.\n",
    "Marslen-Wilson, W. D. (1989). Access and integration: Projecting sound onto meaning. In W. D. Marslen-Wilson (Ed.), Lexical representation and process. Cambridge, MA: MIT Press.\n",
    ""
   ]
  },
  "zwitserlood00_swap": {
   "authors": [
    [
     "Pienie",
     "Zwitserlood"
    ],
    [
     "Else",
     "Coenen"
    ]
   ],
   "title": "Consequences of assimilation for word recognition and lexical representation",
   "original": "swap_011",
   "page_count": 4,
   "order": 3,
   "p1": "11",
   "pn": "14",
   "abstract": [
    "In connected speech, consonants can adopt the place of articulation of neighbouring consonants. This happens word-internally and at word boundaries. The latter situation, in particular, creates a challenge for word recognition. There are a number of ways to deal with how surface variation affects word recognition.\n",
    "One is to treat all variation as 'noise' to the system. Whether such variation is harmful to the system depends on the consequences of a less than perfect fit between input and sublexical and lexical representations. In TRACE, word units are not punished by mismatch with the input. Mismatch is essentially the same as lack of information. In both cases, word units will be activated to a lesser extent, compared to a complete match between input and word unit. Simulations with MERGE show that mismatch affects the behaviour of word nodes. In the Cohort model, mismatch can have negative consequences, depending on the legality of the variation.\n",
    "The idea that mismatch can be legal or illegal comes from a different perspective to surface variation. This perspective has its roots in phonological theory and focuses on lexical specification. If representations of lexical form (or word units) are fully specified - as they are in TRACE and MERGE - all types of mismatch will have an effect. If lexical representations are underspecified - as in Cohort - changes that reflect phonologically legal variation will not harm word recognition, illegal changes will.\n",
    "Another way to deal with variation due to assimilation is in terms of a context-sensitive undoing of assimilation. If licensed by the local, segmental context, assimilations can be recovered in some way, such that the altered segment can be mapped onto the original one. Gaskell and Marslen-Wilson (1996, 1998) describe such a process in terms of phonological inference. Their recent position, couched in terms of a network model, is that we need abstractness at the level of representation as well as phonological inference processes.\n",
    "Evidence for abstractness and phonological inference mainly comes from English. We present crossmodal priming data from German, with assimilations in terms of place of articulation and voice, in regressive and progressive directions. We contrasted phonologically legal and illegal assimilations, in isolation and in context, where the context could be viable or unviable. First, the data show that effects depend on the availability of context. With one exception, no reliable effects are found for altered stimuli presented in isolation. With context, both contextual viability and phonological legality have an impact. We discuss implications of our findings for the approaches to word recognition and lexical representation mentioned above.\n",
    ""
   ]
  },
  "nooteboom00_swap": {
   "authors": [
    [
     "Sieb",
     "Nooteboom"
    ],
    [
     "Esther",
     "Janse"
    ],
    [
     "Hugo",
     "Quené"
    ],
    [
     "Saskia te",
     "Riele"
    ]
   ],
   "title": "Multiple activation and early context effects",
   "original": "swap_015",
   "page_count": 4,
   "order": 4,
   "p1": "15",
   "pn": "19",
   "abstract": [
    "It has been suggested and it is widely believed that the experimental technique of crossmodal semantic priming with partial primes, combined with lexical decision as a subjects task, is a suitable method for obtaining information on the temporal course of multiple lexical activation and selection during spoken-word recognition (Zwitserlood, 1989; Frauenfelder & Floccia, 1999). We will report on such an experiment that was in many ways a replication of Zwitserlood (1989), although some changes were made in linguistic material and design. Specifically, a within-subjects design was used instead of a balanced-incomplete-blocks design. We failed to produce consistent priming effects with partial primes. This was reason for a computer simulation comparing the statistical analysis of a balanced-incomplete-blocks design and a within-subjects design, using fictitious data sets. The balanced-incomplete-blocks design together with normalization as applied by Zwitserlood, was shown to easily produce spurious significance. We argue that, unfortunately, crossmodal semantic priming with partial primes is not a reliable method to study the time course of multiple activation and selection.\n",
    "In order to study the time course of lexical activation as a function of sensory information and context, we must apply a different technique. In another experiment we used rhyme monitoring. Admittedly, rhyme monitoring may bias the results because the rhyme cue very likely causes phonological priming and thus pre-activation of the word candidate. However, if in the presence of such pre-activation no early context effects are found, one can make a strong case for context only affecting the late stages of word recognition. If, on the other hand, one does obtain early context effects in a rhyme monitoring experiment, this would show that at least in some conditions context may affect lexical activation before sensory information has led to a unique choice. Our experiment shows clear context effects at a moment in time that disambiguating sensory information cannot possibly have arrived. We conclude that the early stages of word recognition are not always impenetrable for context effects.\n",
    ""
   ]
  },
  "marslenwilson00_swap": {
   "authors": [
    [
     "William D.",
     "Marslen-Wilson"
    ]
   ],
   "title": "Organising principles in lexical access and representation? A view acrosslanguages",
   "original": "swap_019",
   "page_count": 4,
   "order": 5,
   "p1": "19",
   "pn": "22",
   "abstract": [
    "The notion \"word\" is investigated cross-linguistically in a series of studies systematically comparing lexical representation and processing in English, Polish, Arabic, and Mandarin Chinese, using a variety of priming techniques. The studies so far reveal considerable diversity, with languages differing widely in types of lexical organisation. Mandarin appears to rely primarily on non-combinatorial representations, while English and Polish employ in addition a decompositional, morphemically based system. The non-concatenative morphology of Arabic is also highly combinatorial, and plays an obligatory morpho-phonological structural role. We find little evidence for specific cross-linguistic constraints on lexical structure and content.\n",
    ""
   ]
  },
  "boudelaa00_swap": {
   "authors": [
    [
     "Sami",
     "Boudelaa"
    ],
    [
     "William D.",
     "Marslen-Wilson"
    ]
   ],
   "title": "Non-concatenative morphemes in language processing: Evidence from Modern Standard Arabic",
   "original": "swap_023",
   "page_count": 4,
   "order": 6,
   "p1": "23",
   "pn": "26",
   "abstract": [
    "The role of morphology in lexical processing and representation is a critical issue in psycholinguistics. Most research into this has been in languages with a concatenative morphology, with the exception of recent work in Hebrew. Another Semitic language, Modern Standard Arabic (MSA), which also has a non-concatenative morphology, offers additional opportunities for probing not only the use of discontinuous morphemes that never surface on their own, but also discontinuous morphemes that do not carry semantic information.\n",
    "In MSA surface word forms fall into three categories: primitive nouns, deverbal nouns and verbs. Each of these comprises three interwoven morphemes: a consonantal root morpheme that carries semantic information, a vocalic morpheme conveying syntactic information and a skeletal morpheme providing a canonical shape associated with a particular meaning or grammatical function. Thus, the word [katam] (\"conceal\") is composed of the root morpheme {ktm} with the semantic load \"concealment\", the vocalic morpheme {a-a} with the syntactic information \"active\" and the skeletal morpheme {CVCVC} with the information \"past tense\". The combination of vocalic and skeletal morphemes gives rise to an abstract complex morphological unit called the Word-Pattern.\n",
    "In a series of cross-modal priming experiments we investigated the processing and representation of these root and word pattern morphemes. When prime and target are primitive nouns sharing the two components of the word-pattern (e.g., [qamarun]-[maraqun], \"moon-sauce\") no priming is obtained. When deverbal nouns sharing a word-pattern (e.g., [xuDuuAun]-[HuduuTun], \"submission-happening\") are used as primes and targets, cross-modal priming occurs only if the overlap is both at the level of the vocalic and the skeletal morphemes. Pairs like [sujuunun]-[ HuduuTun] (\"prisons-happening\") which share the form of the word-pattern but not its meaning fail to yield any priming effects. Word pattern priming is further obtained with verbs both when prime and target share the form and meaning of the word pattern and when they share solely its form. This suggests that the use of the word pattern morpheme as a \"phonological structure\" or as a \"meaning-carrying unit\" depends on the transparency and the productivity of the surface form at hand.\n",
    "Turning to the root morpheme in MSA, we focussed on verbs and deverbal nouns, using primes and targets sharing the root morpheme while co-varying the semantic relationship between them. Significant cross-modal priming was obtained even in the absence of a transparent semantic relationship between prime and target.\n",
    "Taken together the results point clearly to word-patterns and roots being lexical units in MSA. They indicate that the language processor picks up on highly abstract morphological units, which never surface on their own. The morphological priming effects observed both with opaque root morphemes and with word patterns are hard to account for as simply an interaction between form and meaning.\n",
    ""
   ]
  },
  "mauth00_swap": {
   "authors": [
    [
     "Kerstin",
     "Mauth"
    ]
   ],
   "title": "Does morphological information influence phonetic categorization?",
   "original": "swap_027",
   "page_count": 4,
   "order": 7,
   "p1": "27",
   "pn": "30",
   "abstract": [
    "Results from phonetic categorization experiments show that listeners use lexical information as well as semantic and syntactic information from sentences to make phonemic decisions about ambiguous sounds. This series of phonetic categorization experiments was constructed to test whether preceding sentential context can influence the perception of inflectional morphemes. The relevant morpheme was the verbal 3rd person singular marker -t in Dutch. Listeners were presented with two different types of sentence constructions: (A) Vraag jij of Jan morgen gaat? 'Do you ask whether Jan leaves tomorrow?' (B) Zie jij nog wel eens een plaat? 'Do you now and then see a record?' In all conditions the final consonant was a stop plosive that varied in place of articulation along a continuum from [t] to [k] in which the [k] endpoints always formed nonwords. The main question was whether the shift in the categorization function towards the [t] endpoint would be any different for VP's as compared to NP's. Is people's perception influenced by the fact that in the VP the very last consonant [t] constitutes an inflectional morpheme which is predictable from the context? If yes, this might be an indication of a decompositional process being at work. Because the lexical shift that was expected for both sentence constructions where the [t] endpoints were real words (gaat vs. plaat) might mask a morphological effect, two other conditions were included. The existing verbs and nouns at the end of the sentences were replaced by nonwords which had a real noun embedded within them (vla 'custard' + t = vlaat). Would listeners be more willing to label an ambiguous sound as [t] when on the basis of the context (vraag jij of Jan morgen vlaat?), they can decompose the last nonword into two meaningful units than when this decomposition wouldn't make sense (zie jij nog wel eens een vlaat?)? Would this difference go away when the last word doesn't include an embedded noun (e.g., snaat) which would make a decomposition in either context impossible? Preliminary results show that when existing words are compared with nonwords, the lexicality effect for the verbal construction is significantly larger than for the nominal construction. The next step is to compare nonwords with possible words like vlaat.\n",
    ""
   ]
  },
  "meunier00_swap": {
   "authors": [
    [
     "Fanny",
     "Meunier"
    ],
    [
     "William D.",
     "Marslen-Wilson"
    ],
    [
     "Mike",
     "Ford"
    ]
   ],
   "title": "Suffixed Word Lexical Representations in French",
   "original": "swap_031",
   "page_count": 4,
   "order": 8,
   "p1": "31",
   "pn": "34",
   "abstract": [
    "In English and using a cross-modal paradigm, Marslen-Wilson et al. (1994) did not observe a priming effect between two suffixed words derived from the same stem. They did however observe priming between a suffixed word and its stem, a prefixed word and its stem, as well as between two prefixed words derived from the same stem. For these authors, this lack of priming between two suffixed words derived from the same stem reflects competition processes between suffixed forms belonging to the same morphological family when the common stem is accessed. In French, we observed that lexical decision time associated with suffixed words presented auditorily depends on the number of high-frequency suffixed candidates derived from the same stem (Meunier & Segui, 1999). We interpreted this result in the framework proposed by Marslen-Wilson et al. (1994) by postulating inhibitory processes between suffixed words that share a stem.The aim of the present experiment is to see whether or not, in French and using the same design and procedure as Marslen-Wilson et al. (1994), we observe priming effects between two suffixed words derived from the same stem. In French suffixed words can be derived from different types of stem. It seems clear that the type of stem can play a role on the way suffixed words are represented (decomposed or not). Consequently in our experiment we study the priming effect observed between two suffixed words depending on the type of stem they are derived from. We contrasted three types of stem: free stems such as sport found in sportif where the stem is a noun or an adjective; verbal free stems such as attest- found in attestation where the stem can appear only as verbal form, and bound stems such as ocul- found in oculaire where the stem is not a word by itself. Suffixed words were presented visually as targets in two auditory priming conditions: preceded either by another suffixed word derived from the same stem or by a control prime. We also had two control conditions: a suffixed word-stem condition and a formal condition where primes and targets were formally related but not semantically or morphologically related.We observed the classical suffixed word-stem priming effect. And while no formal effect was observed, the results showed a morphological priming effect for the three suffixed-suffixed word conditions: suffixed word primes significantly facilitated lexical decision responses for suffixed words whatever type of stem they were derived from. No interaction was found, which means that the facilitatory effect did not differ with the type of the stem. These results showed that, in French, suffixed words derived from the same stem prime each other and that this morphological effect is not modify by the type of stem these words are derived from. The results obtained in French contrast with those observed in English. This highlights the major role of the structure of languages on the way words are lexically accessed and represented as well as the importance of conducting experiments in different languages.\n",
    ""
   ]
  },
  "reid00_swap": {
   "authors": [
    [
     "Agnieszka A.",
     "Reid"
    ],
    [
     "William D.",
     "Marslen-Wilson"
    ]
   ],
   "title": "Complexity and alternation in the Polish mental lexicon",
   "original": "swap_035",
   "page_count": 4,
   "order": 9,
   "p1": "35",
   "pn": "38",
   "abstract": [
    "One of the most challenging areas for the study of lexical representation, and its role in lexical access from speech, is morphological complexity and allomorphy. Cross-linguistic evidence is especially critical here to deconfound language specific and language universal characteristics of lexical representation. Here we report some results from Polish, which has very rich derivational and inflectional morphology, and a widespread system of morphophonological alternations.\n",
    "A first series of experiments investigate a combinatorial approach to lexical representation in Polish. Experiment 1, using cross-modal priming, probed the representation of stems and affixes. The affixes involved mostly do not occur in English, and range through derivational and aspectual-derivational to diminutive affixes. The data support a combinatorial model of the mental lexicon with stems and affixes stored separately, so that the same morphemes can participate in the representation of many different words. Experiment 2 used an auditory-auditory delayed priming task (designed to dissociate semantic effects from morphological ones) to find out whether semantic compositionality determines combinatorial storage in Polish to the same extent as in English (Marslen-Wilson et al., 1994, '96), as opposed to Semitic languages such as Hebrew (Deutsch et al. 1998) and Arabic (Boudelaa, personal communication). The experiment contrasted words varying in morphological complexity and in semantic transparency. Priming was obtained only for words which were semantically compositional, irrespective of morphological complexity. This suggests that semantic compositionality plays an important role in determining the lexical representation in Polish, with semantically compositional words represented in a combinatorial fashion, and non-semantically compositional words possibly represented as full forms.\n",
    "A second series of experiments investigated the abstractness of underlying representations in Polish, probing the representation of verbs and nouns with regular and irregular alternations. Again we used both the cross-modal technique (nouns and verbs) and auditory-auditory delayed repetition (verbs). The results indicate that both regular and irregular alternants map onto the same underlying morpheme, with strong priming both immediately and at a delay, regardless of degree and type of alternation. The results overall, which cannot be attributed to either semantic or phonological factors, support a morphemic combinatorial approach to lexical representation, with alternants of the same verb mapping onto the same underlying lexical entry, and with semantic compositionality playing a crucial role in determining the nature of the representation. The results for the Polish mental lexicon correspond to those obtained for English and are in contrast to the results for Hebrew and Arabic. The theoretical implications of this are discussed.\n",
    ""
   ]
  },
  "content00_swap": {
   "authors": [
    [
     "Alain",
     "Content"
    ],
    [
     "Nicolas",
     "Dumay"
    ],
    [
     "Uli",
     "Frauenfelder"
    ]
   ],
   "title": "The role of syllable structure in lexical segmentation: Helping listeners avoid mondegreens",
   "original": "swap_039",
   "page_count": 4,
   "order": 10,
   "p1": "39",
   "pn": "42",
   "abstract": [
    "Because of the continuous nature of speech, and of the absence of clear and salient word boundary information, any sequence of phonemes or syllables is compatible with multiple lexical interpretations. Therefore, one important challenge for theories of word recognition is to determine how the listener recovers the intended lexical segmentation. In this talk, we argue that syllable structure may provide one source of constraint on lexical segmentation. More specifically, we propose that syllable onsets constitute potential alignment points for the mapping process.\n",
    "Our research started from an exploration of listeners intuitions about syllable structure, using explicit syllable segmentation tasks. In contrast to the widely held belief that syllable structure is salient and unambiguous in French, we observed that even for the simplest structures, CVCV words, listeners do not consistently assign the intervocalic consonant to the second syllable. Interestingly, most of the variability was related to first-syllable offset, while participants generally agreed on the location of the onset of the second syllable, suggesting that syllable onsets might correspond to more reliable and salient cues.\n",
    "To test the hypothesis that syllable onsets are used as alignment points for lexical mapping, we examined word spotting performance, manipulating the match between targets initial or final consonant and syllable boundaries. As predicted, the results showed a significant mismatch cost at initial position, and a small and non-significant cost for the final position. Other experiments currently underway using the cross-modal repetition priming technique provide further tests of the predominant role of syllable onsets.\n",
    "Any theory that attributes a role to syllable structure in speech perception must deal with the issue of resyllabification in continuous speech. In another set of studies, we examined the nature of acoustic/phonetic cues to word boundaries in lexically ambiguous sequences, and assessed their influence on syllabification and in online lexical segmentation. Systematic durational variations were observed for obstruent-liquid clusters, but not for /s/ + obstruent clusters. When present, acoustic/phonetic cues influenced both syllabification choices and word spotting latencies.\n",
    "Finally, we contrast our syllable onset segmentation heuristic with other views about the perceptual role of syllabic structure, and speculate on the complementary role of syllable onsets, of other stimulus-based cues such as rhythmic and prosodic information and of lexical competition in the process of lexical segmentation.\n",
    ""
   ]
  },
  "norris00_swap": {
   "authors": [
    [
     "Dennis",
     "Norris"
    ],
    [
     "Anne",
     "Cutler"
    ],
    [
     "James M.",
     "McQueen"
    ],
    [
     "Sally",
     "Butterfield"
    ],
    [
     "Ruth",
     "Kearns"
    ]
   ],
   "title": "Language-universal constraints on the segmentation of English",
   "original": "swap_043",
   "page_count": 4,
   "order": 11,
   "p1": "43",
   "pn": "46",
   "abstract": [
    "Norris, McQueen, Cutler and Butterfield (1997) proposed that listeners employ a Possible Word Constraint (PWC) during word recognition in continuous speech. The PWC disfavours interpretations which would leave an unparseable residue between the end of a candidate word and a \"known boundary\". Such an unparseable residue would be any string that could not constitute a possible word in the language. For example, in English it would not be possible for a word to consist solely of a sequence of consonants, or solely of an open syllable with a lax vowel. This constraint offers a powerful method for inhibiting activation of spuriously embedded words in spoken utterances. Thus the input \"the committee met a fourth time\" might activate, inter alia, \"for\" and \"metaphor\", but their activation could be reduced on the grounds that they would leave a single consonant as residue (th), and a single consonant cannot constitute a word.\n",
    "Norris et al.'s experiments used the word-spotting task (detection of any real word, in short nonsense strings), and successfully demonstrated that words are harder to spot when they would leave a single consonant as a residue (e.g. apple in fapple, or sea in seash) than when they would leave a syllable as residue (e.g. apple in vuffapple, sea in seashub). In this presentation we will describe two further experiments which addressed the issue of whether the PWC is language-specific or language-universal. If it is language-specific, then an open syllable with a lax vowel, for instance, will form a difficult context for word-spotting in English, since such a string cannot be a real word of that language. Other languages (e.g. French, Japanese) do, however, allow words of this structure; thus if the PWC is language-universal, it must allow possible words of this type, and therefore an open syllable with a lax vowel will constitute an easy context for word-spotting in all languages, including English. In the first experiment we compared the spotting of words in three contexts: open syllable with long vowel (canal in zeecanal, with the same vowel as in peek), open syllable with short lax vowel (zEcanal, with the same vowel as in peck), and consonant (scanal). Word-spotting was significantly harder in the consonant context than in the syllable contexts, which did not differ; this result is consistent with a language-universal form of the PWC.\n",
    "In the second experiment we compared contexts containing a strong (full) versus weak (reduced) vowel -- bell in bellshif (with the same vowel as in pick) or bellsh@f (where @ represents schwa); again these were contrasted with a consonant context (bellsh). In this experiment, word-spotting was again significantly harder in the consonant context than in both syllable contexts, which did not differ. Other studies have shown that boundaries before weak syllables tend to be overlooked in listening to English, but this language-specific effect appears not to affect the PWC. Although the \"known boundaries\" on which the PWC operates appear to be determined by language-specific (e.g. rhythmic and phonotactic) properties of the language, the PWC itself may be a language-universal constraint, disfavouring interpretations which leave any nonsyllabic residue.\n",
    ""
   ]
  },
  "mcqueen00_swap": {
   "authors": [
    [
     "James M.",
     "McQueen"
    ],
    [
     "Anne",
     "Cutler"
    ],
    [
     "Dennis",
     "Norris"
    ]
   ],
   "title": "Why merge really is autonomous and parsimonious",
   "original": "swap_047",
   "page_count": 4,
   "order": 12,
   "p1": "47",
   "pn": "50",
   "abstract": [
    "We briefly describe the Merge model of phonemic decision-making, and, in the light of general arguments about the possible role of feedback in spoken-word recognition, defend Merge's feedforward structure. Merge not only accounts adequately for the data, without invoking feedback connections, but does so in a parsimonious manner.\n",
    ""
   ]
  },
  "samuel00_swap": {
   "authors": [
    [
     "Arthur G.",
     "Samuel"
    ]
   ],
   "title": "Some empirical tests of Merge's architecture",
   "original": "swap_051",
   "page_count": 4,
   "order": 13,
   "p1": "51",
   "pn": "54",
   "abstract": [
    "In Merge, Norris, McQueen, and Cutler have proposed a somewhat different architecture than has usually been suggested in models of word recognition. In particular, they have split the phonemic level of processing into two separate parts, the \"input phonemic\" and \"phoneme decision\" processes. A consequence of this approach is that most of the results in the literature that show lexical effects on phonemic processing can be attributed to the action of the \"phoneme decision\" mechanism. I will discuss this architecture, and report on the results of experiments that bear on the model.\n",
    "If one accepts the division of the phonemic level into these two components, then as Norris et al point out, studies showing lexical influences on the identification of phonemes cannot unambiguously be attributed to an effect on encoding the phonemic information. Such a top-down attribution can only be shown through tasks in which the lexical information influences phonemic processing in a manner that does not require the listener to label the phoneme itself.\n",
    "One such demonstration has been provided by Samuel [Cognitive Psychology, 1997, 32, 97-127]. In that study, lexical context led to the phonemic restoration of a particular phoneme, and the resulting phoneme altered the labeling of other phonemes, through selective adaptation. I will report on a set of new selective adaptation experiments that provide further evidence that lexical activation can affect the activation of phonemes. The new experiments use a \"Ganong\" type of lexical influence to push the perception of a phonemically-neutral sound toward one lexical item or another, which in turn causes the sound itself to be perceived as one phoneme or another. For example, a fricative noise that is perceptually midway between /s/ and /S/ will be heard as /s/ when placed at the end of \"bronchiti\", but as /S/ at the end of \"aboli\". Selective adaptation with words made with this sort of concatenation produces changes in how listeners hear test syllables from an \"iss\"-\"ish\" continuum. Appropriate control conditions rule out artifactual explanations for these results. The data thus require top-down lexical-phonemic connections, suggesting that Merges architecture must be modified.\n",
    ""
   ]
  },
  "alphen00_swap": {
   "authors": [
    [
     "Petra van",
     "Alphen"
    ]
   ],
   "title": "Does subcategorical variation influence lexical access?",
   "original": "swap_055",
   "page_count": 4,
   "order": 14,
   "p1": "55",
   "pn": "58",
   "abstract": [
    "In the process of spoken language comprehension, the listener must map acoustic-phonetic information in the waveform onto stored representations of lexical form in the mental lexicon. A major question concerns the nature of the representations which make contact with the lexicon. Are small acoustic details within a phoneme present in this representation, or are they lost at a lower level? In this research project the Voice Onset Time (VOT) of the two Dutch voiced plosives was varied to find out whether lexical access is affected by such subcategorical variation.\n",
    "In Dutch there are two voiced plosives, namely /b/ and /d/. They are said to have a negative VOT, this means that the vocal cords start vibrating during the closure phase. A small production experiment was carried out to establish the natural variation in VOT for both /b/ and /d/. The results showed that VOT of Dutch voiced plosives varies considerably in natural speech and is therefore a suitable type of subcategorical variation to be studied.\n",
    "To find out how variation in VOT is perceived, listeners were asked to rate the initial phoneme of several high frequency (HF) words, low frequency (LF) words and nonwords (NW) on a scale from 1 (poor exemplar) to 7 (good exemplar). The items started with a /b/ or /d/, varying from zero periods to 18 periods of prevoicing. The outcomes were used to guide the construction of three variations of both voiced plosives: normal amount of prevoicing, too much prevoicing and too little prevoicing.\n",
    "In an associative priming experiment subjects were asked to perform a lexical decision task on visual targets that were preceded by an auditory prime. The primes were either HF words or LW words. The results showed a significant priming effect: reaction times to targets preceded by an unrelated prime were significantly slower than to targets preceded by a related prime. There was no difference between the first 3 conditions and there was no interaction with frequency. These results suggest that natural variation within a phoneme such as differences in VOT of voiced plosives does not influence lexical access.\n",
    ""
   ]
  },
  "bolte00_swap": {
   "authors": [
    [
     "Jens",
     "Bölte"
    ],
    [
     "Else",
     "Coenen"
    ]
   ],
   "title": "Domato primes paprika: Mismatching pseudowords activate semantic and phonological representations",
   "original": "swap_059",
   "page_count": 4,
   "order": 15,
   "p1": "59",
   "pn": "62",
   "abstract": [
    "Psycholinguistic theories of lexical access have to define the properties of sublexical and lexical representations. They need to specify the mapping from the speech signal information onto higher processing levels and need to set the limits that a lexical representation may deviate from the signal. Put differently, how far may a target representation deviate from the input to still be counted as matching rather than mismatching the input?\n",
    "We addressed this question by manipulating the phonological similarity of pseudowords to a real word. Lexical status of the target, the task, and the relationship of prime and target were manipulated to determine the level at which effects of mismatching information become evident.\n",
    "The same set of primes was used in four experiments. The degree of phonological relatedness to the intended word was varied in terms of phonological features (*domato, *somato, tomato). Prime (*domato) and target (TOMATO) were phonologically related in phonological priming experiments. In semantic priming experiments, the prime (*domato) was phonologically related to a semantic associate (tomato) of the target (PAPRIKA). Similar conditions were present when targets were pseudowords. Lexical decision or naming latencies were measured in a cross-modal priming situation.\n",
    "In both the semantic and the phonological priming experiment, priming effects were obtained for word targets. Priming effects were larger with phonological priming than with semantic priming, but did not vary with degree of mismatch. Pseudoword decisions were accelerated only in the phonological priming experiment. In addition, the pattern of results questions whether appropriate baseline conditions were used in earlier mediated semantic priming experiments (Marslen-Wilson, 1993; Connine, Blasko, & Titone, 1993).\n",
    "The finding suggests that phonological mismatch of the kind used here is not carried over to the semantic level. One could assume that a semantic representation is activated if its phonological representation exceeds a critical activation level. Gradual phonological activation will not result in gradual semantic activation.\n",
    ""
   ]
  },
  "cutler00_swap": {
   "authors": [
    [
     "Anne",
     "Cutler"
    ],
    [
     "Dennis",
     "Norris"
    ],
    [
     "James M.",
     "McQueen"
    ]
   ],
   "title": "Tracking TRACE's troubles",
   "original": "swap_063",
   "page_count": 4,
   "order": 16,
   "p1": "63",
   "pn": "66",
   "abstract": [
    "A major distinction among models of the recognition of spoken words concerns whether or not a model allows feedback from logically later to logically earlier levels of processing. One of the leading current models is TRACE (McClelland & Elman, 1986). TRACE is an interactive model which allows feedback from words to phonemic representations. Other models (e.g. Shortlist; Norris, 1994) do not allow such feedback.\n",
    "A challenge to TRACE was posed by Marslen-Wilson and Warren (1994), who conducted experiments in which listeners had to perform phonetic decisions or lexical decisions on words and nonwords, some cross-spliced so that they contained acoustic-phonetic mismatches. Compared to unspliced words, both words cross-spliced with words and words cross-spliced with nonwords produced equivalently slower decisions. However, performance on nonwords was poorer for nonwords cross-spliced with words than for nonwords cross-spliced with nonwords. Marslen-Wilson and Warren were unable to simulate their findings with the standard version of TRACE. They used this failure as a reason to reject the TRACE model, and they attributed the problems in particular to the inter-word competition process in the model and to TRACE's use of phonemic representations.\n",
    "These cannot be the reasons why TRACE could not simulate Marslen-Wilson and Warren's data, however. Norris, McQueen and Cutler (in press) have shown that the Merge model, a model of phonemic decision-making which is integrated with Shortlist, can simulate the same data. Merge has both phonemic representations and inter-word competition.\n",
    "Merge and Shortlist differ from TRACE in a number of ways. TRACE incorporates feedback between processing levels, Merge and Shortlist do not. Furthermore, in Merge/Shortlist, in contrast to TRACE, the inter-word competition process produces a continuously optimal lexical parse of the input. We therefore decided to explore the reasons why TRACE could not simulate the Marslen-Wilson and Warren data. Using a small-scale version of TRACE we systematically altered features of the model and compared the performance of each version with the successful Merge simulation.\n",
    "In line with Marslen-Wilson and Warren, the small-scale version, unaltered, failed to simulate the pattern of findings correctly. It wrongly predicted a difference between the two types of cross-spliced word, and it grossly exaggerated the difference between the two types of cross-spliced nonword. However an adapted version of the model did produce an acceptable simulation. In effect, the more we made the adapted model resemble Merge, the better it performed. However, even the very best version never produced as close a simulation of the data as Merge did.\n",
    "The best-performing TRACE version differed from Merge principally by having feedback. Thus the presence of feedback could not be the reason for the failure of the simulation by the standard version of TRACE. Instead, the crucial feature which the TRACE adaptation required was the addition of the Shortlist optimisation procedure. There is no reason to prefer the model with feedback given that the model without feedback performs better, and given also that there are other data which crucially challenge the feedback assumption (Pitt & McQueen, 1998).\n",
    ""
   ]
  },
  "dahan00_swap": {
   "authors": [
    [
     "Delphine",
     "Dahan"
    ],
    [
     "James S.",
     "Magnuson"
    ],
    [
     "Michael K.",
     "Tanenhaus"
    ],
    [
     "Ellen M.",
     "Hogan"
    ]
   ],
   "title": "Tracking the time course of subcategorical mismatches on lexical access: Evidence for lexical competition",
   "original": "swap_067",
   "page_count": 4,
   "order": 17,
   "p1": "67",
   "pn": "70",
   "abstract": [
    "Marslen-Wilson and Warren (1994) found that lexical-decision latencies to cross-spliced word sequences whose initial CV portion had been excised from an existing word (e.g., jo(g)b) did not differ from latencies to word sequences excised from a nonword (e.g., jo(d)b). They interpreted this result as strong evidence against models incorporating lexical competition via lateral inhibition. McQueen, Norris, and Cutler (in press), who replicated this result, argued that lexical competition between 'jog' and 'job' is resolved before the activation level for the target sequence 'job' reaches the threshold that triggers lexical-decision responses. However, to simulate the lexical-decision data, their model depends on choosing a threshold within an extremely restricted range. Here, we argue that mean lexical-decision latencies may not be an appropriate measure of activation for the target 'job', because the activation of its competitor 'jog' also influences the probability of responding 'yes', making it difficult to relate lexical decisions to the underlying activation functions. By contrast, tracking eye movements can provide a continuous measure of lexical activation over time. Participants' eye movements to pictured objects were recorded as they followed instructions to click on one of the four objects (e.g., 'click on the net'). Allopenna, Magnuson, and Tanenhaus (1998) demonstrated that the proportion of fixations to each picture over time, as the target word is heard, can be mapped onto lexical-activation functions using a simple linking hypothesis.\n",
    "In the present study, the name of the target object was cross-spliced from another token of itself (ne(t)t), from another word (ne(ck)t), or from a nonword (ne(p)t). In Experiment 1, the competitor picture (neck) was displayed along with the target picture and two unrelated pictures. Fixations to the target over time indicated a fast rise in the ne(t)t condition, intermediate in the ne(p)t condition, and slowest in the ne(ck)t condition. The competitor picture was fixated most in the ne(ck)t condition, intermediate in the ne(p)t condition and least often the ne(t)t condition. The time course of fixations to the target and competitor pictures mirrored predictions generated from activations in a TRACE-style interactive activation model, using the Allopenna et al. (1998) linking hypothesis. Eliminating lexical competition significantly reduced the fit between the model and the data. In Experiment 2, the competitor picture was not displayed. Under these conditions, the model predicted that the time course of fixations to the target would be similar in the ne(p)t and ne(t)t conditions and slower in the ne(ck)t condition. The data closely matched these predictions. Finally, the model simulates the lexical-decision data across a range of response thresholds under the assumption that a 'yes' response is triggered probabilistically when either the target or competitor reaches threshold.\n",
    ""
   ]
  },
  "pitt00_swap": {
   "authors": [
    [
     "Mark A.",
     "Pitt"
    ],
    [
     "Lisa",
     "Shoaf"
    ]
   ],
   "title": "Beyond traditional measures of lexical influences on perception",
   "original": "swap_075",
   "page_count": 4,
   "order": 18,
   "p1": "75",
   "pn": "78",
   "abstract": [
    "Phoneme detection and identification tasks, and their more recent hybrids, have been the primary tools used to explore whether and how lexical memory influences earlier perceptual processes, with differences between words and pseudowords suggestive of such influences. Processing differences between words and pseudowords are also found in the Verbal Transformation Effect, which is a perceptual phenomenon in which continuous, rapid repetition of a word causes listeners to hear the word transform into other utterances. In this talk, the nature of this lexical effect will be described and two mechanisms responsible for verbal transformations will be discussed. The results tie in with recent work exploring lexical feedback and suggest that the influences of lexical memory extend to perceptual processes beyond those involved in phoneme perception.\n",
    ""
   ]
  },
  "frauenfelder00_swap": {
   "authors": [
    [
     "Uli H.",
     "Frauenfelder"
    ],
    [
     "Alain",
     "Content"
    ]
   ],
   "title": "Activation flow in models of spoken word recognition",
   "original": "swap_079",
   "page_count": 4,
   "order": 19,
   "p1": "79",
   "pn": "82",
   "abstract": [
    "Although current models generally agree that spoken word recognition involves the activation of a set of lexical competitors and the selection of the target word from this activated set, they differ in their view of the exact mechanisms underlying the selection process. Indeed, there are differences in how inappropriate competitors are eliminated from the activated set: bottom-up inhibition (Cohort), lateral inhibition (TRACE) and both (Shortlist). Moreover, the models also disagree in their assumptions concerning the role of top-down feed back in this process: existence of top-down feedback (TRACE) and no feedback (Cohort and Shortlist). Our research tests these divergent views with simulation and experimental studies.\n",
    "Two phoneme detection experiments examined how lexical candidates are activated and deactivated by partial phonological mismatches at onset and item internal positions. The first experiment on initial one-feature mismatches at word onset provided evidence for late lexical activation. This result was more consistent with Shortlist than TRACE simulation findings. In the second experiment the mismatching information arrived after some initial lexical activation. The results showed complete lexical deactivation by the mismatching information, a result consistent with bottom-up inhibition assumed in the Shortlist model and not with lateral inhibition. These results taken together with other findings in the literature suggest that the selection process is based on the combination of the two inhibitory mechanisms.\n",
    "Proponents of interactive models claim that top-down feedback facilitates word recognition, especially in cases where the input is defective. However, this claim about facilitatory feedback effects has been tested exclusively on phoneme recognition but not on word recognition. We evaluated this latter issue in a series of TRACE simulations in which we manipulated: the amount of phonological mismatch, lexicon size, and top-down feedback. Simulations showed that TRACE's recognition performance for small phonological mismatches was relatively poor (about 50%). This results suggest that, contrary to what is widely accepted, TRACE does not reliably recognize words with small initial mismatches. Moreover, when the parameter controlling the top-down feedback from word to phoneme levels was turned off, the recognition rate for both original and mismatch stimuli improved considerably with the larger lexica. These results suggest a detrimental rather than a beneficial effect of top-down feedback in word recognition. This paradox as well of the problems of scaling and parameters will be addressed.\n",
    ""
   ]
  },
  "vroomen00_swap": {
   "authors": [
    [
     "Jean",
     "Vroomen"
    ],
    [
     "Beatrice de",
     "Gelder"
    ]
   ],
   "title": "Lipreading and the compensation for coarticulation mechanism",
   "original": "swap_083",
   "page_count": 4,
   "order": 20,
   "p1": "83",
   "pn": "86",
   "abstract": [
    "Listeners compensate for coarticulatory influences of one speech sound on another. We examined whether lipread information penetrates this perceptual compensation mechanism. Experiment 1 replicated that when an /as/ or /a / sound preceded a /ta/-/ka/ continuum, more velar stops were perceived in the context of /as/ ([1]). Experiments 2 and 3 investigated whether the same phoneme boundary shift would be obtained when the context was lipread instead of heard. An ambiguous sound between /as/ and /a / was dubbed on the video of a speaker articulating /as/ or /a /. Subjects relied on the lipread information when identifying the ambiguous fricative sound, but there was no boundary shift in the following /ta/-/ka/ continuum. These results indicate that biasing of the fricative and compensation for coarticulation can be dissociated.\n",
    ""
   ]
  },
  "fowler00_swap": {
   "authors": [
    [
     "Carol A.",
     "Fowler"
    ],
    [
     "Lawrence",
     "Brancazio"
    ]
   ],
   "title": " Feedback in audiovisual speech perception",
   "original": "swap_087",
   "page_count": 4,
   "order": 21,
   "p1": "87",
   "pn": "90",
   "abstract": [
    "Lexical information affects identification of ambiguous phones (Ganong, 1980). Contrary to interactive models of auditory word recognition, recent evidence suggests that this effect is not due to feedback from a lexical to a phonemic processing layer. A phone ambiguous between /s/ and /S/ identified as /s/ following /dZu/ (making \"juice\" rather than \"joosh\") does not engender compensation for coarticulatiory effects of /s/ on a following /t/ or /k/ (Pitt & McQueen, 1998). However, compensation for coarticulation by phones identified bottom-up does occur. Because compensation is assumed to be prelexical, these results imply distinct phonological and lexical processing stages linked by a feedforward-only relation.\n",
    "Lexical information also affects phoneme identification when optical and acoustic information for speech gestures are discrepant (Brancazio, 1998): optical /d/ dubbed onto acoustic /b/ leads to fewer /b/ percepts preceding audiovisual /Ens/ (bense-dense) than preceding /EntS/ (bench-dench). Audio-visual integration also fosters compensation for coarticulation; a visual /l/ dubbed onto an ambiguous acoustic signal increases identification of a following /g/ (Fowler, Brown & Mann, in press). This suggests that audiovisual integration is prelexical.\n",
    "Our question now is where the lexical effect on audiovisual integration occurs. Does it occur at a lexical stage of processing after audiovisual integration produces bottom-up ambiguity? Or is the effect due to feedback from a lexical level of processing to one at which audiovisual integration and compensation for coarticulation occur?\n",
    "We are currently running experiments to address this issue. We are testing whether an audiovisually-induced compensation for coarticulation effect will be modulated by a lexical influence on audiovisual integration. A finding that the magnitude of the compensation effect shifts with lexically-induced variation in the visual influence in audiovisual integration would imply that lexical information feeds back onto audiovisual integration. A lexical effect on audiovisual integration that had no effect on compensation for coarticulation would be consistent with a feedforward-only account.\n",
    "Brancazio, L. (1998). Contributions of the lexicon to audiovisual speech perception. Ph.D. dissertation. University of Connecticut, Storrs, CT.\n",
    "Fowler, C. A., Brown, J. M., & Mann, V. A. (in press). Contrast effects do not underlie effects of preceding liquid consonants on stop identification in humans. Journal of Experimental Psychology: Human Perception and Performance.\n",
    "Ganong, W. F. (1980). Phonetic categorization in auditory word perception. Journal of Experimental Psychology, 6, 110-125.\n",
    "Pitt, M. A., & McQueen, J. M. (1998). Is compensation for coarticulation mediated by the lexicon? Journal of Memory and Language, 39, 347-370.\n",
    ""
   ]
  },
  "amano00_swap": {
   "authors": [
    [
     "Shigeaki",
     "Amano"
    ],
    [
     "Tadahisa",
     "Kondo"
    ]
   ],
   "title": "Neighborhood and cohort in lexical processings of Japanese spoken words",
   "original": "swap_091",
   "page_count": 4,
   "order": 22,
   "p1": "91",
   "pn": "94",
   "abstract": [
    "A word familiarity database (Amano & Kondo, 1999) was developed for about 70,000 spoken words to provide a basis for a psycholinguistic research in Japanese. Using this database, analyses and experiments were conducted on two lexical competitor sets, the neighborhood and the cohort, to examine their validity in lexical processings of Japanese spoken words. Firstly, the characteristics of the neighborhood and the cohort were analyzed in terms of their descriptive variables such as density (i.e., size), mean familiarity, maximum familiarity, sum of familiarity, and the uniqueness point as a function of a word familiarity. Correlation between the descriptive variables were also analyzed. Secondly, to investigate effects of the neighborhood and the cohort on Japanese spoken word recognition, a lexical decision experiment and a word recognition experiment were conducted using two sets of 100 spoken words which were selected from the database. The familiarity scores of the word stimuli ranged between middle (4.0) and high (7.0). These words and 100 nonwords were randomly presented to 30 Japanese subjects through headphones in the lexical decision experiment. The subjects were instructed to press one of two keys as soon as possible according to their judgment of whether they heard a word or a nonword. Reaction times were measured from the beginning of the stimulus. Partial correlation analyses between there action time and their descriptive variables excluding a factor of target word familiarity showed that there are no significant neighborhood effects or cohort effects on the reaction time. In the word recognition experiment, the words were randomly presented to 40Japanese subjects without noise or with noise at signal-to-noise ratios of -5, -2.5, 0, and 2.5 dB through headphones. The subjects were instructed to type what they heard using a computer keyboard. Recognition scores were obtained by dividing the number of correct answers by the total number of answers. Partial correlation analyses showed that there are significant neighborhood effects but no cohort effects on recognition scores. The results of two experiments showed that only the neighborhood affects the spoken word recognition in Japanese at least in a kind of 'unspeeded' task. It is suggested that the neighborhood is more plausible than the cohort as a lexical competitor set, and that some amount of time is necessary for the neighborhood to be activated and effective in lexical processing of spoken word recognition.\n",
    ""
   ]
  },
  "brink00_swap": {
   "authors": [
    [
     "Dannie van den",
     "Brink"
    ],
    [
     "Colin",
     "Brown"
    ],
    [
     "Peter)",
     "Hagoort"
    ]
   ],
   "title": "The N200 as an electrophysiological manifestation of early contextual influences on spoken-word recognition",
   "original": "swap_095",
   "page_count": 4,
   "order": 23,
   "p1": "95",
   "pn": "98",
   "abstract": [
    "In everyday speech, words are usually processed in the context of other words. In the literature on language comprehension there is evidence to suggest that contextual influences play a role in the on-line recognition of spoken words. Recently, a number of researchers have attempted to investigate the time-course of contextual influences on spoken-word recognition with a technique that involves the use of event-related brain potentials (Connolly & Phillips, 1994; Van Petten, Coulson, Rubin, Plante, & Parks, 1999). This experimental technique yields a high temporal resolution and, in addition, does not require the subject to perform a task other than listening to spoken language. Our study extends the two studies mentioned above, and was aimed at finding electrophysiological correlates of spoken-word processes that are influenced by contextual information. In addition to the often reported N400 component, that reflects lexical-semantic integration processes, we obtained a negative component that preceded the N400. This component, the N200, could be an indicator of a lexical selection process, where word-form information resulting from an initial phonological analysis\n",
    ""
   ]
  },
  "goswami00_swap": {
   "authors": [
    [
     "Usha",
     "Goswami"
    ],
    [
     "Bruno de",
     "Cara"
    ]
   ],
   "title": "Lexical Representations and Development: The Emergence of Rime Processing",
   "original": "swap_099",
   "page_count": 4,
   "order": 24,
   "p1": "99",
   "pn": "102",
   "abstract": [
    "We were interested in whether phonological awareness in young children emerges primarily as a result of lexical restructuring processes argued to be an integral part of language acquisition (Metsala & Walley, 1998). 'Lexical restructuring theory' proposes that segmental representations emerge primarily as the result of spoken vocabulary growth and associated changes in the familiarity of individual lexical items and inter-item phonological similarity relations. However, lexical restructuring theory as currently stated does not consider the nature of the phonological neighbours in the child's lexicon. For example, rime neighbours like pot, onset-vowel or lead neighbours like lock, and consonant neighbours like lit, are all considered equal neighbours of a target word like lot. However, given the abundant data demonstrating the psychological salience of the rime to young children (Goswami & Bryant, 1990, for review), it may be that many phonological neighbours in English are rime neighbours. If a prevalence of rime neighbours characterises the English phonological lexicon, then this might contribute to the developmental salience and utility of onset-rime representations in English.\n",
    "We could find no analyses of phonological neighbourhoods in English in terms of type of phonological neighbour. We therefore analysed the corpus of single-syllable words in the Luce & Pisoni (1 998) database in terms of rime neighbours, onset-vowel or lead neighbours, and consonant neighbours in dense versus sparse neighbourhoods respectively, by type. This analysis showed that rime neighbours predominate in dense neighbourhoods. We then examined the effects of two factors on the development of rime awareness in young children, phonological neighbourhood density and sonority profile. Our experiment followed a 2 x 2 design, crossing sonority profile (good, poor) with neighbourhood density (dense, sparse). Five- and 6year-old children were given two tasks to measure awareness of the rime, the 'oddity' task and the same-different judgement task. The former measured awareness of the rime (e.g., pit, hit, got / meat, weak, seat / peak, dot, not), and the latter awareness of both rime (e.g., pit, hit) and coda (pit, got). In each task, simple CVC monosyllabic stimuli from high and low neighbourhoods were contrasted, as were stimuli with 1 good' or 'poor' sonority profiles. We found significant effects of both neighbourhood density and sonority profile on rime-level processing. Children were more accurate at making rime judgements about words in dense neighbourhoods and about words with 'poor' sonority profiles. However, there was no interaction between these factors. The results are interpreted with respect to developmental 'lexical restructuring theory' (Metsala & Walley, 1998).\n",
    ""
   ]
  },
  "rodd00_swap": {
   "authors": [
    [
     "Jennifer M.",
     "Rodd"
    ],
    [
     "M. Gareth",
     "Gaskell"
    ],
    [
     "William D.",
     "Marslen-Wilson"
    ]
   ],
   "title": "Semantic ambiguity in spoken word recognition",
   "original": "swap_103",
   "page_count": 4,
   "order": 25,
   "p1": "103",
   "pn": "106",
   "abstract": [
    "Different models of word recognition make contrasting predictions about the effects of semantic ambiguity. These predictions are critically influenced by the nature of the representations that are involved. Models in which words are recognised as familiar distributed patterns of semantic activation predict that, for ambiguous words (e.g. bark), interference between their meanings should delay their recognition relative to unambiguous words. In contrast, models that represent words as abstract nodes may predict an advantage for ambiguous words that have multiple entries in the race for recognition.\n",
    "Surprisingly, the effect of semantic ambiguity on the recognition on individual spoken words has not to our knowledge been investigated. However, reports in the visual domain of faster lexical decisions for high-ambiguity words are problematic for distributed semantic models. More recently, Rodd, Gaskell & Marslen-Wilson (1999) have shown that such findings reflect a confound between lexical ambiguity and word senses. They report a disadvantage for ambiguous words when this confound is removed.\n",
    "Here, we establish that this distinction between word meanings and word senses is also important in spoken word recognition. Two auditory lexical decision experiments found slower responses for ambiguous words (e.g. bark) than for unambiguous words (e.g. frog). We argue that this ambiguity disadvantage is the result of semantic competition between the meanings of ambiguous words. Further, responses to words with many senses (e.g. slide) were faster than for those with few senses (e.g. shirt). This can be explained by assuming that words with many senses are associated with a large store of readily available semantic or contextual information. Alternatively, it may reflect a difference in the structure of the attractor basins that develop within distributed semantic representations. The attractor basins associated with words with multiple senses will be broad and shallow, and contain more than one stable state. It is possible that settling into the correct attractor may be quicker for such broad attractors, or that the multiple stable states within the attractor may lead to faster settling times.\n",
    "These contrasting effects, of an ambiguity disadvantage and of a multiple sense advantage, support models of spoken and visual word recognition which employ distributed semantic representations, and they pose problems for recognition models that do not actively involve semantic representations in the process of lexical access and selection.\n",
    "Rodd, J.M., Gaskell, M.G., and Marslen-Wilson, W.D. (1999). Semantic Competition and the Ambiguity Disadvantage. In M. Hahn & S. C. Stoness (Eds.), Proceedings of the Twenty First\n",
    "Annual Conference of the Cognitive Science Society (pp. 608-613). Mahweh, New Jersey: Lawrence Erlbaum Associates.\n",
    ""
   ]
  },
  "tanenhaus00_swap": {
   "authors": [
    [
     "Michael K.",
     "Tanenhaus"
    ],
    [
     "James S.",
     "Magnuson"
    ],
    [
     "Bob M.",
     "McMurray"
    ],
    [
     "Richard N.",
     "Aslin"
    ]
   ],
   "title": "Evidence from research with an artificial lexicon",
   "original": "swap_107",
   "page_count": 4,
   "order": 26,
   "p1": "107",
   "pn": "110",
   "abstract": [
    "Phenomena in spoken language processing that can be explained using representations\n",
    "at one level of processing often have an alternative explanation based on distributional patterns of units at a lower level of analysis. The absence of (a) good corpora for spoken language and (b) good theories of the relevant distributional statistics makes it difficult to distinguish among these alternatives. Research with artificial languages offers a promising methodological tool, allowing for precise control over the properties of the input. After demonstrating that it is feasible to explore word recognition using an artificial lexicon, we use an artificial language to examine whether lexical feedback affects perception in compensatory co-articulation as proposed by Elman and McClelland (1988) or whether all compensatory effects can be explained by probabilistic phonotactics as proposed by Pitt and McQueen (1998).\n",
    "We first present research by Magnuson and colleagues in which participants learned to associate names with novel shapes in a forced choice training paradigm with feedback. A 16 word artificial lexicon was constructed so each word had a cohort competitor (e.g., pibu had pibo) and a rhyme competitor (e.g., pibu had dibu) The frequency of presentation of the words and their competitors varied systematically during training. During testing, eye movements were measured as subjects followed the same type of instructions used in training. After less than two hours of training, subjects were processing the words incrementally, making eye movements to the target shape before the end of the word. We found frequency effects, cohort and rhyme effects, and neighborhood density effects that closely match those found with real words.\n",
    "We then report ongoing research that is examining perceptual effects of compensatory co-articulation using a richer artificial language. The language includes eight nouns, each associated with a novel shape and eight adjectives, each associated with a different texture. The crucial stimuli are adjectives ending in s or esh followed by nouns beginning with k or t. The words are synthesized to preserve the acoustic pattern associated with compensatory co-articulation in natural English. The test stimuli are synthesized so that some of the adjectives end in an ambiguous s/esh and the nouns vary along a five-step t/k continuum. The participants task is to click on the appropriate shape, among four alternatives.\n",
    "The language is constructed to allow for a clear test of whether lexical status has effects on compensatory co-articulation above and beyond effects that can be attributed to probabilistic phonotactics.\n",
    ""
   ]
  },
  "luce00_swap": {
   "authors": [
    [
     "Paul A.",
     "Luce"
    ],
    [
     "Nathan R.",
     "Large"
    ]
   ],
   "title": "Do spoken words have attractors?",
   "original": "swap_111",
   "page_count": 4,
   "order": 27,
   "p1": "111",
   "pn": "114",
   "abstract": [
    "Previous research has demonstrated that increases in probabilistic phonotactics facilitate spoken word processing (e.g., Pitt & Samuel, 1995; Vitevitch & Luce, 1999), whereas increased competition among lexical representations is typically associated with slower and less accurate recognition (e.g., Luce & Pisoni, 1998). We examined the combined effects of probabilistic phonotactics and lexical competition by generating words and nonwords that varied orthogonally on phonotactics and similarity neighborhood density. As predicted, the results revealed simultaneous facilitative effects of phonotactics and inhibitory effects of lexical competition. However, one anomalous result emerged: Certain stimuli with high probability phonotactics and low neighborhood density were processed far more slowly than predicted by a simple model in which effects of phonotactics and density are additive. Using a speeded task in which we asked participants to produce the nearest sound-based neighbor of a spoken target item, we tested the hypothesis that items with high sublexical frequencies but few lexical neighbors have strong lexical attractors that may control processing times under certain specific circumstances. Our results are not consistent with models of spoken word recognition in which degree of lexical competition is a simple function of the weighted sum of the activations of lexical competitors, nor are our results consistent with a model in which a single, highest-frequency neighbor is responsible for all competition effects. Instead, lexical competition seems to be a complex function of the weighted activations of the lexical and sublexical representations associated with the target and its competitors.\n",
    ""
   ]
  },
  "connine00_swap": {
   "authors": [
    [
     "Cynthia M.",
     "Connine"
    ]
   ],
   "title": "The time course of lexical activation: Sequential constraint, co-articulatory preview and additional processing time",
   "original": "swap_115",
   "page_count": 4,
   "order": 28,
   "p1": "115",
   "pn": "118",
   "abstract": [
    "Three experiments are reported that examine the manner in which lexical information unfolding over time modulates lexical activation. Three word lengths (one, two and three or more syllables) were used to manipulate the degree of constraint afforded the word-final target phoneme.\n",
    "An additional manipulation (developed in Experiment 1) permits a measure of changes in activation given additional processing time where a silent interval is introduced prior to the target phoneme. The silent interval facilitates phoneme monitoring reaction time and provides a sensitive measure of the time course of lexical activation. Experiment 2 manipulates degree of activation via mismatching segments and shows that information subsequent to the mismatch is utilized to modulate activation in a manner dependent on word length and the availability of additional processing time. Experiment 3 examines the time course of co-articulatory effects as a function of word length and processing time. Taken together, the results show a dynamic use of information over time where information across the extent of a word contributes to lexical activation. Complimentary to this, activation of lexical representations is not statically tied to acoustic-phonetic input but benefits from additional processing time.\n",
    ""
   ]
  },
  "shillcock00_swap": {
   "authors": [
    [
     "Richard",
     "Shillcock"
    ]
   ],
   "title": "Spoken word access: evidence from statistical analyses of the lexicon",
   "original": "swap_119",
   "page_count": 4,
   "order": 29,
   "p1": "119",
   "pn": "122",
   "abstract": [
    "Spoken word access is typically investigated by means of on-line experimentation, yet the lexicon itself contains a wealth of structure from which we can make inferences about processing. I review existing studies of the statistical structure of the English lexicon and conclude the following: (a) the lexicon is shot through with partial but statistically significant correspondences between form and meaning/function, reflecting the brain's predisposition for creating topographic mappings; (b) constraints within the phonology of English concerning the ordering of segments reflect the demands of incremental processing on the part of the listener; (c) conversational ­ \"fast\" ­ speech involves phonological changes that accentuate important aspects of the structure of the lexicon and protect the intelligibility of speech. Regarding the vexed issue of whether or not top-down processing exists in speech perception, I will argue that statistical studies of the lexicon can complement on-line experimentation and that they are at least consistent with a genuine flow of information from \"higher\" to \"lower\" levels of representation and processing.\n",
    ""
   ]
  },
  "pierrehumbert00_swap": {
   "authors": [
    [
     "Janet",
     "Pierrehumbert"
    ]
   ],
   "title": "Why phonological constraints are so granular",
   "original": "swap_123",
   "page_count": 4,
   "order": 30,
   "p1": "123",
   "pn": "126",
   "abstract": [
    "The most common word length in the lexicon lies in the middle of the total range. The shortest words -- light V or CV monosyllables -- are necessarily few because a cross-product of the consonantal and vocalic phonemes generates only a small number of combinations. As length increases, the number of possible forms explodes, but fewer and fewer are actually used in any given language. Experimental and computational studies relate the sparsity of long forms to the fact that the likelihood of forms as determined by a stochastic parse decreases with length. This effect occurs because long forms have more subparts than short ones, and the likelihood of any given subpart is always less than 1.0. (Coleman and Pierrehumbert 1997, Frisch et al in press). The disadvantage that long forms have in achieving a high well-formedness score is a distinct phenomenon from the tendency of individual long words to have low token frequencies (though there may of course be some deep relationship between these characteristcs). In English, a morphologically impoverished language, the most common type of word is the disyllable.\n",
    "This paper undertakes to relate the distribution of word lengths in the lexicon to the surprising simplicity of phonological constraints. A considerable body of results shows that people have implicit knowledge of phonological constraints which is projected from the lexicon. Experiments have revealed implicit knowledge of syllabic onsets and rhymes, syllable junctures, OCP-Place, vowel harmony, and foot structure. This knowledge originates in the lexicon in the sense that it shows systematic and gradient dependencies on the lexical statistics of particular languages. It is \"projected from\" the lexicon in the sense that it appears to be coarse-grained in comparison to the set of all possible epiphenomenal regularities which could in principle arise in the lexicon. Phonological constraints are formally shorter than the largest phonological objects which people can encode and remember. Phonological constraints are also cruder than would be predicted by purely phonetic factors, as discussed in recent work by Hayes and Hyman. And long-distance constraints -- such as vowel harmony -- tend to be cruder than local constraints.\n",
    "Why don't people form long constraints on the basis of the many long words that they know? Why don't they have far more numerous and fine-grained constraints, reflecting the detailed phonetic knowledge that informs individual acts of speech production and perception? In this paper, we undertake to evaluate the contribution of three interacting assumptions to the answer to these questions.\n",
    "Phonological generalizations are statistically trained over the lexicon.\n",
    "Phonological generalizations need to be shared by a speech community, because they influence productive behaviour crucial to communicative success, ranging from signal parsing during speech perception to assimilation of neologisms and loan words.\n",
    "Different members of the speech community have different lexicons; the lexicon of an individual can be roughly viewed as a random frequency- weighted downsampling of a large community dictionary such as CELEX.\n",
    "In combination, these assumptions imply that the viable phonological generalizations are those which are statistically stable under downsampling of the dictionary. That is, independent downsamples -- reflecting different personal experiences of lexical acquisition -- should lead to essentially comparable estimates of the statistical strength of the constraint. Calculations of the rate of degradation of various possible constraints for English under slight to severe downsampling of CELEX will be presented in connection with this argument. Coarse generalizations over medium length words emerge as particularly viable due to the large number and high frequency of the medium length words.\n",
    ""
   ]
  },
  "dumay00_swap": {
   "authors": [
    [
     "Nicolas",
     "Dumay"
    ],
    [
     "Uli H.",
     "Frauenfelder"
    ],
    [
     "Alain",
     "Content"
    ]
   ],
   "title": "Acoustic-phonetic cues and lexical competition in segmentation of continuous speech",
   "original": "swap_127",
   "page_count": 4,
   "order": 31,
   "p1": "127",
   "pn": "130",
   "abstract": [
    "Since the pioneering work by Durand (1936), several studies of the French \"enchaînement\" phenomenon have isolated subtle phonetic contrasts that contribute to the segmentation of lexically ambiguous strings. Word-strings with enchaînement like \"tante roublarde\" have been shown to differ from \"temps troublant\" both in the perceptual duration attributed to the pre-boundary vowel (as a function of pitch factors), and in the articulatory properties of the following consonants (Delattre 1940; Malmberg 1971). Using lexically ambiguous sequences (e.g. t (#)t(#)ru) extracted from similar pairs of two-word strings (cf. \"temps troublant\" vs. \"tante roublarde\"), we recently showed that both the production and the perception of differences in such pairs depended upon the cluster class (Dumay et al. 1999). In obstruent-liquid but not in /s/-obstruent clusters, the pre-boundary vowel and the cluster second consonant were found to be longer when the word-boundary was located within the cluster. Moreover, word-spotting experiments demonstrated consistent initial (V#CCVC) as well as final (CV#CCV) misalignment effects between the CVC target and the intended word-boundary, in obstruent-liquid clusters only. The present research explored the relation between the effect of final misalignment and the activation of overlapping lexical competitors that begin at the second syllable onset. Two experiments were performed in which respectively CVC (t t) or CV (t ) initially-embedded words had to be spotted in aligned or misaligned CV(#)C(#)CV sequences. In one condition (t #tru vs. t t#ru), the CCV final portion (tru) of the carrier was a lexical competitor of the CVC target due to obstruent overlap. In another condition, the CCV (trø) and CV (rø) final portions of the carrier were nonwords (t #trø from \"temps treublant\" vs. t t#rø from \"tante reublarde\"). Spotting the CVC words showed an alignment effect only when a competitor was present at the following onset (t #tru). No effect was obtained in the absence of competitor (t #trø). Spotting the CV words showed the reverse pattern of results: the alignment effect was found when the CCV portion was a nonword (t t#rø) but not when it was a word (t t#ru). The re-assignment of the obstruent to a misaligned CVC target seems thus harder when the obstruent already supports another lexical hypothesis. Similarly, the rejection of the obstruent to overcome the misalignment of a CV target seems easier when the obstruent can form a word with the rest of the sequence. We conclude that the location of word-boundaries in ambiguous sequences is tied to both the nature of competing lexical hypotheses and to word-boundary acoustic-phonetic cues which modulate the activation level of lexical candidates.\n",
    "Delattre, P. (1940). Le mot est-il une entité phonétique en français? Le Français Moderne, 8 (1), 47-56.\n",
    "Dumay, N., Content, A., & Frauenfelder, U.H. (1999). Acoustic-phonetic cues to word boundary location: evidence from word-spotting. In Proceedings of the International Congress of Phonetic Sciences, (pp. 281-284), San Francisco, CA, USA.\n",
    "Durand, M. (1936). Le genre grammatical en français parlé Paris et en région parisienne. Bibliothèque du Français Moderne, Paris.\n",
    "Malmberg, B. (1971). Phonétique générale et romane: Etudes en allemand, anglais, espagnol et français. The Hague/Paris, Mouton.\n",
    ""
   ]
  },
  "kirk00_swap": {
   "authors": [
    [
     "Cecilia",
     "Kirk"
    ]
   ],
   "title": "Syllabic cues to word segmentation",
   "original": "swap_131",
   "page_count": 4,
   "order": 32,
   "p1": "131",
   "pn": "134",
   "abstract": [
    "The Shortlist model of spoken word recognition proposes that the lexical competition process is modulated by word boundary information. Listeners do not assume that words can begin at any point in the input, but instead they use their knowledge of what constitutes a possible word in their language to limit the number of competitors (Norris et al., 1997). It has been shown that listeners use metrical structure (Cutler et al., 1988) and phonotactic knowledge (McQueen, 1998) to select the most likely locations for word boundaries. The present study uses the wordspotting task to investigate the role of the syllable in the segmentation of English for those cases where syllabification is not dictated by phonotactics.\n",
    "Listeners appear to follow a strategy of maximizing the onset when segmenting the input. The word smooth was spotted much more quickly and accurately when embedded in voosmooth than spotting of neat embedded in voosneat. Wordspotting in the latter case is more difficult since onset maximization ensures that the syllable boundary is misaligned with the word boundary even though there is no phonotactic requirement that s and the following nasal be syllabified together. The mean percentage of missed targets and mean reaction times for each of the five hypotheses under investigation are summarized below.\n",
    "Another segmentation strategy used by English listeners is the attraction of consonants to stressed syllables. A consonant which occurs with a vowel on either side is parsed as belonging to an adjacent stressed syllable. Listeners found it easier to spot the word edge embedded in ZEENedge than when embedded in zeeNEDGE (upper case indicates stress).\n",
    "The devoicing of a glide following an aspirated stop in the same syllable gives an important cue to a speaker's intended syllabification. If wine is preceded by k syllabified together with the glide in the onset, as in zee.khwine, it is more difficult to spot than when the k is syllabified as a coda and no devoicing occurs.\n",
    "When alveolar stops occur together with [r] in the syllable onset, they are very commonly pronounced with a strongly retroflexed articulation. This retroflexion provides important information about possible word boundaries. The word rock is much more difficult to spot when the preceeding d in fidrock is retroflexed than when it is not.\n",
    "Words must contain at least one vowel, and the rime of English monosyllabic words must contain either a tense vowel or a lax vowel and coda consonant. This restriction on possible words in English causes the lax vowel in the first syllable of vesland to attract the s to the coda resulting in alignment of both the syllable and word boundary. Listeners were faster to spot land when embedded in vesland than when embedded in voosland, although they were not more accurate.\n",
    "These findings indicate that sensitivity to syllabic structure, in addition to knowledge about phonotactics and metrical structure, helps to restrict the list of candidates entered into the competition process.\n",
    ""
   ]
  },
  "lugt00_swap": {
   "authors": [
    [
     "Arie van der",
     "Lugt"
    ]
   ],
   "title": "The time-course of competition",
   "original": "swap_135",
   "page_count": 4,
   "order": 33,
   "p1": "135",
   "pn": "138",
   "abstract": [
    "Previous studies have shown that competition is a central mechanism of spoken word recognition. Using identity priming, Vroomen and De Gelder (1995) demonstrated that visual lexical decision to the Dutch word MELK (milk) was slower after hearing melkaam than after hearing melkeum. This difference was interpreted as evidence for competition. In Dutch, there are many words that start with kaa(m) but only few words that start with keu(m). If the end of the speech input is consistent with many words, lexical decision to an overlapping competitor (e.g. melk) is less facilitated than when the final part is consistent with only a few words. Other evidence for lexical competition was obtained by McQueen, Norris and Cutler (1994) for English using word spotting. They found that it is harder to spot a word like mess in /d« mE s/, which is the beginning of domestic, than in /n« mE s/, which is not the beginning of an English word. Similar materials were here evaluated in two identity priming experiments, that differed only in the relative timing of the visual stimulus. In the critical conditions spoken word fragment primes overlapped either with the first two syllables of a longer real Dutch word or overlapped in all but the first phoneme. In Experiment 1, with the visual target presented at the offset of the auditory stimulus, a facilitatory effect was observed for both the overlapping and the mismatching conditions. In Experiment 2, the visual target was presented at the beginning of the embedded word, and no priming was found for any of the conditions. In short, these experiments find no differential effect of competition. This is somewhat unexpected given the results of McQueen et al. (1994), and those of Vroomen & de Gelder (1995). The results are interpreted in the context the SHORTLIST model (Norris, 1994). Specifically, the absence of lexical inhibition is explained in terms of the time-course of the competition process. The first experiment likely taps in too early and the second experiment likely taps in too late to observe competition effects.\n",
    "References  McQueen, J. M., Norris, D. & Cutler, A. (1994). Competition in spoken word recognition: Spotting words in other words. Journal of Experimental Psychology: Learning, Memory and Cognition, 20, 621-638.\n",
    "Norris, D. (1994). Shortlist: A connectionist model of continuous speech recognition. Cognition, 52, 189-234.\n",
    "Vroomen, J. & de Gelder, B. (1995). Metrical segmentation and lexical inhibition in spoken word recognition, Journal of Experimental Psychology: Human Perception and Performance, 21, 98-108.\n",
    ""
   ]
  },
  "smith00_swap": {
   "authors": [
    [
     "Rachel",
     "Smith"
    ],
    [
     "Sarah",
     "Hawkins"
    ]
   ],
   "title": "Allophonic influences on word-spotting experiments",
   "original": "swap_139",
   "page_count": 4,
   "order": 34,
   "p1": "139",
   "pn": "142",
   "abstract": [
    "The key role of prosody and phonotactics in word segmentation is well established; allophonic detail, acknowledged to play a part, has received less attention (Norris et al., 1997). Yet allophones, as positional variants of phonemes, convey prosodic and phonotactic information that should guide listeners. This work describes a first attempt to assess the role of allophonic detail in segmentation: in a word-spotting paradigm broadly following McQueen (1998), one critical allophone (CA) of the target word to be spotted was either appropriate or inappropriate for that word in citation form.\n",
    "Seventy-two monosyllabic target words were embedded in pairs of nonsense contexts, one member containing the appropriate CA, and the other the inappropriate one. 26 \"Onset\" stimuli had targets in syllable 2, and CAs at syllable onset, e.g. reef in [vI\"ri:f] (Appropriate) and [vI\"kri:f] (Inappropriate). 16 \"Coda\" stimuli had targets in syllable 1, and CAs at syllable coda, e.g. bell in [\"bELp@sIm]* (Appropriate) and [\"bEl@sIm] (Inappropriate). \"Morpheme\" stimuli comprised a target word and a pseudomorphemic affix: 20 \"Coda\" (ill in [\"ILl@s]* (Appropriate) and [\"Il@s] (Inappropriate)) and 10 \"Onset\" (pip in [dIs\"phIp] (Appropriate) and [dI\"spIp] (Inappropriate)). Thirty listeners pressed a button upon spotting words and said the words aloud. Allophonic appropriateness interacted significantly with stimulus category in ANOVAs on reaction times and errors. Responses were slower and less accurate to Inappropriate than Appropriate allophones in \"Onset\" and \"Morpheme\" stimuli, but no difference was found with \"Coda\" stimuli.\n",
    "Do allophones affect segmentation independently of the prosodic and phonotactic information they carry? The largest effects confounded allophones with phonological structure: targets in stimuli like [vI\"kri:f] had to be extracted from syllables with complex onsets; stress was not varied, as target syllables were strong and nonsense contexts weak. Nonparametric tests on subsets of the data indicate that allophonic detail was important despite these confounds. Firstly, words seemed harder to spot when the CA in onset clusters was devoiced (e.g. /pl/ versus /bl/). Secondly, allophonic appropriateness affected \"Coda Morpheme\" items, which did not cluster CA with tautosyllabic consonants. Ongoing work will be presented which aims to disentangle metrical and allophonic factors, e.g. by varying allophones in weak syllables. Implications will be discussed with regard to phoneme-free processing, and episodic versus abstractionist approaches to sound categorisation.\n",
    "Note: L designates dark /l/.\n",
    "McQueen, J.M. (1998). Journal of Memory and Language, 39, 21-46.\n",
    "Norris, D., J.M. McQueen, A. Cutler and S. Butterfield (1997). Cognitive Psychology, 34, 191-243.\n",
    ""
   ]
  },
  "weber00_swap": {
   "authors": [
    [
     "Andrea",
     "Weber"
    ]
   ],
   "title": "Native language phonotactics and nonnative language segmentation",
   "original": "swap_143",
   "page_count": 4,
   "order": 35,
   "p1": "143",
   "pn": "146",
   "abstract": [
    "Spoken utterances are continuous and contain few reliable word boundary cues for the listener. To understand a spoken utterance a listener must isolate individual words from the speech stream. This lexical segmentation process is influenced by competition between candidate words and knowledge of native phonology (such as knowledge about phonotactic constraints). A recent study using the word spotting task has shown that Dutch listeners find it easier to detect a word at the end of a nonsense sequence when the beginning of the word is aligned with a syllable boundary than when it is misaligned.\n",
    "Adult listeners are language-specific perceivers. Therefore speech segmentation differs across listeners as a function of their native language. Whereas the sequence /sm/ may cue a clear syllable boundary for German listeners due to phonotactic constraints, the same sequence may not be a clear boundary for English listeners.\n",
    "The present study seeks to examine the effects of native phonotactic constraints on segmentation of a nonnative language. Since phonotactic cues help to solve the segmentation problem in native listening, and processing of a nonnative language is influenced by the native language, one wonders how nonnative segmentation is influenced by native phonotactic alignment or misalignment.\n",
    "This question is being addressed using the word spotting task in a study of English and German. In /biS leg/, leg is aligned with a syllable boundary by English phonotactics, but not by German. In /kitleg/, leg is aligned for both languages. If German listeners respond more slowly to leg in the former, this would show that listeners make use of their native phonotactic constraints in segmenting a nonnative language. If German listeners are more slowly to detect an English word which is aligned with a clear syllable boundary according to German phonotactics (but not according to English phonotactics, e.g., leg in /tusleg/) than an English word which is aligned with syllable boundaries defined by the phonotactics of both languages (e.g., leg in /kitleg/), this would show that listeners make use of phonotactic constraints of their learned nonnative language when segmenting it.\n",
    "The results of the present study can confirm the role of phonotactics in segmenting continuous speech and shed light on the how nonnative segmentation is influenced by both native and nonnative phonotactic constraints.\n",
    ""
   ]
  },
  "whalen00_swap": {
   "authors": [
    [
     "Doug H.",
     "Whalen"
    ]
   ],
   "title": "Occam's Razor is a double-edged sword: Reduced interaction is not necessarily reduced power",
   "original": "swap_147",
   "page_count": 4,
   "order": 36,
   "p1": "147",
   "pn": "150",
   "abstract": [
    "Although Norris, McQueen and Cutler have provided convincing evidence that there is no need for contributions from the lexicon to phonetic processing, their simplification of the communication between levels comes at a cost to the processes themselves. While their arrangement may ultimately prove correct, its validity is not due to a successful application of Occam's Razor. The kind of statistical accumulation that they propose also presupposes that there are units of a particular size that are the sites for those accumulations. The units could be segments, but the perceptual effects of units of other sizes would perhaps lead to the expectation that frequencies could accumulate at any of those levels. The evidence from the lexical access literature is lacking on this point, since the evidence for accumulation at the segmental level is quite recent. But we would expect listeners to be sensitive to the statistical properties of any unit that they used perceptually. These considerations can also be contrasted with the evidence that listeners parse the signal into the various phonetic units in a fairly complete way, without strict left-to-right processing. If the results of that parsing are the units at which the statistical properties accumulate, then the parsing must be complete before the statistical likelihoods can have an effect, or the statistical likelihoods are part of the parsing itself. The first possibility seems to require yet another level between speech perception and lexical access; the second possibility requires a far more complex version of speech perception than we are typically used to. At this point, it is only possible to make preliminary suggestions for a resolution.\n",
    ""
   ]
  },
  "kingston00_swap": {
   "authors": [
    [
     "John",
     "Kingston"
    ]
   ],
   "title": "Context effects on sensitivity and response bias",
   "original": "swap_151",
   "page_count": 4,
   "order": 37,
   "p1": "151",
   "pn": "154",
   "abstract": [
    "The massive acoustic redundancy with which minimal contrasts are conveyed and the perceptual integration of these acoustic properties ensure that the listener can identify the intended speech sound from bottom-up information alone most of the time (Kingston, 1991; Kingston & Diehl, 1994, 1995; Kingston & Macmillan, 1995; Kingston, et al., 1997; Macmillan, et al., 1999). The identification of speech sounds is, however, systematically distorted by context (Repp, 1982). These distortions are all contrastive. That is, if the context sound lies at one extreme of an acoustic continuum, the listener gives more responses corresponding to the category at the opposite extreme of the continuum. The criterion shifts both when the extreme value of the acoustic property that varies continuously in the target sound can actually be heard in context sound and when it can only be predicted. When the extreme value of the acoustic property is audible in the context, it creates the contrast regardless of whether it occurs in speech sounds (Mann, 1980, Mann & Repp, 1981) or acoustically analogous nonspeech sounds (Lotto & Kluender, 1998). And when the extreme value can only be predicted, it creates the contrast regardless of whether it's predicted from simultaneous visual information (Fowler, et al., 1999), transitional probabilities (Pitt & McQueen, 1998), or stimulus blocking (Ohala & Feder, 1994; Bradlow & Kingston, 1990).\n",
    "The audible context effects probably have a different source than the predictable ones. When the extreme value of the acoustic property is audible in the context, it interacts auditorily with its value in the target sound, such that the listener literally hears a contrastive different value in the target. But when the extreme value in the context is predictable, the listener hears the same value in the target sound as when the context isn't present, but shifts the criterion to take into account simultaneous visual, statistical, or inferential information. Audible context effects therefore change the listener's sensitivity as well as response bias to differences in the acoustic property being judged, but predictable context effects change only response bias.\n",
    "We have recently devised a quantitative model relating sensitivity and bias measures in the fixed classification and trading relations tasks that we can apply to studying these two kinds of context effects (Macmillan, et al., 1999). In the talk I will give at SWAP, I will present the results of an ongoing study comparing the effects of audible vs predictable extreme context values on a well-studied case, the judgment of /t-k/ in the context of /s: /. Two properties that are known to be acoustic and perceptual correlates of the stop place contrast, spectral compactness and transition duration, are varied orthogonally, and listeners both categorize and discriminate the stimuli. Four preceding context conditions are compared, two audible and two predictable. Audible: (1) speech /s/ vs / /, (2) nonspeech, dense, inharmonic tone complexes with the same center frequencies and bandwidths as /s/ and / /. Predictable: The fricative is ambiguous between /s/ and / / but predictable from (3) transitional probabilities between it and the preceding vowel or (4) from the quality of the fricative in the immediately preceding three trials. Conditions (1) and (3) replicate and extend the studies by Mann & Repp and Pitt & McQueen, respectively; condition (2) extends the study of the very similar effect on /d-g/ judgments of /l/ vs /r/ or sine waves mimicking their F3 values by Lotto & Kluender; and condition (4) is entirely novel.\n",
    ""
   ]
  },
  "goldinger00_swap": {
   "authors": [
    [
     "Stephen D.",
     "Goldinger"
    ]
   ],
   "title": "The Role of Perceptual Episodes in Lexical Processing",
   "original": "swap_155",
   "page_count": 4,
   "order": 38,
   "p1": "155",
   "pn": "158",
   "abstract": [
    "Nearly all theories of spoken word perception presume a lexicon with singular entries corresponding to each word. In turn, the perceptual system is presumed to operate by matching such entries (e.g., nodes, spectral templates, distributed patterns) to the variable signals that speakers produce, requiring either a process of normalization or sophisticated guessing. In contrast, episodic theories assume that people store multiple entries, in the form of detailed perceptual traces, for each in the lexicon. Such multiple-trace theories are robust to variation, and they provide a natural explanation for extra-linguistic learning, such as learning voices.\n",
    "In this presentation, I will review selected evidence for detailed perceptual episodes, including assessment of their specificity, longevity, and the factors that shape their memorial forms. Following this, I will address the continuity of memory and perception, emphasizing the role of stored traces in perceptual tasks.\n",
    "Finally, I will review my applications of a specific model (Hintzman's MINERVA 2) to data in speech perception and production. Key issues and challenges, including the wide-ranging roles of selective attention, will be discussed.\n",
    ""
   ]
  },
  "pallier00_swap": {
   "authors": [
    [
     "Christophe",
     "Pallier"
    ]
   ],
   "title": "Word recognition: Do we need phonological representations?",
   "original": "swap_159",
   "page_count": 4,
   "order": 39,
   "p1": "159",
   "pn": "162",
   "abstract": [
    "In the field of visual object recognition, it is debated whether objects are stored in memory using abtract, 3D, structural representations, or, rather, quasi-pictorial, low-level, 2D analogical representations. The same question can be translated into the field of auditory word recognition where one can broadly distinguish between two classes of word recognition models: Models in the first class postulate that the mental representations of word forms specify detailed acoustic/phonetic features and the mapping between the signal and these representation is a \"direct\" comparison. Models in the second class postulate abstract phonological representations and a process of phonological parsing between the signal and these representations. We will present several psycholinguistic experiments that attempt to distinguish between the two types of models.\n",
    ""
   ]
  },
  "cooper00_swap": {
   "authors": [
    [
     "Nicole",
     "Cooper"
    ]
   ],
   "title": "Native and non-native preprocessing of lexical stress in English word recognition",
   "original": "swap_163",
   "page_count": 4,
   "order": 40,
   "p1": "163",
   "pn": "166",
   "abstract": [
    "Listeners learn to preprocess their native language in a way that provides the most useful information for word recognition, and they may also use their native strategies when listening to a second language (Cutler, Mehler, Norris and Segui, JML, 1986; Cutler and Otake, JML, 1994). Dutch and English are both lexical stress languages, but in English, stress information covaries with vowel quality information, and stress is not used by English listeners in lexical access (Fear, Cutler and Butterfield, JASA, 1995; Culter and Clifton, 1984). The stress/vowel-quality relation is less strong in Dutch, and experimental evidence shows that Dutch listeners do use lexical stress to constrain lexical access in their native language (Koster and Cutler, Eurospeech, 1997; van Donselaar and Cutler, 1997). Can Dutch listeners also process stress cues in English?\n",
    "Two fragment priming experiments investigated Dutch and English listeners' processing of lexical stress in English. Pairs of words were found with the same unreduced first syllable and following phoneme but mismatching stress pattern, eg MUsic, muSEUM. Visual lexical decisions to one of these words (eg MUSEUM) were measured following auditory presentation of 1) a phonemically- and stress- matched fragment (muS); 2) a phonemically matched- but stress-mismatched fragment (MUs); 3) a control fragment (MONs). Both English and Dutch listeners' responses after phonemically matching fragments (MUs/muS) were significantly faster than after control fragments, and both showed sensitivity to stress information, giving faster responses after fragments with correct stress (muS) than after fragments with incorrect stress (MUs). The facilitatory effect in the stress mismatching condition is surprising for both language groups, first because a similar experiment with Dutch listeners attending to Dutch stimuli showed an inhibitory effect following stress mismatches (MUs), and secondly because previous research with English listeners suggested that English listeners are not sensitive to stress cues in English. To investigate whether the difference betweeen the current results and the previous English results could be explained by differences in unconscious and conscious levels of processing, an off-line two-alternative forced choice task was given to English listeners to determine if they could consciously select which word of the stress pair each fragment came from. The fragment priming results will be discussed in the light of data from the forced choice experiment.\n",
    ""
   ]
  },
  "hawkins00_swap": {
   "authors": [
    [
     "Sarah",
     "Hawkins"
    ],
    [
     "Noel",
     "Nguyen"
    ]
   ],
   "title": "Predicting syllable-coda voicing from the acoustic properties of syllable onsets",
   "original": "swap_167",
   "page_count": 4,
   "order": 41,
   "p1": "167",
   "pn": "170",
   "abstract": [
    "Recent work shows that syllable-onset /l/s are longer and darker in syllables with voiced codas compared with voiceless codas. In some circumstances, listeners are sensitive to these coda-dependent acoustic differences in onset /l/s as a cue to coda voicing in word recognition. These findings are at variance with segmental models of word recognition such as TRACE and they provide support for an alternative approach in which partitioning the speech signal into segments is not essential to lexical access. The present experiment further explores the effects of the acoustic characteristics of an onset /l/ on the perception of coda voicing.\n",
    "The word 'led' was synthesized from a natural token. From this stimulus, five more were made such that the duration of /l/ varied in 20-ms steps over the range 70-170 ms. At each of the six durations, a total of 4 stimuli were made by using different combinations of F2 frequency and f0 during the /l/. F2 was constant during the /l/, at either 1850 Hz or 1740 Hz, jumping to 1860 Hz at vowel onset. F0 began at either 180 Hz or 168 Hz, and fell to 162 Hz at vowel onset. Thus there were in total 24 stimuli, identical except for the properties of the initial /l/. Each stimulus was truncated 80 ms after vowel onset, and 300 ms of white noise appended to it, so that the end of the stimulus seemed to be obliterated by noise. The randomised stimuli were played ten times each to 12 listeners in a two-alternative word-identification task (led / let).\n",
    "Both duration and f0 had the predicted effect on responses: shorter /l/s and higher f0 produced more let responses. Duration had the greatest influence, with a mean of 73% let responses when /l/ was shortest, falling to 36% when /l/ was longest (F(5,55) = 16.21, p < .0001). F0 alone had a smaller effect: 59% vs. 51% let to stimuli with high vs. low f0 respectively (F(1,11) = 12.12, p = .005). There were no significant interactions. Contrary to expectations derived from production data, F2 frequency did not significantly affect the listeners' responses. The potential effect of F2 on the perceived coda voicing in the absence of durational and f0 differences remains to be determined in ongoing experiments. Overall, however, our results confirm that listeners use acoustic properties of onset /l/ to predict the voicing of the tautosyllabic coda.\n",
    "Hawkins, S., & Nguyen, N. (to appear). Effects on word recognition of syllable-onset cues to syllable-coda voicing, Papers in Laboratory Phonology VI (Cambridge University Press, Cambridge, UK).\n",
    "Nguyen, N., & Hawkins, S. (1998). Syllable-onset acoustic properties associated with syllable-coda voicing, Proceedings of the 5th International Conference on Spoken Language Processing, Sydney, 30 Nov. - 4 Dec. 1998.\n",
    "Nguyen, N., & Hawkins, S. (1999). Implications for word recognition of phonetic dependencies between syllable onsets and codas, Proceedings of the 14th International Congress of Phonetic Sciences, San Francisco, 1-7 August 1999.\n",
    ""
   ]
  },
  "sereno00_swap": {
   "authors": [
    [
     "Joan",
     "Sereno"
    ],
    [
     "Hugo",
     "Quené"
    ]
   ],
   "title": "Facilitatory and inhibitory effects using a segmental phonetic priming paradigm",
   "original": "swap_171",
   "page_count": 4,
   "order": 42,
   "p1": "171",
   "pn": "174",
   "abstract": [
    "This research examines the representation and access of lexical form during spoken language comprehension. Previous research using a phonological priming methodology has shown contrasting results depending on whether overlap is initial or final. When overlap occurs in onset position, inhibitory effects are found that are mediated by relatedness proportion, ISI, and task demands. Explanations for such effects point to the involvement of lexical competition. With final overlap, results are often facilitatory in nature although this facilitation seems to require rime overlap. These results have been attributed to the operation of automatic perceptual processes. A series of segmental phonetic priming experiments were conducted in Dutch to address the automatic and perceptual nature of the processes involved. A single phonetic segment (rather than a word or nonword) was used to prime a target containing that segment. The phonetic priming paradigm offers several advantages. As with previous methodologies, it allows variation in overlap (e.g., initial or final) between prime and target. Importantly, however, using a single phonetic segment strips the prime of lexical status. Onset and offset priming can be directly compared. Presentation of a single phonetic segment also allows for a reduction of the temporal interval between prime and target, enhancing sensitivity to early processes in auditory word recognition.\n",
    "In the onset phonetic priming experiments, fricative segments [f,s,x] were used as primes for target items (e.g., faam 'fame', soep 'soup', and gek 'weird'). Matching pairs ([f]-faam, [s]-soep, [x]-gek) were contrasted to non-matching items ([s]-faam, [x]-faam, [f]-soep, [x]-soep, [f]-gek, [s]-gek). Both primes and targets were presented auditorily in a shadowing task. The offset phonetic priming experiments were identical to the onset experiments except that overlap occurred at the end of target items (e.g., [f]-dief, [s]-dief, [x]-dief 'thief'). Compared to the phonological priming results, the present preliminary data (60 subjects) show a different pattern of results. For onset overlap, the data show a small but consistent facilitation, with matching items facilitated compared with non-matching controls. Since primes are phonetic segments, lexical level inhibition does not contribute. In contrast to the onset results, the offset fricative experiments show an inhibitory effect, with naming latencies to matching items slowed compared to non-matching controls. Two additional experiments investigated CV-onset and VC-rime contrasts to evaluate the supplementary role of the nucleus in mediating these facilitatory and inhibitory effects (e.g., onset: [bu]-boek 'book'; offset: [it]-riet 'reed'). Results will be discussed in terms of recent models of auditory word recognition.\n",
    ""
   ]
  },
  "tao00_swap": {
   "authors": [
    [
     "Liang",
     "Tao"
    ]
   ],
   "title": "Prosody and Word Recognition: A case study",
   "original": "swap_175",
   "page_count": 4,
   "order": 43,
   "p1": "175",
   "pn": "178",
   "abstract": [
    "Three experiments tested the role of an emerging syntactic tone in spoken word recognition of Beijing Mandarin Chinese. The study tested whether there are two prelexical representations of a word following yi55 (one) with two different tones, and how synchronic tonal variations is mapped onto word recognition during online language processing.\n",
    "Beijing Mandarin is currently undergoing syntactic changes in its noun phrase formation. The obligatory rule of the classifier is changing, caused by sound erosion on high-frequency units in oral discourse. Such changes leave a 'frozen' tone that does not follow the Mandarin tone sandhi rules, and a noun phrase without the classifier. Although Mandarin tones are lexical, the 'frozen' tone assumes a syntactic function (Tao, 1999). Specifically, the tone is a high-rising tone associated with the word yi55 (one). When yi55 is used with a noun that may be a classifier or a noun (e.g., che55: car), the frozen tone indicates the word is a noun (yi35 che55: a car). A lexical tone with the sandhi change indicates the word is a classifier (yi51 che55: a carload). The change may be the beginning of an article in the language.\n",
    "Four passages were designed each ending with a phrase that may be either a noun phrase or an elliptical nominal expression, depending on the crucial tonal. Passages were recorded twice with two different tones on the word yi55 (one). Following each passage were four multiple-choice questions, focusing on the definition of the word following the crucial tone. Experiments 1 and 2 tested participants individually with the answers videotaped. Participants were asked to retell the passages using their own words in Experiment 2. Experiment 3 tested 100 participants in two groups. The answers were printed on paper.\n",
    "Preliminary analysis indicates that the tonal variations on the same word did have an impact: The lexical tone was readily processed and its following word correctly recognized, yet the words following the syntactic tone were correctly identified only by those familiar with the multiple-choice test format. The others either ignored the syntactic tone during word recognition, or chose a heuristic answer to compensate. The results suggest a conflict between perception and mental representation of the spoken word, indicating that with reference to an emerging language change, the acoustic image of the word does not readily match the actual sound; thus the mental lexicon lags behind language change.\n",
    ""
   ]
  },
  "nearey00_swap": {
   "authors": [
    [
     "Terrance M.(2000)",
     "Nearey"
    ]
   ],
   "title": "Phoneme-like units and speech perception",
   "original": "swap_179",
   "page_count": 4,
   "order": 44,
   "p1": "179",
   "pn": "182",
   "abstract": [
    "In English, the perception of syllables and words can be largely predicted from the perception of 'smaller' phoneme-like units. Experiments reviewed by Allen (1994) show that the correct identification of nonsense CVC syllables in noise can be extremely well predicted from the marginal correct identification rates of their constituent phonemes. Simulations by Nearey (to appear, submitted) suggest that this result can be readily achieved when syllable patterns can be 'factored' into phoneme parts, while representations allowing even slightly idiosyncratic relationships between stimuli and syllables cannot reproduce such results. Parametric experiments with synthetic speech, wherein listeners hear syllables that span two or more categories of two or more segments (e.g. bad, bed, bat, bet ) also provide evidence for phoneme-factorability (Nearey 1997, submitted.) This paper summarizes how models with phoneme-like units as the core phonetic-transponders can be supplemented with bias elements that accommodate such phenomena as transitional probabilities and lexical frequency to provide a good account for many phenomena in the perception of words and nonsense syllables alike. A progress report the construction a prototype automatic speech recognition system for a subset of English CVC syllables that conforms to this structure will also be presented.\n",
    "Allen, J. (1994). How do humans process and recognize speech? IEEE Transactions on Speech and Audio Processing, 2, 567-577.\n",
    "Nearey, T. (1997). Speech perception as pattern recognition. J. Acoust. Soc. Amer. 101, 3241-3254.\n",
    "Nearey, T. (to appear). The factorability of phonological units in speech perception: Simulating results on speech reception in noise., in Festschrift for Bruce L. Derwing, edited by R. Smyth.\n",
    "Nearey, T. (submitted). On the factorability of phonological units in speech perception. Submitted to Labphon VI.\n",
    ""
   ]
  },
  "miller00_swap": {
   "authors": [
    [
     "Joanne L.",
     "Miller"
    ]
   ],
   "title": "Mapping from Acoustic Signal to Phonetic Category: Nature and Role of Internal Category Structure",
   "original": "swap_183",
   "page_count": 4,
   "order": 45,
   "p1": "183",
   "pn": "186",
   "abstract": [
    "A widely held view for many years was that listeners derive an abstract phonetic representation during speech processing and, in so doing, discard detailed information about the signal. However, more recent research has shown that the representations of speech are richer than this emphasis on abstract categories would suggest, and that listeners retain in memory a substantial amount of fine-grained information. One line of evidence for this new view comes from research showing that phonetic categories are internally structured in a graded fashion, with some members of the category perceived as better exemplars than others. In this talk I will briefly review some findings that demonstrate basic characteristics of such structure and then present results from two new sets of studies that further examine this structure and its role in processing. In both sets, we examined the syllable-initial voicing distinction specified by voice-onset time (VOT), with a specific focus on the voiceless category. The first set of studies, which used a category-goodness rating task, investigated how a higher-order linguistic contextual variable, lexical status, affects the internal structure of the category. We knew from previous work that acoustic-phonetic contextual factors (e.g., speaking rate) alter not only perception of stimuli in the category boundary region, but also which stimuli are perceived as the best category exemplars. The new results showed that lexical status has a qualitatively different effect: Although a change in lexical status also altered perception of stimuli in the category boundary region, the best exemplars of the category remained relatively fixed. Thus contextual variables that have comparable effects in the boundary region can in some cases be dissociated with respect to their effects on a category's best exemplars. The second set of studies, which used a speeded categorization task, examined how differences in perceived category goodness affect the speed of phonetic categorization. The results showed that although categorization time was relatively slow for poor exemplars in the voiced-voiceless boundary region, poor exemplars with VOT values longer than the best exemplars (and thus far from the category boundary) were identified as quickly as the best exemplars themselves. This suggests that categorization time depends more on an exemplar's location in perceptual space vis-à-vis a boundary with a competing category than on its perceived category goodness. Taken together, the new findings underscore the importance of considering both internal category structure and phonetic category boundaries when developing models of phonetic categorization.\n",
    ""
   ]
  }
 },
 "sessions": [
  {
   "title": "Programme",
   "papers": [
    "bard00_swap",
    "gaskell00_swap",
    "zwitserlood00_swap",
    "nooteboom00_swap",
    "marslenwilson00_swap",
    "boudelaa00_swap",
    "mauth00_swap",
    "meunier00_swap",
    "reid00_swap",
    "content00_swap",
    "norris00_swap",
    "mcqueen00_swap",
    "samuel00_swap",
    "alphen00_swap",
    "bolte00_swap",
    "cutler00_swap",
    "dahan00_swap",
    "pitt00_swap",
    "frauenfelder00_swap",
    "vroomen00_swap",
    "fowler00_swap",
    "amano00_swap",
    "brink00_swap",
    "goswami00_swap",
    "rodd00_swap",
    "tanenhaus00_swap",
    "luce00_swap",
    "connine00_swap",
    "shillcock00_swap",
    "pierrehumbert00_swap",
    "dumay00_swap",
    "kirk00_swap",
    "lugt00_swap",
    "smith00_swap",
    "weber00_swap",
    "whalen00_swap",
    "kingston00_swap",
    "goldinger00_swap",
    "pallier00_swap",
    "cooper00_swap",
    "hawkins00_swap",
    "sereno00_swap",
    "tao00_swap",
    "nearey00_swap",
    "miller00_swap"
   ]
  }
 ]
}