{
 "location": "Tangalooma Wild Dolphin Resort, Moreton Island, Queensland, Australia",
 "startDate": "26/9/2008",
 "endDate": "29/9/2008",
 "conf": "AVSP",
 "year": "2008",
 "name": "avsp_2008",
 "series": "AVSP",
 "SIG": "AVISA",
 "title": "Auditory-Visual Speech Processing",
 "title1": "Auditory-Visual Speech Processing",
 "date": "26-29 September 2008",
 "papers": {
  "vatikiotisbateson08_avsp": {
   "authors": [
    [
     "Eric",
     "Vatikiotis-Bateson"
    ]
   ],
   "title": "Concurrency, synchrony, and temporal organization",
   "original": "av08_001",
   "page_count": 1,
   "order": 1,
   "p1": "1",
   "pn": "",
   "abstract": [
    "The organization of time-varying linguistic behavior, while controlled, is not precisely timed. This claim is supported empirically, but it is also motivated theoretically by an idea that will be fleshed out in the talk; namely, language behavior necessitates a neuro-cognitive displacement from the fine-grained spatial and temporal instantiation of language. This obviates the need for the precise synchronization that can be observed in that other peculiarly human activity - music.\n",
    ""
   ]
  },
  "cohn08_avsp": {
   "authors": [
    [
     "Jeffrey F.",
     "Cohn"
    ]
   ],
   "title": "Facial dynamics reveals person identity and communicative intent, regulates person perception and social interaction",
   "original": "av08_003",
   "page_count": 1,
   "order": 2,
   "p1": "3",
   "pn": "",
   "abstract": [
    "In both biometrics and facial expression recognition, previous literature emphasizes the configuration of facial features instead of their dynamics. I propose that dynamics provide essential information about person identity, neural control, communicative intent, and interpersonal coordination. To investigate this hypothesis, my colleagues and I have used multiple methods, including perceptual judgment, manual annotation of facial actions, and automatic facial image analysis and synthesis. My talk will review our findings in support of this hypothesis, including our use of real-time avatars and expression cloning to separate the joint influence of appearance and dynamics. We show that the dynamics of facial action are critical to individual differences in perception and expression of emotion and pain, communicative intent, person perception, and social interaction. I also review our related findings for vocal expression. In all, this work highlights the importance of dynamics to a broad range of intra- and interpersonal functions.\n",
    ""
   ]
  },
  "matthews08_avsp": {
   "authors": [
    [
     "Iain",
     "Matthews"
    ]
   ],
   "title": "Active appearance models for facial analysis",
   "original": "av08_005",
   "page_count": 1,
   "order": 3,
   "p1": "5",
   "pn": "",
   "abstract": [
    "The computer vision community and industry have made great progress on face detection and recognition. The current popular approaches are now mostly based on single image analysis using sliding detection windows and boosted classifiers. They work impressively well for frontal face images. However, there are many more adjuvant application areas for facial analysis if we are able to accurately locate and describe faces in real-time through video sequences. This allows analysis of what a face is \"doing\", rather than just where it is and who it belongs to. This is a more appealing and difficult problem and the subject of this talk.\n",
    ""
   ]
  },
  "theobald08_avsp": {
   "authors": [
    [
     "Barry-John",
     "Theobald"
    ],
    [
     "Nicholas",
     "Wilkinson"
    ],
    [
     "Iain",
     "Matthews"
    ]
   ],
   "title": "On evaluating synthesised visual speech",
   "original": "av08_007",
   "page_count": 6,
   "order": 4,
   "p1": "7",
   "pn": "12",
   "abstract": [
    "This paper describes issues relating to the subjective evaluation of synthesised visual speech. Two approaches to synthesis are compared: a text-driven synthesiser and a speech-driven synthesiser. Both synthesisers are trained using the same data and both use the same model for rendering the synthesised visual speech. Naturalness is used as a performance metric, and the naturalness of real visual speech re-rendered on the same model is used as a benchmark. The naturalness of the textdriven synthesiser is significantly better than the speech-driven synthesiser, but neither synthesiser can yet achieve the naturalness of real visual speech. The impact of likely sources of error apparent in the synthesised visual speech is investigated. Similar forms of error are introduced into real visual speech sequences and the degradation in naturalness is measured using the same naturalness ratings used to evaluate the performance of the synthesisers. We find that the overall perception of sentencelevel utterances is severely degraded when only a small region of an otherwise perfect rendering of the visual sequence is incorrect. For example, if the visual gesture for only a single syllable in an utterance is incorrect, the overall naturalness of this real sequence is rated lower than the text-based synthesiser.\n",
    ""
   ]
  },
  "fels08_avsp": {
   "authors": [
    [
     "Sidney",
     "Fels"
    ],
    [
     "Robert",
     "Pritchard"
    ],
    [
     "Eric",
     "Vatikiotis-Bateson"
    ]
   ],
   "title": "Building a portable gesture-to-audio/visual speech system",
   "original": "av08_013",
   "page_count": 6,
   "order": 5,
   "p1": "13",
   "pn": "18",
   "abstract": [
    "We have constructed an easy-to-use portable, wearable gesture-to-speech system based on the Glove-TalkII and GRASSP gesture-controlled speech systems and a vizeme based face-synthesizer. Our new portable system is called a Digital Ventriloquized Actor (DIVA) and refines the use of the formant speech synthesizer. Using a DIVA, a user can speak using hand gestures mapped to both synthetic sound and face using a mapping function that preserves gesture trajectories. By making DIVAs portable and self-contained, speakers can communicate with others in the community and perform in new music/theatre stage productions. DIVA performers also allow us to study the relationship between visible gestures and speech/song production.\n",
    ""
   ]
  },
  "brungart08_avsp": {
   "authors": [
    [
     "Douglas S.",
     "Brungart"
    ],
    [
     "Nandini",
     "Iyer"
    ],
    [
     "Brian D.",
     "Simpson"
    ],
    [
     "Virginie van",
     "Wassenhove"
    ]
   ],
   "title": "The effects of temporal asynchrony on the intelligibility of accelerated speech",
   "original": "av08_019",
   "page_count": 6,
   "order": 6,
   "p1": "19",
   "pn": "24",
   "abstract": [
    "When the audio and visual portions of a speech stimulus are presented synchronously, the resulting enhancement in intelligibility is generally much larger than the one obtained when the audio and visual stimuli are presented sequentially. However, perfect synchronization is not required to obtain a substantial audiovisual (AV) benefit: many studies have shown that AV integration is maximum when the audio signal is slightly delayed relative to the visual signal, and other studies have shown that substantial AV intelligibility enhancement typically extends over a 250+ ms range of AV delay values, from a 50 ms lag in the visual stimulus to a 200 ms lag in the audio stimulus. In this study, artificially accelerated speech stimuli were used to examine the impact that speaking rate has on the characteristics of this temporal integration window. The results indicate that maximal AV enhancement occurs over a progressively narrower range of delay values when the speaking rate increases. The results for the fastest speaking rates also show that peak AV enhancement occurred at a larger AV delay value (150ms) than has been reported in previous studies. However, there was no conclusive evidence to suggest that the audio delay value for peak AV enhancement systematically changed with the speaking rate of the stimulus.\n",
    ""
   ]
  },
  "chaloupka08_avsp": {
   "authors": [
    [
     "Josef",
     "Chaloupka"
    ],
    [
     "Jan",
     "Nouza"
    ],
    [
     "Jindrich",
     "Zdansky"
    ]
   ],
   "title": "Audio-visual voice command recognition in noisy conditions",
   "original": "av08_025",
   "page_count": 6,
   "order": 7,
   "p1": "25",
   "pn": "30",
   "abstract": [
    "This paper presents an experiment of audio-visual voice command recognition from small vocabulary in simulated noisy conditions. Electric and electronics devices should be controlled by these voice commands mainly in homes of motor-handicapped people and visual part of speech could improve recognition rate if a noise is relatively strong. Therefore the main aims of this experiment were to find out how visual part of speech can improve resulting recognition rate and if it is possible to use successfully two-stream Hidden Markov Models (HMMs) for audio-visual voice command recognition.\n",
    ""
   ]
  },
  "giorgolo08_avsp": {
   "authors": [
    [
     "Gianluca",
     "Giorgolo"
    ],
    [
     "Frans A. J.",
     "Verstraten"
    ]
   ],
   "title": "Perception of speech-and-gesture² integration",
   "original": "av08_031",
   "page_count": 6,
   "order": 8,
   "p1": "31",
   "pn": "36",
   "abstract": [
    "This paper describes two experiments conducted to identify the role of synchronization in the perception of speech and gesture communication and to isolate the parameters that determine the perception of temporal alignment. The results of the first experiment show that the synchronization between audio and visual signals determines the felicitousness of a multimodal utterance. With the second experiment we were able to determine that prosodic alignment is a parameter that our subjects used to judge the well-formedness of speech and gesture input.\n",
    ""
   ]
  },
  "ishi08_avsp": {
   "authors": [
    [
     "Carlos Toshinori",
     "Ishi"
    ],
    [
     "Hiroshi",
     "Ishiguro"
    ],
    [
     "Norihiro",
     "Hagita"
    ]
   ],
   "title": "Analysis of inter- and intra-speaker variability of head motions during spoken dialogue",
   "original": "av08_037",
   "page_count": 6,
   "order": 9,
   "p1": "37",
   "pn": "42",
   "abstract": [
    "Head motions naturally occur in synchrony with speech and may carry paralinguistic information, such as intentions, attitudes and emotions, in dialogue communication. With the aim of verifying the relationship between head motions and the linguistic and paralinguistic functions carried by speech, analyses were conducted on motion-captured data of several speakers during natural dialogues. The analysis results firstly confirmed the trends of our previous work, showing that regardless the speaker, nods frequently occur during speech utterances, not only for expressing dialogue acts such as agreement and affirmation, but also appearing at the last syllable of the phrases, in strong phrase boundaries, especially when the speaker is confidently talking, or expressing interest to the interlocutors talk. Inter-speaker variability indicated that the frequency of head motions may vary according to the speakers age or status, while intraspeaker variability indicated that the frequency of head motions also differ depending on the inter-personal relationship with the interlocutor.\n",
    ""
   ]
  },
  "fagel08_avsp": {
   "authors": [
    [
     "Sascha",
     "Fagel"
    ],
    [
     "Gérard",
     "Bailly"
    ]
   ],
   "title": "German text-to-audiovisual-speech by 3-d speaker cloning",
   "original": "av08_043",
   "page_count": 4,
   "order": 10,
   "p1": "43",
   "pn": "46",
   "abstract": [
    "Visible speech movements were optically motion captured and parameterized by means of a guided PCA. Co-articulated consonantal targets were extracted from VCVs, vocalic targets were extracted from these VCVs and from sustained vowels. Targets were selected or combined to derive target sequences for phone chains of arbitrary German utterances. Parameter trajectories for these utterances are generated by interpolating targets through linear to quadratic functions that reflect the degree of co-articulatory influence. Videos of test words embedded in a carrier sentence were rendered from parameter trajectories for an evaluation in the form of a rhyme test in noise. Results show that the synthetic videos - although intelligible only somewhat above chance level when played alone - significantly increase the recognition scores from 45.6% in audio alone presentation to 60.4% in audiovisual presentation.\n",
    ""
   ]
  },
  "behne08_avsp": {
   "authors": [
    [
     "Dawn",
     "Behne"
    ],
    [
     "Yue",
     "Wang"
    ],
    [
     "Stein-Ove",
     "Belsby"
    ],
    [
     "Solveig",
     "Kaasa"
    ],
    [
     "Lisa",
     "Simonsen"
    ],
    [
     "Kirsti",
     "Back"
    ]
   ],
   "title": "Visual field advantage in the perception of audiovisual speech segments",
   "original": "av08_047",
   "page_count": 4,
   "order": 11,
   "p1": "47",
   "pn": "50",
   "abstract": [
    "Research has shown that speech segment perception is predominantly a function of the left hemisphere. The aim of this study is to determine if the visual field in which visual speech is presented affects the audiovisual percept. Results show increased audiovisual fusion in the right visual field and exclude that the difference was a result of facial asymmetry. Findings indicate more efficient early audiovisual processing in the right visual field, likely associated with early left hemisphere brain functions for speech segments.\n",
    ""
   ]
  },
  "tamura08_avsp": {
   "authors": [
    [
     "Satoshi",
     "Tamura"
    ],
    [
     "Chiyomi",
     "Miyajima"
    ],
    [
     "Norihide",
     "Kitaoka"
    ],
    [
     "Satoru",
     "Hayamizu"
    ],
    [
     "Kazuya",
     "Takeda"
    ]
   ],
   "title": "CENSREC-AV: evaluation frameworks for audio-visual speech recognition",
   "original": "av08_051",
   "page_count": 4,
   "order": 12,
   "p1": "51",
   "pn": "54",
   "abstract": [
    "This paper introduces incoming evaluation frameworks for bimodal speech recognition in noisy conditions and real environments. In order to develop a robust speech recognition in noisy environments, bimodal speech recognition which uses acoustic and visual information has been paid attention to particularly for this decade. As a lot of methods and techniques for bimodal speech recognition have been proposed, a common evaluation framework, including audio-visual speech data and baseline system, is needed to estimate and compare these techniques and bimodal speech recognition schemes. Audio-visual evaluation frameworks, CENSREC-1-AV and CENSREC-2-AV, have been being built by the CENSREC project in Japan; CENSREC- 1-AV includes artificially noise-added waveforms and image sequences, whereas CENSREC-2-AV consists of audio-visual data recorded in in-car environments. A baseline method and its recognition results will be also provided with these corpora.\n",
    "Index Terms: evaluation framework, audio-visual speech corpus, bimodal speech recognition, noisy environments.\n",
    ""
   ]
  },
  "kroos08_avsp": {
   "authors": [
    [
     "Christian",
     "Kroos"
    ],
    [
     "Ashlie",
     "Dreves"
    ]
   ],
   "title": "Mcgurk effect persists with a partially removed visual signal",
   "original": "av08_055",
   "page_count": 4,
   "order": 13,
   "p1": "55",
   "pn": "58",
   "abstract": [
    "This study investigated the impact of prolonged consonant durations on the McGurk effect and the role of static visual information in this cross-modal illusion. It was found that the McGurk effect did not break down nor diminish in strength when the consonant duration was extended up to 3.8s. This remained unchanged when the video frames in the quasi-static phase of the prolonged target consonant were substituted with copies of a single frame. Surprisingly, the McGurk effect even persisted when the visual signal in the quasi-static phase was completely removed.\n",
    ""
   ]
  },
  "fagel08b_avsp": {
   "authors": [
    [
     "Sascha",
     "Fagel"
    ],
    [
     "Katja",
     "Madany"
    ]
   ],
   "title": "Guided non-linear model estimation (gnoME)",
   "original": "av08_059",
   "page_count": 4,
   "order": 14,
   "p1": "59",
   "pn": "62",
   "abstract": [
    "The present work describes a new method for modeling 3-D motion capture data of speech movements. 27 markers on the face of a speaker uttering 27 VCVs were tracked by a Vicon motion capture system. The 3-D coordinates of the markers in all frames of the recording were modeled in four ways: 1) a plain PCA, 2) a guided PCA where each component is determined on a subset of markers that represent an articulator and the component is used to reconstruct the data by linear regression, 3) a cubic model where the components are determined by a PCA and the components are used to reconstruct the data by a polynomial of third order, and 4) a guided cubic model where each component is determined by a PCA on a subset of markers and the components are used to reconstruct the data by a polynomial of third order. Results show that the latter method - called guided non-linear model estimation (gnoME) with cubic regression leads to meaningful articulatory parameters like the guided PCA while the variance explanation equals that one of the plain PCA and the accuracy of the 3-D data reconstruction is higher compared to the plain PCA.\n",
    ""
   ]
  },
  "troille08_avsp": {
   "authors": [
    [
     "Emilie",
     "Troille"
    ],
    [
     "Marie-Agnès",
     "Cathiard"
    ],
    [
     "Christian",
     "Abry"
    ],
    [
     "Lucie",
     "Ménard"
    ],
    [
     "Denis",
     "Beautemps"
    ]
   ],
   "title": "Multimodal perception of anticipatory behavior - Comparing blind, hearing and cued speech subjects",
   "original": "av08_063",
   "page_count": 6,
   "order": 15,
   "p1": "63",
   "pn": "68",
   "abstract": [
    "In a previous study [1] on vowel rounding anticipatory perception through a fricative consonant stream with a gating paradigm, we discovered that auditory information could be, in this VCV frame, ahead of visual information, and even of audiovisual information (hence A>AV>V). We propose here to extend this experiment to different modality-impaired groups of adults and children, namely: (i) to blind and impaired-hearing adults vs. normal hearing and sighted ones; (ii) and to impaired-hearing vs. normal hearing children (7-11- year-old). Blind and sighted adults had globally comparable results in the auditory modality, though individually less categorical for the blind. In the visual condition, hearingimpaired adults had the same delayed performance as normal. In addition the same hearing-impaired adults were tested when practicing French Cued Speech (CS). Their performance indicated clearly that, with CS, they succeeded in the identification task as precociously as normal hearing in the auditory condition. Normal hearing children obtained the same ranking in their perceptual modalities, i.e. A>AV>V, but with a global delay of 20-30 ms compared to adults. Hearingimpaired children obtained again an anticipatory gain with CS comparable to the audio of normal children. The conclusion is that each group takes the best advantage of the natural timing of their available sensory modalities, by the face, the hand and the ear. For Cued Speech coders, the hand-to-face coordination can clearly substitute in time to the ear.\n",
    "",
    "",
    "E. Troille, M.-A. Cathiard & C. Abry. Consequences on bimodal perception of the timing of the consonant and vowel audiovisual flows. Proceedings of AVSP 2007, 31 August - 3 September, Kasteel Groenendael, Hilvarenbeek, Pays-Bas, 281-286, 2007 (ISCA Archive, http://www.isca-speech.org/archive/avsp07)\n",
    ""
   ]
  },
  "lucey08_avsp": {
   "authors": [
    [
     "Patrick",
     "Lucey"
    ],
    [
     "Gerasimos",
     "Potamianos"
    ],
    [
     "Sridha",
     "Sridharan"
    ]
   ],
   "title": "Patch-based analysis of visual speech from multiple views",
   "original": "av08_069",
   "page_count": 6,
   "order": 16,
   "p1": "69",
   "pn": "74",
   "abstract": [
    "Obtaining a robust feature representation of visual speech is of crucial importance in the design of audio-visual automatic speech recognition systems. In the literature, when visual appearance based features are employed for this purpose, they are typically extracted using a \"holistic\" approach. Namely, a transformation of the pixel values of the entire region-of-interest (ROI) is obtained, with the ROI covering the speakers mouth and often surrounding facial area. In this paper, we instead consider a \"patch\" based visual feature extraction approach, within the appearance based framework. In particular, we conduct a novel analysis to determine which areas (patches) of the mouth ROI are the most informative for visual speech. Furthermore, we extend this analysis beyond the traditional frontal views, by investigating profile views as well. Not surprisingly, and for both frontal and profile views, we conclude that the central mouth patches are the most informative, but less so than the holistic features of the entire ROI. Nevertheless, fusion of holistic and the best patch based features further improves visual speech recognition performance, compared to either feature set alone. Finally, we discuss scenarios where the patch based approach may be preferable to holistic features.\n",
    ""
   ]
  },
  "fagel08c_avsp": {
   "authors": [
    [
     "Sascha",
     "Fagel"
    ],
    [
     "Christine",
     "Kuehnel"
    ],
    [
     "Benjamin",
     "Weiss"
    ],
    [
     "Ina",
     "Wechsung"
    ],
    [
     "Sebastian",
     "Moeller"
    ]
   ],
   "title": "A comparison of German talking heads in a smart home environment",
   "original": "av08_075",
   "page_count": 4,
   "order": 17,
   "p1": "75",
   "pn": "78",
   "abstract": [
    "The authors describe a newly developed German Text-Toaudiovisual- Speech (TTavS) synthesis system based on the English speaking HeadZero. Targets of the control parameters of the talking head are generated by mapping of German phonemes to the originally English visemic blend shapes controls. The resulting German version of HeadZero and the German talking head MASSY were extended to generate audiovisual speech utterances from text with voices of both the MARY and the MBROLA audio speech synthesizers. A test was designed to evaluate the quality of the talking heads combined with the two synthetic voices in the context of a smart home environment. The results show a significant user preference for the new German HeadZero. Both heads are rated better when combined with the MARY voice.\n",
    ""
   ]
  },
  "sakamoto08_avsp": {
   "authors": [
    [
     "Shuichi",
     "Sakamoto"
    ],
    [
     "Akihiro",
     "Tanaka"
    ],
    [
     "Shun",
     "Numahata"
    ],
    [
     "Atsushi",
     "Imai"
    ],
    [
     "Tohru",
     "Takagi"
    ],
    [
     "Yôiti",
     "Suzuki"
    ]
   ],
   "title": "Effect of audio-visual asynchrony between time-expanded speech and a moving image of a talker²s face on detection and tolerance thresholds",
   "original": "av08_079",
   "page_count": 4,
   "order": 18,
   "p1": "79",
   "pn": "82",
   "abstract": [
    "In this study, we measured detection and tolerance thresholds of auditory-visual asynchrony between time-expanded speech and a moving image of the talkers face. During experiments, words were presented under two conditions: asynchrony by time-expanded speech (expansion condition: EXP) and simple timing shift (asynchronous condition: ASYN). We used 16 Japanese shorter words (four morae) and 20 Japanese longer words (seven or eight morae). All auditory speech was presented in pink noise to avoid the ceiling effect. The SNRs for shorter and longer words were respectively set to -10 dB and -3.5 dB. For EXP, auditory speech signals were analyzed and resynthesized using STRAIGHT to change the words duration (Kawahara et al., 1998). The resynthesized auditory signals were combined with the visual signals so that the onset of the utterance was synchronous. For ASYN, the auditory speech signal was simply lagged behind the visual speech signal. Results showed that detection and tolerance thresholds in longer words were higher than those for shorter words. However, when the threshold was recalculated as a function of the ratio of the expansion rate to word duration, these differences were not observed. These results suggest that detection and tolerance thresholds for auditory-visual asynchrony between timeexpanded speech and a moving image of talkers face might depend on the ratio of the expansion rate to word duration.\n",
    ""
   ]
  },
  "kroger08_avsp": {
   "authors": [
    [
     "Bernd J.",
     "Kröger"
    ],
    [
     "Jim",
     "Kannampuzha"
    ]
   ],
   "title": "A neurofunctional model of speech production including aspects of auditory and audio-visual speech perception",
   "original": "av08_083",
   "page_count": 6,
   "order": 19,
   "p1": "83",
   "pn": "88",
   "abstract": [
    "A computer-implemented neurofunctional model of speech production is introduced, which is capable of articulating vowels, VC-, and CV-syllables (C = voiced plosives; V = vowels). It will be shown in this paper that this production model is capable of simulating basic effects of auditory and audio-visual speech perception like (i) categorical perception of consonants and vowels and (ii) the McGurk effect. These typical features of speech perception directly result from the topological ordering of stored speech items at a supra-modal neural level, called a phonetic map of this model. This phonetic map is a self-organizing neural map which is trained and structured during early phases of speech acquisition. The neurofunctional model introduced here illustrates the close relationship between speech production and speech perception.\n",
    ""
   ]
  },
  "dohen08_avsp": {
   "authors": [
    [
     "Marion",
     "Dohen"
    ],
    [
     "Chun-Huei",
     "Wu"
    ],
    [
     "Harold",
     "Hill"
    ]
   ],
   "title": "Auditory-visual perception of prosodic information: inter-linguistic analysis - contrastive focus in French and Japanese",
   "original": "av08_089",
   "page_count": 6,
   "order": 20,
   "p1": "89",
   "pn": "94",
   "abstract": [
    "Audition and vision are combined for the perception of speech segments and recent studies have shown that this is also the case for some types of supra-segmental information such as prosodic focus. The integration of vision and audition for the perception of speech segments however seems to be less important in Japanese. This study aims at comparing auditory-visual perception of prosodic contrastive focus in French and in Japanese. Two parallel focus identification tests were conducted for three modalities: AV, A and V in the two languages. Four speakers were recorded in both languages. For French, there was no AV advantage due to a ceiling effect. The same was observed for Japanese even though auditory only performances did not reach a ceiling. The results suggest that there are visual cues to prosodic focus in Japanese as well as in French but that these are not systematically combined with auditory information to enhance AV perception in Japanese. However, we also found that in some cases, especially when auditory alone perception is poor, visual and auditory information can be combined to enhance perception in Japanese.\n",
    ""
   ]
  },
  "garnier08_avsp": {
   "authors": [
    [
     "Maëva",
     "Garnier"
    ]
   ],
   "title": "May speech modifications in noise contribute to enhance audio-visible cues to segment perception?",
   "original": "av08_095",
   "page_count": 6,
   "order": 21,
   "p1": "95",
   "pn": "100",
   "abstract": [
    "In this study we explore how acoustic and lip articulatory characteristics of bilabial consonants and three extreme French vowels vary in Lombard speech. In the light of several theories of segments perception we have shown that formant modifications should decrease the audio intelligibility of vowels in noise. On the contrary, modification in lip articulation should improve the visual intelligibility of vowels and bilabial consonants. This is not in agreement with previous studies which reported a global increased intelligibility of Lombard speech especially in the audio domain and not a lot in the visual one [1-3]. Thus, more detailed research is needed about the segmental and prosodic contribution to the increased intelligibility of Lombard speech\n",
    "s Junqua, J., \"The lombard reflex and it role on human listener and automatic speech recognizers\", Journal of the Acoustic Society of America, 93(1): 510-524, 1993. Davis, C., Kim, J., Grauwinkel, K. et al., \"Lombard speech: Auditory(A), Visual(V) and AV effects\", in Proceedings of Speech prosody, Dresden, Germany2006. (ISCA Archive, http://www.isca-speech.org/archive/sp2006) Chung, V., Mirante, N., Otten, J. et al., \"Audiovisual processing of Lombard speech\", in Proceedings of AVSP-2005, 55-56, 2005. (ISCA Archive, http://www.isca-speech.org/archive/avsp05)\n",
    ""
   ]
  },
  "jesse08_avsp": {
   "authors": [
    [
     "Alexandra",
     "Jesse"
    ],
    [
     "Elizabeth K.",
     "Johnson"
    ]
   ],
   "title": "Audiovisual alignment in child-directed speech facilitates word learning",
   "original": "av08_101",
   "page_count": 6,
   "order": 22,
   "p1": "101",
   "pn": "106",
   "abstract": [
    "Adult-to-child interactions are often characterized by prosodically-exaggerated speech accompanied by visually captivating co-speech gestures. In a series of adult studies, we have shown that these gestures are linked in a sophisticated manner to the prosodic structure of adults' utterances. In the current study, we use the Preferential Looking Paradigm to demonstrate that two-year-olds can use the alignment of these gestures to speech to deduce the meaning of words.\n",
    ""
   ]
  },
  "kim08_avsp": {
   "authors": [
    [
     "Jeesun",
     "Kim"
    ],
    [
     "Christian",
     "Kroos"
    ],
    [
     "Chris",
     "Davis"
    ]
   ],
   "title": "Hearing a talking face: an auditory influence on a visual detection task",
   "original": "av08_107",
   "page_count": 4,
   "order": 23,
   "p1": "107",
   "pn": "110",
   "abstract": [
    "Parsing of information from the world into objects and events occurs in both the visual and auditory modalities. It has been suggested that visual and auditory scene perception involve similar principles of perceptual organization. This study investigated cross-modal scene perception by determining whether an auditory stimulus could facilitate visual object segregation. Specifically, we examined whether the presentation of matched auditory speech would facilitate the detection of a point-light talking face amid point-light distractors. An adaptive staircase procedure (3 up 1 down rule) was used to estimate the 79% correct threshold in a twoalternative forced-choice (2AFC) procedure. To determine if different degrees of speech motion would show different sized auditory influence, three speech modes were tested (speech in quiet; whispered and Lombard speech). A facilitatory auditory effect on talking face detection was found; the size of this effect did not differ across the different speech modes.\n",
    ""
   ]
  },
  "bailly08_avsp": {
   "authors": [
    [
     "Gérard",
     "Bailly"
    ],
    [
     "Antoine",
     "Bégault"
    ],
    [
     "Frédéric",
     "Elisei"
    ],
    [
     "Pierre",
     "Badin"
    ]
   ],
   "title": "Speaking with smile or disgust: data and models",
   "original": "av08_111",
   "page_count": 4,
   "order": 24,
   "p1": "111",
   "pn": "114",
   "abstract": [
    "This paper presents preliminary analysis and modelling of facial motion capture data recorded on a speaker uttering nonsense syllables and sentences with various acted facial expressions. We analyze here the impact of facial expressions on articulation and determine prediction errors of simple models trained to map neutral articulation to the various facial expressions targeted. We show that movement of some speech organs such as the jaw and lower lip are relatively unaffected by the facial expressions considered here (smile, disgust) while others such as the movement of the upper lip or the jaw translation are quite perturbed. We also show that these perturbations are not simply additive, and that they depend on articulation.\n",
    ""
   ]
  },
  "chetty08_avsp": {
   "authors": [
    [
     "Girija",
     "Chetty"
    ],
    [
     "Michael",
     "Wagner"
    ]
   ],
   "title": "A multilevel fusion approach for audiovisual emotion recognition",
   "original": "av08_115",
   "page_count": 6,
   "order": 25,
   "p1": "115",
   "pn": "120",
   "abstract": [
    "The human computer interaction will be more natural if computers are able to perceive and respond to human nonverbal communication such as emotions. Although several approaches have been proposed to recognize human emotions based on facial expressions or speech, relatively limited work has been done to fuse these two, improve the accuracy and robustness of the emotion recognition system. This paper analyzes the strengths and the limitations of systems based only on facial expressions or acoustic information. It also analyses two approaches used to fuse these two modalities: decision level and feature level integration, and proposes a new multilevel fusion approach for enhancing the person dependant and person independent classification performance for different emotions. Two different audiovisual emotion data corpora was used for the evaluating the proposed fusion approach - DaFEx [1,2] and ENTERFACE [3] comprising audiovisual emotion data from several actors eliciting five different emotions - anger, disgust, fear, happiness, sadness and surprise. The results of the experimental study reveal that the system based on fusion of facial expression with acoustic information yields better performance than the system based on just acoustic information or facial expressions, for the emotions considered. Results also show an improvement in classification performance of different emotions with a multilevel fusion approach as compared to either feature level or score-level fusion.\n",
    "s Battocchi, A.; Pianesi, F.. 2004. DaFEx: Un Database di Espressioni Facciali Dinamiche. In Proceedings of the SLI-GSCP Workshop \"Comunicazione Parlata e Manifestazione delle Emozioni\", Padova (Italy) 30 Novembre - 1 Dicembre 2004. Mana N., Cosi P., Tisato G., Cavicchio F., Magno E. and Pianesi F., An Italian Database of Emotional Speech and Facial Expressions, In Proceedings of \"Workshop on Emotion: Corpora for Research on Emotion and Affect\", in association with \"5th International Conference on Language, Resources and Evaluation (LREC2006), Genoa, Italy, 24-25- 26 May 2006. Martin O., Adell J., Huerta A., Kotsia I., Savran A., Sebbe R., Multimodal Caricatural Mirror, Proceedings Enterface05,Workshop, http://www.enterface.net/enterface05/docs/results/reports/project2.pdf (ISCA Archive, http://www.isca-speech.org/archive/einterface05)\n",
    ""
   ]
  },
  "wu08_avsp": {
   "authors": [
    [
     "Junru",
     "Wu"
    ],
    [
     "Xiaosheng",
     "Pan"
    ],
    [
     "Jiangping",
     "Kong"
    ],
    [
     "Alan Wee-Chung",
     "Liew"
    ]
   ],
   "title": "Statistical correlation analysis between lip contour parameters and formant parameters for Mandarin monophthongs",
   "original": "av08_121",
   "page_count": 6,
   "order": 26,
   "p1": "121",
   "pn": "126",
   "abstract": [
    "In this study we examine quantitatively the correlation between the geometric lip contour parameters and the formant parameters for Mandarin monophthongs, and carry out a multiple linear regression study between the two parameters. We explicitly analyze the relationship between different geometric lip parameters and the formant parameters, which have some linguistic significance instead of the usual acoustic parameters such as MFCC. We analyzed the linguistic meaning of the regression formula, and found it in accord with the classical result on the relationship between vocaltrack and speech acoustics. And those regions with relatively poor effect of estimation are related to specific phonetic conditions.\n",
    ""
   ]
  },
  "burnham08_avsp": {
   "authors": [
    [
     "Denis",
     "Burnham"
    ],
    [
     "A.",
     "Abrahamyan"
    ],
    [
     "L.",
     "Cavedon"
    ],
    [
     "Chris",
     "Davis"
    ],
    [
     "A.",
     "Hodgins"
    ],
    [
     "Jeesun",
     "Kim"
    ],
    [
     "Christian",
     "Kroos"
    ],
    [
     "Takaaki",
     "Kuratate"
    ],
    [
     "T.",
     "Lewis"
    ],
    [
     "M.",
     "Luerssen"
    ],
    [
     "G.",
     "Paine"
    ],
    [
     "D.",
     "Powers"
    ],
    [
     "M.",
     "Riley"
    ],
    [
     "Stelarc",
     "Stelarc"
    ],
    [
     "K.",
     "Stevens"
    ]
   ],
   "title": "From talking to thinking heads: report 2008",
   "original": "av08_127",
   "page_count": 4,
   "order": 27,
   "p1": "127",
   "pn": "130",
   "abstract": [
    "The Thinking Head project has as it aims to develop (i) a new generation Talking Thinking Head that embodies human attributes, and improves human-machine interaction; and (ii) a plug-and-play research platform for users to test software in an interactive real-time environment. Here, project progress is discussed in terms of the four teams: 1. Head Building - (i) Plug-and-Play architecture, (ii) Thinking Media Framework, and (iii) Animation; 2. Human-Head Interaction (HHI) - (i) Wizard of Oz studies, and (ii) joint attention by human and head; 3. Evaluation; and 4. Performance in (i) the Beijing Head and (ii) the Pedestal Head. Directions for future research are outlined as appropriate.\n",
    ""
   ]
  },
  "barbosa08_avsp": {
   "authors": [
    [
     "Adriano V.",
     "Barbosa"
    ],
    [
     "Hani C.",
     "Yehia"
    ],
    [
     "Eric",
     "Vatikiotis-Bateson"
    ]
   ],
   "title": "Algorithm for computing spatiotemporal coordination",
   "original": "av08_131",
   "page_count": 6,
   "order": 28,
   "p1": "131",
   "pn": "136",
   "abstract": [
    "This work presents an algorithm that allows the coupling between auditory and visual concomitants of spoken communication to be computed as it evolves through time, thus allowing coordination of events to be examined computationally in the time domain.\n",
    ""
   ]
  },
  "dean08_avsp": {
   "authors": [
    [
     "David",
     "Dean"
    ],
    [
     "Sridha",
     "Sridharan"
    ]
   ],
   "title": "Fused HMM adaptation of synchronous HMMs for audio-visual speaker verification",
   "original": "av08_137",
   "page_count": 5,
   "order": 29,
   "p1": "137",
   "pn": "141",
   "abstract": [
    "A technique known as fused hidden Markov models (FHMMs) was recently proposed as an alternative multi-stream modelling technique for audio-visual speaker recognition. In this paper, we will show that instead of being treated as separate modelling technique, FHMMs can be adopted as a novel method of training synchronous hidden Markov models (SHMMs). SHMMs are traditionally jointly trained on both the acoustic and visual modalities, and while this technique has worked well for speech-recognition applications, limitations of adaptation algorithms in the commonly used HMM Toolkit software have stymied the use of SHMMs in speaker-recognition scenarios. Our FHMM adaptation method can adapting the multistream models directly from single-stream audio HMMs, allowing both background and speaker dependent SHMMs models to be generated easily. However, experiments conducted on the XM2VTS database show that there does not appear to be any advantage in the frame-level fusion available through the FHMM-adapted SHMMs over the simpler approach of output score fusion of unimodal HMM classifiers.\n",
    ""
   ]
  },
  "cosi08_avsp": {
   "authors": [
    [
     "Piero",
     "Cosi"
    ],
    [
     "Graziano",
     "Tisato"
    ]
   ],
   "title": "Describing \"INTERFACE\" a Matlab© tool for building talking heads",
   "original": "av08_143",
   "page_count": 4,
   "order": 30,
   "p1": "143",
   "pn": "146",
   "abstract": [
    "INTERFACE is a Matlab© tool for simplifying and automating many of the operations needed for building a talking head. INTERFACE consists of set of processing tools, focusing on dynamic articulatory data physically extracted by an automatic optotracking 3D movement analyzer. The final aim of INTERFACE is that of building up the animation engine of LUCIA our emotive/expressive Italian talking head. LUCIA can be directly driven by an emotional XML tagged input text, thus realizing a true audio visual expressive synthesis. LUCIA's voice is based on an Italian version of FESTIVAL-MBROLA packages, modified for expressive synthesis by means of an appropriate APML/VSML tagged language. Moreover, by using INTERFACE, it is possible to copy a real human talking by recreating the correct WAV and FAP files needed for the animation by reproducing the movements of some markers positioned on his face and recorded by an optoelectronic device. In this work the latest improvements of INTERFACE will be described and few examples of their application to real cases will be illustrated.\n",
    ""
   ]
  },
  "zelezny08_avsp": {
   "authors": [
    [
     "Miloš",
     "Železný"
    ]
   ],
   "title": "Analysis of technologies and resources for multimodal information kiosk for deaf users",
   "original": "av08_147",
   "page_count": 6,
   "order": 31,
   "p1": "147",
   "pn": "152",
   "abstract": [
    "This paper presents an analysis of technologies and resources needed for building of a multimodal information kiosk for deaf people. The considered information kiosk will use sign language as main communication means, since it is main language of communication of deaf users. Thus, the sign language synthesis and recognition modules will be main part of the kiosk. Additional modules include lip-reading support for sign language recognition (since sign language is accompanied by lip articulation), touch screen input, and graphical user interface. To design and build the mentioned modules, large amounts of data are needed for training, especially in the case of recognition modules. The information kiosk is intended for use for provididng/ gathering information to/from deaf users, such us train connection or communication with authorities.\n",
    ""
   ]
  },
  "bailly08b_avsp": {
   "authors": [
    [
     "Gérard",
     "Bailly"
    ],
    [
     "Yu",
     "Fang"
    ],
    [
     "Frédéric",
     "Elisei"
    ],
    [
     "Denis",
     "Beautemps"
    ]
   ],
   "title": "Retargeting cued speech hand gestures for different talking heads and speakers",
   "original": "av08_153",
   "page_count": 6,
   "order": 32,
   "p1": "153",
   "pn": "158",
   "abstract": [
    "Cued Speech is a communication system that complements lip-reading with a small set of possible handshapes placed in different positions near the face. Developing a Cued Speech capable system is a time-consuming and difficult challenge. This paper focuses on how an existing bank of reference Cued Speech gestures, exhibiting natural dynamics for hand articulation and movements, could be reused for another speaker (augmenting some video or 3D talking heads). Any Cued Speech hand gesture should be recorded or considered with the concomitant facial locations that Cued Speech specifies to leverage the lip reading ambiguities (such as lip corner, chin, cheek and throat for French). These facial target points are moving along with head movements and because of speech articulation. The post-treatment algorithm proposed here will retarget synthesized hand gestures to another face, by slightly modifying the sequence of translations and rotations of the 3D hand. This algorithm preserves the coarticulation of the reference signal (including undershooting of the trajectories, as observed in fast Cued Speech) while adapting the gestures to the geometry, articulation and movements of the target face. We will illustrate how our Cued Speech capable audiovisual synthesizer - built using simultaneously recorded hand trajectories and facial articulation of a single French Cued Speech user - can be used as a reference signal for this retargeting algorithm. For the ongoing evaluation of our algorithm, an intelligibility paradigm has been retained, using natural videos for the face. The intelligibility of some video VCV sequences with composited hand gestures for Cued Speech is being measured using a panel of Cued Speech users.\n",
    ""
   ]
  },
  "lidestam08_avsp": {
   "authors": [
    [
     "Björn",
     "Lidestam"
    ]
   ],
   "title": "A, V, and AV discrimination of vowel duration",
   "original": "av08_159",
   "page_count": 4,
   "order": 33,
   "p1": "159",
   "pn": "162",
   "abstract": [
    "Discrimination of vowel duration was explored with regard to JNDs, error bias, and effects of modality and consonant context. 90 normal-hearing participants discriminated either auditorily, visually, or audiovisually between pairs of stimuli differing with regard to duration of the vowel /a/. Duration differences varied in 24 steps: 12 with the first token longer and 12 with the second token longer (33-400 ms). Results: accuracy was lower for V than A and AV; step difference affected performance in all modalities; error bias was affected by modality and consonant context; and JNDs (> 50% correct) were not possible to establish.\n",
    ""
   ]
  },
  "zoric08_avsp": {
   "authors": [
    [
     "Goranka",
     "Zoric"
    ],
    [
     "Igor S.",
     "Pandzic"
    ]
   ],
   "title": "Towards real-time speech-based facial animation applications built on HUGE architecture",
   "original": "av08_163",
   "page_count": 4,
   "order": 34,
   "p1": "163",
   "pn": "166",
   "abstract": [
    "This paper examines how our existing HUGE architecture aids fulfilling requirements emerging from the everyday expansion of new multimedia applications. Trends in humancomputer interaction stream towards creating natural user interfaces which apply rules from human-human interaction. Virtual humans who look natural, and behave believably fit perfectly in the concept of creating natural user interfaces. Universal architecture for statistically based HUman GEsturing (HUGE) is a system for producing and using statistical models for facial gestures based on any kind of inducement. We present the general architecture and its adoption to speech-based facial gesturing, and further justify it through the analysis of requirements for building multimedia application. We believe that this universal architecture is useful for experimenting with various kinds of potential applications since it enables automatic generation of full facial animation in the real time.\n",
    ""
   ]
  },
  "lucey08b_avsp": {
   "authors": [
    [
     "Patrick",
     "Lucey"
    ],
    [
     "Jessica",
     "Howlett"
    ],
    [
     "Jeffrey F.",
     "Cohn"
    ],
    [
     "Simon",
     "Lucey"
    ],
    [
     "Sridha",
     "Sridharan"
    ],
    [
     "Zara",
     "Ambadar"
    ]
   ],
   "title": "Improving pain recognition through better utilisation of temporal information",
   "original": "av08_167",
   "page_count": 6,
   "order": 35,
   "p1": "167",
   "pn": "172",
   "abstract": [
    "Automatically recognizing pain from video is a very useful application as it has the potential to alert carers to patients that are in discomfort who would otherwise not be able to communicate such emotion (i.e young children, patients in postoperative care etc.). In previous work [1], a \"pain-no pain\" system was developed which used an AAM-SVM approach to good effect. However, as with any task involving a large amount of video data, there are memory constraints that need to be adhered to and in the previous work this was compressing the temporal signal using K-means clustering in the training phase. In visual speech recognition, it is well known that the dynamics of the signal play a vital role in recognition. As pain recognition is very similar to the task of visual speech recognition (i.e. recognising visual facial actions), it is our belief that compressing the temporal signal reduces the likelihood of accurately recognising pain. In this paper, we show that by compressing the spatial signal instead of the temporal signal, we achieve better pain recognition. Our results show the importance of the temporal signal in recognizing pain, however, we do highlight some problems associated with doing this due to the randomness of a patients facial actions.\n",
    "",
    "",
    "Ashraf, A., Lucey, S., Cohn, J., Chen, T., Ambadar, Z., Prkachin, K., Solomon, P., & Theobald, B., \"The painful face: Pain expression using active appearance models\", In Proceedings of the International Conference on Multimodal Interfaces - ICMI, 9-14, 2007\n",
    ""
   ]
  },
  "barbosa08b_avsp": {
   "authors": [
    [
     "Adriano V.",
     "Barbosa"
    ],
    [
     "Hani C.",
     "Yehia"
    ],
    [
     "Eric",
     "Vatikiotis-Bateson"
    ]
   ],
   "title": "Linguistically valid movement behavior measured non-invasively",
   "original": "av08_173",
   "page_count": 5,
   "order": 36,
   "p1": "173",
   "pn": "177",
   "abstract": [
    "We use optical flow to extract reliable kinematics from video for motions of the head, face, torso, and hands during speech and musical performance. Unlike dot- and marker- based measures, these markerless measures are non-invasive and require no a priori specification of measurement locations. Reliability is compared with marker tracking data and the methods utility is demonstrated for data from Plains Cree, English, and Shona.\n",
    ""
   ]
  },
  "cox08_avsp": {
   "authors": [
    [
     "Stephen",
     "Cox"
    ],
    [
     "Richard",
     "Harvey"
    ],
    [
     "Yuxuan",
     "Lan"
    ],
    [
     "Jacob",
     "Newman"
    ],
    [
     "Barry-John",
     "Theobald"
    ]
   ],
   "title": "The challenge of multispeaker lip-reading",
   "original": "av08_179",
   "page_count": 6,
   "order": 37,
   "p1": "179",
   "pn": "184",
   "abstract": [
    "In speech recognition, the problem of speaker variability has been well studied. Common approaches to dealing with it include normalising for a speakers vocal tract length and learning a linear transform that moves the speaker-independent models closer to to a new speaker. In pure lip-reading (no audio) the problem has been less well studied. Results are often presented that are based on speaker-dependent (single speaker) or multispeaker (speakers in the test-set are also in the training-set) data, situations that are of limited use in real applications. This paper shows the danger of not using different speakers in the trainingand test-sets. Firstly, we present classification results on a new single-word database AVletters 2 which is a high-definition version of the well known AVletters database. By careful choice of features, we show that it is possible for the performance of visual-only lip-reading to be very close to that of audio-only recognition for the single speaker and multi-speaker configurations. However, in the speaker independent configuration, the performance of the visual-only channel degrades dramatically. By applying multidimensional scaling (MDS) to both the audio features and visual features, we demonstrate that lip-reading visual features, when compared with the MFCCs commonly used for audio speech recognition, have inherently small variation within a single speaker across all classes spoken. However, visual features are highly sensitive to the identity of the speaker, whereas audio features are relatively invariant.\n",
    ""
   ]
  },
  "haq08_avsp": {
   "authors": [
    [
     "Sanaul",
     "Haq"
    ],
    [
     "Philip J. B.",
     "Jackson"
    ],
    [
     "James D.",
     "Edge"
    ]
   ],
   "title": "Audio-visual feature selection and reduction for emotion classification",
   "original": "av08_185",
   "page_count": 6,
   "order": 38,
   "p1": "185",
   "pn": "190",
   "abstract": [
    "Recognition of expressed emotion from speech and facial gestures was investigated in experiments on an audio-visual emotional database. A total of 106 audio and 240 visual features were extracted and then features were selected with Plus l-Take Away r algorithm based on Bhattacharyya distance criterion. In the second step, linear transformation methods, principal component analysis (PCA) and linear discriminant analysis (LDA), were applied to the selected features and Gaussian classifiers were used for classification of emotions. The performance was higher for LDA features compared to PCA features. The visual features performed better than audio features, for both PCA and LDA. Across a range of fusion schemes, the audio-visual feature results were close to that of visual features. A highest recognition rate of 53% was achieved with audio features, 98% with visual features, and 98% with audio-visual features selected by Bhattacharyya distance and transformed by LDA.1\n",
    "Index Terms: emotion recognition, multimodal feature selection, principal component analysis 1. Introduction\n",
    ""
   ]
  },
  "kuratate08_avsp": {
   "authors": [
    [
     "Takaaki",
     "Kuratate"
    ]
   ],
   "title": "Text-to-AV synthesis system for Thinking Head Project",
   "original": "av08_191",
   "page_count": 4,
   "order": 39,
   "p1": "191",
   "pn": "194",
   "abstract": [
    "Here we introduce our new text-to-AV (speech and face animation) system created for our Thinking Head project that provides a modular research platform to the AV community. This includes a novel phone-to-face motion module capable of synthesizing face animation from triphone data. Using phoneme timing information from human speech and combining this with information derived from our speech face motion database built from motion capture data, we build correspondences between di- and tri-phones, and face motion. A comparison between face motion synthesized from speech using only our system and face motion generated from motion capture during speech verifies our capability to synthesize AV speech motion with equivalent quality as for motion-capturedriven speech face motion.\n",
    ""
   ]
  },
  "madany08_avsp": {
   "authors": [
    [
     "Katja",
     "Madany"
    ],
    [
     "Sascha",
     "Fagel"
    ]
   ],
   "title": "Objective and perceptual evaluation of parameterizations of 3d motion captured speech data",
   "original": "av08_195",
   "page_count": 4,
   "order": 40,
   "p1": "195",
   "pn": "198",
   "abstract": [
    "In this paper four different approaches of parameterization (dimension reduction) applied to facial speech movements of 3D motion captured speech data were compared. The threedimensional coordinates of 27 markers glued on the face of a speaker articulating a series of German VCV sequences were extracted and parameterized. As methods of dimension reduction a principal component analysis (PCA), and a guided PCA were used. Additionally, we used a newly proposed method called guided non-linear Model Estimation (gnoME). For a better comparison between the methods (especially for comparison with the plain PCA) we also used a non-guided version of gnoME (in the following called noME). In order to evaluate the different approaches of parameterization the speech data was reconstructed by each method using the first five components, respectively. Furthermore, the original motion capture data was re-synthesized. The reconstructed and re-synthesized visual speech stimuli, the original video and the audio signal without video presentation were compared in a perception experiment with respect to intelligibility. As objective measures the explained variance and the mean marker deviation (mean Euclidian error) were used. Results of the objective measures show that the nonlinear Model Estimation represents the data more accurate, while the intelligibility scores from the parameterizations equal those of the original data.\n",
    ""
   ]
  },
  "sato08_avsp": {
   "authors": [
    [
     "Marc",
     "Sato"
    ],
    [
     "Emilie",
     "Troille"
    ],
    [
     "Lucie",
     "Ménard"
    ],
    [
     "Marie-Agnès",
     "Cathiard"
    ],
    [
     "Vincent",
     "Gracco"
    ]
   ],
   "title": "Listening while speaking: new behavioral evidence for articulatory-to-auditory feedback projections",
   "original": "av08_199",
   "page_count": 6,
   "order": 41,
   "p1": "199",
   "pn": "204",
   "abstract": [
    "The existence of feedback control mechanisms from motor to sensory systems is a central idea in speech production research. Consistent with the view that articulation modulates the activity of the auditory cortex, it has been shown that silent articulation improved identification of concordant speech sounds [1]. In the present study, we replicated and extended this finding by demonstrating that, even in the case of perfect perceptual identification, concurrent mouthing of a syllable may speed the perceptual processing of auditory and auditoryvisual speech stimuli. These results provide new behavioral evidence for the existence of motor-to-sensory discharge in speech production and suggest a functional connection between action and perception systems.\n",
    "",
    "",
    "Sams, M., Möttönen, R. and Sihvonen, T., \"Seeing and hearing others and oneself talk\", Brain Res Cogn Brain Res., 23:429- 435, 2005.\n",
    ""
   ]
  },
  "alm08_avsp": {
   "authors": [
    [
     "Magnus",
     "Alm"
    ],
    [
     "Dawn",
     "Behne"
    ]
   ],
   "title": "Age-related experience in audio-visual speech perception",
   "original": "av08_205",
   "page_count": 4,
   "order": 42,
   "p1": "205",
   "pn": "208",
   "abstract": [
    "The current study addresses the changing use of audio and visual cues in audio-visual speech perception during adulthood by testing young and middle-aged adults using stimuli with incongruent audio-visual consonants. Young adults gave more audio responses and fewer visual responses than middle-aged adults. Results show no age differences in auditory or visual sensitivity. The discrepancy in use of modalities is therefore concluded to most likely be due to middle-aged adults being more flexible perceivers than young adults resulting from more audio-visual speech experience.\n",
    ""
   ]
  },
  "hararson08_avsp": {
   "authors": [
    [
     "Þórir",
     "Harðarson"
    ],
    [
     "Hans-Heinrich",
     "Bothe"
    ]
   ],
   "title": "A model for the dynamics of articulatory lip movements",
   "original": "av08_209",
   "page_count": 6,
   "order": 43,
   "p1": "209",
   "pn": "214",
   "abstract": [
    "The present work is part of a framework to design and implement a language laboratory for speech reading/lip reading for multiple languages. It is based on the interdisciplinary project LIPPS at Technical University of Berlin, Germany, which aims to develop a training-aid for speech reading by employing a text-driven facial animation from a single passport photo with the help of 2D image morphing. The LIPPS system may be particularly helpful for patients with a sudden profound hearing-loss, enabling them to start learning speech reading already in the hospital after operation or during subsequent rehabilitation.\n",
    "The present project uses dynamic models for the changes of important visual features. We apply the ideas of i) specific characteristic images being related to the sounds or phonemes of an utterance and ii) visemes being related to the phonemes and represented by the dynamics of linear secondorder models.\n",
    "We aim to extend the idea that visemes are related to single characteristic images or poses of the face towards temporally varying units, as it is the case for the correlating auditory units, the phonemes.\n",
    "We analyzed video clips with moving faces and modeled the prediction of certain visual features at locations of the characteristic images (the characteristic instances) as well as of transitional changes of the feature sets between neighboring characteristic instances. Contextual modulations of the visual features are described with the help of a dominance model. High dominance is given to visemes with indispensable features as, for instance, complete or partial lip closure (e.g., bilabial or fricative visemes), whereas low dominance is given to practically invisible visemes (e.g., guttural visemes), when the lips mainly prepare the transition towards later dominant phonemes.\n",
    "The described method may also be applied to other types of facial animation systems as to the control parameters of anatomical face models.\n",
    ""
   ]
  },
  "krnoul08_avsp": {
   "authors": [
    [
     "Zdeněk",
     "Krňoul"
    ],
    [
     "Patrik",
     "Roštík"
    ],
    [
     "Miloš",
     "Železný"
    ]
   ],
   "title": "Evaluation of synthesized sign and visual speech by deaf",
   "original": "av08_215",
   "page_count": 4,
   "order": 44,
   "p1": "215",
   "pn": "218",
   "abstract": [
    "This paper is focused on an evaluation of quality of synthesized sign speech and a comparison of sign and visual speech. The evaluation has been performed with the Czech sign speech synthesis system. The system produces a manual component as well as a non-manual component given by the lip articulation. The perception test by deaf children from primary school is scored on the isolated signs. Two studies are designed for 5-6 and 11-13 years old pupils. It uses the multiple-choice test composed from picture of the signs to get an intelligibility score. The results confirm intelligibility of the synthesized sign speech as well as visual speech and indicate also statistically a significant difference between perception of sign and visual speech.\n",
    ""
   ]
  },
  "ozgur08_avsp": {
   "authors": [
    [
     "Erol",
     "Ozgur"
    ],
    [
     "Berkay",
     "Yilmaz"
    ],
    [
     "Harun",
     "Karabalkan"
    ],
    [
     "Hakan",
     "Erdogan"
    ],
    [
     "Mustafa",
     "Unel"
    ]
   ],
   "title": "Lip segmentation using adaptive color space training",
   "original": "av08_219",
   "page_count": 4,
   "order": 45,
   "p1": "219",
   "pn": "222",
   "abstract": [
    "In audio-visual speech recognition (AVSR), it is beneficial to use lip boundary information in addition to texture-dependent features. In this paper, we propose an automatic lip segmentation method that can be used in AVSR systems. The algorithm consists of the following steps: face detection, lip corners extraction, adaptive color space training for lip and non-lip regions using Gaussian mixture models (GMMs), and curve evolution using level-set formulation based on region and image gradients fields. Region-based fields are obtained using adapted GMM likelihoods. We have tested the proposed algorithm on a database (SU-TAV) of 100 facial images and obtained objective performance results by comparing automatic lip segmentations with hand-marked ground truth segmentations. Experimental results are promising and much work has to be done to improve the robustness of the proposed method.\n",
    ""
   ]
  },
  "wang08_avsp": {
   "authors": [
    [
     "S. L.",
     "Wang"
    ],
    [
     "Alan Wee-Chung",
     "Liew"
    ]
   ],
   "title": "Static and dynamic lip feature analysis for speaker verification",
   "original": "av08_223",
   "page_count": 5,
   "order": 46,
   "p1": "223",
   "pn": "227",
   "abstract": [
    "As we all know, various speakers have their own talking styles. Hence, lip shape and its movement can be used as a new biometrics and infer the speakers identity. Compared with the traditional biometrics such as human face and fingerprint, person verification based on the lip feature has the advantage of containing both static and dynamic information. Many researchers have demonstrated that incorporating dynamic information such as lip movement help improve the verification performance. However, which is more discriminative, the static features or the dynamic features remained unsolved. In this paper, the discriminative power analysis of the static and dynamic lip features is performed. For the static lip features, a new kind of feature representation including the geometric features, contour descriptors and texture features is proposed and the Gaussian Mixture Model (GMM) is employed as the classifier. For the dynamic features, Hidden Markov Model (HMM) is employed as the classifier for its superiority in dealing with time-series data. Experiments are carried out on a database containing 40 speakers in our lab. Detailed evaluation for various static/dynamic lip feature representation is made along with a corresponding discussion on the discriminative ability. The experimental results disclose that the dynamic lip shape information and the static lip texture information contain much identity-relevant information.\n",
    ""
   ]
  },
  "edge08_avsp": {
   "authors": [
    [
     "James D.",
     "Edge"
    ],
    [
     "Adrian",
     "Hilton"
    ],
    [
     "Philip J. B.",
     "Jackson"
    ]
   ],
   "title": "Parameterisation of 3d speech lip movements",
   "original": "av08_229",
   "page_count": 6,
   "order": 47,
   "p1": "229",
   "pn": "234",
   "abstract": [
    "In this paper we describe a parameterisation of lip movements which maintains the dynamic structure inherent in the task of producing speech sounds. A stereo capture system is used to reconstruct 3D models of a speaker producing sentences from the TIMIT corpus. This data is mapped into a space which maintains the relationships between samples and their temporal derivatives. By incorporating dynamic information within the parameterisation of lip movements we can model the cyclical structure, as well as the causal nature of speech movements as described by an underlying visual speech manifold. It is believed that such a structure will be appropriate to various areas of speech modeling, in particular the synthesis of speech lip movements.\n",
    ""
   ]
  },
  "gocke08_avsp": {
   "authors": [
    [
     "Roland",
     "Göcke"
    ],
    [
     "Akshay",
     "Asthana"
    ]
   ],
   "title": "A comparative study of 2d and 3d lip tracking methods for AV ASR",
   "original": "av08_235",
   "page_count": 6,
   "order": 48,
   "p1": "235",
   "pn": "240",
   "abstract": [
    "Over the past two decades, many algorithms have been proposed to detect and track a human face and its facial features. Of particular interest to the Automatic Speech Recognition (ASR) community are algorithms that can track the shape of the lips, as such visual speech input can then be used in an auditoryvisual (AV) ASR system to improve the recognition accuracy of traditional audio-only ASR systems, particularly in the presence of acoustic noise. Despite the large number of face and lip tracking algorithms that have been proposed over the years, there is a lack of a comparative study that evaluates such algorithms in the context of AV ASR performance. In this paper, the performance of various 2D and 3D lip tracking algorithms is compared from a point of view of AV ASR. In particular, the focus of this study is on algorithms that use explicit lip models. A number of variants of the recently popular Active Appearance Models (AAMs) are compared with a 3D lip tracking algorithm that uses stereo vision. All performance evaluations are made using the AVOZES data corpus.\n",
    ""
   ]
  }
 },
 "sessions": [
  {
   "title": "Invited Papers",
   "papers": [
    "vatikiotisbateson08_avsp",
    "cohn08_avsp",
    "matthews08_avsp"
   ]
  },
  {
   "title": "Contributed Papers",
   "papers": [
    "theobald08_avsp",
    "fels08_avsp",
    "brungart08_avsp",
    "chaloupka08_avsp",
    "giorgolo08_avsp",
    "ishi08_avsp",
    "fagel08_avsp",
    "behne08_avsp",
    "tamura08_avsp",
    "kroos08_avsp",
    "fagel08b_avsp",
    "troille08_avsp",
    "lucey08_avsp",
    "fagel08c_avsp",
    "sakamoto08_avsp",
    "kroger08_avsp",
    "dohen08_avsp",
    "garnier08_avsp",
    "jesse08_avsp",
    "kim08_avsp",
    "bailly08_avsp",
    "chetty08_avsp",
    "wu08_avsp",
    "burnham08_avsp",
    "barbosa08_avsp",
    "dean08_avsp",
    "cosi08_avsp",
    "zelezny08_avsp",
    "bailly08b_avsp",
    "lidestam08_avsp",
    "zoric08_avsp",
    "lucey08b_avsp",
    "barbosa08b_avsp",
    "cox08_avsp",
    "haq08_avsp",
    "kuratate08_avsp",
    "madany08_avsp",
    "sato08_avsp",
    "alm08_avsp",
    "hararson08_avsp",
    "krnoul08_avsp",
    "ozgur08_avsp",
    "wang08_avsp",
    "edge08_avsp",
    "gocke08_avsp"
   ]
  }
 ]
}