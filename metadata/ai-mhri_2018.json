{
 "title": "FAIM/ISCA Workshop on Artificial Intelligence for Multimodal Human Robot Interaction",
 "location": "Stockholm, Sweden",
 "startDate": "14/7/2018",
 "endDate": "15/7/2018",
 "URL": "https://ai-mhri.github.io/",
 "chair": "Chairs: Gérard Bailly, Laura Hiatt, Kristiina Jokinen, Tatsuya Kawahara, Roger Moore, Elin A. Topp",
 "conf": "AI-MHRI",
 "year": "2018",
 "name": "ai-mhri_2018",
 "series": "",
 "SIG": "",
 "title1": "FAIM/ISCA Workshop on Artificial Intelligence for Multimodal Human Robot Interaction",
 "date": "14-15 July 2018",
 "booklet": "ai-mhri_2018.pdf",
 "papers": {
  "bailly18_ai-mhri": {
   "authors": [
    [
     "Gérard",
     "Bailly"
    ],
    [
     "Frédéric",
     "Elisei"
    ]
   ],
   "title": "Demonstrating and Learning Multimodal Socio-communicative Behaviors for HRI: Building Interactive Models from Immersive Teleoperation Data",
   "original": "10",
   "page_count": 5,
   "order": 11,
   "p1": 39,
   "pn": 43,
   "abstract": [
    "The main aim of artificial intelligence (AI) is\nto provide machines with intelligence. Machine\nlearning is now widely used to extract such intelligence\nfrom data. Collecting and modeling multimodal\ninteractive data is thus a major issue for\nfostering AI for HRI. We first discuss the egg-and-chicken\nproblem of collecting ground-truth HRI\ndata without actually disposing of robots with mature\nsocial skills. Particular issues raised by the current\nmultimodal end-to-end mapping frameworks\nare also commented. We then analyze the benefits\nand challenges raised by using immersive teleoperation\nfor endowing humanoid robots with such\nskills. We finally argue for establishing stronger\ngateways between HRI and Augmented/Virtual Reality\nresearch domains."
   ],
   "doi": "10.21437/AI-MHRI.2018-10"
  },
  "yamagami18_ai-mhri": {
   "authors": [
    [
     "Katsuyoshi",
     "Yamagami"
    ],
    [
     "Hirokazu",
     "Kiyomaru"
    ],
    [
     "Sadao",
     "Kurohashi"
    ]
   ],
   "title": "Knowledge-based Dialog Approach for Exploring User's Intention",
   "original": "13",
   "page_count": 4,
   "order": 15,
   "p1": 53,
   "pn": 56,
   "abstract": [
    "In this paper, we address an integrated framework of domain knowledge and dialog system that can understand user’s intention as comprehensively as\npossible through interaction to recommend contents that meet the user’s preferences. The essential concept of our framework is exploring user’s\nintention by controlling topic in dialog through domain knowledge. We consider our framework to be a promising solution especially when the user\ndoes not have enough knowledge of target domain of contents. We also report our preliminary work toward our research goal."
   ],
   "doi": "10.21437/AI-MHRI.2018-13"
  },
  "campbell18_ai-mhri": {
   "authors": [
    [
     "Nick",
     "Campbell"
    ]
   ],
   "title": "Using Multimodal Information to Support Spoken Dialogue Interaction between Humans and Robots without Intrusive Language processing",
   "original": "2",
   "page_count": 5,
   "order": 2,
   "p1": 4,
   "pn": 8,
   "abstract": [
    "This position paper expounds our recent views on autonomous spoken dialogue processing without (or with only minimal use of)\nautomatic speech recognition (ASR). It argues that autonomous systems can be trained to read non-verbal signals which facilitate\ntransition through a pre-prepared or stored utterance sequence so that only minimal processing of actual spoken content is needed.\nOf course no system, machine or human, will be able to continue an extended conversation without understanding the meaning, but\nwe claim that it is not necessary to process each and every spoken word in order to satisfactorily complete an everyday spoken interaction."
   ],
   "doi": "10.21437/AI-MHRI.2018-2"
  },
  "jokinen18_ai-mhri": {
   "authors": [
    [
     "Kristiina",
     "Jokinen"
    ]
   ],
   "title": "AI-based Dialogue Modelling for Social Robots",
   "original": "14",
   "page_count": 4,
   "order": 16,
   "p1": 57,
   "pn": 60,
   "abstract": [
    "The paper seeks to address various issues related to\nsituated interaction between a robot agent and a\nhuman user. The focus is on dialogue modelling\nand implementation of a speech-based multimodal\ndialogue management model in an interactive intelligent\nrobot agent, with the aim to enable the agent\nto conduct natural type of interaction. Several topics\nrelated to the type and representation of\nknowledge and the robot’s focus of attention to select\nappropriate knowledge as the focal point of the\ndialogue are discussed, taking into account the robot’s\ndual characteristics as a powerful computer\nand an interactive agent."
   ],
   "doi": "10.21437/AI-MHRI.2018-14"
  },
  "thellman18_ai-mhri": {
   "authors": [
    [
     "Sam",
     "Thellman"
    ],
    [
     "Tom",
     "Ziemke"
    ]
   ],
   "title": "Studying the Craft of Folk Psychology in HRI",
   "original": "7",
   "page_count": 4,
   "order": 8,
   "p1": 27,
   "pn": 30,
   "abstract": [
    "Human interaction with intelligent autonomous\nsystems depends on the interpretation of behavior\nin terms of mental states. However, studies of mental\nstate attribution to robots have so far focused\nprimarily on folk theories about robots (“How do\npeople think about the mental states of robots?”)\nwithout considering the function of mental state\nattribution in human–robot interactions. This paper\nhighlights a number of limitations to this approach\nand argues the importance of studying: (1)\nrobots in ecologically valid contexts, (2) how specific\nattributions affect people’s ability to predict\nand explain robot behavior, and (3) what causes\npeople to attribute specific kinds of mental states\nto robots. Two novel methodological approaches\nare proposed and discussed."
   ],
   "doi": "10.21437/AI-MHRI.2018-7"
  },
  "bauer18_ai-mhri": {
   "authors": [
    [
     "Adrian Simon",
     "Bauer"
    ],
    [
     "Peter",
     "Birkenkampf"
    ],
    [
     "Alin",
     "Albu-Schäffer"
    ],
    [
     "Daniel",
     "Leidner"
    ]
   ],
   "title": "Bridging the Gap Between Supervised Autonomy and Teleoperation",
   "original": "11",
   "page_count": 4,
   "order": 12,
   "p1": 44,
   "pn": 47,
   "abstract": [
    "Human teleoperation of robots and autonomous\noperations go hand in hand in many of todays service robots.\nWhile robot teleoperation is typically performed on low to\nmedium levels of abstraction, automated planning has to take\nplace on a higher abstraction level, i.e. by means of semantic\nreasoning. Accordingly, an abstract state of the world has to be\nmaintained in order to enable an operator to switch seamlessly\nbetween both operational modes. We propose a novel approach\nthat combines simulation-based geometric tracking and semantic\nstate inference by means of so called State Inference Entities\nto overcome this issue. The system is demonstrated in real-world\nexperiments conducted with the humanoid robot Rollin’ Justin."
   ],
   "doi": "10.21437/AI-MHRI.2018-11"
  },
  "ijuin18_ai-mhri": {
   "authors": [
    [
     "Koki",
     "Ijuin"
    ],
    [
     "Shohei",
     "Fujio"
    ],
    [
     "Albara",
     "Khalifa"
    ],
    [
     "Tsuneo",
     "Kato"
    ],
    [
     "Seiichi",
     "Yamamoto"
    ]
   ],
   "title": "Comparison on Effect of Eye Gaze Activities between Human-human and Human-robot Conversations in Second-Language",
   "original": "5",
   "page_count": 4,
   "order": 5,
   "p1": 19,
   "pn": 22,
   "abstract": [
    "This paper examines how eye gaze activities are\ndifferent in between human-human and human-robot\nconversations in second language (L2). The\nresults show that the mainly-gazed-at listener gazes\nmore at the speaker and he/she takes more often a\nfloor in L2 conversations than in L1 conversations,\nwhereas the speaker’s eye gaze activity is almost\nthe same in both conversations. The result shows\nthat there is a significant positive correlation between\nthe mainly-gazed-at listener’s gazing ratio\nand the ratios of mainly-gazed-at listener taking a\nfloor. Comparative analyses of eye gaze activities\nbetween human-human and human-robot conversations\nare also conducted. The results show that the\nlistener gazes more at the speaker in human-robot\nconversations than in human-human conversations,\nwhereas the robots do not provide the nonverbal information\nrelated to the contents of the utterances.\nThese results may show that listeners gaze more at\nthe speaker to show their intention to take a floor\nin both human-human and human-robot conversations."
   ],
   "doi": "10.21437/AI-MHRI.2018-5"
  },
  "papaioannou18_ai-mhri": {
   "authors": [
    [
     "Ioannis",
     "Papaioannou"
    ],
    [
     "Christian",
     "Dondrup"
    ],
    [
     "Oliver",
     "Lemon"
    ]
   ],
   "title": "Human-Robot Interaction Requires More Than Slot Filling - Multi-Threaded Dialogue for Collaborative Tasks and Social Conversation",
   "original": "15",
   "page_count": 4,
   "order": 17,
   "p1": 61,
   "pn": 64,
   "abstract": [
    "Work on spoken dialogue systems (SDS) has\nlargely been dominated by “slot filling” applications for the\npast decade or more, where information-gathering tasks such\nas restaurant search and flight booking have been the main\nfocus of research. An important class of dialogues, about planning\nand executing tasks, has correspondingly been somewhat\nneglected. However, planning and execution dialogues become\nvery important when we consider Human-Robot Interaction\n(HRI), as does the ability to carry out “social” open-domain\nconversation. This paper describes a new architecture that\nsupports complex multi-threaded task planning and execution\ndialogues, interleaved with “social” dialogue with an open-domain\nchatbot."
   ],
   "doi": "10.21437/AI-MHRI.2018-15"
  },
  "akalin18_ai-mhri": {
   "authors": [
    [
     "Neziha",
     "Akalin"
    ],
    [
     "Andrey",
     "Kiselev"
    ],
    [
     "Annica",
     "Kristoffersson"
    ],
    [
     "Amy",
     "Loutfi"
    ]
   ],
   "title": "Enhancing Social Human-Robot Interaction with Deep Reinforcement Learning",
   "original": "12",
   "page_count": 3,
   "order": 13,
   "p1": 48,
   "pn": 50,
   "abstract": [
    "This research aims to develop an autonomous social\nrobot for elderly individuals. The robot will\nlearn from the interaction and change its behaviors\nin order to enhance the interaction and improve user\nexperience. For this purpose, we aim to use Deep\nReinforcement Learning. The robot will observe\nthe user’s verbal and nonverbal social cues by using\nits camera and microphone, the reward will be\npositive valence and engagement of the user."
   ],
   "doi": "10.21437/AI-MHRI.2018-12"
  },
  "topp18_ai-mhri": {
   "authors": [
    [
     "Elin Anna",
     "Topp"
    ],
    [
     "Jacek",
     "Malec"
    ]
   ],
   "title": "A Knowledge Based Approach to User Support for Robot Programming",
   "original": "8",
   "page_count": 4,
   "order": 9,
   "p1": 31,
   "pn": 34,
   "abstract": [
    "We summarize our successful efforts to support intuitive\nprogramming of industrial robotic assembly\ntasks with a knowledge based approach to the representation\nof skills. These skills can be specified,\nre-used, refined and transferred between robots\nwith the help of a multimodal interface combined\nwith kinesthetic teaching. We argue that while it\nis certainly possible and suitable to have robots\nacquire skills or skill primitives through various\nlearning methods, it is still crucial to provide explicit\nknowledge and semantics available to them."
   ],
   "doi": "10.21437/AI-MHRI.2018-8"
  },
  "rizwan18_ai-mhri": {
   "authors": [
    [
     "Momina",
     "Rizwan"
    ],
    [
     "Volkan",
     "Patoglu"
    ],
    [
     "Esra",
     "Erdem"
    ]
   ],
   "title": "Human-Robot Collaborative Assembly Planning using Hybrid Conditional Planning",
   "original": "6",
   "page_count": 4,
   "order": 6,
   "p1": 23,
   "pn": 26,
   "abstract": [
    "For assembly planning, robots necessitate certain\ncognitive skills: high-level planning of actuation\nactions is needed to decide for the order of actuation\nactions, while geometric reasoning is needed\nto check the feasibility of these actions. For collaborative\nassembly tasks with humans, robots require\nfurther cognitive capabilities, such as commonsense\nreasoning, sensing, and communication\nskills, not only to cope with the uncertainty caused\nby incomplete knowledge about the humans’ behaviors\nbut also to ensure safe collaborations. We\nintroduce a novel formal framework for collaborative\nassembly planning that utilizes hybrid conditional\nplanning extended with commonsense reasoning\nand a rich set of communication actions for\ncollaborative tasks. We evaluate this method by a\nset of experiments in a furniture assembly domain."
   ],
   "doi": "10.21437/AI-MHRI.2018-6"
  },
  "maraev18_ai-mhri": {
   "authors": [
    [
     "Vladislav",
     "Maraev"
    ],
    [
     "Chiara",
     "Mazzocconi"
    ],
    [
     "Christine",
     "Howes"
    ],
    [
     "Jonathan",
     "Ginzburg"
    ]
   ],
   "title": "Integrating laughter into spoken dialogue systems: preliminary analysis and suggested programme",
   "original": "3",
   "page_count": 6,
   "order": 3,
   "p1": 9,
   "pn": 14,
   "abstract": [
    "This paper presents an exploratory scheme, which\naims at investigating perceptual features that characterise\nlaughables (the arguments laughter is related\nto) in dialogue context. We present the results\nof a preliminary study and sketch an updated questionnaire\non laughables types and laughter functions\naimed to be used for Amazon Mechanical\nTurk experiments. Furthermore we present preliminary\nprogramme for integrating laughter into spoken\ndialogue systems."
   ],
   "doi": "10.21437/AI-MHRI.2018-3"
  },
  "huang18_ai-mhri": {
   "authors": [
    [
     "Hung-Hsuan",
     "Huang"
    ],
    [
     "Seiya",
     "Kimura"
    ],
    [
     "Kazuhiro",
     "Kuwabara"
    ],
    [
     "Toyoaki",
     "Nishida"
    ]
   ],
   "title": "Proposal of a Multimodal Framework for Generating Robot’s Spontaneous Attention Directions and Nods in Group Discussion",
   "original": "4",
   "page_count": 4,
   "order": 4,
   "p1": 15,
   "pn": 18,
   "abstract": [
    "Our ongoing project is aiming to build a robot that can participate group discussion, so that its users can repeatedly practice group discussion with\nit. In this paper, we propose a multimodal framework to incorporate the modules to generate spontaneous head movements, shifts of attention focus\nand nodding of the robot. The generation models are derived from human-human group discussion data corpus with support vector classifiers. Dedicated models are developed according to conversation situations: when the robot is speaking, when the robot is listening to other participants, and when\nno participant is speaking. Low-level verbal and non-verbal (speech turn, prosody, face direction, and head activities) features extracted from the participants other than the focused one (the robot) are adopted in the learning process."
   ],
   "doi": "10.21437/AI-MHRI.2018-4"
  },
  "philipsen18_ai-mhri": {
   "authors": [
    [
     "Mark",
     "Philipsen"
    ],
    [
     "Matthias",
     "Rehm"
    ],
    [
     "Thomas",
     "Moeslund"
    ]
   ],
   "title": "Industrial Human-Robot Collaboration",
   "original": "9",
   "page_count": 4,
   "order": 10,
   "p1": 35,
   "pn": 38,
   "abstract": [
    "In the future, robots are envisioned to work side by\nside with humans in dynamic environments both in\nmanufacturing and in societal contexts like health\ncare, education, and commerce. Before this vision\ncan be realized, robots must be socially accepted.\nAcceptance will have to be build through improvements\nin robot adaptability and through a gradual\nintroduction, where robots learn on the job. It is\nour conviction that this can be achieved through\na combination of human-robot interaction, multimodal\nsignal processing and AI techniques. We\nseek to prove this in real world applications. We\ndiscuss how we intend to utilize end-users in the\ncontinuous training and refinement of AIs and we\nhighlight some of the challenges involved in building\ncollaborative production cells."
   ],
   "doi": "10.21437/AI-MHRI.2018-9"
  },
  "andre18_ai-mhri": {
   "authors": [
    [
     "Elisabeth",
     "André"
    ]
   ],
   "title": "Socially-Sensitive Technologies for HRI: Challenges and Perspectives",
   "original": "1",
   "page_count": 3,
   "order": 1,
   "p1": 1,
   "pn": 3,
   "abstract": [
    "Starting the recent years, a significant amount of effort\nhas been dedicated to explore the potential of\nsocial signal processing in human interaction with\nembodied conversational agents and social robots.\nWhile there is a proliferation of studies that investigate\nspecific aspects of embodied social interaction\nunder laboratory conditions, less attention has\nbeen paid to the design and realization of naturalistic\nsocial settings in which artificial agents autonomously\ninteract with human users. To bring social\nagents to the people’s daily environment, user-agent\ncommunication should be properly situated\nin the context of the application at hand rather than\nisolated as a laboratory experiment. The objective\nof the paper is to identify relevant knowledge and\nreasoning capabilities to enhance the social sensitivity\nof artificial agents."
   ],
   "doi": "10.21437/AI-MHRI.2018-1"
  },
  "pandey18_ai-mhri": {
   "authors": [
    [
     "Amit Kumar",
     "Pandey"
    ]
   ],
   "title": "An industrial perspective on AI needs for Multimodal HRI",
   "original": "abs2",
   "page_count": 2,
   "order": 14,
   "p1": 51,
   "pn": 52,
   "abstract": [
    "Never before in the history of robotics, robots have been so close to us, in our society. We are ‘evolving’, so as our society, lifestyle and the needs. AI has been with us for decades, and now embodied in robots, penetrating more in our day-to-day life. All these are converging towards creating a smarter ecosystem of living, where social robots will coexist with us in harmony, for a smarter, healthier, safer and happier life. Such robots are supposed to be socially intelligent and behave in socially expected and accepted manners. The talk will reinforce that social robots have a range of potential societal applications and hence impacting the education needs and job opportunities as well. The talk will begin with illustrating some of the social robots and highlight what does it mean to develop a socially intelligent robot, and the associated R&D challenges. This will be followed by some use cases, end user feedback and the market analysis. The talk will conclude with some open challenges ahead, including social and ethical issues and emphasize on the greater need of a bigger and multi-disciplinary effort and eco-system of different stakeholders including policy makers."
   ]
  },
  "scheutz18_ai-mhri": {
   "authors": [
    [
     "Matthias",
     "Scheutz"
    ]
   ],
   "title": "Is that what you want? Architectural Challenges of Engaging in Multi-Modal Natural Language Interactions with Humans",
   "original": "abs1",
   "page_count": 0,
   "order": 7,
   "p1": "",
   "pn": "",
   "abstract": [
    "Task-based natural language interactions with robots naturally involve\nmulti-modal aspects, from simple gestures accompanying utterances, to\nthe resolution of complex referential expressions that require\nperceptual integration and reasoning.  In this presentation, I will\ndiscuss several architectural challenges of integrated natural language\nunderstanding, perception and action that have to be addressed for\nrobots to be able to handle natural multi-modal interactions with\nhumans.  These challenges include resolving references appropriately in\nopen worlds using perceptual information and common sense reasoning,\nunderstanding intended meanings in indirect speech acts, and\nautomatically applying normative constraints throughout the\ninteractions.  I will illustrate the various conceptual points with\nexamples and robot demonstrations from our own attempts to tackle some\nof these challenges.\n"
   ]
  }
 },
 "sessions": [
  {
   "title": "Keynote: Elisabeth André (chair: G. Bailly)",
   "papers": [
    "andre18_ai-mhri"
   ]
  },
  {
   "title": "Socially sensitive technologies (chair: E. André)",
   "papers": [
    "campbell18_ai-mhri",
    "maraev18_ai-mhri"
   ]
  },
  {
   "title": "Cognition: mindreading and reasoning (chair: G. Bailly)",
   "papers": [
    "huang18_ai-mhri",
    "ijuin18_ai-mhri",
    "rizwan18_ai-mhri"
   ]
  },
  {
   "title": "Keynote: Matthias Scheutz (chair: E. A. Topp)",
   "papers": [
    "scheutz18_ai-mhri"
   ]
  },
  {
   "title": "Interaction frameworks (chair: M. Scheutz)",
   "papers": [
    "thellman18_ai-mhri",
    "topp18_ai-mhri",
    "philipsen18_ai-mhri"
   ]
  },
  {
   "title": "Data and AI technologies for HRI (chair: K. Jokinen)",
   "papers": [
    "bailly18_ai-mhri",
    "bauer18_ai-mhri",
    "akalin18_ai-mhri"
   ]
  },
  {
   "title": "Keynote: Amit Kumar Pandey (chair: K. Jokinen)",
   "papers": [
    "pandey18_ai-mhri"
   ]
  },
  {
   "title": "Robots and dialogue modelling (chair: A.K. Pandey)",
   "papers": [
    "yamagami18_ai-mhri",
    "jokinen18_ai-mhri",
    "papaioannou18_ai-mhri"
   ]
  }
 ],
 "doi": "10.21437/AI-MHRI.2018"
}