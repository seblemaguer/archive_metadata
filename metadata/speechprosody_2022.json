{
 "﻿series": "SpeechProsody",
 "title": "Speech Prosody 2022",
 "location": "Lisbon, Portugal",
 "startDate": "23/5/2022",
 "endDate": "26/5/2022",
 "URL": "http://labfon.letras.ulisboa.pt/sp2022/index.html",
 "chair": "Chairs: Sónia Frota and Marina Vigário",
 "intro": "booklet.pdf",
 "ISSN": "2333-2042",
 "conf": "SpeechProsody",
 "year": "2022",
 "name": "speechprosody_2022",
 "series": "SpeechProsody",
 "SIG": "SProSIG",
 "title1": "Speech Prosody 2022",
 "date": "23-26 May 2022",
 "booklet": "speechprosody_2022.pdf",
 "papers": {
  "grice22_speechprosody": {
   "authors": [
    [
     "Martine",
     "Grice"
    ],
    [
     "Simon",
     "Wehrle"
    ]
   ],
   "title": "Prosody and Conversational Behaviour in Autism Spectrum Disorder",
   "original": "k1",
   "page_count": 0,
   "order": 1,
   "p1": "",
   "pn": "",
   "abstract": [
    "This talk will discuss how adults with Autism Spectrum Disorder (ASD) use prosody in communication. The studies reported on will include perception and production in isolated sentences, monologues and more interactive dyadic tasks, the latter with a special focus on conversational behaviours such as turn transitions and feedback signals (backchannels). The vast majority of previous studies on prosody and communication in ASD are based on the speech of autistic children or adolescents, usually elicited either as monologues or in dialogues with non-autistic adults. Conflicting reports in the literature are likely to be related to differing developmental trajectories and a failure to appropriately account for individual-specific behaviour. Moreover, very little is known not only about the behaviour of autistic adults (as opposed to children), but also about how autistic adults communicate with each other (as opposed to with non-autistic adults). We show that in-depth, multi-dimensional analysis is necessary to reveal the variable behaviour of different individuals and dyads on the autism spectrum and that, although difficulties in interpersonal communication belong to the defining characteristics of the disorder, different individuals have their own strengths and weaknesses and also compensate for any potential difficulties in their own way."
   ]
  },
  "lorenzen22_speechprosody": {
   "authors": [
    [
     "Janne",
     "Lorenzen"
    ],
    [
     "Simon",
     "Roessig"
    ],
    [
     "Stefan",
     "Baumann"
    ]
   ],
   "title": "Information status and tonal context jointly modulate prosodic prominence relations in German",
   "original": "69",
   "page_count": 5,
   "order": 5,
   "p1": 7,
   "pn": 11,
   "abstract": [
    "We conducted an interactive online production experiment on German in which participants were asked to read aloud stories for a fellow player who then had to sort picture cards corresponding to single sentences of the stories in the correct order. The target sentences contained two target words, an indirect object followed by a direct object, which were either new or accessible in their discourse context. Our aim was to investigate the paradigmatic and syntagmatic effects of information status as well as the (syntagmatic) influence of tonal context on the prosodic prominence relation between the target words. Results show that (i) new referents are generally marked by more prominent accent types than accessible referents, (ii) the prominence of sentence-final accents increases/decreases when their referents are newer/more accessible than non-final referents (carrying ‘medial’ accents), and (iii) the type of medial accent determines the relation between accent type and information status in the final pitch accent. The results support the idea of a prominence budget that is distributed across pitch accents within an utterance. Furthermore, modulations of a specific distribution of prominences may reflect changes in both meaning-related factors (such as information status) and form-related factors (such as accent type) simultaneously."
   ],
   "doi": "10.21437/SpeechProsody.2022-2"
  },
  "ip22_speechprosody": {
   "authors": [
    [
     "Martin Ho Kwan",
     "Ip"
    ],
    [
     "Alex de",
     "Carvalho"
    ],
    [
     "John",
     "Trueswell"
    ]
   ],
   "title": "Prosody-to-Focus Mapping and Alternative Processing in Word Learning",
   "original": "119",
   "page_count": 5,
   "order": 6,
   "p1": 12,
   "pn": 16,
   "abstract": [
    "The present experiment examined two issues concerning the role of prosody in word learning. First, we explored whether learners use prosodic focus to map words produced with contrastive stress onto contextually new visual referents. Second, we asked whether prosodic focus facilitates better memory for focused words and their contextual alternatives. In an eyetracking task, 48 monolingual English-speaking adults (18-40 yrs-old) were familiarized with videos of different people doing different actions. At test, participants saw two side-by-side videos, one showing a novel person performing a familiar action, and the other a familiar person performing a novel action. Participants then heard an utterance with prosodic focus on the noun or the verb (e.g., “Now JOHNNY is blicking!” vs. “Now Johnny is BLICKING!”). As predicted, participants paired the prosodically focused word with the contextually novel referent; participants who heard name-focused sentences looked longer at the novel person, while those who heard verb- focused sentences looked longer to the novel action. However, verb-focused sentences during word learning led to better recall of people’s names than name-focused sentences. Further, eyegaze turns to alternative events during word learning led to better recall, indicating that eye movements during word learning support and reflect alternative processing."
   ],
   "doi": "10.21437/SpeechProsody.2022-3"
  },
  "strickland22_speechprosody": {
   "authors": [
    [
     "Emmett",
     "Strickland"
    ],
    [
     "Anne",
     "Lacheret-Dujour"
    ],
    [
     "Candide",
     "Simard"
    ]
   ],
   "title": "Prosody and cognitive accessibility in left-detached topics: lessons from Nigerian Pidgin",
   "original": "8",
   "page_count": 5,
   "order": 7,
   "p1": 17,
   "pn": 21,
   "abstract": [
    "In Nigerian Pidgin, or Naija, a language spoken by some 100 million people in Nigeria, the role of prosody in topic-marking remains an understudied question, particularly with regards to the way in which speakers distinguish between topics which are readily present in the minds of their interlocutors, and less cognitively accessible ones such as those being introduced for the first time. Advances in the field of natural language processing have yielded effective methods for the automatic extraction of stylized prosodic contours from aligned sound files, facilitating the wide-scale study of syntactic units and their melodic properties. Through a combination of manual pragmatic annotation, automatic prosodic stylization, and exploratory statistical analysis, we aim to better understand the prosodic differences between categories of left-detached topics. We hypothesize that there exists a clear relationship between the cognitive accessibility of topics and their associated prosodic contours. More precisely, the most cognitively accessible topics will be characterized by falling F0 contours with low fundamental frequency ranges. Conversely, we consider that less accessible topics will be associated with rising contours and ones covering wide frequency ranges. We also postulate that contours ending in a very high final pitch will be almost exclusive to less accessible topics."
   ],
   "doi": "10.21437/SpeechProsody.2022-4"
  },
  "hu22_speechprosody": {
   "authors": [
    [
     "Na",
     "Hu"
    ],
    [
     "Aoju",
     "Chen"
    ],
    [
     "Fang",
     "Li"
    ],
    [
     "Hugo",
     "Quené"
    ],
    [
     "Ted",
     "Sanders"
    ]
   ],
   "title": "A Trade-off Relationship between Lexical and Prosodic Means in Expressing Subjective and Objective Causality",
   "original": "265",
   "page_count": 5,
   "order": 8,
   "p1": 22,
   "pn": 26,
   "abstract": [
    "Prosody carries a variety of information in spoken communication. However, how it is involved in expressing the structure of discourse remains less well understood. This study addressed this issue by investigating the use of prosody in expressing two types of causality: subjective and objective causality. Specifically, we explored the trade-off between prosody and lexical means, i.e., specialized causal connectives. Several theories suggested that when these two types of causality were expressed by specialized causal connectives, as is the case in Mandarin, it would be unnecessary to use prosody to distinguish them because the specialized causal connectives already specified the type of causality. On the other hand, when the two types of causality were expressed by a general causal connective as in English, it would be necessary to use prosody because their distinctions are not explicit at the lexical level. To test these hypotheses, we conducted two production experiments, with Experiment 1 investigating the prosodic realizations of subjective and objective causality in Mandarin and Experiment 2 in English. We used a self-designed dialogue task to elicit speech samples and a Bayesian approach to evaluate the effects of subjectivity on measurements. The results of these two experiments support the hypotheses."
   ],
   "doi": "10.21437/SpeechProsody.2022-5"
  },
  "seeliger22_speechprosody": {
   "authors": [
    [
     "Heiko",
     "Seeliger"
    ],
    [
     "Constantijn",
     "Kaland"
    ]
   ],
   "title": "Boundary tones in German wh-questions and wh-exclamatives - a cluster-based approach",
   "original": "70",
   "page_count": 5,
   "order": 9,
   "p1": 27,
   "pn": 31,
   "abstract": [
    "We present data from a production study investigating verb-final German wh-questions and wh-exclamatives. We focus here on the variety of boundary tones found in the data. While most exclamatives were produced with falling contours and most questions with rising ones, there was substantial overlap between the two speech acts. Many utterance-final pitch offsets and pitch movements were also ambiguous in height and direction, providing a challenge for manual annotation. We apply a contour-based cluster analysis to the data in order to semi-automatically group intonation contours. We show that a cluster analysis based on the entire contour can shed light on the variety of boundary tones, although some clusters are 'mixed' and contain both high and low boundary tones. We then supplement this analysis with another cluster analysis based on the final two syllables of the utterance, which succeeds in splitting the mixed clusters. Finally, we use the combined cluster analysis to identify and describe the ambiguous boundary tones and argue that their form-to-function mapping falls outside the scope of the canonical inventory of GToBI boundary tones. Specifically, we found late falls after high plateaus and level, medium-high plateaus that are distinct both from continuation rises and calling contours."
   ],
   "doi": "10.21437/SpeechProsody.2022-6"
  },
  "song22_speechprosody": {
   "authors": [
    [
     "Yu Jin",
     "Song"
    ],
    [
     "Cynthia G.",
     "Clopper"
    ],
    [
     "Laura",
     "Wagner"
    ]
   ],
   "title": "Children’s Use of Uptalk in Narratives",
   "original": "58",
   "page_count": 5,
   "order": 10,
   "p1": 32,
   "pn": 36,
   "abstract": [
    "Uptalk refers to the use of rising intonation on declarative utterances. Previous research has shown that, at age 6 years, children use rising contours with declaratives more frequently than adults, and this pattern appears to persist until 14 years of age. However, it is unclear why such a trend persists. To gain a clearer developmental picture of uptalk, the present study analyzed the form and function of uptalk produced by children aged 6 to 7 and 10 to 11 years from the American Midwest, using a storytelling task. Contrary to previous findings, the results indicate that children of both age groups use uptalk in an adult-like way: they overwhelmingly favor L-H% over H-H% boundary tones, and most strongly associate the contour with continuation. The lack of age differences suggests that children’s use of uptalk is comparable to that of adults by the age of 6, at least in certain narrative contexts. The use of a familiar storytelling task in the current study may explain the greater success observed for children than in previous studies, suggesting the relative importance of the elicitation task in the investigation of child speech.\n"
   ],
   "doi": "10.21437/SpeechProsody.2022-7"
  },
  "wehrle22_speechprosody": {
   "authors": [
    [
     "Simon",
     "Wehrle"
    ],
    [
     "Francesco",
     "Cangemi"
    ],
    [
     "Kai",
     "Vogeley"
    ],
    [
     "Martine",
     "Grice"
    ]
   ],
   "title": "New evidence for melodic speech in Autism Spectrum Disorder",
   "original": "76",
   "page_count": 5,
   "order": 11,
   "p1": 37,
   "pn": 41,
   "abstract": [
    "Since the very beginnings of research into Autism Spectrum Disorder (ASD), there have been contradicting descriptions of speech in ASD as being “singsongy” or melodic on the one hand and “robotic” or monotonous on the other. We highlight some issues regarding the terminology and methodologies used in previous studies as well as their comparability, concluding that previous accounts, particularly of monotonous speech in ASD, may have been misleading. We expand on a previous pilot study in using the same method of quantifying the spaciousness and liveliness of speech along two dimensions in order to analyse an extended data set (~ 5 hours) of semi-spontaneous conversations. We compare 14 German adults diagnosed with ASD and 14 matched control speakers (CTR), recorded in disposition-matched dyads (ASD-ASD; CTR-CTR). Using Bayesian modelling, we present evidence that most (but not all) ASD speakers in our corpus produced a more melodic intonation style than non-autistic CTR speakers, while, crucially, none produced a more monotonous intonation style. We emphasise the importance of inter-individual variability in groups of autistic speakers and point out that our results align with a clear tendency in recent studies to report more melodic speech in ASD."
   ],
   "doi": "10.21437/SpeechProsody.2022-8"
  },
  "lehnertlehouillier22_speechprosody": {
   "authors": [
    [
     "Heike",
     "Lehnert-LeHouillier"
    ],
    [
     "Steven",
     "Snadoval"
    ]
   ],
   "title": "Conversational Correlates of Prosodic Entrainment in Youth with and without Autism Spectrum Disorder",
   "original": "132",
   "page_count": 5,
   "order": 12,
   "p1": 42,
   "pn": 46,
   "abstract": [
    "Research on prosodic entrainment has shown correlations between the degree of prosodic entrainment and several dimensions of conversational success. Individuals with autism spectrum disorder (ASD) often encounter difficulties with a variety of skills necessary for conversational success, especially with the social dimensions of conversational behavior. The goal of the current study was to investigate whether children and teens with an autism diagnosis show similar correlations between prosodic entrainment in mean fundamental frequency (f0) and conversational effectiveness, duration of conversations, and conversational turn-taking behavior when compared to their neurotypical peers. Significant interaction effects by group were found between mean f0 entrainment on the one hand and conversational duration, conversational effectiveness, and number of turns per speaker on the other. However, we found no overall group mean differences in conversational effectiveness, duration of conversations, and the number of turns speakers in each group used. These results suggest that even though speakers with ASD may show surface conversational behaviors similar to their neurotypical peers, the prosodic manifestation of conversational speech clearly marks conversation partners with ASD as different from their neurotypical peers."
   ],
   "doi": "10.21437/SpeechProsody.2022-9"
  },
  "daigmorte22_speechprosody": {
   "authors": [
    [
     "Chloé",
     "Daigmorte"
    ],
    [
     "Jessica",
     "Tallet"
    ],
    [
     "Corine",
     "Astésano"
    ]
   ],
   "title": "On the foundations of rhythm-based methods in Speech Therapy",
   "original": "201",
   "page_count": 5,
   "order": 13,
   "p1": 47,
   "pn": 51,
   "abstract": [
    "Speech-Language Pathologists (SLPs) use a variety of rhythm-based methods for speech and language rehabilitation, but their theoretical foundations remain imprecise. In order to investigate what “rhythm” refers to in speech therapy, we conducted an online survey among French-speaking SLPs, exploring their representations and practices involving rhythm. 398 French-speaking SLPs completed the survey. Results are in part puzzling. While SLPs frequently use rhythm-based interventions for oral language disorders, and intuitively consider rhythm as “a prerequisite for oral language development,” they do not primarily link rhythm to a linguistic function. This is probably due to the historical psychomotor anchoring of French speech therapy, integrating mainly physio-cognitive aspects of rhythm. It may also explain why they relate rhythm deficits to widely different disorders: those where speech rhythm is noticeably impaired (prosodic disfluencies), and certain disorders showing no such impairment. Finally, SLPs reckon lacking insight on the underlying concepts and uses of rhythm. We argue that reconciling linguistic, psychological and motor concepts of rhythm brings coherence to SLPs’ practices, and gives credit to rhythm-based interventions. It also opens the way to the development of relevant assessment tools integrating all aspects of rhythm—which, as reported by our participants, are still cruelly lacking."
   ],
   "doi": "10.21437/SpeechProsody.2022-10"
  },
  "pettorino22_speechprosody": {
   "authors": [
    [
     "Massimo",
     "Pettorino"
    ],
    [
     "Marta",
     "Maffia"
    ],
    [
     "Brigitte",
     "Bigi"
    ]
   ],
   "title": "A diachronic study on Italian speech rhythm in Parkinson’s Disease",
   "original": "216",
   "page_count": 5,
   "order": 14,
   "p1": 52,
   "pn": 56,
   "abstract": [
    "Parkinson’s Disease dysarthria affects the speech motor control, causing alterations at the suprasegmental level of speech. In previous researches, vowel percentage (%V) and the mean interval between two consecutive vowel onset points (VtoV) were effectively used in the synchronic description of the rhythmic variations of Italian PD speech, compared to healthy speech, even at a very early stage of the disease.\nThis study aims at verifying the early alteration of PD speech rhythm using a diachronic approach. To reach this goal, a corpus of read speech produced by a single PD subject (female, 66 years old) has been collected, consisting of 15 radiophonic speech samples (about 100 s each) on the same topic, recorded between 2001 and 2021.\nThe speech samples were manually segmented in consonantal and vocalic intervals by means of Praat, allowing the calculation of %V and VtoV.\nThe results show an alteration of %V values since 2018, two years before the diagnosis and the insurgence of motor symptoms.\nMoreover, first results of the application of the automatic segmentation performed by SPPAS on a selection of PD speech samples will also be presented."
   ],
   "doi": "10.21437/SpeechProsody.2022-11"
  },
  "boecher22_speechprosody": {
   "authors": [
    [
     "Janina",
     "Boecher"
    ],
    [
     "Kathryn",
     "Franich"
    ],
    [
     "Evan",
     "Usler"
    ]
   ],
   "title": "Rhythm of Speech in People Who Do and Do Not Stutter - A Quantitative Analysis Using the Normalized Pairwise Variability Index",
   "original": "230",
   "page_count": 5,
   "order": 15,
   "p1": 57,
   "pn": 61,
   "abstract": [
    "Aberrant speech rhythm has previously been identified as a hallmark of stuttering (Dalton & Hardcastle, 1977). However, empirical evidence of dysrhythmic speech in people who stutter (PWS) is currently sparse. The present study is a quantitative analysis of speech in PWS and fluent peers (PWNS). We hypothesized that the fluent utterances of PWS would be more rhythmic than those that contain disfluency. Furthermore, it was expected that the fluent utterances of PWS would be less rhythmic than PWNS. Speech data was obtained from the Voices-AWS-Corpus available via FluencyBank (Bernstein Ratner & MacWhinney, 2008). PWNS were recruited independently. Participants completed a reading passage (SSI-4; PWS: n = 24; PWNS: n = 11) and an interview task (PWS: n = 21; PWNS: n = 11). Speech rhythm was quantified using the normalized pairwise variability index (nPVI; Grabe & Low, 2008). Results revealed that utterances leading up to a disfluency were associated with greater variability in vowel and intervocalic interval duration within utterances (= higher nPVI) than fluent utterances in PWS (p = .04). Although mean nPVI did not differ between PWS and PWNS, PWS showed greater intra-speaker variability in terms rhythm across utterances (p < .001)."
   ],
   "doi": "10.21437/SpeechProsody.2022-12"
  },
  "martin22_speechprosody": {
   "authors": [
    [
     "Vincent P.",
     "Martin"
    ],
    [
     "Brice",
     "Arnaud"
    ],
    [
     "Jean-Luc",
     "Rouas"
    ],
    [
     "Pierre",
     "Philip"
    ]
   ],
   "title": "Does sleepiness influence reading pauses in hypersomniac patients?",
   "original": "14",
   "page_count": 5,
   "order": 16,
   "p1": 62,
   "pn": 66,
   "abstract": [
    "Sleepiness and Excessive Daytime Sleepiness (EDS) are major public health concerns impacting the daily life and performances of subjects experiencing them. Usually measured by electroencephalographic measures or self-reported questionnaires, previous studies have shown that it is possible to measure them through voice analysis. In this article, we propose to investigate potential new vocal biomarkers of sleepiness and EDS on 93 patients affected by hypersomnia, namely reading pauses. We analyze the location and duration of the pauses annotated by a fully automated system and propose a new set of speech features. Based on these 12 descriptors, we have identified seven reading behavior profiles, that are almost fully explained by the physical and medical characteristics of the patients, including their level of EDS. Regarding short-term sleepiness, the observed differences are mainly due to the differences of texts and have a weak correlation with objective and subjective sleepiness measures."
   ],
   "doi": "10.21437/SpeechProsody.2022-13"
  },
  "ward22_speechprosody": {
   "authors": [
    [
     "Nigel",
     "Ward"
    ],
    [
     "Ambika",
     "Kirkland"
    ],
    [
     "Marcin",
     "Wlodarczak"
    ],
    [
     "Éva",
     "Székely"
    ]
   ],
   "title": "Two Pragmatic Functions of Breathy Voice in American English Conversation",
   "original": "62",
   "page_count": 5,
   "order": 20,
   "p1": 82,
   "pn": 86,
   "abstract": [
    "Although the paralinguistic and phonological significance of breathy voice is well known, its pragmatic roles have been little studied. We report a systematic exploration of the pragmatic functions of breathy voice in American English, using a small corpus of casual conversations, using the Cepstral Peak Prominence Smoothed measure as an indicator of breathy voice, and using a common workflow to find prosodic constructions and identify their meanings. We found two prosodic constructions involving breathy voice. The first involves a short region of breathy voice in the midst of a region of low pitch, functioning to mark self-directed speech. The second involves breathy voice over several seconds, combined with a moment of wider pitch range leading to a high pitch over about a second, functioning to mark an attempt to establish common ground. These interpretations were confirmed by a perception experiment."
   ],
   "doi": "10.21437/SpeechProsody.2022-17"
  },
  "portes22_speechprosody": {
   "authors": [
    [
     "Cristel",
     "Portes"
    ],
    [
     "Uwe",
     "Reyle"
    ]
   ],
   "title": "Combining syntax and prosody to signal information structure: the case of French",
   "original": "196",
   "page_count": 5,
   "order": 21,
   "p1": 87,
   "pn": 91,
   "abstract": [
    "The work on informational structure (IS) in French highlights two types of markers: syntactic constructions and prosody, but often without looking at their interactions. The only studies that focus on both aspects have studied the prosody of syntactic constructions themselves without conceiving the complementarity of syntax and prosody. Our perspective is different and shows that syntax and prosody operate both independently and jointly to shape the informational structure of French.\nThis paper relies on the extensive analysis of a 45 min radio debate, entirely annotated for IS, syntax and prosody. For IS, we used an annotation procedure that retrieves the implicit question under discussion (QUD) for each utterance, and defines its focus, focus domain, potential contrastive topic, topic and not-at-issue contents. For syntax, we identified the constructions that have been proposed to encode IS: clefts, left and right dislocation, presentationals and subject verb inversion. For prosody, we used a phonological approach and the French ToBI framework.\nThe intersection of syntactic, prosodic and QUD analyses show that, indeed the syntactic constructions cited above encode topic, focus and background, that prosody alone encodes IS in sentences without these constructions, but crucially, that syntax and prosody interplay in conveying more subtle IS organization."
   ],
   "doi": "10.21437/SpeechProsody.2022-18"
  },
  "pronina22_speechprosody": {
   "authors": [
    [
     "Mariia",
     "Pronina"
    ],
    [
     "Iris",
     "Hübscher"
    ],
    [
     "Ingrid",
     "Vilà-Giménez"
    ],
    [
     "Pilar",
     "Prieto"
    ]
   ],
   "title": "Pragmatic prosody development from 3 to 8 years of age: A cross-sectional study in Catalan",
   "original": "103",
   "page_count": 5,
   "order": 22,
   "p1": 92,
   "pn": 96,
   "abstract": [
    "Research on prosodic development has mostly focused on infants’ skills, and there is much less research on preschool and older children’s prosodic abilities and in particular on the advanced pragmatic uses of prosody (i.e., pragmatic prosody). The present cross-sectional study assesses children’s expressive pragmatic prosody profiles in three developmental stages. A total of 167 3-to-8-year-old Catalan-speaking children undertook the Audiovisual Pragmatic Test, which includes 35 everyday scenarios and elicits different speech act types. A total of 5697 answers were scored for prosodic appropriateness: 34%, 73% and 90% of the prosodically felicitous answers were produced at ages 3–4, 5–6 and 7–8, correspondingly. The results showed that, at all ages, children successfully produced basic expressive acts, unbiased assertions and requests. A significant difference was found between the performance of biased vs. unbiased, and basic vs. complex expressive acts at ages 3 and 5 (ps < .010, ps < .05), but not at 8 (ps > .05). Between ages 3 and 6 years, children develop pragmatic unbiased and basic expressive marking through prosody, while the biased and complex expressive meanings are refined later, between ages 6 and 8 years. Our finding suggest that prosodic abilities undergo developmental changes throughout childhood."
   ],
   "doi": "10.21437/SpeechProsody.2022-19"
  },
  "kim22_speechprosody": {
   "authors": [
    [
     "Jiseung",
     "Kim"
    ],
    [
     "Anja",
     "Arnhold"
    ]
   ],
   "title": "Prosodic focus marking in Canadian English",
   "original": "239",
   "page_count": 4,
   "order": 23,
   "p1": 97,
   "pn": 100,
   "abstract": [
    "The current study investigated how broad focus, narrow focus and given information are produced in Canadian English. Given previous findings that showed different varieties of English signal information structure differently, we hypothesized that the effects of focus on acoustic correlates involving duration, f0, and intensity would manifest differently in Canadian English than in American English. Thirty-eight native speakers of Canadian English produced 24 short transitive sentences in different focus conditions: broad focus and narrow focus in different locations (Subject, Verb, Object). A total of 2,736 words were analyzed. While some acoustic correlates such as duration and maximum intensity replicated the same patterns as previous findings in American English, mean intensity and f0 measures showed different patterns. These results suggest that speakers of Canadian English may employ a different set of acoustic correlates from speakers of American English. The study sheds light on the role of dialect in the production of broad and narrow focus and expands our knowledge about the fine-grained details of the phonetic realization of prosodic focus marking in English."
   ],
   "doi": "10.21437/SpeechProsody.2022-20"
  },
  "thorson22_speechprosody": {
   "authors": [
    [
     "Jill",
     "Thorson"
    ],
    [
     "Jill",
     "Trumbell"
    ],
    [
     "Kimberly",
     "Nesbitt"
    ]
   ],
   "title": "Expressing information status through prosody in the spontaneous speech of American English-speaking children",
   "original": "253",
   "page_count": 4,
   "order": 24,
   "p1": 101,
   "pn": 104,
   "abstract": [
    "Prosody is used to express information structure and status differences in American English. For this study, our motivation was to analyze these abilities during an ecologically valid interaction where we traded control for more natural spontaneous speech. We ask how children package information when playing with their parents during exhibit exploration in a children’s museum. Specifically, we employed a MAE_ToBI analysis to look at the production of new and given information status differences during these interactions. Parent-child dyads were recorded while playing in a museum exhibit at a children’s museum. Preliminary analyses were conducted on one 4-year-old, one 5-year-old, and one 6-year-old speaker. As predicted, we found a particular set of pitch accents to be commonly found as well as considerable variation in nuclear configuration patterns due to pragmatic effects. While pitch accent types largely stayed the same over the three ages analyzed to date, the H+!H* pitch accent was only found in the speech of the 4-year-old speaker. These data continue to add to the knowledge of how pitch accent selection relates to both information status and the pragmatics of the discourse.\n"
   ],
   "doi": "10.21437/SpeechProsody.2022-21"
  },
  "wang22_speechprosody": {
   "authors": [
    [
     "Ting",
     "Wang"
    ],
    [
     "Heng",
     "Ding"
    ]
   ],
   "title": "Mandarin Disyllabic Word Imitation in Children with and without Autism Spectrum Disorder",
   "original": "166",
   "page_count": 5,
   "order": 25,
   "p1": 105,
   "pn": 109,
   "abstract": [
    "Atypical pitch production and perception in individuals with autism spectrum disorders (ASD) have been reported mainly from non-tonal language backgrounds. In tonal languages such as Mandarin, the changes of pitch not only signal prosody at a sentence level but also contrast word meanings known as tones at a lexical level. It remains unclear whether children with ASD from tonal language backgrounds show a deficit in the use of pitch at both levels. Therefore, the current study aims to exploit whether Mandarin-speaking children with ASD exhibit atypical lexical pitch production and whether their performance is influenced by semantic information in a disyllabic true and pseudo-words imitation task. Results from acoustic analysis demonstrated significant differences in pitch and duration measures between both subject groups and word types."
   ],
   "doi": "10.21437/SpeechProsody.2022-22"
  },
  "ito22_speechprosody": {
   "authors": [
    [
     "Kiwako",
     "Ito"
    ],
    [
     "Elizabeth",
     "Kryszak"
    ],
    [
     "Teresa",
     "Ibanez"
    ]
   ],
   "title": "Effect of Prosodic Emphasis on the Processing of Joint-Attention Cues in Children with ASD",
   "original": "268",
   "page_count": 5,
   "order": 26,
   "p1": 110,
   "pn": 114,
   "abstract": [
    "Prosodic emphasis affects referential processing in children, yet its effects on non-immediate discourse representation is not well understood. Using a collaborative object search task, we elicited responses to the actor’s speech as well as to later jointattention cues from toddlers with Autism Spectrum Disorder (ASD). Children with ASD responded to “Where is X?” (with or without an emphasis on X) more slowly and weakly as compared to their typically developing (TD) peers. Although this seems to reinforce the view that children with ASD are insensitive to prosodic emphasis, their responses to the jointattention cues revealed otherwise hidden effects of emphasis on referential representation. During the sequential joint-attention cues (head-turn, pointing and reaching), children with ASD shifted their gazes from the actor’s face to the object more swiftly when the object was previously mentioned with emphasis than without. Interestingly, the timing of the gaze shift was also much earlier than TD children. Taken together, the present data suggest that although young children with ASD may not be able to process the prosodic emphasis rapidly, they are sensitive to the prominence and make use of it to represent referential salience, which can facilitate the communication later in the discourse."
   ],
   "doi": "10.21437/SpeechProsody.2022-23"
  },
  "lin22_speechprosody": {
   "authors": [
    [
     "Yi",
     "Lin"
    ],
    [
     "Chuoran",
     "Li"
    ],
    [
     "Qing",
     "Fan"
    ],
    [
     "Yueqi",
     "Chen"
    ],
    [
     "Jiaqi",
     "Zhang"
    ],
    [
     "Hongwei",
     "Ding"
    ]
   ],
   "title": "Effects of sensory dominance and gender differences on impaired emotion perception in schizophrenic patients",
   "original": "100",
   "page_count": 5,
   "order": 27,
   "p1": 115,
   "pn": 119,
   "abstract": [
    "Patients with schizophrenia have been repeatedly reported to show dysfunctions in emotion perception. However, it remains unclear how contextual (e.g., communication channels) and individual (e.g., gender differences) factors interact to modulate their perceptual performances. The present study examined how emotions conveyed through three different sensory channels (i.e., face, prosody, and semantics) were perceived by schizophrenic patients and healthy controls of the two genders. Fourteen (7 women and 7 men) schizophrenic patients and sixteen healthy controls (8 women and 8 men) were asked to identify the emotion displayed visually through facial expressions, or auditorily through prosody or semantics in a fixed-choice format. The integration of accuracy and reaction time data revealed that schizophrenic patients showed impaired emotion perception compared to healthy controls. For both patients and controls, emotional faces gained most perceptual salience among the three communicative channels, but patients were more biased towards emotional semantics whereas controls were more dominated by emotional prosody. Additionally, the preserved abilities in semantic processing were more pronounced for male patients compared to their female counterparts. To sum up, schizophrenic patients demonstrate impaired nonverbal emotion perception irrespective of perceivers’ gender, while their intact abilities in verbal emotion perception are restrictive to males."
   ],
   "doi": "10.21437/SpeechProsody.2022-24"
  },
  "cho22_speechprosody": {
   "authors": [
    [
     "Sunghye",
     "Cho"
    ],
    [
     "Galit",
     "Agmon"
    ],
    [
     "Sanjana",
     "Shellikeri"
    ],
    [
     "Katheryn",
     "Cousins"
    ],
    [
     "Sharon",
     "Ash"
    ],
    [
     "David",
     "Irwin"
    ],
    [
     "Meredith",
     "Spindler"
    ],
    [
     "Andres Deik Acosta",
     "Madiedo"
    ],
    [
     "Lauren",
     "Elman"
    ],
    [
     "Colin",
     "Quinn"
    ],
    [
     "Mark",
     "Liberman"
    ],
    [
     "Murray",
     "Grossman"
    ],
    [
     "Naomi",
     "Nevler"
    ]
   ],
   "title": "Prosodic characteristics of prepausal words produced by patients with neurodegenerative disease",
   "original": "92",
   "page_count": 5,
   "order": 28,
   "p1": 120,
   "pn": 124,
   "abstract": [
    "Prosody of patients with neurodegenerative disease is often impaired. We investigated changes to two prosodic cues in patients: the pitch contour and the duration of prepausal words. We analyzed recordings of picture descriptions produced by patients with neurodegenerative conditions that included either cognitive (n=223), motor (n=68), or mixed cognitive and motor impairments (n=109), and by healthy controls (n=28; HC). A speech activity detector identified pauses. Words were aligned to the acoustic signal; pitch values were normalized in scale and duration. Analyses of pitch showed that the ending (90th-100th percentile) of prepausal words had a lower pitch in the mixed and motor groups than the cognitive group and HC. The pitch contour from the midpoint of words to the end showed a steep rising slope for HC, but patients showed a gentle rising or flat slope. This suggests that HC signaled the continuation of their description after the pause with rising contour; patients either failed to keep describing the picture due to cognitive impairment or could not raise pitch due to motor impairments. Prepausal words showed longer duration relative to non-prepausal words with no significant differences between the groups. This suggests that prepausal lengthening is preserved in patients."
   ],
   "doi": "10.21437/SpeechProsody.2022-25"
  },
  "kruyt22_speechprosody": {
   "authors": [
    [
     "Joanna",
     "Kruyt"
    ],
    [
     "Štefan",
     "Beňuš"
    ],
    [
     "Catherine",
     "Faget"
    ],
    [
     "Christophe",
     "Lançon"
    ],
    [
     "Maud",
     "Champagne-Lavau"
    ]
   ],
   "title": "Prosodic and lexical entrainment in adults with and without schizophrenia",
   "original": "180",
   "page_count": 5,
   "order": 29,
   "p1": 125,
   "pn": 129,
   "abstract": [
    "Entrainment refers to the tendency people have to speak more similarly during a conversation. Although entrainment has been observed frequently, the underlying mechanisms of the phenomenon are debated. A specific point of disagreement is the role of social or higher-order cognitive factors in entrainment. The present study aimed to explore prosodic and lexical entrainment in small groups of individuals with schizophrenia, a disorder that has been associated with theory of mind impairments and social difficulties, and a control group without schizophrenia. All participants completed a referential communication task with an experimenter. To determine prosodic entrainment, the measures proposed by Levitan and Hirshberg (2011) were used. Results seem to suggest that the effect of task role on prosodic entrainment was larger than any possible effects of group, suggesting that social factors affect prosodic entrainment behaviour more than individual differences in cognition or other factors. Conversely, lexical entrainment was not affected by task role or group. Importantly, no clear patterns in entrainment on different dimensions, levels, or features could be observed, highlighting the complex and multifaceted nature of entrainment.\n"
   ],
   "doi": "10.21437/SpeechProsody.2022-26"
  },
  "white22_speechprosody": {
   "authors": [
    [
     "Laurence",
     "White"
    ],
    [
     "Hannah",
     "Grimes"
    ]
   ],
   "title": "Articulation rate in psychotherapeutic dialogues for depression: patients and therapists",
   "original": "235",
   "page_count": 5,
   "order": 30,
   "p1": 130,
   "pn": 134,
   "abstract": [
    "Prosodic features anecdotally associated with the speech of people with clinical depression include slower rate, lower pitch range and reduced loudness, but there is a significant degree of contradiction in the literature regarding depressed prosody. This complex picture reflects the heterogeneity of depression aetiology, symptomatology and prognosis. It is also likely to be influenced by elicitation methods, in particular, whether natural dialogue contexts are employed and whether the interlocutor’s prosody is also considered. We analysed 40 patient-therapist dialogues from the first and last of 29 weekly sessions of a behavioural therapy for refractory depression, sampling early and late in both sessions. Across all dialogues, we found that therapists spoke faster than patients, as expected, but in female-female therapist-patient dialogues (the majority of our sample), patients’ articulation rate increased substantially over the first session. Moreover, and contrary to expectations, there was a positive correlation between articulation rate and assessed depression severity (PHQ-9 scale) in the final therapy session, also evident in therapists’ speech for female-female dialogues. We suggest that this may reflect features of anxiety in speakers with ongoing depression and possibly also personality characteristics. We also consider evidence for prosodic convergence between patients and therapists."
   ],
   "doi": "10.21437/SpeechProsody.2022-27"
  },
  "sahkai22_speechprosody": {
   "authors": [
    [
     "Heete",
     "Sahkai"
    ],
    [
     "Eva Liina",
     "Asu"
    ],
    [
     "Pärtel",
     "Lippus"
    ]
   ],
   "title": "Prosodic characteristics of canonical and non-canonical questions in Estonian",
   "original": "65",
   "page_count": 5,
   "order": 31,
   "p1": 135,
   "pn": 139,
   "abstract": [
    "This paper presents a comparison of the prosodic characteristics of canonical questions with two types of non-canonical interrogative utterances in Estonian. The data consisted of string-identical interrogative sentences with the question-word kuidas (‘how’) elicited in three readings: information-seeking question, rhetorical question and surprise question.\nA three-way distinction between the three utterance types emerged. First, there was a binary distinction between canonical and non-canonical questions in mean pitch, utterance duration and voice quality: non-canonical questions were characterised by lower mean pitch, longer duration and a larger proportion of non-modal (creaky) voice quality. Second, there was a three-way distinction in pitch range: information-seeking questions had the narrowest and surprise questions the widest pitch range while rhetorical questions were placed in the middle. Third, surprise questions were further distinguished from information-seeking and rhetorical questions by a different placement of focal accent. There were, however, no differences in intonational pitch accent types and boundary tones between the three utterance types.\nThe results imply that the lower pitch level signals the indirect illocutionary force of the non-canonical questions while the longer duration, non-modal voice quality and larger pitch range signal their affective nature. Surprise questions are additionally associated with a specific information structure."
   ],
   "doi": "10.21437/SpeechProsody.2022-28"
  },
  "crocco22_speechprosody": {
   "authors": [
    [
     "Claudia",
     "Crocco"
    ],
    [
     "Barbara Gili",
     "Fivela"
    ],
    [
     "Mariapaola",
     "D'Imperio"
    ]
   ],
   "title": "Comparing prosody of Italian varieties and dialects: data from Neapolitan",
   "original": "199",
   "page_count": 5,
   "order": 32,
   "p1": 140,
   "pn": 144,
   "abstract": [
    "In this paper, we provide a qualitative examination of the prosody of Neapolitan dialect (ND) as it relates to Neapolitan Italian (NI). Taking NI as baseline for comparison, ND data seem characterized by several phonetic-phonological strategies to enhance prosodic prominence, suggesting that phonetic parameters have a larger and more dynamic range of variation in ND than in NI. The data also highlight the interlacement between rhythmic, metric, and intonational facts, and the importance of sociolinguistic factors in shaping prosody. In particular, the larger variability of phonetic parameters observed in ND is likely to index dialectal speech as socially marked. We hence identify a number of prosodic discrepancies between ND and NI involving gradient features and tonal organization that call for further investigation. Future studies need to examine such differences in relation to sociolinguistic factors and consider the range of prosodic variation between Italian varieties and dialects spontaneously used by less linguistically-informed speaker."
   ],
   "doi": "10.21437/SpeechProsody.2022-29"
  },
  "jang22_speechprosody": {
   "authors": [
    [
     "Jiyoung",
     "Jang"
    ],
    [
     "Argyro",
     "Katsika"
    ]
   ],
   "title": "The coordination of boundary tones with constriction gestures in Seoul Korean, an edge-prominence language",
   "original": "259",
   "page_count": 5,
   "order": 33,
   "p1": 145,
   "pn": 149,
   "abstract": [
    "Boundary tones mark major phrase boundaries and are expected to be coordinated with speech gestures adjacent to the boundaries. Research on Greek has indeed shown that the onset of the BT (boundary tone) gestures co-occurs with the gestural target of the phrase-final vowel. Interestingly, this coordination was found to be modulated by lexical stress even in the absence of phrasal pitch accent. The present electromagnetic articulography study examines the coordination of BT gestures with respect to constriction gestures in Seoul Korean, a language with no lexical prosody and an edge-prominence system, and further investigates whether prominence affects this coordination. To this end, the distance of the prominent linguistic unit to the boundary was manipulated in a variety of ways. Results indicate that the onset of BT gestures in Korean is most proximate to the peak velocity of the phrase-final vowel gesture, but also suggest that a c-center account is viable. Prominence fine-tunes this coordination. BT gestures are initiated earlier when focus is not phrase-final as opposed to final, as long as a specific distance between focus and the boundary tone is not exceeded. Based on these results, implications on the relationships between the lexical and phrasal prosodic levels are discussed."
   ],
   "doi": "10.21437/SpeechProsody.2022-30"
  },
  "otundo22_speechprosody": {
   "authors": [
    [
     "Billian Khalayi",
     "Otundo"
    ],
    [
     "Martine",
     "Grice"
    ]
   ],
   "title": "Intonation in advice-giving in Kenyan English and Kiswahili",
   "original": "202",
   "page_count": 5,
   "order": 34,
   "p1": 150,
   "pn": 154,
   "abstract": [
    "We examine salient prosodic features used in advice-giving in Kenyan English and Kiswahili from a radio phone-in programme. Our pilot corpus constitutes 40 sequences from The Breakfast Show, a Kenyan radio phone-in programme aired on Classic 105 fm. Although the programme is moderated in English, advice is given in both English and Kiswahili, since Kenya is highly multilingual with frequent code-switching. In this paper, we focus on the pragmatic strategies of expressing advice involving forms that furnish the recipient with little optionality in carrying out the suggested action, including, imperatives, declaratives with modal verbs, and conditional forms. In both languages we observe a terminal falling intonation in advice-giving. However, whilst the global pitch contours in Kenyan English follow a marked downtrend for expressing advice in imperative, declarative and conditional forms, interpreted as a downstepping sequence of H* accents, those in Kiswahili have alternating rises and falls, indicating a more elaborate intonational phonology. In instances of code-switching, imperative forms of advice generally reveal alternating rises and falls. This pattern is also found in declarative and conditional forms, although with a greater pitch range. These preliminary findings are useful in applications such as identification of language and variety, especially in multilingual interactions."
   ],
   "doi": "10.21437/SpeechProsody.2022-31"
  },
  "thurgood22_speechprosody": {
   "authors": [
    [
     "Ela",
     "Thurgood"
    ],
    [
     "Paul",
     "Olejarczuk"
    ]
   ],
   "title": "The Effects of Intonation on the Sentence-Final Particle nyei in Iu-Mien",
   "original": "11",
   "page_count": 5,
   "order": 35,
   "p1": 155,
   "pn": 159,
   "abstract": [
    "This study focuses on the interaction between tone and intonation on the prosodic realization of the sentence-final particle nyei³³ in Iu-Mien, a Hmong-Mien language spoken in parts of China and Southeast Asia. While intonation patterns of questions in colloquial Iu-Mien, in which sentence-final particle nyei³³ does not typically occur, have been described, intonation patterns with the sentence-final nyei³³ used in less colloquial settings have not been analyzed yet. Our study aims to fill this gap. Using data from five female speakers, we show that the mid-level tone 33 of nyei³³ is preserved when in the final position of statements, but surfaces as a rising or falling contour at the end of yes-no questions. In addition, we find coarticulatory effects of the preceding tone on the F0 contour of the particle.\n"
   ],
   "doi": "10.21437/SpeechProsody.2022-32"
  },
  "vallsrates22_speechprosody": {
   "authors": [
    [
     "Io",
     "Valls-Ratés"
    ],
    [
     "Oliver",
     "Niebuhr"
    ],
    [
     "Pilar",
     "Prieto"
    ]
   ],
   "title": "Unguided VR public-speaking training enhances your confidence - but does not improve your intonation",
   "original": "146",
   "page_count": 5,
   "order": 36,
   "p1": 160,
   "pn": 164,
   "abstract": [
    "Public speaking is essential in our daily life. However, standing in front of a crowd is more often than not challenging for people. VR simulations can help speakers meet this challenge. Our study employed a between-subjects design with a VR group (N=17) and a Non-VR group (N=14). Both groups gave a 2-minute speech in front of a live audience before (PRE) and after (POST) they practiced public speaking in front of a VR audience or an empty wall. Each group had three of these VR or Non-VR training sessions, one per week. Acoustic analyses of both groups PRE vs. POST prosodies show that 1) both groups did not differ significantly in f0-related parameters as a function of training (f0 level, f0 range, f0 sd, f0 min/max), 2) the VR group has, unlike the Non-VR group, developed a stronger, clearer and more confident way of presenting, in terms of a longer talking time, fewer disfluent pauses, lower speaking rate, higher CPP and HNR, and lower jitter and shimmer levels. Thus, unguided VR training can help people give more persuasive speeches in real-life presentations, but we assume (consistent with previous research) that guided feedback is required to also improve people's speech melody."
   ],
   "doi": "10.21437/SpeechProsody.2022-33"
  },
  "huttenlauch22_speechprosody": {
   "authors": [
    [
     "Clara",
     "Huttenlauch"
    ],
    [
     "Marie",
     "Hansen"
    ],
    [
     "Carola de",
     "Beer"
    ],
    [
     "Isabell",
     "Wartenburger"
    ],
    [
     "Sandra",
     "Hanne"
    ]
   ],
   "title": "Individual variability in prosodic marking of locally ambiguous sentences",
   "original": "184",
   "page_count": 5,
   "order": 37,
   "p1": 165,
   "pn": 169,
   "abstract": [
    "The German case marking system contains case syncretisms, which, along with a relatively free word order, allows sentences with local ambiguities, for instance, SVO and OVS sentences with string-identical noun phrases (NPs) in sentence-initial position. Prosodic marking constitutes one possibility for ambiguity resolution. Perception studies showed that listeners are sensitive to f0-manipulations on NP1 of such sentences (e.g., different pitch accents). Here, we investigated how speakers cope with the task to provide disambiguation in production as early as possible when faced with locally ambiguous sentences: We elicited productions of German SVO and OVS verb-second main clauses, that begin with a case-ambiguous NP1 and are string-identical up to the post-verbal unambiguous NP2. We focused our analysis on the f0-contours in the ambiguous part of the sentences. Overall, there was no consistent f0-pattern that distinguished SVO from OVS sentences. However, analyses with Generalised Additive Mixed Models revealed distinctive f0-contours on the individual level with later and higher f0-peaks on NP1 in SVO vs. OVS sentences. We found that (at least some) speakers systematically distinguish word order in locally case-ambiguous structures by prosodic cues (f0, silent intervals). The variability in our data suggest to consider the individual level when dealing with specific tasks."
   ],
   "doi": "10.21437/SpeechProsody.2022-34"
  },
  "jabeen22_speechprosody": {
   "authors": [
    [
     "Farhat",
     "Jabeen"
    ]
   ],
   "title": "Production and perception of Intonational Phrase boundaries in Urdu polar questions",
   "original": "147",
   "page_count": 5,
   "order": 38,
   "p1": 170,
   "pn": 174,
   "abstract": [
    "Urdu polar questions are reported to be produced with low or high IP boundary tones. However, existing studies do not clarify the source of this variation. In this research, I report a production and a perception experiment to investigate the use of L% and H% in Urdu polar questions and how it is affected by the position of questioned words. In the production experiment, Urdu speakers produced polar questions with the questioned words at either initial, medial, immediately preverbal or sentence final positions. The results show that speakers preferred to use L% when the questioned words were at non-final positions. H% was used predominantly when the sentence final word was questioned. To examine the role of IP tones in their perception, questions from the production experiment carrying either L% or H% were played to the participants of the perception experiment. The analysis of their responses showed that questions produced with L% were more frequently perceived as polar questions as compared with their counterparts realized with H%. These experiments indicate Urdu speakers' preference for L% for producing and perceiving polar questions and raise questions regarding the prosodic difference between Urdu declaratives and polar questions."
   ],
   "doi": "10.21437/SpeechProsody.2022-35"
  },
  "magistro22_speechprosody": {
   "authors": [
    [
     "Giuseppe",
     "Magistro"
    ],
    [
     "Claudia",
     "Crocco"
    ]
   ],
   "title": "Rising declaratives in Veneto dialects",
   "original": "177",
   "page_count": 5,
   "order": 39,
   "p1": 175,
   "pn": 179,
   "abstract": [
    "The Romance dialects spoken in Veneto, North-East of Italy, have been described as possessing ‘lilting cadences’ (Ferguson 2007). Such cadences have only been reported in anecdotal terms, also by native speakers, who perceive and describe their dialects as marked by a final ‘melodic tilt’. In our paper, we aim at examining the phonetic correlates of this trait. We recorded 30 dialectal speakers from Venice, Padua, and Gazzolo (near Verona), eliciting declarative sentences (N=900) across distinct three informational categories. The 3 areas differ not only in terms of geographic distribution but also by relative sociolinguistic prestige in the region. Utterances were first auditorily and visually inspected, and subsequently examined using Generalized Additive Mixed Models. The results indicated that broad focus statements in Gazzolo dialect, the most rural variety, often end with a final rise, a tonal feature possibly contributing to the lilting cadence. After providing an acoustic description of the rise, we discuss the theoretical and sociolinguistic implications of the data. Theoretically, rising declaratives are cross-linguistically seldom and can represent counter-examples for the Frequency code. From a sociolinguistic perspective, the Gazzolo rise may index the speakers not only geographically but also socially."
   ],
   "doi": "10.21437/SpeechProsody.2022-36"
  },
  "bongiorno22_speechprosody": {
   "authors": [
    [
     "Julia",
     "Bongiorno"
    ],
    [
     "Sophie",
     "Herment"
    ]
   ],
   "title": "High Rising Terminals in Dublin: forms, functions and gender",
   "original": "83",
   "page_count": 5,
   "order": 40,
   "p1": 180,
   "pn": 184,
   "abstract": [
    "High Rising Terminals, Uptalk, or Upspeak, are stylistic rises that can be found at the end of declarative statements. They have been studied in numerous varieties of English and in other languages too. It has been shown that these rises can take on different phonetic and phonological forms and convey various pragmatic functions depending on the varieties in which they are found. The present study provides a description of these forms and functions in Dublin (Republic of Ireland). Based on a corpus of 5 speakers from the PAC-Dublin corpus that was recorded in the Irish capital in 2018, the study shows that HRTs are mainly realized with late rises and nuclear rises and that they are different from interrogative and continuative rises, notably because they are steeper than the latter. A sociolinguistic analysis of our corpus also shows that the gender of the speakers has an influence on the occurrence of the phenomenon, which does not seem to be the case for age range. This article thus provides a multidimensional analysis of stylistic rising tones in statements in Dublin."
   ],
   "doi": "10.21437/SpeechProsody.2022-37"
  },
  "rossi22_speechprosody": {
   "authors": [
    [
     "Martina",
     "Rossi"
    ],
    [
     "Kathrin",
     "Feindt"
    ],
    [
     "Margaret",
     "Zellers"
    ]
   ],
   "title": "Individual variation in F0 marking of turn-taking in natural conversation in German and Swedish",
   "original": "64",
   "page_count": 5,
   "order": 41,
   "p1": 185,
   "pn": 189,
   "abstract": [
    "The linguistic mechanisms organizing turn-taking in conversation are still not fully understood. Especially disputed is the relevance of various linguistic features to signal the disposition to yield the floor. The present study adds to this discussion by examining the role of prosody for TT in two different languages, German and Swedish. F0 movement is measured at three points - offset (P1), 200ms (P2) and 500ms (P3) before the turn end and normalized. Sentence type (declarative, question), type of speaker change (change, keep, backchannel) and transition (gap, no-gap-no-overlap, overlap) were also annotated, among other features. Preliminary results show that German uses a much wider span of F0 values compared to Swedish. Since F0 has a lexical-phonological function (i.e. pitch accent) in Swedish, the potential prosodic structure is restricted. On the other hand, the flexibility of German manifests itself in extreme F0 movements and an accommodation of F0 between interlocutors. Although there is evidence for accommodation of F0 in German, this is not as strongly demonstrated in the Swedish data. As our F0 normalization should exclude a physiological explanation, we argue for a certain entrainment."
   ],
   "doi": "10.21437/SpeechProsody.2022-38"
  },
  "hasan22_speechprosody": {
   "authors": [
    [
     "Malek Al",
     "Hasan"
    ],
    [
     "Shakuntala",
     "Mahanta"
    ]
   ],
   "title": "The Intonational Phonology of Syrian Arabic: A Preliminary Analysis",
   "original": "217",
   "page_count": 5,
   "order": 42,
   "p1": 190,
   "pn": 194,
   "abstract": [
    "Native speakers of Syrian Arabic are distinguished from speakers of other Arabic dialects by using “singing intonation” in their utterances. This paper examines the intonational patterns occurring in Syrian Arabic using the autosegmental-metrical (AM) approach to intonation. The dialect used in this study is the one spoken in Damascus (Damascene). The analysis is based on two experiments wherein the second experiment utterances were investigated in three word orders of Syrian Arabic (VSO), (SVO), and (VOS). Declarative sentences showed an initial rise (due to stress on the first syllable) and a falling contour towards the end. Stress position and word order were found to change pitch accent type and alignment. Phrase-final drawl found to be exaggerated in questions leading to vowel lengthening."
   ],
   "doi": "10.21437/SpeechProsody.2022-39"
  },
  "wepner22_speechprosody": {
   "authors": [
    [
     "Saskia",
     "Wepner"
    ],
    [
     "Barbara",
     "Schuppler"
    ],
    [
     "Gernot",
     "Kubin"
    ]
   ],
   "title": "How prosody affects ASR performance in conversational Austrian German",
   "original": "206",
   "page_count": 5,
   "order": 43,
   "p1": 195,
   "pn": 199,
   "abstract": [
    "Currently available Automatic Speech Recognition (ASR) systems achieve good word error rates (WER) for read speech (2–10%), but not for conversational speech (20–40%), a speaking style especially relevant for dialogue systems, as they become more conversational and interactional. Here, we analyse how prosody affects WER in a Kaldi-based speech recognition system for a corpus of conversational Austrian German. This analysis is a step towards improving ASR systems and increasing our knowledge about which aspects are relevant to consider for ASR of conversational speech. For this purpose, we compare a typical language model (LM) with an oracle LM trained on the utterances from the whole corpus, thus knowing each possible N-gram in advance. We find that short, deaccented words have the lowest recognition accuracy, which also cannot be compensated for by the oracle LM. Despite our overall high WERs, the highly prominent words were recognised significantly better. Our findings suggest that reporting global WERs for an ASR system of conversational speech does not predict its usefulness in dialogue systems. Given the role of prominent words in carrying meaning and function in conversation, our analysis is relevant for researchers developing automatic speech understanding systems."
   ],
   "doi": "10.21437/SpeechProsody.2022-40"
  },
  "ng22_speechprosody": {
   "authors": [
    [
     "Si-Ioi",
     "Ng"
    ],
    [
     "Rui-Si",
     "Ma"
    ],
    [
     "Tan",
     "Lee"
    ],
    [
     "Raymond Kim-Wai",
     "Sum"
    ]
   ],
   "title": "Acoustical Analysis of Speech Under Physical Stress in Relation to Physical Activities and Physical Literacy",
   "original": "188",
   "page_count": 5,
   "order": 44,
   "p1": 200,
   "pn": 204,
   "abstract": [
    "Human speech production encompasses physiological processes that naturally react to physic stress. Stress caused by physical activity (PA), e.g., running, may lead to significant changes in a person's speech. The major changes are related to the aspects of pitch level, speaking rate, pause pattern, and breathiness. The extent of change depends presumably on physical fitness and well-being of the person, as well as intensity of PA. The general wellness of a person is further related to his/her physical literacy (PL), which refers to a holistic description of engagement in PA. This paper presents the development of a Cantonese speech database that contains audio recordings of speech before and after physical exercises of different intensity levels. The corpus design and data collection process are described. Preliminary results of acoustical analysis are presented to illustrate the impact of PA on pitch level, pitch range, speaking and articulation rate, and time duration of pauses. It is also noted that the effect of PA is correlated to some of the PA and PL measures.\n"
   ],
   "doi": "10.21437/SpeechProsody.2022-41"
  },
  "siqueira22_speechprosody": {
   "authors": [
    [
     "Veronica P.",
     "Siqueira"
    ],
    [
     "Beatriz Raposo de",
     "Medeiros"
    ]
   ],
   "title": "Synchronous speech and semantic incongruity: what do outliers tell us about it?",
   "original": "91",
   "page_count": 5,
   "order": 45,
   "p1": 205,
   "pn": 209,
   "abstract": [
    "Synchronous speech, considered to be an easily performed task, is investigated in two experimental conditions designated as original (OC) and altered (AC), with focus on the outliers’ behavior. The hypothesis raised is that AC, which offers semantic incongruities, would lead individuals to a poorer synchronization performance, i.e, producing greater lag duration. Divided equally in two groups (A and B), 24 dyads were recorded reading two fables in Brazilian Portuguese, in both original and altered conditions. Asynchrony duration was obtained by extracting the lag between vowel onsets, after aligning speakers' waveforms in each dyad. Considering results related to the entire dataset, speakers are able to synchronize in both conditions (OC and AC). However, a great number of outliers was observed throughout the dataset. Its distribution in AC is significantly different from the distribution in OC, the former showing greater values for both variance and mean. In this exploratory study, one promising explanation for these results will be discussed taking into account aspects such as the outliers location throughout the text. These initial results prompt further investigation, in order to verify a more accurate relation between the outliers' duration and the semantic incongruities' place of occurrence."
   ],
   "doi": "10.21437/SpeechProsody.2022-42"
  },
  "fernandessvartman22_speechprosody": {
   "authors": [
    [
     "Flaviane",
     "Fernandes-Svartman"
    ],
    [
     "Larissa",
     "Berti"
    ],
    [
     "Marcus",
     "Martins"
    ],
    [
     "Beatriz R.",
     "Medeiros"
    ],
    [
     "Marcelo",
     "Queiroz"
    ]
   ],
   "title": "Temporal prosodic cues for COVID-19 in Brazilian Portuguese speakers",
   "original": "81",
   "page_count": 5,
   "order": 46,
   "p1": 210,
   "pn": 214,
   "abstract": [
    "Temporal aspects of pause in COVID-19 patients' speech are investigated as part of the SPIRA project. Pause is presented as an important candidate to differentiate the speech of COVID-19 patients (target group, n=94) from the speech of healthy subjects (control group, n=99). In order to investigate pause duration and its distribution along the sentence as a prosodic cue, three hypotheses were raised: (1) patient speech includes more pauses than control speech; (2) patient pauses are longer than control pauses; (3) pause distribution is different between groups. Results show that patients' speech has more pauses (3.16 versus 0.85), which are also longer (0.53s versus 0.13s) with respect to control pauses (all differences with p<0.001, Mann-Whitney U-Test). A time series analysis was used to model pause distribution along the sentence, which is shown to be randomly spread in the target group, with pauses occurring at unexpected places, contrasting with predictable pauses for controls. Furthermore, a correct classification was obtained for 87-89% of both target and control groups. These findings, grounded in prosodic aspects, are promising and point out the important role of pause as a biomarker in the speech of COVID-19 patients.\n"
   ],
   "doi": "10.21437/SpeechProsody.2022-43"
  },
  "crouch22_speechprosody": {
   "authors": [
    [
     "Caroline",
     "Crouch"
    ],
    [
     "Argyro",
     "Katsika"
    ],
    [
     "Ioana",
     "Chitoran"
    ]
   ],
   "title": "Georgian Syllables, Uncentered? ",
   "original": "244",
   "page_count": 5,
   "order": 47,
   "p1": 215,
   "pn": 219,
   "abstract": [
    "Both sonority, via the Sonority Sequencing Principle (SSP), and timing, via the coupled oscillator model advanced within Articulatory Phonology (AP), have been invoked to define the syllable as a unit. Georgian presents challenges for both definitions. The irrelevance of the SSP for Georgian phonotactics is well documented, while it is unclear whether Georgian displays the AP-predicted timing pattern of syllable onsets, i.e., the c-center effect. We investigate the relationship between sonority shape and global timing in complex onsets in Georgian by the means of a series of Electromagnetic Articulography (EMA) experiments. We use a direct measure of c-center stability relative to an anchor point in the vowel, and demonstrate that, contrary to predictions, the c-center is not an invariant point in the onset. Instead, our results suggest that Georgian syllables contain exclusively anti-phase coordination, even between the prevocalic consonant and the vowel. This coordination is not affected by sonority shape, although sonority is reflected in patterns of overlap. We discuss these results in relationship to the phonological and morphological profile of Georgian and suggest that syllable-wide anti-phase coordination is possible given Georgian’s permissive phonotactics, and aids in the formation of morphologically complex words. Typological extensions of this account are made."
   ],
   "doi": "10.21437/SpeechProsody.2022-44"
  },
  "tannander22_speechprosody": {
   "authors": [
    [
     "Christina",
     "Tånnander"
    ],
    [
     "David",
     "House"
    ],
    [
     "Jens",
     "Edlund"
    ]
   ],
   "title": "Syllable duration as a proxy to latent prosodic features",
   "original": "22",
   "page_count": 5,
   "order": 48,
   "p1": 220,
   "pn": 224,
   "abstract": [
    "Recent advances in deep-learning have pushed text-to-speech synthesis (TTS) very close to human speech. In deep-learning, latent features refer to features that are hidden from us; notwithstanding, we may meaningfully observe their effects. Analogously, latent prosodic features refer to the exact features that constitute e.g. prominence that are unknown to us, although we know (some of) the functions of prominence and (some of) its acoustic correlates.\nDeep-learned speech models capture prosody well, but leave us with little control and few insights. Previously, we explored average syllable duration on word level - a simple and accessible metric - as a proxy for prominence: in Swedish TTS, where verb particles and numerals tend to receive too little prominence, these were nudged towards lengthening while allowing the TTS models to otherwise operate freely. Listener panels overwhelmingly preferred the nudged versions to the unmodified TTS.\nIn this paper, we analyse utterances from the modified TTS. The analysis shows that duration-nudging of relevant words changes the following features in an observable manner: duration is predictably lengthened, word-initial glottalization occurs, and the general intonation pattern changes. This supports the view of latent prosodic features that can be reflected in deep-learned models and accessed by proxy."
   ],
   "doi": "10.21437/SpeechProsody.2022-45"
  },
  "zebe22_speechprosody": {
   "authors": [
    [
     "Franka",
     "Zebe"
    ]
   ],
   "title": "Durational consonant categories in Alemannic and Swiss Standard German across tempo and age",
   "original": "25",
   "page_count": 5,
   "order": 49,
   "p1": 225,
   "pn": 229,
   "abstract": [
    "German-speaking Switzerland classifies as diglossia, meaning that the Swiss German public speak both a variety of Alemannic and a respective variety of Swiss Standard German (SSG). Additionally, there is a quantitative contrast in obstruent consonants, resulting in different durational categories. One aim of this study is to investigate these categories, focusing on plosives, in both Alemannic and SSG. Furthermore, articulation rate (AR) and its influence on these categories are examined. 20 speakers of two age groups, i.e. younger and older adults, from the canton of Lucerne (LU) were analyzed regarding these aspects. They produced words containing different vowel consonant combinations (VC combinations) within carrier sentences in two conditions, i.e. normal and fast speech tempo. Results show that older speakers have a lower AR in SSG while they behave similar to younger speakers in Alemannic. Most importantly, this investigation confirms a three-way distinction in consonant durations, namely lenis, fortis, and extrafortis in both Alemannic and SSG. In addition, older speakers produce longer consonant durations in the extrafortis category than younger speakers. Despite these age-related differences, durational consonant categories are stable across both tempo and age.\n"
   ],
   "doi": "10.21437/SpeechProsody.2022-46"
  },
  "mok22_speechprosody": {
   "authors": [
    [
     "Ivy",
     "Mok"
    ],
    [
     "Lieke van",
     "Maastricht"
    ],
    [
     "Nuria",
     "Esteve-Gibert"
    ]
   ],
   "title": "Do head gestures function as precursors for prosodic focus marking in the L2?",
   "original": "24",
   "page_count": 5,
   "order": 17,
   "p1": 67,
   "pn": 71,
   "abstract": [
    "Prior research on the acquisition of discourse focus marking has traditionally focused on prosodic cues while disregarding visual cues to mark information status. Recently, [1] has shown that French children used head gestures to highlight new/contrastive discourse referents before developing the necessary prosodic cues. As prosodic discourse marking is challenging for both L1 and L2 learners, we investigate whether this entrainment function of head gestures in prosodic focus marking also occurs in an L2 context. Hence, Catalan/Spanish learners of English were audio-visually recorded while producing semi-spontaneous utterances in three focus conditions (broad focus; contrastive narrow focus; corrective narrow focus). We analyzed (alignment between) prosodic (F0 and syllable duration) and gestural (gesture type and phases) correlates of focus. The results show a longer duration for words in the contrastive and corrective conditions than in the broad focus condition, but no effect of focus condition on pitch range or gesture presence. Moreover, pitch range in gesture-accompanied words was always higher than pitch range in non-gesture-accompanied words, irrespective of focus condition. These results imply that gesture and prosody may be so tightly coupled in this context that L2 learners emphasize the same, possibly inaccurate, part of the utterance in both modalities."
   ],
   "doi": "10.21437/SpeechProsody.2022-14"
  },
  "everhardt22_speechprosody": {
   "authors": [
    [
     "Marita",
     "Everhardt"
    ],
    [
     "Anastasios",
     "Sarampalis"
    ],
    [
     "Matt",
     "Coler"
    ],
    [
     "Deniz",
     "Baskent"
    ],
    [
     "Wander",
     "Lowie"
    ]
   ],
   "title": "Interpretation of prosodically marked focus in cochlear implant-simulated speech by non-native listeners",
   "original": "71",
   "page_count": 5,
   "order": 18,
   "p1": 72,
   "pn": 76,
   "abstract": [
    "This study assesses how a cochlear implant (CI) simulation influences the interpretation of prosodically marked linguistic focus in a non-native language. In an online experiment, two groups of normal-hearing native Dutch learners of English of different ages (12–14 year-old adolescents vs. 18+ year-old adults) and with different proficiency levels in English (A2 vs. B2/C1) were asked to listen to CI-simulated and non-CI-simulated English sentences differing in prosodically marked focus and indicate which of four possible context questions the speaker answered. Results show that, as expected, focus interpretation is significantly less accurate in the CI-simulated condition compared to the non-CI-simulated condition and that more proficient non-native listeners outperform less proficient non-native listeners. However, there was no interaction between the influence of the spectro-temporal degradation of the CI-simulated speech signal and that of the English proficiency level of the non-native listeners, suggesting that less proficient non-native listeners are not more strongly affected by the spectro-temporal degradation of an electric speech signal than more proficient non-native listeners.\n"
   ],
   "doi": "10.21437/SpeechProsody.2022-15"
  },
  "du22_speechprosody": {
   "authors": [
    [
     "Kexin",
     "Du"
    ],
    [
     "Sergey",
     "Avrutin"
    ],
    [
     "Aoju",
     "Chen"
    ]
   ],
   "title": "Building bridges: The role of prosody in Mandarin-speaking adults' and children's anaphora resolution",
   "original": "108",
   "page_count": 5,
   "order": 19,
   "p1": 77,
   "pn": 81,
   "abstract": [
    "Past research on the role of prosody in reference is primarily concerned with how adults and children use prosodic cues to signal accessibility change from givenness to newness of the same noun phrase. This study explores the role of prosody in the referential dependencies between one antecedent noun-phrase and one reflexive anaphor ‘zi-ji’ (oneself) in Mandarin-speaking adults and children. In sentences like “Boris dreamed that Miffy painted ‘zi-ji’”, ‘zi-ji’ can establish two types of anaphor-antecedent dependencies: (1) a local dependency where ‘zi-ji’ refers to Miffy, (2) a non-local dependency where ‘zi-ji’ refers to Boris. Such sentences were elicited in both interpretations from Mandarin-speaking adults and 6 to 10-year-olds in a picture-matching game. Duration analysis on ‘zi-ji’ shows that adults produced ‘zi-ji’ with a longer duration in the non-local dependency condition than in the local dependency condition. This result can be explained by the economy hierarchy model, whereby the local antecedent made more accessible by the locality constraint is preferred, thus necessitating the use of more prosodic prominence to mark the less accessible non-local antecedent. This pattern was not found in children’s production, suggesting prolonged acquisition of using prosody to build anaphor-antecedent dependencies for ‘zi-ji’.\n"
   ],
   "doi": "10.21437/SpeechProsody.2022-16"
  },
  "gervain22_speechprosody": {
   "authors": [
    [
     "Judit",
     "Gervain"
    ]
   ],
   "title": "How the neural mechanisms of encoding prosody lay the foundations for early language development",
   "original": "k2",
   "page_count": 0,
   "order": 2,
   "p1": "",
   "pn": "",
   "abstract": [
    "In adults, the perception of suprasegmental units has been found to be supported by a powerful system of hierarchically embedded neural oscillations (Giraud & Poeppel 2012). This talk investigates how this system of oscillations emerges in development, as a function of newborns' and young infants’ experience with language and discusses how these mechanisms pave the way for the acquisition of syntax and vocabulary. Specifically, the talk will present brain imaging studies with young infants investigating the presence and development of neural oscillations and other neural mechanisms encoding prosody from birth till 6 months of age. Then behavioral studies will be presented to illustrate how the neural mechanisms of the early encoding of prosody may support subsequent language acquisition."
   ]
  },
  "cruzpavia22_speechprosody": {
   "authors": [
    [
     "Irene de La",
     "Cruz-Pavía"
    ]
   ],
   "title": "The role of audio-visual phrasal prosody in bootstrapping the acquisition of word order",
   "original": "45",
   "page_count": 5,
   "order": 50,
   "p1": 230,
   "pn": 234,
   "abstract": [
    "From early in development infants integrate auditory and visual facial information while processing language. The potential role of visual cues in the acquisition of grammar remains however virtually unexplored. Phrasal prosodic prominence correlates systematically with basic word order in natural languages. Co-verbal gestures—head and eyebrow motion—act in turn as markers of auditory prosody. Here, we examine whether co-verbal gestures could help infants parse the input into prosodic units such as phrases, and discover the basic word order of the native language. In a first study we show that adult talkers spontaneously produce co-verbal gestures signalling phrase boundaries across languages and speech styles: Japanese and English, adult- and infant-directed speech. A second study shows that adult speakers use co-verbal information, specifically head nods marking phrasal prosodic prominence, to parse an artificial language into phrase-like units that follow the native language’s word order. Finally, a third study shows that the presence of co-verbal gestures—i.e. head nods—also impacts 8-month-old infants’ segmentation preferences of a structurally ambiguous artificial language. However, infants’ ability to use this cue is still limited, suggesting that co-verbal gestures might be acquired later in development than visual speech, presumably due to their greater inter-/intra-speaker variability."
   ],
   "doi": "10.21437/SpeechProsody.2022-47"
  },
  "xi22_speechprosody": {
   "authors": [
    [
     "Zhenyang",
     "Xi"
    ],
    [
     "Yan",
     "Gu"
    ],
    [
     "Gabriella",
     "Vigliocco"
    ]
   ],
   "title": "Speaking Rate in 3-4-Year-Old Children: Its Correlation with Gesture Rate and Word Learning",
   "original": "155",
   "page_count": 5,
   "order": 51,
   "p1": 235,
   "pn": 239,
   "abstract": [
    "Past research has shown that children before 3-year-old often use gestures while speaking and that the relationship between speech and gesture may relate to vocabulary development. Using a new corpus of semi-naturalistic interaction between caregivers and their 3-4-year-old children (ECOLANG Corpus), this study investigates the relationship between speaking and gesture rates and assesses their correlation with word learning. Specifically, we studied speaking and gesture rates of 32 English-speaking children while talking with their caregivers about sets of pre-selected toys. The children completed a vocabulary test at the time of the experiment and one year later. Looking at 3-4-year-old children is key to capturing developmental changes from using gestures as a complement to speech to using gesture and speech with good synchronization. Results show that children with a fast speaking rate also demonstrated a higher gesture rate. However, there was no correlation between speaking rate or gesture rate and word learning. Thus, our findings show that by 3-4 years of age, children produce utterances where speech and gesture are synchronized. Moreover, by this age, the relationship between the two is no longer a predictor of vocabulary learning. We speculate that at this age, gesture use can support conceptual development and processing.\n"
   ],
   "doi": "10.21437/SpeechProsody.2022-48"
  },
  "yoon22_speechprosody": {
   "authors": [
    [
     "Tae Jin",
     "Yoon"
    ],
    [
     "Seunghee",
     "Ha"
    ],
    [
     "Jungmin",
     "So"
    ]
   ],
   "title": "Developmental Patterns of Accentual Phrases in Korean Children’s Speech",
   "original": "105",
   "page_count": 4,
   "order": 52,
   "p1": 240,
   "pn": 243,
   "abstract": [
    "The Accentual Phrase (AP) in Korean is said to be conditioned by the glottal status in the AP-initial segments, with the canonical shape of LHLH, when the initial segment of AP bears [-stiff vocal cords]. While the tonal pattern of adult speakers has been examined extensively, relatively few studies exist that examine the tonal patterns realized by children’s speech. The paper examines the developmental trajectory of the canonical tonal shape of AP in Korean by analyzing speech samples of children aged 3 to 10 years old in a publicly available corpus. The VOT of the initial plain stop was also examined to see whether the tonal shape of the AP in children’s speech was affected by the phonetic properties of the initial segment. The results showed that the tonal contours of the Accentual Phrases were best understood to be phonologically encoded and produced by children across most age groups."
   ],
   "doi": "10.21437/SpeechProsody.2022-49"
  },
  "guan22_speechprosody": {
   "authors": [
    [
     "Qianwen",
     "Guan"
    ],
    [
     "Yaru",
     "Wu"
    ],
    [
     "Ioana",
     "Chitoran"
    ]
   ],
   "title": "A corpus-based study of /CR/ and /RC/ clusters in French: Prosodic and segmental effects",
   "original": "113",
   "page_count": 5,
   "order": 53,
   "p1": 244,
   "pn": 248,
   "abstract": [
    "This paper aims to provide a better understanding of the production of /R/-clusters in a corpus of spoken French. We study word-final CR#, word-final RC#, and word-initial #CR in the LOCAS-F corpus, one of the few French corpora with prosodic annotation (Degand et al. 2014). Different factors are considered in this study in terms of three production outcomes (R-deletion, schwa insertion, and canonical pronunciation of the clusters), including prosodic boundaries, post-lexical context, voicing of the within-word adjacent consonant, speech style, and word frequency. Results reveal that for word-final CR#, 17% of the /R/s are deleted, and 39% of the time a schwa is inserted. Post-lexical context, prosodic boundaries, speech style and word frequency, but not voicing, have significant impact on the realization of CR# clusters. The presence of a major prosodic boundary significantly disfavours R-deletion in word-final CR#. For word-final CR# and RC#, post-lexical consonantal contexts favour non-canonical realizations. Interestingly, for word-final RC#, epenthetic schwa is still observed in 20% of the cases and post-lexical context has a significant influence on the insertions. In word-initial #CR, the canonical pronunciation is systematically observed."
   ],
   "doi": "10.21437/SpeechProsody.2022-50"
  },
  "cronenberg22_speechprosody": {
   "authors": [
    [
     "Johanna",
     "Cronenberg"
    ],
    [
     "Nicola",
     "Klingler"
    ],
    [
     "Felicitas",
     "Kleber"
    ],
    [
     "Michael",
     "Pucher"
    ]
   ],
   "title": "On the role of asymmetry in prosodic change of consonant duration: Results from an agent-based model with two German varieties",
   "original": "101",
   "page_count": 5,
   "order": 54,
   "p1": 249,
   "pn": 253,
   "abstract": [
    "This study investigates how an asymmetric conversational situation between two standard varieties of German can influence the speakers' consonant quantity in /V:C:/ sequences, i.e. a prosodic feature. Asymmetric conversations between German and Austrian speakers, i.e. scenarios in which one of the varieties provided the majority of speech input, were simulated using an agent-based model which adhered to principles from Exemplar Theory and the mechanistic view on sound change. The results showed that Austrian agents were more prone to prosodic change than Germans due to a higher variability and bias towards shortened consonants in their acoustic representation of /V:C:/. The findings are discussed in light of the interactive-phonetic model of sound change."
   ],
   "doi": "10.21437/SpeechProsody.2022-51"
  },
  "huaute22_speechprosody": {
   "authors": [
    [
     "Ray",
     "Huaute"
    ]
   ],
   "title": "A Preliminary Intonation Model of Torres-Martinez Desert Cahuilla",
   "original": "228",
   "page_count": 5,
   "order": 55,
   "p1": 254,
   "pn": 258,
   "abstract": [
    "This paper presents an analysis of the intonational patterns and phrasal domains in simple declaratives in Torres-Martinez Desert (TMD) Cahuilla, a critically endangered Uto-Aztecan language spoken in Southern California. While the word-prosodic system of Cahuilla has been addressed in previous literature [1], [2], [3], no intonational analysis has been proposed for any Cahuilla variety. Using novel data, I motivate a preliminary intonational model for declarative sentences in TMD Cahuilla within the AM (autosegmental-metrical) framework. Specifically, I analyze TMD Cahuilla as having two distinct levels of prosodic constituency, the Intonational Phrase (IP), which is composed of at least one Accentual Phrase (AP). The AP, which consists of at least one prosodic word, has an obligatory H* pitch accent on the lexically stressed syllable and a La edge tone aligned to the right edge of the phrase. The IP ends in a L% boundary tone, which overwrites the La when they co-occur. Phrase-final lengthening and optional pauses delineate the IP. Given the paucity of intonational research on American Indian languages, this paper contributes to a growing body of cross-linguistic research that tests the ability of an AM-based system of annotation such as ToBI, to model a wide range of intonation systems."
   ],
   "doi": "10.21437/SpeechProsody.2022-52"
  },
  "bujok22_speechprosody": {
   "authors": [
    [
     "Ronny",
     "Bujok"
    ],
    [
     "Antje",
     "Meyer"
    ],
    [
     "Hans Rutger",
     "Bosker"
    ]
   ],
   "title": "Visible lexical stress cues on the face do not influence audiovisual speech perception",
   "original": "12",
   "page_count": 5,
   "order": 56,
   "p1": 259,
   "pn": 263,
   "abstract": [
    "Producing lexical stress leads to visible changes on the face, such as longer duration and greater size of the opening of the mouth. Research suggests that these visual cues alone can inform participants about which syllable carries stress (i.e., lip-reading silent videos). This study aims to determine the influence of visual articulatory cues on lexical stress perception in more naturalistic audiovisual settings. Participants were presented with seven disyllabic, Dutch minimal stress pairs (e.g., VOORnaam [first name] & voorNAAM [respectable]) in audio-only (phonetic lexical stress continua without video), video-only (lip-reading silent videos), and audiovisual trials (e.g., phonetic lexical stress continua with video of talker saying VOORnaam or voorNAAM). Categorization data from video-only trials revealed that participants could distinguish the minimal pairs above chance from seeing the silent videos alone. However, responses in the audiovisual condition did not differ from the audio-only condition. We thus conclude that visual lexical stress information on the face, while clearly perceivable, does not play a major role in audiovisual speech perception. This study demonstrates that clear unimodal effects do not always generalize to more naturalistic multimodal communication, advocating that speech prosody is best considered in multimodal settings."
   ],
   "doi": "10.21437/SpeechProsody.2022-53"
  },
  "bruggeman22_speechprosody": {
   "authors": [
    [
     "Laurence",
     "Bruggeman"
    ],
    [
     "Jenny",
     "Yu"
    ],
    [
     "Anne",
     "Cutler"
    ]
   ],
   "title": "Listener adjustment of stress cue use to fit language vocabulary structure",
   "original": "141",
   "page_count": 4,
   "order": 57,
   "p1": 264,
   "pn": 267,
   "abstract": [
    "In lexical stress languages, phonemically identical syllables can differ suprasegmentally (in duration, amplitude, F0). Such stress cues allow listeners to speed spoken-word recognition by rejecting mismatching competitors (e.g., unstressed set- in settee rules out stressed set- in setting, setter, settle). Such processing effects have been observed in several stress languages (e.g., Spanish, Dutch, German); but English listeners have long been known to largely ignore stress cues. Listeners from other stress languages even outdo English listeners in distinguishing stressed versus unstressed English syllables. This has been attributed to the relative frequency across the stress languages of unstressed syllables with full vowels; in English most unstressed syllables contain schwa, instead, and stress cues on full vowels are thus least often informative in this language. If only informativeness matters, would English listeners encountering situations where such cues would pay off for them (e.g. learning one of those other stress languages) then shift to using stress cues? Likewise, would stress cue users with English as L2, if mainly using English, shift away from using the cues in English? Here we report tests of these two questions, with each receiving a yes answer. We propose that English listeners’ disregard of stress cues is purely pragmatic."
   ],
   "doi": "10.21437/SpeechProsody.2022-54"
  },
  "bruggeman22b_speechprosody": {
   "authors": [
    [
     "Anna",
     "Bruggeman"
    ],
    [
     "Leonie",
     "Schade"
    ],
    [
     "Marcin",
     "Włodarczak"
    ],
    [
     "Petra",
     "Wagner"
    ]
   ],
   "title": "Beware of the individual: Evaluating prominence perception in spontaneous speech",
   "original": "149",
   "page_count": 5,
   "order": 58,
   "p1": 268,
   "pn": 272,
   "abstract": [
    "Much of the existing research on prominence perception has focused on read speech in American English and German. The present paper presents two experiments that build on and extend insights from these studies in two ways. Firstly, we elicit prominence judgments on spontaneous speech. Secondly, we investigate gradient rather than binary prominence judgments by introducing a finger tapping task. We additionally provide a within-participant comparison of gradient prominence results with binary prominence judgments to evaluate their correspondence. Our results show that participants exhibit different success rates in tapping the prominence pattern of spontaneous data, but generally tapping results correlate well with binary prominence judgments within individuals. Random forest analysis of the acoustic parameters involved shows that pitch accentuation and duration play important roles in both binary judgments and prominence tapping patterns. We can also confirm earlier findings from read speech that differences exist between participants in the relative importance rankings of various signal and systematic properties."
   ],
   "doi": "10.21437/SpeechProsody.2022-55"
  },
  "azzaboukacem22_speechprosody": {
   "authors": [
    [
     "Soundess",
     "Azzabou-Kacem"
    ],
    [
     "Alice",
     "Turk"
    ]
   ],
   "title": "Fine gradations of prosodic boundary strength can drive the assignment of prominence",
   "original": "242",
   "page_count": 5,
   "order": 59,
   "p1": 273,
   "pn": 277,
   "abstract": [
    "We know that post-lexical stress shift is blocked when two words with abutting main prominences (e.g., afternoon and hike) are separated by Intonational Phrase (IP) boundaries. However, when the clashing words belong to the same IP (e.g., afternoon hike), we do not know whether the shift can also be obstructed by finer gradations of prosodic boundary strength, below the IP level. This paper reports on an experiment manipulating prosodic boundary strength via constituent length variations. Twenty-nine speakers produced in context 28 pairs of potentially clashing sequences (e.g., canteen soup vs. canteen supervisor), where Word 2 was one syllable (e.g., soup, Short condition designed to elicit weaker prosodic boundaries) or four syllables long (e.g., supervisor, Long condition to elicit stronger boundaries). The length manipulation affected the stress shift rate, which was higher in the Short condition. The acoustic analyses show significant changes of syllable duration, f0 and SPL in both syllables of the target. The results provide evidence that the grouping within one IP of clashing words is not sufficient condition for stress shift to take place, as the shift can be obstructed by gradations in prosodic boundary strength between the clashing words, even if they belong to the same IP."
   ],
   "doi": "10.21437/SpeechProsody.2022-56"
  },
  "severijnen22_speechprosody": {
   "authors": [
    [
     "Giulio",
     "Severijnen"
    ],
    [
     "Hans Rutger",
     "Bosker"
    ],
    [
     "James",
     "McQueen"
    ]
   ],
   "title": "Acoustic correlates of Dutch lexical stress re-examined: Spectral tilt is not always more reliable than intensity",
   "original": "13",
   "page_count": 5,
   "order": 60,
   "p1": 278,
   "pn": 282,
   "abstract": [
    "The present study examined two acoustic cues in the production of lexical stress in Dutch: spectral tilt and overall intensity. Sluijter and van Heuven (1996) reported that spectral tilt is a more reliable cue to stress than intensity. However, that study included only a small number of talkers (10) and only syllables with the vowel /a/ and /ɔ/. The present study re-examined this issue in a larger and more variable dataset. We recorded 38 native speakers of Dutch (20 females) producing 744 tokens of Dutch segmentally overlapping words (e.g., VOORnaam vs. voorNAAM, “first name” vs. “respectable”), targeting 10 different vowels, in variable sentence contexts. For each syllable, we measured overall intensity and spectral tilt following Sluijter and van Heuven (1996). Results from Linear Discriminant Analyses showed that, for the vowel /a/ alone, spectral tilt showed an advantage over intensity, as evidenced by higher stressed/unstressed syllable classification accuracy scores for spectral tilt. However, when all vowels were included in the analysis, the advantage disappeared. These findings confirm that spectral tilt plays a larger role in signaling stress in Dutch /a/ but show that, for a larger sample of Dutch vowels, overall intensity and spectral tilt are equally important."
   ],
   "doi": "10.21437/SpeechProsody.2022-57"
  },
  "gussenhoven22_speechprosody": {
   "authors": [
    [
     "Carlos",
     "Gussenhoven"
    ],
    [
     "Wei-Rong",
     "Chen"
    ]
   ],
   "title": "Segmental intonation in Zwara Berber voiceless stressed syllable rimes",
   "original": "21",
   "page_count": 4,
   "order": 61,
   "p1": 283,
   "pn": 286,
   "abstract": [
    "Zwara Berber has regular word stress on the penultimate syllable. Syllable peaks may contain any vowel or consonant. When voiceless fricatives or plosives fill the peaks of stressed syllables, the f0 profile of the pitch accent associated with the stressed syllable is interrupted voiceless portion of the speech signal. The interruption may continue into the next syllable onset, which frequently happens when voiceless geminates straddle the rime-onset boundary. In order to establish whether missing f0 profiles affect friction and burst spectra, a male speaker recorded a corpus of 9 words with voiceless fricatives and 4 words with voiceless plosives in four intonation conditions four times. Plosive bursts and three one-third portions of friction intervals were judged for friction pitch in an AX-experiment whereby the pitch of the voiceless friction of X relative to A was to be judged on a 7-point scale. Scores show that (i) the same segments have similar pitch profiles in different words; (ii) questions have higher pitch profiles than statements, replicating results with CoG measurements by Niebuhr 2008; (iii) pitch profiles do not mimic missing f0-contours. We conclude that the speaker controlled the spectral properties of friction and bursts only to reflect the declarative-interrogative contrast."
   ],
   "doi": "10.21437/SpeechProsody.2022-58"
  },
  "gibbon22_speechprosody": {
   "authors": [
    [
     "Dafydd",
     "Gibbon"
    ]
   ],
   "title": "Speech rhythms: learning to discriminate speech styles",
   "original": "32",
   "page_count": 5,
   "order": 65,
   "p1": 302,
   "pn": 306,
   "abstract": [
    "This study addresses the role of speech rhythms in characterising speech styles. The assumptions are: speech rhythms are wave-like oscillations with frequencies below 10 Hz, which can be detected as peaks in low frequency spectral analysis of amplitude and frequency modulations of speech; there are superimposed rhythms with different frequencies; the rhythm frequencies vary within formant-like spectral frequency zones; long-term speech rhythms vary with speech style. Further, it is assumed that speech styles are rhythmically sufficiently distinct to be detected by basic unsupervised machine learning methods. The study combines (1) signal processing by rhythm formant analysis (RFA) and (2) basic unsupervised machine learning (ML) with k-means and hierarchical clustering, and (3) detailed explanation of the phonetic ML results with reference to linguistically annotated data. These methods are applied to two styles from the Aix-MARSEC English speech style database. The main result is that rhythmic properties of these speech styles can be clearly distinguished and explained using the methods described. The study combines novel methodological exploration with confirmatory testing using established methods, and applies these to a domain which is rarely examined quantitatively: speech rhythms in long utterances."
   ],
   "doi": "10.21437/SpeechProsody.2022-62"
  },
  "franich22_speechprosody": {
   "authors": [
    [
     "Kathryn",
     "Franich"
    ],
    [
     "Hermann",
     "Keupdjio"
    ]
   ],
   "title": "The Influence of Tone on the Alignment of Speech and Co-Speech Gesture",
   "original": "227",
   "page_count": 5,
   "order": 66,
   "p1": 307,
   "pn": 311,
   "abstract": [
    "Evidence continues to accrue suggesting that co-speech gestures form an integrated part of the prosodic system of languages. Several studies have highlighted a tight link between the timing of gestures of the hands and head with syllables bearing prosodic prominence. Most work to date has examined this relationship in Indo-European languages, where gestures appear to be crucially timed with respect to pitch-accented syllables. Less work has examined the timing of co-speech gestures in tonal languages, where pitch plays quite a different role within the phonological system. Here, we examine the influence of tone on the timing of manual co-speech gestures in Medʉmba, a Grassfields Bantu language spoken in Cameroon. We investigate 1) whether certain tones are more likely than others to associate with manual gestures in the language; and 2) whether the fine timing of the speech-gesture relationship is influenced by the tone or relative f0 of the syllable it co-occurs with. Our findings indicated no preference for any one tone to occur with co-speech gestures. However, gesture apexes were found to align significantly later with respect to the accompanying syllable’s vowel for low-toned syllables as compared with syllables of other tones.\n"
   ],
   "doi": "10.21437/SpeechProsody.2022-63"
  },
  "werner22_speechprosody": {
   "authors": [
    [
     "Raphael",
     "Werner"
    ],
    [
     "Jürgen",
     "Trouvain"
    ],
    [
     "Bernd",
     "Möbius"
    ]
   ],
   "title": "Optionality and variability of speech pauses in read speech across languages and rates",
   "original": "73",
   "page_count": 5,
   "order": 67,
   "p1": 312,
   "pn": 316,
   "abstract": [
    "Most prosodic boundaries are optional regarding their location. Moreover, markers of prosodic boundaries such as speech pauses show great variability in the duration of pauses as a whole and that of breath noises occurring in these pauses. Optionality and variability of pauses are particularly observable when the speaking rate is varied. In this study we investigated pausing behaviour in six languages of the BonnTempoCorpus with 46 speakers who read aloud a semantically similar short passage at five rates, from very fast to very slow. The general picture across languages shows that pause duration, as expected, correlates with rate, and breath noise duration correlates with total pause duration. The optionality of pauses is reflected by the number of pauses (within but also across different rates) and by the importance of some locations. The variability is evident in pause and inhalation durations: pause durations and number of inhalations decrease at faster rates, whereas both increase when a pause is less optional at a given location in the text. We consider a closer look at details of pauses to be an important step for prosody modelling and essential for exploring and explaining stylistic variation in prosodic phrasing."
   ],
   "doi": "10.21437/SpeechProsody.2022-64"
  },
  "li22_speechprosody": {
   "authors": [
    [
     "Jinyu",
     "Li"
    ],
    [
     "Leonardo",
     "Lancia"
    ]
   ],
   "title": "Effects of delayed auditory feedback interacting with prosodic structure",
   "original": "173",
   "page_count": 5,
   "order": 68,
   "p1": 317,
   "pn": 321,
   "abstract": [
    "Speakers usually respond to time-delayed auditory feedback (DAF) by decreasing their speech rate (i.e., lengthening syllables). However, the syllable position in prosodic structure may affect syllabic prominence and duration. In the present study, we investigated whether the lengthening effect of DAF on syllables could depend on their position in French utterance. We analyzed recordings of several repetitions of three five-syllables French sentences from 10 French speakers under three conditions of DAF (0, 60, 120ms). The results suggest that the duration of syllables is generally longer when DAF is present, and it increases with the increasing DAF level. Accented vowels are more lengthened by DAF in relation to nonaccented vowels in the same accentual group. Final sentence vowels, which bear the nuclear pitch accent and may be additionally affected by final lengthening, could even be more lengthened by DAF. Given that the extent of lengthening effect is not correlated with the original syllabic duration, we assume that the greater lengthening effect on accented vowels could not be due to the longer duration of these vowels in general. Overall, our results suggest that speakers’ responses to DAF depend on the syllabic status in prosodic hierarchy."
   ],
   "doi": "10.21437/SpeechProsody.2022-65"
  },
  "white22b_speechprosody": {
   "authors": [
    [
     "Laurence",
     "White"
    ],
    [
     "Sven",
     "Mattys"
    ],
    [
     "Sarah",
     "Knight"
    ],
    [
     "Tess",
     "Saunders"
    ],
    [
     "Laura",
     "Macbeath"
    ]
   ],
   "title": " Temporal expectations and the interpretation of timing cues to word boundaries",
   "original": "207",
   "page_count": 5,
   "order": 69,
   "p1": 322,
   "pn": 326,
   "abstract": [
    "In many languages, speech sounds adjacent to prosodic boundaries are lengthened. Moreover, listeners – particularly learners – exploit word-initial consonant lengthening to locate word boundaries, whilst phrase-final lengthening indicates upcoming prosodic breaks and conversational transitions. How lengthening is detected in the temporally-linear speech stream remains unclear, however. We investigated listeners’ possible use of predictive mechanisms to generate hypotheses about upcoming segment durations and thereby interpret deviations from temporal expectations (specifically, lengthening) as linguistically meaningful. In novel nonword segmentation experiments, listeners heard 90 twelve-syllable nonsense streams, with – on half the trials – trisyllabic nonword targets embedded (e.g., dumipako*libeku*binudafolu). Segment duration was systematically varied. On the 45 target-present trials, targets were early, medial or late in the nonsense stream (but at least two syllables from utterance edges). Listeners had to respond quickly and accurately when detecting targets. As expected, target-initial consonant lengthening boosted detection, but this was strongly conditioned by target location within utterances. Specifically, differential effects of timing on detection were much stronger for utterance-late targets than utterance-medially (with uniformly poor utterance-early performance). We explore the factors contributing to this (initially unexpected) pattern, in particular, the hypothesis that predictions about segment duration rely on sufficient experience of foregoing speech rate.\n"
   ],
   "doi": "10.21437/SpeechProsody.2022-66"
  },
  "kuznetsova22_speechprosody": {
   "authors": [
    [
     "Natalia",
     "Kuznetsova"
    ],
    [
     "Elena",
     "Markus"
    ]
   ],
   "title": "Ongoing vowel shortening in vanishing Soikkola Ingrian: challenges for description, codification, and typology",
   "original": "211",
   "page_count": 5,
   "order": 70,
   "p1": 327,
   "pn": 331,
   "abstract": [
    "The vanishing Soikkola dialect of Ingrian (Finnic; Uralic) manifests an ongoing shortening of second syllable unstressed long vowels (V2) in trisyllables. Our major acoustic study [1] showed that the original phonological contrast of long and short V2 is currently in a state of fine-grained continuum from contrast maintenance to complete merger, depending on the structure.\nStructural variation is aggravated by considerable interspeaker variability, addresed in this paper. Out of the five studied speakers, three were innovative and two conservative as regards long V2 shortening. Speakers do not communicate in the language any longer, which affects the natural curve of sound changes. Moreover, this particular sound change is likely never to be completed due to imminent language loss.\nUnfinished long V2 shortening with its high interspeaker variability creates challenges for the development of practical transcription, needed for language description, codification, and teaching, and for the typological placement of the rare ternary quantity contrast of consonants attested in Soikkola Ingrian."
   ],
   "doi": "10.21437/SpeechProsody.2022-67"
  },
  "yazawa22_speechprosody": {
   "authors": [
    [
     "Kakeru",
     "Yazawa"
    ],
    [
     "Mariko",
     "Kondo"
    ]
   ],
   "title": "A Comparison of Rhythm Metrics for L2 Speech",
   "original": "106",
   "page_count": 5,
   "order": 71,
   "p1": 332,
   "pn": 336,
   "abstract": [
    "A wide range of rhythm metrics (global metrics: %V, Δ, Varco, and segVarco; pairwise metrics: rPVI, nPVI, CCI, and DnCCI) was applied to L1 Japanese speakers’ L2 English speech data. Less proficient Japanese speakers of English are expected to show less durational variability for both vocalic and consonantal intervals (because of insufficient stress realization and transfer of CV syllable structure), although this pattern may be obscured by their slower speech rate (which increases interval durations in general). To test if the metrics can capture the L2 rhythmic characteristics, each metric was applied to read speech samples of “The North Wind and the Sun” by 183 Japanese speakers in the J-AESOP corpus. Only %V, VarcoV, and segVarcoV/C were successful; other metrics yielded inconsistent or implausible results likely due to insufficient rate normalization. The overall results indicate that global metrics can effectively quantify L2 rhythm if speech rate is normalized by the mean duration of segments (which is a good predictor of tempo) rather than the mean interval duration (which is popular but susceptible to syllable complexity).\n"
   ],
   "doi": "10.21437/SpeechProsody.2022-68"
  },
  "morand22_speechprosody": {
   "authors": [
    [
     "Marie-Anne",
     "Morand"
    ],
    [
     "Melissa",
     "Bruno"
    ],
    [
     "Sandra",
     "Schwab"
    ],
    [
     "Stephan",
     "Schmid"
    ]
   ],
   "title": "Syllable rate and speech rhythm in multiethnolectal Zurich German: a comparison of speaking styles",
   "original": "28",
   "page_count": 5,
   "order": 72,
   "p1": 337,
   "pn": 341,
   "abstract": [
    "Multiethnolectal ways of speaking have been emerging for 30 years in culturally and linguistically diverse neighborhoods of European cities, including Zurich (Switzerland). Among the prosodic features of Germanic multiethnolects, a so-called ‘staccato’ rhythm has been mentioned in several studies. For instance, a comparison between two groups of adolescents (12 speakers each) showed that speakers of multiethnolectal Zurich German displayed slower syllable rates and less vowel duration variability than speakers of a rather traditional dialect. This study compares syllable rate and speech rhythm metrics (nPVI-V, nPVI-C) in spontaneous and read speech of 48 Zurich German adolescents. In a regression analysis, rhythmic measures were compared with the perception of how multiethnolectal the speakers sounded (rating score). The results showed that syllable rate and nPVI-V were related to rating score independently of speaking style (read, spontaneous speech): Speakers who were perceived as more multiethnolectal had a slower syllable rate and less vowel duration variability. Such findings were not observed for nPVI-C. These results suggest that syllable rate and speech rhythm (at least, vowel duration variability) are stable phonetic features of multiethnolectal Zurich German, since the relationship between these features and the perception of multiethnolectal speech was observed in both read and spontaneous speech."
   ],
   "doi": "10.21437/SpeechProsody.2022-69"
  },
  "zhu22_speechprosody": {
   "authors": [
    [
     "Zhiqiang",
     "Zhu"
    ],
    [
     "Peggy Pik Ki",
     "Mok"
    ]
   ],
   "title": "Can speech rate transfer between languages? Evidence from Japanese and Mandarin Chinese",
   "original": "95",
   "page_count": 5,
   "order": 73,
   "p1": 342,
   "pn": 346,
   "abstract": [
    "Whether speech rate can transfer between languages with distinctive speech rates is an understudied issue. Impressionistically, Japanese is faster than Mandarin Chinese. We investigated the speech rate of native Japanese and Mandarin speakers, advanced L2 learners and simultaneous bilinguals respectively. Nine native Beijing Mandarin speakers (NM), five native Japanese speakers (NJ), thirteen advanced L1 Japanese learners of Mandarin (AJ) and eleven Japanese-Mandarin simultaneous bilinguals (SB) participated in a passage reading task and a spontaneous speech task. The comparison between Japanese and Mandarin by NM and NJ speakers confirmed that the speech rate of native Japanese was faster than that of Mandarin. Comparison between the speech rate of Japanese and Mandarin by AJ and SB speakers showed that both AJ and SB speakers produced Japanese constantly faster than their Mandarin. Both AJ and SB speakers produced Japanese similarly as NJ speakers did. However, the Mandarin speech rate by AJ speakers was significantly slower than that of NM speakers, while the Mandarin speech rate between SB speakers and NM speakers remained non-significant. The findings challenge previous proposals that speech rate transfer could happen at a language level. Moreover, simultaneous bilinguals showed an advantage over advanced L2 learners in speech rate mastery."
   ],
   "doi": "10.21437/SpeechProsody.2022-70"
  },
  "wang22b_speechprosody": {
   "authors": [
    [
     "Chengxia",
     "Wang"
    ],
    [
     "Yi",
     "Xu"
    ],
    [
     "Jinsong",
     "Zhang"
    ]
   ],
   "title": "The invalidity of rhythm class hypothesis",
   "original": "240",
   "page_count": 5,
   "order": 74,
   "p1": 347,
   "pn": 351,
   "abstract": [
    "Languages are said to be stress-timed, syllable-timed or mora-timed. In a stress-timed language, inter-stress intervals are or tend to be constant, hence, isochronous, while in a syllable-timed or mora-timed language, successive syllables or morae are or tend to be equal in duration. Empirical research has failed to find evidence of isochrony in any language, yet the hypothesis is now sustained by perception accounts or phonetic metrics that do not measure isochrony. We have re-examined the rhythm class hypothesis by looking for evidence of at least a tendency toward isochrony, through a comparison of English, an alleged stress-timed language, and Mandarin, an alleged syllable-timed language. The results show that in English, segments are not compressible to allow equal syllable duration, and syllables are incompressible to enable equal inter-stress interval duration and phrase duration. In contrast, Mandarin shows a small tendency toward both equal syllable duration and equal phrase duration. These findings are exactly the opposite of what would be predicted by the rhythm class hypothesis. We therefore argue that the hypothesis is not just flawed, but simply untenable, and the so-called rhythm classes should no longer be held as a basic fact of human language."
   ],
   "doi": "10.21437/SpeechProsody.2022-71"
  },
  "ambrazaitis22_speechprosody": {
   "authors": [
    [
     "Gilbert",
     "Ambrazaitis"
    ],
    [
     "Johan",
     "Frid"
    ],
    [
     "David",
     "House"
    ]
   ],
   "title": "Auditory vs. audiovisual prominence ratings of speech involving spontaneously produced head movements",
   "original": "102",
   "page_count": 5,
   "order": 75,
   "p1": 352,
   "pn": 356,
   "abstract": [
    "Visual information can be integrated in prominence perception, but most available evidence stems from controlled experimental settings, often involving synthetic stimuli. The present study provides evidence from spontaneously produced head gestures that occurred in Swedish television news readings. Sixteen short clips (containing 218 words in total) were rated for word prominence by 85 adult volunteers in a between-subjects design (44 in an audio-visual vs. 41 in an audio-only condition) using a web-based rating task. As an initial test of overall rating behavior, average prominence across all 218 words was compared between the two conditions, revealing no significant difference. In a second step, we compared normalized prominence ratings between the two conditions for all 218 words individually. These results displayed significant (or near significant, p<.08) differences for 28 out of 218 words, with higher ratings in either the audiovisual (13 words) or the audio-only-condition (15 words). A detailed examination revealed that the presence of head movements (previously annotated) can boost prominence ratings in the audiovisual condition, while words with low prominence tend to be rated slightly higher in the audio-only condition. The study suggests that visual prominence signals are integrated in speech processing even in a relatively uncontrolled, naturalistic setting."
   ],
   "doi": "10.21437/SpeechProsody.2022-72"
  },
  "barrault22_speechprosody": {
   "authors": [
    [
     "Axel",
     "Barrault"
    ],
    [
     "James",
     "German"
    ],
    [
     "Pauline",
     "Welby"
    ]
   ],
   "title": "Anticipatory marking of (non-corrective) contrastive focus by the Initial Rise in French",
   "original": "212",
   "page_count": 5,
   "order": 76,
   "p1": 357,
   "pn": 361,
   "abstract": [
    "This study addresses tonal marking of non-corrective contrastive focus in French. Speakers read aloud sentences composed of two parallel clauses, where the structure of the post verbal constituent under focus was varied by the presence (i.e., Noun focus) or absence (i.e., Noun Phrase focus) in the second clause of the final adjective appearing in the first clause. This way, we were able to test whether the Initial Rise is a marker of the span of an upcoming non-corrective contrast in the second clause. We posited that French speakers mark contrast tonally in the first and/or second clause. Corroborating previous findings, we found that a faster speech rate is associated with fewer Initial Rises. More importantly, an Initial Rise occurs on the direct object of the first clause more often when narrow focus spans only the Noun. Additionally, neither the height of the Initial Rise peak nor Hi peaks scaling depend on focus structure. These results suggest that anticipatory use of the Initial Rise may signal an upcoming contrast and represents additional complexity in the tonal encoding of the left edge of a contrastively focused constituent."
   ],
   "doi": "10.21437/SpeechProsody.2022-73"
  },
  "michelas22_speechprosody": {
   "authors": [
    [
     "Amandine",
     "Michelas"
    ],
    [
     "Sophie",
     "Dufour"
    ]
   ],
   "title": "Gradiency vs. categoricity: How French speakers perceive accentual information in their native language?",
   "original": "44",
   "page_count": 5,
   "order": 77,
   "p1": 362,
   "pn": 366,
   "abstract": [
    "Previous works have suggested that French speakers have difficulties to process accentual information at an abstract level of processing when they are confronted to lexical stress found in languages such as English or Spanish. However, the way they process accentual information in their own language (a language without lexical stress) deserves further investigation. In this experiment, we address the issue of the nature of the perception of native accentual information by French speakers in a categorical perception paradigm. French speakers had to identify lexical sequences on an acoustic continuum ranging from an unaccented first syllable [depla'se] ‘moved’ to an accented first syllable ['de] [pla'se] ‘a dice placed’. Results showed that, when tested on an acoustic continuum, French speakers perceive accentual information in a gradual manner. These results corroborate previous studies on the perception of non-native stress patterns by French speakers showing that, in simple perception tasks, their perception of accentual information is acoustically-based and not categorical."
   ],
   "doi": "10.21437/SpeechProsody.2022-74"
  },
  "karpinski22_speechprosody": {
   "authors": [
    [
     "Maciej",
     "Karpiński"
    ],
    [
     "Ewa",
     "Jarmołowicz-Nowikow"
    ],
    [
     "Katarzyna",
     "Klessa"
    ]
   ],
   "title": "High-pitched prominences in the speeches of male Polish members of parliament",
   "original": "157",
   "page_count": 5,
   "order": 78,
   "p1": 367,
   "pn": 371,
   "abstract": [
    "The tendency to use a lower voice in public speeches may be well justified by evolutionary and social factors. Low, stable voice is often associated with authority and persuasive power while high-pitched voices are linked to intensive active emotions. In this light, the occurrence of extremely high intonation peaks or falsetto voice in public speeches by male speakers is puzzling and requires explanation. In this study we analyze the usage of high-pitched intonation peaks in ten male members of the Polish parliament based on recordings selected from the MuMoStance Corpus of German and Polish Parliamentary Speeches. The speeches are scrutinized for high pitched prominences using the criteria of (a) relative pitch range and (b) top pitch value relative to the mean pitch of each speaker. The material includes also prominence areas annotations for: hand gestures, head, body movements, and the discourse functions realized by the respective utterances. We qualitatively explore on the distribution, form, and function of high-pitched prominences, and their co-occurrence with gestures and body movement. We find the usage of high-pitched prominences to significantly differ among and within speeches. While they tend to co-occur with gestures, the connection of their characteristics with gestural features is not straightforward."
   ],
   "doi": "10.21437/SpeechProsody.2022-75"
  },
  "ukaszewicz22_speechprosody": {
   "authors": [
    [
     "Beata",
     "Łukaszewicz"
    ],
    [
     "Janina",
     "Mołczanow"
    ],
    [
     "Anna",
     "Łukaszewicz"
    ]
   ],
   "title": "Pretonic Lengthening as the Lexical Stress Domain Extension",
   "original": "117",
   "page_count": 5,
   "order": 79,
   "p1": 372,
   "pn": 376,
   "abstract": [
    "This paper reports on an acoustic study of pretonic lengthening in Ukrainian, a language with a hybrid metrical system comprising lexical stress and grammatical secondary (rhythmic) stress, both cued by vowel duration ([1], [2]). The existence of pretonic lengthening has gone unnoticed in the traditional descriptions, and although recent acoustic research ([1], [3], 4]) has pointed to its presence in Ukrainian, this phenomenon has not been systematically investigated. The phenomenon is interesting because by reducing the temporal difference between the pretonic and the tonic syllable, pretonic lengthening weakens the durational cue to lexical stress. It also distorts the otherwise regular iteration of secondary stresses. In this paper, we report on the measurements of the duration and formant (F1/F2) structure of the vowel /a/ in different prosodic positions (lexical stress and three preceding syllables). The analysis of the data collected from 11 native speakers of Ukrainian points to a gradual effect of lengthening and the presence of vowel undershoot across syllables preceding lexical stress. We argue that the existence of pretonic lengthening in Ukrainian is closely related to the domain of lexical stress, and may reflect a more universal anticipatory effect induced by lexical stress."
   ],
   "doi": "10.21437/SpeechProsody.2022-76"
  },
  "santiago22_speechprosody": {
   "authors": [
    [
     "Fabian",
     "Santiago"
    ],
    [
     "Paolo",
     "Mairano"
    ],
    [
     "Bianca De",
     "Paolis"
    ]
   ],
   "title": "The effects of prosodic prominence on the acquisition of L2 phonological features",
   "original": "150",
   "page_count": 5,
   "order": 80,
   "p1": 377,
   "pn": 381,
   "abstract": [
    "Mainstream L2 phonology models do not make clear predictions concerning how the prosodic structure can interact with the acquisition of segments. Our goal is to provide empirical evidence for the positive effects of prosody on the acquisition of challenging L2 French sounds. We analyzed oral productions of 40 participants: 10 French native speakers and 30 L2 French learners with L1 Spanish, L1 English and L1 Italian. We extracted acoustic parameters for ~8k vowels and calculated the degree of acoustic overlap via Pillai scores for the following triplets: /i/ ~ /y/ ~ /u/, /e/ ~ /ø/ ~ /o/ and /ɛ/ ~ /œ/ ~ /ɔ/. We analyzed these vowels in two different prosodic positions: (1) word internal (unaccented), (2) in initial/final boundaries of Accentual Phrases. Our results show that the production of initial/final accents at the AP level results in a smaller acoustic overlap of L2 French vowel contrasts. However, this positive effect is not observed across all the vowel pairs analysed. We discuss the importance of the prosody-phonetics interface on the acquisition of L2 sounds.\n"
   ],
   "doi": "10.21437/SpeechProsody.2022-77"
  },
  "kalashnikova22_speechprosody": {
   "authors": [
    [
     "Marina",
     "Kalashnikova"
    ],
    [
     "Cristina",
     "Naranjo"
    ]
   ],
   "title": "Prosody in bilingual caregiver’s infant-directed speech: Cues for infants’ acquisition of their languages’ intonational structure",
   "original": "86",
   "page_count": 5,
   "order": 81,
   "p1": 382,
   "pn": 386,
   "abstract": [
    "Adults rely on various cues to differentiate among utterance types (e.g., declarative vs. interrogative utterances), including the conversational context, morpho-syntactic markers, and intonation. Infants have limited access to many of these language-specific cues, but they can rely on prosodic information in their speech input to learn the intonational patterns that differentiate utterance types in their language. Unlike monolinguals, bilingual infants must also use this information to differentiate the intonational patterns of each of their native languages. This study investigated the role that prosodic adjustments in bilingual caregivers’ infant-directed speech (IDS) play in facilitating this task for young bilingual infants (12-30 months). This was assessed in naturally produced IDS and adult-directed speech (ADS) by Spanish-Basque bilinguals. These two languages differ drastically in their lexicon, morpho-syntactic structure, and prosody. We measured pitch, duration, and intonational contours of mothers’ infant-directed and adult-directed productions of declarative, WH- and yes/no interrogative utterances. Across utterance types and languages, IDS prosody was exaggerated compared to ADS. Additionally, intonation contours in Spanish and Basque IDS magnified the differences between WH- and yes/no interrogative utterances. These findings suggest that bilingual caregivers’ IDS may contain prosodic cues that facilitate infants’ early acquisition of the intonational patterns of their two languages."
   ],
   "doi": "10.21437/SpeechProsody.2022-78"
  },
  "sousa22_speechprosody": {
   "authors": [
    [
     "Ricardo",
     "Sousa"
    ],
    [
     "Susana",
     "Silva"
    ],
    [
     "Sónia",
     "Frota"
    ]
   ],
   "title": "Early Prosodic Development predicts Lexical Development in typical and atypical language acquisition",
   "original": "192",
   "page_count": 5,
   "order": 82,
   "p1": 387,
   "pn": 391,
   "abstract": [
    "An early sensitivity to prosody is well-documented in the language development literature, and has been suggested to facilitate language learning. However, prosodic development and its relation to other areas of language development has been less studied. Some studies suggested that early prosodic development and early lexical development are related in language production [1,2]. In this longitudinal study, we investigated whether early prosodic development (assessed before 19 months) predicted receptive and expressive vocabulary outcomes in three groups of children: typically developing, at risk for language impairment, and with Down Syndrome. We used data from two parental reports: a new parental report of prosodic skills (ProsoQuest, [3]), and the CDI short forms for infants and toddlers [4]. Data from 23 and 79 pairs of reports was analyzed respectively for receptive and expressive vocabulary, using linear mixed models. The prosody comprehension part of ProsoQuest was found to predict receptive vocabulary development, and the prosody production part to predict expressive vocabulary development. There was no interaction with group. These findings suggest that at a young age prosody predicts lexical development, and thus early assessment of prosody may play a crucial role in the screening, prevention, early intervention and diagnosis of language impairments."
   ],
   "doi": "10.21437/SpeechProsody.2022-79"
  },
  "arciuli22_speechprosody": {
   "authors": [
    [
     "Joanne",
     "Arciuli"
    ],
    [
     "Kate",
     "Philips"
    ],
    [
     "Benjamin",
     "Bailey"
    ],
    [
     "Alexandre",
     "Forndran"
    ],
    [
     "Adam",
     "Vogel"
    ],
    [
     "Kirrie",
     "Ballard"
    ]
   ],
   "title": "Lexical Stress Matures Late in Typically Developing Children",
   "original": "232",
   "page_count": 4,
   "order": 83,
   "p1": 392,
   "pn": 395,
   "abstract": [
    "Previous studies on the development of lexical stress production in English include narrow age ranges. We addressed this issue by eliciting acoustic data from 253 typically developing 3-12 year olds and adults. Here we report a subset of results from our larger study. We report the results of acoustic analyses of word productions based on picture naming of 12 weak-strong (WS) words. WS words may be more challenging for English speaking children due to trochaic bias. We measured stress contrastivity (duration, intensity and fundamental frequency) across the first two syllables of each correct word production using the normalized pairwise variability index (PVI). While word productions were intelligible and exhibited stress contrastivity, generally, children did not exhibit adult-like magnitudes of stress contrastivity."
   ],
   "doi": "10.21437/SpeechProsody.2022-80"
  },
  "carvalho22_speechprosody": {
   "authors": [
    [
     "Alex de",
     "Carvalho"
    ],
    [
     "Leticia",
     "Kolberg"
    ],
    [
     "John",
     "Trueswell"
    ],
    [
     "Anne",
     "Christophe"
    ]
   ],
   "title": "Cross-linguistic evidence for the role of phrasal prosody in syntactic and lexical acquisition",
   "original": "194",
   "page_count": 5,
   "order": 84,
   "p1": 396,
   "pn": 400,
   "abstract": [
    "Phrasal prosody is believed to be crucial for syntactic and lexical acquisition, since prosodic boundaries in speech coincide with lexical and syntactic boundaries. However, studies on children’s ability to use prosody for parsing have reported mixed results, with some providing positive evidence in French while many studies in other languages failed to observe this ability in children up to 6-years-old. To investigate whether the discrepancies between these studies are due to cross-linguistic or methodological differences, we adapted recent eye-tracking experiments showing that French children successfully use prosody to constrain parsing, to two different languages: English and Brazilian-Portuguese. In English, we used locally ambiguous sentences containing noun-verb homophones that could be disambiguated by prosody. In Brazilian-Portuguese, we tested preschoolers’ ability to use prosody to interpret ellipsis (e.g., [The tiger is hitting!][The duck too!]) vs transitive sentences (e.g., [The tiger][is hitting the duck too!]). The results showed that 3-to-5-year-olds in the two languages successfully exploited prosodic structure to constrain their parsing. These studies provide cross-linguistic evidence for the role of phrasal prosody in parsing."
   ],
   "doi": "10.21437/SpeechProsody.2022-81"
  },
  "munozcoego22_speechprosody": {
   "authors": [
    [
     "Sara",
     "Munoz-Coego"
    ],
    [
     "Júlia",
     "Florit-Pons"
    ],
    [
     "Patrick Louis",
     "Rohrer"
    ],
    [
     "Ingrid",
     "Vilà-Giménez"
    ],
    [
     "Pilar",
     "Prieto"
    ]
   ],
   "title": "The prosodic and gestural marking of the information status of referents in children’s narrative speech: A longitudinal study",
   "original": "220",
   "page_count": 5,
   "order": 85,
   "p1": 401,
   "pn": 405,
   "abstract": [
    "Developmental studies have claimed that 5- to 7-year-old children use prosodic prominence to mark the status of referential expressions in discourse, where new and accessible referents tend to be prosodically more prominent than given referents. Studies on the gestural marking of information structure have shown that gestures tend to co-occur more frequently with new and focused referents in adult and child speech. Although prosody and gesture are tightly integrated in speech, to our knowledge no previous approach has jointly looked at the development of these multimodal cues as markers of the information status of referents. The present study investigates how Catalan-speaking children use prosody and manual gestures to signal the information status of referents in narrative speech. A longitudinal database was used containing elicited narratives by 83 children at two points in development (5-6 and 7-9 years old). Results showed that new referents (in contrast with accessible and given referents) were signaled through the use of non-referential gestures, specially at the ages of 7-9, and through nuclear accentuation already at the ages of 5-6. Results also revealed that all types of referents were pitch accented at both points in development, regardless of their information status.\n"
   ],
   "doi": "10.21437/SpeechProsody.2022-82"
  },
  "gervain22b_speechprosody": {
   "authors": [
    [
     "Judit",
     "Gervain"
    ]
   ],
   "title": "Word frequency and prosody bootstrap basic word order in prelexical infants",
   "original": "49",
   "page_count": 4,
   "order": 86,
   "p1": 406,
   "pn": 409,
   "abstract": [
    "Languages systematically vary in their basic word order, which infants need to learn as they acquire their native language. The talk presents two behavioral (looking time) experiments with 8-month-old infants and a brain imaging study with newborns suggesting that word frequency and prosody serve as powerful cues to help infants bootstrap the basic lexical categories of functors, frequent and phonologically minimal items, and content words, infrequent, but phonologically heavy words, and guide infants about the relative order of these two categories in their native language. The acoustic realization of prosodic prominence in phonological phrases correlates with basic word order, as functor-initial languages typically rely on phrase-final lengthening, while functor-final language on phrase-initial pitch and/or intensity rise (Nespor et al. 2008). The first study shows that 8-month-old infants can use this acoustic cue to determine the word order of an artificial language. A second study shows that infants expect this prosodic information to be aligned with word frequency, i.e. frequent words to be prosodically non-prominent, as are natural language functors. A near-infrared spectroscopy imaging study suggests that sensitivity to the acoustic realization of prosodic prominence and the resulting rhythmic (iambic/trochaic) grouping derives from babies’ prenatal experience with speech."
   ],
   "doi": "10.21437/SpeechProsody.2022-83"
  },
  "lan22_speechprosody": {
   "authors": [
    [
     "Chen",
     "Lan"
    ],
    [
     "Peggy",
     "Mok"
    ]
   ],
   "title": "A preliminary study on the acquisition of Mandarin neutral tone by young heritage children",
   "original": "109",
   "page_count": 5,
   "order": 87,
   "p1": 410,
   "pn": 414,
   "abstract": [
    "The present study examined the pitch and duration of three types of Mandarin neutral tone (T0) – the possessive particle -de, the noun suffix -zi, and reduplicated words, in four tonal environments (following T1/T2/T3/T4) produced by two heritage language (HL) children longitudinally at 3;0 and 4;0 and two HL children cross-sectionally at 4;0, comparing them with two adult native speakers. Unlike the monolingual children in a previous study, our results indicated that the HL children have not developed a robust neutral tone category by 4;0. Acoustic measurements showed that HL children shared similar shapes of neutral tone pitch contour across types and tonal environments with the adult speakers when they were at 4;0, with a falling pitch contour following T1, T2, T4 and a rising pitch contour following T3. However, even though both HL children and the reference speakers reduced duration while producing neutral tone compared to the preceding tone, significant differences were found for their duration variations; while the reference speakers produced neutral tone after T1, T2, and T4 with a shorter duration than T3 across types, the patterns of HL children varied according to different types of the neutral tone. Individual difference also existed among the HL children."
   ],
   "doi": "10.21437/SpeechProsody.2022-84"
  },
  "xu22_speechprosody": {
   "authors": [
    [
     "Wenwei",
     "Xu"
    ],
    [
     "Chunyu",
     "Ge"
    ],
    [
     "Wentao",
     "Gu"
    ],
    [
     "Peggy",
     "Mok"
    ]
   ],
   "title": "A preliminary analysis on children’s phonation contrast in Kunshan Wu Chinese tones",
   "original": "136",
   "page_count": 5,
   "order": 88,
   "p1": 415,
   "pn": 419,
   "abstract": [
    "Previous studies have established that phonation contrasts can be, apart from pitch, an important dimension of tonal contrasts in some languages, and modern Wu Chinese is a good example in which the lower register tones are produced with breathier phonation than the upper register tones. Nevertheless, researchers have shown that such phonation contrast is declining among young speakers in Shanghai and Suzhou Wu. This pilot study is thus motivated to investigate children’s production in Kunshan Wu, a neighboring yet rather understudied dialect with more tones, in order to see if a similar trend is ongoing. Two male and two female school-age children (8;4 to 10;4) were recorded reading isolated monosyllabic words with different lexical tones, and simultaneous acoustic and electroglottographic (EGG) data were collected. Results of EGG and acoustic parameters demonstrate that at least near the onset of the vowel, glottal constriction is smaller and glottal closure is less abrupt in the lower register tones than in the upper register tones, suggesting that the lower register tones are generally produced with breathier phonation. Therefore, school-age child speakers of Kunshan Wu are still able to produce the phonation contrast between the tone registers."
   ],
   "doi": "10.21437/SpeechProsody.2022-85"
  },
  "simko22_speechprosody": {
   "authors": [
    [
     "Juraj",
     "Šimko"
    ],
    [
     "Adaeze",
     "Adigwe"
    ],
    [
     "Antti",
     "Suni"
    ],
    [
     "Martti",
     "Vainio"
    ]
   ],
   "title": "A Hierarchical Predictive Processing Approach to Modelling Prosody",
   "original": "67",
   "page_count": 5,
   "order": 89,
   "p1": 420,
   "pn": 424,
   "abstract": [
    "Prosodic patterns-and linguistic structures in general-are hierarchical in nature, providing for efficient means for encoding information in temporally constrained situations where communicative events occur. However, there are no theoretical frameworks that are capable of representing the full extent of linguistic behaviour in a cohesive way that could capture the paradigmatic and syntagmatic links between the organizational levels present in everyday speech. Here we propose a novel theoretical and modelling account of perception and production of prosodic patterns in speech communication, derived from the influential Predictive Processing theory of neural implementation of perception and action based on a hierarchical system of generative models producing progressively more detailed probabilistic predictions of future events. The framework provides a conceptualization of the hierarchical organization of speech prosody as well as a principled way of unifying speech perception and production by postulating a single processing hierarchy shared by both modalities. We discuss the possible implications of the theory for prosodic analysis of speech communication, including conversational setting. In addition, we outline a viable computational implementation in form of a machine learning architecture that can be used as a testbed for generating and evaluating predictions brought forth by the theory."
   ],
   "doi": "10.21437/SpeechProsody.2022-86"
  },
  "lai22_speechprosody": {
   "authors": [
    [
     "Li-Fang",
     "Lai"
    ],
    [
     "Janet G. van",
     "Hell"
    ],
    [
     "John",
     "Lipski"
    ]
   ],
   "title": "The Role of Rhythm and Vowel Space in Speech Recognition",
   "original": "256",
   "page_count": 5,
   "order": 90,
   "p1": 425,
   "pn": 429,
   "abstract": [
    "This paper explores the role of rhythm and vowel space in automatic speech recognition (ASR), with a particular focus on Midland and Southern American English in the Appalachian region. Three sets of analysis were conducted. First, we computed the word error rates between the ground truth and the transcripts generated by DARLA. Consistent with previous studies, the results show higher error rates for Southern English (59.5%) than for Midland English (47.2%), suggesting a dialect gap in speech recognition. Next, we examined whether the error rates are influenced by rhythm. The results show that neither %V nor ΔV reliably predicted ASR performance. We also sought to draw a link between vowel space, speech intelligibility, and ASR performance. Three vowel space metrics were considered: convex hull, formant dispersion, and the polygon area. We noticed that as convex hull and formant dispersion increase, the error rates decrease, particularly for Midland speakers. This aligns with our hypothesis that more expanded vowel space enhances speech intelligibility, thus reducing the error rate for the Midland cohort. No clear connection between the polygon area, speech intelligibility, and error rates was found. These results, albeit suggestive, point out some promising directions for improving acoustic modeling in speech recognition."
   ],
   "doi": "10.21437/SpeechProsody.2022-87"
  },
  "mikhailava22_speechprosody": {
   "authors": [
    [
     "Veranika",
     "Mikhailava"
    ],
    [
     "John",
     "Blake"
    ],
    [
     "Evgeny",
     "Pyshkin"
    ],
    [
     "Natalia",
     "Bogach"
    ],
    [
     "Sergey",
     "Chernonog"
    ],
    [
     "Artyom",
     "Zhuikov"
    ],
    [
     "Maria",
     "Lesnichaya"
    ],
    [
     "Iurii",
     "Lezhenin"
    ],
    [
     "Roman",
     "Svechnikov"
    ]
   ],
   "title": "Dynamic Assessment during Suprasegmental Training with Mobile CAPT",
   "original": "57",
   "page_count": 5,
   "order": 91,
   "p1": 430,
   "pn": 434,
   "abstract": [
    "This paper reports the results of a small-scale longitudinal study on the use of StudyIntonation, a computer-assisted pronunciation teaching environment. StudyIntonation aims to scaffold learners through their zone of proximal development by drawing eclectically on concepts, such as Vygotskian socio-cultural theory, dynamic assessment and second language development. Learners perform shadowing tasks, aiming to replicate the suprasegmental prosodic aspects of model sentences. The pitch curves of the model and user attempts are displayed to help learners see their progress. We observed a group of learners who performed shadowing tasks in StudyIntonation for 24 months. The resultant corpus comprises 1050 speech records labelled with orthographic transcript, pitch readings, and similarity metrics. Longitudinal and microgenetic analysis of L2 pronunciation development was conducted on this dataset. Prosodic synchronization between speakers as well as longitudinal pronunciation assessment may allow for quantitative evaluation by means of a dynamical modeling technique of cross-recurrence quantification analysis (CRQA). We located the zone of proximal development of each learner, where she/he reveals an increased responsiveness to input audiovisual stimuli, through the oscillations of pitch similarity metrics of dynamic time warping and CRQA. The rates of cross-recurrence between the model and learner are helpful synchronization indicators and performance predictors."
   ],
   "doi": "10.21437/SpeechProsody.2022-88"
  },
  "mixdorff22_speechprosody": {
   "authors": [
    [
     "Hansjörg",
     "Mixdorff"
    ],
    [
     "Albert",
     "Rilliard"
    ],
    [
     "Philippe Boula De",
     "Mareüil"
    ]
   ],
   "title": "Perceptual Identification of Speech Acts in Gallo-Romance Dialects: A Study Based on Prosody Re-synthesis",
   "original": "124",
   "page_count": 5,
   "order": 92,
   "p1": 435,
   "pn": 439,
   "abstract": [
    "In earlier works we examined four types of speech acts in several Gallo-Romance dialects: statements, polar questions, incredulous questions (expecting a negative response), and queries for confirmation (expecting a positive response). In a perception experiment we found that the first two types are generally reliably identified whereas for the latter two, only a limited number of utterances yielded recognition rates far above chance. We identified confusions mostly between polar and incredulous questions, as well as between the other two. We also established that differences that facilitated discrimination mostly concerned the fundamental frequency (F0) contours. In the current work we conduct a perceptual experiment employing synthetic stimuli, involving pairs of reliably recognized, but by type confusable utterances and attempt to morph one type of speech act into the confusable other. Our results indeed show, that manipulating the utterance-final F0 contour appropriately actually increases the probability of one type of speech act being confused with the other, and what the categorical perception thresholds are.\n"
   ],
   "doi": "10.21437/SpeechProsody.2022-89"
  },
  "taheriardali22_speechprosody": {
   "authors": [
    [
     "Mortaza",
     "Taheri-Ardali"
    ],
    [
     "Daniel",
     "Hirst"
    ]
   ],
   "title": "Building a Persian-English OMProDat Database Read by Persian Speakers",
   "original": "82",
   "page_count": 5,
   "order": 93,
   "p1": 440,
   "pn": 444,
   "abstract": [
    "OMProDat is an open multilingual prosodic database, which aims to collect, archive, and distribute recordings and annotations of directly comparable data from different languages. As part of the OMProDat project, this paper focuses on the creation of a bilingual Persian-English prosodic database read by Persian native speakers. This collection contains 40 continuous, thematically connected paragraphs, each with five sentences, originally taken from the European SAM project. This collection was recorded with 5 male and 5 female standard Persian speakers from monolingual families. The Persian texts were Romanized and transcribed phonetically using the ASCII phonetic alphabet SAMPA. The recordings comprise TextGrids annotations obtained semi-automatically from the sound and the orthographic transcription using the SPPAS alignment software. This considerable amount of data will allow us to compare the production of Persian and English as L1 and L2, respectively. In addition, a cross-linguistic comparison with other languages in OMProDat is easily feasible.\n"
   ],
   "doi": "10.21437/SpeechProsody.2022-90"
  },
  "peirolilja22_speechprosody": {
   "authors": [
    [
     "Alex",
     "Peiró-Lilja"
    ],
    [
     "Guillermo",
     "Cámbara"
    ],
    [
     "Mireia",
     "Farrús"
    ],
    [
     "Jordi",
     "Luque"
    ]
   ],
   "title": "Naturalness and Intelligibility Monitoring for Text-to-Speech Evaluation",
   "original": "53",
   "page_count": 5,
   "order": 94,
   "p1": 445,
   "pn": 449,
   "abstract": [
    "Current text-to-speech (TTS) systems are deep learning-based models capable of learning phonetic articulation and intelligibility, as well as prosodic attributes that model speaking style, providing naturalness to synthetic voices. However, the performance of these models highly depends on their training hyper-parameters and iterations. Besides, a conventional loss function does not reflect a correct voice modeling; thus, we believe a dedicated training assessment on TTS is needed. To this end, we monitor intelligibility and naturalness during training of Tacotron2 model in a 2-step process. First, we report the analysis of a method to follow up the intelligibility of the TTS in terms of character-level token error rate (TER) by using five different automatic speech recognition (ASR) systems. Second, we extend this work with a recently published TTS naturalness predictor that estimates this aspect in terms of mean opinion scores (MOS). Finally, we unify predicted MOS with TER measurements to return, over each training checkpoint, a single score that we name Full Assessment Score (FAS). We report the relevant preference of our listeners on the checkpoint with maximum FAS rather than the one with minimum validation loss, both in intelligibility and naturalness ---up to 62.3% in the latter."
   ],
   "doi": "10.21437/SpeechProsody.2022-91"
  },
  "ballier22_speechprosody": {
   "authors": [
    [
     "Nicolas",
     "Ballier"
    ],
    [
     "Adrien",
     "Méli"
    ],
    [
     "Taylor",
     "Arnold"
    ],
    [
     "Alice",
     "Henderson"
    ]
   ],
   "title": "Revisiting Paratone Prosodic Features with the EIIDA corpus",
   "original": "189",
   "page_count": 5,
   "order": 95,
   "p1": 450,
   "pn": 454,
   "abstract": [
    "This paper discusses the prosodic properties of the paratone, the oral paragraph of speech. With manually annotated paratones and automatic prosodic labels on the EIIDA corpus, we re-examine the claims proposed by Tench (1996). We show that rhythmic cues to signal paratone boundaries seem to be more reliable than absolute pitch values"
   ],
   "doi": "10.21437/SpeechProsody.2022-92"
  },
  "kachkovskaia22_speechprosody": {
   "authors": [
    [
     "Tatiana",
     "Kachkovskaia"
    ],
    [
     "Alla",
     "Menshikova"
    ],
    [
     "Daniil",
     "Kocharov"
    ],
    [
     "Pavel",
     "Kholiavin"
    ],
    [
     "Anna",
     "Mamushina"
    ]
   ],
   "title": "Social and situational factors of speaker variability in collaborative dialogues",
   "original": "231",
   "page_count": 5,
   "order": 96,
   "p1": 455,
   "pn": 459,
   "abstract": [
    "The acoustic features of the speaker's voice in dialogues are liable to change due to various situational factors, such as success of communication, social distance between the interlocutors, conversational roles etc. This paper presents an analysis of variation in the basic prosodic features---pitch, intensity, and speech tempo---across speakers' gender, conversational role (information leader vs. follower), and social distance. The research is based on the SibLing speech corpus where five degrees of social distance between the interlocutors are presented: there are dialogues between same-gender siblings, same-gender friends, same-gender and opposite-gender strangers, strangers of different age and social status. Each pair of interlocutors played a card-matching game and performed a classical map task. The factor of conversational role revealed a significant influence on all the analysed speech features: pitch, intensity, and speech tempo. Gender was not found to influence speech tempo, unlike pitch and loudness. Social distance was shown to play a significant role for speech tempo (e.g., it tends to be lower in dialogues with strangers of different age and social status), and also, in interaction with other factors, for pitch and loudness. There was also a significant influence of the type of task: card-matching game vs. map task."
   ],
   "doi": "10.21437/SpeechProsody.2022-93"
  },
  "seyssel22_speechprosody": {
   "authors": [
    [
     "Maureen de",
     "Seyssel"
    ],
    [
     "Guillaume",
     "Wisniewski"
    ],
    [
     "Emmanuel",
     "Dupoux"
    ],
    [
     "Bogdan",
     "Ludusan"
    ]
   ],
   "title": " Investigating the usefulness of i-vectors for automatic language characterization",
   "original": "114",
   "page_count": 5,
   "order": 97,
   "p1": 460,
   "pn": 464,
   "abstract": [
    "Work done in recent years has shown the usefulness of using automatic methods for the study of linguistic typology. However, the majority of proposed approaches come from natural language processing and require expert knowledge to predict typological information for new languages. An alternative would be to use speech-based methods that do not need extensive linguistic annotations, but considerably less work has been done in this direction. The current study aims to reduce this gap, by investigating a promising speech representation, i-vectors, which by capturing suprasegmental features of language, can be used for the automatic characterization of languages. Employing data from 24 languages, covering several linguistic families, we computed the i-vectors corresponding to each sentence and we represented the languages by their centroid i-vector. Analyzing the distance between the language centroids and phonological, inventory and syntactic distances between the same languages, we observed a significant correlation between the i-vector distance and the syntactic distance. Then, we explored in more detailed a number of syntactic features and we proposed a method for predicting the value of the most promising feature, based on the i-vector information. The obtained results, an 87% classification accuracy, are encouraging and we envision to extend this method further."
   ],
   "doi": "10.21437/SpeechProsody.2022-94"
  },
  "sloan22_speechprosody": {
   "authors": [
    [
     "Rose",
     "Sloan"
    ],
    [
     "Adaeze",
     "Adigwe"
    ],
    [
     "Sahana",
     "Mohandoss"
    ],
    [
     "Julia",
     "Hirschberg"
    ]
   ],
   "title": "Incorporating Prosodic Events in Text-to-Speech Synthesis",
   "original": "195",
   "page_count": 5,
   "order": 62,
   "p1": 287,
   "pn": 291,
   "abstract": [
    "While producing accurate prosody can significantly improve the naturalness and comprehensibility of synthesized speech, many Text-to-Speech (TTS) systems still do not explicitly model prosody. In this paper, we present an approach for incorporating prosodic events, specifically phrase breaks and pitch accents, into TTS output using a two-step pipeline. In the first step, we use a large number of linguistic features to create a model for predicting the locations of prosodic events from text. In the second step, we incorporate these events into the front end of a DNN-based TTS pipeline. We crowd-source labels for pairs of utterances created with and without the new pipeline. Our results show that listeners strongly prefer a voice created using this pipeline to the baseline voice, indicating that this approach of explicitly modeling prosodic events is a fruitful area of research."
   ],
   "doi": "10.21437/SpeechProsody.2022-59"
  },
  "juliao22_speechprosody": {
   "authors": [
    [
     "Mariana",
     "Julião"
    ],
    [
     "Alberto",
     "Abad"
    ],
    [
     "Helena",
     "Moniz"
    ]
   ],
   "title": "Can Prosody Transfer Embeddings be Used for Prosody Assessment?",
   "original": "205",
   "page_count": 5,
   "order": 63,
   "p1": 292,
   "pn": 296,
   "abstract": [
    "In voice conversion, it is possible to transfer some characteristic components of a (target) speech utterance, such as the content, pitch, or speaker identity, from the corresponding component from another (source) utterance. This has recently been achieved by characterizing these components through neural-based vector embeddings which encode the specific information to be transferred. In the particular case of neural prosody embeddings, to the best of our knowledge, no work has explored the informativeness of these embeddings for other purposes, such as prosody assessment or comparison of prosodic patterns. In this work, we use an intonation data set and a voice conversion corpus to explore how these neural prosody embeddings group for utterances of different intonation, content, and speaker identity. We compare these neural prosody embeddings to hand-crafted acoustic-prosodic features and to content embeddings. We found that neural prosody embeddings can achieve a geometrical separability index as high as 0.956 for highly contrastive intonations, and 0.706 for different sentence types.\n"
   ],
   "doi": "10.21437/SpeechProsody.2022-60"
  },
  "cole22_speechprosody": {
   "authors": [
    [
     "Jennifer",
     "Cole"
    ],
    [
     "Jeremy",
     "Steffman"
    ],
    [
     "Sam",
     "Tilsen"
    ]
   ],
   "title": "Shape matters: Machine classification and listeners’ perceptual discrimination of American English intonational tunes",
   "original": "93",
   "page_count": 5,
   "order": 64,
   "p1": 297,
   "pn": 301,
   "abstract": [
    "In Autosegmental-Metrical models of intonational phonology, pitch accents, phrase accents and boundary tones may combine freely to create a predicted set of phonologically distinct phrase-final “nuclear” tunes. In this study we ask if an 8-way distinction in nuclear tune shape in American English, predicted from combinations of 2 (monotonal) pitch accents, 2 phrase accents and 2 boundary tones, is manifest in speech production and in speech perception. F0 trajectories from an imitative speech production experiment were analyzed using (i) neural net classification, and (ii) human listeners’ perceptual discrimination of the model utterances. Pairwise classification accuracy of the imitative productions is highest for tune pairs that differ in holistic shape (high-rising vs. rise-fall), and poorest for tunes with the same shape that differ in (higher vs. lower) final f0. Perception results show a similar pattern, with poor pairwise discrimination for tunes that differ primarily, but by a small degree, in final f0. Together the results suggest a hierarchy of distinctiveness among nuclear tunes, with a robust distinction based on holistic tune shape, which only partly aligns with distinctions in tonal specification, and a weak/poorly differentiated distinction between tunes with the same holistic shape but small differences in final f0.\n"
   ],
   "doi": "10.21437/SpeechProsody.2022-61"
  },
  "toro22_speechprosody": {
   "authors": [
    [
     "Juan Manuel",
     "Toro"
    ]
   ],
   "title": "Using prosody to organize the signal: Sensitivities across species set the stage for prosodic bootstrapping",
   "original": "k3",
   "page_count": 6,
   "order": 3,
   "p1": 1,
   "pn": 6,
   "abstract": [
    "Prosody is a major source of information that both adults and infants use to organize the speech signal, from segmenting words to inferring syntactic structures. Here, I will explore the extent to which the ability to take advantage of prosodic cues that we observe in humans might emerge from sensibilities already present in other species. I will review recent studies along 2 lines of research. The first one covers research into how listeners follow the principles described by the Iambic-Trochaic Law to group sounds. The second one explores how they take advantage of sonority differences and natural prosodic contours to better identify words. Together, the evidence gathered so far suggests that, similarly to humans, non-human animals use certain acoustic cues present in the signal to extract difficult-to-find regularities. More broadly, they provide support to the idea that general perceptual biases that form the bases for prosodic bootstrapping are already present in other animals. Importantly, in humans but not in other animals, such biases are combined with domain-specific representations that guide the discovery of linguistic structures."
   ],
   "doi": "10.21437/SpeechProsody.2022-1"
  },
  "wang22c_speechprosody": {
   "authors": [
    [
     "Sheng-Fu",
     "Wang"
    ]
   ],
   "title": "Pre-boundary lengthening modulates predictability effects on durational variability in Taiwan Southern Min",
   "original": "135",
   "page_count": 5,
   "order": 98,
   "p1": 465,
   "pn": 469,
   "abstract": [
    "Linguistic units with lower predictability (i.e., higher surprisal) tend to be realized with stronger acoustic cues, so do units closer to a prosodic break. The study examined how predictability measurements such as lexical frequency, surprisal, and neighborhood density affect the variations of syllable duration and how these effects interact with durational marking of prosodic phrasing in Taiwan Southern Min. Speech data were extracted from an eight-hour spontaneous speech corpus. Surprisal was estimated with trigram language models trained on a written corpus with 4.13M words. Results show that syllable duration had a positive correlation with surprisal but a negative correlation with neighborhood density. As for positional effects, all predictability effects were neutralized at the pre-boundary syllable, even though there were still predictability effects in cases where penultimate and ante-penultimate lengthening is observed. These ﬁndings highlight how predictability effects are modulated by the durational marking of prosodic phrasing and have implications for an information theoretic view of the balance of signal redundancy in human speech.\n"
   ],
   "doi": "10.21437/SpeechProsody.2022-95"
  },
  "plug22_speechprosody": {
   "authors": [
    [
     "Leendert",
     "Plug"
    ],
    [
     "Robert",
     "Lennon"
    ],
    [
     "Rachel",
     "Smith"
    ]
   ],
   "title": "Schwa deletion and perceived tempo in English",
   "original": "78",
   "page_count": 5,
   "order": 99,
   "p1": 470,
   "pn": 474,
   "abstract": [
    "We report on an experiment aimed to test the hypothesis that listeners orient to canonical forms when judging the tempo of reduced speech. Orientation to canonical forms should yield higher tempo estimates than orientation to surface phone strings when canonical phones are deleted. We tested the hypothesis for English, capitalizing on the fact that the non-realization of schwa in an unstressed syllable (e.g. 'support') may result in a surface phone string associated with a different word than the intended one ('sport'). We presented listeners with sentences containing ambiguous surface realizations, along with orthographic representations which convinced some that they were listening to disyllabic words ('support' etc.) and others that they were listening to monosyllabic ones ('sport' etc.). Asking listeners to judge the tempo of the sentences allowed us to assess whether the difference in imposed lexical interpretation had an impact on perceived tempo. Our results reveal the predicted effect of the imposed interpretation: sentences with a \"disyllabic\" interpretation for the ambiguous word form were judged faster than (the same) sentences with a \"monosyllabic\" interpretation."
   ],
   "doi": "10.21437/SpeechProsody.2022-96"
  },
  "silbervarod22_speechprosody": {
   "authors": [
    [
     "Vered",
     "Silber-Varod"
    ],
    [
     "Ella",
     "Alfon"
    ],
    [
     "Noam",
     "Amir"
    ]
   ],
   "title": "Perception of the strength of prosodic breaks in three conditions: Explicit pause, implicit pause, and no pause",
   "original": "51",
   "page_count": 5,
   "order": 100,
   "p1": 475,
   "pn": 479,
   "abstract": [
    "In this study we examine the perceptual strength of prosodic boundaries in Hebrew speech. The stimuli consisted of 28 sequences of two inter-pausal units (IPUs) taken from the Map Task recordings in Hebrew. Listeners were exposed only to the silent pause following the first IPU (hence, Explicit pauses) while the second pause was omitted (hence, Implicit pauses) thus creating a stimulus model of IPU-pause-IPU. Ten female listeners labeled the strength of each break between adjacent words on a scale from 1 (no break) to 5 (strong break). Higher average scores were assigned to the implicit pauses as compared to the explicit ones, however scores for explicit pauses received higher agreement between raters. Moreover, we found only borderline significant influence of the explicit pause duration on the raters' scores. Looking at gender differences, the results suggest that raters' scores were higher when the speakers were females. Further, an interaction was found between the gender of the speaker and the gender of the recipient (i.e., the interlocutor). In particular, female speakers received a higher score overall, and for male speakers the rating was higher when they spoke to males than to females."
   ],
   "doi": "10.21437/SpeechProsody.2022-97"
  },
  "wlodarczak22_speechprosody": {
   "authors": [
    [
     "Marcin",
     "Wlodarczak"
    ],
    [
     "Mattias",
     "Heldne"
    ]
   ],
   "title": "Contribution of voice quality to prediction of turn-taking events",
   "original": "48",
   "page_count": 5,
   "order": 102,
   "p1": 485,
   "pn": 489,
   "abstract": [
    "This paper evaluates the contribution of acoustic voice quality measures to prediction of upcoming floor change and retention. In order to minimize the influence of vocal tract resonances, the measures were calculated from miniature accelerometers attached to the tracheal wall. Overall, speaker changes accompanied by silence were characterized by lower periodicity and steeper spectral slope than turn-holds and speaker changes involving overlapping speech. When used on their own, voice quality features contributed to prediction of turn-taking category, this was particularly true of smoothed cepstral peak prominence (CPPS). At the same time, their importance was limited when used in combination with fundamental frequency and intensity, especially compared to the joint effect of these two predictors.\n"
   ],
   "doi": "10.21437/SpeechProsody.2022-99"
  },
  "passetti22_speechprosody": {
   "authors": [
    [
     "Renata R.",
     "Passetti"
    ],
    [
     "Sandra",
     "Madureira"
    ],
    [
     "Plínio A.",
     "Barbosa"
    ]
   ],
   "title": "Voice perception on a voice messaging app: implications for Forensic Phonetics",
   "original": "66",
   "page_count": 5,
   "order": 103,
   "p1": 490,
   "pn": 494,
   "abstract": [
    "This study aims to analyze the effects of a voice messaging app transmission on voice quality and voice dynamics perception and to discuss how they can impact forensic analysis. The study design comprised a perceptual experiment on pairs of stimuli from 10 Brazilian speakers recorded directly by a digital recorder and over WhatsApp voice messages. A group of four voice-specialized judges has assessed the stimuli set by means of the Vocal Profile Analysis (VPA), which comprises both voice quality and voice dynamics settings. Data analysis has included reliability measures, and the spatial arrangement of the stimuli pairs through a multidimensional scaling technique (MDS). The settings mainly correlated to MDS dimensions were “Lowered Larynx”, “Tense Larynx” and “Creaky” as well as “Pitch Mean” and “Pitch Extensive Range”. The voice quality settings were less affected by the recording condition, since the perceptual distances between stimuli pairs in this group were lower than those related to voice dynamics. However, the attested perceptual differences are not limited to stimuli acoustic quality only since none of the MDS perceptual dimensions could separate the recording conditions. Implications for Forensic Phonetics are considered.\n"
   ],
   "doi": "10.21437/SpeechProsody.2022-100"
  },
  "mixdorff22b_speechprosody": {
   "authors": [
    [
     "Hansjörg",
     "Mixdorff"
    ],
    [
     "Oliver",
     "Niebuhr"
    ]
   ],
   "title": "The Effects of Fujisaki Model Parameter Manipulation on Perceived Charisma",
   "original": "127",
   "page_count": 5,
   "order": 104,
   "p1": 495,
   "pn": 499,
   "abstract": [
    "In an earlier exploratory study we examined the prosody of speeches by two IT industry leaders, Steve Jobs and Marc Zuckerberg, whose perceived charisma differs greatly, with Jobs usually regarded as the much more captivating speaker. This previous study focused mainly on fundamental frequency contours as well as on the perceived local speech rate. Instead of analyzing the raw F0 data directly, we modeled the F0 contours using the Fujisaki model and examined the differ-ences in the respective model components. Whereas in our comparison between Jobs and Zuckerberg we were only able to examine distributions of Fujisaki model parameters, in the current study we decided to systematically vary some of the Fujisaki model parameters on a fixed set of utterances and investigate their effects on perceived charisma. We found that in general pitch range extensions are beneficial, especially when connected to accented syllables, but also that effects differ considerably between male and female speakers.\n"
   ],
   "doi": "10.21437/SpeechProsody.2022-101"
  },
  "roessig22_speechprosody": {
   "authors": [
    [
     "Simon",
     "Roessig"
    ],
    [
     "Lena",
     "Pagel"
    ],
    [
     "Doris",
     "Mücke"
    ]
   ],
   "title": "Speaking loudly reduces flexibility and variability in the prosodic marking of focus types",
   "original": "75",
   "page_count": 5,
   "order": 105,
   "p1": 500,
   "pn": 504,
   "abstract": [
    "Modulations of F0 height and movement magnitude are used to mark focus and differentiate between focus types. Similar F0 changes have been described for loud speech. In this production study, we investigate the interplay of speaking style (habitual vs. loud speech) and focus type (broad vs. contrastive focus) by analyzing characteristics of nuclear pitch accents in German. Our study reveals that the prosodic system becomes less flexible in the differentiation of focus types in loud speech. While nuclear accents of falling and rising types occur in habitual speech, there are only rising accents in loud speech. In loud speech, the differentiation between broad and contrastive focus may be maintained by a gradual difference in the magnitude of the pitch accent rise excursions, but this difference is weaker than in habitual speech. Interestingly, the speaking styles are characterized by different variability profiles within and across speakers: In loud speech, the productions across speakers become more similar and speakers tend to be more consistent in their individual patterns. We discuss the findings in the light of physiological explanations and suggest that they may exemplify how communicative demands and production constraints interact in shaping prosodic patterns.\n"
   ],
   "doi": "10.21437/SpeechProsody.2022-102"
  },
  "volin22_speechprosody": {
   "authors": [
    [
     "Jan",
     "Volín"
    ],
    [
     "Radek",
     "Skarnitzl"
    ]
   ],
   "title": "The Impact of Prosodic Position on Post-Stress Rise in Three Genres of Czech",
   "original": "111",
   "page_count": 5,
   "order": 106,
   "p1": 505,
   "pn": 509,
   "abstract": [
    "In general phonetics, stressed syllables are described as more prominent due to greater duration, higher intensity, less steep spectral slope and/or higher fundamental frequency. However, there are languages in which lexical stress commonly manifests with a post-stress rise (L*+H). Previous studies dedicated to post-stress rises in Czech were limited in material and/or methodology. Our current study extends the material to sizeable samples of three genres of speech: professional story-telling, poetry reciting and news reading. Over 30,000 syllables were manually labelled in terms of their accent-group status. The main focus of the study was the step between the stressed and the following syllable, but apart from the frequency of occurrence and size of the step, we also examined the influence of the position within a prosodic phrase. The results suggest that the post-stress rise should be considered a typical pitch accent in Czech, but that it does not occur uniformly across the examined genres and prosodic positions.\n"
   ],
   "doi": "10.21437/SpeechProsody.2022-103"
  },
  "petrone22_speechprosody": {
   "authors": [
    [
     "Caterina",
     "Petrone"
    ],
    [
     "Arina",
     "Antonenko"
    ],
    [
     "Sophie",
     "Dufour"
    ]
   ],
   "title": "Does emotional prosody affect word recognition in French?",
   "original": "23",
   "page_count": 5,
   "order": 107,
   "p1": 510,
   "pn": 514,
   "abstract": [
    "In the word-processing literature, there is extensive evidence that the semantic context facilitates word recognition. However, much less is known about the potential role of contextual prosodic information. We test whether, in French, semantically neutral sentences (e.g., \"She is taking the train\") uttered with emotional prosody (angry, happy) facilitate the recognition of negative emotional words (e.g., war). The sentences and the words were semantically unrelated. Two studies were run. The first study employs a unimodal priming paradigm, in which both the sentences and the target words were presented auditorily. Target words were uttered in a neutral prosody. Data on forty-five listeners showed no priming effects, possibly because of an incongruency between the neutral prosody on the target words and their emotional valence. The second experiment is a preliminary study on fifteen listeners employing an auditory-visual priming paradigm. Here, we found an affective priming effect: Negative words that were more strongly associated with anger were more easily recognized when preceded by angry than by happy prosody. Hence, prosodic context influences word processing depending upon whether prosody and words share the same emotion category, not just the same valence.\n"
   ],
   "doi": "10.21437/SpeechProsody.2022-104"
  },
  "barbosa22_speechprosody": {
   "authors": [
    [
     "Plinio",
     "Barbosa"
    ]
   ],
   "title": "The Acoustics of Pleasantness in Poetry Declamation in Two Varieties of Portuguese",
   "original": "3",
   "page_count": 5,
   "order": 108,
   "p1": 515,
   "pn": 519,
   "abstract": [
    "This work investigates the link between a poem declamation and the sensation of pleasantness in two varieties of Portuguese: European and Brazilian Portuguese. A group of ten Brazilian and ten Portuguese reciters had their performances evaluated by a group of ten Brazilian and ten Portuguese listeners invited to judge the degree of pleasantness. The perceptual evaluation was done in two ways: first, the listeners of the respective linguistic community carried out a Likert-scale test to attribute a degree from “very unpleasant” to “very pleasant” in five steps for the declamations of Alberto Caeiro’s poem “Quando vier a primavera”. Then, the listeners of the two varieties together carried out a discriminant test to point which one of two declamations of the same chunk from the poem was more pleasant. Both reciters and listeners were balanced in gender. A set of 22 prosodic-acoustic parameters was extracted from the verses and taken as predictors in logistic regression and Linear Discriminant Analysis models for explaining the perceptual evaluations. Results point to pause duration and rate, articulation rate, rate of F0 falls, spectral emphasis and LTAS slope as the most relevant predictors of pleasantness. Implications for an acoustic of pleasantness are discussed."
   ],
   "doi": "10.21437/SpeechProsody.2022-105"
  },
  "matsuda22_speechprosody": {
   "authors": [
    [
     "Takuto",
     "Matsuda"
    ],
    [
     "Yoshiko",
     "Arimoto"
    ]
   ],
   "title": "Acoustic discriminability of unconscious laughter and scream during game-play",
   "original": "37",
   "page_count": 5,
   "order": 120,
   "p1": 575,
   "pn": 579,
   "abstract": [
    "For the growing demand to make social signals available to various systems, such as laughter for clinical treatment or emergent screams for security reasons, it is essential to detect those signals as appropriate social functions. This study demonstrates the acoustic discriminability of laughter and scream by conducting three machine-learning-based classification experiments and features selection experiments based on logistic regression analysis, using classical acoustic features. The result of the speaker-and-corpus-closed experiment revealed that the models acoustically discriminate laughter from screams by yielding high accuracies at 93.52% (DNN) and 95.54% (SVM). Moreover, the result of the leave-four-speaker-out cross-validation (LFOCV) revealed that our model can correctly classify laughter and screams regardless of the speakers by exhibiting only approximately 1% lower accuracies than the result of the speaker-closed model. The results of the corpus-open experiment exhibited approximately 5% and 7.8% lower accuracies for the DNN and SVM models, respectively, than those of the corpus-closed experiment. However, our model can still classify laughter and screams in the different recording conditions at approximately 88% accuracy. Finally, the result of logistic regression analysis showed that the harmonics-to-noise ratio was the most contributed acoustic feature to discriminate laughter from screams."
   ],
   "doi": "10.21437/SpeechProsody.2022-117"
  },
  "li22b_speechprosody": {
   "authors": [
    [
     "Aini",
     "Li"
    ],
    [
     "Wei",
     "Lai"
    ],
    [
     "Jianjing",
     "Kuang"
    ]
   ],
   "title": "How do listeners identify creak? The effects of pitch range, prosodic position and creak locality in Mandarin",
   "original": "133",
   "page_count": 5,
   "order": 101,
   "p1": 480,
   "pn": 484,
   "abstract": [
    "As a non-modal phonation, creak has been found to influence the perception of pitch range and prosodic boundary. However, few studies have examined how these factors could in turn affect listeners’ perception of creak (e.g., Davidson 2019). This study examines the effects of pitch range, prosodic position, and creak locality on creak identification in Mandarin. 40 native Mandarin listeners listened to 128 auditory sentences online and identified whether and where (i.e., at which syllables) they heard creak. Sentences carrying target syllables were manipulated in terms of pitch range (high vs. low), the prosodic position of creak (final vs. non-final), and creak locality (global vs. local). Mixed-effects logistic regression was implemented to predict listeners' identification responses, with Creak locality, Pitch range, and Prosodic position as fixed effects and Item, Participant and Tone as random intercepts. The results revealed lower accuracy of creak identification at sentence-final positions than non-final positions. While low pitch facilitated creak identification, it also increased false-alarmed creak identification of modal speech. Global creak in the sentential context led to higher creak identification than local creak at specific syllables. These findings imply that creak perception is context dependent and reflects listeners’ knowledge about its acoustic and linguistic distributions.\n"
   ],
   "doi": "10.21437/SpeechProsody.2022-98"
  },
  "meireles22_speechprosody": {
   "authors": [
    [
     "Alexsandro Rodrigues",
     "Meireles"
    ],
    [
     "Hansjörg",
     "Mixdorff"
    ]
   ],
   "title": "Acoustic Study of the Voice Quality of Brazilian Portuguese Stressed Vowels",
   "original": "55",
   "page_count": 5,
   "order": 113,
   "p1": 540,
   "pn": 544,
   "abstract": [
    "This paper compares the voice quality of Brazilian Portuguese stressed oral vowels in a reading task. Four native speakers of Brazilian Portuguese participated in the experiment. Thirteen voice quality parameters were automatically extracted using the VoiceSauce software.\n",
    "The motivation for the study is the lack of a full study that analyzes the influence of the vowel quality and the subject in voice quality parameters related to the source spectrum, and, therefore, the absence of detailed information on how to interpret and differentiate the different laryngeal voice qualities influenced by vowel quality.\n",
    "Our results have shown that: a) low vowels have lower f0 values than mid and high vowels; b) high vowels have higher cepstral peak prominence (CPP) than mid and high vowels; c) harmonics-to-noise ratio from 0 to 500 Hz (HNR5) is higher in low vowels, intermediate in mid vowels, and smaller in high vowels. Moreover, the parameters H1*, H1*H2*, H1*A3, HNR15, HNR25, HNR35, and Energy did not show a clear pattern of distinction that could be correlated with vowel height, suggesting that we should control f0 and intensity, so as to find a robust acoustic pattern for differentiating voice quality among vowels."
   ],
   "doi": "10.21437/SpeechProsody.2022-110"
  },
  "huang22_speechprosody": {
   "authors": [
    [
     "Yaqian",
     "Huang"
    ]
   ],
   "title": "Articulatory properties of period-doubled voice in Mandarin",
   "original": "254",
   "page_count": 5,
   "order": 114,
   "p1": 545,
   "pn": 549,
   "abstract": [
    "Period-doubled phonation is a type of creaky voice that contains two alternating periods. By presenting data from Mandarin Chinese read speech recordings, this study probes the articulatory properties of period-doubled phonation and its tonal distribution based on time-domain measures using electroglottography (EGG). Period doubling (PD) was found across all the tones (T3: 43% > T2 > T4 > T1: 11%), which was more prevalent than vocal fry, found mainly in T3 (48%) and T2 (43%), and only sporadically in T4 (7%) and T1 (2%). We calculated the two alternating glottal periods in PD, and they exhibited a ratio close to 3:2 or 2:1. The two pulses also alternated between higher and lower amplitudes with a mean ratio of 2 or 1.6. Women tended to produce more PD than men. Moreover, the contact quotient of PD, measured via EGG using the hybrid method, was around 0.5, similar to modal voice (0.54) and smaller than that of vocal fry (0.74), implying a more balanced opening and contact phase during phonation. Alternation of contact quotient and symmetry quotient was also seen in a few samples, suggesting that PD is likely articulated through two alternating pulses with distinct voice qualities and pitches.\n"
   ],
   "doi": "10.21437/SpeechProsody.2022-111"
  },
  "li22c_speechprosody": {
   "authors": [
    [
     "Xinyue",
     "Li"
    ],
    [
     "Carlos Toshinori",
     "Ishi"
    ],
    [
     "Changzeng",
     "Fu"
    ],
    [
     "Ryoko",
     "Hayashi"
    ]
   ],
   "title": "Prosodic and Voice Quality Analyses of Filled Pauses in Japanese Spontaneous Conversation by Chinese learners and Japanese Native Speakers",
   "original": "96",
   "page_count": 5,
   "order": 115,
   "p1": 550,
   "pn": 554,
   "abstract": [
    "\tThe present study documents (1) how Japanese native speakers and L1-Chinese learners of L2 Japanese differ in the production of filled pauses during spontaneous conversations, and (2) how the vowels of filled pauses and ordinary lexical items differ in spontaneous conversation.\nProsodic and voice quality measurements were extracted from vowels in filled pauses and ordinary lexical items produced by Japanese native speakers and Chinese learners of L2 Japanese. Statistical results revealed that there are significant differences in prosodic and voice quality measurements including duration, F0mean, intensity, spectral tilt-related indices, jitter and shimmer, (1) between Japanese native speakers and Chinese learners of L2 Japanese, as well as (2) between filled pauses and ordinary lexical items. In addition, random forest analysis was conducted to examine how much the measurements contribute to the classification of filled pauses and ordinary lexical items. Results indicate that duration and intensity play the most significant role, while voice quality related features make a secondary contribution to the classification. Results also suggest that the filled pause production patterns of Chinese learners of L2 Japanese are influenced by L1 background."
   ],
   "doi": "10.21437/SpeechProsody.2022-112"
  },
  "cambara22_speechprosody": {
   "authors": [
    [
     "Guillermo",
     "Cámbara"
    ],
    [
     "Mireia",
     "Farrús"
    ],
    [
     "Jordi",
     "Luque"
    ]
   ],
   "title": "Voice Quality and Pitch Features in Transformer-Based Speech Recognition",
   "original": "10",
   "page_count": 5,
   "order": 116,
   "p1": 555,
   "pn": 559,
   "abstract": [
    "Jitter and shimmer measurements have shown to be carriers of voice quality and prosodic information which enhance the performance of tasks like speaker recognition, diarization or automatic speech recognition (ASR). However, such features have been seldom used in the context of neural-based ASR, where spectral features often prevail. In this work, we study the effects of incorporating voice quality and pitch features altogether and separately to a Transformer-based ASR model, with the intuition that the attention mechanisms might exploit latent prosodic traits. For doing so, we propose separated convolutional front-ends for prosodic and spectral features, showing that this architectural choice yields better results than simple concatenation of such pitch and voice quality features to mel-spectrogram filterbanks. Furthermore, we find mean Word Error Rate relative reductions of up to 5.6% with the LibriSpeech benchmark. Such findings motivate further research on the application of prosody knowledge for increasing the robustness of Transformer-based ASR.\n"
   ],
   "doi": "10.21437/SpeechProsody.2022-113"
  },
  "ludusan22_speechprosody": {
   "authors": [
    [
     "Bogdan",
     "Ludusan"
    ],
    [
     "Petra",
     "Wagner"
    ]
   ],
   "title": "ha-HA-hha? Intensity and voice quality characteristics of laughter",
   "original": "170",
   "page_count": 5,
   "order": 117,
   "p1": 560,
   "pn": 564,
   "abstract": [
    "Laughter is one of the most widely-encountered paralinguistic phenomena in human interaction and it has been studied from different perspectives throughout the years, including its acoustic-prosodic realization. However, previous studies have mostly focused on fundamental frequency and duration measures, with other prosodic features being less studied. We examine here the acoustic marking of laughter in terms of intensity and voice quality characteristics, by using a corpus of spontaneous conversations. We operationalized the two cues by means of the root-mean-square energy and the cepstral peak prominence, respectively. Examining laughs, speech-laughs and speech instances at two different levels (that of the entire event and at the syllable nucleus level) we observed the least regular phonation for laughs and the most regular one for speech, while intensity was the highest for speech-laughs, followed by laughter and the lowest for speech. Using mixed effect models we determined that all three vocalization classes differ significantly from one another, in terms of both acoustic cues. Moreover, an interesting effect of syllable position was seen for laughter, with phonation becoming more regular for later syllables."
   ],
   "doi": "10.21437/SpeechProsody.2022-114"
  },
  "crochiquia22_speechprosody": {
   "authors": [
    [
     "Alice",
     "Crochiquia"
    ],
    [
     "Anders",
     "Eriksson"
    ],
    [
     "Plinio",
     "Barbosa"
    ],
    [
     "Sandra",
     "Madureira"
    ]
   ],
   "title": "A perceptual and acoustic study of dubbed voices in an animated film",
   "original": "193",
   "page_count": 5,
   "order": 118,
   "p1": 565,
   "pn": 569,
   "abstract": [
    "Listeners rely on speech vocal cues to judge speakers’ age, size, personality, and other paralinguistic and extralinguistic features. These judgements are often based on vocal stereotypes which may be universally or culturally determined. This study examines how physical, psychological, social, and vocal features are perceived by listeners and which acoustic features may influence their judgements. An experiment integrating a perceptual test and acoustic measurements was performed. The corpus consisted of speech utterances produced by five animated film characters, dubbed in Brazilian Portuguese. The stimuli were judged by 77 Brazilian Portuguese native speakers, 46 women and 31 men, aged 20 to 50. The acoustic analysis was performed automatically. Acoustic measures included meanf0, baseline, spectral emphasis and H1-H2. For inter-rater agreement analysis, Cronbach's Alpha was chosen. The results indicated close agreements among judges for all characters. Overall scores obtained for all characters were above .9. In interpreting the results, the influence sound symbolism codes may have on listeners’ judgments and the factors influencing vocal stereotypes have been considered. The discussion of the acoustic and perceptual analysis results takes into consideration if dubbers adapt their voices to fit the characters or otherwise are cast because of their natural voice characteristics.\n"
   ],
   "doi": "10.21437/SpeechProsody.2022-115"
  },
  "ge22_speechprosody": {
   "authors": [
    [
     "Chunyu",
     "Ge"
    ],
    [
     "Wenwei",
     "Xu"
    ],
    [
     "Wentao",
     "Gu"
    ],
    [
     "Peggy",
     "Mok"
    ]
   ],
   "title": "An electroglottographic study of phonation types in tones of Suzhou Wu Chinese",
   "original": "137",
   "page_count": 5,
   "order": 119,
   "p1": 570,
   "pn": 574,
   "abstract": [
    "Tonal systems often involve cues other than F0, such as phonation types. This paper investigated the phonation distinction of tones in Suzhou Wu Chinese. Simultaneous electroglottographic (EGG) and audio data of isolated syllables were collected from ten speakers aged above 65. Closed quotient (CQ) and peak increase contact (PIC) were measured on the EGG signals. Generalized Additive Mixed Models (GAMM) were conducted to analyze the time course of CQ and PIC, with tone and speaker’s gender as smoothing terms. CQ and PIC were lower for low register than high register tones, with smaller differences in female than male speakers. The time courses of CQ and PIC were also varied with genders. The correlations between EGG and acoustic measurements were also calculated. H1*-H2* and H1*-A1* were more strongly correlated with CQ, whereas the correlations between them and PIC were weak. This paper showed that low register tones in Suzhou Wu were pronounced with breathy voice, which was more prominent at the onset of vowel, while the degree of breathiness and its time course differed between females and males. The EGG measurements and their correlations with acoustic measurements provide evidence to explicate the evolution of tones in Suzhou Wu.\n"
   ],
   "doi": "10.21437/SpeechProsody.2022-116"
  },
  "niebuhr22_speechprosody": {
   "authors": [
    [
     "Oliver",
     "Niebuhr"
    ]
   ],
   "title": "Prosody in hate speech perception: A step towards understanding the role of implicit prosody",
   "original": "168",
   "page_count": 5,
   "order": 109,
   "p1": 520,
   "pn": 524,
   "abstract": [
    "When speaking out hate-speech posts aloud, the prosody that speakers use can drastically change how listeners rate these posts in terms of personal (un)acceptability and consequences for the originator. But, does this also apply to the silent (implicit) prosody that we hear in our head while reading a post? Paving the way to answering this question, the present paper investigated how readers' explicit prosody is connected to their hate-speech ratings. Results provide evidence for this connection and, moreover, show that migration background plays a role for both the prosody and evaluation of hate-speech posts.\n"
   ],
   "doi": "10.21437/SpeechProsody.2022-106"
  },
  "chen22_speechprosody": {
   "authors": [
    [
     "Yu",
     "Chen"
    ],
    [
     "Ting",
     "Wang"
    ],
    [
     "Hongwei",
     "Ding"
    ]
   ],
   "title": "Effect of Age and Gender on Categorical Vocal Emotion Recognition in Mandarin Chinese",
   "original": "167",
   "page_count": 5,
   "order": 121,
   "p1": 580,
   "pn": 584,
   "abstract": [
    "Categorical perception (CP) effect, as a fundamental feature of perception which allows humans to rapidly and properly respond to sensory cues, has received accumulating evidences from various perceptual experiments, such as musical tone perception and facial emotion identification. The current study attempted to detect the potential CP effect of vocal emotion expression in Mandarin Chinese and further explored the effect of age and gender on emotional prosody categorization. Three emotional prosody continua (happiness-sadness, neutral-happiness, neutral-sadness) were created using speech synthesis. Each continuum consists of 11 emotional speeches with equal-sized physical differences. Two groups of subjects (children and adults) were instructed to finish both the identification tasks and discrimination tasks. Results indicated that adults showed significantly narrower boundary width compared to younger children, and women’s boundary width was also significantly narrower than men’s. Moreover, the emotion categories of children group did not coincide with those of adults. These findings provide novel evidence for categorical emotion recognition from voice and also illuminate the effect of age and gender on prosodic perception of emotional speech. Particularly, the current research indicates that children’s capacity to decode emotions follows a slow course of development before changing into adults’ emotional category pattern.\n"
   ],
   "doi": "10.21437/SpeechProsody.2022-118"
  },
  "xia22_speechprosody": {
   "authors": [
    [
     "Sylvain",
     "Xia"
    ],
    [
     "Dominique",
     "Fourer"
    ],
    [
     "Liliana",
     "Audin-Garcia"
    ],
    [
     "Jean-Luc",
     "Rouas"
    ],
    [
     "Takaaki",
     "Shochi"
    ]
   ],
   "title": "Speech Emotion Recognition using Time-frequency Random Circular Shift and Deep Neural Networks",
   "original": "7",
   "page_count": 5,
   "order": 122,
   "p1": 585,
   "pn": 589,
   "abstract": [
    "This paper addresses the problem of emotion recognition from a speech signal. Thus, we investigate a data augmentation technique based on circular shift of the input time-frequency representation which significantly enhances the emotion prediction results using a deep convolutional neural network method. After an investigation of the best combination of the method parameters, we comparatively assess several neural network architectures (Alexnet, Resnet and Inception) using our approach applied on two publicly available datasets: eNTERFACE05 and EMO-DB. Our results reveal an improvement of the prediction accuracy in comparison to a more complicated technique of the state of the art based on Discriminant Temporal Pyramid Matching (DCNN-DTPM).\n"
   ],
   "doi": "10.21437/SpeechProsody.2022-119"
  },
  "mady22_speechprosody": {
   "authors": [
    [
     "Katalin",
     "Mády"
    ],
    [
     "Beáta",
     "Gyuris"
    ],
    [
     "Hans-Martin",
     "Gärtner"
    ],
    [
     "Anna",
     "Kohári"
    ],
    [
     "Ádám",
     "Szalontai"
    ],
    [
     "Uwe D.",
     "Reichel"
    ]
   ],
   "title": "Perceived emotions in infant-directed narrative across time and speech acts",
   "original": "182",
   "page_count": 5,
   "order": 123,
   "p1": 590,
   "pn": 594,
   "abstract": [
    "One important function of infant-directed speech (IDS) is to express positive emotions towards the baby. This has been shown based on prosodic parameters before, but parameters such as f0 and energy encode emotion expression only indirectly. In this study, we aim to access emotion expression (arousal and valence) in IDS directly, through labellers' perception. Recordings were made in the first 18 months of the baby at four different time points. The sentences and the contexts were fixed. Our questions were the following: (1) Does emotion expression in IDS and adult-directed speech (ADS) differ in narratives when sentences and contexts do not vary? (2) Do the strength and polarity of emotions change over time in mothers' speech up to 18 months of the baby? (3) Do observed differences pattern similarly in various uses of speech acts? Both arousal and valence scores were higher in IDS. No changes in IDS were observed during the first 18 months. Requests received higher arousal and valence scores than exclamations, but in IDS, they only differed by valence. This means that these speech acts are only held apart consistently by valence in the two registers, not by arousal.\n"
   ],
   "doi": "10.21437/SpeechProsody.2022-120"
  },
  "erickson22_speechprosody": {
   "authors": [
    [
     "Donna",
     "Erickson"
    ],
    [
     "Albert",
     "Rilliard"
    ],
    [
     "Ela",
     "Thurgood"
    ],
    [
     "João Antônio de",
     "Moraes"
    ],
    [
     "Takaaki",
     "Shochi"
    ]
   ],
   "title": "A Valence-Arousal-Dominance Study of American English Social Affective Expressions",
   "original": "19",
   "page_count": 5,
   "order": 124,
   "p1": 595,
   "pn": 599,
   "abstract": [
    "This study examines acoustic and perceptual characteristics of six American English social affects, authority, declaration, irritation, sincerity, uncertainty and walking-on-eggs, produced by two females, all spoken on the same linguistic sentence, Mary was dancing. Fourteen listeners rated the expressions in terms of the Valence, Arousal, and Dominance emotional dimensions (VAD). Main acoustical dimensions show that the two speakers use different vocal strategies. While arousal was linked in both speakers to raised intensity (and linked to irritation), this is reached by increased normalized amplitude quotient (NAQ) in the first speaker (potentially linked to her vocal folds amplitude of vibration); NAQ and intensity also play the main roles in her ratings of Valence, possibly linked to Extroversion in her case. Her dominant expression was authority, related to lower HNR and F0 (creaky voice), a pattern predicted by the Frequency code. For the other speaker, F0 and intensity played the principal roles in ratings of Dominance and Arousal. Her dominant (and aroused) expression was irritation characterized by high intensity and F0, a strategy linked to the Effort code, that predicts large pitch span, produced by more effort / subglottal pressure, to an interpretation of larger size–and thus dominant behavior.\n"
   ],
   "doi": "10.21437/SpeechProsody.2022-121"
  },
  "he22_speechprosody": {
   "authors": [
    [
     "Jiayong",
     "He"
    ],
    [
     "Jing",
     "Tang"
    ],
    [
     "Stella",
     "Gryllia"
    ],
    [
     "Aoju",
     "Chen"
    ]
   ],
   "title": "Prosodic realization of politeness in the presence of non-prosodic cues in Mandarin Chinese",
   "original": "97",
   "page_count": 5,
   "order": 125,
   "p1": 600,
   "pn": 604,
   "abstract": [
    "Recent years have seen a growing interest in the prosodic realization of politeness. But little is still known on the interplay between prosody and non-prosodic cues to politeness, such as the use of a hesitation marker and choice of utterance types. This study examines whether prosody is used differently in the presence of the hesitation marker en ‘uhm’ and whether there is a functional trade-off between prosody and utterance types in Mandarin Chinese polite speech. Neutral and polite speech was elicited from 22 native speakers of Mandarin Chinese via a dialogue game in imperatives, yes/no questions and statements. Speech rate, mean pitch, pitch span and voice quality were measured at the sentence level. We found that both mean pitch and pitch span were raised in polite speech, contra previous evidence for the use of mean pitch only in the absence of a hesitation marker. Interestingly, the male speakers raised mean pitch and pitch span more than the female speakers in polite speech, whereas the female speakers used breathier voice more. Finally, we found utterance types didn’t interact with pitch or voice quality, but interacted with speech rate, contra the hypothesis of a functional trade-off.\n"
   ],
   "doi": "10.21437/SpeechProsody.2022-122"
  },
  "marty22_speechprosody": {
   "authors": [
    [
     "Emilie",
     "Marty"
    ],
    [
     "Roxane",
     "Bertrand"
    ],
    [
     "Caterina",
     "Petrone"
    ],
    [
     "James",
     "German"
    ]
   ],
   "title": "Prosodic Correlates of Discourse Structure and Emotion in Discourse Markers that Preface Announcements of News",
   "original": "267",
   "page_count": 5,
   "order": 126,
   "p1": 605,
   "pn": 609,
   "abstract": [
    "Discourse markers can serve important structuring functions such as concluding a contribution or resuming a previous topic. We address whether, along with their role in structuring discourse, discourse markers carry prosodic cues to the emotional valence of upcoming news, perhaps to prepare the listener’s emotional reaction. Specifically, we explore the realization of “voilà donc” when occurring between an announcement of news and its preface: “Je vous appelle au sujet de votre chat qui était malade, voilà donc il est désormais guéri” (“I’m calling about your cat that was sick, yeah so he’s now cured”). For this, we recorded speakers while they read announcements in the form of voicemail messages including an instance of voilà donc, and which varied in the valence of the announcement. Our results showed that while phonologically, intonation patterns were in line with previous findings regarding the functions of voilà donc, valence did not influence the choice of pattern. Valence was, however, associated with phonetic variation in that high targets were higher for positive and neutral valence and pitch range was larger for positive valence. This finding suggests that phonetic variation projects the emotional valence of upcoming news even though discourse function primarily determines the phonological pattern.\n"
   ],
   "doi": "10.21437/SpeechProsody.2022-123"
  },
  "levitan22_speechprosody": {
   "authors": [
    [
     "Sarah Ita",
     "Levitan"
    ],
    [
     "Julia",
     "Hirschberg"
    ]
   ],
   "title": "Believe It or Not: Acoustic-Prosodic Cues to Trusting and Untrusting Speech in Interview Dialogues",
   "original": "255",
   "page_count": 5,
   "order": 127,
   "p1": 610,
   "pn": 614,
   "abstract": [
    "\tTrust is a fundamental component of human-human and human-computer interaction. In this work we examine the acoustic-prosodic features of trust in a corpus of interview dialogues. While previous studies have explored the characteristics of speech that is trusted or mistrusted by others, we study a complementary problem: what are the characteristics of trusting vs. untrusting speech? That is, are there specific acoustic-prosodic cues in an interviewer’s speech that indicate whether the interviewer believes their interlocutor, or whether they are skeptical?\nWe use a corpus of deceptive and truthful interview dialogues, where trust labels are explicitly provided by the interviewer for every question asked. We analyze acoustic-prosodic features extracted from interviewer turns and compare the features of trusting and untrusting speech and identify several significant differences in features. Further, we compare the features of trusting speech in our study of human-human dialogue, with previous findings from a study of trusting speech in human-computer dialogue. This work sheds light on the nature of trusting speech, and how it manifests itself when humans communicate with human vs. machine interlocutors."
   ],
   "doi": "10.21437/SpeechProsody.2022-124"
  },
  "alvarez22_speechprosody": {
   "authors": [
    [
     "Aitor Arronte",
     "Alvarez"
    ],
    [
     "Elsayed",
     "Issa"
    ],
    [
     "Mohammed",
     "Alshakhori"
    ]
   ],
   "title": "Computational modeling of intonation patterns in Arabic emotional speech",
   "original": "134",
   "page_count": 5,
   "order": 128,
   "p1": 615,
   "pn": 619,
   "abstract": [
    "The expression of emotion in speech communication has been frequently studied from the analysis of F0 contours. Global features such as the mean level and range of F0 as well as the slope of the contour, have been related to the degree of activation of an emotion. However, the existence of specific patterns associated with basic human emotions has not been empirically demonstrated, and studies generally find no conclusive answer to this question. In this paper we present a computational study of emotional speech in Arabic. A computational method for obtaining tonal contours based on F0 peak-to-peak and valley-to-valley distances is presented that is able to capture tonal rhythm (macro-rhythm). We introduce a model for extracting frequent tonal patterns to characterize emotions based on the typology of the patterns. Comparative analysis between sets of neutral and emotional utterances shows that distinctive patterns exist for anger, sadness, happiness, and surprise, that are completely absent in neutral speech. Findings highlight the effectiveness of the comparative computational methodology presented to discover macro-rhythmic emotion-specific patterns."
   ],
   "doi": "10.21437/SpeechProsody.2022-125"
  },
  "verheul22_speechprosody": {
   "authors": [
    [
     "Suzanne",
     "Verheul"
    ],
    [
     "Adriana",
     "Hartman"
    ],
    [
     "Roselinde",
     "Supheert"
    ],
    [
     "Aoju",
     "Chen"
    ]
   ],
   "title": "Gender effects on perception of emotional speech- and visual-prosody in a second language: Emotion recognition in English-speaking films",
   "original": "122",
   "page_count": 4,
   "order": 129,
   "p1": 620,
   "pn": 623,
   "abstract": [
    "Speakers use both speech prosody and visual prosody (facial expressions, gestures, body postures) to express emotion. Receivers register and recognise emotion via both types of prosodic cues. In this study, we examined gender differences in both recognition of type of emotion (e.g. anger vs. joy) and perceived emotionality (e.g. the degree of anger) expressed via speech prosody and visual prosody in a second language (L2). In a perception experiment using film scenes, proficient Dutch learners of English rated the emotionality of each protagonist and identified the specific type of emotion expressed by each protagonist in each scene in both the visual-only and audio-only modality. We have found no evidence for gender-related differences in perceived emotionality, possibly due to potential difficulty of participants in identifying with the protagonists portrayed in a different society. However, the female Dutch learners of English were more accurate in recognising type of emotion than the male Dutch learners of English from both speech prosody and visual prosody. These findings suggest that there is transfer of learners’ ability in recognising type of emotion in the native language to L2 and that female L2 learners may be better at learning cues in speech prosody to emotion in L2."
   ],
   "doi": "10.21437/SpeechProsody.2022-126"
  },
  "tschinse22_speechprosody": {
   "authors": [
    [
     "Lisa Maria",
     "Tschinse"
    ],
    [
     "Ali",
     "Asadi"
    ],
    [
     "Anna",
     "Gutnyk"
    ],
    [
     "Oliver",
     "Niebuhr"
    ]
   ],
   "title": "Keep on smiling...? An exploration of the gender-specific connections between smiling duration and perceived speaker attributes in business pitches",
   "original": "2",
   "page_count": 5,
   "order": 130,
   "p1": 624,
   "pn": 628,
   "abstract": [
    "Is there an overdose threshold for smiling? If so, is this threshold the same for women and men These questions are addressed in our line of research. The current paper presents the first pilot study of this line of research. It is based on field data from English native speakers. From the hundreds of English-language tutors on italki, we have selected 18, 9 men and 9 women, who represent three different groups of smilers based on visual annotation: non-smilers (smile time in video <5%), occasional smilers (smile time in video 30-50 %), and permanent smilers (smile time in video> 95%). In a perception experiment, we examined the effect of these three groups on listeners using audio-only stimuli (n = 40, within-subjects design). Our results show that smiling makes both male and female speakers more attractive and their voices more pleasant. However, with regard to charismatic, professional and persuasive attributes, it is the occasional smile that turns our best for men, whereas women perform best along these attributes if they do not smile at all. We discuss our findings with a view to (1) phonetic explanations, (2) practical advice and (3) implications for future studies."
   ],
   "doi": "10.21437/SpeechProsody.2022-127"
  },
  "wei22_speechprosody": {
   "authors": [
    [
     "Huan",
     "Wei"
    ],
    [
     "Yifei",
     "He"
    ],
    [
     "Christina",
     "Kauschke"
    ],
    [
     "Mathias",
     "Scharinger"
    ],
    [
     "Ulrike",
     "Domahs"
    ]
   ],
   "title": "An EEG-study on L2 categorization of emotional prosody in German",
   "original": "18",
   "page_count": 5,
   "order": 131,
   "p1": 629,
   "pn": 633,
   "abstract": [
    "Previous behavioral studies on the processing of emotional prosody in L2 learners showed similarities and differences between L1- and L2-processing and suggested that emotional perception has both universal and culture-specific aspects. However, little is known about the processing of emotional prosody in L2 learners' brains. Therefore, the present study used event-related potentials to compare the processing of emotional prosodies between German native speakers and Chinese L2 learners of German. Participants performed a prosody recognition task with semantically neutral German words recorded with emotional \"neutral\", \"like\", and \"disgust\" prosodies. The accuracy ratings of categorizing emotional prosodies of L2 learners were above chance but significantly better for the L1 speakers. Both groups yielded an early and a late positivity for processing \"like\" in comparison to \"disgust\", reflecting the emotional prosodic predictive processing. However, an early left anterior negativity (ELAN) and a late anterior negativity observed in the L2 learners suggest that they are more sensitive to acoustic differences of the presented stimuli. Overall, our findings support the assumption that the processing of emotional prosody is in principle universal across languages, but that in addition to the general mechanisms involved in the processing of emotional speech language-specific aspects also modify emotional processing."
   ],
   "doi": "10.21437/SpeechProsody.2022-128"
  },
  "holliday22_speechprosody": {
   "authors": [
    [
     "Nicole",
     "Holliday"
    ]
   ],
   "title": "Kamala Harris, Maya Rudolph and the Prosody of Parody",
   "original": "31",
   "page_count": 5,
   "order": 132,
   "p1": 634,
   "pn": 638,
   "abstract": [
    "Despite advances in the studies of both ethnolinguistic and prosodic variation, linguists still know relatively little about how individual speakers may use prosody to construct and perform aspects of their identity in dynamic ways. One novel way to study how individuals employ both personal and ethnolinguistic variation is to examine salient linguistic features that occur both in a natural context and in parodies of that same context. The current study analyzes speech from U.S. Vice President Kamala Harris and actor Maya Rudolph, who frequently parodies Harris on the American television program Saturday Night Live. Using a comparative analysis of data coded in the Autosegmental Metrical Phonology framework using MAE-ToBI conventions, I show that Rudolph closely mirrors a number of the unique prosodic patterns employed by Harris, but that Rudolph does not simply mimic Harris; rather she exaggerates Harris’ patterns by using even higher F0 peaks and more phrase-initial falsetto phonation. The results of this study expand our knowledge about how specific idiolectal variants connected to social and ethnic styles are enregistered as part of the public discourse. Additionally, it demonstrates the value of examining parodic performance to better understand and contextualize the speech of public figures."
   ],
   "doi": "10.21437/SpeechProsody.2022-129"
  },
  "cruz22_speechprosody": {
   "authors": [
    [
     "Marisa",
     "Cruz"
    ],
    [
     "Jovana",
     "Pejovic"
    ],
    [
     "Catia",
     "Severino"
    ],
    [
     "Marina",
     "Vigario"
    ],
    [
     "Sónia",
     "Frota"
    ]
   ],
   "title": "Auditory and visual cues in face-masked infant-directed speech",
   "original": "187",
   "page_count": 5,
   "order": 133,
   "p1": 639,
   "pn": 643,
   "abstract": [
    "Language includes auditory and visual cues relevant to language learning, and infants have been shown to take advantage of those cues while processing speech. With COVID-19 the use of face masks became pervasive, affecting the auditory and visual cues available to the listener, especially the young language learner. This study examined how acoustic and visual cues (head and eyebrow movements) changed in infant-directed masked-speech (FMS), using a corpus from a word segmentation task [1]. The corpus included utterances with target pseudo-words in (non-prominent) medial position and prosodic-edge position. Video recordings of 96 utterances produced by a female speaker with and without a N95 mask were obtained. We measured mean pitch, pitch range, mean intensity, intensity range, and RMS, as well as vertical displacement of the head and eyebrows. FMS had lower intensity overall. Vertical head displacement was larger in FMS, whereas eyebrow displacement was smaller. Unlike with no mask, in FMS there was no contrast in mean pitch, mean intensity and RMS, or in degree of head displacement, between utterances with target words in medial and edge position. These findings suggest both a general and selective effect of face masks in auditory and visual cues, with implications for language development."
   ],
   "doi": "10.21437/SpeechProsody.2022-130"
  },
  "shang22_speechprosody": {
   "authors": [
    [
     "Peizhu",
     "Shang"
    ],
    [
     "Wendy",
     "Elvira-García"
    ],
    [
     "Xinyi",
     "Li"
    ]
   ],
   "title": "Cue weighting differences in perception of Spanish sentence type between native listeners of Chinese and Spanish",
   "original": "126",
   "page_count": 5,
   "order": 134,
   "p1": 644,
   "pn": 648,
   "abstract": [
    "This study examined the acoustic cue weighting in Spanish question-statement identification between native listeners of a tonal (Chinese) and a non-tonal language (Spanish). Listeners’ performance was evaluated using two identification tests, whereby the stimuli were generated by manipulating the F0 contour, duration, and amplitude of the word-final syllable. A logistic sigmoid model was used to fit the question-statement identification function, and the model parameters were further estimated using a linear mixed-effect analysis. Results showed that Spanish listeners were more sensitive to the F0 linear transitions perceived as intonation, suggesting that the tonal language benefit in pitch perception may be limited to specific dimensions of pitch events. Nevertheless, Spanish listeners needed a higher terminal pitch to identify questions than Chinese speakers, especially when the duration or amplitude was decreased. Besides, our study revealed that native Spanish speakers gave more weight to the secondary cues of duration and amplitude and made greater compensations for the acoustic perturbations than Chinese listeners. These cross-linguistic differences of cue weighting between the two groups can be attributed to several factors, including listeners’ previous linguistic experience with the first and the target language and the perceptual compensatory capacity in speech."
   ],
   "doi": "10.21437/SpeechProsody.2022-131"
  },
  "mizuguchi22_speechprosody": {
   "authors": [
    [
     "Shinobu",
     "Mizuguchi"
    ],
    [
     "Koichi",
     "Tateishi"
    ]
   ],
   "title": "Perception of Boundary and Prominence in Spontaneous Japanese: An RPT Study",
   "original": "131",
   "page_count": 5,
   "order": 135,
   "p1": 649,
   "pn": 653,
   "abstract": [
    "Traditional studies on prosody argue that prominence is highly tied to changes of F0 but recent perceptual research of utterance-level prosodic prominence using Rapid Prosody Transcription (RPT) shows that perception strategy is much more complex, as it involves not only phonetic cues but also phonological, semantic and information cues. This paper considers Japanese in the RPT framework. Since it is a mora-timed pitch language and uses pitch both for lexical accent and utterance-level prosody, it is expected that Japanese has a different perception strategy from some Indo-European languages that use pitch movement for utterance-level prosody only. It is also expected that our experiments will provide concrete data for the hot topic in Japanese literature, ‘Does focal prominence reset a phrase boundary?’, based on the utterance-level perception strategy. We will show that (i) contra literature on Japanese focus, acoustic features of F0, duration, and intensity are not strong prominence cues in Japanese, (ii) perceived prominence is strongly tied to pitch movement and its location in an utterance, and (iii) not only content words but also function morphemes get highlighted in Japanese. Perception strategies vary among languages, as predicted."
   ],
   "doi": "10.21437/SpeechProsody.2022-132"
  },
  "jeon22_speechprosody": {
   "authors": [
    [
     "Hae-Sung",
     "Jeon"
    ],
    [
     "Antje",
     "Heinrich"
    ]
   ],
   "title": "Perception of Pitch Height and Prominence by Old and Young listeners",
   "original": "87",
   "page_count": 5,
   "order": 136,
   "p1": 654,
   "pn": 658,
   "abstract": [
    "For understanding speech, it is crucial for listeners to perceive intonational cues associated with high-information sites, such as stress and accents. Although much research has been done about the perception of intonation in younger listeners, little is known about age-related differences. The present study tested younger (21-35 years old) and older (63-75 years old) listeners who were native English speakers using an online platform. Participants carried out a two-alternative forced-choice task to identify relative pitch height or prominence between either two f0 peaks or valleys in an utterance. Against our expectations that older listeners would show reduced discrimination than younger listeners, we did not find substantial differences between the age groups. However, small differences were found for prominence judgement, particularly for the valleys. Some older listeners may have declined capacity to track rapidly changing f0 contours."
   ],
   "doi": "10.21437/SpeechProsody.2022-133"
  },
  "zhang22_speechprosody": {
   "authors": [
    [
     "Yuanyuan",
     "Zhang"
    ],
    [
     "Hongwei",
     "Ding"
    ]
   ],
   "title": "Asymmetry in L1 and L2 listeners’ use of prosody for PP-attachment disambiguation",
   "original": "56",
   "page_count": 5,
   "order": 137,
   "p1": 659,
   "pn": 663,
   "abstract": [
    "The role of prosody in sentence processing has received little attention in L2 spoken-language comprehension research, and studies of prosodic disambiguation of PP-attachment ambiguity by L2 learners are rare. We investigated the effects of prosody (accompanied by visual context from the pictures) on PP-attachment ambiguity resolution by native Australian English speakers and Chinese-speaking learners of English, comparing their attachment preferences to examine L1-L2 sentence processing differences. Our results show that prosody can guide both native speakers and L2 learners towards the resolution of sentence-level ambiguity. However, the observed effects varied. For native speakers, prosody overrode bias for the NP-attachment induced by the visual context; L2 learners in contrast showed a preference for the VP-attachment independent of prosodic cues, indicating that they did not detect the ambiguity within the structure from the visual context and that it is difficult for them to effectively integrate different domains of information in ambiguity resolution. These L1-L2 differences might reflect L2 learners’ limited processing capacity for detecting the ambiguity and integrating multiple domains of information during L2 sentence processing."
   ],
   "doi": "10.21437/SpeechProsody.2022-134"
  },
  "nath22_speechprosody": {
   "authors": [
    [
     "Anindita",
     "Nath"
    ],
    [
     "Nigel",
     "Ward"
    ]
   ],
   "title": "On the Predictability of the Prosody of Dialog Markers from the Prosody of the Local Context",
   "original": "26",
   "page_count": 5,
   "order": 138,
   "p1": 664,
   "pn": 668,
   "abstract": [
    "Dialog markers, such as yeah, and okay generally seem to fit smoothly in the flow of dialog, with prosody that is natural and appropriate for the local context. We are interested in measuring the extent to which the prosody is in fact determined by the local context. Previous investigations of the prosody of dialog markers, and of the predictability of prosody in dialog, have not directly addressed this question. Using 72 prosodic features representing the local context, we built simple models able to predict the average log energy, pitch, cepstral flux, and harmonic ratio for the 12 most common dialog markers of American English. Using the model gave modestly good predictions, reducing the pitch prediction error rate, for example, by 23%."
   ],
   "doi": "10.21437/SpeechProsody.2022-135"
  },
  "ibrahim22_speechprosody": {
   "authors": [
    [
     "Omnia",
     "Ibrahim"
    ],
    [
     "Ivan",
     "Yuen"
    ],
    [
     "Bistra",
     "Andreeva"
    ],
    [
     "Bernd",
     "Möbius"
    ]
   ],
   "title": "The effect of predictability on German stop voicing is phonologically selective",
   "original": "104",
   "page_count": 5,
   "order": 139,
   "p1": 669,
   "pn": 673,
   "abstract": [
    "Cross-linguistic evidence suggests that syllables in predictable contexts have shorter duration than in unpredictable contexts. However, it is not clear if predictability uniformly affects phonetic cues of a phonological feature in a segment. The current study investigated the effect of predictability on the durational correlates of the phonological stop voicing contrast in German, viz.\\ voice onset time (VOT) and closure duration (CD). The target stop consonants /b, p, d, k/ occurred in stressed CV syllables in polysyllabic words embedded in a sentence, with either voiced or voiceless preceding contexts. The syllable occurred in either a low or a high predictable condition, which was based on a syllable-level trigram language model. We measured VOT and CD of the target consonants (voiced vs.\\ voiceless) when the preceding context was voiced or voiceless in either low or high predictable contexts. Our results showed an interaction effect of predictability and target consonants on VOT, but a uniform effect on closure duration. This interaction effect on a primary cue like VOT indicates a selective effect of predictability on VOT, but not on CD. This suggests that the effect of predictability is sensitive to phonological relevance of a language-specific phonetic cue."
   ],
   "doi": "10.21437/SpeechProsody.2022-136"
  },
  "yang22_speechprosody": {
   "authors": [
    [
     "Yike",
     "Yang"
    ],
    [
     "Si",
     "Chen"
    ]
   ],
   "title": "Does prosody influence segments differently in Cantonese and Mandarin? A case study of the open vowel /a/",
   "original": "27",
   "page_count": 5,
   "order": 140,
   "p1": 674,
   "pn": 678,
   "abstract": [
    "The interaction between segment and prosody has been receiving increasing attention. While speakers of European languages are found to hyper-articulate their speech to maintain the distinction between the focused and unfocused portions, little is known about focus effects on vowels in Chinese languages. This study investigated the potential interaction between prosodic focus and vowels and tested whether the effects of focus function differently in Cantonese and Mandarin, two closely related Chinese languages. In a focus production experiment, the target vowels were analysed on the duration, formants and distances. The results showed that prosodic focus influenced the open vowel /a/ differently in Cantonese and Mandarin. Although focus increased the vowel duration in both languages, the on-focus vowels were lengthened to a greater extent in Cantonese. The effect of focus was minimal on the vowel formants, especially in Cantonese. For the Euclidean distances between the vowels under broad focus and those under the remaining focus types, no difference was found, but Cantonese and Mandarin diverged in the directions in which each focus type moved away from broad focus. These results suggest that, while speakers of both languages hyper-articulate on-focus vowels, there are more differences than similarities between the two languages."
   ],
   "doi": "10.21437/SpeechProsody.2022-137"
  },
  "benitezburraco22_speechprosody": {
   "authors": [
    [
     "Antonio",
     "Benítez-Burraco"
    ],
    [
     "Wendy",
     "Elvira-García"
    ]
   ],
   "title": "Human self-domestication and the evolution of prosody",
   "original": "46",
   "page_count": 4,
   "order": 141,
   "p1": 679,
   "pn": 682,
   "abstract": [
    "Human self-domestication refers to a new evolutionary hypothesis. According to this view, humans have experienced changes that are similar to those observed in domesticated mammals and that have provided us with many of the behavioral and perhaps cognitive pre-requisites for supporting our social practices and advanced culture. At the core of this hypothesis is the claim that self-domestication is triggered by a reduction in reactive aggression. Since the findings of increased complexity in the communicative signals of domesticated animals compared to their wild conspecific, the human self-domestication hypothesis has been used to account for the sophistication of the grammars of human languages. Nonetheless, less research has been done in the domain of phonology. In this talk, we apply this evolutionary model to the evolution of human prosody, arguing for a progressive complexification of prosody that parallels (and is triggered by) the complexification of grammar, also in response to a reduction in reactive aggression levels. Two different types of evidence support our claim: the parallel complexification of prosody and grammar found in emerging sign languages and the parallel sophistication of prosody and grammar during language acquisition, which in turn parallels an increased control over the mechanisms involved in reactive aggression."
   ],
   "doi": "10.21437/SpeechProsody.2022-138"
  },
  "glasbergenplas22_speechprosody": {
   "authors": [
    [
     "Aliza",
     "Glasbergen-Plas"
    ],
    [
     "Stella",
     "Gryllia"
    ],
    [
     "Leticia Pablos",
     "Robles"
    ],
    [
     "Jenny",
     "Doetjes"
    ]
   ],
   "title": "Scripted Simulated Dialogue: a new elicitation paradigm",
   "original": "88",
   "page_count": 5,
   "order": 142,
   "p1": 683,
   "pn": 687,
   "abstract": [
    "Two methods are commonly used to elicit production data for prosody research. The first, in which participants read out a series of written sentences, gives good control over what data are elicited. The second, in which participants perform a task designed to elicit the speech of interest (e.g., a Referential Communication Task), is suitable for studying speech in context. However, certain research topics require the combination of these qualities. We developed an elicitation paradigm, Scripted Simulated Dialogue, that (a) gives precise control over the data that are elicited and (b) is suitable for studying speech in context. In addition, it allows the researcher to control or manipulate the preceding discourse, whereas a Referential Communication Task provides discourse that may be analysed afterwards. The paradigm simulates a series of short dialogues, in which the participant reads her text from a screen and the ‘interlocutor’ is a recorded voice. The participants are not made aware of which speech turn in the dialogue contains the target sentence. We illustrate how Scripted Simulated Dialogue may be used to manipulate the context and make the E-Prime script available to other researchers."
   ],
   "doi": "10.21437/SpeechProsody.2022-139"
  },
  "murphy22_speechprosody": {
   "authors": [
    [
     "Andrew",
     "Murphy"
    ],
    [
     "Irena",
     "Yanushevskaya"
    ],
    [
     "Ailbhe Ní",
     "Chasaide"
    ],
    [
     "Christer",
     "Gobl"
    ]
   ],
   "title": "Affect Expression: Global and Local Control of Voice Source Parameters",
   "original": "237",
   "page_count": 5,
   "order": 110,
   "p1": 525,
   "pn": 529,
   "abstract": [
    "This paper explores how the acoustic characteristics of the voice signal affect. It considers the proposition that the cueing of affect relies on variations in voice source parameters (including f0) that involve both global, uniform shifts across an utterance, and local, within-utterance changes, at prosodically relevant points. To test this, a perception test was conducted with stimuli where modifications were made to voice source parameters of a synthesised baseline utterance, to target angry and sad renditions. The baseline utterance was generated with the ABAIR Irish TTS system, for one male and one female voice. The voice parameter manipulations drew on earlier production and perception experiments, and involved three stimulus series: those with global, local and a combination of global and local adjustments. 65 listeners judged the stimuli as one of the following: angry, interested, no emotion, relaxed and sad, and indicated how strongly any affect was perceived. Results broadly support the initial proposition, in that the most effective signalling of both angry and sad affect involved those stimuli which combined global and local adjustments. However, results for stimuli targeting angry were often judged as interested, indicating that the negative valence is not consistently cued by the manipulations in these stimuli."
   ],
   "doi": "10.21437/SpeechProsody.2022-107"
  },
  "schauffler22_speechprosody": {
   "authors": [
    [
     "Nadja",
     "Schauffler"
    ],
    [
     "Fabian",
     "Schubö"
    ],
    [
     "Toni",
     "Bernhart"
    ],
    [
     "Gunilla",
     "Eschenbach"
    ],
    [
     "Julia",
     "Koch"
    ],
    [
     "Sandra",
     "Richter"
    ],
    [
     "Gabriel",
     "Viehhauser"
    ],
    [
     "Thang",
     "Vu"
    ],
    [
     "Lorenz",
     "Wesemann"
    ],
    [
     "Jonas",
     "Kuhn"
    ]
   ],
   "title": "Prosodic realisation of enjambment in recitations of German poetry",
   "original": "171",
   "page_count": 5,
   "order": 111,
   "p1": 530,
   "pn": 534,
   "abstract": [
    "A salient feature of poetry is the organisation in lines and stanzas. Versification serves as a structural (and aesthetic) layer that either conforms to syntactic units or breaks them up. If line breaks disrupt syntactic units, we speak of enjambment – the line suggests a pause while the syntactic information continues. Since prosodic boundaries typically reflect syntactic boundaries, the question is which information is marked – the line or the syntactic unit. In a preliminary study with one professional speaker of German, we investigated how line breaks are prosodically realised in recitations of twenty poems by Friedrich Hölderlin. We compared cases of enjambment to line breaks without enjambment and looked at lengthening of the line-final segment, frequency and duration of pauses, and possible downstep of pitch accent peaks across the line break, which are typical cues used for the prosodic marking of phrase boundaries in speech. We found that while pauses and F0 reset are tuned down in cases of enjambment, the line break is still prosodically marked by means of final lengthening. This preliminary result supports the idea that a speaker can convey both the syntactic continuity as well as the versification of the poem by strengthening different cues."
   ],
   "doi": "10.21437/SpeechProsody.2022-108"
  },
  "west22_speechprosody": {
   "authors": [
    [
     "Nicola",
     "West"
    ],
    [
     "Tamara",
     "Rathcke"
    ],
    [
     "Rachel",
     "Smith"
    ]
   ],
   "title": "Timing in speech and music of contemporary English and Scottish composers",
   "original": "214",
   "page_count": 5,
   "order": 112,
   "p1": 535,
   "pn": 539,
   "abstract": [
    "This research examines the links connecting speech and music, and seeks to provide innovative evidence for the long-debated question whether or not speech prosody of a language leaves a notable imprint in its music. Previous studies have frequently focused on the distinction between music from composers who speak a language classed either as ‘stress-timed’ (like English, German) or as ‘syllable-timed’ (like French, Italian), with rather mixed results. Given that the typological distinction between the two rhythm templates is considered controversial, it may be not surprising that to date, empirical support for the idea that the native language of a composer influences their music is limited. The present study aimed to test this hypothesis by eliminating some methodological issues known from previous research. It collected spoken and musical data from contemporary classical composers, native speakers either of Standard Southern British English or of Standard Scottish English. The two varieties of English were chosen for their distinct timing patterns. The results provide support for the idea that music of a composer shares some temporal patterning with their spoken language, and are discussed with reference to cognitive processes of implicit statistical learning."
   ],
   "doi": "10.21437/SpeechProsody.2022-109"
  },
  "goswami22_speechprosody": {
   "authors": [
    [
     "Usha",
     "Goswami"
    ]
   ],
   "title": "Acoustic Structure in the Amplitude Envelope and Speech Prosody: A Psycholinguistic and Developmental Perspective",
   "original": "k4",
   "page_count": 0,
   "order": 4,
   "p1": "",
   "pn": "",
   "abstract": [
    "Neural statistical learning of physical stimulus characteristics of the speech signal is important for the development of cognitive systems like language. Rhythm patterns are a core component of linguistic systems, and rhythm is key to language acquisition by infants. Rhythmic processing and prosodic sensitivity are also impaired in children with developmental dyslexia. Across languages, children with dyslexia find it difficult to discriminate amplitude ‘rise times’, physical cues to rhythm in the speech envelope that also automatically trigger speech-brain alignment (neural entrainment to speech). To understand these developmental relationships, we have previously developed demodulation computational approaches to reveal the modulation structure of the amplitude envelope of English nursery rhymes and “Babytalk” (infant-directed speech, IDS) (Spectral-Amplitude Modulation Phase Hierarchy models, S-AMPH). In my talk, I will outline our demodulation approaches to nursery rhymes and IDS, and review the recent application of our findings to understanding the difficulties with speech prosody experienced by children with developmental dyslexia."
   ]
  },
  "frota22_speechprosody": {
   "authors": [
    [
     "Sónia",
     "Frota"
    ],
    [
     "Marina",
     "Vigário"
    ],
    [
     "Marisa",
     "Cruz"
    ],
    [
     "Friederike",
     "Hohl"
    ],
    [
     "Bettina",
     "Braun"
    ]
   ],
   "title": "Amplitude envelope modulations across languages reflect prosody",
   "original": "179",
   "page_count": 5,
   "order": 143,
   "p1": 688,
   "pn": 692,
   "abstract": [
    "The speech signal has been shown to contain a fine structure that consists of the fast changing spectral content (e.g., formant transitions, voicing, spectral energy distributions), together with amplitude modulations of the envelope with different timescales. These different modulation frequencies have been associated with linguistic units of different sizes, and neuronal oscillations seem to track this linguistic structure. As the amplitude envelope mostly captures suprasegmental information, the different modulation frequencies are natural candidates to convey prosodic information. In this paper we put these assumptions to test by comparing effects of sentence length and language, focusing on languages with distinct prosodic profiles: Brazilian Portuguese (syllable-timed), European Portuguese (mixed rhythm, with syllable-timed and stress-timed properties), and German (stress-timed). There are further differences regarding the roles of the syllable, the foot, the prosodic word and the intonation phrase. In this paper, we analyzed wideband amplitude envelopes using general additive mixed models and show that German differs from Brazilian Portuguese and European Portuguese in the delta (1-2Hz) and theta bands (6-8Hz). European and Brazilian Portuguese also differ, but only in the delta band (1-2Hz). The language differences in amplitude modulation are discussed in terms of speech rhythm and differences in prosodic structure across languages."
   ],
   "doi": "10.21437/SpeechProsody.2022-140"
  },
  "stehwien22_speechprosody": {
   "authors": [
    [
     "Sabrina",
     "Stehwien"
    ],
    [
     "Lars",
     "Meyer"
    ]
   ],
   "title": "Short-Term Periodicity of Prosodic Phrasing: Corpus-based Evidence",
   "original": "54",
   "page_count": 6,
   "order": 144,
   "p1": 693,
   "pn": 698,
   "abstract": [
    "Speech is perceived a sequence of meaningful units of various lengths, from phones to phrases. Prosody is one of the means by which these are segmented: Prosodic boundaries subdivide utterances into prosodic phrases. In this corpus study, we study prosodic boundaries from a neurolinguistic perspective. To be perceived correctly, prosodic phrases must obey neurobiological constraints In particular, electrophysiological processing has been argued to operate periodically, with one electrophysiological processing cycle being devoted to the processing of exactly one prosodic phrase. We thus hypothesized that prosodic phrases as such should show periodicity. We assess the DIRNDL corpus of German radio news, which has been annotated for intonational and intermediate phrases. We find that sequences of 2--5 intermediate phrases are periodic at 0.8--1.6 Hertz within their superordinate intonation phrase. Across utterances, the duration of intermediate phrases alternates with the duration of superordinate intonation phrases, indicating a dependence of prosodic time scales. While the determinants of periodicity are unknown, the results are compatible with an association between periodic electrophysiological processing mechanisms and the rhythm of prosody. This contributes to closing the gap between the the neurobiology of language and linguistic description.\n"
   ],
   "doi": "10.21437/SpeechProsody.2022-141"
  },
  "kuang22_speechprosody": {
   "authors": [
    [
     "Jianjing",
     "Kuang"
    ],
    [
     "May Pik Yu",
     "Chan"
    ],
    [
     "Nari",
     "Rhee"
    ]
   ],
   "title": "The effects of syntactic and acoustic cues on the perception of prosodic boundaries",
   "original": "138",
   "page_count": 5,
   "order": 145,
   "p1": 699,
   "pn": 703,
   "abstract": [
    "This study investigates how the perception of prosodic boundaries is shaped by syntactic phrasing and acoustic cues for English and Mandarin listeners. Syntactically-parsed speech corpora were used as the stimuli for the perception experiment. The relative strength of the syntactic boundary of both the left and right sides of the constituents was extracted from the syntactic parsing annotations. A wide range of acoustic cues of both prosodic domain-final and domain-initial positions were examined. Linear-mixed-effects modeling of the likelihood of boundary perception suggests that, for both languages, prosodic boundary perception was influenced by both the strength of syntactic boundary and acoustic cues: boundary perception was heavily driven by the presence of pause; pause also modulated the contribution of other acoustic cues; and larger syntactic boundaries were generally more likely to be perceived as prosodic boundaries. However, there is also cross-linguistic variation: the effect of syntactic phrasing cues was generally stronger for English; acoustically, the effect of final lengthening and pitch reset was stronger in English, while pause was the dominant cue in Mandarin. We discuss the important implications of these findings related to the nature of prosodic hierarchy, and the nature of the prosody-syntax interface.\n"
   ],
   "doi": "10.21437/SpeechProsody.2022-142"
  },
  "maastricht22_speechprosody": {
   "authors": [
    [
     "Lieke van",
     "Maastricht"
    ],
    [
     "Marieke",
     "Hoetjes"
    ],
    [
     "Lisette van der",
     "Heijden"
    ]
   ],
   "title": "Learning L2 Prosody using Gestures: The Role of Individual Differences related to Musicality",
   "original": "17",
   "page_count": 5,
   "order": 149,
   "p1": 718,
   "pn": 722,
   "abstract": [
    "The present study aimed to disentangle the influence of gesture type, physical involvement level, and individual differences in learner characteristics, i.e., working memory (WM) capacity and musicality, in determining the effectiveness of L2 lexical stress training. To this end, 60 native speakers of Dutch read aloud Spanish phrases containing cognates, which were counterbalanced for lexical stress position compared to their Dutch counterpart (e.g., ‘piRÁmides’ in Spanish, ‘piraMIde’ in Dutch). They did so as a pre-test before receiving lexical stress training (T1) and as a post-test both directly after training (T2), and approximately one hour later (T3). Subjects received lexical stress training in one of five conditions varying in gesture type and physical involvement level: audio-visual (AV), AV-beat-perception, AV-beat-production, AV- metaphoric-perception, AV-metaphoric-production. Between T2 and T3, subjects performed a WM capacity and musical aptitude task. The results show that irrespective of training condition subjects significantly improved their L2 lexical stress production from T1 to T2 and T3. Although differences between training conditions were non-significant, there were several significant three-way interactions between WM capacity or musical aptitude and testing time and training condition. This underlines the importance of considering task and learner characteristics in determining the gestural benefit in learning L2 prosody."
   ],
   "doi": "10.21437/SpeechProsody.2022-146"
  },
  "zhang22b_speechprosody": {
   "authors": [
    [
     "Yuan",
     "Zhang"
    ],
    [
     "Florence",
     "Baills"
    ],
    [
     "Pilar",
     "Prieto"
    ]
   ],
   "title": "Training with embodied musical activities has positive effects on unfamiliar language imitation skills",
   "original": "176",
   "page_count": 5,
   "order": 150,
   "p1": 723,
   "pn": 727,
   "abstract": [
    "Research shows that musical expertise benefits second language (L2) phonological learning [1], however little is known on the potential effects of training musical melodic and rhythmic skills on language production skills. This study investigated the role of training musical features such as melody, rhythm, and accent with embodied activities [2] (e.g., without any spoken input) on imitation skills. We hypothesized that embodied musical learning would activate participants' melodic and rhythmic musical skills, which will transfer to their ability to imitate speech. Fifty Chinese adolescents participated in three 45-minute classroom training sessions. They were randomly assigned to one of two conditions: the Embodied Music group followed rhythmic and melodic activities involving body movements; the Non-Embodied Music group followed traditional music classes. Before and after training, participants took part in a sentence-imitation task involving six unfamiliar languages. For each language, imitation skills were evaluated in terms of accentedness by three native speakers. Results showed a significantly higher improvement in the Embodied Music group. These findings demonstrate for the first time the beneficial effects of training students with embodied musical activities on speech imitation skills and add evidence to the relationship between musical rhythmic and melodic features and speech."
   ],
   "doi": "10.21437/SpeechProsody.2022-147"
  },
  "lopezbarrios22_speechprosody": {
   "authors": [
    [
     "Wílmar",
     "López-Barrios"
    ]
   ],
   "title": "Language-specific intonation in the Palenquero/Spanish bilinguals",
   "original": "218",
   "page_count": 6,
   "order": 151,
   "p1": 728,
   "pn": 733,
   "abstract": [
    "Creole languages from the Caribbean seem to exhibit a hybrid prosodic system with tones from African substrate languages, and stress from European dominant languages. It is unknown however whether bilingual speakers of creole languages, such as Palenquero, have specific contexts where their two languages are prosodically distinct. Hence, this study examined whether the bilingual Palenquero/Spanish speakers keep their two languages prosodically distinct in statements and yes/no questions. Speakers performed two discourse completion tasks in two unilingual sessions, the first one in Palenquero and the second one in Caribbean Spanish. F0 contours and final lengthening of 189 five-syllable statements and 153 yes/no questions, from 9 participants, were tested with functional principal component and linear regression analyses. Results demonstrated that their two languages did not have distinct intonation in statements, and that final lengthening was not conditioned by language. Despite that, these speakers kept their two languages prosodically distinct in questions. Palenquero yes/no questions ending with iambic rhythm exhibited F0 peaks at the same height, yielding the global implementation of flat and plateau-shaped contours that did not occur in their Caribbean Spanish. This implies that these bilingual speakers, having two languages with a high overlap, can acquire/develop language-specific intonations in specific contexts."
   ],
   "doi": "10.21437/SpeechProsody.2022-148"
  },
  "sbranna22_speechprosody": {
   "authors": [
    [
     "Simona",
     "Sbranna"
    ],
    [
     "Eduardo",
     "Möking"
    ],
    [
     "Simon",
     "Wehrle"
    ],
    [
     "Martine",
     "Grice"
    ]
   ],
   "title": "Backchannelling across Languages: Rate, Lexical Choice and Intonation in L1 Italian, L1 German and L2 German",
   "original": "59",
   "page_count": 5,
   "order": 152,
   "p1": 734,
   "pn": 738,
   "abstract": [
    "Backchannel (BC) realisations differ from language to language and between native (L1) and non-native (L2) speakers. Our study is the first comprehensive, cross-linguistic analysis of backchannels in L1 and L2 speakers. We recorded 20 dyads of Italian learners of German in both their L1 and L2 (9 beginner and 11 advanced) and 5 dyads of German native speakers performing a Map Task. We analysed backchannel rate, lexical type, function (marking passive recipiency, PR, or incipient speakership, IS) and prosodic realisation. BC rate was similar across languages in all groups and across proficiencies in L2 speech. Intonation was dependent on the lexical choice of BC and/or the function expressed. “Mm-hm” is mostly rising and marks PR. “Genau” is predominantly falling across functions. German “ja” and Italian “sì” were produced with more falling contours for IS and more rising contours for PR. Learners showed a high degree of variability overall, but clearly preferred German lexical BCs that were shared with their L1 Italian. Overall, we found a complex, non-arbitrary mapping between lexical type, function and intonation in both languages. For L2 speech, speaker-specific behaviour (across languages) has a stronger effect than level of proficiency."
   ],
   "doi": "10.21437/SpeechProsody.2022-149"
  },
  "savino22_speechprosody": {
   "authors": [
    [
     "Michelina",
     "Savino"
    ],
    [
     "Simona",
     "Sbranna"
    ],
    [
     "Caterina",
     "Ventura"
    ],
    [
     "Aviad",
     "Albert"
    ],
    [
     "Martine",
     "Grice"
    ]
   ],
   "title": "Imitating intonation in a non-native variety: the influence of the native repertoire",
   "original": "85",
   "page_count": 5,
   "order": 153,
   "p1": 739,
   "pn": 743,
   "abstract": [
    "Previous investigations have shown that speakers of one language variety are able to imitate (or approximate) the F0 contour of another variety, if the two contours share the same phonological function and overall F0 shape but differ in their phonetic implementation. In the current study, we asked speakers of Bari Italian, with a rise-fall(-rise) contour for questions, to imitate the question contour of Lecce Italian, which has a (level-)rise contour. Since the Bari Italian repertoire also has a rise (used to convey non-finality) that has a different phonetic implementation, we tested whether this native rise has an influence on the imitation of the Lecce Italian question rise. Results show that Bari Italian speakers can produce a question rise when asked to imitate the Lecce contour, although they are not able to imitate the whole F0 shape, possibly because of the interference of their native non-final rise. Imitators are more successful in reproducing the contour on the final syllable than on the preceding (level) part, possibly because of the perceptual salience of the rise, combined with its final position triggering a recency effect."
   ],
   "doi": "10.21437/SpeechProsody.2022-150"
  },
  "adams22_speechprosody": {
   "authors": [
    [
     "Jamie",
     "Adams"
    ],
    [
     "Sam",
     "Hellmuth"
    ]
   ],
   "title": "Taiwanese and Beijing Mandarin listeners’ perception of English focus prosody",
   "original": "160",
   "page_count": 5,
   "order": 154,
   "p1": 744,
   "pn": 748,
   "abstract": [
    "Post-focal compression (PFC) of F0 is a known cue to focus in English and Beijing Mandarin (BM), but PFC is neither present in Taiwan Mandarin (TM) production nor interpreted as a cue to focus in perception [1]. Studies of variation in L2 English production by BM and TM learners of English confirm transfer of some L1 patterns into their L2 English [2]. This paper explores for the first time how BM and TM listeners’ interpret PFC in English. It also seeks to clarify L2 listeners’ interpretation of PFC in contexts where discourse-new post-focal material carries a post-focal prominence in English [3]. Following [4] we presented L1 BM, TM and English listeners with a series of written discourse contexts and two prosodically congruous or incongruous audio responses in a between-participants design. One set of listeners in each language group judged SVO English stimuli produced in either all-new context (NN) or with initial narrow focus followed by discourse-given post-focal material (FG). Another set of listeners in each group judged the same all-new (NN) stimuli against recordings with initial narrow focus followed by discourse-new post-focal material (FN). Results indicate differential interpretation of on-focus and post-focal prosody matching a L1 perceptual transfer hypothesis."
   ],
   "doi": "10.21437/SpeechProsody.2022-151"
  },
  "andreeva22_speechprosody": {
   "authors": [
    [
     "Bistra",
     "Andreeva"
    ],
    [
     "Snezhina",
     "Dimitrova"
    ]
   ],
   "title": "The influence of L1 prosody on Bulgarian-accented German and English",
   "original": "153",
   "page_count": 5,
   "order": 158,
   "p1": 764,
   "pn": 768,
   "abstract": [
    "The present study investigates L2 prosodic realizations in the readings of two groups of Bulgarian informants: (a) with L2 German, and (b) with L2 English. Each group consisted of ten female learners, who read the fable “The North Wind and the Sun” in their L1 and in the respective L2. We also recorded two groups of female native speakers of the target languages as controls. The following durational parameters were obtained: mean accented syllable duration, accented/naccented duration ratio, speaking rate. With respect to F0 parameters, mean, median, minimum, maximum, span in semitones, and standard deviations per IP were measured. Additionally, we calculated the number of accented and unaccented syllables, IPs and pauses in each reading. Statistical analyses show that the two groups differ in their use of F0. Both groups use higher standard deviation and level in their L2, whereas the ‘German group’ use higher pitch span as well. The number of accented syllables, IPs and pauses is also higher in L2. Regarding duration, both groups use slower articulation rate. The accented/unaccented syllable duration ratio is lower in L2 for the ‘English group’. We also provide original data on speaking rate in Bulgarian from an information theoretical perspective."
   ],
   "doi": "10.21437/SpeechProsody.2022-155"
  },
  "wang22d_speechprosody": {
   "authors": [
    [
     "Xiaoqing",
     "Wang"
    ],
    [
     "Wentao",
     "Gu"
    ]
   ],
   "title": "Effects of Gender and Language Proficiency on Phonetic Accommodation in Chinese EFL Learners",
   "original": "229",
   "page_count": 4,
   "order": 159,
   "p1": 769,
   "pn": 772,
   "abstract": [
    "Phonetic accommodation is ubiquitous in cross-linguistic/cultural speech communication. The present study examined the effects of gender and language proficiency on phonetic accommodation in Chinese EFL learners. Five vowels /i/, /u/, /æ/, /ɑ/ and /ʌ/ were embedded in a pair of syllables /hVt/ and /hVd/ to compose ten target words. Three groups of Chinese EFL learners differing in the level of English language proficiency (i.e., elementary, intermediate, and advanced) participated in the experiment. To elicit spontaneous conversational speech, a Diapix task embedded with all ten target words was conducted between each participant and a model talker who was a native speaker of American English. Also, each participant read aloud the ten words before and after the Diapix task. Phonetic accommodation was measured by acoustic analysis of vowel duration and formants. For vowel duration, the higher-proficiency learners converged more than the lower-proficiency ones. For vowel formants, a significant interaction effect was found between gender and language proficiency, i.e., females converged less than males in the advanced learners, whereas females converged more than males in the lower-proficiency learners.\n"
   ],
   "doi": "10.21437/SpeechProsody.2022-156"
  },
  "bramlett22_speechprosody": {
   "authors": [
    [
     "Adam",
     "Bramlett"
    ],
    [
     "Seth",
     "Wiener"
    ]
   ],
   "title": "jTRACE modeling of L2 Mandarin learners’ spoken word recognition at two time points in learning",
   "original": "183",
   "page_count": 4,
   "order": 160,
   "p1": 773,
   "pn": 776,
   "abstract": [
    "This study used the TRACE model of spoken word recognition to simulate adult second language (L2) learners’ spoken word recognition at two time points in learning. The pre-existing architecture of jTRACE with the TRACE-T phonology was used to simulate spoken Mandarin word recognition by adult L2 learners at week 1 and week 15 of structured elementary classroom learning. A modified lexicon with reduced tonal information was used to capture recognition during week 1 of learning. Partially restored tonal information was used to capture the change observed at week 15. jTRACE simulations were validated by comparing the results to eye fixation data taken at week 1 and week 15. The eye-tracking task consisted of viewing four Mandarin words written in pinyin while one of the words was presented auditorily. Roughly half of the trials contained words that were segmentally and tonally contrastive (e.g., gān, chá, pǐ, xiàn). The remaining trials contained a target and competitor that were segmentally identical but tonally contrastive (e.g., mā, má, pěn, gòng). Proportion of looks to the target were calculated and compared to the jTRACE simulations using multiple linear regression. The results showed evidence of activation and recognition, thereby corroborating our modeling approach."
   ],
   "doi": "10.21437/SpeechProsody.2022-157"
  },
  "kallio22_speechprosody": {
   "authors": [
    [
     "Heini",
     "Kallio"
    ],
    [
     "Rosa",
     "Suviranta"
    ],
    [
     "Mikko",
     "Kuronen"
    ],
    [
     "Anna von",
     "Zansen"
    ]
   ],
   "title": "Creaky voice and utterance fluency measures in predicting fluency and oral proficiency of spontaneous L2 Finnish",
   "original": "40",
   "page_count": 5,
   "order": 161,
   "p1": 777,
   "pn": 781,
   "abstract": [
    "While utterance fluency measures are often studied in relation to perceived L2 fluency and proficiency, the effect of creaky voice remains ignored. However, creaky voice is frequent in a number of languages, including Finnish, where it serves as a cue for phrase-boundaries and turn-taking. In this study we investigate the roles of creaky voice and utterance fluency measures in predicting fluency and proficiency ratings of spontaneous L2 Finnish (F2) speech. In so doing, 16 expert raters participated in assessing narrative spontaneous speech samples from 160 learners of Finnish. The effect of creaky voice and utterance fluency measures on proficiency and fluency ratings was studied using linear regression models. The results indicate that creaky voice can contribute to both oral proficiency and fluency alongside utterance fluency measures. Furthermore, average duration of composite breaks -- a measure combining breakdown and repair phenomena -- proved to be the most significant predictor of fluency. Based on these findings we recommend further investigation of the effect of creaky voice to the assessment of L2 speech as well as reconsideration of the utterance fluency measures used in predicting L2 fluency or proficiency.\n"
   ],
   "doi": "10.21437/SpeechProsody.2022-158"
  },
  "li22d_speechprosody": {
   "authors": [
    [
     "Yanping",
     "Li"
    ],
    [
     "Catherine",
     "Best"
    ],
    [
     "Michael",
     "Tyler"
    ],
    [
     "Denis",
     "Burnham"
    ]
   ],
   "title": "Native Beijing listeners’ perceptual assimilation of Mandarin lexical tones produced by L2-Mandarin speakers from Yantai, Shanghai, and Guangzhou",
   "original": "145",
   "page_count": 5,
   "order": 162,
   "p1": 782,
   "pn": 786,
   "abstract": [
    "The four lexical tones of standard Beijing Mandarin (henceforth, Mandarin), i.e., level, rising, dipping, and falling, are produced with regional accents by speakers from other regions of China. This study investigated how native Beijing listeners categorize and rate second language (L2) Mandarin tones produced by Yantai, Shanghai, and Guangzhou speakers, whose native dialect tone systems differ from Mandarin and from each other. Native Beijing listeners (n = 35) heard Mandarin words (/ba, di, du, gu/ × 4 tones) produced by speakers of the three regional dialects and by Beijing speakers (baseline). For each word, they selected one of four minimal-tone quadruplet words and rated its similarity to Beijing pronunciation. While they identified the words with high accuracy (> 90%) in all four accents, the regionally accented words produced lower ratings and longer decision times than Beijing stimuli. This indicates that although native Beijing listeners reliably recognize regionally accented tones, the phonetic variability in regional accents modulates their tone categorization. This study demonstrated the impact of regional accents on Beijing listeners’ perception of L2 Mandarin tones, which lays a foundation for better understanding of how native listeners perceive non-native tone production.\n"
   ],
   "doi": "10.21437/SpeechProsody.2022-159"
  },
  "benus22_speechprosody": {
   "authors": [
    [
     "Štefan",
     "Beňuš"
    ]
   ],
   "title": "Prosodic imitation of audiovisual and audio-only prompts in L2 English",
   "original": "209",
   "page_count": 5,
   "order": 163,
   "p1": 787,
   "pn": 791,
   "abstract": [
    "Speech entrainment, also alignment, convergence, or accommodation, is a pervasive, sub-conscious, and complex tendency of interlocutors to speak more similarly over time. It has been shown that audiovisual speech facilitates this tendency more than auditory-only speech, and that this applies to both segmental features such as VOT or vowel formants and prosodic features such as f0. The effect of audio-visual vs. audio-only prompts on the prosodic imitation of non-native speakers, as a special case of this tendency, has not been extensively studied but can add to our understanding of how the visual modality might be relevant in imitation and second-language distant learning. 90 L2 speakers produced triplets of words first on their own and then imitating a model speaker presented either as a video clip or as a sound only. The words varied in their f0 contours (fall, rise, fall-rise). The imitation of f0 and duration was assessed acoustically, not perceptually. Contrary to the expectation, the audiovisual modality did not facilitate greater imitation than the audio-only prompts for f0, and for duration the audio-only modality resulted in slightly better imitation scores. We discuss how the specific features of this dataset might have influenced the results.\n"
   ],
   "doi": "10.21437/SpeechProsody.2022-160"
  },
  "judkins22_speechprosody": {
   "authors": [
    [
     "Lucie",
     "Judkins"
    ],
    [
     "Charlotte",
     "Alazard-Guiu"
    ],
    [
     "Corine",
     "Astésano"
    ]
   ],
   "title": "How do we chunk and pause in non-native vs native speech? Methodological implications for SLA",
   "original": "204",
   "page_count": 5,
   "order": 164,
   "p1": 792,
   "pn": 796,
   "abstract": [
    "The present preliminary study looks at speech units and pauses’ quantity, duration and distribution in native (L1) vs non-native (L2) spontaneous speech. 6 (2 females) English L1 – French L2 speakers were recorded in their L1 and L2 on a conversation task. 3 speakers had an elementary L2 proficiency level, the other 3 were advanced. We looked at 1) the effect of speaking an L2 vs an L1 (within-speaker analysis) and 2) the effect of the proficiency level (between-speaker analysis) on the quantity, distribution and duration of silent and voiced pauses and the quantity and duration of Inter Pausal Units (IPU). Our results show that the IPU tend to be more numerous and shorter in L2 but those differences do not reach significance. Pauses are more numerous in L2 regardless of the proficiency level and the difference between L1 and L2 is less important in the advanced group. These tendencies concern mainly the quantity and less so the duration and distribution. Additionally, we notice a greater diversity in the type of pauses in L2. These observations shed light on the reality of interlanguage and the methodological issues involved in its description. Implications for L2 speech analysis and evaluation are discussed."
   ],
   "doi": "10.21437/SpeechProsody.2022-161"
  },
  "tu22_speechprosody": {
   "authors": [
    [
     "Jung-Yueh",
     "Tu"
    ],
    [
     "Jih-Ho",
     "Cha"
    ]
   ],
   "title": "Mandarin third tone sandhi application in trisyllabic words by L2 learners",
   "original": "35",
   "page_count": 5,
   "order": 165,
   "p1": 797,
   "pn": 801,
   "abstract": [
    "Mandarin tone 3 (T3) sandhi is a phonological rule, where a low dipping T3 followed by another T3 is changed as a high rising tone, similar to tone 2 (T2), described as sandhi rising tone2 (S2) in this study. The application of T3 sandhi rule in disyllabic words is robust whereas that in trisyllables or polysyllables with all T3 is more complicated, which involves not only prosodic but also morph-syntactic domains. Building on previous works, the current study aims to investigate whether L2 learners could apply this T3 sandhi rule and how they apply the T3 sandhi rule in polysyllabic words. Specifically, the study examines the production of T3 sandhi in trisyllabic Mandarin words by Japanese and Korean learners of Mandarin. The stimuli include 13 disyllabic words, 13 trisyllabic words (with different sandhi patterns ‘T3+S2+T3’ and ‘S2+S2+T3’) and 12 hexasyllabic sentences with T3 in sequences. The production errors are analyzed into four patterns: over application, under application, combination, and others. The results are discussed from the perspective of phonology-syntax interface in T3 sandhi as well as effects of language experience."
   ],
   "doi": "10.21437/SpeechProsody.2022-162"
  },
  "baills22_speechprosody": {
   "authors": [
    [
     "Florence",
     "Baills"
    ],
    [
     "Fabián",
     "Santiago"
    ],
    [
     "Paolo",
     "Mairano"
    ],
    [
     "Pilar",
     "Prieto"
    ]
   ],
   "title": "The effects of prosodic training with logatomes and prosodic gestures on L2 spontaneous speech",
   "original": "210",
   "page_count": 5,
   "order": 166,
   "p1": 802,
   "pn": 806,
   "abstract": [
    "Training L2 suprasegmental features benefits pronunciation accuracy and comprehensibility, especially through the use of hand gestures. However, studies have mainly looked at the effect of prosodic training in controlled tasks and less is known about spontaneous speech. The present study explores the effect of prosodic training with and without gestures depicting prosody on several dimensions of pronunciation in spontaneous speech.\nFifty Catalan learners of French practiced oral reading and sentence-by-sentence imitation with short dialogues during three 30-minute sessions in one of three conditions: speech-only, repeating a selection of sentences from the dialogues; logatome, repeating logatome sequences (series of same consonant-vowel syllable leaving out lexical information) corresponding to the prosody of target sentences; and logatome+gesture, additionally mimicking gestures representing phrasal prosodic patterns.\nPerceptive evaluations of learners’ spontaneous speech at pre- and posttest revealed that training prosody did not have any significant effect on participants’ comprehensibility, accentedness, and suprasegmental accuracy in spontaneous speech. Acoustic analyses further showed null effects of prosodic training on fluency and the pronunciation of difficult vowel contrasts. Our findings suggest that L2 prosodic training should be better tailored for spontaneous speech. Alternatively, learners may need more time to transfer beneficial effects from controlled tasks to spontaneous speech."
   ],
   "doi": "10.21437/SpeechProsody.2022-163"
  },
  "bottcher22_speechprosody": {
   "authors": [
    [
     "Marlene",
     "Böttcher"
    ]
   ],
   "title": "A Comparison of Pitch Accent Patterns in Contrastive Adjective+Noun Structures in Bilingual Englishes",
   "original": "68",
   "page_count": 5,
   "order": 167,
   "p1": 807,
   "pn": 811,
   "abstract": [
    "This study examines the prosody of modified NPs in bilingual Englishes exploring spontaneous productions by 48 speakers of different first languages (L1 Greek and Russian) and 24 monolinguals. The realization of adjectives is related to information structure as they are more likely to be produced when expressing a contrast. In English contrast is connected to pitch accent (PA) placement. This prosodic contrast marking is likely to change in language contact because of the marked status of such pragmatic constraints on prosody. While many studies specifically elicit contrastive adjectives in a controlled setup, this study adds to the previous research the analysis of spontaneous data and provides evidence for language contact phenomena in bilingual prosody of NPs. In the analyzed data the bilinguals’ prosody generally follows pragmatic considerations resulting in native-like contrastive adjective accentuation. The Russian-English bilinguals frequently realize double accentuations with PAs on the contrastive adjective and the noun. This is in line with previous findings on overaccentuation in bilingual speech as a result of structural considerations. The Greek-English bilinguals, however, generally pattern with the monolinguals. A first look at the Russian data of the same speakers shows a similar PA pattern in their L1, suggesting a language-specific contact phenomenon."
   ],
   "doi": "10.21437/SpeechProsody.2022-164"
  },
  "zerbian22_speechprosody": {
   "authors": [
    [
     "Sabine",
     "Zerbian"
    ],
    [
     "Marlene",
     "Böttcher"
    ],
    [
     "Yulia",
     "Zuban"
    ]
   ],
   "title": "Prosody of contrastive adjectives in mono- and bilingual speakers of English and Russian: a corpus study",
   "original": "264",
   "page_count": 5,
   "order": 168,
   "p1": 812,
   "pn": 816,
   "abstract": [
    "The study reports on the frequency of occurrence and prosodic realization of adjective-noun phrases in which the adjective is contrastively focused. The productions of bilingual speakers are investigated in both their languages, Heritage Russian and majority English. The data are extracted from a corpus of semi-spontaneous speech which was collected in a comparable way from mono- and bilingual speakers in the U.S. and Russia. Results of the analysis show that there is a language-specific difference in that Russian speakers use ADJCF+N combinations less frequently than English speakers despite a reported parallel between the languages in terms of syntax, semantics and prosody. Moreover, English and Russian seem to differ in their accentuation pattern in ADJCF+N. Also, speakers of Russian as a Heritage Language frequently use double accents in ADJCF+N. Across English and Russian, double accents in ADJCF+N occur more frequently in formal than in informal situation, and more frequently in bilingual than in monolingual speakers. The results are discussed in light of the often reported tendency in heritage language grammars to avoid ambiguity."
   ],
   "doi": "10.21437/SpeechProsody.2022-165"
  },
  "gao22_speechprosody": {
   "authors": [
    [
     "Sichang",
     "Gao"
    ],
    [
     "Mingwei",
     "Pan"
    ]
   ],
   "title": "Developing and validating a rating scale of speaking prosody ability for learners of Chinese as a second language",
   "original": "33",
   "page_count": 6,
   "order": 169,
   "p1": 817,
   "pn": 822,
   "abstract": [
    "This study aims to develop a rating scale for evaluating the speech prosody of learners of Chinese as a second language (L2). The researchers first gathered 41 descriptors perceived as crucial prosody ability indicators through interviewing ten L2 Chinese teachers, analyzing existing Chinese speaking proficiency scales from five universities in Mainland China. After rating the perception of the selected descriptors by ninety-four L2 Chinese teachers and consulting with four expert-teachers, 15 out of 41 descriptors remained to form a rating scale. Principal component analysis revealed that 15 descriptors with three dimensions (prosodic strategic competence, fluency, and prosodic naturalness) could describe L2 Chinese prosody meaningfully. Finally, using the 15 descriptors, 29 samples of L2 Chinese learners’ speech were evaluated by four raters. A combination of the structural equation modelling and the Many-Facets Rasch modelling confirmed that all the 15 descriptors fit well with the construct of prosody ability measured, demonstrating a good validity of this rating scale."
   ],
   "doi": "10.21437/SpeechProsody.2022-166"
  },
  "schwab22_speechprosody": {
   "authors": [
    [
     "Sandra",
     "Schwab"
    ],
    [
     "Michael",
     "Mouthon"
    ],
    [
     "Justine",
     "Salvadori"
    ],
    [
     "Eugenia Ferreira da",
     "Silva"
    ],
    [
     "Ilona",
     "Yakoub"
    ],
    [
     "Nathalie",
     "Giroud"
    ],
    [
     "Jean-Marie",
     "Annoni"
    ]
   ],
   "title": "Neural correlates and L2 lexical stress learning: an fMRI study",
   "original": "36",
   "page_count": 5,
   "order": 170,
   "p1": 823,
   "pn": 827,
   "abstract": [
    "Stress detection in a second/foreign language (L2) is a complex task which depends of different linguistic and cognitive factors. The objective was to investigate to what extent the activation of specific brain regions involved in L2 stress perception was related to the listeners' ability to detect L2 stress, and to their stress learning capacity. French-speaking participants with no knowledge of Spanish took part in an fMRI study in Spanish, as well as in a pre/post-training experiment, also in Spanish. Results showed that native listeners of French improved their stress perception in a L2 after training, even though the training effect was rather subtle and interindividually variable. The results also revealed that there was a link between neural activation in left inferior frontal gyrus and the participants' performance in L2 stress identification before training. No correlation was found with the amount of learning after training. These data highlight the fact the interindividual differences observed in L2 stress processing might be (at least partially) related to neural interindividual differences."
   ],
   "doi": "10.21437/SpeechProsody.2022-167"
  },
  "buzan22_speechprosody": {
   "authors": [
    [
     "Thales",
     "Buzan"
    ],
    [
     "Cristina",
     "Name"
    ],
    [
     "Juan",
     "Sosa"
    ]
   ],
   "title": "Intonational interference in English-L2 Brazilian speakers: production and perception",
   "original": "107",
   "page_count": 4,
   "order": 171,
   "p1": 828,
   "pn": 831,
   "abstract": [
    "We investigate the phenomenon of intonational contour interference, in both production and perception, in the Brazilian Portuguese/English (BP/EN) language pair. Brazilian learners and English natives were tested. We analysed polar questions as their final contour patterns differ – rise-fall in BP and rise in US English. A production task was conducted which revealed the typical BP yes/no question contours, approximately 33%, when Brazilians uttered English questions. Regarding perception, a forced decision task was conducted in which EN native participants had to choose whether they heard a question or a statement. This task was to investigate (a) the perception of EN-L1 listeners to EN questions with BP intonational interference, and (b) the perception of BP-L1 listeners to: (i) the same questions with interference, and (ii) the interrogatives produced by English-L1 speakers. Results show that EN-L1 listeners tend to perceive questions with the BP contour as statements. Conversely, Brazilians do not have significant problems recognizing their own question contour, or the US English characteristic rising contour as proper questions. In perceptual terms, BP-L1 listeners do recognize the English typical interrogative contour. When there is BP interference, they also display a high rate of question recognition, although with a delayed reaction time.\n"
   ],
   "doi": "10.21437/SpeechProsody.2022-168"
  },
  "torres22_speechprosody": {
   "authors": [
    [
     "Catalina",
     "Torres"
    ]
   ],
   "title": "Pitch range modulations in an edge-marking language",
   "original": "43",
   "page_count": 5,
   "order": 172,
   "p1": 832,
   "pn": 836,
   "abstract": [
    "Prosodic phrasing is a topic that has received considerable attention over the last decades. However, most research has dealt with well studied (mostly European) languages, and quantitative production studies of under-resourced languages are under-represented. To better inform the field of intonational phonology, more data from a more diverse set of languages is needed. This study investigates pitch range modulations in Drehu, an Oceanic language from New Caledonia. Recent experimental work suggests Drehu is edge-marking and the right-edge is prosodically salient. In this study, the phonological and phonetic realisation of prosodic boundary marking is investigated. To determine whether pitch range modulations contribute to phrasing, the intonational marking of noun phrases of different sizes is analysed. An experiment was conducted to examine the extent to which fundamental frequency (F0) modulations contribute to the signalling of right-boundaries and if these are associated with the marking of different prosodic levels. The results show evidence for pitch range adjustments between a phrase initial low tone and a phrase final high tone depending on the position in the noun phrase. These modulations show a blocking of downstep and suggest pitch range adjustments could be indicative of an intermediate phrase (ip) level.\n"
   ],
   "doi": "10.21437/SpeechProsody.2022-169"
  },
  "arvaniti22_speechprosody": {
   "authors": [
    [
     "Amalia",
     "Arvaniti"
    ],
    [
     "Stella",
     "Gryllia"
    ],
    [
     "Cong",
     "Zhang"
    ],
    [
     "Katherine",
     "Marcoux"
    ]
   ],
   "title": "Disentangling emphasis from pragmatic contrastivity in the English H* ~ L+H* contrast",
   "original": "197",
   "page_count": 5,
   "order": 173,
   "p1": 837,
   "pn": 841,
   "abstract": [
    "English H* and L+H* indicate new and contrastive information respectively, though some argue the difference between them is solely one of phonetic emphasis. We used (modified) Rapid Prosody Transcription to test these views. Forty-seven speakers of Standard Southern British English (SSBE) listened to 86 SSBE utterances and marked the words they considered prominent or emphatic. Accents (N = 281) were independently coded as H* or L+H* using phonetic criteria, and as contrastive or non-contrastive using pragmatic criteria. If L+H* is an emphatic H*, all L+H*s should be more prominent than H*s. If the accents mark pragmatic information, contrastivity should drive responses. Contrastive accents and L+H*s were considered more prominent than non-contrastive accents and H*s respectively. Individual responses showed different strategies: for some participants, all L+H*s were more prominent than H*s, for others, contrastive accents were more prominent than non-contrastive accents, and for still others, there was no difference between categories. These results indicate that a reason for the continuing debate about English H* and L+H* may be that the two accents form a weak contrast which some speakers acquire and attend to while others do not.\n"
   ],
   "doi": "10.21437/SpeechProsody.2022-170"
  },
  "zhao22_speechprosody": {
   "authors": [
    [
     "Liang",
     "Zhao"
    ],
    [
     "Shayne",
     "Sloggett"
    ],
    [
     "Eleanor",
     "Chodroff"
    ]
   ],
   "title": "Top-Down and Bottom-up Processing of Familiar and Unfamiliar Mandarin Dialect Tone Systems",
   "original": "241",
   "page_count": 5,
   "order": 174,
   "p1": 842,
   "pn": 846,
   "abstract": [
    "Speech processing involves active integration of bottom-up and top-down information types. In the present study, we investigated the relative weighting of top-down expectedness and bottom-up lexical tone in the perception of familiar and unfamiliar lexical tone systems. Standard Mandarin and Chengdu Mandarin are mutually intelligible language varieties with comparable segmental and highly distinct tonal realizations. In a spoken semantic-plausibility judgment task, we manipulated whether a word was high-surprisal or low-surprisal given the preceding context and dialect-specific tone. All participants were native Standard Mandarin speakers with minimal Chengdu Mandarin experience. Lower judgment accuracy was observed when the stimulus was Chengdu Mandarin, and suggested that expectedness (i.e. top-down) information overrides tonal (i.e. bottom-up) information in sentence plausibility judgments. However, judgment reaction times to sentence surprisal were uniform across stimuli from both dialects, suggesting that speakers are aware of the surprisal conveyed by a non-standard tone, even if not used in their final decision. These findings reveal listener sensitivity to both top-down expectedness and bottom-up tone regardless of the initial tone reliability. For unfamiliar tone systems, top-down influence overrides bottom-up processing to access utterance meaning, but bottom-up processing is indeed present and may reflect rapid learning of the unfamiliar tone system.\n"
   ],
   "doi": "10.21437/SpeechProsody.2022-171"
  },
  "rodriquez22_speechprosody": {
   "authors": [
    [
     "Francesco",
     "Rodriquez"
    ],
    [
     "Paolo",
     "Roseano"
    ],
    [
     "Teresa Cabré",
     "Monné"
    ]
   ],
   "title": "Text-tune accommodation processes in the intonation of European Portuguese yes-no questions: an OT analysis",
   "original": "52",
   "page_count": 5,
   "order": 175,
   "p1": 847,
   "pn": 851,
   "abstract": [
    "A key notion in intonational research is the independence of sentence intonation (tune) and its segmental configuration (text). Yet, there is one aspect of intonation that fewer studies are concerned with: the text-tune interface (TTI). The goal of this paper is rendering accommodation processes at the TTI more explicit by formalizing them within the framework of Optimality Theory (OT) and the autosegmental-metrical (AM) model of intonation. The combination of OT and AM is suitable because OT takes conflict resolution as its central notion, and AM successfully bridges the gap between the continuous nature of the distribution of fundamental frequency and OT’s deterministic violation mark assignment. We base our analysis on reported acoustic data of information-seeking yes-no questions in different varieties of European Portuguese (EP), where three text-tune accommodation (TTA) strategies can be observed: tonal truncation, vowel epenthesis, and blocking of vowel deletion. By postulating the phonological representation of tonal alignment in pitch-accents and boundary tones, we put forward a set of intonational as well as segmental faithfulness and markedness constraints, which capture TTA strategy selection in EP and prove to be useful in intonational typology, laying the groundwork for further research on TTA processes in a consistent and comparable way."
   ],
   "doi": "10.21437/SpeechProsody.2022-172"
  },
  "volin22b_speechprosody": {
   "authors": [
    [
     "Jan",
     "Volín"
    ],
    [
     "Michaela",
     "Svatošová"
    ],
    [
     "Pavel",
     "Šturm"
    ]
   ],
   "title": "Fundamental Frequency Variation in Polarity Questions of Czech",
   "original": "98",
   "page_count": 5,
   "order": 176,
   "p1": 852,
   "pn": 856,
   "abstract": [
    "Descriptions of intonation patterns of questions in Czech have so far been impression-based or fragmentary. Our study explores the phonological typology of polarity question melodies proposed in literature and provides quantitative data for the modeling of individual patterns. Although polarity questions are usually found to be more frequent than other interrogative types, their immense variation in form and function still needs to be captured. We use material from a large number of acted minidialogues produced by 34 Czech speakers. K-means clustering together with auditory analysis and visual inspection of F0 tracks offered an insight into both categorial and continuous variation. Emerging patterns have been specified in terms of their F0 parameters and frequency of occurrence. Although the overall variation in the data is substantial, we suppose that most of it is not random. We expect our results to serve in perception test design, in which various components of meanings other than plain request for information could be tested."
   ],
   "doi": "10.21437/SpeechProsody.2022-173"
  },
  "steffman22_speechprosody": {
   "authors": [
    [
     "Jeremy",
     "Steffman"
    ],
    [
     "Stefanie",
     "Shattuck-Hufnagel"
    ],
    [
     "Jennifer",
     "Cole"
    ]
   ],
   "title": "The rise and fall of American English pitch accents: Evidence from an imitation study of rising nuclear tunes",
   "original": "112",
   "page_count": 5,
   "order": 177,
   "p1": 857,
   "pn": 861,
   "abstract": [
    "Rising pitch movements associated with pitch accents are frequently described in terms of alignment and scaling; for example, L+H* versus H* accents vary in these parameters. We examine how 12 American English nuclear tunes, created by combining three pitch accents {H*, L+H*, L*+H}, and four edge tone sequences {H-H%, H-L%, L-H%, L-L%}, are distinguished in an imitative speech production paradigm. Bottom-up clustering analyses of unlabeled time-series f0 identify a robust distinction between trajectories that rise throughout (rise-only) and those with rising-falling movements (rise-fall). Additional clustering distinctions between tunes with different pitch accents are observed only in the rise-only cluster, and further reflect variation in holistic nuclear tune shape. For rise-fall movements, further distinctions in clustering are best defined by ending f0, corresponding to a boundary tone distinction {H%, L%}. With only 4 distinct clusters emerging from the imitated tunes, it appears that some tune distinctions are lost. Nevertheless, modeling trajectories with ToBI labels using a GAMM, and testing alignment of f0 turning points, reveals small differences between tunes in f0 scaling and alignment, distinguishing ToBI labels that were grouped together in clustering. We discuss these results in terms of the hierarchy of distinctions they imply, and categories of tune shapes."
   ],
   "doi": "10.21437/SpeechProsody.2022-174"
  },
  "kachkovskaia22b_speechprosody": {
   "authors": [
    [
     "Tatiana",
     "Kachkovskaia"
    ],
    [
     "Svetlana",
     "Zimina"
    ],
    [
     "Alena",
     "Portnova"
    ],
    [
     "Daniil",
     "Kocharov"
    ]
   ],
   "title": "Social variability of peak alignment in Russian rise-fall tunes",
   "original": "225",
   "page_count": 5,
   "order": 178,
   "p1": 862,
   "pn": 866,
   "abstract": [
    "In Russian, rise-fall tunes (H*L) are very typical in yes-no questions and non-utterance-final clauses. In standard descriptions of Russian intonation, the melodic maximum in this tune is located late in the stressed vowel. However, studies of modern Russian intonation, especially within the younger age group, report on cases of \"displaced\" melodic peaks---shifted significantly to the right, so that the F0 maximum occurs on the post-stressed syllable. In this paper we analyse the frequency of such misplaced peaks in Russian dialogue speech, with respect to the factors of gender, age and social distance between the interlocutors. The research is based on the SibLing speech corpus: 90 dialogues with varying relationship between the interlocutors."
   ],
   "doi": "10.21437/SpeechProsody.2022-175"
  },
  "li22e_speechprosody": {
   "authors": [
    [
     "Xin",
     "Li"
    ],
    [
     "Wentao",
     "Gu"
    ]
   ],
   "title": "Phonological Representation of Tone Sandhi in Nanjing Mandarin",
   "original": "148",
   "page_count": 4,
   "order": 179,
   "p1": 867,
   "pn": 870,
   "abstract": [
    "Although tone sandhi has been well studied in Chinese languages, how sandhi tones are represented in the mental lexicon remains controversial. In Nanjing Mandarin, there are two sandhi types: falling tone sandhi (HL.HL->HH.HL) and low tone sandhi (LL.LL->LH.LL) – the latter is the same as in Standard Mandarin. We investigated the phonological representation of tone sandhi in Nanjing Mandarin using a cross-modal picture-word interference paradigm. A group of native speakers of Nanjing Mandarin were asked to name each visual stimulus with a disyllabic target word while listening to the audio stimulus which was a monosyllabic sound functioning as the distractor, and their reaction time was measured using a voice key. In terms of phonological relatedness of the distractor to the first syllable of the target word, three conditions were compared: surface tone, underlying tone, and control conditions. For low-frequency words in both sandhi types, results showed that the reaction time was significantly faster in the underlying tone condition, and was marginally significantly faster in the surface tone condition than the control condition. These suggest that for both sandhi types in Nanjing Mandarin the underlying tone is activated, while the surface tone may also be activated, but at a lesser degree."
   ],
   "doi": "10.21437/SpeechProsody.2022-176"
  },
  "borise22_speechprosody": {
   "authors": [
    [
     "Lena",
     "Borise"
    ],
    [
     "David",
     "Erschler"
    ]
   ],
   "title": "Mora count and the alignment of rising pitch accents in Iron Ossetic",
   "original": "60",
   "page_count": 5,
   "order": 180,
   "p1": 871,
   "pn": 875,
   "abstract": [
    "Based on instrumental results, we provide an Autosegmental-Metrical analysis of the patterns of formation and acoustic marking of Phonological Phrases (φs) in Iron Ossetic (IrO), an understudied East Iranian language of Russia. Traditional descriptions emphasize the importance of prosodic phrasing in IrO [1]–[3]. We demonstrate that, indeed, (i) φs in IrO correspond to nominal and postpositional phrases and (ii) left φedges are consistently marked with stress-aligned rising pitch accents. Next, we show that IrO has two phonetically distinct rising pitch accents, L*+H and L+H*, and that their distribution and the anchoring of individual tones to metrical targets depend on the moraic structure of the stressed syllable. We account for these facts by extending the analysis of rising pitch accents in Romance by Prieto et al. [4] and the analysis of Franconian prosody by Köhnlein [5]. We argue that bi-moraic stressed syllables in IrO host two tones, L and H, and H undergoes secondary association with the next syllable, giving rise to L+H*. Mono-moraic stressed syllables only host L, with H realized on the next syllable, giving rise to L*+H. Overall, our account, couched in the classic (monostratal) Optimality Theory framework, contributes to the debated typology of rising pitch accents"
   ],
   "doi": "10.21437/SpeechProsody.2022-177"
  },
  "li22f_speechprosody": {
   "authors": [
    [
     "Peng",
     "Li"
    ],
    [
     "Yuan",
     "Zhang"
    ],
    [
     "Xianqiang",
     "Fu"
    ],
    [
     "Florence",
     "Baills"
    ],
    [
     "Pilar",
     "Prieto"
    ]
   ],
   "title": "Melodic perception skills predict Catalan speakers’ imitation abilities of unfamiliar languages",
   "original": "203",
   "page_count": 5,
   "order": 181,
   "p1": 876,
   "pn": 880,
   "abstract": [
    "Musical perception skills have been shown to influence second language speech production. Likewise, working memory may also affect nonnative speech production abilities. However, very few studies have assessed their respective role in speech imitation abilities. The present study thus investigates the predictive role of musical perception skills and working memory on speech imitation abilities of unfamiliar languages. Sixty-one adult Catalan speakers imitated twelve sentences in six languages that were unfamiliar to them. Participants’ music perception skills were tested by four PROMS-S subsets, namely accent, melody, pitch, and rhythm, and their working memory was measured by a forward digit span test. A linear regression analysis revealed that melodic perception skills were the unique predictor among all four musical perception subtests and that working memory was not a significant predictor. Our findings show that melodic perception skills are key in predicting the capability in imitating unfamiliar speech and thus may be important for learning foreign language pronunciation."
   ],
   "doi": "10.21437/SpeechProsody.2022-178"
  },
  "hong22_speechprosody": {
   "authors": [
    [
     "Yu-Siang",
     "Hong"
    ],
    [
     "Sin-Horng",
     "Chen"
    ]
   ],
   "title": "A Data-driven Approach to Constructing a Prosodic Grammar for Mandarin Read Speech",
   "original": "74",
   "page_count": 5,
   "order": 182,
   "p1": 881,
   "pn": 885,
   "abstract": [
    "A new approach to constructing a prosodic grammar of Mandarin read speech which describes the mapping from syntactic patterns to prosodic patterns is proposed. It first prepares a large read-speech corpus with syntactic-tree parsing for texts and break-index labeling representing four-layer non-recursive prosodic hierarchical structures for utterances. Then, all realizations of syntactic pattern-break pattern pairs are extracted to learn prosodic grammatical rules. For a syntactic pattern, rules are inferred via calculating the break-type distributions of pre-boundary, post-boundary, and intra-pattern word junctures from these realizations. In the study, we only investigate the prosodic grammatical rules for four syntactic patterns of determinative-measure (DM) compound, DM+N, DM+DE+N and DM+Modifier+DE+N to verify the feasibility of the proposed approach. With considering the ten syntactic functions of Subject, Object, Topic, Head, Modifier, Attributive, Noun Predicate, Quantitative Complement, and embedded in DE phrase and in Prepositional Phrase, the entropies of pre- and post-boundaries of these four syntactic patterns are reduced significantly. Moreover, detailed rules are inferred via exploring linguistic/semantic interpretations for the occurrence of main prosodic pattern and outliers using the information of phonetic constituents and contexts of syntactic-pattern realizations. Some important factors such as length and semantic relation are found to seriously affect the syntax-prosody mapping."
   ],
   "doi": "10.21437/SpeechProsody.2022-179"
  },
  "moon22_speechprosody": {
   "authors": [
    [
     "Changyun",
     "Moon"
    ],
    [
     "Chuyu",
     "Huang"
    ],
    [
     "Daiki",
     "Hashimoto"
    ]
   ],
   "title": "The Effect of Japanese Pitch Accent System on Musical Cognitive Ability",
   "original": "63",
   "page_count": 5,
   "order": 183,
   "p1": 886,
   "pn": 890,
   "abstract": [
    "This study investigates the influence of the pitch accent system in Japanese on the musical cognitive ability. Previous studies report that the prosodic patterns in one’s language facilitate the perception of music patterns. In contrast, other studies suggest that prosody in one’s language may interfere the perception. To verify whether the prosodic system in one’s native language affects non-linguistic pitch perception, we conducted an AX discrimination experiment on the speakers of two Japanese dialects with different accent systems (Kanto vs. Kansai dialect) who have similar music education background. Two factors are prepared: Accent type, containing one type seen in both dialects and the other only existing in Kansai dialect, and pitch interval. Sequences of piano melody with five consecutive notes (220Hz) and its variations were presented. The results show that the main effects of the native dialect and accent type are significant. The interaction of the native dialect and the pitch interval is also significant. More specifically, Kansai speakers whose dialect has more accentual patterns have a lower accuracy rate than Kanto speakers and particular in the pattern existing in Kansai dialect. This indicates that the prosodic patterns in native language may interfere with the perception of non-linguistic pitch change."
   ],
   "doi": "10.21437/SpeechProsody.2022-180"
  },
  "turk22_speechprosody": {
   "authors": [
    [
     "Helen",
     "Türk"
    ],
    [
     "Pärtel",
     "Lippus"
    ],
    [
     "Merit",
     "Niinemägi"
    ],
    [
     "Karl",
     "Pajusalu"
    ],
    [
     "Pire",
     "Teras"
    ]
   ],
   "title": "The Durational Structure of Tetrasyllabic Words in Inari Saami",
   "original": "181",
   "page_count": 5,
   "order": 184,
   "p1": 891,
   "pn": 895,
   "abstract": [
    "This study focuses on the temporal properties of Inari Saami tetrasyllabic words that consist of two metric feet of different quantity degrees. In this study, we analyze the durational structures of primary and secondary stressed feet. The material comes from four elderly native speakers of Inari Saami who read carrier sentences with tetrasyllabic test words. The durations of all segments in the test words were measured and the durations and duration ratios of the feet were analyzed. The temporal properties of tetrasyllabic words are compared to earlier findings on di- and trisyllabic words in Inari Saami and to Estonian and Livonian which both exhibit a three-way consonantal quantity distinction similar to Inari Saami. The results showed that tetrasyllabic words are mainly divided into two disyllabic feet where the first two syllables form a primary stressed foot and the secondary stress is on the third syllable. However, a pattern where a trisyllabic foot is followed by a stressed fourth syllable also occurs in our data. The phonological three-way distinction in consonants is exhibited in primary stressed feet; in secondary stressed feet the opposition is binary as consonants are either short or long."
   ],
   "doi": "10.21437/SpeechProsody.2022-181"
  },
  "baltazani22_speechprosody": {
   "authors": [
    [
     "Mary",
     "Baltazani"
    ],
    [
     "Katerina",
     "Nicolaidis"
    ]
   ],
   "title": "Phrasing and speech rate effects on segmental and prosodic variability in Greek",
   "original": "190",
   "page_count": 5,
   "order": 185,
   "p1": 896,
   "pn": 900,
   "abstract": [
    "This study examines how different phrasings in segmentally identical utterances are marked by pitch, pre-boundary lengthening, sandhi phenomena, and pausing. It also examines the influence of speech rate on these parameters. Eight speakers (4F, 4M) produced utterances in Greek containing structures of the type “noun (N) + clitic (C) + verb (V)” in two different phrasings, [(N + C) (V)] and [(N) (C+V)], in normal and fast speech rate. Results show that both phrasing and speech rate influenced different measures. Specifically, in both phrasings, pre-boundary lengthening affected the phrase-final word, the phrase-final syllable, as well as the phrase pre-final syllable, with vowels rather than consonants lengthened. Significantly higher scaling and earlier alignment of the H tone was found in [(N + C) (V)] than in [(N) (C+V)] phrasing. Finally, the [(N + C) (V)] phrasing induced greater degree of sandhi effects. In fast rate, word and vowel durations in phrase-final position were shorter than in normal rate, while consonants were not affected significantly. Overall, results on the effect of phrasing agree with previous literature on Greek and other languages, while novel findings on the patterns and speaker-specific strategies are revealed by the combined effect of phrasing and speech rate."
   ],
   "doi": "10.21437/SpeechProsody.2022-182"
  },
  "yan22_speechprosody": {
   "authors": [
    [
     "Mengzhu",
     "Yan"
    ],
    [
     "Sasha",
     "Calhoun"
    ]
   ],
   "title": "Prosodic prominence and clefting in L2 focus interpretation",
   "original": "39",
   "page_count": 5,
   "order": 186,
   "p1": 901,
   "pn": 905,
   "abstract": [
    "Speech cues available in the utterance can guide the listener to the most important information (focus) and thus facilitate discourse comprehension. It has long been established that prosodic prominence plays an important role in the search for focus in a variety of languages including Mandarin and English; however, it is not yet clear how prosodic prominence interacts with other cues, such as clefting, available in the utterance, especially how L2 learners integrate multiple cues in processing information structure. This paper investigates the relative roles of prosodic and clefting cues in the interpretation of focus position in English utterances by moderate or high proficiency Mandarin listeners of English. It was found that both cues played an important role, with clefting weighting slightly higher. This study contributes significantly to our limited knowledge of how information structure is processed in L2 and has implications for speech perception, language learning and prosodic typology."
   ],
   "doi": "10.21437/SpeechProsody.2022-183"
  },
  "thorson22b_speechprosody": {
   "authors": [
    [
     "Jill",
     "Thorson"
    ],
    [
     "Rachel Steindel",
     "Burdin"
    ]
   ],
   "title": "The interpretation and phonetic implementation of !H* in American English",
   "original": "72",
   "page_count": 5,
   "order": 155,
   "p1": 749,
   "pn": 753,
   "abstract": [
    "Downstep in American English has been understudied relative to other types of pitch accents. Our aim is to investigate both the interpretation and the phonetic implementation of !H*. The first experiment investigated participant’s preference for H* vs. !H* pitch accents in new vs. accessible contexts. Results from this experiment show that participants showed a preference for H* for both new and accessible contexts, but that they choose !H* relatively more in the accessible ones. Additionally, participants were more likely to select !H* when it had a smaller fall onto the stressed syllable. The second experiment explored whether participants could distinguish between !H* pitch accents with larger and smaller falls. The results showed that participants were more accurate discriminating between stimuli that were further apart in pitch; however, this effect was mediated by the stimuli’s f0 range, with lower stimuli being easier to discriminate than higher ones. Together, these experiments reveal the complexities of downstep in American English in both where it occurs pragmatically and how it is phonetically produced and perceived.\n"
   ],
   "doi": "10.21437/SpeechProsody.2022-152"
  },
  "gryllia22_speechprosody": {
   "authors": [
    [
     "Stella",
     "Gryllia"
    ],
    [
     "Amalia",
     "Arvaniti"
    ],
    [
     "Cong",
     "Zhang"
    ],
    [
     "Katherine",
     "Marcoux"
    ]
   ],
   "title": "The many shapes of H*",
   "original": "186",
   "page_count": 5,
   "order": 156,
   "p1": 754,
   "pn": 758,
   "abstract": [
    "We examined individual and task-related variability in the realization of Greek nuclear H* followed by L-L% edge tones. The accents (N = 748) were elicited from native speakers of Greek, producing scripted and unscripted speech, and examined using functional Principal Components Analysis. The accented vowel onset was used for landmark registration to capture accent shape and the alignment of the fall. The resulting PCs were analysed using LMEMs (fixed factors: speaker; task type (scripted, unscripted); accented syllable distance from the analysis window offset, to examine the effects of tonal crowding). Tonal scaling and the steepness of the fall (reflected in PC1 and PC2 respectively) changed by task in ways that differed across speakers. PC3, which captured accent shape, also varied by speaker, reflecting shape differences between a rise-fall and (the expected) plateau-plus-fall realization. Tonal crowding did not have consistent effects. In short, the overall accent shape and the alignment of the accentual fall varied by speaker and task. These results hint at substantial variability in tonal realization. At the same time, they indicate that tonal alignment is not as consistent as is sometimes portrayed and thus it should not be the sole criterion for tone categorization."
   ],
   "doi": "10.21437/SpeechProsody.2022-153"
  },
  "rohr22_speechprosody": {
   "authors": [
    [
     "Christine T.",
     "Röhr"
    ],
    [
     "Michelina",
     "Savino"
    ],
    [
     "Martine",
     "Grice"
    ]
   ],
   "title": "The effect of intonational rises on serial recall in German",
   "original": "79",
   "page_count": 5,
   "order": 157,
   "p1": 759,
   "pn": 763,
   "abstract": [
    "This paper uses a serial recall task to investigate the role of rising intonation in the allocation of attentional resources in German. It has been shown for Italian that rising intonation at prosodic boundaries enhances recall of digits in auditorily presented lists. Since resources are usually allocated to prominent items, and since pitch accents are primary encoders of prominence in both languages, we investigate whether an accentual rise leads to better recall than a boundary rise. In a serial recall task on nine-digit sequences in German we compare the effect on working memory of sequences grouped by marking the last item of the two non-final triplets with (i) a high/rising accent followed by an equally high boundary, (ii) a low accent followed by a boundary rise, or (iii) a low/falling accent-boundary sequence, as compared to (iv) ungrouped sequences as controls. Results reveal that items with a rise are recalled more accurately than items without a rise, with no evidence for superior recall of items with accent rises over those with boundary rises. However, boundary rises appear to facilitate recall over a larger domain than accentual rises."
   ],
   "doi": "10.21437/SpeechProsody.2022-154"
  },
  "rhee22_speechprosody": {
   "authors": [
    [
     "Nari",
     "Rhee"
    ],
    [
     "Jianjing",
     "Kuang"
    ],
    [
     "Aoju",
     "Chen"
    ]
   ],
   "title": "The effect of musicality on the development of Mandarin prosody",
   "original": "139",
   "page_count": 4,
   "order": 146,
   "p1": 704,
   "pn": 707,
   "abstract": [
    "Past work has shown a link between children’s musicality and language learning. But research is still sparse on the effect of musicality on the development of prosody, which uses tonal and temporal cues also relevant for processing music. In particular, the questions of when and how musicality affects the development of various aspects of the prosodic grammar remain largely unknown. In this study, we investigate the effect of musicality on the development of focus-marking in Mandarin-speaking 4- to 6-year-olds using speech data elicited in a controlled but interactive setting. We have found that the development of focus-marking in Mandarin is only weakly affected by the learner's musicality. Specifically, children produce adult-like distinction between on-focus and pre-focus positions, regardless of musicality. A musicality effect is observed in the contrast between on-focus and post-focus positions only in the 4-year-olds. The limited musicality effect on focus-marking is in contrast with our previous work, in which we found that musicality has a salient effect on the lexical tone production by children younger than 6 years. Together, the current results suggest that musicality advantage in the development of prosody depends on aspects of the prosodic grammar and the stage of development."
   ],
   "doi": "10.21437/SpeechProsody.2022-143"
  },
  "lameris22_speechprosody": {
   "authors": [
    [
     "Tim",
     "Laméris"
    ]
   ],
   "title": "The Effect of L1 Pitch Status and Extralinguistic Factors on L2 Tone Learning",
   "original": "130",
   "page_count": 5,
   "order": 147,
   "p1": 708,
   "pn": 712,
   "abstract": [
    "Some L2 learners acquire tone more easily than others do. Such inter-learner variability has been attributed to both linguistic factors, (such as the lexical status of pitch in the L1 or the shape of the tones to be acquired), and extralinguistic factors (such as pitch perception aptitude, musical experience, and working memory). However, the relative importance of all these factors when taken together is not well understood. Therefore, this study investigated tonal pseudolanguage word learning by 114 native speakers of languages on a spectrum of lexical pitch usage: Dutch (stress), Japanese and Swedish (pitch accent), and Thai (tonal). Participants were tested and matched for musical experience, working memory and pitch perception aptitude. After a training session, they were tested on their ability to identify the meaning of nine words with a three-way segmental (/lala/ /lele/ /lili/) and a three-way tonal contrast (level, falling, peak). Bayesian inference analyses suggest that tone word learning was primarily facilitated by individual pitch perception aptitude and musical experience, beyond lexical status of pitch in the L1. The findings are discussed in the light of ‘Functional Pitch Hypothesis', and highlight the importance to account for extralinguistic individual aptitudes in speech learning."
   ],
   "doi": "10.21437/SpeechProsody.2022-144"
  },
  "jansen22_speechprosody": {
   "authors": [
    [
     "Nelleke",
     "Jansen"
    ],
    [
     "Eleanor",
     "Harding"
    ],
    [
     "Hanneke",
     "Loerts"
    ],
    [
     "Deniz",
     "Başkent"
    ],
    [
     "Wander",
     "Lowie"
    ]
   ],
   "title": "The relation between musical ability and sentence-level intonation perception: A meta-analysis comparing L1 and non-native listening",
   "original": "233",
   "page_count": 5,
   "order": 148,
   "p1": 713,
   "pn": 717,
   "abstract": [
    "Studies investigating the relationship between musical abilities and speech prosody report that musicians show an altered—often enhanced—perception of prosody, or report positive correlations between music perception and prosody perception. However, some studies on L1 perception find no such benefits, but show good prosody perception across listeners. In contrast, even advanced L2 users may show difficulties in processing sentence intonation. We hypothesised that musicality might especially be beneficial in challenging circumstances of non-native intonation perception. To test this, we conducted a meta-analysis of previous research investigating the effect of musical abilities on the perception of sentence-level intonation in L1, L2, and unfamiliar languages. Studies were systematically collected, and included various measures of musicality and intonation perception. The meta-analysis combining these outcomes showed a robust positive correlation between musical ability and intonation perception. This effect did not differ between studies on L1 and unfamiliar languages. We suggest intonation perception in unfamiliar languages might be relatively easy due to the absence of semantic interference. Data on L2 users was lacking. Because semantic processing plays a role in L2 perception, we suggest further research is needed to investigate the influence of musical ability on intonation perception in L2 listening."
   ],
   "doi": "10.21437/SpeechProsody.2022-145"
  }
 },
 "sessions": [
  {
   "title": "Keynotes",
   "papers": [
    "grice22_speechprosody",
    "gervain22_speechprosody",
    "toro22_speechprosody",
    "goswami22_speechprosody"
   ]
  },
  {
   "title": "Oral session 1: Prosody and pragmatics",
   "papers": [
    "lorenzen22_speechprosody",
    "ip22_speechprosody",
    "strickland22_speechprosody"
   ]
  },
  {
   "title": "Oral session 2: Production of Intonation",
   "papers": [
    "hu22_speechprosody",
    "seeliger22_speechprosody",
    "song22_speechprosody"
   ]
  },
  {
   "title": "Oral session 3: Prosody and speech and language impairments",
   "papers": [
    "wehrle22_speechprosody",
    "lehnertlehouillier22_speechprosody",
    "daigmorte22_speechprosody",
    "pettorino22_speechprosody",
    "boecher22_speechprosody",
    "martin22_speechprosody"
   ]
  },
  {
   "title": "Special oral session 1: Prosodic marking of information structure in language learners: Prosody and beyond",
   "papers": [
    "mok22_speechprosody",
    "everhardt22_speechprosody",
    "du22_speechprosody"
   ]
  },
  {
   "title": "Poster session 1",
   "papers": [
    "ward22_speechprosody",
    "portes22_speechprosody",
    "pronina22_speechprosody",
    "kim22_speechprosody",
    "thorson22_speechprosody",
    "wang22_speechprosody",
    "ito22_speechprosody",
    "lin22_speechprosody",
    "cho22_speechprosody",
    "kruyt22_speechprosody",
    "white22_speechprosody",
    "sahkai22_speechprosody",
    "crocco22_speechprosody",
    "jang22_speechprosody",
    "otundo22_speechprosody",
    "thurgood22_speechprosody",
    "vallsrates22_speechprosody",
    "huttenlauch22_speechprosody",
    "jabeen22_speechprosody",
    "magistro22_speechprosody",
    "bongiorno22_speechprosody",
    "rossi22_speechprosody",
    "hasan22_speechprosody",
    "wepner22_speechprosody",
    "ng22_speechprosody",
    "siqueira22_speechprosody",
    "fernandessvartman22_speechprosody",
    "crouch22_speechprosody",
    "tannander22_speechprosody",
    "zebe22_speechprosody"
   ]
  },
  {
   "title": "Oral session 4: Prosody in language acquisition",
   "papers": [
    "cruzpavia22_speechprosody",
    "xi22_speechprosody",
    "yoon22_speechprosody"
   ]
  },
  {
   "title": "Oral session 5: Segments, prosody and intonation",
   "papers": [
    "guan22_speechprosody",
    "cronenberg22_speechprosody",
    "huaute22_speechprosody"
   ]
  },
  {
   "title": "Oral session 6: Stress and prominence",
   "papers": [
    "bujok22_speechprosody",
    "bruggeman22_speechprosody",
    "bruggeman22b_speechprosody",
    "azzaboukacem22_speechprosody",
    "severijnen22_speechprosody",
    "gussenhoven22_speechprosody"
   ]
  },
  {
   "title": "Oral session 7: Computational modelling and applications of prosody",
   "papers": [
    "sloan22_speechprosody",
    "juliao22_speechprosody",
    "cole22_speechprosody"
   ]
  },
  {
   "title": "Special poster session: Timing and Rhythm Across Languages",
   "papers": [
    "gibbon22_speechprosody",
    "franich22_speechprosody",
    "werner22_speechprosody",
    "li22_speechprosody",
    "white22b_speechprosody",
    "kuznetsova22_speechprosody",
    "yazawa22_speechprosody",
    "morand22_speechprosody",
    "zhu22_speechprosody",
    "wang22b_speechprosody"
   ]
  },
  {
   "title": "Poster session 2",
   "papers": [
    "ambrazaitis22_speechprosody",
    "barrault22_speechprosody",
    "michelas22_speechprosody",
    "karpinski22_speechprosody",
    "ukaszewicz22_speechprosody",
    "santiago22_speechprosody",
    "kalashnikova22_speechprosody",
    "sousa22_speechprosody",
    "arciuli22_speechprosody",
    "carvalho22_speechprosody",
    "munozcoego22_speechprosody",
    "gervain22b_speechprosody",
    "lan22_speechprosody",
    "xu22_speechprosody",
    "simko22_speechprosody",
    "lai22_speechprosody",
    "mikhailava22_speechprosody",
    "mixdorff22_speechprosody",
    "taheriardali22_speechprosody",
    "peirolilja22_speechprosody",
    "ballier22_speechprosody",
    "kachkovskaia22_speechprosody",
    "seyssel22_speechprosody"
   ]
  },
  {
   "title": "Oral session 8: Perception of prosody",
   "papers": [
    "wang22c_speechprosody",
    "plug22_speechprosody",
    "silbervarod22_speechprosody"
   ]
  },
  {
   "title": "Oral session 9: Voice quality, tone and intonation",
   "papers": [
    "li22b_speechprosody",
    "wlodarczak22_speechprosody",
    "passetti22_speechprosody"
   ]
  },
  {
   "title": "Special oral session 2: Measuring, modelling, and training of speaking styles",
   "papers": [
    "mixdorff22b_speechprosody",
    "roessig22_speechprosody",
    "volin22_speechprosody",
    "petrone22_speechprosody",
    "barbosa22_speechprosody",
    "niebuhr22_speechprosody"
   ]
  },
  {
   "title": "Oral session 10: Prosody, emotion, and art",
   "papers": [
    "murphy22_speechprosody",
    "schauffler22_speechprosody",
    "west22_speechprosody"
   ]
  },
  {
   "title": "Poster session 3",
   "papers": [
    "meireles22_speechprosody",
    "huang22_speechprosody",
    "li22c_speechprosody",
    "cambara22_speechprosody",
    "ludusan22_speechprosody",
    "crochiquia22_speechprosody",
    "ge22_speechprosody",
    "matsuda22_speechprosody",
    "chen22_speechprosody",
    "xia22_speechprosody",
    "mady22_speechprosody",
    "erickson22_speechprosody",
    "he22_speechprosody",
    "marty22_speechprosody",
    "levitan22_speechprosody",
    "alvarez22_speechprosody",
    "verheul22_speechprosody",
    "tschinse22_speechprosody",
    "wei22_speechprosody",
    "holliday22_speechprosody",
    "cruz22_speechprosody",
    "shang22_speechprosody",
    "mizuguchi22_speechprosody",
    "jeon22_speechprosody",
    "zhang22_speechprosody",
    "nath22_speechprosody",
    "ibrahim22_speechprosody",
    "yang22_speechprosody",
    "benitezburraco22_speechprosody",
    "glasbergenplas22_speechprosody"
   ]
  },
  {
   "title": "Oral session 11: Prosody, the signal and the brain",
   "papers": [
    "frota22_speechprosody",
    "stehwien22_speechprosody",
    "kuang22_speechprosody"
   ]
  },
  {
   "title": "Special oral session 3: Musical power: The effect of musicality on prosodic learning",
   "papers": [
    "rhee22_speechprosody",
    "lameris22_speechprosody",
    "jansen22_speechprosody"
   ]
  },
  {
   "title": "Oral session 12: Prosody in L2 and bilingual speakers",
   "papers": [
    "maastricht22_speechprosody",
    "zhang22b_speechprosody",
    "lopezbarrios22_speechprosody",
    "sbranna22_speechprosody",
    "savino22_speechprosody",
    "adams22_speechprosody"
   ]
  },
  {
   "title": "Oral session 13: Intonation: phonetics, phonology, processing",
   "papers": [
    "thorson22b_speechprosody",
    "gryllia22_speechprosody",
    "rohr22_speechprosody"
   ]
  },
  {
   "title": "Poster session 4",
   "papers": [
    "andreeva22_speechprosody",
    "wang22d_speechprosody",
    "bramlett22_speechprosody",
    "kallio22_speechprosody",
    "li22d_speechprosody",
    "benus22_speechprosody",
    "judkins22_speechprosody",
    "tu22_speechprosody",
    "baills22_speechprosody",
    "bottcher22_speechprosody",
    "zerbian22_speechprosody",
    "gao22_speechprosody",
    "schwab22_speechprosody",
    "buzan22_speechprosody",
    "torres22_speechprosody",
    "arvaniti22_speechprosody",
    "zhao22_speechprosody",
    "rodriquez22_speechprosody",
    "volin22b_speechprosody",
    "steffman22_speechprosody",
    "kachkovskaia22b_speechprosody",
    "li22e_speechprosody",
    "borise22_speechprosody",
    "li22f_speechprosody",
    "hong22_speechprosody",
    "moon22_speechprosody",
    "turk22_speechprosody",
    "baltazani22_speechprosody",
    "yan22_speechprosody"
   ]
  }
 ],
 "doi": "10.21437/SpeechProsody.2022"
}