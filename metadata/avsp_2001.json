{
 "location": "Aalborg, Denmark",
 "startDate": "7/9/2001",
 "endDate": "9/9/2001",
 "conf": "AVSP",
 "year": "2001",
 "name": "avsp_2001",
 "series": "AVSP",
 "SIG": "AVISA",
 "title": "Auditory-Visual Speech Processing",
 "title1": "Auditory-Visual Speech Processing",
 "date": "7-9 September 2001",
 "papers": {
  "lidestam01_avsp": {
   "authors": [
    [
     "Björn",
     "Lidestam"
    ],
    [
     "Björn",
     "Lyxell"
    ]
   ],
   "title": "Speechreading essentials: signal, paralinguistic cues, and skill",
   "original": "av01_001",
   "page_count": 6,
   "order": 1,
   "p1": "1",
   "pn": "6",
   "abstract": [
    "An overview of some results from our own studies on speechreading is presented. Focus is on paralinguistic cues that assist in perception of the spoken signal. We elaborate on how the contextual, and especially the emotional cues are utilised in speechreading. We further discuss which skills are important in integrating paralinguistic cues and the linguistic signal in speechreading. As a basis for this overview, empirical data as well as hypotheses under scrutiny in a present design are discussed. Finally, some practical implications regarding synthetic faces as communicative aids are discussed: how the speech should be presented, incorporation of paralinguistic cues into the design, and how to match user and aid.\n",
    ""
   ]
  },
  "auerjr01_avsp": {
   "authors": [
    [
     "Edward T.",
     "Auer Jr."
    ],
    [
     "Lynne E.",
     "Bernstein"
    ],
    [
     "Sven",
     "Mattys"
    ]
   ],
   "title": "The INFLUENCE OF THE LEXICON ON VISUAL SPOKEN WORD RECognition",
   "original": "av01_007",
   "page_count": 6,
   "order": 2,
   "p1": "7",
   "pn": "12",
   "abstract": [
    "In this paper, we report on experiments that investigated form-based similarity effects in visual spoken word recognition. Specifically, we tested whether accuracy of speechreading a word was related to the number of words (neighbors) perceptually similar to that stimulus word and to its frequency of occurrence. In the first Experiment, the Neighborhood Activation Model (NAM) [1,2] was adapted to generate predictions about the accuracy of visual spoken word identification. In the second Experiment, we used the concept of the Lexical Equivalence Class Size [3] to generate predictions regarding the accuracy of visual spoken word recognition. Both experiments provided evidence that words are identified more accurately if they have few neighbors and occur frequently in the language. Correlational analyses provided evidence that a word's neighbors, or close competitors, are based on perceptually defined similarity. The results of the current experiments are interpreted as evidence of a common spoken word recognition system for both auditory and visual speech information, which retains sensitivity to form-based stimulus similarity among words.\n",
    ""
   ]
  },
  "ellis01_avsp": {
   "authors": [
    [
     "Tara",
     "Ellis"
    ],
    [
     "Mairead",
     "MacSweeney"
    ],
    [
     "Barbara",
     "Dodd"
    ],
    [
     "Ruth",
     "Campbell"
    ]
   ],
   "title": "TAS: A new test of adult speechreading - deaf people really can be better speechreaders",
   "original": "av01_013",
   "page_count": 5,
   "order": 3,
   "p1": "13",
   "pn": "17",
   "abstract": [
    "A new Test of Adult Speechreading (TAS) is described. The TAS was designed for use with the born-deaf speechreader. It uses picture choice responses, and vocabulary and syntax appropriate to such users.  Over 100 deaf and hearing people have so far been tested using the TAS. The pattern of performance of subsets of users is described.  The effects of factors including age (18-68), gender, non-verbal IQ, education, regional speech community, language preference, and the hearing status of participants and their parents is summarised. Among the notable findings to date: in deaf but not hearing people, visible regional accent and level of education (tertiary vs. non-tertiary) affected performance markedly. Participants with similar southern regional accents to the talkers achieved higher scores than those with northern accents, and those with a tertiary level education performed better than those without. In the hearing participants, parental hearing status predicted performance (people with deaf parents were better speechreaders). Most strikingly, after close matching of deaf and hearing individuals for all psychometric and background variables, deaf participants out-performed their hearing peers by a significant margin.\n",
    ""
   ]
  },
  "schwartz01_avsp": {
   "authors": [
    [
     "Jean-Luc",
     "Schwartz"
    ],
    [
     "Christophe",
     "Savariaux"
    ]
   ],
   "title": "Is it easier to lipread one's own speech gestures than those of somebody else? it seems not!",
   "original": "av01_018",
   "page_count": 6,
   "order": 4,
   "p1": "18",
   "pn": "23",
   "abstract": [
    "In this paper, we attempt to adapt an experimental procedure inspired by Beardsworth and Buckner (1981), in which they studied the ability to recognise one's own versus somebody else's walking movements. They showed that certain subjects were better at recognising themselves than at recognising their friends, thanks to \"some sort of kinesthetic-visual cross-modal transfer\". We study the lipreading scores of French spoken digits uttered by 6 speakers and identified by the same 6 subjects. It appears that the performances are the same whether or not the subject is also the speaker. Hence we failed in our attempt to demonstrate a perceptuo-motor transfer in this experiment.\n",
    ""
   ]
  },
  "kroos01_avsp": {
   "authors": [
    [
     "Christian",
     "Kroos"
    ],
    [
     "Saeko",
     "Masuda"
    ],
    [
     "Takaaki",
     "Kuratate"
    ],
    [
     "Eric",
     "Vatikiotis-Bateson"
    ]
   ],
   "title": "Towards the facecoder: dynamic face synthesis based on image motion estimation in speech",
   "original": "av01_024",
   "page_count": 6,
   "order": 5,
   "p1": "24",
   "pn": "29",
   "abstract": [
    "The (digital) transmission of talking faces requires a high bandwidth that not every target channel is able to provide, even if powerful image compression algorithms are used. Therefore, a special face coding algorithm would be highly desirable. Unfortunately, development of such an algorithm has been hindered by the general problem of image motion estimation. In this paper we present a video-based system for face motion processing similar to the well-known voder-vocoder system for processing and coding acoustic speech signals. Like the vocoder, our 'face coder' consists of two independent parts: an analysis part for tracking non-rigid face motion, and a synthesis part for producing face animations. Results are shown for face motion tracking and the subsequent animation derived from either the raw motion data or the outcome of Principal Component Analysis. The automatic tracking results were evaluated by comparison with a set of manually tracked points.\n",
    ""
   ]
  },
  "kshirsagar01_avsp": {
   "authors": [
    [
     "Sumedha",
     "Kshirsagar"
    ],
    [
     "Nadia",
     "Magnenat-Thalmann"
    ]
   ],
   "title": "Viseme space for realistic speech animation",
   "original": "av01_030",
   "page_count": 6,
   "order": 6,
   "p1": "30",
   "pn": "35",
   "abstract": [
    "For realistic speech animation, smooth viseme and expression transitions, blending and co-articulation so far have been studied and experimented in depth. In this paper, we describe an approach for speech animation by smooth viseme transition. Though this method cannot form an alternative to the co-articulation phenomenon, it certainly takes us a step nearer to realistic speech animation. The approach is devised as a result of the Principal Component Analysis of facial capture extracted data using an optical tracking system. The system extracts the 3D positions of markers attached at the specific feature point locations on face to capture the facial movements of a talking person. We form a vector space representation by using the Principal Component Analysis of this data. We call this space the \"viseme space\". We use the viseme space to generate convincing speech animation and to make smooth transitions from one viseme to another. As the analysis and the resulting viseme space automatically consider the dynamics of and the deformation constraints on the facial movements, the resulting facial animation is very realistic.\n",
    ""
   ]
  },
  "bohning01_avsp": {
   "authors": [
    [
     "M.",
     "Bohning"
    ],
    [
     "Ruth",
     "Campbell"
    ],
    [
     "A.",
     "Karmiloff-Smith"
    ]
   ],
   "title": "Audiovisual speech perception in Williams Syndrome",
   "original": "av01_036",
   "page_count": 4,
   "order": 7,
   "p1": "36",
   "pn": "39",
   "abstract": [
    "In the rare genetic disorder of Williams syndrome (WS), visuospatial abilities, including face processing can be impaired. Auditory speech processing, on the other hand, may be less so. Claims have also been made that WS may impair integrative processing. In this context, the exploration of visual and audiovisual speech perception in WS is of interest.  Tokens from a single natural English speaker of the form /?ba:/, /?va:/, /??a:/, /?da:/ and /?ga:/, were digitally manipulated and presented in unimodal (vision alone, audition alone) and audiovisual conditions, for participants to identify each token. Compared with age-matched controls, WS participants were impaired at visual but not auditory identification, and in audiovisual testing showed correspondingly reduced effects of vision on report of auditory token identity. Audiovisual integration was nevertheless demonstrable in WS. Visual phoneme identification may require visual skills that do not reach age-appropriate levels in WS, despite their age-appropriate (auditory) phonological abilities.\n",
    ""
   ]
  },
  "auerjr01b_avsp": {
   "authors": [
    [
     "Edward T.",
     "Auer Jr."
    ],
    [
     "Lynne E.",
     "Bernstein"
    ],
    [
     "Manbir",
     "Singh"
    ]
   ],
   "title": "Comparing cortical activity during the perception of two forms of biological motion for language communication",
   "original": "av01_040",
   "page_count": 5,
   "order": 8,
   "p1": "40",
   "pn": "44",
   "abstract": [
    "Speaking and fingerspelling words are dynamic, biological, visual activities that serve language communication. However, fingerspelling is a manual articulation of orthography, whereas the visible aspect of spoken language is a product of speech articulation. These two stimulus types provide a revealing contrast for examining the cortical substrate for language perception. Functional magnetic resonance imaging (fMRI) was used to investigate cortical activity due to viewing spoken (lipread) vs. fingerspelled words. In Experiment 1, young adults with prelingual-onset severe-to-profound hearing impairments were imaged. In Experiment 2, young adults with normal hearing and minimal previous experience with fingerspelling were imaged. In both participant groups, fingerspelling and lipreading activated regions of the superior temporal sulcus (STS). However, in the normal-hearing participants, fingerspelling activated fewer regions in the STS. In both participant groups, other activation was observed for fingerspelling, including several dorsolateral parietal areas. These results suggest that the perception of different forms of biological motion (spoken and fingerspelled) occurs in partially shared but partially distinct cortical networks, depending on linguistic significance/ experience of the perceiver.\n",
    ""
   ]
  },
  "callan01_avsp": {
   "authors": [
    [
     "Daniel",
     "Callan"
    ],
    [
     "Akiko",
     "Callan"
    ],
    [
     "Eric",
     "Vatikiotis-Bateson"
    ]
   ],
   "title": "Neural areas underlying the processing of visual speech information under conditions of degraded auditory information",
   "original": "av01_045",
   "page_count": 5,
   "order": 9,
   "p1": "45",
   "pn": "49",
   "abstract": [
    "The goal of this study was to localize, using fMRI, the neural processes involved with visual aspects of speech perception under conditions of degraded auditory information. Brain activity underlying aspects of visual speech processing was determined by comparing conditions with visual speech information to appropriate auditory only conditions. Consistent with the idea of a 'mirror neuron system,' results suggest that speech motor areas (Broca's area) of the brain may be involved with the recognition of phonetic gestures inherent in the visual speech signal under conditions of degraded auditory information.\n",
    ""
   ]
  },
  "bernstein01_avsp": {
   "authors": [
    [
     "Lynne E.",
     "Bernstein"
    ],
    [
     "Jintao",
     "Jiang"
    ],
    [
     "Abeer",
     "Alwan"
    ],
    [
     "Edward T.",
     "Auer Jr."
    ]
   ],
   "title": "Similarity structure in visual phonetic perception and optical phonetics",
   "original": "av01_050",
   "page_count": 6,
   "order": 10,
   "p1": "50",
   "pn": "55",
   "abstract": [
    "This study was undertaken to examine relationships between the similarity structures of optical phonetic measures and visual phonetic perception. For this study, four talkers who varied in visual intelligibility were recorded simultaneously with a 3-dimensional optical recording system and a video camera. Subjects perceptually identified the talkers' consonant-vowel nonsense syllable utterances in a forced-choice identification task. Then, perceptual confusion matrices were analyzed using multidimensional scaling, and Euclidean distances among stimulus phonemes were obtained. Physical Euclidean distances between phonemes were computed on the raw 3-dimensional optical recordings for the phonemes used in the perceptual testing. Multilinear regression was used to generate a transformation vector between physical and perceptual distances. Then, correlations were computed between transformed physical and perceptual distances. These correlations ranged between .77 and .81 (59% and 66% variance accounted for), depending on the vowel context. This study showed that the relatively raw representations of the physical stimuli were effective in accounting for visual speech perception, a result consistent with the hypothesis that perceptual representations and similarity structures for visual speech are modality-specific.\n",
    ""
   ]
  },
  "colin01_avsp": {
   "authors": [
    [
     "C.",
     "Colin"
    ],
    [
     "M.",
     "Radeau"
    ],
    [
     "P.",
     "Deltenre"
    ]
   ],
   "title": "The mismatch negativity (MMN) and the McGurk effect",
   "original": "av01_056",
   "page_count": 6,
   "order": 11,
   "p1": "56",
   "pn": "61",
   "abstract": [
    "The McGurk illusory percept is obtained by dubbing an incongruent articulatory movement onto an auditory signal. The Mismatch Negativity (MMN) of the auditory Event-Related Potential (ERP) reflects the detection of a deviant stimulus within auditory short-term memory (STM) and besides an acoustic component, possesses, under certain conditions, a phonetic one. The present study assessed the existence of an MMN evoked by McGurk percepts. Three experimental conditions were investigated: auditory alone, visual alone and audiovisual (all with the same auditory components). The auditory deviant syllables and the audiovisual incongruent syllables elicited a significant MMN. In the visual condition, no negativity was observed. An MMN can be evoked by audiovisual stimuli that are deviant by their visual content only, provided they are presented in a suitable auditory context leading to a phonetically significant interaction.\n",
    ""
   ]
  },
  "nicholson01_avsp": {
   "authors": [
    [
     "Karen",
     "Nicholson"
    ],
    [
     "Shari",
     "Baum"
    ],
    [
     "Lola",
     "Cuddy"
    ],
    [
     "Kevin",
     "Munhall"
    ]
   ],
   "title": "A case of multimodal aprosodia: impaired auditory and visual speech prosody perception in a patient with right hemisphere damage",
   "original": "av01_062",
   "page_count": 4,
   "order": 12,
   "p1": "62",
   "pn": "65",
   "abstract": [
    "A single-case study was carried out on a patient (KB), who presented with \"aprosodia\" following a right hemisphere stroke, to explore the cross-modal integration of auditory and visual cues in prosodic speech perception. KB was tested on two prosodic speech perception tasks: sentence intonation categorization (i.e., statement or question) and emphatic stress categorization (i.e., first or second noun was stressed).  In addition, he was tested on two segmental speech perception tasks: McGurk Task and speech-in-noise. In all tasks, there were three presentation conditions: audio-only, visual-only, and audiovisual. Results showed that KB performed at about chance on both prosody perception tasks in all three presentation conditions. In contrast, he performed near ceiling in the visual-only and audiovisual conditions on both tasks of segmental speech perception. His performance on the speech-in-noise task showed that he was able to use visual information to compensate for impoverished auditory information in segmental speech perception. Also, his results on the McGurk task were indicative of cross-modal integration in segmental speech perception. The results suggest that, although KB's ability to process visual information in segmental speech tasks is intact, he is nonetheless unable to process prosodic speech information in either the auditory or visual modality.\n",
    ""
   ]
  },
  "lin01_avsp": {
   "authors": [
    [
     "I-Chen",
     "Lin"
    ],
    [
     "Jeng-Sheng",
     "Yeh"
    ],
    [
     "Ming",
     "Ouhyoung"
    ]
   ],
   "title": "Extraction of 3D facial motion parameters from mirror-reflected multi-view video for audio-visual synthesis",
   "original": "av01_066",
   "page_count": 6,
   "order": 13,
   "p1": "66",
   "pn": "71",
   "abstract": [
    "The goal of our project is to collect the dataset of 3D facial motion parameters for the synthesis of talking head. However, the capture of human facial motion is usually an expensive task in some related researches, since special devices must be applied, such as optical or electronic trackers.  In this paper, we propose a robust, accurate and inexpensive approach to estimate human facial motion from mirror-reflected videos. The approach takes advantages of the characteristics between original and mirrored image, and can be more robust than most of other general-purposed stereovision approach in the motion analysis for mirror-reflected videos. A preliminary dataset of facial motion parameters of MPEG-4 and French visemes and with voice data has been acquired, the estimated data are also applied to our facial animation system.\n",
    ""
   ]
  },
  "pelachaud01_avsp": {
   "authors": [
    [
     "C.",
     "Pelachaud"
    ],
    [
     "E.",
     "Magno-Caldognetto"
    ],
    [
     "C.",
     "Zmarich"
    ],
    [
     "P.",
     "Cosi"
    ]
   ],
   "title": "Modelling an Italian talking head",
   "original": "av01_072",
   "page_count": 6,
   "order": 14,
   "p1": "72",
   "pn": "77",
   "abstract": [
    "Our goal is to create a natural Italian talking face with, in particular, lip-readable movements. Based on real data extracted from an Italian speaker with the ELITE system, we have approximated the data using radial basis functions. In this paper we present our 3D facial model based on MPEG-4 standard and our computational model of lip movements for Italian. Our experiment is based on some phonetic-phonological considerations on the parameters defining labial orifice, and on identification tests of visual articulatory movements.\n",
    ""
   ]
  },
  "theobald01_avsp": {
   "authors": [
    [
     "Barry J.",
     "Theobald"
    ],
    [
     "J. Andrew",
     "Bangham"
    ],
    [
     "Iain",
     "Matthews"
    ],
    [
     "Gavin C.",
     "Cawley"
    ]
   ],
   "title": "Visual speech synthesis using statistical models of shape and appearance",
   "original": "av01_078",
   "page_count": 6,
   "order": 15,
   "p1": "78",
   "pn": "83",
   "abstract": [
    "In this paper we present preliminary results of work towards a video-realistic visual speech synthesizer based on statistical models of shape and appearance. A sequence of images corresponding to an utterance is formed by concatenation of synthesis units (in this case triphones) from a pre-recorded inventory. Initial work has concentrated on a compact representation of human faces, accommodating an extensive visual speech corpus without incurring excessive storage costs. The minimal set of control parameters of a combined appearance model is selected according to formal subjective testing. We also present two methods used to build statistical models that account for the perceptually important regions of the face.\n",
    ""
   ]
  },
  "arb01_avsp": {
   "authors": [
    [
     "Allan",
     "Arb"
    ],
    [
     "Steven",
     "Gustafson"
    ],
    [
     "Timothy",
     "Anderson"
    ],
    [
     "Raymond",
     "Slyh"
    ]
   ],
   "title": "Hidden Markov models for visual speech synthesis with limited data",
   "original": "av01_084",
   "page_count": 6,
   "order": 16,
   "p1": "84",
   "pn": "89",
   "abstract": [
    "This paper addresses a problem often encountered when estimating control points used in visual speech synthesis. First, Hidden Markov Models (HMMs) are estimated for each viseme present in stored video data.  Second, models are generated for each triseme (a viseme in context with the previous and following visemes) in the training set. Next, a decision tree is used to cluster and relate states in the HMMs that are similar in a contextual and statistical sense. The tree is also used to estimate HMMs for any trisemes that are not present in the stored video data when control points for such trisemes are required for synthesizing the lip motion for a sentence. Finally, the HMMs are used to generate sequences of visual speech control points for those trisemes not occurring in the stored data. Comparisons of mouth shapes generated from the artificially generated control points and the control points estimated from video not used to train the HMMs indicate that the process estimated accurate control points for the trisemes tested. This paper thus establishes a useful method for synthesizing realistic audio-synchronized video facial features.\n",
    ""
   ]
  },
  "elisei01_avsp": {
   "authors": [
    [
     "F.",
     "Elisei"
    ],
    [
     "M.",
     "Odisio"
    ],
    [
     "Gérard",
     "Bailly"
    ],
    [
     "Pierre",
     "Badin"
    ]
   ],
   "title": "Creating and controlling video-realistic talking heads",
   "original": "av01_090",
   "page_count": 8,
   "order": 17,
   "p1": "90",
   "pn": "97",
   "abstract": [
    "We present a linear three-dimensional modeling paradigm for lips and face, that captures the audiovisual speech activity of a given speaker by only six parameters. Our articulatory models are constructed from real data (front and profile images), using a linear component analysis of about 200 3D coordinates of fleshpoints on the subject's face and lips. Compared to a raw component analysis, our construction approach leads to somewhat more comparable relations across subjects: by construction, the six parameters have a clear phonetic/articulatory interpretation. We use such a speaker's specific articulatory model to regularize MPEG-4 facial articulation parameters (FAP) and show that this regularization process can drastically reduce bandwidth, noise and quantization artifacts. We then present how analysis-by-synthesis techniques using the speaker-specific model allows the tracking of facial movements. Finally, the results of this tracking scheme have been used to develop a text-to-audiovisual speech system.\n",
    ""
   ]
  },
  "morishima01_avsp": {
   "authors": [
    [
     "Shigeo",
     "Morishima"
    ],
    [
     "Shin",
     "Ogata"
    ],
    [
     "Satoshi",
     "Nakamura"
    ]
   ],
   "title": "Multimodal translation",
   "original": "av01_098",
   "page_count": 6,
   "order": 18,
   "p1": "98",
   "pn": "103",
   "abstract": [
    "A stand-in is a common technique for movies and TV programs in foreign languages. The current stand-in that only substitutes the voice channel results awkward matching to the mouth motion. Videophone with automatic voice translation are expected to be widely used in the near future, which may face the same problem without lip-synchronized speaking face image translation. We introduce a multi-modal English-to-Japanese and Japanese-to-English translation system that also translates the speaker's speech motion while synchronizing it to the translated speech. To retain the speaker's facial expression, we substitute only the speech organ's image with the synthesized one, which is made by a three-dimensional wire-frame model that is adaptable to any speaker. Our approach enables image synthesis and translation with an extremely small database.  Also, we propose a method to track motion of the face from the video image. In this system, movement and rotation of the head is detected by template matching using a 3D personal face wire-frame model. By this technique, an automatic multimodal translation can be achieved.\n",
    ""
   ]
  },
  "bernstein01b_avsp": {
   "authors": [
    [
     "Lynne E.",
     "Bernstein"
    ],
    [
     "Curtis W.",
     "Ponton"
    ],
    [
     "Edward T.",
     "Auer Jr."
    ]
   ],
   "title": "Electrophysiology of unimodal and audiovisual speech perception",
   "original": "av01_104",
   "page_count": 6,
   "order": 19,
   "p1": "104",
   "pn": "109",
   "abstract": [
    "Based on behavioral evidence, audiovisual speech perception is generally thought to proceed linearly from initial unimodal perceptual processing to integration of the unimodally processed information. We investigated unimodal versus audiovisual speech processing using electrical event-related potentials (ERPs) obtained from twelve adults. Nonsense syllable stimuli were presented in an oddball paradigm to evoke the mismatch negativity (MMN). Conditions were (1) audiovisual incongruent stimuli (visual /ga/ + auditory /ba/) versus congruent audiovisual stimuli (visual /ba/ + auditory /ba/), (2) visual-only stimuli from the audiovisual condition (/ga/ vs. /ba/), and (3) auditory-only stimuli (/ba/ vs. /da/). A visual- alone MMN was obtained on occipital and temporo-parietal electrodes, and the classical auditory MMN was obtained at the vertex electrode, Cz. Under audiovisual conditions, the negativity recorded at the occipital electrode locations was reduced in amplitude and latency compared to that recorded in the visual-only condition. Also, under the audiovisual condition, the vertex electrode showed a smaller negativity with increased latency relative to the auditory MMN. The neurophysiological evidence did not support a simple bottom-up linear flow from unimodal processing to audiovisual integration.\n",
    ""
   ]
  },
  "kim01_avsp": {
   "authors": [
    [
     "Jinyoung",
     "Kim"
    ],
    [
     "Seungho",
     "Choi"
    ],
    [
     "Joohun",
     "Lee"
    ]
   ],
   "title": "Development of a lip-sync algorithm based on an audio-visual corpus",
   "original": "av01_110",
   "page_count": 5,
   "order": 20,
   "p1": "110",
   "pn": "114",
   "abstract": [
    "In this paper, we propose a corpus-based lip-sync algorithm for natural face animation. An audio-visual (AV) corpus was constructed from the video-recorded announcer's facial shot, speaking the given texts selected from newspapers. To obtain lip parameters, we attached 19 markers on the speaker's face, and we extracted the marker positions by the color filtering followed by the center-of-gravity methods. Also, the spoken utterances were labeled with HTK and such prosodic information as duration, pitch and intensity was extracted as parameters. By combining the audio information with the lip parameters, we constructed audio-visual corpus.\n",
    "Based on this AV corpus, we propose a concatenating method of AV units, which is similar to corpus-based Text-to-speech. For an AV unit search, we used a CVC-syllable unit as a basic synthetic unit. There are two procedures to get lip parameters for given texts and speech. First, top-N candidates for necessary CVC units are selected by two proposed distance measures. The one measure is a phonetic environment distance and the other is a prosodic distance. Second, the best path is estimated from the top-N AV unit sequence and Viterbi search algorithm is used for it. From the computer simulation results, we found that the information not only about duration but also about pitch and intensity is useful to enhance the lip-sync performance. The reconstructed lip parameters are almost equal to the original parameters.\n",
    ""
   ]
  },
  "goecke01_avsp": {
   "authors": [
    [
     "Roland",
     "Goecke"
    ],
    [
     "J. Bruce",
     "Millar"
    ],
    [
     "Alexander",
     "Zelinsky"
    ],
    [
     "Jordi",
     "Robert-Ribes"
    ]
   ],
   "title": "Analysis of audio-video correlation in vowels in Australian English",
   "original": "av01_115",
   "page_count": 6,
   "order": 21,
   "p1": "115",
   "pn": "120",
   "abstract": [
    "This paper investigates the statistical relationship between acoustic and visual speech features for vowels. We extract such features from our stereo vision AV speech data corpus of Australian English. A principal component analysis is performed to determine which data points of the parameter curve for each feature are the most important ones to represent the shape of each curve. This is followed by a canonical correlation analysis to determine which principal components, and hence which data points of which features, correlate most across the two modalities. Several strong correlations are reported between acoustic and visual features. In particular, F1 and F2 and mouth height were strongly correlated. Knowledge about the correlation of acoustic and visual features can be used to predict the presence of acoustic features from visual features in order to improve the recognition rate of automatic speech recognition systems in environments with acoustic noise.\n",
    ""
   ]
  },
  "ekvall01_avsp": {
   "authors": [
    [
     "Christel",
     "Ekvall"
    ],
    [
     "Bertil",
     "Lyberg"
    ],
    [
     "Michael",
     "Randén"
    ]
   ],
   "title": "Non-verbal correlates to focal accents in Swedish",
   "original": "av01_121",
   "page_count": 6,
   "order": 22,
   "p1": "121",
   "pn": "126",
   "abstract": [
    "Speech is normally accompanied or supplemented with different gestures such as eyebrow movements and head movements. These movements seem to be of great importance in face-to-face communication. In this study we were studying the visual correlates to focal accent in read speech. We were especially interested in the timing of the non-verbal events in relation to the speech signal and thereby the segmental flow. It was found that the head movements were highly correlated to the focused word of the utterance. The head movements were either down and forward or up and back.\n",
    ""
   ]
  },
  "kim01b_avsp": {
   "authors": [
    [
     "Jeesun",
     "Kim"
    ],
    [
     "Chris",
     "Davis"
    ]
   ],
   "title": "Visible speech cues and auditory detection of spoken sentences: an effect of degree of correlation between acoustic and visual properties",
   "original": "av01_127",
   "page_count": 5,
   "order": 23,
   "p1": "127",
   "pn": "131",
   "abstract": [
    "An experiment is reported that extends the work of Grant Seitz [1]. Grant and Seitz employed an elegant method for measuring AV effects using a detection paradigm, however the force of their results was diminished because: (i) they used only a small number of test sentences (three) and (ii) a key feature of their results, the relation of the magnitude of masking release with degree of correlation between aspects of speech (intensity) and vision (lip movement), was post hoc. In this paper we used eight stimuli that were explicitly selected to contrast the degree of correlation between speech intensity and lip movement. In order to minimize expectancy effects, the speech materials were in a language unknown to the participants and a method of constant stimuli was adopted. The results supported those of Grant and Seitz: seeing the face of the speaker facilitated detection and this facilitation was best for the stimuli where the correlation between F3 and inter-lip distance was high.\n",
    ""
   ]
  },
  "grant01_avsp": {
   "authors": [
    [
     "Ken W.",
     "Grant"
    ],
    [
     "Steven",
     "Greenberg"
    ]
   ],
   "title": "Speech intelligibility derived from asynchronous processing of auditory-visual information",
   "original": "av01_132",
   "page_count": 6,
   "order": 24,
   "p1": "132",
   "pn": "137",
   "abstract": [
    "The current study examines the temporal parameters associated with cross-modal integration of auditory-visual information for sentential material. The speech signal was filtered into 1/3-octave channels, all of which were discarded except for a low-frequency (298-375 Hz) and a high-frequency (4762-6000 Hz) band. The intelligibility of this audio-only signal ranged between 9% and 31% for nine normal-hearing subjects. Visual-alone presentation of the same material ranged between 1% and 22% intelligibility. When the audio and video signals are combined and presented in synchrony, intelligibility climbs to an average of 63%. When the audio signal leads the video, intelligibility declines appreciably for even the shortest asynchrony of 40 ms, falling to an asymptotic level of performance for asynchronies of approximately 120 ms and longer. In contrast, when the video signal leads the audio, intelligibility remains relatively stable for onset asynchronies up to 160-200 ms. Hence, there is a marked asymmetry in the integration of audio and visual information that has important implications for sensory-based models of auditory-visual speech processing.\n",
    ""
   ]
  },
  "cathiard01_avsp": {
   "authors": [
    [
     "M.A.",
     "Cathiard"
    ],
    [
     "Jean-Luc",
     "Schwartz"
    ],
    [
     "C.",
     "Abry"
    ]
   ],
   "title": "Asking a naive question about the McGurk effect: Why does audio [b] give more [d] percepts with visual [g] than with visual [d]?",
   "original": "av01_138",
   "page_count": 5,
   "order": 25,
   "p1": "138",
   "pn": "142",
   "abstract": [
    "Why does audio [b] give more [d] percepts with visual [g] than with visual [d], as in the present classical McGurk experiment? An explanation given for this asymmetry could be a language bias towards [d]. Contrary to what is sometimes taken for granted in the lipreading literature, visual [g] does not give more [d] than [g] responses. In fact [d] and [g] are neither visemically, nor auditorily equivalent. They are fully distinguishable in audio as well as in vision, where 80% correct identifications are current in laboratory conditions, as in the present experiment. We show here that in spite of these highly differenciating scores, FLMP modelling can quite surprisingly account for such an asymmetry by tuning very small remaining values; which is highly unsatisfactory. We suggest another explanation for this asymmetry which could be grounded on brain mechanisms dedicated to the computation of auditory and visual mouth opening movements, i.e. audio movement and visual velocity detectors and integrators, dedicated to the bimodal integration of place targets in speech.\n",
    ""
   ]
  },
  "mccotter01_avsp": {
   "authors": [
    [
     "M.V.",
     "McCotter"
    ],
    [
     "T.R.",
     "Jordan"
    ]
   ],
   "title": "Investigating the role of luminance boundaries in visual and audiovisual speech recognition using line drawn faces",
   "original": "av01_143",
   "page_count": 6,
   "order": 26,
   "p1": "143",
   "pn": "148",
   "abstract": [
    "Two experiments are reported which investigate the contribution of luminance boundaries to visual and audiovisual speech perception using colour, grey scale and line drawn talking faces. Use of line drawn faces should isolate basic luminance boundaries, while removing the distribution of luminance from the face. Unimodal auditory and visual syllables were combined to produce congruent (matching) and incongruent (McGurk) speech stimuli. Visual speech presented in line drawn faces was highly recognisable and influenced perception of congruent and incongruent auditory speech. However, visual speech presented in colour and grey scale faces was slightly more accurate and influential on perception of auditory speech perception than visual speech presented in line drawn faces. In light of these findings, the role of luminance boundaries in perception of visual and audiovisual speech is discussed.\n",
    ""
   ]
  },
  "ortegallebaria01_avsp": {
   "authors": [
    [
     "M.",
     "Ortega-Llebaria"
    ],
    [
     "A.",
     "Faulkner"
    ],
    [
     "Valerie",
     "Hazan"
    ]
   ],
   "title": "Auditory-visual L2 speech perception: Effects of visual cues and acoustic-phonetic context for Spanish learners of English",
   "original": "av01_149",
   "page_count": 6,
   "order": 27,
   "p1": "149",
   "pn": "154",
   "abstract": [
    "This study was designed to identify English speech contrasts that might be appropriate for the computer-based auditory-visual training of Spanish learners of English. It examines auditory-visual and auditory consonant and vowel confusions by Spanish speaking students of English and a native English control group. 36 Spanish listeners were tested on their identification of 16 consonants and 9 vowels of British English. For consonants, both L2 learners and controls showed significant improvements in the audiovisual condition, with larger effects for syllable final consonants. The patterns of errors by L2 learners were strongly predictable from our knowledge of the relation between the phoneme inventories of Spanish and English. Consonant confusions which were language-dependent - mostly errors in voicing and manner - were not reduced by the addition of visual cues whereas confusions that were common to both listener groups and related to acoustic-phonetic sound characteristics did show improvements. Spanish listeners did not use visual cues that disambiguated contrasts that are phonemic in English but have allophonic status in Spanish. Visual features therefore have different weights when cueing phonemic and allophonic distinctions.\n",
    ""
   ]
  },
  "burnham01_avsp": {
   "authors": [
    [
     "Denis",
     "Burnham"
    ],
    [
     "Susanna",
     "Lau"
    ],
    [
     "Helen",
     "Tam"
    ],
    [
     "Colin",
     "Schoknecht"
    ]
   ],
   "title": "Visual discrimination of cantonese tone by tonal but non-Cantonese speakers, and by non-tonal language speakers",
   "original": "av01_155",
   "page_count": 6,
   "order": 28,
   "p1": "155",
   "pn": "160",
   "abstract": [
    "A previous study by the first two authors suggests there is visual information for tone perception: under certain conditions Cantonese speakers are able to identify spoken words as one of six Cantonese words differing only in tone on the basis of lip and face movements at a rate better than chance [1]. Here, non-native (tonal, Thai, and non-tonal, English) language speakers were tested on a discrimination version of this task in three modes: auditory-visual (AV), auditory only (AO), and visual only (VO). Auditory stimuli were presented either in the clear or accompanied by noise. In all conditions, even VO, performance was significantly better than chance. In the clear, both English and Thai perceivers performed better in the AO and AV conditions than in the VO condition. With auditory noise added, Thai perceivers performed better in the AV than the AO condition. The results support the existence of visual information for tone, and show that this is available in the absence of experience with the language in question, and even in the absence of experience with the lexical use of tone.\n",
    ""
   ]
  },
  "hardison01_avsp": {
   "authors": [
    [
     "Debra M.",
     "Hardison"
    ]
   ],
   "title": "Bimodal word identification: effects of modality, speech style, sentence and phonetic/visual context",
   "original": "av01_161",
   "page_count": 6,
   "order": 29,
   "p1": "161",
   "pn": "166",
   "abstract": [
    "In this study, the gating paradigm was extended to the visual modality of speech to investigate the influence of speech style on the information value of visual cues (lip movements) in spoken word identification. Two female native speakers (NSs) of American English (AE) participated in separate videotaped conversations (unscripted speech) with the author.  Sentences were selected, transcribed, and later reproduced on videotape by the talkers as scripted speech. Through digital editing, target words were gated (using two-frame gates) and then presented to NSs for identification. Variables were modality of presentation: auditory-visual (AV) vs. auditory-only (A-only), speech style (unscripted vs. scripted), condition (sentence context vs. excised word), word length (1 or 2 syllables), and initial consonant visual category. Results revealed significantly earlier identification in AV presentation, with context, and in scripted speech but interactions varied between talkers. For both, there was a significant interaction between initial consonant, speech style and modality. Findings highlight the talker- and context-dependent nature of bimodal spoken language processing and contribute to an understanding of the influence of speech style and visual cues in word identification in connected speech.\n",
    ""
   ]
  },
  "tiippana01_avsp": {
   "authors": [
    [
     "K.",
     "Tiippana"
    ],
    [
     "M.",
     "Sams"
    ],
    [
     "T. S.",
     "Andersen"
    ]
   ],
   "title": "Visual attention influences audiovisual speech perception",
   "original": "av01_167",
   "page_count": 5,
   "order": 30,
   "p1": "167",
   "pn": "171",
   "abstract": [
    "The purpose of this study was to investigate whether integration of audiovisual speech occurs automatically so that information from heard speech and seen articulatory movements of the talking face are combined without any voluntary effort. The McGurk effect, where seeing discrepant visual speech changes the auditory speech percept, was used as a tool since it reflects the extent of audiovisual integration. The McGurk effect was measured in two conditions which manipulated the subjects' attention to visual speech. In both conditions, the subjects' task was to attend to auditory speech and report what the talker said. In the 'Attend Face' condition, subjects were instructed to pay attention also to the talking face, presented in synchrony with auditory speech. In the 'Ignore Face' condition, subjects were instructed to ignore the talking face and to pay attention to a visual distractor presented in the same location as the face. The proportion of auditory responses was higher in the latter condition, indicating that the influence of visual speech was weaker when the face was not attended. This result suggests that integration of audiovisual speech is not entirely automatic. The mechanism underlying this attentional effect was investigated by fitting the Fuzzy Logical Model of Perception (FLMP) [1] to the results, and the good fit of the model implies that attention influences unimodal information processing before integration across modalities takes place.\n",
    ""
   ]
  },
  "andersen01_avsp": {
   "authors": [
    [
     "T.S.",
     "Andersen"
    ],
    [
     "K.",
     "Tiippana"
    ],
    [
     "J.",
     "Lampinen"
    ],
    [
     "M.",
     "Sams"
    ]
   ],
   "title": "Modeling of audiovisual speech perception in noise",
   "original": "av01_172",
   "page_count": 5,
   "order": 31,
   "p1": "172",
   "pn": "176",
   "abstract": [
    "We present three models of audiovisual speech perception at varying signal-to-noise ratios (SNR). The first model is Massaro's Fuzzy Logical Model of Perception (FLMP)1 applied at each SNR. The second model imposes the constraint that the visual response probabilities are the same regardless of the SNR. Both models describe the data well. Root Mean Squared Error (RMSE) corrected for the numbers of degrees of freedom was smaller for the latter model. In concordance, cross-validated paired t-test showed that the latter model was significantly better at predicting individual performance despite the lower number of parameters. In a third model - a weighted FLMP - the SNR is parameterized reducing the number of free parameters substantially. This model fits the data significantly worse than the other two models, but does capture salient features of the change in performance with varying SNR.\n",
    ""
   ]
  },
  "potamianos01_avsp": {
   "authors": [
    [
     "Gerasimos",
     "Potamianos"
    ],
    [
     "Chalapathy",
     "Neti"
    ]
   ],
   "title": "Automatic speechreading of impaired speech",
   "original": "av01_177",
   "page_count": 6,
   "order": 32,
   "p1": "177",
   "pn": "182",
   "abstract": [
    "We investigate the use of visual, mouth-region information in improving automatic speech recognition (ASR) of the speech impaired. Given the video of an utterance by such a subject, we first extract appearance-based visual features from the mouth region-of-interest, and we use a feature fusion method to combine them with the subject's audio features into bimodal observations. Subsequently, we adapt the parameters of a speaker-independent, audio-visual hidden Markov model, trained on a large database of hearing subjects, to the audio-visual features extracted from the speech impaired videos. We consider a number of speaker adaptation techniques, and we study their performance in the case of a single speech impaired subject uttering continuous read speech, as well as connected digits. For both tasks, maximum-a-posteriori adaptation followed by maximum likelihood linear regression performs the best, achieving a word error rate relative reduction of 61% and 96%, respectively, over unadapted audio-visual ASR, and a 13% and 58% relative reduction over audio-only speaker-adapted ASR. In addition, we compare audio-only and audio-visual speaker-adapted ASR of the single speech impaired subject to ASR of subjects with normal speech, over a wide range of audio channel signal-to-noise ratios. Interestingly, for the small-vocabulary connected digits task, audio-visual ASR performance is almost identical across the two populations.\n",
    ""
   ]
  },
  "berthommier01_avsp": {
   "authors": [
    [
     "Frederic",
     "Berthommier"
    ]
   ],
   "title": "Audio-visual recognition of spectrally reduced speech",
   "original": "av01_183",
   "page_count": 6,
   "order": 33,
   "p1": "183",
   "pn": "188",
   "abstract": [
    "Perceptual experiments on audio-visual consonant recognition based on the spectral reduction of the speech (SRS) have been carried out with coherent and incoherent (McGurk) audiovisual pairs. The main interest of SRS in four sub-bands is to have a partial suppression of the information transmitted for the place of articulation. The integration of manner, restricted to the fricative/occlusive contrast, is also of concern, and a new 'cross-manner' combination is tested. As expected, we have a good audio-visual complementarity for SRS and a high amount of McGurk responses, but new interesting effects are observed. For the interpretation of human confusion about place of articulation, the Bayesian model proposed by Massaro and Stork [8] is compared to a new place identification model which is based on averaging as well as on the separate identification of articulatory features. This decomposition is a promising way for the development of multi-stream speech recognition models.\n",
    ""
   ]
  },
  "heckmann01_avsp": {
   "authors": [
    [
     "Martin",
     "Heckmann"
    ],
    [
     "Frederic",
     "Berthommier"
    ],
    [
     "Kristian",
     "Kroschel"
    ]
   ],
   "title": "A hybrid ANN/HMM audio-visual speech recognition system",
   "original": "av01_189",
   "page_count": 6,
   "order": 34,
   "p1": "189",
   "pn": "194",
   "abstract": [
    "In this paper we present a system for audio-visual speech recognition based on a hybrid Artificial Neural Network/Hidden Markov Model (ANN/HMM) approach. To setup the system it was necessary to record a new audio-visual database. We will describe the recording and labeling of the database. The fusion of audio and video data is a key aspect of the paper. Three conditions, when only the audio or only the video data is reliable and when they are both equally reliable, will attract our attention. A method to combine the video and audio information based on these three conditions will be presented. An implementation of this method in an automatic fusion depending on the noise level in the audio channel is developed. The performance of the complete system is demonstrated using two types of additive noise at varying SNR.\n",
    ""
   ]
  },
  "patterson01_avsp": {
   "authors": [
    [
     "E. K.",
     "Patterson"
    ],
    [
     "S.",
     "Gurbuz"
    ],
    [
     "Z.",
     "Tufekci"
    ],
    [
     "J. N.",
     "Gowdy"
    ]
   ],
   "title": "Noise-based audio-visual fusion for robust speech recognition",
   "original": "av01_195",
   "page_count": 4,
   "order": 35,
   "p1": "195",
   "pn": "198",
   "abstract": [
    "A major goal of current speech recognition research is to improve the robustness of recognition systems used in noisy environments. Recent strides in computing technology have allowed consideration of systems that use visual information to augment the decision capability of the recognizer, allowing superior performance in these difficult environments. A crucial area of research in audiovisual speech recognition is how to combine the separate modes of information. Late integration, an approach whereby separate audio-based and video-based decisions are made and then combined \"late\" in the process, has emerged as one of the simplest yet most effective techniques. Research has suggested that the fusion method for this technique (and similar methods such as multi-stream HMMs) is affected somewhat by the level of interfering audio noise. This paper further defines the relationship between data fusion in the presence of audio noise and demonstrates that optimal data fusion can only be performed if both the noise level and type are considered.\n",
    ""
   ]
  },
  "bothe01_avsp": {
   "authors": [
    [
     "Hans-Heinrich",
     "Bothe"
    ]
   ],
   "title": "LIPPS - A visual telephone for hearing-impaired",
   "original": "av01_199a",
   "page_count": 0,
   "order": 36,
   "p1": "199",
   "pn": "",
   "abstract": [
    "This paper describes an approach for the creation of natural audio-video speech signals from an acoustic speech signal or from a text input. It converts speech signals, which are taken from a microphone input, into a facial animation on the computer screen, displaying a static background image with movements of the mouth region and of the eyes. The video output is synchronized with the acoustic signal. LIPPS can also display a facial animation, which is driven by keyboard or text file input. Two applications are at hand, an intelligent audio-video telephone terminal and a training aid for lipreading. A prototype system has been implemented.\n",
    ""
   ]
  },
  "calvert01_avsp": {
   "authors": [
    [
     "G. A.",
     "Calvert"
    ],
    [
     "M. J.",
     "Brammer"
    ],
    [
     "Ruth",
     "Campbell"
    ]
   ],
   "title": "Cortical substrates of seeing speech: still and moving faces",
   "original": "av01_199b",
   "page_count": 0,
   "order": 37,
   "p1": "199",
   "pn": "",
   "abstract": [
    "Seen speech is dynamically produced and is usually perceived as a natural movement sequence. However, it can (also) be processed as sequences of prototype images of mouth positions (eg closed lips for bilabials, specific mouth shapes for point vowels - that is as stilled images. Using a novel paradigm controlling carefully for movement artifacts between still frames, we present fMRI evidence for partially dissociable cortical networks for reading speech from stilled and moving faces.\n",
    ""
   ]
  },
  "kabisch01_avsp": {
   "authors": [
    [
     "Bjorn",
     "Kabisch"
    ],
    [
     "Carol",
     "Nisch"
    ],
    [
     "Eckart R.",
     "Straube"
    ],
    [
     "Ruth",
     "Campbell"
    ]
   ],
   "title": "Development of a completely computerized McGurk design under variation of the signal to noise ratio",
   "original": "av01_199c",
   "page_count": 0,
   "order": 38,
   "p1": "199",
   "pn": "",
   "abstract": [
    "Empirical findings show that speech perception is not only based on auditory input but also on visual modality (lip movement). One prominent phenomenon to demonstrate the power of intermodal information processing is the \"McGurk Effect\", which occurs if incongruent auditory and visual information are given. Participants often report hearing syllables, which were presented neither auditory nor visually. A computer based version of this test was created. Contrary to the results of the studies with English-speaking participants, the German subjects showed almost no \"fusions\" (e.g. auditory \"ABA\" and visually \"AGA\" results in \"ADA\") were found. In a series of three consecutive experiments, diminishing signal to noise ratio decreased the rate of correct identifications of acoustic as well as visual information. Whereas increasing the noise level led to increasing combinations, the fusion rate was nearly unchanged. We are currently comparing using this task to compare schizophrenic patients to controls.\n",
    ""
   ]
  },
  "stiefelhagen01_avsp": {
   "authors": [
    [
     "Rainer",
     "Stiefelhagen"
    ],
    [
     "Jie",
     "Yang"
    ],
    [
     "Alex",
     "Waibel"
    ]
   ],
   "title": "Estimating focus of attention based on gaze and sound",
   "original": "av01_200a",
   "page_count": 0,
   "order": 39,
   "p1": "200",
   "pn": "",
   "abstract": [
    "Estimating a person's focus of attention is useful for various human-computer interaction applications, such as smart meeting rooms, where a user's goals and intent have to be monitored. In work presented here, we are interested in modeling focus of attention in a meeting situation.  We have developed a system capable of estimating participants' focus of attention from multiple cues. We employ an omni- directional camera to simultaneously track participants' faces around a meeting table and use neural networks to estimate their head poses. In addition, we use microphones to detect who is speaking. The system predicts participants' focus of attention from acoustic and visual information separately, and then combines the output of the audio- and video-based focus of attention predictors. We have evaluated the system using the data from three recorded meetings. The acoustic information has provided 8% error reduction on average compared to using a single modality.\n",
    ""
   ]
  },
  "wojdel01_avsp": {
   "authors": [
    [
     "Jacek C.",
     "Wojdel"
    ],
    [
     "Leon J.M.",
     "Rothkrantz"
    ]
   ],
   "title": "Obtaining person-independent feature space for lip reading",
   "original": "av01_200b",
   "page_count": 0,
   "order": 40,
   "p1": "200",
   "pn": "",
   "abstract": [
    "A person-independent representation of the lip-movements is crucial in developing a multimodal speech recognizer. The geometric models used in most lip-tracking techniques can remove some of the features such as skin texture or color, and appropriate normalization of the data and it's projection in the principal components space can reduce the amount of person-specific features even further. Although using Principal Component Analysis (PCA) of the multi-person dataset reveals some interesting features, the inter-person variation is too big to allow for robust speech recognition. There are, however, substantial similarities in the lip-shape variations when analyzing only single-person data sets. We propose to use an adaptive PCA that updates the projection coefficients with respect to the data available for the specific person.\n",
    ""
   ]
  },
  "cohen01_avsp": {
   "authors": [
    [
     "Michael M.",
     "Cohen"
    ],
    [
     "Rashid",
     "Clark"
    ],
    [
     "Dominic W.",
     "Massaro"
    ]
   ],
   "title": "Animated speech: research progress and applications",
   "original": "av01_200c",
   "page_count": 0,
   "order": 41,
   "p1": "200",
   "pn": "",
   "abstract": [
    "We describe recent progress we have made in facial animation and visible speech synthesis. We have added new internal structures (three-dimensional teeth, hard palate, and higher resolution tongue with a larger number of controls). These structures and the outside of the face are being trained on measurements of real faces, using optotrak, electropalatography, and ultrasound. We have developed an algorithm to warp Baldi into new shapes, and controlling these faces in animated speech. Finally, we are adding resolution to Baldi's wireframe to allow more precise control and more realistic animation. These development efforts and their evaluation will be described.\n",
    ""
   ]
  }
 },
 "sessions": [
  {
   "title": "Visible Speech for Animation and Speechreading by Humans",
   "papers": [
    "lidestam01_avsp",
    "auerjr01_avsp",
    "ellis01_avsp",
    "schwartz01_avsp",
    "kroos01_avsp",
    "kshirsagar01_avsp"
   ]
  },
  {
   "title": "Brain Activation in Auditory Visual Processing",
   "papers": [
    "bohning01_avsp",
    "auerjr01b_avsp",
    "callan01_avsp",
    "bernstein01_avsp",
    "colin01_avsp",
    "nicholson01_avsp"
   ]
  },
  {
   "title": "Facial Animation and Visual Speech Synthesis",
   "papers": [
    "lin01_avsp",
    "pelachaud01_avsp",
    "theobald01_avsp",
    "arb01_avsp",
    "elisei01_avsp",
    "morishima01_avsp"
   ]
  },
  {
   "title": "Correlates of Auditory and Visual Speech",
   "papers": [
    "bernstein01b_avsp",
    "kim01_avsp",
    "goecke01_avsp",
    "ekvall01_avsp",
    "kim01b_avsp",
    "grant01_avsp"
   ]
  },
  {
   "title": "Auditory Visual Speech Perception by Humans",
   "papers": [
    "cathiard01_avsp",
    "mccotter01_avsp",
    "ortegallebaria01_avsp",
    "burnham01_avsp",
    "hardison01_avsp",
    "tiippana01_avsp"
   ]
  },
  {
   "title": "Auditory Visual Speech Recognition by Humans and by Machine",
   "papers": [
    "andersen01_avsp",
    "potamianos01_avsp",
    "berthommier01_avsp",
    "heckmann01_avsp",
    "patterson01_avsp"
   ]
  },
  {
   "title": "Poster Presentations (no full papers available)",
   "papers": [
    "bothe01_avsp",
    "calvert01_avsp",
    "kabisch01_avsp",
    "stiefelhagen01_avsp",
    "wojdel01_avsp",
    "cohen01_avsp"
   ]
  }
 ]
}