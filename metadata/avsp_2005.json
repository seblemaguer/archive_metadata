{
 "location": "British Columbia, Canada",
 "startDate": "24/7/2005",
 "endDate": "27/7/2005",
 "conf": "AVSP",
 "year": "2005",
 "name": "avsp_2005",
 "series": "AVSP",
 "SIG": "AVISA",
 "title": "Auditory-Visual Speech Processing",
 "title1": "Auditory-Visual Speech Processing",
 "date": "24-27 July 2005",
 "papers": {
  "bavelas05_avsp": {
   "authors": [
    [
     "Janet Beavin",
     "Bavelas"
    ]
   ],
   "title": "Appreciating face-to-face dialogue",
   "original": "av05_001",
   "page_count": 1,
   "order": 1,
   "p1": "1",
   "pn": "",
   "abstract": [
    "Many scholars agree that face-to-face dialogue is the basic format of language use. Our program of research focuses on two unique dimensions of face-to-face dialogue: (i) communicative use of hand gestures, facial displays, and eye gaze and (ii) reciprocity and mutual influence between the participants. These two features often work together in ways that require new theories and research methods. Results of our research will be illustrated through microanalysis of a brief videotaped interaction that demonstrates the complexity and precision of even ordinary face-to-face dialogues.\n",
    ""
   ]
  },
  "payan05_avsp": {
   "authors": [
    [
     "Yohan",
     "Payan"
    ]
   ],
   "title": "How to model face and tongue biomechanics for the study of speech production?",
   "original": "av05_071",
   "page_count": 1,
   "order": 2,
   "p1": "71",
   "pn": "72",
   "abstract": [
    "Speech is a motor task during which speaker communicate a message using temporal variation of an acoustic signal that is shaped by orofacial gestures, while listeners perceive this message using both the auditory perception of the acoustic signal and the vision of the articulatory gestures. In order to study this motor task, scientists try to collect and to analyze a large amount of kinematic data measured on the final effectors such as face, jaw, tongue, velum, and vocal folds. These observations are considered to characterize the behavior of the peripheral motor system during different tasks and under variable conditions, and their interpretation largely contributed to the elaboration of major theories of motor control. Nevertheless, one must keep in mind that peripheral signals are in fact the result of a often complicated interaction between the motor control system and the peripheral apparatus. Both systems do contribute to shape the measured signals. In order to properly infer motor control strategies form these signals, it is important to assess the possible contribution of the physical systems. In this framework, an interesting approach, which is complementary to the experimental one, consists in building up physical models of the peripheral apparatus, in controlling them according to specific motor control model, and in comparing the obtained simulations with experimental data.\n",
    "This talk will focus on the physical modeling of two speech articulators, namely the face and the tongue, which behaviors are very specific compared to other articulators. Indeed, facial and lingual tissues are non isotropic and deform themselves at high velocities. The biomechanical modeling of such tissues is therefore very complex and requires a strong attention, especially in the framework described above, i.e. the use of realistic physical models of speech articulators in order to evaluate motor control strategies. A short review of biomechanical models proposed in the literature will be exposed, with a more specific focus onto facial biomechanics (an important concern for the AVSP community). Then, the physically based methods used to model soft tissues will be presented and discussed. In this framework, the Finite Element model of the face we have introduced in the context of computer-aided maxillo facial surgery will be described and its use for speech production will be discussed.\n",
    ""
   ]
  },
  "hannam05_avsp": {
   "authors": [
    [
     "Alan G.",
     "Hannam"
    ]
   ],
   "title": "Structure and function in the human jaw",
   "original": "av05_151",
   "page_count": 1,
   "order": 3,
   "p1": "151",
   "pn": "",
   "abstract": [
    "The lower jaw is a light, beam-like structure. When loaded by muscle tensions, it can bend parasagittally and laterally, and twist longitudinally. Its essentially-frictionless articulations with the skull normally work in compression while allowing condylar pitch, jaw, roll and translation. The upper part of the jaw motion envelope is constrained by dental contact, while the lower is shaped by active, and passive (viscoelastic) muscle tensions. While co-contraction can add significantly to the jaw's stiffness, mandibular resting posture is likely maintained by both muscle thixotropy and some fluctuating, active tone. The masticatory muscles function differently than those in the limbs; the jaw closers are multipennated, spindle-innervated, and compartmentalized, whereas the openers are more parallel-fibred, devoid of spindles and are not compartmentalized. The closers function over shorter lengths than the openers. There are no reciprocal stretch reflexes. Jaw-opening and closing reflexes are brainstem-mediated, though longer-loop paths are probably involved in unloading reflexes. The brainstem is responsible for opening and closing reflex modulation during functional activity, and is also implicated in rhythm and pattern generation. Suprasegmental mechanisms include sites in the basal ganglia and motor cortex, and the system always produces bilateral descending drive. All normal jaw movements are strongly regulated by peripheral feedback from several sources, the periodontal innervation playing a particularly significant role regulating high dental forces. The pterygoid and temporal muscles are primary contributors to various postures and movements with horizontal components, though regions of the masseteric complex also function differentially during some tasks.\n",
    "This presentation offers an overview of these features of the functioning jaw. Though much information can be derived from direct studies in humans, recent dynamic models are providing insight into variables that are difficult or impossible to measure in vivo, and these aspects are also addressed.\n",
    ""
   ]
  },
  "mixdorff05_avsp": {
   "authors": [
    [
     "Hansjörg",
     "Mixdorff"
    ],
    [
     "Patavee",
     "Charnvivit"
    ],
    [
     "Denis K.",
     "Burnham"
    ]
   ],
   "title": "Auditory-visual perception of syllabic tones in Thai",
   "original": "av05_003",
   "page_count": 6,
   "order": 4,
   "p1": "3",
   "pn": "8",
   "abstract": [
    "This paper presents results concerning the use of visual cues in the perception of Thai tones. The lower part of a female speaker's face was recorded on digital video as she uttered 24 sets of syllabic tokens covering the five different tones of Thai. A perception study was conducted in which audio sound track alone; as well as audio plus video were presented to native Thai speakers who were required to decide which tone they perceived. Audio was presented in various conditions: clear, pink-noise masked at different SNR levels, and devoiced conditions using LPC resynthesis. Some subjects were presented with a video only, silent condition. In the devoiced and the clear audio conditions, there was little augmentation due to the addition of video. However, the addition of visual information significantly improved perception in the pink-noise masked condition, and the effect increased with decreasing SNR. Results on video only are close to chance suggesting that the improvement in noise-masked conditions is not due to additional information in the video per se, but rather to an effect of early integration of acoustic and visual cues facilitating auditory-visual speech perception.\n",
    ""
   ]
  },
  "massaro05_avsp": {
   "authors": [
    [
     "Dominic W.",
     "Massaro"
    ],
    [
     "Miguel",
     "Hidalgo-Barnes"
    ]
   ],
   "title": "Read my lips: an animated face helps communicate musical lyrics",
   "original": "av05_009",
   "page_count": 2,
   "order": 5,
   "p1": "9",
   "pn": "10",
   "abstract": [
    "Understanding the lyrics of many contemporary songs is difficult. Watching the talker's face improves speech understanding when the speech is degraded by noise or hearing difficulty. To explore whether the face can be similarly helpful in music, 34 phrases from the song ``The Pressman'' by Primus (1993) were played to thirteen college students. These phrases were aligned with Baldi, a computer-animated talking head. There were three presentation conditions: original audio, Baldi's mouthing of the lyrics, and the auditory lyrics aligned with Baldi. The students were asked to watch and listen and to type in as many words as they could understand. Performance was significantly better in the bimodal condition than the auditory condition, showing that visual information from the face contributes to the recognition of musical lyrics. The contribution of the face was somewhat small relative to that found in speech, however, and reasons for this difference remain to be determined.\n",
    ""
   ]
  },
  "ali05_avsp": {
   "authors": [
    [
     "Azra N.",
     "Ali"
    ],
    [
     "Ashraf",
     "Hassan-Haj"
    ],
    [
     "Michael",
     "Ingleby"
    ],
    [
     "Ali",
     "Idrissi"
    ]
   ],
   "title": "McGurk fusion effects in Arabic words",
   "original": "av05_011",
   "page_count": 6,
   "order": 6,
   "p1": "11",
   "pn": "16",
   "abstract": [
    "Our earlier work, on incongruent audiovisual segments embedded in English word- and phrase-stimuli, showed that McGurk fusion is sensitive to the phonological context of a segment. In this paper we describe the preparation and early testing of Arabic word stimuli with an incongruent phonetic segment. We confirm that in the context of Arabic words, fusion similar to that in English words still occurs, but is affected by differences in phonological contrasts between Arabic and English speech. In previous work on English, we have contrasted fusion in onset and coda sites at word-medial and word-edge positions. In Arabic this taxonomy of sites is rendered questionable by disagreements about syllabification and by the widespread occurrence of geminate consonants, which possibly behave as both coda and onset. We develop an improved site taxonomy, design a small lexicon of Arabic words that covers place and manner contrasts at all types of site, and outline some of the problems encountered when creating incongruent data for this lexicon. Finally, we report on fusion experiments using 10 participants that suggest there are no coda-onset distinctions in the mental models that mediate word perception amongst native Arabic speakers.\n",
    ""
   ]
  },
  "kim05_avsp": {
   "authors": [
    [
     "Jeesun",
     "Kim"
    ],
    [
     "Chris",
     "Davis"
    ],
    [
     "Guillaume",
     "Vignali"
    ],
    [
     "Harold",
     "Hill"
    ]
   ],
   "title": "A visual concomitant of the Lombard reflex",
   "original": "av05_017",
   "page_count": 5,
   "order": 7,
   "p1": "17",
   "pn": "22",
   "abstract": [
    "The aim of the study was to examine how visual speech (speech related head and face movement) might vary as a function of communicative environment. To this end, measurements of head and face movement were recorded for four talkers who uttered the same ten sentences in quiet and four types of background noise condition (Babble and White noise presented through ear plugs or loud speakers). These sentences were also spoken in a whisper. Changes between the normal and in-noise conditions were apparent in many of the Principal Components (PCs) of head and face movement. To simplify the analysis of differences between conditions only the first six movement PCs were considered. The strength and composition of the changes was variable. Large changes occurred for jaw and mouth movement, face expansion and contraction and head rotation in the Z axis. Minimal change occurred for PC3 (rigid head translation in the Z axis). Whispered speech showed many of the characteristics of speech produced in noise but was distinguished by a marked increase in head translation in the Z axis. Analyses of the correlation between auditory speech intensity and movement under the different production conditions also revealed a complex pattern of changes. The correlations between RMS speech energy and the PCs that involved jaw and mouth movement (PC1 and 2) increased markedly from the normal to in-noise production conditions. An increase in the RMS and movement correlation also occurred for head Z-rotation as a function of speaking condition. No increases were observed for the movement associated with head Z-translation, lip protrusion or mouth opening with face contraction. These findings suggest that the relationships underlying Audio-Visual speech perception may differ depending on how that speech was produced.\n",
    ""
   ]
  },
  "lees05_avsp": {
   "authors": [
    [
     "Nicole",
     "Lees"
    ],
    [
     "Denis K.",
     "Burnham"
    ]
   ],
   "title": "Facilitating speech detection in style!: the effect of visual speaking style on the detection of speech in noise",
   "original": "av05_023",
   "page_count": 6,
   "order": 8,
   "p1": "23",
   "pn": "28",
   "abstract": [
    "Speakers naturally modify the way they produce speech depending on the listening environment. A hyper-articulatory speaking style is typically employed when listening circumstances are difficult. In contrast, hypo-articulation is adopted when the elements for listening are favourable. Previous research suggests that the intentions of the speaker are realized in listener's recognition of spoken utterances [1]. Given recent findings that the sight of articulating faces increase the detectability of speech in noise [2, 3, 4, 5]; the present study investigates whether speech in noise is more detectable when the listener views hyper-articulated compared to hypo-articulated speech. Normal, Hyper- and Hypo-articulated styles of ``ba, bi, bu, da, ga, tha'' spoken by three speakers were paired with corresponding static images (auditory information only) or dynamic (audio-visual condition) visual articulations in a two interval (noise only/speech plus noise) forced choice detection task at three signal-to-noise ratios (0dB, -2dB, and -4dB). Seeing the articulating face provides a significant advantage for detecting speech in noise compared to auditory only presentation regardless of speaking style. Additionally, hyper-articulated speech offered a significant advantage over hypo-articulated speech suggesting that the amount of facial movement may modulate the AV facilitation effect in the detection of speech in noise.\n",
    ""
   ]
  },
  "swerts05_avsp": {
   "authors": [
    [
     "Marc",
     "Swerts"
    ],
    [
     "Emiel",
     "Krahmer"
    ]
   ],
   "title": "Cognitive processing of audiovisual cues to prominence",
   "original": "av05_029",
   "page_count": 2,
   "order": 9,
   "p1": "29",
   "pn": "30",
   "abstract": [
    "Speakers use both auditory markers (e.g., pitch accents, increased syllable durations, and visual markers (e.g., head nods and eyebrow movements, to indicate important words in an utterance. Auditory markers have stronger cue value for the observer visual ones, but visual markers also have a strong impact [1, 2]. Prominence judgement tasks with incongruent stimuli (utterances in which auditory and visual prominence markers are associated with different words) reveal that these lead to increased confusion among perceivers [3], and that such incongruencies are disliked, presumably because they are unnatural [4].\n",
    "This paper addresses two related questions regarding prominence perception: How important are facial compared to auditory features? Which facial areas are most important to signal prominent words?\n",
    "In a production experiment, native Dutch speakers produced a Dutch sentence, in a number of different conditions, each time with emphasis on a different word. A selection from these AV recordings was used for two perception experiments. In the first, AV recordings were manipulated such that auditory and visual accents were either congruent (on the same word) or incongruent (on different words). Speeded prominence judgement task results reveal that incongruent stimuli are processed more slowly than congruent stimuli, but only when participants perceived the auditory accented word as most prominent. Thus subjects are sensitive to visual information to prominence, even when they do not use this information in their actual choice. In the second perception experiment subjects were presented with production experiment materials in which sound and video were manipulated to create stimuli with monotonous pitch, but with a visual accent on either the first, second or third noun phrase. In addition, entire face, upper half, lower half, right half, or left half of the face were shown. Results show that the upper facial area has stronger cue value for prominence detection than the bottom part, and that the left part of the face is more important than the right part. We are currently exploring to what extent the results are due to localisation of speaker expressiveness [e.g., 5], or observer attentional effects [e.g., 6].\n",
    ""
   ]
  },
  "capek05_avsp": {
   "authors": [
    [
     "Cheryl M.",
     "Capek"
    ],
    [
     "Ruth",
     "Campbell"
    ],
    [
     "Mairead",
     "MacSweeney"
    ],
    [
     "Marc",
     "Seal"
    ],
    [
     "Dafydd",
     "Waters"
    ],
    [
     "Bencie",
     "Woll"
    ],
    [
     "Tony",
     "David"
    ],
    [
     "Philip",
     "McGuire"
    ],
    [
     "Mick",
     "Brammer"
    ]
   ],
   "title": "Reading speech and emotion from still faces: fMRI findings",
   "original": "av05_031",
   "page_count": 3,
   "order": 10,
   "p1": "31",
   "pn": "34",
   "abstract": [
    "Cognitive and neurocognitive models of face processing suggest that independent (modular) processing routes are engaged for different types of face processing tasks. This study used fMRI (1.5 T) to examine common and distinctive processing routes for the perception of speech and emotion from posed photographs. For speech categorization (`Is that a vowel or a consonant?'), there was bilateral activation in frontal and inferior temporal (including fusiform) regions. The emotion task (`Is the expression positive or negative?') also showed activation in inferior temporal and frontal regions, but this was right lateralized. While an incidental task (`Can you see her teeth?'), using the same pictures, activated identical regions for the emotion faces, this was not the case for speech, where activation was limited to fusiform regions. Emotionally expressive, but not speech face images, readily activate a right-lateralized temporo-frontal circuit, whatever the task demands. Speech categorization from still images recruits extensive frontal regions, bilaterally, confirming previous findings [1]. Unlike previous reports, superior temporal regions did not show significant levels of activation for speech faces, although right superior temporal sulcus (STS) was activated by the emotional faces.\n",
    ""
   ]
  },
  "jesse05_avsp": {
   "authors": [
    [
     "Alexandra",
     "Jesse"
    ],
    [
     "Dominic W.",
     "Massaro"
    ]
   ],
   "title": "Towards a lexical fuzzy logical model of perception: the time-course of audiovisual speech processing in word identification",
   "original": "av05_035",
   "page_count": 2,
   "order": 11,
   "p1": "35",
   "pn": "36",
   "abstract": [
    "This study investigates the time-course of information processing in both visual as well as in the auditory speech as used for word identification in face-to-face communication. It extends the limited previous research on this topic and provides a valuable database for future research in audiovisual speech perception. An evaluation of models of speech perception by ear and eye in their ability to account for the audiovisual gating data shows a superior role of the fuzzy logical model of perception (FLMP) [1] over additive models of perception. A new dynamic version of the FLMP seems to be a promising model to account for the complex interplay of perceptual and cognitive information in audiovisual spoken word recognition.\n",
    ""
   ]
  },
  "koreman05_avsp": {
   "authors": [
    [
     "Jacques",
     "Koreman"
    ],
    [
     "Georg",
     "Meyer"
    ]
   ],
   "title": "The integration of coarticulated segments in visual speech",
   "original": "av05_037",
   "page_count": 2,
   "order": 12,
   "p1": "37",
   "pn": "38",
   "abstract": [
    "The perception of phones in visible speech is affected by coarticulation. In this study, visual speech stimuli are created by concatenating an image of the articulatory target position of the vowels /i:,a:,u:/ with an image of the target position of the consonants /p,(textesh),k/. These phones differ in their labial articulations: /u:/ is the only rounded vowel, the close articulation of /i:/ is compatible with rounding, while the open articulation for /a:/ is not; /p/ is a labial consonant, /(textesh)/ has secondary labialization and /k/ only acquires coarticulatory rounding in the context of a rounded vowel. In this paper, the different effects of the consonant articulation on vowel perception are investigated, and vice versa. Differences between the phones in susceptibility to coarticulatory effects are confirmed, and it is shown that in the stimuli used here the perception of a phone is affected by its neighbor, particularly the rounding in an /u:k/ sequence can be interpreted as labiality of the consonant. Even if there is no coarticulatory rounding in /k/, because it was taken from an /i:/ or /a:/ context, the rounding of /u:/ can lead to the perception of labiality in the following consonant. Vice versa, the vowel /i:/ can be perceived as /u:/ when followed by a /k/ taken from an /u:/ context; for /a:/, this is less often the case. This shows that the labial articulation can be reinterpreted as a property of the neighboring phone (if it is compatible with its other articulatory properties), not just of the phone on which it appears.\n",
    ""
   ]
  },
  "jiang05_avsp": {
   "authors": [
    [
     "Jintao",
     "Jiang"
    ],
    [
     "Lynne E.",
     "Bernstein"
    ],
    [
     "Edward T.",
     "Auer Jr."
    ]
   ],
   "title": "Perception of congruent and incongruent audiovisual speech stimuli",
   "original": "av05_039",
   "page_count": 6,
   "order": 13,
   "p1": "39",
   "pn": "44",
   "abstract": [
    "Previous studies of audiovisual (AV) speech integration have used behavioral methods to examine perception of congruent and incongruent AV speech stimuli. Such studies have investigated responses to a relatively limited set of the possible incongruent combinations of AV speech stimuli. A central issue for examining a wider range of incongruent AV speech stimuli is developing a systematic method for alignment that will work with a wide variety of segments. In the present study, we investigated the use of three different landmarks (consonant-onset, vowel-onset, and minimum distance) for aligning incongruent AV stimuli. Acoustic /ba/ or /la/ syllables were dubbed onto eight visual Consonant-/a/ syllables that spanned different places and manners of articulation. The AV stimuli were presented to ten participants. Results indicated that the effect of alignment landmark was not significant. The distance measures were found to be related to visual influence. Acoustic /ba/ tokens were more influenced by visual stimuli than acoustic /la/ tokens. Visual influence on the acoustic /ba/ tokens was mainly of the McGurk-type and/or of voicing confusion; while visual influence on the acoustic /la/ tokens was mainly of the combination type (/ba/ + /la/ = /bla/).\n",
    ""
   ]
  },
  "ouni05_avsp": {
   "authors": [
    [
     "Slim",
     "Ouni"
    ],
    [
     "Michael M.",
     "Cohen"
    ],
    [
     "Hope",
     "Ishak"
    ],
    [
     "Dominic W.",
     "Massaro"
    ]
   ],
   "title": "Visual contribution to speech perception: measuring the intelligibility of talking heads",
   "original": "av05_045",
   "page_count": 2,
   "order": 14,
   "p1": "45",
   "pn": "46",
   "abstract": [
    "Animated agents are becoming increasingly frequent in research and applications in speech science. An important challenge is to evaluate the effectiveness of the agent in terms of the intelligibility of its visible speech. Sumby and Pollack (1954) proposed a metric to describe the benefit provided by the face relative to the auditory speech presented alone. We extend this metric to describe the benefit provided by a synthetic animated face relative to the benefit provided by a natural face. The validity of the metric is tested in a new experiment in which auditory speech is presented under 5 different noise levels and is paired with either our synthetic talker Baldi or a natural talker (the standard). A valid metric would allow direct comparisons across different experiments and would give measures of the benefit of a synthetic animated face relative to a natural face and how this benefit varies as a function of the type of synthetic face, the test items (e.g., syllables versus sentences, viseme class), different individuals, and applications.\n",
    ""
   ]
  },
  "walsh05_avsp": {
   "authors": [
    [
     "Michael",
     "Walsh"
    ],
    [
     "Stephen",
     "Wilson"
    ]
   ],
   "title": "An agent-based framework for auditory-visual speech investigation",
   "original": "av05_047",
   "page_count": 6,
   "order": 15,
   "p1": "47",
   "pn": "52",
   "abstract": [
    "This paper presents a framework for investigating the relationship between both the auditory and visual modalities in speech. This framework employs intentional agents to analyse multilinear bimodal representations of speech utterances in line with an extended computational phonological model.\n",
    ""
   ]
  },
  "callan05_avsp": {
   "authors": [
    [
     "Daniel E.",
     "Callan"
    ]
   ],
   "title": "Internal models differentially implicated in audiovisual perception of non-native vowel contrasts",
   "original": "av05_053",
   "page_count": 2,
   "order": 16,
   "p1": "53",
   "pn": "54",
   "abstract": [
    "This study investigates neural processes underlying audiovisual perception of a non-native vowel contrast using fMRI. Eight native Japanese and eight native English speakers participated in the study. The two-alternative forced choice task was to identify which vowel was present in the CVC English word presented under audiovisual (AV) and audio only (AO) conditions. Three sessions were conducted with different vowel contrasts; two native contrasts for Japanese ([o-e)], [o-i]), and one non-native ([o-U]). A block design was employed. Significantly greater activity between the native Japanese group and the native English group for the contrast of AV-AO for the non-native session minus the native sessions occurs in the cerebellum. A decrease in activity occurs in the left transverse temporal gyrus. Increased activity in the cerebellum is consistent with the hypothesis that audiovisual non-native phonetic perception utilizes internal models to a greater extent than audio processing alone or to audiovisual processing of native contrasts. Decreases of activity in the right transverse temporal gyrus suggests that multisensory integration may involve higher order mapping of phonetic categories.\n",
    ""
   ]
  },
  "chung05_avsp": {
   "authors": [
    [
     "Victor",
     "Chung"
    ],
    [
     "Nicole",
     "Mirante"
    ],
    [
     "Jolien",
     "Otten"
    ],
    [
     "Eric",
     "Vatikiotis-Bateson"
    ]
   ],
   "title": "Audiovisual processing of Lombard speech",
   "original": "av05_055",
   "page_count": 2,
   "order": 17,
   "p1": "55",
   "pn": "56",
   "abstract": [
    "Perception results are presented that address the role of Lombard speech in auditory and audiovisual speech perception. Basically, visual enhancement neutralizes the advantage of Lombard speech observed for auditory perception. It remains an open question whether or not Lombard speech is preferable for perception studies of speech in noise.\n",
    ""
   ]
  },
  "erdener05_avsp": {
   "authors": [
    [
     "V. Dogu",
     "Erdener"
    ],
    [
     "Denis K.",
     "Burnham"
    ]
   ],
   "title": "Development of auditory-visual speech perception in English-speaking children: the role of language-specific factors",
   "original": "av05_057",
   "page_count": 6,
   "order": 18,
   "p1": "57",
   "pn": "62",
   "abstract": [
    "It has been found that auditory-visual speech perception differs over languages, particularly between English and Japanese speakers. This difference emerges due to increased use of visual information by English speakers between 6 and 8 years [14]. This study investigates the linguistic factors that may cause changes in auditory-visual speech perception development. Children aged between 5 and 8 years were given tests of reading, articulation, language-specific speech perception and auditory-visual speech perception. The results show that only language-specific speech perception predicts auditory-visual speech perception. Possible reasons for this relationship are discussed.\n",
    ""
   ]
  },
  "hill05_avsp": {
   "authors": [
    [
     "Harold",
     "Hill"
    ],
    [
     "Eric",
     "Vatikiotis-Bateson"
    ]
   ],
   "title": "Using graphics to study the perception of speech-in-noise, and vice versa",
   "original": "av05_063",
   "page_count": 2,
   "order": 19,
   "p1": "63",
   "pn": "64",
   "abstract": [
    "This work aims to use the speech in noise task to assess talking head animations, and talking head animations to investigate the perception of speech-in- noise. The theoretical aim is to determine what visual information is important for speech, while the practical aim is to develop an effective talking head animation system adaptable to robots.\n",
    "The first experiment used the ``cuboid'', a deliberately abstract face. Head, jaw and mouth movement were presented separately and in combination. Results showed an advantage of mouth movement independent of the other factors. This shows that even an abstract structure can carry useful facial speech information and that mouth movement is an essential component.\n",
    "In this paper we test perception of facial speech using both a deliberately abstract structure and a more realistic head model. Two other experiments reported used ATR's in-house animation system [1] to look at the relative contribution of face and head movement. The first experiment replicated a combined head and face movement advantage [2]. A 2 Head Movement (present/absent) x 2 Face Movement (present/absent) experiment showed a main effect of face movement, but no effect of head movement or any interaction.\n",
    "We conclude that abstract faces can carry useful visual speech information and that, while mouth and face movement are primary, head and jaw movement do not interfere with and can help.\n",
    ""
   ]
  },
  "robert05_avsp": {
   "authors": [
    [
     "Vincent",
     "Robert"
    ],
    [
     "Brigitte",
     "Wrobel-Dautcourt"
    ],
    [
     "Yves",
     "Laprie"
    ],
    [
     "Anne",
     "Bonneau"
    ]
   ],
   "title": "Inter speaker variability of labial coarticulation with the view of developing a formal coarticulation model for French",
   "original": "av05_065",
   "page_count": 6,
   "order": 20,
   "p1": "65",
   "pn": "70",
   "abstract": [
    "Explaining the effects of labial coarticulation is a difficult problem that gave rise to many studies and models. Most of the time small corpora were exploited to design these models. In this paper we describe the realization and exploitation of a corpus with ten speakers. This corpus enables the most invariant labial features (protrusion, stretching and lip opening) to be established. Then we propose a formal prediction algorithm that relies on a standard phonetic description of French phonemes. We conducted a first evaluation of this algorithm that shows its relevancy.\n",
    ""
   ]
  },
  "lucey05_avsp": {
   "authors": [
    [
     "Patrick",
     "Lucey"
    ],
    [
     "David",
     "Dean"
    ],
    [
     "Sridha",
     "Sridharan"
    ]
   ],
   "title": "Problems associated with current area-based visual speech feature extraction techniques",
   "original": "av05_073",
   "page_count": 6,
   "order": 21,
   "p1": "73",
   "pn": "78",
   "abstract": [
    "Techniques such as principle component analysis (PCA), linear discriminant analysis (LDA) and the discrete cosine transform (DCT) have all been used to good effect in face recognition. As these techniques are able to compactly represent a set of features, researchers have sought to use these methods to extract the visual speech content for audio-visual speech recognition (AVSR). In this paper, we expose the problems of employing such techniques in AVSR by running some visual-only speech recognition experiments. The results of these experiments illustrate that current area-based feature extraction techniques are heavily dependent on the visual front-end, as well as being ineffective in decoupling adequate speech content from a speaker's mouth. As a potential solution, we introduce the concept of a free-parts representation, which may be able to circumvent many of these problems experienced by current area-based techniques.\n",
    ""
   ]
  },
  "potamianos05_avsp": {
   "authors": [
    [
     "Gerasimos",
     "Potamianos"
    ],
    [
     "Patricia",
     "Scanlon"
    ]
   ],
   "title": "Exploiting lower face symmetry in appearance-based automatic speechreading",
   "original": "av05_079",
   "page_count": 6,
   "order": 22,
   "p1": "79",
   "pn": "84",
   "abstract": [
    "Appearance-based visual speech feature extraction is being widely used in the automatic speechreading and audio-visual speech recognition literature. In its most common application, the discrete cosine transform (DCT) is utilized to compress the image of the speaker's mouth region-of-interest (ROI), and the highest energy spatial frequency components are retained as visual features. Good generalization performance of the resulting system however requires robust ROI extraction and its consistent normalization, designed to compensate for speaker headpose and other data variations. In general, one expects that the ROI - if correctly normalized - will be nearly laterally symmetric, due to the approximate symmetry of human faces. We thus argue that forcing lateral ROI symmetry can be beneficial to automatic speechreading, providing a mechanism to compensate for small face and mouth tracking errors, which would otherwise result to incorrect ROI normalization. In this paper, we propose to achieve such ROI symmetry indirectly, by considering the spatial frequency domain and exploiting the DCT properties. In particular, we propose to remove the odd frequency DCT components from the selected visual feature vector. We experimentally demonstrate that, in general, this approach does not hurt speechreading performance, while it reduces computation, since it results to less DCT features. In addition, for the same number of features, as in traditional DCT coefficient selection, the method results in significant speechreading improvements. For the connected-digit automatic speechreading experiments considered, and for low feature dimensionalities, such can reach up to 12% relative reduction in word error rate.\n",
    ""
   ]
  },
  "lucey05b_avsp": {
   "authors": [
    [
     "Simon",
     "Lucey"
    ],
    [
     "Patrick",
     "Lucey"
    ]
   ],
   "title": "Improved speech reading through a free-parts representation",
   "original": "av05_085",
   "page_count": 2,
   "order": 23,
   "p1": "85",
   "pn": "86",
   "abstract": [
    "Motivated by the success of free-parts based representations in face recognition [1] we have attempted to address some of the problems associated with applying such a philosophy to the task of speaker-independent automatic speech reading. Hitherto, a major problem with canonical area-based approaches in automatic speech reading is the intrinsic lack of training observations due to the visual speech modality's low sample rate and large variability in appearance. We believe a free-parts representation can overcome many of these limitations due to its natural ability to generalize by producing many observations from a single mouth image, whilst still preserving the ability to discriminate between various visual-speech units. This approach additionally requires a modification to traditional techniques employed for the estimation of hidden Markov Models (HMMs), whose resultant models we currently refer to as free-parts HMMs (FP-HMMs). Results will be presented on the CUAVE audio-visual speech database.\n",
    ""
   ]
  },
  "barcenas05_avsp": {
   "authors": [
    [
     "Edson",
     "Bárcenas"
    ],
    [
     "Mauricio",
     "Díaz"
    ],
    [
     "Rafael",
     "Carrillo"
    ],
    [
     "Ricardo",
     "Solano"
    ],
    [
     "Carolina",
     "Soto"
    ],
    [
     "Luis",
     "Valderrama"
    ],
    [
     "Javier",
     "Villegas"
    ],
    [
     "Pedro",
     "Vizcaya"
    ]
   ],
   "title": "A coding method for visual telephony sequences",
   "original": "av05_087",
   "page_count": 6,
   "order": 24,
   "p1": "87",
   "pn": "92",
   "abstract": [
    "Usually the design of a vector quantizer involves the minimization of a distortion measure such as the MSE. In this paper we present a new paradigm applied to the design of a visual telephony coding scheme: the synthesis of credible image sequences. In other words, the creation of smooth and coherent transitions between the images to be reproduced. This new paradigm requires the redefinition of the samples representative of each class in the Lloyd- Max algorithm. In our case the design criterion is the minimization of the maximum error within the class samples and their representative. The results obtained from the proposed method are compared to those obtained from the Lloyd-Max original algorithm.\n",
    "Video transmission over asynchronous networks without real time control frequently suffers from information losses that can cause the loss of entire images. The present paper introduces a method based on the interpolation of the received images to estimate the lost images. The interpolation uses the search of credible image sequences by means of the Viterbi algorithm and a modified sequence distance.\n",
    ""
   ]
  },
  "cisar05_avsp": {
   "authors": [
    [
     "Petr",
     "Cisar"
    ],
    [
     "Milos",
     "Zelezny"
    ],
    [
     "Zdenek",
     "Krnoul"
    ],
    [
     "Jakub",
     "Kanis"
    ],
    [
     "Jan",
     "Zelinka"
    ],
    [
     "Ludek",
     "Müller"
    ]
   ],
   "title": "Design and recording of Czech speech corpus for audio-visual continuous speech recognition",
   "original": "av05_093",
   "page_count": 4,
   "order": 25,
   "p1": "93",
   "pn": "96",
   "abstract": [
    "In this paper we describe the design, recording, and content of a large audio-visual speech database intended for training and testing of audio-visual continuous speech recognition systems. The UWB- 05-HSCAVC database contains high resolution video and quality audio data suitable for experiments on audio-visual speech recognition.\n",
    "The corpus consists of nearly 40 hours of audiovisual records of 100 speakers in laboratory conditions. The whole database was collected using static illumination. Recorded subjects were asked to remain static with almost no head movements. The whole corpus is annotated and pre-processed to be ready to use in audio-visual speech recognition experiments.\n",
    "The purpose of the corpus is to provide data for evaluation of visual speech parameterizations. The corpus pre-processing was designed for use with both image-based and contour-based visual speech parameterizations. The head-tracking is carried out and its output is provided with the database. User thus does not need to find region of interest, since this information is attached to each frame of the database records.\n",
    "The presented database is collected, annotated, and preprocessed and is ready to use for subsequent experiments on visual speech parameterizations.\n",
    ""
   ]
  },
  "dean05_avsp": {
   "authors": [
    [
     "David",
     "Dean"
    ],
    [
     "Patrick",
     "Lucey"
    ],
    [
     "Sridha",
     "Sridharan"
    ]
   ],
   "title": "Audio-visual speaker identification using the CUAVE database",
   "original": "av05_097",
   "page_count": 5,
   "order": 26,
   "p1": "97",
   "pn": "102",
   "abstract": [
    "The freely available nature of the CUAVE database allows it to provide a valuable platform to form benchmarks and compare research. This paper shows that the CUAVE database can successfully be used to test speaker identifications systems, with performance comparable to existing systems implemented on other databases. Additionally, this research shows that the optimal configuration for decision-fusion of an audio-visual speaker identification system relies heavily on the video modality in all but clean speech conditions.\n",
    ""
   ]
  },
  "xue05_avsp": {
   "authors": [
    [
     "Jianxia",
     "Xue"
    ],
    [
     "Jintao",
     "Jiang"
    ],
    [
     "Abeer",
     "Alwan"
    ],
    [
     "Lynne E.",
     "Bernstein"
    ]
   ],
   "title": "Consonant confusion structure based on machine classification of visual features in continuous speech",
   "original": "av05_103",
   "page_count": 6,
   "order": 27,
   "p1": "103",
   "pn": "108",
   "abstract": [
    "This study is a first step in selecting an appropriate subword unit representation to synthesize highly intelligible 3D talking faces. Consonant confusions were obtained with optic features from a 320-sentence database, spoken by a male talker, using Gaussian mixture models and maximum a posteriori classification methods. The results were compared to consonant confusions obtained from visual-only human perception tests of non-sense CV syllables spoken by the same talker. At the phoneme level, machine classification results for the continuous speech database had better performance than human perception with isolated syllables. However, the number of distinguishable consonant clusters by machine is less than that by humans. To model the optic feature for continuous visual speech synthesis, the results suggest that for most consonants, modeling optic feature in phoneme level is more appropriate than modeling in phoneme clusters determined from visual-only human perception tests. For some consonants, modeling in a context-dependent manner might be helpful in improving the modeling accuracy for the talker studied in this paper.\n",
    ""
   ]
  },
  "goecke05_avsp": {
   "authors": [
    [
     "Roland",
     "Goecke"
    ]
   ],
   "title": "3d lip tracking and co-inertia analysis for improved robustness of audio-video automatic speech recognition",
   "original": "av05_109",
   "page_count": 6,
   "order": 28,
   "p1": "109",
   "pn": "114",
   "abstract": [
    "Multimodality is a key issue in robust human-computer interaction. The joint use of audio and video speech variables has been shown to improve the performance of automatic speech recognition (ASR) systems. However, robust methods in particular for the real-time extraction of video speech features are still an open research area. This paper addresses the robustness issue of audio-video (AV) ASR systems by exploring a real-time 3D lip tracking algorithm based on stereo vision and by investigating how learned statistical relationships between the sets of audio and video speech variables can be employed in AV ASR systems. The 3D lip tracking algorithm combines colour information from each cameras' images with knowledge about the structure of the mouth region for different degrees of mouth openness. By using a calibrated stereo camera system, 3D coordinates of facial features can be recovered, so that the visual speech variable measurements become independent from the head pose. Multivariate statistical analyses enable the analysis of relationships between sets of variables. Co-inertia analysis is a relatively new method and has not yet been widely used in AVSP research. Its advantage is its superior numerical stability compared to other multivariate methods in the case of small sample size. Initial results are presented, which show how 3D video speech information and learned statistical relationships between audio and video speech variables can improve the performance of AV ASR systems.\n",
    ""
   ]
  },
  "dohen05_avsp": {
   "authors": [
    [
     "Marion",
     "Dohen"
    ],
    [
     "Hélène",
     "Loevenbruck"
    ],
    [
     "Harold",
     "Hill"
    ]
   ],
   "title": "A multi-measurement approach to the identification of the audiovisual facial correlates of contrastive focus in French",
   "original": "av05_115",
   "page_count": 2,
   "order": 29,
   "p1": "115",
   "pn": "116",
   "abstract": [
    "The aims of this study are twofold. We first seek to validate a previous study conducted on one speaker. It identified lower face visible articulatory correlates of contrastive focus in French for real (non-reiterant) speech. We thus conducted the same study for another speaker. Our second goal was to enlarge the set of cues measured to other facial movements. To do so we used a complementary measurement technique (Optotrak) on the same corpus. The articulatory measurements showed that there was a set of consistent visible articulatory correlates of contrastive focus in French across speakers: a) an increase in lip area and jaw opening on the focused item b) a lengthening of the focal syllables c) a post-focal hypo-articulation. However, there are also some speaker specific strategies. The first speaker showed an anticipation strategy but the second speaker did not. The Optotrak measurements confirmed that: a) focus corresponded to an increase in lip height and/or in lip width b) the post-focal sequence was hypo-articulated. We also found that: a) the focused item was often correlated with a slight head movement (nod) b) facial movements such as cheek movements were amplified when there was focus and c) eyebrow movements were not correlated with focus.\n",
    ""
   ]
  },
  "rubin05_avsp": {
   "authors": [
    [
     "Philip",
     "Rubin"
    ],
    [
     "Gordon",
     "Ramsay"
    ],
    [
     "Mark",
     "Tiede"
    ]
   ],
   "title": "The history of articulatory synthesis at Haskins laboratories",
   "original": "av05_117",
   "page_count": 2,
   "order": 30,
   "p1": "117",
   "pn": "118",
   "abstract": [
    "In the 1970s, Rubin, Baer and Mermelstein turned Mermelstein's vocal tract model into the first articulatory synthesizer (ASY) regularly used as a research tool for exploring the relationship between speech perception and production. This system was designed to allow for simple control of tract shape by direct manipulation of articulators. A significant extension is CASY, the configurable articulatory synthesizer, that lets users fit outlines of the vocal tract model to acquired sagittal images and represents fixed surfaces of the tract parametrically. Future developments include the integration of a new key articulator controlling parasagittal shape of the tongue dorsum motivated by volumetric MRI data and an improved voice source model. This paper will provide an overview of the history of articulatory synthesis at Haskins, including current work which aims to incorporate better models of biomechanics and fluid dynamics. The model will also be related to other approaches to articulatory synthesis.\n",
    ""
   ]
  },
  "fels05_avsp": {
   "authors": [
    [
     "Sidney",
     "Fels"
    ],
    [
     "Florian",
     "Vogt"
    ],
    [
     "Kees van den",
     "Doel"
    ],
    [
     "John E.",
     "Lloyd"
    ],
    [
     "Oliver",
     "Guenther"
    ]
   ],
   "title": "Artisynth: an extensible, cross-platform 3d articulatory speech synthesizer",
   "original": "av05_119",
   "page_count": 6,
   "order": 31,
   "p1": "119",
   "pn": "124",
   "abstract": [
    "We describe our progress on the construction of a combined 3D face and vocal tract simulator for articulatory speech synthesis called ArtiSynth. The architecture provides six main modules: (1) a simulator engine and synthesis framework, (2) a two and three-dimensional model development component, (3) a numerics engine, (4) a graphical renderer, (5) an audio synthesis engine and (6) a graphical user interface (GUI). We have created infrastructure for creating vocal tract models based on combinations of rigid body, spring-mass, and finite element models, and some parametric models. Our infrastructure provides mechanisms to ``glue'' these and other model types together to create hybrids. Dynamical models whose equations of motion are integrated numerically and animatable parametric models are combined in a single framework. Using ArtiSynth we have created a complex, dynamic jaw model based on muscle models, a parametric tongue model, a face model, two lip models, and a source-filter based acoustic model linked to the vocal tract model via an airway model. These have been connected together to form a complete vocal tract that produces speech and is drivable both by data and by dynamics.\n",
    ""
   ]
  },
  "elisei05_avsp": {
   "authors": [
    [
     "Frédéric",
     "Elisei"
    ],
    [
     "Gérard",
     "Bailly"
    ],
    [
     "Guillaume",
     "Gibert"
    ],
    [
     "Remi",
     "Brun"
    ]
   ],
   "title": "Capturing data and realistic 3d models for cued speech analysis and audiovisual synthesis",
   "original": "av05_125",
   "page_count": 6,
   "order": 32,
   "p1": "125",
   "pn": "130",
   "abstract": [
    "We have implemented a complete text-to-speech synthesis system by concatenation that addresses French Manual Cued Speech (FMCS). It uses two separate dictionaries, one for multimodal diphones with audio and facial articulation, and the other with the gestures between two consecutive FMCS keys (``dikeys''). Dictionaries were built from real data.\n",
    "This paper presents our methodology and the final results, illustrated by accompanying videos. We recorded and analyzed the 3D trajectories of 50 hand and 63 facial fleshpoints during the production of 238 utterances carefully designed to cover all possible diphones of French. Linear and non-linear statistical models of hand and face deformations and postures were developed using both separate and joint corpora. Additional data allowed us to capture the shape of the hand and face with a higher spatial density (2,600 points for the hand and forearm and 2,000 for the face), as well as their appearance. We succeeded in building new high-density articulated models that were compatible with the previous emerging set of control parameters. This allows the outputted synthesis parameters to drive the more realistic 3D models instead of the low-density ones.\n",
    ""
   ]
  },
  "kuratate05_avsp": {
   "authors": [
    [
     "Takaaki",
     "Kuratate"
    ]
   ],
   "title": "Statistical analysis and synthesis of 3d faces for auditory-visual speech animation",
   "original": "av05_131",
   "page_count": 6,
   "order": 33,
   "p1": "131",
   "pn": "136",
   "abstract": [
    "In this paper, we demonstrate a statistical approach for creating a 3D face from photographs by exploiting the face information gained from faces scanned into a large 3D face database. We also estimate facial expressions using this database, creating speech-related deformations used for talking head animation for auditory-visual speech research.\n",
    "The database has 9 different face postures from over two hundred people and is analyzed by principal component analysis (PCA). A small set of feature points from the face and the profile silhouette line extracted from front and side view photographs are used to create a novel 3D face from PCA results by linear estimation. Any new neutral 3D face can be easily represented by fifty eigen vectors obtained by this PCA, and its deformation characteristics can be estimated from faces in the database that are close to the input face in the eigen vector space. The estimated facial expressions are quite natural. This same method can also be applied to human-like faces such as those found in statues or dolls. Additional PCA of the estimated face postures can then be used to create 3D talking head animation.\n",
    ""
   ]
  },
  "sangari05_avsp": {
   "authors": [
    [
     "Sonia",
     "Sangari"
    ],
    [
     "Mustapha",
     "Skhiri"
    ],
    [
     "Bertil",
     "Lyberg"
    ]
   ],
   "title": "Computational model of some communication head movements in a speech act",
   "original": "av05_137",
   "page_count": 5,
   "order": 34,
   "p1": "137",
   "pn": "142",
   "abstract": [
    "Speech communication involves normally not only speech but also face and head movements. In the present investigation the visual correlates to focal accent and to confirmation in Swedish are studied and a computational model for the movements is hypothesized. The head movements are recorded with the Qualisys MacReflex motion tracking system simultaneously with the speech signal. The results confirm earlier results that the head movements that co-occur with the signalling of focal accent in the speech signal will have the extreme values at the primary stressed syllable of the word carrying focal accent independent of the word accent type in Swedish. The nodding that is signalling confirmation is signalled by means of a damped oscillation of the head. The head movements in both cases may be simulated by a second order linear system and the different patterns are two of the three possible solutions to the equations.\n",
    ""
   ]
  },
  "vogt05_avsp": {
   "authors": [
    [
     "Florian",
     "Vogt"
    ]
   ],
   "title": "Finite element modeling of the tongue",
   "original": "av05_143",
   "page_count": 2,
   "order": 35,
   "p1": "143",
   "pn": "144",
   "abstract": [
    "There is an increased need to study the human oropharyngeal anatomy for research in speech production, feeding motion, and breathing activities. Computational anatomic models are also useful to conduct virtual experiments without extensive use of human subjects, to plan and train surgeries. Recent improved methods for fast and high resolution imaging of internal organs, such as, Magnetic Resonance Imaging (MRI), three-dimensional Ultrasound, and Computer Tomography (CT) combined with automatic image extraction techniques, dynamic modeling techniques, and increased computation capacity, enhance possibilities to create realistic computational models.\n",
    "One central organ of the vocal tract is the tongue, which has been modeled using statistical parametric models [1, 2], explicit shape descriptions [3, 4], physiological models [5], and dynamical models (both spring-mass [6], and finite element models [7, 8, 9]). A recent survey [10] describes existing methods in detail. For many surgical simulation applications, tongue models need to support the following:\n",
    "Two and three-dimensional coherent models; Separate tissue and muscle behavior;  Dynamic interaction with other anatomical parts like jaw, palate;  Large deformations;  Non-linear tissue properties;  Real-time simulation;  Registration to different individuals.\n",
    "For these requirements, finite element modeling provides a good solution and has a long tradition in Engineering [11, 12]. So far, only non real-time solutions have been developed for tongue models [7, 8, 13, 9]. Recent developments in the fields of physical-based animation [14, 15] and surgical simulations [16] provide finite element algorithms which can run in real-time to provide plausible results even for large deformations. Compared to spring-mass models, they have better stability and accuracy.\n",
    "In this work, we will create a real-time two and three dimensional finite element model of tongue tissues and demonstrate a complete work flow from image data extraction to model configuration to model validation in the framework of ArtiSynth [17].\n",
    ""
   ]
  },
  "wrobeldautcourt05_avsp": {
   "authors": [
    [
     "Brigitte",
     "Wrobel-Dautcourt"
    ],
    [
     "M. O.",
     "Berger"
    ],
    [
     "B.",
     "Potard"
    ],
    [
     "Yves",
     "Laprie"
    ],
    [
     "Slim",
     "Ouni"
    ]
   ],
   "title": "A low-cost stereovision based system for acquisition of visible articulatory data",
   "original": "av05_145",
   "page_count": 6,
   "order": 36,
   "p1": "145",
   "pn": "150",
   "abstract": [
    "In this paper, we present the 3D acquisition infrastructure we developed for building a talking face and studying some aspects of visual speech. A short-term aim is to study coarticulation for the French language and to develop a model which respects a real talker articulation. One key factor is to be able to acquire a large amount of 3D data with a low-cost system more flexible than existing motion capture systems (using infrared cameras and glued markers).\n",
    "Our system only uses two standard cameras, a PC and painted markers that do not change speech articulation and provides a sufficiently fast acquisition rate to enable an efficient temporal tracking of 3D points. We present here our stereovision data capture system and how these data can be used in acoustic-to-articulatory inversion.\n",
    ""
   ]
  }
 },
 "sessions": [
  {
   "title": "Invited Lectures",
   "papers": [
    "bavelas05_avsp",
    "payan05_avsp",
    "hannam05_avsp"
   ]
  },
  {
   "title": "Human Perception and Processing of Auditory-Visual Speech",
   "papers": [
    "mixdorff05_avsp",
    "massaro05_avsp",
    "ali05_avsp",
    "kim05_avsp",
    "lees05_avsp",
    "swerts05_avsp",
    "capek05_avsp",
    "jesse05_avsp",
    "koreman05_avsp",
    "jiang05_avsp",
    "ouni05_avsp",
    "walsh05_avsp",
    "callan05_avsp",
    "chung05_avsp",
    "erdener05_avsp",
    "hill05_avsp",
    "robert05_avsp"
   ]
  },
  {
   "title": "Machine-based Recognition and Processing of Auditory-Visual Speech",
   "papers": [
    "lucey05_avsp",
    "potamianos05_avsp",
    "lucey05b_avsp",
    "barcenas05_avsp",
    "cisar05_avsp",
    "dean05_avsp",
    "xue05_avsp"
   ]
  },
  {
   "title": "The Production of Auditory-Visual Speech",
   "papers": [
    "goecke05_avsp",
    "dohen05_avsp",
    "rubin05_avsp",
    "fels05_avsp",
    "elisei05_avsp",
    "kuratate05_avsp",
    "sangari05_avsp",
    "vogt05_avsp",
    "wrobeldautcourt05_avsp"
   ]
  }
 ]
}