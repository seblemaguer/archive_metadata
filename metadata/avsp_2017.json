{
 "title": "The 14th International Conference on Auditory-Visual Speech Processing",
 "location": "Stockholm, Sweden",
 "startDate": "25/8/2017",
 "endDate": "26/8/2017",
 "URL": "https://avsp2017.loria.fr",
 "chair": "Chairs: Slim Ouni, Chris Davis, Alexandra Jesse, Jonas Beskow",
 "ISSN": "2308-975X",
 "conf": "AVSP",
 "year": "2017",
 "name": "avsp_2017",
 "series": "AVSP",
 "SIG": "AVISA",
 "title1": "The 14th International Conference on Auditory-Visual Speech Processing",
 "date": "25-26 August 2017",
 "papers": {
  "burnham17_avsp": {
   "authors": [
    [
     "Denis",
     "Burnham"
    ]
   ],
   "title": "Eric Vatikiotis-Bateson and the Birth of AVSP",
   "original": "1",
   "page_count": 5,
   "order": 1,
   "p1": 1,
   "pn": 5,
   "abstract": [
    "Eight years after Barbara Dodd and Ruth Campbell’s Hearing by Eye (1987), \"the first edited book on the psychology of lip-reading\", the NATO Advanced Studies Institute (ASI) Workshop, Speechreading by Man and Machine was held at the Chateau de Bonas, Castera-Verduzan, France. This two-week meeting was the first interdisciplinary meeting devoted to the subject of speechreading. It was a one of a kind. And it sowed the seeds for AVSP research to come. Forty-five researchers (from twelve countries), whose work covered topics such as human perception and cognition, linguistics, neuroscience, computer animation, machine learning and computer vision, held forth in a never-to-be-repeated cavalcade of science show and tell. What follows is first an episodic account of scenes from the meeting, that begins with the author’s initial encounter with Eric, from which the story shifts to the formation of the Audio-Visual Speech Association (AVISA), the establishment of the AVSP conferences, and the role that Eric played therein, and finally, it traces a few outlines of the bigger picture of Eric’s impact on AVSP.\n"
   ],
   "doi": "10.21437/AVSP.2017-1"
  },
  "massaro17_avsp": {
   "authors": [
    [
     "Dominic",
     "Massaro"
    ]
   ],
   "title": "The McGurk Effect: Auditory Visual Speech Perception’s Piltdown Man",
   "original": "25",
   "page_count": 6,
   "order": 26,
   "p1": 131,
   "pn": 136,
   "abstract": [
    "I draw an analogy of the McGurk effect to an episode in natural science. Piltdown Man was claimed to be the fossilized remains of a previously unknown early human. It took roughly 4 decades of controversy to conclusively learn that Piltdown Man was as a hoax because the natural scientists focused on the fossil of Piltdown Man rather than searching for other paleoanthropological evidence. I argue that the slow progress in understanding the McGurk effect is analogous because behavioral and speech scientists have not broadened their scope of inquiry much beyond the original McGurk finding. I then review a few representative examples of misguided research and theory that has resulted from this type of narrow inquiry. These include a hindering of the development of theoretical models, the belief that there are qualitative differences among individuals in terms of how they process auditory-visual speech, that different language communities process auditory-visual speech differently, and that speech is somehow special. To provide an alternative to the Piltdown Man approach, the Fuzzy Logical Model of Perception (FLMP) is briefly described to serve as a more appropriate paradigm for research and theoretical inquiry. The limitations of various neural measures are described, and when these limitations are surmounted, there appears to be some neural evidence for the independence of processing auditory and visual speech at the initial stage of speech processing.\n"
   ],
   "doi": "10.21437/AVSP.2017-25"
  },
  "mixdorff17_avsp": {
   "authors": [
    [
     "Hansjörg",
     "Mixdorff"
    ],
    [
     "Angelika",
     "Hönemann"
    ],
    [
     "Albert",
     "Rilliard"
    ],
    [
     "Tan",
     "Lee"
    ],
    [
     "Matthew",
     "Ma"
    ]
   ],
   "title": "Cross-Language Perception of Audio-visual Attitudinal Expressions",
   "original": "23",
   "page_count": 6,
   "order": 24,
   "p1": 119,
   "pn": 124,
   "abstract": [
    "This paper presents results from a cross-language free labeling experiment employing short audio-visual utterances of Cantonese produced with varying attitudinal expressions. German perceivers were asked to specify a single word that best described these stimuli, some of which were presented in audio-only and video-only modality. The resulting terms were classified with respect to the emotional dimensions of valence, activation and dominance, as well as the linguistic dimension of assertion/interrogation. We compared the results with the outcomes from a similar experiment employing German stimuli. Most types of attitudes presented in Cantonese were rated less positive and portrayed with less activation than those presented by Germans. Video-supported stimuli yielded significantly higher activation and dominance levels than audio-only ones. The main dimensions separating expressions are assertive vs. interrogation, valence, and dominance. Illocutionary strength is associated with the perceived activation, and primarily linked to the visual channel, while linguistic information is primarily conveyed by acoustic cues, but of course only for the German stimuli.\n"
   ],
   "doi": "10.21437/AVSP.2017-23"
  },
  "hoenemann17_avsp": {
   "authors": [
    [
     "Angelika",
     "Hoenemann"
    ],
    [
     "Petra",
     "Wagner"
    ]
   ],
   "title": "Facial activity of attitudinal speech in German",
   "original": "24",
   "page_count": 6,
   "order": 25,
   "p1": 125,
   "pn": 130,
   "abstract": [
    "The current study is a continuation of previous work on how attitudes are expressed in speech. Generally, we found that the availability of facial expression (together with speech) leads to a higher plausibility of an expressed attitude. Therefore, a further analysis of facial cues in the expression of attitudinal states appears useful. For this purpose, we selected a subset of sixteen attitudes portrayed by five performers, who have shown to be most convincing in their portrayal of attitudinal states. The Facial Action Coding System (FACS) is then applied on the output of OpenFace, which automatically detects the presence and the intensity of Action Units using a computer vision algorithm. Based on detected Action Units (Aus), a cluster analysis on one representative frame for each video is carried out. Furthermore, a linear discriminant analysis of the clusters is carried out including the whole dataset. The analysis shows that based on two linear discriminants, no clear separation of the cluster groups is possible. Some of the results for the facial expressions can be explained by their corresponding attitudes' distribution in emotional space. Not surprisingly, we also found that the facial expressions are strongly influenced by their phonetic content, thus, the attitudinal expression related to lip movements may be difficult to detect in speech.\n"
   ],
   "doi": "10.21437/AVSP.2017-24"
  },
  "yamamoto17_avsp": {
   "authors": [
    [
     "Hisako W.",
     "Yamamoto"
    ],
    [
     "Misako",
     "Kawahara"
    ],
    [
     "Akihiro",
     "Tanaka"
    ]
   ],
   "title": "The developmental path of multisensory perception of emotion and phoneme in Japanese speakers",
   "original": "20",
   "page_count": 4,
   "order": 21,
   "p1": 105,
   "pn": 108,
   "abstract": [
    "The purpose of this study is to investigate the developmental path of multisensory perception of emotion and phoneme in Japanese speakers. In Experiment 1, Japanese children from age of 5 to 12 and adults were engaged in Emotion perception task, in which speakers’ emotions expressed in their face and voice were congruent or incongruent, and Phoneme perception task, in which auditory and visual phonemic information are congruent or incongruent (i.e., McGurk type movie). Children’s judgement weighting on voice information in emotion perception increased over age, whereas the weighting of audiovisual information in phoneme perception remained the same during childhood. Interestingly, adults’ emotion perception based on voice information was less than those of 11-12-year-olds. Experiment 2 examined whether adults’ multisensory perception was affected by their parenting experience. The results showed that parents who were rearing their children judged speakers’ emotion relying on voice information less than non-parents, whereas visual influence in phoneme perception was not different between parents and non- parents; adults’ multisensory perception may be affected by daily interaction with their children as for emotion judgement. Taken together, these results show differential integration processes between emotion and phoneme perception.\n"
   ],
   "doi": "10.21437/AVSP.2017-20"
  },
  "kawahara17_avsp": {
   "authors": [
    [
     "Misako",
     "Kawahara"
    ],
    [
     "Disa",
     "Sauter"
    ],
    [
     "Akihiro",
     "Tanaka"
    ]
   ],
   "title": "Impact of Culture on the Development of Multisensory Emotion Perception",
   "original": "21",
   "page_count": 6,
   "order": 22,
   "p1": 109,
   "pn": 114,
   "abstract": [
    "Recent studies have demonstrated that multisensory emotion perception is modulated by culture. Tanaka et al. (2010) showed that Japanese people are more tuned than Dutch people to vocal processing in adults. The present study investigated how such a cultural difference develops in children aged 5-12 years. In the experiment, a face and a voice, expressing either congruent or incongruent emotions, were presented simultaneously on each trial. Participants judged whether the person is happy or angry. The results showed that the rate of vocal responses was higher in Japanese than Dutch in adults, especially when in-group speakers expressed a happy face with an angry voice. The rate of vocal responses was low in both Japanese and Dutch 5-6- year-olds, while it increased over age only in Japanese participants. These results suggest that combinations of facial and vocal emotions have specific meanings and that culture- specific multisensory display rules are acquired with age in childhood.\n"
   ],
   "doi": "10.21437/AVSP.2017-21"
  },
  "websdale17_avsp": {
   "authors": [
    [
     "Danny",
     "Websdale"
    ],
    [
     "Ben",
     "Milner"
    ]
   ],
   "title": "Using visual speech information and perceptually motivated loss functions for binary mask estimation",
   "original": "9",
   "page_count": 6,
   "order": 9,
   "p1": 41,
   "pn": 46,
   "abstract": [
    "This work is concerned with using deep neural networks for estimating binary masks within a speech enhancement frame- work. We first examine the effect of supplementing the audio features used in mask estimation with visual speech information. Visual speech is known to be robust to noise although not necessarily as discriminative as audio features, particularly at higher signal-to-noise ratios. Furthermore, most DNN approaches to mask estimate use the cross-entropy (CE) loss function which aims to maximise classification accuracy. However, we first propose a loss function that aims to maximise the hit minus false-alarm (HIT-FA) rate of the mask, which is known to correlate more closely to speech intelligibility than classification accuracy. We then extend this to a hybrid loss function that combines both the CE and HIT-FA loss functions to pro- vide a balance between classification accuracy and HIT-FA rate of the resulting masks. Evaluations of the perceptually motivated loss functions are carried out using the GRID and larger RM-3000 datasets and show improvements to HIT-FA rate and ESTOI across all noises and SNRs tested. Tests also found that supplementing audio with visual information into a single bi- modal audio-visual system gave best performance for all measures and conditions tested.\n"
   ],
   "doi": "10.21437/AVSP.2017-9"
  },
  "fort17_avsp": {
   "authors": [
    [
     "Mathilde",
     "Fort"
    ],
    [
     "Núria",
     "Sebastián-Gallés"
    ]
   ],
   "title": "Impact of early bilingualism on infants’ ability to process talking and non-talking faces: new data from 9-month-old infants",
   "original": "abs2",
   "page_count": 1,
   "order": 27,
   "p1": 137,
   "pn": 137,
   "abstract": [
    "",
    "When watching a face talking in their native language, infants from 8 months onwards pay more attention to the mouth rather than to the eyes of the speaker [1]. During the same period, bilingual infants show a preference for the mouth region of the speaker earlier in their development (at 4-month-old) and this preference remains unchanged later on (at least until 12 month-old, [2]).\nHere, we investigated whether this preference for the talker’s mouth makes it more difficult for infants to detect and/or learn to anticipate the apparition of a visual event displayed in the talker’s eyes region. In a previous study using the same paradigm, we showed that at 15-months of age, both monolingual and bilingual 15-month-olds could detect the apparition of a visual cue appearing in the eyes region but only 15-month-old monolinguals and 18-month-old bilinguals could learn to anticipate its appearance during the sentence phase [3]. One possible explanation for this result is that at 15 months of age, bilinguals, as compared to their monolingual peers, need to rely more on the cues provided by the mouth region of the speaker to cope for their challenging language environment.\nUsing the same paradigm, we tested whether at a younger age (10-month-olds), both monolingual and bilingual infants, who are less expert to process their native language (and may thus rely more on the mouth region of the talker), fail to anticipate the visual cue in the eyes region. We thus recorded Spanish/Catalan bilingual and Spanish and Catalan monolingual 9-month-olds’ eye gazes while they watched and listened to a bilingual Spanish/Catalan female speaker producing short sentences (Speech Event). At the end of each sentence she produced a Non-Speech movement. For a first group of infants, she systematically raised her eyebrows (Eyebrow-raise condition: N=21, 12 bilinguals) while for the second group, she systematically protruded her lips (Lip-protrusion condition: N=18, 7 bilinguals).\nThe results show that during the presentation of the Speech Event (cf. Figure 1, left panel), no effect of the Non-speech movement was found, suggesting that neither bilinguals nor monolinguals could anticipate its apparition (both p > .05). Interestingly, during the presentation of the Non-Speech movement itself (cf. Figure 1, right panel), only bilinguals were able to change their pattern of exploration of the face of the speaker: they gazed at the eyes region of the speaker during the Eyebrow-raise movement while they paid more attention to the mouth area during the Lip-protrusion movement (p < .05). However, monolinguals were not able to do so (p > .05). Data collection is still underway but our results, taken together with previous findings [2-3] demonstrate the impact of language\nspecific experience (e.g., early bilingualism) on infant’s ability to process from social entities they encounter on a daily-basis (i.e., audiovisual talking faces). We consider the respective roles of selective attention maturation and vocabulary acquisition to explain these different developmental trajectories.\n"
   ]
  },
  "cullen17_avsp": {
   "authors": [
    [
     "Ailbhe",
     "Cullen"
    ],
    [
     "Naomi",
     "Harte"
    ]
   ],
   "title": "Thin slicing to predict viewer impressions of TED Talks",
   "original": "12",
   "page_count": 6,
   "order": 12,
   "p1": 58,
   "pn": 63,
   "abstract": [
    "Many paralinguistic challenges have looked at predicting affect, speaker state, or other attributes from short segments of speech of less than a minute. There are situations however, where we want to predict how a user might label a talk or lecture of significantly longer duration. For example, would a viewer find a given talk funny? The question then is how to map long talks to single word labels? In this paper, we rely on the concept of thin slicing, which states that humans make similar judgements on short segments of speech as they do on longer segments. We wish to find short segments that are representative of the talk, which can be used to predict the user label. We explore this concept in order to predict user ratings of TED talks as inspiring, persuasive, and funny. In particular, we pose two questions. The first is how thin can we make our slices? Results show that longer slices, of up to a minute in duration are more useful for the prediction of viewer ratings. We also ask where the best position to slice the video is? We compare the performance of classification based on slices extracted from fixed points to that of slices extracted from salient regions, and find that prediction accuracy can be improved by choosing slices according to the speaker’s vocal behaviour or the audience’s reactions.\n"
   ],
   "doi": "10.21437/AVSP.2017-12"
  },
  "jesse17_avsp": {
   "authors": [
    [
     "Alexandra",
     "Jesse"
    ],
    [
     "Paul",
     "Saba"
    ]
   ],
   "title": "Learning to recognize unfamiliar talkers from the word-level dynamics of visual speech",
   "original": "27",
   "page_count": 5,
   "order": 29,
   "p1": 144,
   "pn": 148,
   "abstract": [
    "Familiar speakers can be recognized from seeing their idiosyncratic realization of visual speech [1], suggesting the long-term storage of facial dynamic signatures for speakers. Frameworks of face perception postulate that these facial dynamic signatures are only available for familiar speakers, but not for unfamiliar speakers. Our recent work has shown [2], however, that participants can rapidly learn to recognize unfamiliar speakers from the dynamic information contained in their visual speech when uttering sentences. While sentences can inform about the talker-specific realization of prosody, rate, and phonetic detail, words primarily provide information about variation in fine-phonetic detail, leading listeners to focus on different types of idiosyncrasies in these two types of materials when learning about auditory voices [3]. The present study tested whether representations of facial dynamic signatures can be formed from seeing the phonetic detail contained in words being uttered in isolation. Participants were trained to recognize two speakers from the dynamic information provided by point- light displays of isolated words. Feedback was given during training. At test, participants were tested on the point-light displays presented during training and on point-light displays of new words. Participants learned to recognize speakers from the word-level dynamics of visual speech independent of linguistic content.\n"
   ],
   "doi": "10.21437/AVSP.2017-27"
  },
  "stoltmann17_avsp": {
   "authors": [
    [
     "Katarzyna",
     "Stoltmann"
    ],
    [
     "Susanne",
     "Fuchs"
    ]
   ],
   "title": "The influence of handedness and pointing direction on deictic gestures and speech interaction: Evidence from motion capture data on Polish counting-out rhymes",
   "original": "5",
   "page_count": 5,
   "order": 5,
   "p1": 21,
   "pn": 25,
   "abstract": [
    "Does handedness influence pointing gestures? Within the scope of our study, we investigated the influence of handedness, in this case dominant vs. non-dominant hand, on pointing gestures in Polish counting-out rhymes under two conditions: fast and normal speech. For this, pointing gestures of the index finger were recorded with a motion capture system. Speech acoustics was recorded simultaneously. We hypothesized that a detailed analysis of individual gestures would reveal different kinematic patterns for the dominant versus non-dominant hand. Moreover, we expected that pointing towards the addressee would differ from pointing towards the origo (the speaker), since the pointing hand is part of the origo and different muscle synergies are involved. Results of our study revealed shorter duration of pointing gestures and higher peak velocities under time pressure. Higher velocity peaks were also found for pointing with the dominant hand in comparison to the non-dominant hand, in particular while pointing to the addressee. These findings suggest that hand gestures adapt to temporal constraints, and provide first insights that handedness and pointing direction have an impact on kinematic properties of deictic gestures.\n"
   ],
   "doi": "10.21437/AVSP.2017-5"
  },
  "barone17_avsp": {
   "authors": [
    [
     "Pascal",
     "Barone"
    ],
    [
     "Mathieu",
     "Marx"
    ],
    [
     "Anne",
     "Lasfargues-Delannoy"
    ]
   ],
   "title": "Processing of visuo-auditory prosodic information in cochlear-implanted patients deaf patients",
   "original": "16",
   "page_count": 5,
   "order": 17,
   "p1": 84,
   "pn": 88,
   "abstract": [
    "Linguistic prosody is a poorly treated subject in cochlear- implanted (CI) patients. Our study investigated how CI patients discriminate a question from a statement based only on linguistic prosodic cues in three conditions: visual, auditory, and visuo-auditory. The results demonstrate that CI patients are not better performers than normal-hearing subjects (NHS) in the visual only condition, but that they have better multi-sensory integration skills than controls. During audiovisual stimulation, CI patients fuse the auditory and visual information to enable a better discrimination of prosodic cues. This study confirms the importance of further research into CI patients prosodic discrimination skills notably concerning visual prosody and eye-tracking analysis.\n"
   ],
   "doi": "10.21437/AVSP.2017-16"
  },
  "tiippana17_avsp": {
   "authors": [
    [
     "Kaisa",
     "Tiippana"
    ],
    [
     "Ilmari",
     "Kurki"
    ],
    [
     "Tarja",
     "Peromaa"
    ]
   ],
   "title": "Applying the summation model in audiovisual speech perception",
   "original": "28",
   "page_count": 5,
   "order": 30,
   "p1": 149,
   "pn": 153,
   "abstract": [
    "Discrimination thresholds for consonants [k] and [p] were measured with the method of constant stimuli for auditory, visual and audiovisual speech stimuli. Summation in audiovisual thresholds was assessed using Minkowski metric. When Minkowski metric exponent k is 1, summation is linear. When k=2, summation is quadratic. As k increases, summation decreases. We found that k was 1.7 across 16 participants, indicating strong summation. There was some individual variation, k values being generally between 1 and 2. These findings confirm that multisensory enhancement is substantial in audiovisual speech perception. They also suggest that the amount of summation is not the same in all individuals.\n"
   ],
   "doi": "10.21437/AVSP.2017-28"
  },
  "kawase17_avsp": {
   "authors": [
    [
     "Marina",
     "Kawase"
    ],
    [
     "Ikuma",
     "Adachi"
    ],
    [
     "Akihiro",
     "Tanaka"
    ]
   ],
   "title": "Multisensory Perception of Emotion for Human and Chimpanzee Expressions by Humans",
   "original": "22",
   "page_count": 4,
   "order": 23,
   "p1": 115,
   "pn": 118,
   "abstract": [
    "We examined how we human perceive multimodal affective expressions in chimpanzees and whether the underlying cognitive systems are similar to those for human expressions. In the experiment, we presented audiovisual stimuli in which face and voice express congruent or incongruent emotions. Participants were instructed to ignore vocal emotion but to judge facial emotion. The results showed that in the chimpanzee stimuli, accuracy was marginally lower in the incongruent stimuli than in the congruent stimuli. Moreover, the congruency effect on positive human faces was marginally stronger than on positive chimpanzee faces. The gaze behavior results showed that human participants focused around the eye area when presented with positive and negative facial expressions of humans and negative facial expression of chimpanzees, while they focused both around the eye and the mouth areas when presented with positive facial expressions of chimpanzees. Our findings suggest that humans perceive affective expressions of other species multisensorily, and that underlying cognitive systems are not similar to those for humans.\n"
   ],
   "doi": "10.21437/AVSP.2017-22"
  },
  "ambrazaitis17_avsp": {
   "authors": [
    [
     "Gilbert",
     "Ambrazaitis"
    ],
    [
     "David",
     "House"
    ]
   ],
   "title": "Acoustic features of multimodal prominences: Do visual beat gestures affect verbal pitch accent realization?",
   "original": "17",
   "page_count": 6,
   "order": 18,
   "p1": 89,
   "pn": 94,
   "abstract": [
    "",
    "The interplay of verbal and visual prominence cues has attracted recent attention, but previous findings are inconclusive as to whether and how the two modalities are integrated in the production and perception of prominence. In particular, we do not know whether the phonetic realization of pitch accents is influenced by co-speech beat gestures, and previous findings seem to generate different predictions.\nIn this study, we investigate acoustic properties of prominent words as a function of visual beat gestures in a corpus of read news from Swedish television. The corpus was annotated for head and eyebrow beats as well as sentence- level pitch accents. Four types of prominence cues occurred particularly frequently in the corpus: (1) pitch accent only, (2) pitch accent plus head, (3) pitch accent plus head plus eyebrows, and (4) head only. The results show that (4) differs from (1-3) in terms of a smaller pitch excursion and shorter syllable duration. They also reveal significantly larger pitch excursions in (2) than in (1), suggesting that the realization of a pitch accent is to some extent influenced by the presence of visual prominence cues. Results are discussed in terms of the interaction between beat gestures and prosody with a potential functional difference between head and eyebrow beats.\n"
   ],
   "doi": "10.21437/AVSP.2017-17"
  },
  "liu17_avsp": {
   "authors": [
    [
     "Li",
     "Liu"
    ],
    [
     "Gang",
     "Feng"
    ],
    [
     "Denis",
     "Beautemps"
    ]
   ],
   "title": "Inner Lips Parameter Estimation based on Adaptive Ellipse Model",
   "original": "15",
   "page_count": 6,
   "order": 16,
   "p1": 78,
   "pn": 83,
   "abstract": [
    "In this paper, a novel automatic method using an adaptive ellipse model to estimate inner lips parameters (inner lips width A and height B) of speakers without any artifices is presented. Color based image processing is first applied to delimit preliminary inner lips. A single discontinuity elimination combining horizontal and vertical filling are used to obtain a binary inner lips image as complete as possible. After the pre- processing steps, an optimal adaptive ellipse is determined to match the inner lips, giving A and B parameters. The proposed method is evaluated on 4693 images of three French speakers including one Cued Speech (CS) speaker. It obtains RMSE of 3.37 mm for A parameter and of 0.84 mm for B parameter which outperform the baseline of inner lips parameter estimation in the state of the art. Moreover, CS recognition based on 34 French phonemes shows that using the estimated two parameters achieves an accuracy which is comparable to that using raw lips ROI.\n"
   ],
   "doi": "10.21437/AVSP.2017-15"
  },
  "kroos17_avsp": {
   "authors": [
    [
     "Christian",
     "Kroos"
    ],
    [
     "Rikke",
     "Bundgaard-Nielsen"
    ],
    [
     "Catherine",
     "Best"
    ],
    [
     "Mark D.",
     "Plumbley"
    ]
   ],
   "title": "Using deep neural networks to estimate tongue movements from speech face motion",
   "original": "7",
   "page_count": 6,
   "order": 7,
   "p1": 30,
   "pn": 35,
   "abstract": [
    "This study concludes a tripartite investigation into the indirect visibility of the moving tongue in human speech as reflected in co-occurring changes of the facial surface. We were in particular interested in how the shared information is distributed over the range of contributing frequencies. In the current study we examine the degree to which tongue movements during speech can be reliably estimated from face motion using artificial neural networks. We simultaneously acquired data for both movement types; tongue movements were measured with Electro- magnetic Articulography (EMA), face motion with a passive marker-based motion capture system. A multiresolution analysis using wavelets provided the desired decomposition into frequency subbands. In the two earlier studies of the project we established linear and non-linear relations between lingual and facial speech motions, as predicted and compatible with previous research in auditory-visual speech. The results of the cur- rent study using a Deep Neural Network (DNN) for prediction show that a substantive amount of variance can be recovered (between 13.9 and 33.2% dependent on the speaker and tongue sensor location). Importantly, however, the recovered variance values and the root mean squared error values of the Euclidean distances between the measured and the predicted tongue trajectories are in the range of the linear estimations of our earlier study.\n"
   ],
   "doi": "10.21437/AVSP.2017-7"
  },
  "sterpu17_avsp": {
   "authors": [
    [
     "George",
     "Sterpu"
    ],
    [
     "Naomi",
     "Harte"
    ]
   ],
   "title": "Towards Lipreading Sentences with Active Appearance Models",
   "original": "14",
   "page_count": 6,
   "order": 14,
   "p1": 70,
   "pn": 75,
   "abstract": [
    "Automatic lipreading has major potential impact for speech recognition, supplementing and complementing the acoustic modality. Most attempts at lipreading have been performed on small vocabulary tasks, due to a shortfall of appropriate audio- visual datasets. In this work we use the publicly available TCD- TIMIT database, designed for large vocabulary continuous audio-visual speech recognition. We compare the viseme recognition performance of the most widely used features for lipread- ing, Discrete Cosine Transform (DCT) and Active Appearance Models (AAM), in a traditional Hidden Markov Model (HMM) framework. We also exploit recent advances in AAM fitting. We found the DCT to outperform AAM by more than 6% for a viseme recognition task with 56 speakers. The overall accuracy of the DCT is quite low (32-34%). We conclude that a fundamental rethink of the modelling of visual features may be needed for this task.\n"
   ],
   "doi": "10.21437/AVSP.2017-14"
  },
  "behne17_avsp": {
   "authors": [
    [
     "Dawn",
     "Behne"
    ],
    [
     "Marzieh",
     "Sorati"
    ],
    [
     "Magnus",
     "Alm"
    ]
   ],
   "title": "Perceived Audiovisual Simultaneity in Speech by Musicians and Nonmusicians: Preliminary Behavioral and Event-Related Potential (ERP) Findings",
   "original": "19",
   "page_count": 5,
   "order": 20,
   "p1": 100,
   "pn": 104,
   "abstract": [
    "In audiovisual simultaneity perception, audio-lead and video- lead are generally considered fundamentally different, despite both being occurrences of physical misalignment. The current study pursues this difference, in a preliminary study comparing musicians with non-musicians across audio-lead, synchronous and video-lead alignments in behavioral and ERP simultaneity judgment tasks. Results to date are consistent with the conclusion that musicians are more sensitive to audiovisual asynchrony than non-musicians, in particular for audio-lead, and highlight the role of experience in facilitating sensory processing.\n"
   ],
   "doi": "10.21437/AVSP.2017-19"
  },
  "nixon17_avsp": {
   "authors": [
    [
     "Jessie S.",
     "Nixon"
    ],
    [
     "Catherine T.",
     "Best"
    ]
   ],
   "title": "Acoustic cue variability affects eye movement behaviour during non-native speech perception: a GAMM model",
   "original": "2",
   "page_count": 6,
   "order": 2,
   "p1": 6,
   "pn": 11,
   "abstract": [
    "",
    "Participants in the ‘visual world’ paradigm simultaneously process both auditory and visual cues in order to match speech to target images. Previous research has shown that when native speakers listen to speech that has high within-category variability in the contrastive dimension, auditory perceptual uncertainty increases, resulting in increased looks to competitor objects. This suggests a cross-modal effect, where reduced reliability in the auditory domain leads to increased search for evidence in the visual domain.\nThe present study investigated the effects of within- category acoustic variability on eye movements during the acquisition of a new acoustic dimension not present in the native language, namely English speakers’ acquisition of lexical tone. All participants heard a bimodal distribution of stimuli, with distribution peaks at the prototypical pitch values for high and mid tones; however, presentation frequency differed between conditions: high-variance vs. low-variance. Based on previous research, we expected lower uncertainty and better learning in the low-variance condition.\nGAMM models of eye movement data showed that within- category acoustic variance increases perceptual uncertainty in the auditory domain and hinders acquisition of a cue dimension. The results also show a cross-modal effect: lower relibility in the auditory domain leads to increased search for cues in the visual domain, even when visual cues are held constant across conditions.\n"
   ],
   "doi": "10.21437/AVSP.2017-2"
  },
  "vinay17_avsp": {
   "authors": [
    [
     "Sandhya",
     "Vinay"
    ],
    [
     "Dawn",
     "Behne"
    ]
   ],
   "title": "The Influence of Familial Sinistrality on Audiovisual Speech Perception",
   "original": "6",
   "page_count": 4,
   "order": 6,
   "p1": 26,
   "pn": 29,
   "abstract": [
    "The present study investigates the influence of familial sinistrality on audiovisual speech perception for young adults. Incongruent video stimuli dubbed over dichotic and diotic audio stimuli were utilized. Participants’ responses for the right incongruent, left incongruent and video incongruent audiovisual stimuli were analyzed. Results indicated significantly higher proportion of fusion responses demonstrating increased audiovisual interaction in participants with familial sinistrality compared to those without familial sinistrality. Thus, the presence of familial sinistrality influences the role of visual information on the processing of audiovisual speech.\n"
   ],
   "doi": "10.21437/AVSP.2017-6"
  },
  "irwin17_avsp": {
   "authors": [
    [
     "Julia",
     "Irwin"
    ],
    [
     "Trey",
     "Avery"
    ],
    [
     "Jacqueline",
     "Turcios"
    ],
    [
     "Lawrence",
     "Brancazio"
    ],
    [
     "Barbara",
     "Cook"
    ],
    [
     "Nicole",
     "Landi"
    ]
   ],
   "title": "Atypical phonemic discrimination but not audiovisual speech integration in children with autism and the broader autism phenotype.",
   "original": "26",
   "page_count": 6,
   "order": 28,
   "p1": 138,
   "pn": 143,
   "abstract": [
    "When a speaker talks, the consequences of this can be heard (audio) and seen (visual). We use a novel visual phonemic restoration task to assess behavioral discrimination and neural signatures (ERP) of audiovisual processing in typically developing children with a range of social and communicative skill and in children with autism. In an auditory oddball design we presented two types of stimuli to the listener, a clear exemplar of an auditory consonant-vowel syllable /ba/ and a syllable in which the auditory cues for the consonant was substantially weakened, creating a stimulus which is more like /a/. All speech tokens were paired with a face producing /ba/ or a face with a pixelated mouth, effectively masking visual speech articulation. In this paradigm, the visual /ba/ should cause the auditory /a/ to be perceived as /ba/, creating an attenuated oddball response (phonemic restoration), but the pixelated video should not have this effect. Across two studies we observed behavioral and ERP effects that are consistent with phonemic restoration overall; however, autism-like traits and autism were associated with overall reductions in phonemic discrimination regardless of face context, suggesting that autism may be associated with impairments in speech processing but not AV speech integration.\n"
   ],
   "doi": "10.21437/AVSP.2017-26"
  },
  "ouni17_avsp": {
   "authors": [
    [
     "Slim",
     "Ouni"
    ],
    [
     "Sara",
     "Dahmani"
    ],
    [
     "Vincent",
     "Colotte"
    ]
   ],
   "title": "On the quality of an expressive audiovisual corpus: a case study of acted speech",
   "original": "11",
   "page_count": 5,
   "order": 11,
   "p1": 53,
   "pn": 57,
   "abstract": [
    "",
    "In the context of developing an expressive audiovisual speech synthesis system, the quality of the audiovisual corpus from which the 3D visual data will be extracted is important. In this paper, we present a perceptive case study on the quality of the expressiveness of a set of emotions acted by a semi-professional actor. We have analyzed the production of this actor pronouncing a set of sentences with acted emotions, during a human emotion-recognition task. We have observed different modalities: audio, real video, 3D-extracted data, as unimodal presentations and bimodal presentations (with audio). The results of this study show the necessity of such perceptive evaluation prior to further exploitation of the data for the synthesis system. The comparison of the modalities shows clearly what the emotions are, that need to be improved during production and how audio and visual components have a strong mutual influence on emotional perception.\n"
   ],
   "doi": "10.21437/AVSP.2017-11"
  },
  "nunnemann17_avsp": {
   "authors": [
    [
     "Eva Maria",
     "Nunnemann"
    ],
    [
     "Kirsten",
     "Bergmann"
    ],
    [
     "Helene",
     "Kreysa"
    ],
    [
     "Pia",
     "Knoeferle"
    ]
   ],
   "title": "Referential Gaze Makes a Difference in Spoken Language Comprehension: Human Speaker vs. Virtual Agent Listener Gaze",
   "original": "4",
   "page_count": 5,
   "order": 4,
   "p1": 16,
   "pn": 20,
   "abstract": [
    "An interlocutor's referential gaze is of great importance in face-to-face communication as it facilitates spoken language comprehension. People are also able to exploit virtual agent gaze in interactions. Our study addressed effects of human speaker gaze vs. virtual agent listener gaze on reaction times, accuracy and eye movements. Participants saw videos in which a static scene depicting three characters was visible on a screen. We manipulated: (1) whether the human speaker - uttering the sentence - was visible, (2) whether the agent listener was present and (3) whether the template following each video matched the scene. Eye movements were recorded as participants listened to German SVO sentences describing an inter- action between two of these three characters. After each trial a template schematically depicting three characters and the interaction appeared on screen. Participants verified a match be- tween sentence and template. Participants solved the matching task very well across all conditions. They responded faster to matches than mismatches between sentence and template. Participants were slower when the agent was present. Eye movement results suggest that while hearing the second noun phrase participants tended to look at its referent to a greater extent when the speaker was present compared to the other conditions.\n"
   ],
   "doi": "10.21437/AVSP.2017-4"
  },
  "koumparoulis17_avsp": {
   "authors": [
    [
     "Alexandros",
     "Koumparoulis"
    ],
    [
     "Gerasimos",
     "Potamianos"
    ],
    [
     "Youssef",
     "Mroueh"
    ],
    [
     "Steven J.",
     "Rennie"
    ]
   ],
   "title": "Exploring ROI size in deep learning based lipreading",
   "original": "13",
   "page_count": 6,
   "order": 13,
   "p1": 64,
   "pn": 69,
   "abstract": [
    "Automatic speechreading systems have increasingly exploited deep learning advances, resulting in dramatic gains over traditional methods. State-of-the-art systems typically employ convolutional neural networks (CNNs), operating on a video region-of-interest (ROI) that contains the speaker’s mouth. However, little or no attention has been paid to the effects of ROI physical coverage and resolution on the resulting recognition performance within the deep learning framework. In this paper, we investigate such choices for a visual-only speech recognition system based on CNNs and long short-term memory models that we present in detail. Further, we employ a separate CNN to perform face detection and facial landmark localization, driving ROI extraction. We conduct experiments on a multi-speaker corpus of connected digits utterances, recorded in ideal visual conditions. Our results show that ROI design affects automatic speechreading performance significantly: the best visual-only word error rate (5.07%) corresponds to a ROI that contains a large part of the lower face, in addition to just the mouth, and at a relatively high resolution. Noticeably, the result represents a 27% relative error reduction compared to employing the entire lower face as the ROI.\n"
   ],
   "doi": "10.21437/AVSP.2017-13"
  },
  "zimmermann17_avsp": {
   "authors": [
    [
     "Marina",
     "Zimmermann"
    ],
    [
     "Mostafa Mehdipour",
     "Ghazi"
    ],
    [
     "Hazim Kemal",
     "Ekenel"
    ],
    [
     "Jean-Philippe",
     "Thiran"
    ]
   ],
   "title": "Combining Multiple Views for Visual Speech Recognition",
   "original": "10",
   "page_count": 6,
   "order": 10,
   "p1": 47,
   "pn": 52,
   "abstract": [
    "Visual speech recognition is a challenging research problem with a particular practical application of aiding audio speech recognition in noisy scenarios. Multiple camera setups can be beneficial for the visual speech recognition systems in terms of improved performance and robustness. In this paper, we explore this aspect and provide a comprehensive study on combining multiple views for visual speech recognition. The thorough analysis covers fusion of all possible view angle combinations both at feature level and decision level. The employed visual speech recognition system in this study extracts features through a PCA-based convolutional neural network, followed by an LSTM network. Finally, these features are processed in a tandem system, being fed into a GMM-HMM scheme. The decision fusion acts after this point by combining the Viterbi path log-likelihoods. The results show that the complementary information contained in recordings from different view angles improves the results significantly. For example, the sentence correctness on the test set is increased from 76% for the highest performing single view (30◦) to up to 83% when combining this view with the frontal and 60◦ view angles.\n"
   ],
   "doi": "10.21437/AVSP.2017-10"
  },
  "davis17_avsp": {
   "authors": [
    [
     "Chris",
     "Davis"
    ],
    [
     "Jeesun",
     "Kim"
    ],
    [
     "Outi",
     "Tuomainen"
    ],
    [
     "Valerie",
     "Hazan"
    ]
   ],
   "title": "The effect of age and hearing loss on partner-directed gaze in a communicative task",
   "original": "3",
   "page_count": 4,
   "order": 3,
   "p1": 12,
   "pn": 15,
   "abstract": [
    "The study examined the partner-directed gaze patterns of old and young talkers in a task (DiapixUK) that involved two people (a lead talker and a follower) engaging in a spontaneous dialogue. The aim was (1) to determine whether older adults engage less in partner-directed gaze than younger adults by measuring mean gaze frequency and mean total gaze duration; and (2) examine the effect that mild hearing loss may have on older adult’s partner-directed gaze. These were tested in various communication conditions: a no barrier condition; BAB2 condition in which the lead talker and the follower spoke and heard each other in multitalker babble noise; and two barrier conditions in which the lead talker could hear clearly their follower but the follower could not hear the lead talker very clearly (i.e., the lead talker’s voice was degraded by babble (BAB1) or by a Hearing Loss simulation (HLS). 57 single-sex pairs (19 older adults with mild Hearing Loss, 17 older adults with Normal Hearing and 21 younger adults) participated in the study. We found that older adults with normal hearing produced fewer partner-directed gazes (and gazed less overall) than either the older adults with hearing loss or younger adults for the BAB1 and HLS conditions. We propose that this may be due to a decline in older adult’s attention to cues signaling how well a conversation is progressing. Older adults with hearing loss, however, may attend more to visual cues because they give greater weighting to these for understanding speech.\n"
   ],
   "doi": "10.21437/AVSP.2017-3"
  },
  "tamura17_avsp": {
   "authors": [
    [
     "Satoshi",
     "Tamura"
    ],
    [
     "Koichi",
     "Miyazaki"
    ],
    [
     "Satoru",
     "Hayamizu"
    ]
   ],
   "title": "Lipreading using deep bottleneck features for optical and depth images",
   "original": "abs1",
   "page_count": 2,
   "order": 15,
   "p1": 76,
   "pn": 77,
   "abstract": [
    "",
    "This paper investigates a lipreading scheme employing optical and depth modalities, with using deep bottleneck features. Optical and depth data are captured by Microsoft Kinect v2, followed by computing an appearance-based feature set in each modality. A basic feature set is then converted into a deep bottleneck feature using a deep neural network having a bottleneck layer. Multi-stream hidden Marcov models are used for recognition. We evaluated the method using our connected-digit corpus, comparing to our previous method. It is finally found that we could improve lipreading performance by employing deep bottleneck features.\n"
   ]
  },
  "aubanel17_avsp": {
   "authors": [
    [
     "Vincent",
     "Aubanel"
    ],
    [
     "Cassandra",
     "Masters"
    ],
    [
     "Jeesun",
     "Kim"
    ],
    [
     "Chris",
     "Davis"
    ]
   ],
   "title": "Contribution of visual rhythmic information to speech perception in noise",
   "original": "18",
   "page_count": 5,
   "order": 19,
   "p1": 95,
   "pn": 99,
   "abstract": [
    "Visual speech information helps listeners perceive speech in noise. The cues underpinning this visual advantage appear to be global and distributed, and previous research hasn’t succeeded in pinning down simple dimensions to explain the effect. In this study we focus on the temporal aspects of visual speech cues. In comparison to a baseline of auditory only sentences mixed with noise, we tested the effect of making available a visual speech signal that carries the rhythm of the spoken sentence, through a temporal visual mask function linked to the times of the auditory p-centers, as quantified by stressed syllable onsets. We systematically varied the relative alignment of the peaks of the maximum exposure of visual speech cues with the presumed anchors of sentence rhythm and contrasted these speech cues against an abstract visual condition, whereby the visual signal consisted of a stylised moving curve with its dynamics deter- mined by the mask function. We found that both visual signal types provided a significant benefit to speech recognition in noise, with the speech cues providing the largest benefit. The benefit was largely independent of the amount of delay in relation to the auditory p-centers. Taken together, the results call for further inquiry into temporal dynamics of visual and auditory speech.\n"
   ],
   "doi": "10.21437/AVSP.2017-18"
  },
  "petridis17_avsp": {
   "authors": [
    [
     "Stavros",
     "Petridis"
    ],
    [
     "Yujiang",
     "Wang"
    ],
    [
     "Zuwei",
     "Li"
    ],
    [
     "Maja",
     "Pantic"
    ]
   ],
   "title": "End-to-End Audiovisual Fusion with LSTMs",
   "original": "8",
   "page_count": 5,
   "order": 8,
   "p1": 36,
   "pn": 40,
   "abstract": [
    "Several end-to-end deep learning approaches have been recently presented which simultaneously extract visual features from the input images and perform visual speech classification. How- ever, research on jointly extracting audio and visual features and performing classification is very limited. In this work, we present an end-to-end audiovisual model based on Bidirectional Long Short-Term Memory (BLSTM) networks. To the best of our knowledge, this is the first audiovisual fusion model which simultaneously learns to extract features directly from the pixels and spectrograms and perform classification of speech and non- linguistic vocalisations. The model consists of multiple identical streams, one for each modality, which extract features directly from mouth regions and spectrograms. The temporal dynamics in each stream/modality are modeled by a BLSTM and the fusion of multiple streams/modalities takes place via another BLSTM. An absolute improvement of 1.9% in the mean F1 of 4 nonlingusitic vocalisations over audio-only classification is re- ported on the AVIC database. At the same time, the proposed end-to-end audiovisual fusion system improves the state-of-the- art performance on the AVIC database leading to a 9.7% absolute increase in the mean F1 measure. We also perform audiovisual speech recognition experiments on the OuluVS2 database using different views of the mouth, frontal to profile. The pro- posed audiovisual system significantly outperforms the audio- only model for all views when the acoustic noise is high.\n"
   ],
   "doi": "10.21437/AVSP.2017-8"
  }
 },
 "sessions": [
  {
   "title": "Special Session",
   "papers": [
    "burnham17_avsp"
   ]
  },
  {
   "title": "Gaze and Handedness",
   "papers": [
    "nixon17_avsp",
    "davis17_avsp",
    "nunnemann17_avsp",
    "stoltmann17_avsp",
    "vinay17_avsp"
   ]
  },
  {
   "title": "AV by machines",
   "papers": [
    "kroos17_avsp",
    "petridis17_avsp",
    "websdale17_avsp",
    "zimmermann17_avsp",
    "ouni17_avsp",
    "cullen17_avsp"
   ]
  },
  {
   "title": "Lip reading by machines",
   "papers": [
    "koumparoulis17_avsp",
    "sterpu17_avsp",
    "tamura17_avsp",
    "liu17_avsp"
   ]
  },
  {
   "title": "Prosody and Timing",
   "papers": [
    "barone17_avsp",
    "ambrazaitis17_avsp",
    "aubanel17_avsp",
    "behne17_avsp"
   ]
  },
  {
   "title": "Emotion & Attitudes",
   "papers": [
    "yamamoto17_avsp",
    "kawahara17_avsp",
    "kawase17_avsp",
    "mixdorff17_avsp",
    "hoenemann17_avsp"
   ]
  },
  {
   "title": "AV Perception",
   "papers": [
    "massaro17_avsp",
    "fort17_avsp",
    "irwin17_avsp",
    "jesse17_avsp",
    "tiippana17_avsp"
   ]
  }
 ],
 "doi": "10.21437/AVSP.2017"
}